<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc">TMC - 128</h2>
<ul>
<li><details>
<summary>
(2025). Combating BLE weak links by combining PHY layer symbol extension and link layer coding. <em>TMC</em>, <em>24</em>(10), 11277-11291. (<a href='https://doi.org/10.1109/TMC.2025.3579934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) technology supports various Internet-of-Things (IoT) applications. However, because of their limited transmission power and channel interference, their performance is deficient over weak links. Extending physical layer symbols or using error correction code to the link layer is effective somehow. Introducing excessive BLE bits to both respectively can also decrease the network throughput. To optimize the BLE technology performance, we propose CPL, a combining PHY and link layer optimization technology that adaptively allocates BLE bits to both the physical layer and link layer. Then we propose the Cross-Layer BLE Bits Dynamic Allocation Model that unifies the gain of BLE bits in different layers. Finally, we propose an Interference-Aware Controlled CFO Fine-Tuning Method that calibrates the model according to different interference patterns. We implement CPL on Commercial-Off-The-Shelf (COTS) BLE chips and SDR. The experiment results show that under various interference conditions, CPL achieves 50× and 32.16% throughput improvement over RSBLE and Symphony. CPL reduces energy consumption by 60.42% to 97.95% compared to RSBLE, and 11.04% to 25.15% compared to Symphony.},
  archive      = {J_TMC},
  author       = {Renjie Li and Yeming Li and Jiamei Lv and Hailong Lin and Yi Gao and Wei Dong},
  doi          = {10.1109/TMC.2025.3579934},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11277-11291},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Combating BLE weak links by combining PHY layer symbol extension and link layer coding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for joint trajectory control and task offloading in large-scale partially observable UAV-assisted MEC. <em>TMC</em>, <em>24</em>(10), 11259-11276. (<a href='https://doi.org/10.1109/TMC.2025.3579748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing joint trajectory control and task offloading (JTCTO) algorithms offer ultra-low latency services for smart devices (SDs) in uncrewed aerial vehicle (UAV)-assisted mobile edge computing (MEC). However, these JTCTO algorithms typically require large training datasets to learn the optimal policies, leading to low learning efficiency. Additionally, most existing JTCTO algorithms are difficult to scale to environments with more than a few UAVs, as their complexity increases exponentially with the number of UAVs. In this paper, we propose a decentralized JTCTO algorithm based on the Policy Transfer and Mean Field-based Multi-Agent Actor-Critic (PTMF-MAAC). First, a novel policy transfer algorithm is proposed to determine which UAV’s JTCTO strategy is helpful for each UAV and when to terminate the strategy to accelerate the learning efficiency of the UAV. Second, we propose a partially observable mean field algorithm that significantly reduces the model space by replacing the influence of all other UAVs on a particular UAV with an average value, thereby adapting to large-scale UAV scenarios. Experiments have shown that compared to the baseline, PTMF-MAAC reduces the system cost by 18.44%$\sim$28.57% and improves the model learning efficiency and adaptability to partially observable large-scale UAV-assisted MEC.},
  archive      = {J_TMC},
  author       = {Zhen Gao and Gang Wang and Lei Yang and Yu Dai},
  doi          = {10.1109/TMC.2025.3579748},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11259-11276},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Transfer learning for joint trajectory control and task offloading in large-scale partially observable UAV-assisted MEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative UAV-mounted RISs-assisted energy-efficient communications. <em>TMC</em>, <em>24</em>(10), 11241-11258. (<a href='https://doi.org/10.1109/TMC.2025.3579597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative reconfigurable intelligent surfaces (RISs) are promising technologies for 6G networks to support a great number of users. Compared with the fixed RISs, the properly deployed RISs may improve the communication performance with less communication energy consumption, thereby improving the energy efficiency. In this paper, we consider a cooperative uncrewed aerial vehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs are carried and enhanced by UAVs to serve multiple ground users (GUs) simultaneously such that achieving the three-dimensional (3D) mobility and opportunistic deployment. Specifically, we formulate an energy-efficient communication problem based on multi-objective optimization framework (EEComm-MOF) to jointly consider the beamforming vector of base station (BS), the location deployment and the discrete phase shifts of UAV-RIS system so as to simultaneously maximize the minimum available rate over all GUs, maximize the total available rate of all GUs, and minimize the total energy consumption of the system, while the transmit power constraint of BS is considered. To comprehensively solve EEComm-MOF which is an NP-hard and non-convex problem with constraints, a non-dominated sorting genetic algorithm-II with a continuous solution processing mechanism, a discrete solution processing mechanism, and a complex solution processing mechanism (INSGA-II-CDC) is proposed. Simulations results demonstrate that the proposed INSGA-II-CDC can solve EEComm-MOF effectively and outperforms other benchmarks under different parameter settings. Moreover, the stability of INSGA-II-CDC and the effectiveness of the improved mechanisms are verified. Finally, the implementability analysis of the algorithm is given.},
  archive      = {J_TMC},
  author       = {Hongyang Pan and Yanheng Liu and Geng Sun and Qingqing Wu and Tierui Gong and Pengfei Wang and Dusit Niyato and Chau Yuen},
  doi          = {10.1109/TMC.2025.3579597},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11241-11258},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cooperative UAV-mounted RISs-assisted energy-efficient communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive charging with beam steering. <em>TMC</em>, <em>24</em>(10), 11224-11240. (<a href='https://doi.org/10.1109/TMC.2025.3579692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturation of wireless power transfer technology, Wireless Rechargeable Sensor Networks (WRSNs) have been able to provide a continuous energy supply by scheduling a Mobile Charger (MC). However, traditional charging modes suffer from fixed charging areas that lack the ability to adapt to variable sensor distributions. This inflexibility yields a gap between energy supply and utilization, resulting in relatively low charging efficiency. To address this issue, we propose an adaptive charging mode that utilizes beam steering to dynamically adjust the charging area, thereby catering to different sensor distributions encountered during the charging process. First, we build a dual-symmetric steering charging model to describe the characteristics of dynamic beam steering, enabling precise manipulation of the charging area. Then, we develop a charging power discretization based on steering angle and charging distance to obtain a finite feasible charging strategy set for MC. We reformalize charging utility maximization under energy constraints as a submodular function maximization problem, and propose an approximate algorithm to solve it. Lastly, simulations and field experiments demonstrate that our scheme outperforms other algorithms by 43.9% on average.},
  archive      = {J_TMC},
  author       = {Meixuan Ren and Haipeng Dai and Linglin Zhang and Tang Liu},
  doi          = {10.1109/TMC.2025.3579692},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11224-11240},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive charging with beam steering},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-empowered game theoretical incentive for secure bandwidth allocation in UAV-assisted wireless networks. <em>TMC</em>, <em>24</em>(10), 11209-11223. (<a href='https://doi.org/10.1109/TMC.2025.3579505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the promising unmanned aerial vehicle (UAV)-assisted wireless networks (UAWNs) have emerged by advocating the UAVs to provide wireless transmission services. However, owing to the ever-growing volume of data traffic and the untrusted network operation environment, efficiently and securely assigning limited bandwidth for high-quality wireless communication between UAVs and mobile users poses a significant challenge. To address this challenge, we propose a novel secure UAV-bandwidth allocation scheme to provision reliable wireless transmission services for mobile users in UAWNs. Specifically, we first introduce a novel blockchain-empowered framework for secure bandwidth allocation, designed to automate payment processes and deter malicious activities through the immutable logging of transactional and behavioral data. Wherein, a smart contract is designed to regulate the honest behaviors of both mobile users and UAVs during bandwidth allocation with a distributed manner. Besides, a delegated proof-of-stake (DPoS) with reputation consensus protocol is presented to ensure the authenticity and efficiency of the decision-making process. Further, we apply the Stackelberg game theory to model the dynamic of the bandwidth allocation between mobile users and UAVs. In this game, the UAVs act as game leaders to determine the bandwidth price, while each mobile user acts as a game follower, making decision on the bandwidth request. We utilize the backward induction method to derive the optimal strategies of both parties, culminating in the identification of the Stackelberg equilibrium of the formulated game. Finally, extensive simulations are carried out to show the superiority of the proposed scheme over conventional schemes in terms of security, efficiency, and fairness in bandwidth allocation.},
  archive      = {J_TMC},
  author       = {Qichao Xu and Zhou Su and Haixia Peng and Yuan Wu and Ruidong Li},
  doi          = {10.1109/TMC.2025.3579505},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11209-11223},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-empowered game theoretical incentive for secure bandwidth allocation in UAV-assisted wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed modulation exploiting IRS for secure communications. <em>TMC</em>, <em>24</em>(10), 11193-11208. (<a href='https://doi.org/10.1109/TMC.2025.3579960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the broadcast nature of wireless communications, users’ data transmitted wirelessly is susceptible to security/privacy threats. The conventional modulation scheme “loads” all of the user’s transmitted information onto a physical signal. Then, as long as an adversary overhears and processes the signal, s/he may access the user’s information, hence breaching communication privacy. To counter this threat, we propose IRS-DMSC, a Distributed Modulation based Secure Communication (DMSC) scheme by exploiting Intelligent Reflecting Surface (IRS). Under IRS-DMSC, two sub-signals are employed to realize legitimate data transmission. Of these two signals, one is directly generated by the legitimate transmitter (Tx), while the other is obtained by modulating the phase of the direct signal and then reflecting it at the IRS in an indirect way. Both the direct and indirect signal components superimpose on each other at the legitimate receiver (Rx) to produce a waveform identical to that obtained under traditional centralized modulation (CM), so that the legitimate Rx can employ the conventional demodulation method to recover the desired data from the received signal. IRS-DMSC incorporates the characteristics of wireless channels into the modulation process, and hence can fully exploit the randomness of wireless channels to enhance transmission secrecy. However, due to the distribution and randomization of legitimate transmission, it becomes difficult or even impossible for an eavesdropper to wiretap the legitimate user’s information. Furthermore, in order to address the problem of decoding error incurred by the difference of two physical channels’ fading, we develop Relative Phase Calibration (RPC) and Constellation Point Calibration (CPC), to improve decoding correctness at the legitimate Rx. Our method design, experiment, and simulation have shown the proposed IRS-DMSC to prevent eavesdroppers from intercepting legitimate information while maintaining good performance of the legitimate transmission.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Siwei Le and Kang G. Shin and Jia Liu and Zheng Yan},
  doi          = {10.1109/TMC.2025.3579960},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11193-11208},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed modulation exploiting IRS for secure communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint trajectory and beamforming optimization for AAV-relayed integrated sensing and communication with mobile edge computing. <em>TMC</em>, <em>24</em>(10), 11180-11192. (<a href='https://doi.org/10.1109/TMC.2025.3573702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate joint trajectory and beamforming design for autonomous aerial vehicle (AAV)-relayed integrated sensing and communication (ISAC) systems with mobile edge eomputing (MEC) under the clutter environment. Due to the limited on-board computing capability, the AAV has to offload sensing echoes to the base station (BS) for efficient processing. A novel relay-based ISAC-then-offload frame structure is considered. We aim to maximize the throughput of the BS-AAV-user relaying link while ensuring sensing accuracy and efficient sensing data offloading. The non-convex problem is solved using an alternating optimization algorithm based on successive convex approximation (SCA). Simulation results illustrate that our proposed algorithm achieves near-optimal communication performance while guaranteeing sensing accuracy, addressing the balance between the communication and sensing performance. Furthermore, we evaluate the impact of critical system parameters including sensing constraints, power control factor, and AAV flight duration on communication performance, and explore the trade-offs between energy efficiency and spectral efficiency under varying sensing data intensity and offloading duration.},
  archive      = {J_TMC},
  author       = {Shanfeng Xu and Zhipeng Liu and Le Zhao and Ziyi Liu and Xinyi Wang and Zesong Fei and Arumugam Nallanathan},
  doi          = {10.1109/TMC.2025.3573702},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11180-11192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint trajectory and beamforming optimization for AAV-relayed integrated sensing and communication with mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmZeAR: Zero-effort cross-category action recognition with mmWave radar. <em>TMC</em>, <em>24</em>(10), 11164-11179. (<a href='https://doi.org/10.1109/TMC.2025.3573168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the widespread application of radio frequency (RF) signal-based human action recognition, traditional solutions can only recognize seen categories and the perception scope is restrained by the limited activity classes. When a novel category emerges, the model needs to be optimized again on additionally collected samples at the cost of computation and labor burden. To address this challenge, we develop the mmZeAR system, which learns semantic knowledge from available vision data as class attributes and then transforms the classification into a matching problem. Specifically, we build the attribute space by fusing the coarse-grained video classification features and fine-grained angle change features of 3D joint skeletons. Then we design an efficient feature extraction backbone named TriSqN, which integrates triple radar heatmaps into the final representations by sufficiently exploring the heterogeneous and complementary characteristics. Finally, a projection network is developed between semantic attributes and radar features to construct indirect relationships between samples and labels. By implementing mmZeAR on millimeter wave (mmWave) radar signal datasets, our extensive experiments have demonstrated its remarkable recognition accuracy in novel category recognition with zero effort and achieved state-of-the-art performance.},
  archive      = {J_TMC},
  author       = {Biyun Sheng and Jiabin Li and Hui Cai and Yiping Zuo and Li Lu and Fu Xiao},
  doi          = {10.1109/TMC.2025.3573168},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11164-11179},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmZeAR: Zero-effort cross-category action recognition with mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming catastrophic forgetting in federated continual graph learning for resource-limited mobile devices. <em>TMC</em>, <em>24</em>(10), 11151-11163. (<a href='https://doi.org/10.1109/TMC.2025.3573964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Graph Learning (FGL) enables multiple clients to collaboratively learn node representations from private subgraph data, such as user transactions or social networks. Local models are trained on clients and then aggregated by a central server, supporting large-scale graph learning without sharing raw data. However, most existing FGL methods assume that the number of nodes in the graph remains constant, while real-world scenarios often evolve, with new nodes and edges continually added and older ones removed due to limited device memory. We define this setting as Federated Continual Graph Learning (FCGL). In FCGL, global model aggregation may cause interference occur inter-task and inter-client, therefore, FCGL suffers from the global catastrophic forgetting, as the global model adapts to newly added nodes, it loses knowledge acquired from earlier graph data of clients. To address this, we propose GRE-FL, a generative replay framework, which can mitigate global catastrophic forgetting by generating a global summary graph at the server to preserve critical information from historical nodes. It also improves performance by equipping local models with a gating graph attention network for better feature extraction. Experiments show that GRE-FL achieves strong performance across multiple datasets.},
  archive      = {J_TMC},
  author       = {Jiyuan Feng and Xu Yang and Dongyi Zheng and Weihong Han and Binxing Fang and Qing Liao},
  doi          = {10.1109/TMC.2025.3573964},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11151-11163},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Overcoming catastrophic forgetting in federated continual graph learning for resource-limited mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing and scheduling of diffusion process for text-to-image generation in edge networks. <em>TMC</em>, <em>24</em>(10), 11137-11150. (<a href='https://doi.org/10.1109/TMC.2025.3574065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence-Generated Content (AIGC) technology is transforming content creation by enabling diverse customized and quality services. However, the limited computing resources on mobile devices hinder the provisioning of AIGC services at scale, pose challenges in guaranteeing user-satisfied content quality requirement. To address these challenges, we first investigate the characteristics of prompt category and inference models in Text-to-Image (T2I) diffusion process. It is observed that, model size, denoising steps, and computing resource, are three deciding factors to image generation utility. Based on this insight, we first design an edge-assisted AIGC service system to efficiently process multi-user T2I generative requests, employing a multi-flow queuing model to capture multi-user dynamics and characterize the impact of diffusion scheduling on service latency. The system schedules the diffusion process of T2I generation across edge-deployed models, balancing service quality and computing resource. To maximize generation utility under resource constraints, we propose a Monte Carlo Tree Search-based diffusion scheduling algorithm embedded with adaptive computing resource allocation subroutine. This algorithm ensures that, resource allocation dynamically adapts to scheduling decisions in real time, enabling an effective trade-off between service quality and latency. Extensive experimental comparison against baseline approaches demonstrates that, the proposed system can enhance the generation utility by up to 7.3$\%$, achieving a 2.9$\%$ improvement in quality score and a 33.3$\%$ reduction in service latency.},
  archive      = {J_TMC},
  author       = {Shuangwei Gao and Peng Yang and Yuxin Kong and Feng Lyu and Ning Zhang},
  doi          = {10.1109/TMC.2025.3574065},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11137-11150},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Characterizing and scheduling of diffusion process for text-to-image generation in edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMTO: Multi-vehicle multi-hop task offloading in MEC-enabled vehicular networks. <em>TMC</em>, <em>24</em>(10), 11125-11136. (<a href='https://doi.org/10.1109/TMC.2025.3576154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC)-enabled vehicular networks have emerged as a promising approach to enhancing the performance and efficiency of the Internet-of-Vehicles (IoV) applications. By leveraging some vehicles to act as transmission relays, multi-hop task offloading addresses the problem of intermittent connectivity between vehicles and edge servers to cope with the issues of network congestion or obstacles. However, two critical issues, i.e., uncooperative behaviors of selfish vehicles and network resource dynamics, resulting from multi-vehicle concurrent offloading are not fully considered in the existing work. To fill this gap, this paper proposes a novel and efficient task offloading scheme, namely MMTO, that exploits the multi-hop computational resources to maximize the system-wide profit, and supports incentive compatibility of vehicular users and concurrent offloading. Specifically, an iterative hierarchical estimation algorithm is designed to estimate the offloading delay and energy cost in order to iteratively optimize the offloading decisions. An energy-efficient routing approach is then proposed to schedule the transmission paths for the offloading vehicles. Furthermore, an effective reward-driven auction-based incentive mechanism is designed for incentivizing relayers and calculators to engage in collaboration. Both simulation and field experiments are conducted; extensive results demonstrate that MMTO outperforms the state-of-the-art approaches in terms of the system-wide profit improvement and overall task delay reduction.},
  archive      = {J_TMC},
  author       = {Wenjie Huang and Zhiwei Zhao and Geyong Min and Yang Wang and Zheng Chang},
  doi          = {10.1109/TMC.2025.3576154},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11125-11136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MMTO: Multi-vehicle multi-hop task offloading in MEC-enabled vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential federated learning in hierarchical architecture on non-IID datasets. <em>TMC</em>, <em>24</em>(10), 11110-11124. (<a href='https://doi.org/10.1109/TMC.2025.3573928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a real federated learning (FL) system, communication overhead for passing model parameters between the clients and the parameter server (PS) is often a bottleneck. Hierarchical federated learning (HFL) that poses multiple edge servers (ESs) between clients and the PS can partially alleviate communication pressure but still needs the aggregation of model parameters from multiple ESs at the PS. To further reduce communication overhead, we remove the central PS, so that each iteration only completes model training by transmitting the global model between two adjacent ES. We call this serial learning method Sequential FL (SFL). For the first time, we introduced SFL into HFL and proposed a novel algorithm adapted to this combined framework, called Fed-CHS. Convergence results are derived for strongly convex and non-convex loss functions under various data heterogeneity setups, which show comparable convergence performance with the algorithms for HFL or SFL solely. Experimental results provide evidence of the superiority of our proposed Fed-CHS on both communication overhead saving and test accuracy over baseline methods.},
  archive      = {J_TMC},
  author       = {Xingrun Yan and Shiyuan Zuo and Rongfei Fan and Han Hu and Li Shen and Puning Zhao and Yong Luo},
  doi          = {10.1109/TMC.2025.3573928},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11110-11124},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sequential federated learning in hierarchical architecture on non-IID datasets},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deterministic satellite-terrestrial integrated networks via resource adaptation and differentiated scheduling. <em>TMC</em>, <em>24</em>(10), 11092-11109. (<a href='https://doi.org/10.1109/TMC.2025.3574740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite-terrestrial integrated network (STIN) is a full-scale communication paradigm, which can support joint information processing and seamless service provision by leveraging satellites’ wide coverage and terrestrial networks’ high capacity. The existing STIN operates with insufficient synergy in transmission scheduling, impacting resource allocation efficiency and transmission delay optimization, particularly in complex transmission scenarios. In this paper, we design Deterministic STIN (DetSTIN), a novel architecture for STIN, along with two algorithms tailored for transmission scheduling to collaboratively optimize resource adaptation and service flow scheduling. Specifically, the DetSTIN enables the smooth interconnection and integration of heterogeneous networks by providing layered deterministic services. Besides, a genetic-based resource adaptation algorithm is designed for fixed-mobile-satellite heterogeneous networks to reduce resource allocation overhead while maintaining the network performance. Furthermore, we propose a deep reinforcement learning-based differentiated scheduling algorithm to solve the routing-queue two-dimensional decision problem to differentially optimize transmission delay of service flows, thus obtaining higher transmission scheduling benefit. By addressing resource adaptation and differentiated scheduling synergistically, the proposed solution achieves reduced resource allocation overhead and increased transmission scheduling benefit, ultimately leading to increased network operation revenue of the DetSTIN. Simulation results demonstrate that the proposed solution delivers effective performance across various flow proportions, and as the number of flows increases, the network operation revenue exhibits a noticeable improvement, compared with benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Weiting Zhang and Peixi Liao and Dong Yang and Qiang Ye and Shiwen Mao and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3574740},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11092-11109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward deterministic satellite-terrestrial integrated networks via resource adaptation and differentiated scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMART: Sim2Real meta-learning-based training for mmWave beam selection in V2X networks. <em>TMC</em>, <em>24</em>(10), 11076-11091. (<a href='https://doi.org/10.1109/TMC.2025.3576203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twins (DT) offer a low-overhead evaluation platform and the ability to generate rich datasets for training machine learning (ML) models before actual deployment. Specifically, for the scenario of ML-aided millimeter wave (mmWave) links between moving vehicles to roadside units, we show how DT can create an accurate replica of the real world for model training and testing. The contributions of this paper are twofold: First, we propose a framework to create a multimodal Digital Twin (DT), where synthetic images and LiDAR data for the deployment location are generated along with RF propagation measurements obtained via ray-tracing. Second, to ensure effective domain adaptation, we leverage meta-learning, specifically Model-Agnostic Meta-Learning (MAML), with transfer learning (TL) serving as a baseline validation approach. The proposed framework is validated using a comprehensive dataset containing both real and synthetic LiDAR and image data for mmWave V2X beam selection. It also enables the investigation of how each sensor modality impacts domain adaptation, taking into account the unique requirements of mmWave beam selection. Experimental results show that models trained on synthetic data using transfer learning and meta-learning, followed by minimal fine-tuning with real-world data, achieve up to 4.09× and 14.04× improvements in accuracy, respectively. These findings highlight the potential of synthetic data and meta-learning to bridge the domain gap and adapt rapidly to real-world beamforming challenges.},
  archive      = {J_TMC},
  author       = {Divyadharshini Muruganandham and Suyash Pradhan and Jerry Gu and Torsten Braun and Debashri Roy and Kaushik Chowdhury},
  doi          = {10.1109/TMC.2025.3576203},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11076-11091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SMART: Sim2Real meta-learning-based training for mmWave beam selection in V2X networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scene-aware model adaptation scheme for cross-scene online inference on mobile devices. <em>TMC</em>, <em>24</em>(10), 11061-11075. (<a href='https://doi.org/10.1109/TMC.2025.3574766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmannedaerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).},
  archive      = {J_TMC},
  author       = {Yunzhe Li and Hongzi Zhu and Zhuohong Deng and Yunlong Cheng and Zimu Zheng and Liang Zhang and Shan Chang and Minyi Guo},
  doi          = {10.1109/TMC.2025.3574766},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11061-11075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A scene-aware model adaptation scheme for cross-scene online inference on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-aware designs of multi-UAV deployment, task offloading and service placement in edge computing networks. <em>TMC</em>, <em>24</em>(10), 11046-11060. (<a href='https://doi.org/10.1109/TMC.2025.3574061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution to support wireless devices’ computation-intensive services in the absence of terrestrial infrastructures. Nevertheless, the heterogeneous nature of MEC services and the security vulnerability of wireless channels present significant challenges to achieving efficient and secure computation offloading. In this paper, we investigate a multi-UAV-assisted MEC network in which wireless devices need to process diverse computation tasks. The devices can perform local computing or offload their computation tasks to UAV servers that have pre-cached relevant service programs in the presence of eavesdroppers. To facilitate secure service provisioning, we propose a cooperative jamming-based scheme in which a UAV jammer transmits jamming signals to interfere with eavesdroppers during devices’ computation offloading processes. Taking into account UAV servers’ constrained caching spaces and secure offloading requirements, we minimize the total task completion delay of devices by jointly optimizing multi-UAV deployment, task offloading decisions, service placement, UAV jammer’s transmit power, and devices’ transmit power. To tackle the formulated mixed-integer nonlinear programming problem, we design an optimization-embedding multi-agent twin delayed deep deterministic policy gradient (OE-MATD3) algorithm. Specifically, the MATD3 approach is leveraged to deal with optimization variables concerning UAVs, while a closed-form solution for devices’ transmit power is derived and guides MATD3-based decision-making. Simulation results demonstrate that the proposed scheme outperforms baselines in terms of devices’ task completion delay.},
  archive      = {J_TMC},
  author       = {Mengru Wu and Haonan Wu and Weidang Lu and Lei Guo and Inkyu Lee and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3574061},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11046-11060},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-aware designs of multi-UAV deployment, task offloading and service placement in edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain generalization for CSI-based human activity recognition. <em>TMC</em>, <em>24</em>(10), 11034-11045. (<a href='https://doi.org/10.1109/TMC.2025.3573457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization remains a key challenge in human activity recognition based on channel state information (CSI). Different domains correspond to distinct data distributions, deviating from the typical assumption of independent and identically distributed (i.i.d.) data, which leads to significant performance degradation when models are applied to unseen domains. To address this issue, we propose a novel domain generalization model that integrates meta-learning initialization and an adaptive channel grouping attention mechanism. First, a meta-learning strategy is employed to acquire well-initialized parameters from multiple source domain tasks, enabling the model to implicitly enhance its cross-domain generalization ability. Second, an adaptive grouping attention mechanism is designed in the feature extraction stage to effectively capture the sensitivity differences of different subcarriers to human activities. Meanwhile, a random masking training mechanism is introduced to simulate real-world domain variations and improve model robustness. In addition, a domain adversarial training framework based on the gradient reversal layer (GRL) is adopted to mitigate domain-specific feature dependency, further enhancing the model’s generalization capability. We evaluate our proposed method on both a self-collected dataset, which includes human activity data from nine volunteers across six different environments, and a public CSI dataset. The experimental results demonstrate that our method significantly outperforms existing approaches in domain generalization performance, verifying its effectiveness and practical applicability.},
  archive      = {J_TMC},
  author       = {Tianqi Fan and Sen Qiu and Wei Gong and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3573457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11034-11045},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-source domain generalization for CSI-based human activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedDSV: Shapley value-based contribution estimation in federated learning with dynamic participation. <em>TMC</em>, <em>24</em>(10), 11019-11033. (<a href='https://doi.org/10.1109/TMC.2025.3574784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) succeeds in collaborative and privacy-preserving ML model training among multiple distributed data owners. To maintain a healthy FL ecosystem, it is crucial to estimate the contributions of all participants fairly. Due to provable fairness, Shapley value (SV) is widely used for contribution estimation in FL. However, current studies focus on static scenarios with fixed participants and neglect the dynamic settings with the random joining or leaving of participants in practice. This paper fills the gap by proposing FedDSV, a novel contribution estimation framework for FL with dynamic participation. FedDSV supports flexible weighting mechanisms and is compatible with the SV fairness properties in dynamic scenarios. To reduce the computational complexity, we propose a Monte Carlo variant sampling method (SMC), which can adapt well to dynamic scenarios and approximate the true SVs. To evaluate the effectiveness and efficiency of our proposed approaches, extensive experiments under different settings (e.g., frequency switching, low-quality detection, etc.) are conducted on both i.i.d and non-i.i.d. distributions. Experimental results demonstrate that FedDSV can reflect the real utility contribution of data sources for dynamic FL, and SMC can approximate the exact dynamic SVs with larger similarities in a much shorter time than the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Kaijia Lei and Xuebin Ren and Shusen Yang and Xiaocheng Wang and Fangyuan Zhao},
  doi          = {10.1109/TMC.2025.3574784},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11019-11033},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedDSV: Shapley value-based contribution estimation in federated learning with dynamic participation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPREAR:An efficient attribute-based proxy re-encryption scheme with fast revocation for data sharing in AIoT. <em>TMC</em>, <em>24</em>(10), 11005-11018. (<a href='https://doi.org/10.1109/TMC.2025.3573288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Intelligence of Things (AIoT) is driving human society from “information” to “intelligence”, and the information technology industry is undergoing tremendous changes. However, AIoT data faces security threats such as leakage and illegal access when assisted by third parties. Therefore, some scholars use attribute-based proxy re-encryption (ABPRE) for secure sharing of data. However, the existing ABPRE schemes suffer from high computational overhead and inefficient attribution revocation, which seriously hinders practical application. To solve these problems, in this paper, we propose an efficient attribute-based proxy re-encryption scheme with fast attribute revocation (EPREAR). We design a non-interactive zero-knowledge proof protocol based on blockchain to ensure the verifiability of the key during attribute revocation. Furthermore, we devise a boundless encryption and decryption mechanism to enable the system’s encryption and decryption with a fixed computation overhead, regardless of the size of the attribute set. And EPREAR possesses the ability to add infinite attributes without re-initializing the system. Finally, we perform theoretical and experimental analyses that show EPREAR has excellent computational performance. As a consequence, it has better application value in AIoT.},
  archive      = {J_TMC},
  author       = {Xiaoxiao Li and Yong Xie and Cong Peng and Entao Luo and Xiong Li and Zhili Zhou},
  doi          = {10.1109/TMC.2025.3573288},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11005-11018},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EPREAR:An efficient attribute-based proxy re-encryption scheme with fast revocation for data sharing in AIoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-driven proactive caching with DRL in sustainable cloud-to-edge continuum. <em>TMC</em>, <em>24</em>(10), 10992-11004. (<a href='https://doi.org/10.1109/TMC.2025.3577197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted edge computing scenarios can intelligently cache and update the content periodically, thereby enhancing users’ overall perception of service, which is called quality of experience (QoE). To maximize QoE in cloud-to-edge continuum, we formulate a multi-objective optimization problem, which optimizes the cache hit ratio while simultaneously minimizing traffic load and time latency. Particularly, we present an innovative algorithm named Hyperdimensional Transformer with Priority Experience Playback-based Agent Deep network (HT-PAD), which provides a complete solution for prediction and decision-making for proactive caching. First, to improve the prediction accuracy of cached content, we use the encoding layer in hyperdimensional (HD) computing to extract the information features. Second, HD-Transformer, as the prediction part of HT-PAD, is proposed to make predictions based on user preferences, historical information, and popular information. HD-Transformer uses deep neural networks to predict user preferences and process time series data by combining hyperdimensional computation with the Transformer. Third, to avoid errors in the prediction content, we employ PER-MADDPG as the decision-making part of HT-PAD, which consists of Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Prioritized Experience Replay (PER). We use MADDPG to improve content decision-making and utilize PER to select appropriate training samples for PER-MADDPG. Finally, our experiments show that our proposed approach achieves strong performance in terms of edge hit ratio, latency, and traffic load, thus improving QoE.},
  archive      = {J_TMC},
  author       = {Xiaoming He and Yunzhe Jiang and Huajun Cui and Yinqiu Liu and Mingkai Chen and Maher Guizani and Shahid Mumtaz},
  doi          = {10.1109/TMC.2025.3577197},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10992-11004},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-driven proactive caching with DRL in sustainable cloud-to-edge continuum},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated meta-learning based computation offloading approach with energy-delay tradeoffs in UAV-assisted VEC. <em>TMC</em>, <em>24</em>(10), 10978-10991. (<a href='https://doi.org/10.1109/TMC.2025.3573278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) provides an applicable solution for computation offloading in Unmanned Aerial Vehicle(UAV)-assisted Vehicular Edge Computing (VEC) by preserving privacy. However, the heterogeneity of clients brings challenges to the generalization of models. Therefore, we propose a federated meta-learning (FML) framework to solve computation offloading for UAV-assisted VEC. In this paper, we are concerned with computation offloading of temporary hotspot regions due to traffic congestion. First, we construct a computation offloading problem with energy-delay tradeoffs and convert the problem to a Markov Decision Process (MDP). Then, we use FML to train personalized models for different vehicles while enhancing the generalization, we propose a Graph neural network-based FL Probabilistic Embedding for Actor-critic RL (GFL-PEARL) algorithm. We model the context as a Directed Acyclic Graph (DAG) and use GNN to reconstruct the inference network of the PEARL algorithm to extract the correlation between contexts fully. We dynamically adjust the task priority during the FML training process to improve the sampling efficiency. Finally, we verify the performance of the algorithm through simulation and physical experiments. Experimental results show that our algorithm can reduce average cost and task overtime rate by 31% and 56% respectively compared with the benchmarks.},
  archive      = {J_TMC},
  author       = {Chunlin Li and Chaoyue Deng and Yong Zhang and Shaohua Wan},
  doi          = {10.1109/TMC.2025.3573278},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10978-10991},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated meta-learning based computation offloading approach with energy-delay tradeoffs in UAV-assisted VEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual dependency-aware collaborative service caching and task offloading in vehicular edge computing. <em>TMC</em>, <em>24</em>(10), 10963-10977. (<a href='https://doi.org/10.1109/TMC.2025.3573379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although some studies in recent years have focused on the coexistence of service and task dependencies in the collaborative optimization of service caching and task offloading in Vehicle Edge Computing (VEC), the challenges brought by dual dependencies have not been fully addressed. Therefore, this paper proposes a more comprehensive joint optimization method for service caching and task offloading under dual dependencies. First, this paper proposes a service criticality prediction method based on the Gated Graph Recurrent Network (GGRN) to perceive complex task dependencies and accurately capture the service requirements of critical task types. Based on this, a hierarchical active-passive hybrid caching strategy is designed, which aims to satisfy diverse service demands while reducing the additional overhead caused by remote service requests. Second, a global task priority computation method based on application heterogeneity has been developed to prevent cascading delays in task chains. Finally, this paper formulates a joint optimization problem for service caching and task offloading in a three-layer VEC system, models it as a markov decision process, and applies a proximal policy optimization-driven collaborative optimization algorithm named COHCTO. Simulation results show that COHCTO achieves multi-objective optimization across metrics such as delay, energy consumption, caching hit rate, and application success rate under conditions different from those of other algorithms.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Lu Sun and Ammar Hawbani and Zhi Liu and Xiongyan Tang and Lexi Xu},
  doi          = {10.1109/TMC.2025.3573379},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10963-10977},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual dependency-aware collaborative service caching and task offloading in vehicular edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient collaborative edge transformer inference with hybrid model parallelism. <em>TMC</em>, <em>24</em>(10), 10945-10962. (<a href='https://doi.org/10.1109/TMC.2025.3574695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users’ privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy+, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy+ introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity and memory-aware parallelism planning for fully exploiting the resource potential. To mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments, Galaxy+ devises a tile-based fine-grained overlapping of communication and computation. Furthermore, a fault-tolerant re-scheduling mechanism is developed to address device-level resource dynamics, ensuring stable and low-latency inference. Extensive evaluation based on prototype implementation demonstrates that Galaxy+ remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving a $1.2\times$ to $4.24\times$ end-to-end latency reduction. Besides, Galaxy+ can adapt to device-level resource dynamics, swiftly rescheduling and restoring inference in the presence of unexpected straggler devices.},
  archive      = {J_TMC},
  author       = {Shengyuan Ye and Bei Ouyang and Jiangsu Du and Liekang Zeng and Tianyi Qian and Wenzhong Ou and Xiaowen Chu and Deke Guo and Yutong Lu and Xu Chen},
  doi          = {10.1109/TMC.2025.3574695},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10945-10962},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource-efficient collaborative edge transformer inference with hybrid model parallelism},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicular edge intelligence: DRL-based resource orchestration for task inference in vehicle-RSU-edge collaborative networks. <em>TMC</em>, <em>24</em>(10), 10927-10944. (<a href='https://doi.org/10.1109/TMC.2025.3572296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular edge intelligence, distinct from traditional edge intelligence, exhibits unique characteristics, including the mobility of vehicles, uneven spatial and temporal distribution of vehicles, and variability in the AI models deployed on vehicles, Roadside Units (RSUs), and edge servers (ESs). In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource orchestration scheme for task inference in vehicle-RSU-edge collaborative networks. In our approach, vehicles’ inference tasks can be processed on the vehicles, RSUs, or ESs, encompassing a total of 9 possible scenarios based on the cross-RSU mobility of vehicles. The scheme jointly optimizes task processing decision-making, transmission power allocation, computational resource allocation, and transmission rate allocation. The objective is to minimize the total cost, which involves a trade-off between task processing latency, energy consumption and inference error rate across all vehicle tasks. We design a DRL algorithm that decomposes the original optimization problem into sub-problems and efficiently solves them by combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm with multiple numerical methods. We analyzed the complexity and convergence of the algorithm. Specifically, we demonstrated its low complexity and fast, stable convergence, which prove its effectiveness in solving the problem. And we demonstrate the superiority of our scheme by comparing it with 5 benchmark schemes across 6 different scenarios.},
  archive      = {J_TMC},
  author       = {Wenhao Fan and Yang Yu and Chenhui Bao and Yuan’an Liu},
  doi          = {10.1109/TMC.2025.3572296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10927-10944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vehicular edge intelligence: DRL-based resource orchestration for task inference in vehicle-RSU-edge collaborative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSIPose: Unveiling human poses using commodity WiFi devices through the wall. <em>TMC</em>, <em>24</em>(10), 10914-10926. (<a href='https://doi.org/10.1109/TMC.2025.3571469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of WiFi devices and the development of WiFi sensing have alerted people to the threat of WiFi sensing-based privacy leakage, especially the privacy of human poses. Existing work on human pose estimation is deployed in indoor scenarios or simple occlusion (e.g., a wooden screen) scenarios, which are less privacy-threatening in attack scenarios. To reveal the risk of leakage of the pose privacy to users from commodity WiFi devices, we propose CSIPose, a privacy-acquisition attack that passively estimates dynamic and static human poses in through-the-wall scenarios. We design a three-branch network based on transfer learning, auto-encoder, and self-attention mechanisms to realize the supervision of video frames over CSI frames to generate human pose skeleton frames. Notably, we design AveCSI, a unified framework for preprocessing and feature extraction of CSI data corresponding to dynamic and static poses. This framework uses the average of CSI measurements to generate CSI frames to mitigate the instability of passively collected CSI data, and utilizes a self-attention mechanism to enhance key features. We evaluate the performance of CSIPose across different room layouts, subjects, devices, subject locations, and device locations. Evaluation results emphasize the generalizability of CSIPose. Finally, we discuss measures to mitigate this attack.},
  archive      = {J_TMC},
  author       = {Yangyang Gu and Jing Chen and Congrui Chen and Kun He and Ju Jia and Yebo Feng and Ruiying Du and Cong Wu},
  doi          = {10.1109/TMC.2025.3571469},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10914-10926},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSIPose: Unveiling human poses using commodity WiFi devices through the wall},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You can wash hands better: Accurate daily handwashing assessment with a smartwatch. <em>TMC</em>, <em>24</em>(10), 10900-10913. (<a href='https://doi.org/10.1109/TMC.2025.3571805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand hygiene is among the most effective daily practices for preventing infectious diseases such as influenza, malaria, and skin infections. While professional guidelines emphasize proper handwashing to reduce the risk of viral infections, surveys reveal that adherence to these recommendations remains low. To address this gap, we propose UWash, a wearable solution leveraging smartwatches to evaluate handwashing procedures, aiming to raise awareness and cultivate high-quality handwashing habits. We frame the task of handwashing assessment as an action segmentation problem, similar to those in computer vision, and introduce a simple yet efficient two-stream UNet-like network to achieve this goal. Experiments involving 51 subjects demonstrate that UWash achieves 92.27% accuracy in handwashing gesture recognition, an error of $&lt; $0.5 seconds in onset/offset detection, and an error of $&lt; $5 points in gesture scoring under user-dependent settings. The system also performs robustly in user-independent and user-independent-location-independent evaluations. Remarkably, UWash maintains high performance in real-world tests, including evaluations with 10 random passersby at a hospital 9 months later and 10 passersby in an in-the-wild test conducted 2 years later. UWash is the first system to score handwashing quality based on gesture sequences, offering actionable guidance for improving daily hand hygiene.},
  archive      = {J_TMC},
  author       = {Fei Wang and Tingting Zhang and Xilei Wu and Pengcheng Wang and Xin Wang and Han Ding and Jingang Shi and Jinsong Han and Dong Huang},
  doi          = {10.1109/TMC.2025.3571805},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10900-10913},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {You can wash hands better: Accurate daily handwashing assessment with a smartwatch},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast robustness enhancement for dynamic IIoT topology with adaptive bayesian learning. <em>TMC</em>, <em>24</em>(10), 10886-10899. (<a href='https://doi.org/10.1109/TMC.2025.3571431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In resource-constrained and dynamic Industrial Internet of Things (IIoT) environments, ensuring robust and adaptable network topologies remains a significant challenge. Existing reinforcement learning-based approaches tackle topology optimization but face scalability issues due to high computational complexity and latency under strict time constraints. To address these challenges, we propose FRED-ABL (Fast Robustness Enhancement for Dynamic IIoT topology optimization with Adaptive Bayesian Learning), a novel paradigm that delivers lightweight topology solutions within a constrained time frame. FRED-ABL introduces an innovative topology structure compression method leveraging auxiliary continuous coding, enabling lossless representation of network structures as model inputs. It further defines a new robustness performance metric that integrates considerations of node failures and connection capabilities, serving as a comprehensive evaluation function. By developing an adaptive Bayesian learning model, FRED-ABL efficiently maps the relationship between topology structures and robustness metrics, enabling rapid optimization while significantly reducing computational overhead. Extensive experiments demonstrate that FRED-ABL consistently outperforms state-of-the-art methods, delivering superior robustness and optimization efficiency even in large-scale IIoT deployments.},
  archive      = {J_TMC},
  author       = {Ning Chen and Songwei Zhang and Xiaobo Zhou and Song Cao and Tie Qiu},
  doi          = {10.1109/TMC.2025.3571431},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10886-10899},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast robustness enhancement for dynamic IIoT topology with adaptive bayesian learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAGR: Simultaneous tracking and gait recognition with commodity wi-fi. <em>TMC</em>, <em>24</em>(10), 10868-10885. (<a href='https://doi.org/10.1109/TMC.2025.3570993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based services and identification hold promise for future smart home applications. Through them, we can provide customized services for specific users in current locations. Recent studies have demonstrated that Wi-Fi signals can be leveraged to achieve device-free tracking and gait recognition. Despite their good performance, these two technologies are not effectively integrated for the following reasons: First, the device-free tracking method might yield tracking results that conflict with human gait. Second, extracting gait features relies on knowing or accurately estimating the user’s trajectory. Consequently, gait recognition and tracking are inherently linked, but there has been no effective approach to integrate these two techniques. In this paper, we present STAGR, a system capable of Simultaneous Tracking And Gait Recognition. The main contribution of our technique is that we establish a theoretical model that reveals how to transform path-dependent spectra into path-independent spectra directly. Specifically, we conduct a preliminary study to demonstrate the need for simultaneous tracking and gait recognition. Second, we propose a novel method to extract path-independent gait features, which can significantly save execution time compared with the learning-based method. Third, we design a polar-coordinate filtering method to retain the gait features while correcting the trajectory. We implement a prototype STAGR system and conduct extensive experiments to verify the proposed mechanism. The experimental results show that we can realize simultaneous tracking and gait recognition. The median tracking error is $ 0.45\;{\rm{m}}$, while the recognition accuracy is 95.3% for 6 users.},
  archive      = {J_TMC},
  author       = {Xinyu Tong and Xiaoqiang Xu and Aiwen Yu and Xin Xie and Xiulong Liu and Wenyu Qu},
  doi          = {10.1109/TMC.2025.3570993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10868-10885},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {STAGR: Simultaneous tracking and gait recognition with commodity wi-fi},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust optimization for aerial multi-access edge computing via cooperation of UAVs and HAPs. <em>TMC</em>, <em>24</em>(10), 10853-10867. (<a href='https://doi.org/10.1109/TMC.2025.3571023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an extensive increment of computation demands, the aerial multi-access edge computing (MEC), mainly based on uncrewed aerial vehicles (UAVs) and high altitude platforms (HAPs), plays significant roles in future network scenarios. In detail, UAVs can be flexibly deployed, while HAPs are characterized with large capacity and stability. Hence, in this paper, we provide a hierarchical model composed of an HAP and multi-UAVs, to provide aerial MEC services. Moreover, considering the errors of channel state information from unpredictable environmental conditions, we formulate the problem to minimize the total energy cost with the chance constraint, which is a mixed-integer nonlinear problem with uncertain parameters and intractable to solve. To tackle this issue, we optimize the UAV deployment via the weighted K-means algorithm. Then, the chance constraint is reformulated via the distributionally robust optimization (DRO). Furthermore, based on the conditional value-at-risk mechanism, we transform the DRO problem into a mixed-integer second order cone programming, which is further decomposed into two subproblems via the primal decomposition. Moreover, to alleviate the complexity of the binary subproblem, we design a binary whale optimization algorithm. Finally, we conduct extensive simulations to verify the effectiveness and robustness of the proposed schemes by comparing with baseline mechanisms.},
  archive      = {J_TMC},
  author       = {Ziye Jia and Can Cui and Chao Dong and Qihui Wu and Zhuang Ling and Dusit Niyato and Zhu Han},
  doi          = {10.1109/TMC.2025.3571023},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10853-10867},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributionally robust optimization for aerial multi-access edge computing via cooperation of UAVs and HAPs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward generalized urban computing: Pretraining a spatial-temporal model for diverse urban tasks. <em>TMC</em>, <em>24</em>(10), 10840-10852. (<a href='https://doi.org/10.1109/TMC.2025.3573373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban computing leverages data analysis to improve urban areas’ efficiency and sustainability, tackling tasks like traffic management, crime forecasting, and air quality predictions. Current models, while efficient, often struggle with tasks beyond their initial training due to limited flexibility. Typically, new tasks require developing specialized models, which may not perform optimally with limited data. To overcome these challenges, we propose the development of a universal pretrained model that understands a city’s various aspects comprehensively. This model serves as a robust foundation, ready to be quickly adjusted for different urban tasks as they arise, even if they occur in different cities. Unlike language models, urban computing models must handle unique spatial-temporal dynamics, making standard pretraining techniques inadequate. Our approach includes a spatial-temporal module with multi-graph convolution and temporal attention mechanisms, capturing the necessary spatial-temporal patterns during pretraining. We also integrate a prompt-tuning module within this framework, which can be adapted for new predictive tasks. The results of extensive experiments on four urban predictive tasks across two cities demonstrate the effectiveness of our model.},
  archive      = {J_TMC},
  author       = {Yingqian Zhang and Chao Li and Shibo He and Xiangliang Zhang and Jiming Chen},
  doi          = {10.1109/TMC.2025.3573373},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10840-10852},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward generalized urban computing: Pretraining a spatial-temporal model for diverse urban tasks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute consistency segment resilient routing for LEO satellite mega constellations. <em>TMC</em>, <em>24</em>(10), 10823-10839. (<a href='https://doi.org/10.1109/TMC.2025.3570670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low earth orbit (LEO) satellite mega constellations are regarded to provide pervasive intelligent services in the upcoming sixth generation network via the inter-satellite links (ISL). However, the inherent challenges of LEO satellites including limited onboard resources and failure-prone topology, create substantial hurdles for multi-attribute services routing in mega constellations. In this paper, we propose a multi-attribute consistency segment resilient (MCSR) routing algorithm, and a segmentation approach is designed to partition the mega constellation into non-intersecting segment routing domains (SRDs) through joint optimization of intra- and inter-SDRs update time, which leads to the potential of balancing network load and minimizing routing convergence time. Then, we utilize the multi-attribute consistency to determine the dominant paths of ISLs within and between SRDs for multi-attribute services. Furthermore, we develop a resilient rerouting strategy that utilizes the ephemeris to manage periodic ISL handovers, and selects a reserved/recalculated candidate path from the dominant paths for ISL random failures. Thus, our MCSR routing can converge to an optimal path for multi-attribute services from the dominant paths under ISL failures in mega constellations. Finally, we develop a testbed and simulation results validate the advantages of MCSR routing in handling multi-attribute services and rerouting capability in response to failures.},
  archive      = {J_TMC},
  author       = {Zhuang Du and Jian Jiao and Hao Liu and Ye Wang and Qinyu Zhang},
  doi          = {10.1109/TMC.2025.3570670},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10823-10839},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-attribute consistency segment resilient routing for LEO satellite mega constellations},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward integrated sensing and communication: Interference-resistance design for WiFi sensing. <em>TMC</em>, <em>24</em>(10), 10807-10822. (<a href='https://doi.org/10.1109/TMC.2025.3570752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi has been widely used for local area networking of devices and Internet access in the past two decades. Many researchers exploit WiFi signals for target sensing through analyzing the Channel State Information (CSI) of signals affected by the target movement. With the development of 6G Integrated Sensing and Communication (ISAC), some researchers further consider using communication data packets for WiFi sensing. However, all the current works do not analyze the impact of ubiquitous interference on WiFi sensing performance. In this paper, we propose IRSensing, an interference-resistance design to improve the CSI quality under interference in the ISAC scenario, aiming to improve the WiFi sensing performance. IRSensing exploits the overall WiFi packet for CSI optimization. It first measures the interference level of each subcarrier based on variance analysis, then proposes a CSI optimization method based on maximal ratio combining to improve the CSI quality. It finally proposes a practical CSI enhancement process to adapt to complex interference situations in actual networks. We implement IRSensing on a hardware testbed and evaluate its performance under different settings. Experiment results show that it can significantly decrease the activity detection error rate by up to 80% and improve the classification accuracy by up to 15$\%$.},
  archive      = {J_TMC},
  author       = {Junmei Yao and Chaoyang Liu and Sheng Luo and Lu Wang and Tingting Zhang and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3570752},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10807-10822},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward integrated sensing and communication: Interference-resistance design for WiFi sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AOA sensor placement for anchor-assisted target localization in GNSS-denied environment: Formulation, bounds and optimization. <em>TMC</em>, <em>24</em>(10), 10792-10806. (<a href='https://doi.org/10.1109/TMC.2025.3570768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target localization technology is widely applied in various applications, such as rescue missions, robot navigation, and the Internet of Things. However, in some scenarios, the positions of sensors are unknown due to the load limitation of the sensor carriers and environmental interferences, resulting in the instability of the global navigation satellite system (GNSS). This paper focuses on optimal angle-of-arrival (AOA) sensor placement using multiple position-unknown sensors for target localization accuracy improvement. To guarantee the uniqueness of the target coordinate, at least two anchors are needed. The anchors are some static benchmark objects in the environment with priori known positions. Firstly, a new optimization problem for AOA target localization accuracy improvement incorporating position-unknown sensors and anchors is formulated. Secondly, the optimal theoretical localization accuracies of the unknown sensors and target are derived by minimizing the trace of the Cramér-Rao lower bounds (CRLBs). Thirdly, a mixture optimization method, including a geometrical initialization and the new proposed simultaneous perturbation stochastic approximation and adaptive momentum estimation (SPSA-Adam) algebraic algorithm, is developed. Then, the correctness of the new theoretical findings and the effectiveness of the proposed sensor placement optimization method are verified by simulation examples.},
  archive      = {J_TMC},
  author       = {Sheng Xu and Linlong Wu and Xianliang Li and Xinyu Wu and Tiantian Xu},
  doi          = {10.1109/TMC.2025.3570768},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10792-10806},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AOA sensor placement for anchor-assisted target localization in GNSS-denied environment: Formulation, bounds and optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICGNN: Graph neural network enabled scalable beamforming for MISO interference channels. <em>TMC</em>, <em>24</em>(10), 10778-10791. (<a href='https://doi.org/10.1109/TMC.2025.3570648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the graph neural network (GNN)-enabled beamforming design for interference channels. We propose a model termed interference channel GNN (ICGNN) to solve a quality-of-service constrained energy efficiency maximization problem. The ICGNN is two-stage, where the direction and power parts of beamforming vectors are learned separately but trained jointly via unsupervised learning. By formulating the dimensionality of features independent of the transceiver pairs, the ICGNN is scalable with the number of transceiver pairs. Besides, to improve the performance of the ICGNN, the hybrid maximum ratio transmission and zero-forcing scheme reduces the output ports, the feature enhancement module unifies the two types of links into one type, the subgraph representation enhances the message passing efficiency, and the multi-head attention and residual connection facilitate the feature extracting. Furthermore, we present the over-the-air distributed implementation of the ICGNN. Ablation studies validate the effectiveness of key components in the ICGNN. Numerical results also demonstrate the capability of ICGNN in achieving near-optimal performance with an average inference time less than 0.1 ms. The scalability of ICGNN for unseen problem sizes is evaluated and enhanced by transfer learning with limited fine-tuning cost. The results of the centralized and distributed implementations of ICGNN are illustrated.},
  archive      = {J_TMC},
  author       = {Changpeng He and Yang Lu and Bo Ai and Octavia A. Dobre and Zhiguo Ding and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3570648},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10778-10791},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ICGNN: Graph neural network enabled scalable beamforming for MISO interference channels},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DI2SDiff++: Activity style decomposition and diffusion-based fusion for cross-person generalization in activity recognition. <em>TMC</em>, <em>24</em>(10), 10760-10777. (<a href='https://doi.org/10.1109/TMC.2025.3572220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing domain generalization (DG) methods for cross-person sensor-based activity recognition tasks often struggle to capture both intra- and inter-domain style diversity, leading to significant domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity, termed Diversified Intra- and Inter-domain distributions via activity Style-fused Diffusion modeling (DI2SDiff). In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random style representations from the same class to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible combinations among existing styles to generate a broad spectrum of new style instances. We further extend DI2SDiff into DI2SDiff++ by enhancing the diversity of style guidance. Specifically, DI2SDiff++ integrates a multi-head style conditioner to provide multiple distinct, decomposed substyles and introduces a substyle-fused sampling strategy that allows cross-class substyle fusion for broader guidance. Empirical evaluations on a wide range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have been proven significant and valuable, enabling DI2SDiff and DI2SDiff++ to surpass state-of-the-art DG methods in various cross-person activity recognition tasks.},
  archive      = {J_TMC},
  author       = {Junru Zhang and Cheng Peng and Zhidan Liu and Lang Feng and Yuhan Wu and Yabo Dong and Duanqing Xu},
  doi          = {10.1109/TMC.2025.3572220},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10760-10777},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DI2SDiff++: Activity style decomposition and diffusion-based fusion for cross-person generalization in activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I sense you fast: Simultaneous action and identity inference by slimming multi-branch RadarNet. <em>TMC</em>, <em>24</em>(10), 10743-10759. (<a href='https://doi.org/10.1109/TMC.2025.3570757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing connection between internet and human society, millimeter-wave radar based action recognition and user authentication exhibit remarkable prospects in security scenarios. Existing solutions usually focus on one of the tasks and mainly emphasize accuracy without reducing the inference time. In this paper, we propose a dual-task based Polymorphic Lightweight (PolyLite) RadarNet framework, in which the shared features are fed into two split streams for different tasks under joint supervision. The polymorphic concept here means that the trained network with parallel designs can be slimmed as a single-branch structure for inference. By this design strategy, we can not only efficiently extract spatial-temporal features during the training stage but also largely improve the response speed for simultaneously testing human activities and identities. Specifically, we design triple-view (TRIview) video-like data as the input by successively concatenating the range-velocity and range-angle matrices. Then a PolyLite module with linear and lightweight designs in each branch is integrated into our RadarNet framework to learn discriminative representations. Experimental results demonstrate that our approach is able to reach the accuracy over 98${\%}$ within 0.21 ms inference time. Especially, untrained intruders can also be successfully identified by a simple matching computation.},
  archive      = {J_TMC},
  author       = {Biyun Sheng and Yan Bao and Hui Cai and Linqing Gui and Fu Xiao},
  doi          = {10.1109/TMC.2025.3570757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10743-10759},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {I sense you fast: Simultaneous action and identity inference by slimming multi-branch RadarNet},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning resilient to byzantine attacks and data heterogeneity. <em>TMC</em>, <em>24</em>(10), 10729-10742. (<a href='https://doi.org/10.1109/TMC.2025.3571058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses federated learning (FL) in the context of malicious Byzantine attacks and data heterogeneity. We introduce a novel Robust Average Gradient Algorithm (RAGA), which uses the geometric median for aggregation and allows flexible round number for local updates. Unlike most existing resilient approaches, which base their convergence analysis on strongly-convex loss functions or homogeneously distributed datasets, this work conducts convergence analysis for both strongly-convex and non-convex loss functions over heterogeneous datasets. The theoretical analysis indicates that as long as the fraction of the data from malicious users is less than half, RAGA can achieve convergence at a rate of $\mathcal {O}({1}/{T^{2/3- \delta }})$ for non-convex loss functions, where $T$ is the iteration number and $\delta \in (0, 2/3)$. For strongly-convex loss functions, the convergence rate is linear. Furthermore, the stationary point or global optimal solution is shown to be attainable as data heterogeneity diminishes. Experimental results validate the robustness of RAGA against Byzantine attacks and demonstrate its superior convergence performance compared to baselines under varying intensities of Byzantine attacks on heterogeneous datasets.},
  archive      = {J_TMC},
  author       = {Shiyuan Zuo and Xingrun Yan and Rongfei Fan and Han Hu and Hangguan Shan and Tony Q. S. Quek and Puning Zhao},
  doi          = {10.1109/TMC.2025.3571058},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10729-10742},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated learning resilient to byzantine attacks and data heterogeneity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satellite edge intelligence: DRL-based resource management for task inference in LEO-based satellite-ground collaborative networks. <em>TMC</em>, <em>24</em>(10), 10710-10728. (<a href='https://doi.org/10.1109/TMC.2025.3570799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguished from terrestrial edge intelligence, satellite edge intelligence has unique characteristics, including the rapid mobility of satellites, limitations in computing and energy resources, and differences in the artificial intelligence models deployed on user devices, satellites, and ground cloud servers. In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource management scheme for task inference in Low Earth Orbit (LEO)-based satellite-ground collaborative networks. In our approach, the task of a user can be inferred by the user device itself, the edge server of the current satellite via user-to-satellite transmission, the edge server of a neighboring satellite via satellite-to-satellite transmission, or a ground cloud server via satellite-to-cloud transmission. Our scheme jointly optimizes task offloading, computing resource allocation, and communication resource allocation to minimize the total system cost, which encompasses trade-offs among the task inference delays for all tasks, the energy consumption of system, and the task inference accuracies for all tasks, while ensuring that the transmit power budgets of all satellites and the satellite coverage time constraints for each user are met. A DRL-based algorithm combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm and two numerical methods is designed to solve the optimization problem efficiently. We prove the convergence of our algorithm and demonstrate the superiority of our scheme by performing extensive simulations in 4 scenarios with 4 reference schemes.},
  archive      = {J_TMC},
  author       = {Wenhao Fan and Qingcheng Meng and Guan Wang and Hengwei Bian and Yabin Liu and Yuan’an Liu},
  doi          = {10.1109/TMC.2025.3570799},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10710-10728},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Satellite edge intelligence: DRL-based resource management for task inference in LEO-based satellite-ground collaborative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age-energy analysis in multi-source systems with wake-up control and packet management. <em>TMC</em>, <em>24</em>(10), 10696-10709. (<a href='https://doi.org/10.1109/TMC.2025.3571419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an increasing focus on real-time mobile applications, such as news updates and weather forecast. In these applications, data freshness is of significant importance, which can be measured by age-of-synchronization (AoS). At the same time, the reduction of carbon emission is increasingly required by the communication operators. Thus, how to reduce energy consumption while keeping the data fresh becomes a matter of concern. In this paper, we study the age-energy trade-off in a multi-source single-server system, where the server can turn to sleep mode to save energy. We adopt the stochastic hybrid system (SHS) method to analyze the average AoS and power consumption with three wake-up policies including N-policy, single-sleep policy and multi-sleep policy, and three packet preemption strategies, including Last-Come-First-Serve with preemption-in-Service (LCFS-S), LCFS with preemption-only-in-Waiting (LCFS-W), and LCFS with preemption-and-Queueing (LCFS-Q). The trade-off performance is analyzed via both closed-form expressions and numerical simulations. It is found that N-policy attains the best trade-off performance among all three sleep policies. Among packet management strategies, LCFS-S is suitable for scenarios with high requirements on energy saving and small arrival rate difference between sources. LCFS-Q is suitable for scenarios with high requirements on information freshness and large arrival rate difference between sources.},
  archive      = {J_TMC},
  author       = {Jie Gong and Jiajie Huang},
  doi          = {10.1109/TMC.2025.3571419},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10696-10709},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age-energy analysis in multi-source systems with wake-up control and packet management},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient multi-server federated learning via over-the-air computation. <em>TMC</em>, <em>24</em>(10), 10683-10695. (<a href='https://doi.org/10.1109/TMC.2025.3573600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the Internet of Things (IoT), there has been explosive growth in edge devices, which generate a tremendous amount of data that holds invaluable potential. However, conventional data mining and machine learning (ML) paradigms require transmitting raw data to data centers for further use, which puts a heavy burden on communication networks and is exposed to high privacy risks. Federated learning allows for the training of ML models using distributed datasets, which can be applied to protect data privacy and alleviate transmission burdens. Meanwhile, the technique of over-the-air (OTA) computation can be utilized to exploit the superposition property of wireless communication channels. Motivated by this, in this paper, we propose a co-phase OTA approach for communication-efficient uploading in multi-server federated learning, which does not require expansion of the uplink channel bandwidth when the numbers of users and models increase. Besides, the digital OTA with randomized transmission is proposed to overcome the disadvantages of analog OTA, where the performance analyses of analog OTA and digital OTA are deduced, respectively. Simulation results show that a lower cost function can be obtained by digital OTA while requiring fewer iterations for convergence than that in analog OTA as more users can upload.},
  archive      = {J_TMC},
  author       = {Rui Han and Jiahao Ma and Lin Bai and Jinho Choi and Wei Zhang},
  doi          = {10.1109/TMC.2025.3573600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10683-10695},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Communication-efficient multi-server federated learning via over-the-air computation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDLoRa: Scaling downlink concurrent transmissions with full-duplex LoRa gateways. <em>TMC</em>, <em>24</em>(10), 10668-10682. (<a href='https://doi.org/10.1109/TMC.2025.3572130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional data collection applications which primarily rely on uplink transmissions, emerging applications (e.g., device actuation, firmware update, packet reception acknowledgment) increasingly demand robust downlink transmission capabilities. Current LoRaWAN systems struggle to support these applications due to the inherent asymmetry between downlink and uplink capabilities. While uplink transmissions can handle multiple packets simultaneously, downlink transmissions are restricted to a single logical channel at a time, significantly limiting the deployment of applications that require substantial downlink capacity. To address this challenge, FDLoRa introduces an innovative in-band full-duplex LoRa gateway design, featuring novel solutions to mitigate self-interference (i.e., the strong downlink interference to ultra-weak uplink reception). This approach enables full-spectrum in-band downlink transmissions without compromising the reception of weak uplink packets. Building on the capabilities of full-duplex gateways, FDLoRa presents a new downlink framework that supports concurrent downlink transmissions across multiple logical channels of available gateways. Evaluation results show that FDLoRa enhances downlink capacity by 5.7× compared to LoRaWAN in a three-gateway testbed and achieves 2.58× higher downlink concurrency per gateway than the current leading solutions.},
  archive      = {J_TMC},
  author       = {Shiming Yu and Xianjin Xia and Ziyue Zhang and Ningning Hou and Yuanqing Zheng},
  doi          = {10.1109/TMC.2025.3572130},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10668-10682},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FDLoRa: Scaling downlink concurrent transmissions with full-duplex LoRa gateways},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative perception against data fabrication attacks in vehicular networks. <em>TMC</em>, <em>24</em>(10), 10654-10667. (<a href='https://doi.org/10.1109/TMC.2025.3571013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative perception in vehicular networks enables the connected autonomous vehicle (CAV) to gather sensing data, such as feature maps of light detection and ranging (LiDAR) point clouds, from neighboring CAVs to achieve higher perception accuracy, which has performance degradation against data fabrication attacks that share falsified sensing data with random probability. In this paper, we exploit the spatial consistency check to detect the potentially manipulated regions in LiDAR point clouds and measure the inconsistency degree of the received sensing data based on the number of conflict regions, which is the basis for determining the falsified sensing data if the inconsistency degree exceeds the threshold of the hypothesis test. The reinforcement learning (RL)-based collaborative vehicular perception scheme against data fabrication attacks is further proposed to choose CAVs based on the inconsistency degrees, the data quality measured by the confidence scores, the channel gains and the CAV reputations, which enhances the utility as the weighted sum of perception accuracy, speed and minimum latency requirement for data transmission. In addition, the multi-layer perceptron-based neural networks extract the perception features of sensing data from historical experiences, such as the data quality of received feature maps, as well as compress the RL state that linearly increases with the network scales and the spatial granularity of LiDAR point clouds for faster learning. Experimental results based on 10 CAVs equipped with LiDAR sensors and NVIDIA computational units to detect 20 vehicles against data fabrication attacks show that our proposed scheme outperforms the benchmarks in terms of perception accuracy and speed.},
  archive      = {J_TMC},
  author       = {Zhiping Lin and Liang Xiao and Hongyi Chen and Zefang Lv},
  doi          = {10.1109/TMC.2025.3571013},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10654-10667},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative perception against data fabrication attacks in vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving and autonomous-path access delegation for mobile cloud healthcare systems. <em>TMC</em>, <em>24</em>(10), 10640-10653. (<a href='https://doi.org/10.1109/TMC.2025.3570769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cloud healthcare systems are gaining popularity due to their remote data collection through mobile devices and flexible data access through cloud services. The collected electronic health records (EHRs) are generally encrypted on mobile devices before being uploaded to the cloud, and accessed only by specific users. This incurs inconvenience when re-sharing EHRs with new receivers, especially in heterogeneous scenarios. Although cross-domain proxy re-encryption (CD-PRE) schemes have been studied to transform ciphertexts between different cryptosystems, they can neither support EHRs’ controlled multi-hop delegation between trusted delegatees chosen by the delegator nor prevent EHRs’ privacy inference through delegatees’ information. To this end, we present CAP-PRE for mobile cloud healthcare systems, which is a multi-hop CD-PRE scheme that supports lightweight encryption and re-encryption key generation on mobile devices, as well as privacy-preserving and autonomous-path access delegation. In CAP-PRE, the delegator creates a delegation path that includes preferred delegatees, and generates corresponding re-encryption keys which enables the cloud server to convert the collected ciphertext to an inner product ciphertext for privacy-preserving EHR re-sharing and pass the access rights along the delegation path for controlled multi-hop delegation. We finally prove the security of CAP-PRE and show the better performance of CAP-PRE with extensive experiments.},
  archive      = {J_TMC},
  author       = {Genghui Chi and Qinlong Huang and Caiqun Shi},
  doi          = {10.1109/TMC.2025.3570769},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10640-10653},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving and autonomous-path access delegation for mobile cloud healthcare systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiRescue: Optimal dispatching of rescue teams under flooding disasters. <em>TMC</em>, <em>24</em>(10), 10622-10639. (<a href='https://doi.org/10.1109/TMC.2025.3569757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective dispatching of rescue teams under flooding disasters is crucial. However, previous methods are either incapable of handling flooding disaster situations, or cannot accurately estimate the distribution of rescue requests and accordingly adjust the search of the rescue teams. We propose MobiRescue, a human Mobility based Rescue team dispatching system, which aims to maximize the total number of rescued people, minimize the rescue delay and the number of serving rescue teams. We studied a city-scale human mobility dataset collected under the Hurricane Florence, and observed that several natural and demographic factors are closely related to impact severity, and road segment passability must be considered. Accordingly, we first propose a Support Vector Machine based method to predict the distribution of rescue requests considering the disaster-related factors. Then, we design an Euler path based method to determine the search paths for rescue team dispatching. Subsequently, we develop a Reinforcement Learning based method to guide the search of the rescue teams. Finally, we design a multi-objective optimization problem based method to adapt to the changed road segment passability. Our experiments demonstrate that compared with the other methods, MobiRescue increases the total number of timely served rescue requests by 43.4% in average.},
  archive      = {J_TMC},
  author       = {Li Yan and Bin Yang and Haiying Shen and Shohaib Mahmud and Natasha Zhang Foutz and Joshua Anton},
  doi          = {10.1109/TMC.2025.3569757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10622-10639},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiRescue: Optimal dispatching of rescue teams under flooding disasters},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression meets security: Low-complexity linear collaborative federated learning with enhanced accuracy. <em>TMC</em>, <em>24</em>(10), 10606-10621. (<a href='https://doi.org/10.1109/TMC.2025.3569669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been regarded as a promising paradigm for enabling distributed model training over resource-limited edge devices. Although FL maintains data locality and enhances model generalization, it faces challenges such as model leakage and pressure from frequent model updates. Some existing schemes, such as differential privacy and model encryption, can partially alleviate these issues while sacrificing the accuracy of the modeling training or increasing the computational overheads in training. To address this issue, we design a low-complexity linear collaborative FL (LCFL) framework to enhance the privacy and accuracy of FL. Specifically, we propose the collaborative secrecy transmission (CST) algorithm by integrating a variant of Shamir’s secret-sharing with the model segmentation, which can compresses and encrypts the local models for FL. The decoding complexity of the CST algorithm is only $O(N^{3})$ under the compression ratio of $N$, which reduces the communication overhead and computational complexity. We conduct a quantitative analysis of the model error induced by the CST algorithm and derive its closed-form upper bound. Within LCFL, we formulate an optimization problem to maximize the global model accuracy in wireless FL by optimizing compression ratios, bandwidth allocation, and transmit-powers. Subsequently, we propose a low-complexity algorithm to solve this problem effectively. Numerical simulations demonstrate the efficacy of LCFL in improving FL’s accuracy and security, and the results validate the efficiency of the proposed optimization scheme for wireless FL.},
  archive      = {J_TMC},
  author       = {Tianshun Wang and Peichun Li and Panpan Feng and Xin Wei and Liping Qian and Yuan Wu},
  doi          = {10.1109/TMC.2025.3569669},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10606-10621},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compression meets security: Low-complexity linear collaborative federated learning with enhanced accuracy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACIFISTA: Conflict evaluation and management in open RAN. <em>TMC</em>, <em>24</em>(10), 10590-10605. (<a href='https://doi.org/10.1109/TMC.2025.3570632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms making autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts generated by O-RAN applications that control RAN parameters. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA’s ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems. We use PACIFISTA to demonstrate that users can experience a 16% throughput loss even in the case of xApps with similar goals, and that applications with conflicting goals might cause severe instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify conflicting applications and maintain performance degradation below a tolerable threshold.},
  archive      = {J_TMC},
  author       = {Pietro Brach del Prever and Salvatore D’Oro and Leonardo Bonati and Michele Polese and Maria Tsampazi and Heiko Lehmann and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3570632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10590-10605},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PACIFISTA: Conflict evaluation and management in open RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaShift: Anti-collapse and real-time deep model evolution for mobile vision applications. <em>TMC</em>, <em>24</em>(10), 10573-10589. (<a href='https://doi.org/10.1109/TMC.2025.3572215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computational hardware advance, integrating deep learning (DL) models into mobile devices has become ubiquitous for visual tasks. However, “data distribution shift” in live sensory data can lead to a degradation in the accuracy of mobile DL models. Conventional domain adaptation methods, constrained by their dependence on pre-compiled static datasets for offline adaptation, exhibit fundamental limitations in real-time practicality. While modern online adaptation methodologies enable incremental model evolution, they remain plagued by two critical shortcomings: computational latency from excessive resource demands on mobile devices that compromise temporal responsiveness, and accuracy collapse stemming from error accumulation through unreliable pseudo-labeling processes. To address these challenges, we introduce AdaShift, an innovative cloud-assisted framework enabling real-time online model adaptation for vision-based mobile systems operating under non-stationary data distributions. Specifically, to ensure real-time performance, the adaptation trigger and plug-and-play adaptation mechanisms are proposed to minimize redundant adaptation requests and reduce per-request costs. To prevent accuracy collapse, AdaShift introduces a novel anti-collapse parameter restoration mechanism that explicitly recovers knowledge, ensuring stable accuracy improvements during model evolution. Through extensive experiments across various vision tasks and model architectures, AdaShift demonstrates superior accuracy and 100ms-level adaptation latency, achieving an optimal balance between accuracy and real-time performance compared to baselines.},
  archive      = {J_TMC},
  author       = {Ke Ma and Bin Guo and Sicong Liu and Cheng Fang and Siqi Luo and Zimu Zheng and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3572215},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10573-10589},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaShift: Anti-collapse and real-time deep model evolution for mobile vision applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-variate time series prediction of traffic and users for dynamic RRH-BBU mapping in C-RAN. <em>TMC</em>, <em>24</em>(10), 10557-10572. (<a href='https://doi.org/10.1109/TMC.2025.3570851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular operators face significant challenges in cutting operating expenses while maintaining the quality of service (QoS) for users due to growing network traffic and dynamic user connections. These challenges are addressed by the cloud radio access network (C-RAN) architecture, which includes a centralized pool of baseband units (BBUs) and distributes them from remote radio heads (RRHs). The key to improving C-RAN performance is to dynamically allocate large-scale RRHs to different BBUs in real time. In this paper, we propose a user behavior-aware RRH-BBU mapping framework to improve the performance of large-scale C-RANs by predicting RRH traffic and users in advance. First, we propose a Multivariate RRH time series Prediction Model (MRPM) that captures the spatio-temporal patterns in the data to predict the traffic volume and the number of users of RRHs, which represent key indicators of RRH connection states. Second, we formulate the RRH-BBU mapping as a Markov decision process problem to optimize cost and QoS by considering BBU utilization, BBU energy consumption, RRH migration frequency, and BBU load balancing. Third, we propose a prediction-based RRH-BBU mapping scheme (PB-RBM) to find the optimal RRH-BBU mapping strategy by leveraging the prediction information of MRPM. In the PB-RBM algorithm, we employ an A3C algorithm to learn the mapping policy and group the RRHs based on a defined popularity metric to reduce the state and action space of the reinforcement learning algorithm. Finally, extensive experiments are conducted on a real-world dataset, and our algorithm is compared with several matching algorithms, such as ACKTR, heuristic, etc., to demonstrate its superiority, especially reducing 17.5% in RMSE compared to the best-performing baseline.},
  archive      = {J_TMC},
  author       = {Fan Wu and Shanshan Wang and Jieyu Zhou and Haoye Pan and Conghao Zhou and Wang Yang and Feng Lyu and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3570851},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10557-10572},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-variate time series prediction of traffic and users for dynamic RRH-BBU mapping in C-RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mimir: Data-free federated unlearning through client-specific prompt generation for personalized models. <em>TMC</em>, <em>24</em>(10), 10537-10556. (<a href='https://doi.org/10.1109/TMC.2025.3570018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated unlearning (FU) has become an important area of research due to an increasing need for federated learning (FL) applications to comply with emerging data privacy regulations such as GDPR. It facilitates the removal of certain clients’ data from an already trained FL model while preserving the performance on the remaining client without the need to retrain from scratch. Existing FU methods typically require clients to have access to their training data or historical model updates, which may be impractical in real-world scenarios due to privacy constraints and changes in data availability. Moreover, FU methods may cause catastrophic unlearning, where removing a client’s data from heterogeneous, non-IID settings can negatively impact the model’s performance on data from retained clients. To address the aforementioned issues and leverage the capabilities of personalized federated learning (pFL) in handling non-IID data distributions, this paper introduce Mimir, a novel data-free federated unlearning framework designed for pFL settings. Mimir integrates both learning and unlearning phases by utilizing personalized prompts for each client. We design a distillation structure based on Generative Adversarial Networks (GANs) for client-level unlearning that does not require access to original data or historical updates. By leveraging client-specific prompts generated during the pFL phase, Mimir adapts to heterogeneous data distributions and mitigates catastrophic unlearning on the retained data. We demonstrate the effectiveness of Mimir through extensive experiments on benchmark datasets, showing its ability to forget target client data while preserving model accuracy on the remaining clients.},
  archive      = {J_TMC},
  author       = {Wenhan Wu and Huanghuang Liang and Tianyu Tu and Jiawei Jiang and Chuang Hu and Dazhao Cheng},
  doi          = {10.1109/TMC.2025.3570018},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10537-10556},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mimir: Data-free federated unlearning through client-specific prompt generation for personalized models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedEx: Expediting federated learning over heterogeneous mobile devices by overlapping and participant selection. <em>TMC</em>, <em>24</em>(10), 10523-10536. (<a href='https://doi.org/10.1109/TMC.2025.3572516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training latency is critical for the success of numerous intrigued applications ignited by federated learning (FL) over heterogeneous mobile devices. By revolutionarily overlapping local gradient transmission with continuous local computing, FL can remarkably reduce its training latency over homogeneous clients, yet encounter severe model staleness, model drifts, memory cost and straggler issues in heterogeneous environments. To unleash the full potential of overlapping, we propose, FedEx, a novel federated learning approach to expedite FL training over mobile devices under data, computing and wireless heterogeneity. FedEx redefines the overlapping procedure with staleness ceilings to constrain memory consumption and make overlapping compatible with participation selection (PS) designs. Then, FedEx characterizes the PS utility function by considering the latency reduced by overlapping, and provides a holistic PS solution to address the straggler issue. FedEx also introduces a simple but effective metric to trigger overlapping, in order to avoid model drifts. Experimental results show that compared with its peer designs, FedEx demonstrates substantial reductions in FL training latency over heterogeneous mobile devices with limited memory cost.},
  archive      = {J_TMC},
  author       = {Jiaxiang Geng and Boyu Li and Xiaoqi Qin and Yixuan Li and Liang Li and Yanzhao Hou and Miao Pan},
  doi          = {10.1109/TMC.2025.3572516},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10523-10536},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedEx: Expediting federated learning over heterogeneous mobile devices by overlapping and participant selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated communication and computation resource allocation for the compressive sensing based image transmission. <em>TMC</em>, <em>24</em>(10), 10510-10522. (<a href='https://doi.org/10.1109/TMC.2025.3570447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data compression based transmission has been envisioned as a promising solution to improve the data transmission efficiency with the limited radio resources in the future sixth-generation (6G) wireless networks. In this paper, we propose an integrated communication and computation resource allocation system for image transmission based on compressive sensing (CS), which consists of several camera devices and a base station (BS). The device side first compresses the images, after which the compressed images are transmitted using non-orthogonal multiple access (NOMA) transmission, and finally the BS restores the received compressed images. Due to the limited energy supply, the total system energy consumption is minimized by jointly optimizing the image sampling rate, the image data transmission power, the number of floating point operations per second (FLOPS), the time of image compression and the time of data transmission under the constraints of latency and the peak signal-to-noise ratio (PSNR). Due to the non-convexity of the proposed problem, after a series of equal substitutions we convexify the problem. Then, the Karush–Kuhn–Tucker (KKT) condition and the gradient descent method are used to obtain the optimal solution of the target problem. After simulation experiments, it is concluded that the proposed CS-based image transmission scheme effectively reduces the total energy consumption by a factor of 2.7 compared with frequency division multiple access (FDMA), and the total latency by 180% compared with the original image transmission.},
  archive      = {J_TMC},
  author       = {Qianru Wang and Li Ping Qian and Wei Jiang and Yuan Wu and Xiaoniu Yang},
  doi          = {10.1109/TMC.2025.3570447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10510-10522},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Integrated communication and computation resource allocation for the compressive sensing based image transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A practical DoS attack on commercial UWB ranging systems. <em>TMC</em>, <em>24</em>(10), 10492-10509. (<a href='https://doi.org/10.1109/TMC.2025.3569972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) ranging systems are increasingly deployed in critical, security-sensitive applications due to their precise positioning and secure ranging capabilities. In this work, we introduce a practical DoS attack via reactive jamming, referred to as UWBAD+, which targets commercial UWB ranging systems by exploiting the vulnerabilities of the normalized cross-correlation process. This allows UWBAD+ to selectively and effectively disrupt ranging sessions without requiring prior knowledge of the victim devices’ configurations, leading to potentially severe consequences, such as property loss, unauthorized access, or vehicle theft. The enhanced effectiveness and low detectability of UWBAD+ stem from the following: (i) it can rapidly sniff the physical layer structures of unknown UWB systems, even in the presence of multiple UWB devices operating simultaneously; (ii) it blocks each ranging session efficiently by employing field-level jamming, thus exerting a significant impact on commercial UWB ranging systems; and (iii) its compact, reactive, and selective design based on COTS UWB chips, which makes it both affordable and less noticeable. We successfully executed real-world attacks on commercial UWB ranging systems produced by the three largest UWB chip vendors in the market, including Apple, NXP, and Qorvo. We disclosed our findings to Apple, relevant Original Equipment Manufacturers (OEMs), and the Automotive Security Research Group. As of the time of writing, the involved OEM has acknowledged this vulnerability in their automotive systems and has issued a ${\$} 5,000$ bounty as a reward.},
  archive      = {J_TMC},
  author       = {Yongzhao Zhang and Yuqiao Yang and Zhiwei Chen and Zhongjie Wu and Ting Chen and Jun Li and Jie Yang and Guowen Xu and Wenhao Liu and Xiaosong Zhang and Jingwei Li and Yu Jiang and Zhuo Su},
  doi          = {10.1109/TMC.2025.3569972},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10492-10509},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A practical DoS attack on commercial UWB ranging systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based edge computing service with dynamic entry and exit mechanism. <em>TMC</em>, <em>24</em>(10), 10474-10491. (<a href='https://doi.org/10.1109/TMC.2025.3570646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of 5G and artificial intelligence (AI) technology, the Internet of Things (IoT) has been expanding and integrated into various aspects of our daily lives. However, this also poses challenges such as the ubiquitous demand for communication and computing resources, and data privacy issues. Considering its flexible deployment, high security, and ease of scalability, blockchain-enabled edge computing IoT network (BECIN) has become a promising solution to provide secure and fast communication and computing services. However, existing research on computation offloading in edge computing largely overlooks the stochastic arrival of computational tasks and the potential variability in the number, locations, and resource provisions of edge computing service providers. Therefore, we propose a dynamic, self-adjusting BECIN framework aimed at providing long-term stable, efficient, and secure edge computing data offloading services for ground users in a specific region. This framework supports the dynamic entry and exit of edge computing service providers. Additionally, we introduce a novel dynamic Dueling DDQN approach to update the offloading and resource management policies based on changes in resource provisioning. Experimental results demonstrate the feasibility and superior performance of our framework on system cost and system latency.},
  archive      = {J_TMC},
  author       = {Qiang He and Zheng Feng and Hui Fang and Xingwei Wang and Liang Zhao and Keping Yu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TMC.2025.3570646},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10474-10491},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-based edge computing service with dynamic entry and exit mechanism},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and optimization of heterogeneous coded distributed computing with nonuniform file popularity. <em>TMC</em>, <em>24</em>(10), 10456-10473. (<a href='https://doi.org/10.1109/TMC.2025.3570907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies MapReduce-based heterogeneous coded distributed computing (CDC) where, besides different computing capabilities at workers, input files to be accessed by computing jobs have nonuniform popularity. We propose a file placement strategy that can handle an arbitrary number of input files. Furthermore, we design a nested coded shuffling strategy that can efficiently manage the nonuniformity of file popularity to maximize the coded multicasting opportunity. We then formulate the joint optimization of the proposed file placement and nested shuffling design variables to optimize the proposed CDC scheme. To reduce the high computational complexity in solving the resulting mixed-integer linear programming (MILP) problem, we propose a simple two-file-group-based file placement approach to obtain an approximate solution. Both numerical studies and experimental tests show that the optimized CDC scheme outperforms other alternatives. Also, the proposed two-file-group-based approach achieves nearly the same performance as the conventional branch-and-cut method in solving the MILP problem but with substantially lower computational complexity that is scalable over the number of files and workers. For computing jobs with aggregate target functions that commonly appear in machine learning applications, we propose a heterogeneous compressed CDC (C-CDC) scheme to further improve the shuffling efficiency. The C-CDC scheme uses a local data aggregation technique to compress the data to be shuffled for the shuffling load reduction. We again optimize the proposed C-CDC scheme and explore the two-file-group-based low-complexity approach to find an approximate solution. Numerical studies show that the proposed C-CDC scheme provides a considerable shuffling load reduction over the CDC scheme, and the two-file-group-based file placement approach maintains good performance.},
  archive      = {J_TMC},
  author       = {Yong Deng and Min Dong},
  doi          = {10.1109/TMC.2025.3570907},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10456-10473},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and optimization of heterogeneous coded distributed computing with nonuniform file popularity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing mobile-friendly viewport prediction for live 360-degree video streaming. <em>TMC</em>, <em>24</em>(10), 10441-10455. (<a href='https://doi.org/10.1109/TMC.2025.3571186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to minimize transmission and computation overhead for mobile terminals. To improve viewport prediction accuracy, we utilize both spatial information through a saliency prediction model and temporal information through a modified LSTM model. Different computations introduced by the neural network models are distributed across the network to keep the computation light on mobile devices. To better adapt to the content dynamics in live streaming, we employ the model-agnostic meta-learning (MAML) method for video saliency prediction. The learned saliency prediction model with optimized initialization via offline meta-training can be fast fine-tuned online using a few samples. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results demonstrate that our approach achieves real-time prediction for live video streaming and surpasses existing methods in prediction accuracy on mobile terminals, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. Compared to baseline methods, MFVP achieves a 4.7–28.7% improvement in accuracy and demonstrates faster adaptability to dynamic content changes, enabling rapid fine-tuning and adjustment. When integrated into a streaming system and paired with our adaptive bitrate allocation algorithm, MFVP enhances overall video quality by 5.6–12.9% and reduces quality fluctuations by 33.3–50.9%.},
  archive      = {J_TMC},
  author       = {Lei Zhang and Peng Chen and Cong Zhang and Cheng Pan and Tao Long and Weizhen Xu and Laizhong Cui and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3571186},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10441-10455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing mobile-friendly viewport prediction for live 360-degree video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical learning and computing over space-ground integrated networks. <em>TMC</em>, <em>24</em>(10), 10423-10440. (<a href='https://doi.org/10.1109/TMC.2025.3569887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-ground integrated networks hold great promise for providing global connectivity, particularly in remote areas where large amounts of valuable data are generated by Internet of Things (IoT) devices, but lacking terrestrial communication infrastructure. The massive data is conventionally transferred to the cloud server for centralized artificial intelligence (AI) models training, raising huge communication overhead and privacy concerns. To address this, we propose a hierarchical learning and computing framework, which leverages the low-latency characteristic of low-earth-orbit (LEO) satellites and the global coverage of geostationary-earth-orbit (GEO) satellites, to provide global aggregation services for locally trained models on ground IoT devices. Due to the time-varying nature of satellite network topology and the energy constraints of LEO satellites, efficiently aggregating the received local models from ground devices on LEO satellites is highly challenging. By leveraging the predictability of inter-satellite connectivity, modeling the space network as a directed graph, we formulate a network energy minimization problem for model aggregation, which turns out to be a Directed Steiner Tree (DST) problem. We propose a topology-aware energy-efficient routing (TAEER) algorithm to solve the DST problem by finding a minimum spanning arborescence on a substitute directed graph. Extensive simulations under real-world space-ground integrated network settings demonstrate that the proposed TAEER algorithm significantly reduces energy consumption and outperforms benchmarks.},
  archive      = {J_TMC},
  author       = {Jingyang Zhu and Yuanming Shi and Yong Zhou and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3569887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10423-10440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical learning and computing over space-ground integrated networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XHGA: Expanding the capabilities of cross-modal wrist-worn devices for multi-task hand gesture applications. <em>TMC</em>, <em>24</em>(10), 10405-10422. (<a href='https://doi.org/10.1109/TMC.2025.3569841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture applications (HGA) are essential for human-machine interaction. Although the existing solutions achieve good performance in specific tasks, they still face challenges when users navigate through different application contexts, i.e., requiring multi-task ability to support newly arrived HGA tasks. In this paper, we propose a novel wrist-worn multi-task HGA system named XHGA, which can implement modal-domain combination, data-domain adaptation and label-domain extension to ensure the performance in multi-task scenarios. The system introduces a novel two-stage training strategy, i.e., task-agnostic stage to align cross-modal features from unlabeled arbitrary gestures through contrastive learning, and task-related stage to learn modality contributions with limited labeled data in specific tasks through self-attention mechanism, while achieves multi-objective recognition simultaneously by employing an adaptive loss function weighting method. Extensive experiments demonstrate that XHGA can achieve an average accuracy of 92.7% with only using 15 labeled data per gesture under three HGA tasks. Compared with the state-of-the-art multi-modal approach, XHGA reduces 82.7% training time, and 47.7% storage, with about 5% improvements in accuracy.},
  archive      = {J_TMC},
  author       = {Kaiwen Guo and Hui Tang and Tianyi Xu and Hao Zhou and Mengxia Lyu and Zhi Liu and Xiaoyan Wang and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3569841},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10405-10422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {XHGA: Expanding the capabilities of cross-modal wrist-worn devices for multi-task hand gesture applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRL-based pricing-driven for task offloading and dynamic resource in vehicle edge computing. <em>TMC</em>, <em>24</em>(10), 10389-10404. (<a href='https://doi.org/10.1109/TMC.2025.3569817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Edge Computing (VEC) assists vehicles in performing latency-sensitive tasks by deploying resources near the vehicle. Designing an incentive mechanism for vehicles and VEC is crucial for realizing an intelligent transmission system. Considering the rationality of resource allocation, we model the utility functions of the VEC and the vehicle, which are used as optimization objectives. Specifically, the VEC allocates resources through pricing to maximize revenue under resource-constrained conditions, and the vehicle weighs payments against energy consumption to determine offloading and resource allocation. Given the vehicle movement and the variable channel state, we use the Deep Reinforcement Learning (DRL) algorithm to solve these optimization problems. To reduce the learning difficulty of the DRL algorithm in complex VEC scenarios with multiple optimization variables, we propose a Pricing-Driven Resource Allocation (PDRA) algorithm that performs mobility-aware task offloading and calculates the optimal values of the optimization variables in the utility function of the vehicle to reduce the decision dimension. Furthermore, we also propose a DRL-based Pricing-Driven Dynamic Resource Allocation (DPDDRA) algorithm to achieve efficient resource allocation. Extensive experimental results show that the proposed algorithms can reduce the learning difficulty while maximizing VEC and vehicle revenue in complex VEC scenarios.},
  archive      = {J_TMC},
  author       = {Sijun Wu and Liang Yang and Junjie Li and Hongzhi Guo and Ishtiaq Ahmad and Daniel Benevides Da Costa and Hongbo Jiang and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3569817},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10389-10404},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRL-based pricing-driven for task offloading and dynamic resource in vehicle edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing service provider’s profit in multi-UAV 5G network via deep reinforcement learning and graph coloring. <em>TMC</em>, <em>24</em>(10), 10377-10388. (<a href='https://doi.org/10.1109/TMC.2025.3571804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current 5G network is expected to have a densely populated architecture comprising radio-enabled Service Provider (SP) and heterogeneous User Equipment (UE). Addressing the real-time service demands of UEs with strict deadlines is a critical challenge. Uncrewed Aerial Vehicle (UAV) assisted service provisioning is emerging as an efficient solution for timely service transfers. Therefore, SPs are interested in offering UAV-assisted service transmission to get profited by deploying UAVs. However, this introduces challenges like optimizing the locations of UAVs and Power Level (PL) along with interference management within limited available radio resources. Hence, we proposed a novel framework for multi-UAV-assisted service provisioning, consisting of Base Station (BS), UAVs, and heterogeneous UEs in 5G network. We formulate the SP’s profit maximization problem, optimizing UAVs’ location, PL, and resource allocation while considering service latency, interference management, and UAVs’ energy constraints collectively as an optimization problem. Furthermore, we propose a semi-centralized sub-optimal solution utilizing Multi-agent Deep Reinforcement Learning (MaDRL) and a Graph Coloring-based approach. Extensive simulation analysis demonstrates the proposed algorithm’s effectiveness, achieving an average of 99.05% profit compared to the optimal value.},
  archive      = {J_TMC},
  author       = {Shilpi Kumari and Ajay Pratap},
  doi          = {10.1109/TMC.2025.3571804},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10377-10388},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Maximizing service provider’s profit in multi-UAV 5G network via deep reinforcement learning and graph coloring},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Game-theoretic incentive mechanism for blockchain-based federated learning. <em>TMC</em>, <em>24</em>(10), 10363-10376. (<a href='https://doi.org/10.1109/TMC.2025.3567355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based federated learning (BFL) has gained attention for its potential to establish decentralized trust. While existing research primarily focuses on personalized frameworks for various applications, essential aspects including incentive mechanisms—critical for ensuring stable system operation—remain under-explored. To bridge this gap, we propose a game-theoretic incentive mechanism designed to foster active participation in BFL tasks. Specifically, we model a BFL system comprising a model owner (MO), i.e., task publisher, multiple miners, and training terminals, framing their interactions through two-tier Stackelberg games. In the first-tier game, the MO designs reward strategies to incentivize training terminals to contribute more data, enhancing model accuracy. The second-tier game introduces a multi-leader multi-follower Stackelberg game, enabling miners to set model packaging prices based on competitors’ strategies and anticipated user behavior. By deriving the Stackelberg equilibrium, we identify optimal strategies for all participants, leading to an incentive mechanism balancing individual interests with overall performance. Compared to its benchmarks, our incentive mechanism offers 5.8% and 53.4% higher utilities in the two games compared to its alternatives, accelerating convergence and improving accuracy.},
  archive      = {J_TMC},
  author       = {Wenzheng Tang and Erwu Liu and Wei Ni and Xinyu Qu and Butian Huang and Kezhi Li and Dusit Niyato and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3567355},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10363-10376},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Game-theoretic incentive mechanism for blockchain-based federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint partitioning, allocation, and transmission optimization for federated learning in satellite constellations via multi-task MARL. <em>TMC</em>, <em>24</em>(10), 10345-10362. (<a href='https://doi.org/10.1109/TMC.2025.3568470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orbital edge computing (OEC) is crucial for supporting space intelligence applications within satellite networks. However, individual satellites face resource constraints, and implementing distributed processing techniques, such as federated learning (FL), across multiple satellites introduces significant scheduling complexity. To address these challenges, we first model the key factors influencing complex satellite networks, including satellite constellations, regional resource demands, inter-satellite communication and routing, energy consumption, and battery aging—a novel aspect invoked by OEC operations. We propose an adaptive aggregation method to fundamentally improve communication efficiency in OEC-based FL. To enhance scheduling performance, we formulate a unified optimization problem that jointly considers data partitioning, resource allocation, and aggregation transmission tasks within a decentralized partially observable Markov decision process (Dec-POMDP) framework. Furthermore, we introduce an episodic-phase-recalling reward shaping (EPRS) method to correlate the influences across these phases. Inspired by multi-task learning, we propose an efficient multi-agent reinforcement learning (MARL) algorithm featuring a multi-head actor-critic (MH-AC) network structure and task-equalized adaptation (TEA) technology, designed to optimize latency, energy consumption, network traffic, and battery aging. Extensive experiments validate the effectiveness of the proposed method, showing a 29.9% reduction in total training time, an 11.5% reduction in network traffic, and superior overall performance compared to rule-based methods.},
  archive      = {J_TMC},
  author       = {Chengjia Lei and Shaohua Wu and Yi Yang and Jiayin Xue and Dawei Chen and Pengfei Duan and Qinyu Zhang},
  doi          = {10.1109/TMC.2025.3568470},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10345-10362},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint partitioning, allocation, and transmission optimization for federated learning in satellite constellations via multi-task MARL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized local differential privacy for multi-dimensional range queries over mobile user data. <em>TMC</em>, <em>24</em>(10), 10330-10344. (<a href='https://doi.org/10.1109/TMC.2025.3568511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional range queries performed on the mobile user data records become increasingly important and popular in the fields of e-commerce, social media, transportation logistics, etc. Meanwhile, mobile users usually have different privacy requirements for different attributes of the records. A straightforward and effective approach is to first get low-dimensional range query outcomes by using existing LDP mechanisms at different privacy levels, and then derive high-dimensional range query results at each level, and finally aggregate the results from all levels. However, it incurs low utility of the query results, since the non-fixed privacy budgets and the correlation between dimensions (attributes) detrimentally impact the utility of LDP methods, ultimately rendering them ineffective in practice. In this paper, we propose a new Personalized LDP approach for Multi-dimensional Range queries (PLDP-MR) over mobile user data, consisting of the user grouping, data perturbing, data re-perturbing, and range query results aggregating steps. First, PLDP-MR offers flexible dual grouping based on user-selected privacy levels and relevant attributes to obtain the corresponding one-dimensional and two-dimensional grids. PLDP-MR optimizes the grid granularity to minimize errors from perturbing users’ attribute data with different LDP noises at non-fixed privacy levels. Furthermore, PLDP-MR carefully re-perturbs the LDP-noisy data from mobile users at lower privacy levels (i.e., having the higher utility) to achieve LDP with higher privacy levels and supplement the data volume of the corresponding groups. Thus, the data utility is effectively improved without additional privacy losses. Finally, PLDP-MR aggregates the frequencies in all the one-dimensional and two-dimensional grids related to the multi-dimensional range query at all query intervals and all privacy levels to derive the final query result with considering the correlation between attributes. The aggregations use maximum entropy optimization and maximum likelihood methods to further enhance its utility. The privacy and utility of PLDP-MR are analyzed, and extensive experiments demonstrate its effectiveness.},
  archive      = {J_TMC},
  author       = {Yuanyuan He and Meiqi Wang and Xianjun Deng and Peng Yang and Qiao Xue and Laurence T. Yang},
  doi          = {10.1109/TMC.2025.3568511},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10330-10344},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized local differential privacy for multi-dimensional range queries over mobile user data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly optimizing the energy and time for multi-UAV 3-D coverage of terrestrial regions. <em>TMC</em>, <em>24</em>(10), 10312-10329. (<a href='https://doi.org/10.1109/TMC.2025.3568788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-rotor uncrewed aerial vehicles(UAVs) have been widely employed in various sensing tasks, e.g., environmental monitoring and disaster rescuing, many of which often require full coverage of terrestrial regions by UAVs. Efforts have been devoted to minimizing one of two objectives, i.e., energy consumptions and time costs of UAVs fulfilling such tasks, whereas it is still challenging to jointly optimize both objectives due to their complicated interdependent relationship. Therefore, this paper deals with the tasks of sensing terrestrial regions with multiple UAVs, and focuses on the three-dimensional (3-D) coverage problem by formulating a multi-objective optimization problem of jointly minimizing both objectives. Specifically, in order to optimize energy consumption effectively, an advanced closed-form energy consumption model for multi-rotor UAVs is developed based on a rigorous theoretical analysis by introducing the influences of torque and acceleration, which are often ignored by existing heuristic models. Moreover, considering the NP-hardness of the problem, an innovative swarm intelligence optimization framework is established by leveraging a multitasking learning pattern to exploit cross-task knowledge transfer and adopting an improved multi-objective salp swarm algorithm. Therein, two novel operators, i.e., a variable characteristic-guided hybrid solution initialization operator and a large-scale search-space-oriented multi-mechanism solution update operator, are designed to handle continuous, discrete and even high-dimensional variables involved. Real-world experiments validate the proposed energy model due to the reduction of power consumption estimation error by up to 59% compared to baselines, and besides, extensive simulations demonstrate that the proposed algorithm significantly outperforms the benchmarks in terms of both energy consumptions and time costs.},
  archive      = {J_TMC},
  author       = {Hao Gong and Baoqi Huang and Bing Jia and Lifei Hao and Zhenwei Shi},
  doi          = {10.1109/TMC.2025.3568788},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10312-10329},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Jointly optimizing the energy and time for multi-UAV 3-D coverage of terrestrial regions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized federated averaging via random walk. <em>TMC</em>, <em>24</em>(10), 10295-10311. (<a href='https://doi.org/10.1109/TMC.2025.3569423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a communication-efficient distributed machine learning method that allows multiple devices to collaboratively train models without sharing raw data. FL can be categorized into centralized and decentralized paradigms. The centralized paradigm relies on a central server to aggregate local models, potentially resulting in single points of failure, communication bottlenecks, and exposure of model parameters. In contrast, the decentralized paradigm, which does not require a central server, provides improved robustness and privacy. The essence of federated learning lies in leveraging multiple local updates for efficient communication. However, this approach may result in slower convergence or even convergence to suboptimal models in the presence of heterogeneous and imbalanced data. To address this challenge, we study decentralized federated averaging via random walk (DFedRW), which replaces multiple local update steps on a single device with random walk updates. Traditional Federated Averaging (FedAvg) and its decentralized versions commonly ignore stragglers, which reduces the amount of training data and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial random walk updates, ensuring that each computation contributes to the model update. To further improve communication efficiency, we also propose a quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves convergence upper bound of order $\mathcal {O}(\frac{1}{k^{1-q}})$ under convex conditions. Furthermore, we propose a sufficient condition that reveals when quantization balances communication and convergence. Numerical analysis indicates that our proposed algorithms outperform (decentralized) FedAvg in both convergence rate and accuracy, achieving a 38.3% and 37.5% increase in test accuracy under high levels of heterogeneities, without increasing communication costs for the busiest device.},
  archive      = {J_TMC},
  author       = {Changheng Wang and Zhiqing Wei and Lizhe Liu and Qiao Deng and Yingda Wu and Yangyang Niu and Yashan Pang and Zhiyong Feng},
  doi          = {10.1109/TMC.2025.3569423},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10295-10311},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Decentralized federated averaging via random walk},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trading fresh IoT data with strategic users. <em>TMC</em>, <em>24</em>(10), 10278-10294. (<a href='https://doi.org/10.1109/TMC.2025.3571452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense value of IoT data in real-time applications has led to the rise of fresh IoT data trading. Existing research often neglects strategic users who optimally time their data purchases, significantly affecting market demand and revenue. This paper studies a fresh data market with strategic users arriving stochastically and having heterogeneous data valuations. Strategic users decide purchase timing based on data freshness and price, while the platform optimizes its data pricing policy to maximize profit. We first examine a dynamic pricing policy, offering a price menu to each arriving user. This analysis is technically challenging due to the varied integer programming problems faced by heterogeneous users, making direct price optimization infeasible. To address this, we adopt a mechanism design approach, analytically deriving the optimal dynamic pricing policy. To reduce implementation complexity, we also study two simpler pricing policies: single and two-price pricing. In a two-period refreshing model, we derive the optimal single and two-price pricing policies analytically. Our findings reveal that the optimal two-price policy significantly outperforms the single pricing policy, guaranteeing at least 96% of the revenue achieved by the optimal dynamic pricing policy in a two-period refreshing model. Surprisingly, despite having more purchasing options, strategic users may be worse off than if they were myopic due to higher prices. The platform actually benefits from strategic users, generating up to five times more profit with strategic users than with myopic users, even while reducing data refresh frequency.},
  archive      = {J_TMC},
  author       = {Junyi He and Meng Zhang and Qian Ma and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3571452},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10278-10294},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trading fresh IoT data with strategic users},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDSG: Graph diffusion-based solution generator for optimization problems in MEC networks. <em>TMC</em>, <em>24</em>(10), 10264-10277. (<a href='https://doi.org/10.1109/TMC.2025.3568248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is crucial for the efficiency and reliability of multi-access edge computing (MEC) networks. Many optimization problems in this field are NP-hard and do not have effective approximation algorithms. Consequently, there is often a lack of optimal (ground-truth) data, which limits the effectiveness of traditional deep learning approaches. Most existing learning-based methods require a large amount of optimal data and do not leverage the potential advantages of using suboptimal data, which can be obtained more efficiently. To illustrate this point, we focus on the multi-server multi-user computation offloading (MSCO) problem, a common issue in MEC networks that lacks efficient optimal solution methods. In this paper, we introduce the graph diffusion-based solution generator (GDSG), designed to work with suboptimal datasets while still achieving convergence to the optimal solution with high probability. We reformulate the network optimization challenge as a distribution-learning problem and provide a clear explanation of how to learn from suboptimal training datasets. We develop GDSG, a multi-task diffusion generative model that employs a graph neural network (GNN) to capture the distribution of high-quality solutions. Our approach includes a straightforward and efficient heuristic method to generate a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the GNN architecture to achieve better generalization. Moreover, the proposed GDSG can achieve nearly 100% task orthogonality, which helps prevent negative interference between the discrete and continuous solution generation training objectives. We demonstrate that this orthogonality arises from the diffusion-related training loss in GDSG, rather than from the GNN architecture itself. Finally, our experiments show that the proposed GDSG outperforms other benchmark methods on both optimal and suboptimal training datasets. Regarding the minimization of computation offloading costs, GDSG achieves savings of up to 56.62% on the ground-truth training set and 41.06% on the suboptimal training set compared to existing discriminative methods.},
  archive      = {J_TMC},
  author       = {Ruihuai Liang and Bo Yang and Pengyu Chen and Xuelin Cao and Zhiwen Yu and Mérouane Debbah and Dusit Niyato and H. Vincent Poor and Chau Yuen},
  doi          = {10.1109/TMC.2025.3568248},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10264-10277},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GDSG: Graph diffusion-based solution generator for optimization problems in MEC networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-oriented cooperative VR rendering and dynamic resource leasing in metaverse. <em>TMC</em>, <em>24</em>(10), 10247-10263. (<a href='https://doi.org/10.1109/TMC.2025.3569695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the Metaverse has ushered in a new era of social networking, offering users deeply engaging spaces to connect and participate in social activities. However, rendering these virtual environments is resource-intensive. With many users accessing simultaneously and requiring diverse Metaverse services, optimizing Metaverse resources to deliver the best quality-of-experience (QoE) for users is a significant challenge. In this paper, we propose a cooperative virtual reality (VR) rendering and dynamic resource leasing mechanism to address this issue. Specifically, we first introduce a cooperative VR scene pre-rendering framework between users and Planets (i.e., edge servers hosting users), and establish a new user QoE metric named EdgeVRQoE which considers both rendering delay and visual quality. We formulate the multidimensional rendering resources (e.g., GPU, CPU, and outbound bandwidth) leasing problem between Planets and users as a double-layer decision problem, and devise a hybrid action multi-agent reinforcement learning-based dynamic resource auction mechanism to efficiently allocate limited resources of Planets in a distributed and adaptive manner. Extensive simulations demonstrate that our proposed scheme outperforms the representatives in user QoE and resource utilization efficiency. Particularly, the proposed scheme shows at least an 18-fold improvement in QoE over other schemes, demonstrating its capability in providing immersive Metaverse experiences.},
  archive      = {J_TMC},
  author       = {Nan Liu and Tom H. Luan and Yuntao Wang and Yiliang Liu and Zhou Su},
  doi          = {10.1109/TMC.2025.3569695},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10247-10263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-oriented cooperative VR rendering and dynamic resource leasing in metaverse},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraCemop: Toward federated learning with traceable contribution evaluation and model ownership protection. <em>TMC</em>, <em>24</em>(10), 10230-10246. (<a href='https://doi.org/10.1109/TMC.2025.3569547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) allows multiple clients to collaboratively train machine learning models without the need to share their local private data. As a result, it can effectively address the issue of data fragmentation. Nevertheless, insufficient evaluation of individual contributions and the lack of protections for both the intellectual property rights (IPR) of models and client privacy can greatly reduce clients’ motivations in federated training. To address these challenges, this paper introduces the Traceable Contribution Evaluation and Model Ownership Protection (TraCemop) framework for federated learning, which allows each client to swiftly assess the contributions of others in each round, with integrated support for the traceability of evaluation results. To safeguard the intellectual property of models, a collective watermark is embedded in the global model. Additionally, a secure mechanism for verifying model ownership is also available in case of disputes. Security analysis indicates that TraCemop is capable of resisting data reconstruction attacks as well as various types of model copyright infringements. Finally, we evaluate the proposed framework using two commonly-used datasets, and the experimental results show a significant improvement in the efficiency of contribution evaluation compared to existing methods. Meanwhile, IPR infringement tests on TraCemop reveal that the proposed framework is resilient against malicious efforts to monopolize model ownership.},
  archive      = {J_TMC},
  author       = {Jingwei Liu and Zihan Zhou and Rong Sun and Lei Liu and Rongxing Lu and Schahram Dustdar and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3569547},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10230-10246},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TraCemop: Toward federated learning with traceable contribution evaluation and model ownership protection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient model training in edge networks with hierarchical split learning. <em>TMC</em>, <em>24</em>(10), 10214-10229. (<a href='https://doi.org/10.1109/TMC.2025.3569407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient model training scheme, named Group-based Hierarchical Split Learning (GHSL), which can accelerate the artificial intelligence (AI) training process in edge networks in a “first-sequential-then-parallel” manner. Specifically, the proposed scheme hierarchically splits an AI model into a user-side and server-side model, while dividing a number of users into multiple groups. Users in each group train user-side models with the interaction of the shared server-side model sequentially; different groups perform the above training process parallelly; the AI models of each group are aggregated into a global model. We also carry out the convergence analysis for the proposed scheme over non-independent and identically distributed data, which reveals that the convergence rate depends on user grouping. Furthermore, we propose a data-driven two-stage user grouping algorithm to minimize the overall training delay, taking user resource heterogeneity and the black-box training process into account. The proposed algorithm first utilizes the Gaussian process regression approach to determine the number of groups, and then employs the coalition game theory to determine the optimal user grouping decision. Comprehensive simulation results demonstrate that the proposed scheme can reduce training delay, user-side computational workload, and communication overhead by up to 19%, 53%, and 54%, respectively, comparing to state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Songge Zhang and Wen Wu and Lingyang Song and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3569407},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10214-10229},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient model training in edge networks with hierarchical split learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based intelligent trusted computational resource allocation for low-altitude networks. <em>TMC</em>, <em>24</em>(10), 10200-10213. (<a href='https://doi.org/10.1109/TMC.2025.3568614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-altitude networks, unmanned aerial vehicles (UAVs) can offer services such as logistics, intelligence surveillance, and environmental monitoring, aided by base stations (BSs) with substantial computational resources. However, BSs must defend against malicious UAVs that may overload resources or launch denial-of-service attacks. In this paper, we formulate a blockchain-enabled access control model, which uses the UAV identities (IDs) and trajectories, positive and negative interactions with the BS to evaluate the reputations of UAVs. In the blockchain, the elected miner generates blocks containing UAV IDs, coordinates, interactions, and reputation values. To defend against malicious UAVs, this paper formulates a trusted computational resource allocation optimization problem, solved by safe reinforcement learning (RL) with a three-level hierarchical structure. Specifically, this method uses the designed structure to optimize the BS access control, resource allocations, and block size. In particular, we design an E-network to evaluate the long-term risk resulting from the chosen policy, which is used to refine the policy distribution for safe exploration. A modified reward function accounts for immediate risks, preventing short-term dangerous explorations that could lead to illegal access or computational failures. We prove the Lyapunov asymptotic stability of the proposed system and derive the reward upper bound. Simulation results show that our scheme can converge to the upper bound, outperform the benchmark, and validate the effectiveness via ablation experiments.},
  archive      = {J_TMC},
  author       = {Xiaozhen Lu and Lixin Liu and Zhibo Liu and Qihui Wu and Liang Xiao},
  doi          = {10.1109/TMC.2025.3568614},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10200-10213},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-based intelligent trusted computational resource allocation for low-altitude networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedding chips over the air: Rethink IoT architecture for ubiquitous sensing. <em>TMC</em>, <em>24</em>(10), 10186-10199. (<a href='https://doi.org/10.1109/TMC.2025.3567635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale IoT sensor deployment calls for inexpensive, low-power sensor nodes that still perform long-range, large-scale networking at the system level. However, current sensor nodes are constructed according to the ’one-size-fits-all’ embedded design, where the processor and RF transceiver are indispensable but underutilized in low-duty cycles, resulting in overwhelmingly significant unit price and run-time power. In this paper, we propose a novel processor-sharing IoT architecture that converts the vast majority of sensor nodes from embedded computers to low-end RF peripherals. The conventional full-fledged sensor nodes are smashed into the air, and the scattered chips are scaled well with negligible overheads through a virtual I$^{2}$C bus called RFBus. Specifically, RFBus interface is designed to be backward compatible with the I$^{2}$C bus interface, and thus, RFBus network inherits versatile link layer services transparently from the well-established I$^{2}$C link layer protocol. We design RFBus with joint consideration of system-level performance and deployment costs and evaluate the prototypes both indoors and outdoors. The result indicates that the proposed architecture achieves 6.09 × (indoor) and 6.69 × (outdoor) energy saving and reduces the unit price of sensor nodes by 23.5% (indoor) and 33.5% (outdoor).},
  archive      = {J_TMC},
  author       = {Qianhe Meng and Han Wang and Chong Zhang and Yihang Song and Songfan Li and Li Lu and Hongzi Zhu},
  doi          = {10.1109/TMC.2025.3567635},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10186-10199},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Embedding chips over the air: Rethink IoT architecture for ubiquitous sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal non-uniformity-aware online task scheduling in collaborative edge computing for industrial internet of things. <em>TMC</em>, <em>24</em>(10), 10169-10185. (<a href='https://doi.org/10.1109/TMC.2025.3567615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing mitigates the shortcomings of cloud computing caused by unpredictable wide-area network latency and serves as a critical enabling technology for the Industrial Internet of Things (IIoT). Unlike cloud computing, mobile edge networks offer limited and distributed computing resources. As a result, collaborative edge computing emerges as a promising technology that enhances edge networks’ service capabilities by integrating computational resources across edge nodes. This paper investigates the task scheduling problem in collaborative edge computing for IIoT, aiming to optimize task processing performance under long-term cost constraints. We propose an online task scheduling algorithm to cope with the spatiotemporal non-uniformity of user request distribution in distributed edge networks. For the spatial non-uniformity of user requests across different factories, we introduce a graph model to guide optimal task scheduling decisions. For the time-varying nature of user request distribution and long-term cost constraints, we apply Lyapunov optimization to decompose the long-term optimization problem into a series of real-time subproblems that do not require prior knowledge of future system states. Given the NP-hard nature of the subproblems, we design a heuristic-based hierarchical optimization approach incorporating enhanced discrete particle swarm and harmonic search algorithms. Finally, an imitation learning-based approach is devised to further accelerate the algorithm's operation, building upon the initial two algorithms. Comprehensive theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed schemes.},
  archive      = {J_TMC},
  author       = {Yang Li and Xing Zhang and Yukun Sun and Wenbo Wang and Bo Lei},
  doi          = {10.1109/TMC.2025.3567615},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10169-10185},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatiotemporal non-uniformity-aware online task scheduling in collaborative edge computing for industrial internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TouchHBC: Touch-based human body communication via leakage current. <em>TMC</em>, <em>24</em>(10), 10153-10168. (<a href='https://doi.org/10.1109/TMC.2025.3569282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable devices, including smartwatches, are increasingly popular among consumers due to their user-friendly services. However, transmitting sensitive data like social media messages and payment QR codes via commonly used low-power Bluetooth exposes users to privacy breaches and financial losses. This study introduces TouchHBC, a secure and reliable communication scheme leveraging a smartwatch’s built-in electrodes. This system establishes a touch-based human communication system utilizing a laptop’s leakage current. As the transmitting device, the laptop modulates this current via the CPU. Simultaneously, the smartwatch, equipped with built-in electrodes, captures the current traversing the human body and decodes it. The modulation and decoding processes involve techniques such as amplitude modulation, variational mode decomposition, channel estimation, and retransmission mechanisms. TouchHBC facilitates communication between laptops and smartwatches. Real-world tests demonstrate that our prototype achieves a throughput of $ 19.83\ \text{bps}$. Moreover, TouchHBC offers the potential for enhanced interaction, including improved gaming experiences through vibration feedback and secure touch login for smartwatch applications by synchronizing with a laptop. Furthermore, the system can be integrated with high-throughput communication protocols such as Bluetooth, enhancing its scalability while maintaining a strong foundation of security.},
  archive      = {J_TMC},
  author       = {Dian Ding and Hao Pan and Yongzhao Zhang and Yijie Li and Yu Lu and Yi-Chao Chen and Guangtao Xue},
  doi          = {10.1109/TMC.2025.3569282},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10153-10168},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TouchHBC: Touch-based human body communication via leakage current},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROTR: Role-transformable multi-agent resource allocation for nonstationary vehicular communications. <em>TMC</em>, <em>24</em>(10), 10135-10152. (<a href='https://doi.org/10.1109/TMC.2025.3567652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient wireless resource allocation is essential for supporting multi-vehicle cooperation. The service data exchanged among intelligent vehicles is typically diverse, with varying transmission requirements that shift according to applications and traffic conditions, leading to major fluctuation in communication situations. Existing multi-agent reinforcement learning based resource allocation methods are often inefficient in handling such nonstationary communication situations due to their rigid cooperation patterns. To this end, we propose a ROle-TRansformable multi-agent resource allocation method, named ROTR. This method adopts a hierarchical decision-making process, where a high-level agent at a base station (BS) dynamically plans and distributes cooperation roles (CRs) and cooperation behaviors (CBs) in response to fluctuating communication situations. The Low-level agents within the transmitting vehicles (TVs) perform role transformations based on the assigned CRs and subsequently receive behavioral guidance according to CBs, enabling dynamic adjustments in cooperation patterns to adapt to variable communication situations and make resource allocation decisions. Additionally, we introduce a non-BS-assisted mode based on policy distillation, which enables a seamless transition to independent operation without the BS, relying solely on local states to generate CRs and CBs, thereby facilitating global resource cooperation. Extensive simulation experiments demonstrate that the proposed framework optimizes resource efficiency in nonstationary vehicular communications.},
  archive      = {J_TMC},
  author       = {Yang Li and Quan Yuan and Xiaoyuan Fu and Guiyang Luo and Jiawen Kang and Jinglin Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3567652},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10135-10152},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ROTR: Role-transformable multi-agent resource allocation for nonstationary vehicular communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-oriented joint resource and trajectory optimization in NOMA-enhanced AAV-MEC systems. <em>TMC</em>, <em>24</em>(10), 10118-10134. (<a href='https://doi.org/10.1109/TMC.2025.3575451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Aerial Vehicle (AAV)-assisted Mobile Edge Computing (MEC) has received extensive attention because it provides resilient computation services for multiple Mobile Users (MUs). However, due to the increasing scale of offloaded tasks, the uncertain mobility of MUs, and the limited energy budget of AAV and MUs, it is extremely challenging to achieve satisfactory Quality-of-Service (QoS). Non-Orthogonal Multiple Access (NOMA), a promising technology to serve multiple MUs with limited communication resources, has great potential to be integrated with MEC. To this end, this paper proposes a QoS-oriented NOMA-enhanced AAV-MEC system, which aims to capture the potential gains of uplink NOMA and enable more MUs to benefit from edge computing servers in resource-constrained AAV-assisted MEC environments. This synergy reduces MUs’ uplink energy consumption but poses new challenges in resource allocation and AAV trajectory design. To address these challenges, we define a new metric called System Overhead Ratio (SOR) to reflect the system’s QoS, and then consider a joint optimization problem of resource allocation, transmission power control, and AAV trajectory design, with the goal of minimizing the SOR. Given the NP-hard nature of the optimization problem, we propose a Lyapunov and convex optimization-based Low-complexity Online Resource allocation and Trajectory optimization method (LORT) to solve it, and further analyze the convergence and complexity of LORT. Finally, extensive simulations show that the proposed method surpasses other benchmarks, reducing the SOR by approximately $10\%$-$ 25\%$ under various scenarios.},
  archive      = {J_TMC},
  author       = {Huan Zhou and Yadong Lu and Geyong Min and Zhiwen Yu and Liang Wang and Yao Zhang and Bin Guo},
  doi          = {10.1109/TMC.2025.3575451},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10118-10134},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-oriented joint resource and trajectory optimization in NOMA-enhanced AAV-MEC systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VerDT: A versatile digital twins framework for UAVs-based industrial cyber-physical systems. <em>TMC</em>, <em>24</em>(10), 10099-10117. (<a href='https://doi.org/10.1109/TMC.2025.3567284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cyber-physical systems, Digital Twins (DT)-powered network autonomy is emerging to embrace the fifth-generation industrial revolution. In this context, Unscrewed Aerial Vehicles (UAVs)-based low-altitude networks are expected to be the engines that drive industrial development. As an attractive industry application, UAVs-based intelligent logistics has been widely investigated to achieve a fully automated distribution manner without the aid of a workforce. However, it is difficult to perform real-time DT implementations due to limited computing resources and the high mobility of UAVs. To address the mentioned problems, we propose a Versatile DT (VerDT) framework operating at the edge. It can enable a double DT cooperation manner with a resource scheduling model and a path planning model for real-time and accurate logistics distributions. The resource scheduling model can implement the integration of computing and communication resources among UAVs for feasible cooperative distribution decisions. With the decisions, the path planning model can imitate to derive positions and velocities of UAVs for low-latency distribution performance with energy saving. Experiment results demonstrate the efficiency of our VerDT framework. Compared to state-of-the-art logistics distribution solutions, our solution reduces the distribution latency by 63.9% while improving the successful distribution ratio by 10.9%.},
  archive      = {J_TMC},
  author       = {Longyu Zhou and Supeng Leng and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3567284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10099-10117},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VerDT: A versatile digital twins framework for UAVs-based industrial cyber-physical systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure enhanced IoT-WLAN authentication protocol with efficient fast reconnection. <em>TMC</em>, <em>24</em>(10), 10085-10098. (<a href='https://doi.org/10.1109/TMC.2025.3569593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing integration of Internet of Things (IoT) devices in Wireless Local Area Networks (WLANs) necessitates robust and efficient authentication mechanisms. While existing IoT authentication protocols address certain security concerns, they often fail to provide comprehensive protection against threats such as perfect forward secrecy violations, insider attacks, and key compromise impersonation, or impose significant computational and communication overhead on resource- constrained IoT systems. This paper presents a novel Extensible Authentication Protocol (EAP) based scheme for IoT-WLAN environments that addresses these security challenges while maintaining cost-effectiveness. Our approach utilizes elliptic curve cryptography and incorporates advanced features including perfect forward secrecy, strong identity protection, and explicit key confirmation. We provide a thorough security analysis using informal heuristics, formal methods (Random Oracle Model and BAN Logic), and automated verification with ProVerif. Performance evaluations demonstrate that our protocol achieves lower communication, storage, and computational costs compared to state-of-the-art solutions, with an average 79.6% reduction in computation time. A detailed comparison with existing schemes highlights the efficiency and enhanced security features of our proposed authentication mechanism for IoT-WLAN deployments.},
  archive      = {J_TMC},
  author       = {Weizheng Wang and Qipeng Xie and Zhaoyang Han and Chunhua Su and Joel J. P. C. Rodrigues and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3569593},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10085-10098},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure enhanced IoT-WLAN authentication protocol with efficient fast reconnection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FBDT: Sum-throughput achieving transport layer solution for multi-RAT networks. <em>TMC</em>, <em>24</em>(10), 10069-10084. (<a href='https://doi.org/10.1109/TMC.2025.3569453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging mobile applications give rise to new bandwidth-hungry and latency-sensitive traffic classes that challenge existing wireless systems. Addressing them requires innovative approaches such as simultaneous data transmission across multiple Radio Access Technologies (RATs), e.g., WiFi and WiGig. However, existing transport layer multi-RAT traffic aggregation schemes, e.g., multi-path TCP, suffer from Head-of-Line (HoL) blocking and sub-optimal traffic splitting across the RATs that severely penalize their performance. In this paper, we investigate the design of FBDT, a novel multi-path transport layer solution that for the first time can achieve the sum of the throughput rates across the individual RATs network paths, despite their channel conditions’ dynamics. We have implemented FBDT in the Linux kernel and show substantial improvement in throughput relative to state-of-the-art schemes, e.g, 2.5x gain in a dual-RAT scenario (WiFi and WiGig) when the client is mobile. Second, we extend FBDT to more than two radios and demonstrate that its throughput performance scales linearly with the number of RATs, in contrast to multi-path TCP, whose performance degrades with an increase in the number of RATs. We evaluate the performance of FBDT on different traffic classes and demonstrate: (i) 2-3 times shorter file download times, (ii) up to 10 times shorter streaming times and 10 dB higher video quality for progressive download video applications, and (iii) up to 9 dB higher viewport quality for interactive mobile VR applications, when our viewport quality maximization framework is employed along with FBDT.},
  archive      = {J_TMC},
  author       = {Suresh Srinivasan and Sam Shippey and Ehsan Aryafar and Jacob Chakareski},
  doi          = {10.1109/TMC.2025.3569453},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10069-10084},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FBDT: Sum-throughput achieving transport layer solution for multi-RAT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-sensitive goods delivery and in-situ sensing using a multi-task drone. <em>TMC</em>, <em>24</em>(10), 10055-10068. (<a href='https://doi.org/10.1109/TMC.2025.3570437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones are evolving into highly capable and adaptable devices, prompting the development of advanced control frameworks. This paper introduces a novel online control framework tailored for a multi-task drone, explicitly addressing the simultaneous execution of in-situ sensing and goods delivery. To tackle this complex scenario, a finite-horizon Markov decision process (FH-MDP) is formulated to ensure not only the prompt delivery of goods but also the minimization of energy consumption and the maximization of the drone's reward for in-situ sensing. A significant contribution lies in establishing the monotonicity and subadditivity of the FH-MDP. This mathematical foundation provides evidence for the existence of an optimal, monotone, deterministic Markovian policy. The crux of the optimal policy revolves around flight distance- and time-related thresholds, determining the precise points at which the drone should switch its optimal action. This unique feature empowers the multi-task drone to make real-time decisions, such as adjusting flight speed or engaging in in-situ sensing, by comparing its current state with these predefined thresholds. This process can be accomplished with a linear complexity, ensuring efficiency in decision-making. The optimality of our approach is rigorously demonstrated through numerical validation, where it is compared against a computationally expensive, dynamic programming-based alternative. Under the considered simulation settings, our approach reduces drone energy consumption by a substantial 19.8% compared to existing benchmarks. This not only highlights the practical effectiveness of the proposed framework but also underscores its potential for significant advancements in the field of drone operations and energy efficiency.},
  archive      = {J_TMC},
  author       = {Bin Liu and Wei Ni and Ren Ping Liu and Y. Jay Guo and Hongbo Zhu},
  doi          = {10.1109/TMC.2025.3570437},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10055-10068},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay-sensitive goods delivery and in-situ sensing using a multi-task drone},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Teaching to fish rather than giving a fish: The concentrator method of teaching classic congestion control with learning-based module. <em>TMC</em>, <em>24</em>(10), 10042-10054. (<a href='https://doi.org/10.1109/TMC.2025.3567582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Congestion Control (CC) algorithms are expected to satisfy the diverse demands of applications running over diverse networks. To achieve this goal, the combinations, aiming to inherit both the advantages of classic CC in terms of convergence, overhead, and explainability, and the advantages of learning-based CC on adapting to diverse networks and demands, become a hot topic. In this paper, we reveal the existing combination works are either giving a fish or teaching to fish. Based on the insight of their essential issues, we develop the Concentrator method of teaching to fish. According to this method, we propose Seagull. Specifically, Seagull captures the network characteristics and application demands in a coarse-grained manner via an online learning module. Moreover, this module guides to customize the rate adjustment rules of the classic CC module for fine-grained system evolution. Replacing the assumption on networks by the captured characteristics, the classic CC module of Seagull can fulfill the specified application demands. Real-world experimental results show Seagull respectively outperforms Orca, PCC-Vivace, and CUBIC by $49.3\%,\ 30.4\%$, and 24.9% in terms of throughput over the Internet, and improves the video quality of experience (QoE) by $12.9\sim 33.5\%$ compared to CUBIC over cellular links.},
  archive      = {J_TMC},
  author       = {Haoyang Li and Wanchun Jiang and Jie Wang and Ying Wang and Jiawei Huang and Danfeng Shan and Jianxin Wang},
  doi          = {10.1109/TMC.2025.3567582},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10042-10054},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Teaching to fish rather than giving a fish: The concentrator method of teaching classic congestion control with learning-based module},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADEC: A combinatorial auction for dynamic distributed DNN inference scheduling in edge-cloud networks. <em>TMC</em>, <em>24</em>(10), 10024-10041. (<a href='https://doi.org/10.1109/TMC.2025.3567459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network (DNN) Inference, as a key enabler of intelligent applications, is often computation-intensive and latency-sensitive. Combining the advantages of cloud computing (abundant computing resources) and edge computing (fast transmission), edge-cloud collaborative DNN inference is a powerful solution to these problems. However, in edge-cloud networks with heterogeneous resources, how to obtain reasonable decisions on server selection, model partition and resource allocation for efficient distributed DNN inference is a hard challenge. Furthermore, it is non-trivial to design suitable resource prices to maximize the social welfare. These challenges even escalate in dynamic edge-cloud networks where decisions should be generated as soon as each user arrives without future information. Therefore, we design a combinatorial auction for dynamic distributed DNN inference scheduling, named CADEC. CADEC first constructs a bid set for each user based on convex optimization theory for optimal solution searching. Next, prices of resources in the edge-cloud network are adjusted according to changes in supply-demand relationship, and whether to admit the request of each user is decided. Finally, the dynamic distributed inference scheduling decisions are generated through the primal-dual algorithm to maximize the social welfare. Theoretical analysis shows the good competitive ratio and polynomial time complexity of CADEC. Results of simulation experiments present that CADEC improves social welfare by up to 224% compared with state-of-the-art distributed DNN inference schemes.},
  archive      = {J_TMC},
  author       = {Xiaolong Xu and Yuhao Hu and Guangming Cui and Lianyong Qi and Wanchun Dou and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3567459},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10024-10041},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CADEC: A combinatorial auction for dynamic distributed DNN inference scheduling in edge-cloud networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unimodal training-multimodal prediction: Cross-modal federated learning with hierarchical aggregation. <em>TMC</em>, <em>24</em>(10), 10009-10023. (<a href='https://doi.org/10.1109/TMC.2025.3567535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning has significantly advanced the extraction of features from varied data sources, enhancing model performance. Federated learning (FL) complements this by enabling collaborative training while maintaining data privacy. The fusion of these two fields, multimodal federated learning, offers considerable promise. Yet, standard methods often incorrectly assume that each node in the FL network has a full complement of multimodal data, which is rare in real-world applications. In our study, we present a novel architecture designed to surmount these challenges, termed the Unimodal Training - Multimodal Prediction (UTMP) framework, positioned within the multimodal federated learning paradigm. Our proposed model, the HA-Fedformer, is a transformer-based model crafted to facilitate unimodal training on the client-side using exclusively unimodal datasets and to execute multimodal inference by synthesizing insights from multiple clients. Our HA-Fedformer model effectively handles non-IID data through a novel uncertainty-aware aggregation technique and layer-wise Markov Chain Monte Carlo sampling in local encoders. It also resolves misaligned language sequences via cross-modal decoder aggregation, capturing correlations between decoders trained on different modalities. Our comprehensive evaluations conducted on widely recognized sentiment analysis benchmarks demonstrate the superiority of the HA-Fedformer. The results show that our model achieves a substantial uplift in performance.},
  archive      = {J_TMC},
  author       = {Rongyu Zhang and Xiaowei Chi and Wenyi Zhang and Guiliang Liu and Dan Wang and Fangxin Wang},
  doi          = {10.1109/TMC.2025.3567535},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10009-10023},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unimodal training-multimodal prediction: Cross-modal federated learning with hierarchical aggregation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint scheduling, computing, and load balancing for time sensitive traffic in SDN-enabled space-air-ground integrated 6G networks: A federated reinforcement learning approach. <em>TMC</em>, <em>24</em>(10), 9995-10008. (<a href='https://doi.org/10.1109/TMC.2025.3567289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) constellations and Unmanned Aerial Vehicle (UAV) networks enable wide coverage for the sixth generation (6G) mobile communication. However, it is a challenge to achieve high scheduling success rate, ultra-low latency, and efficient load balance in the Space-Air-Ground Integrated 6G Network (SAGGIN). This paper addresses the following issue: How to effectively and orderly transmit time-sensitive traffic in SAGGIN under strict deadlines, limited computational ability, and restrained link capacity? Specifically, this paper uses Software-Defined Networking (SDN) and designs a joint optimization method to enhance the traffic transmission ability of SAGGIN. Considering response time, computing cost, and link capacity in SAGGIN, the scheduling, computing, and load balance issues are modeled as a multi-objective optimization problem that minimizes the worst-case response time and computing cost of data frames while maximizing the network flow. Then, this paper leverages a Federated Reinforcement Learning (FRL) scheme to solve the problem. Results show that the FRL could achieve great scheduling, computing, and load balance performance. Specifically, our method can successfully schedule 80% of the traffic at most when the current network load is around 90%. Furthermore, the computational delay could reduce around 50%.},
  archive      = {J_TMC},
  author       = {Haitong Sun and Haijun Zhang and Hui Ma and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3567289},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9995-10008},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint scheduling, computing, and load balancing for time sensitive traffic in SDN-enabled space-air-ground integrated 6G networks: A federated reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented mulsemedia communication using unified perceiver and conformal prediction in 6G wireless systems. <em>TMC</em>, <em>24</em>(10), 9980-9994. (<a href='https://doi.org/10.1109/TMC.2025.3567880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prominence of eXtended Reality (XR), holographic-type communications, and metaverse demands truly immersive user experiences by using many sensory modalities, including sight, hearing, touch, smell, taste, etc. Additionally, the widespread deployment of sensors in areas such as agriculture, manufacturing, and smart homes is generating diverse sensory data. A new media format known as multisensory media (mulsemedia) has emerged, which incorporates many sensory modalities beyond the traditional visual and auditory media. 6G wireless systems are envisioned to support the Internet of Senses, making it crucial to explore effective data fusion and communication strategies for mulsemedia. In this paper, we introduce a task-oriented multi-task mulsemedia communication system named MuSeCo, which is developed using unified Perceiver models and Conformal Prediction. This unified model can accept any sensory input and efficiently extract latent semantic features, making it adaptable for deployment across various Artificial Intelligence of Things (AIoT) devices. Conformal Prediction is employed for modality selection and combination, enhancing task accuracy while minimizing data communication overhead. The model is trained using six sensory modalities across four classification tasks. Simulations and experiments demonstrate that it can effectively fuse sensory modalities, significantly reduce end-to-end communication latency and energy consumption, and maintain high accuracy in communication-constrained systems.},
  archive      = {J_TMC},
  author       = {Hongzhi Guo and Ian F. Akyildiz},
  doi          = {10.1109/TMC.2025.3567880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9980-9994},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task-oriented mulsemedia communication using unified perceiver and conformal prediction in 6G wireless systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative video processing of multiple cameras in smart transportation: Content analysis and resource allocation. <em>TMC</em>, <em>24</em>(10), 9965-9979. (<a href='https://doi.org/10.1109/TMC.2025.3567062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of smart transportation, the collaborative processing of video data sourced from multiple cameras plays a pivotal role in promoting efficient traffic management and augmenting safety measures. Nevertheless, the exponential surge in surveillance cameras deployment has concurrently engendered a rapid increase in the magnitude of video analysis tasks and data volume. To address these challenges, we propose a comprehensive framework for collaborative video processing. Primarily, a collaborative content analysis approach is proposed, and which employs a Transformer-based ReID (Re-identification) algorithm to construct key stickers. These key stickers are optimized with cross-cameras correlations and serve as the foundational structure for subsequent online video compression. Subsequently, we propose a collaborative resource allocation approach, and which involves the formulation of a queue model designed for the orchestration of online camera analysis tasks. In addition, we have devised an enhanced deep reinforcement learning algorithm to fine-tune the task scheduling configuration of multiple cameras, with guidance from the queue model. Extensive experiments and simulations were conducted to evaluate the proposed framework. The results demonstrate its effectiveness in achieving accurate and real-time analysis of video data in smart transportation scenarios.},
  archive      = {J_TMC},
  author       = {Lei Du and Ru Huo and Chuang Sun and Shuo Wang and Tao Huang},
  doi          = {10.1109/TMC.2025.3567062},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9965-9979},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative video processing of multiple cameras in smart transportation: Content analysis and resource allocation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location and reward privacy-preserving based secure task allocation in mobile crowdsensing. <em>TMC</em>, <em>24</em>(10), 9951-9964. (<a href='https://doi.org/10.1109/TMC.2025.3564404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-task allocation has become an essential research topic in Mobile Crowdsensing (MCS). Most existing studies merely focus on minimizing the total distance that workers need to travel, but ignore considering the total task rewards, which could lead to a reduction in the willingness of workers to complete tasks. In this paper, to incentivize workers to participate in tasks and protect their privacy, we propose a Location and Reward Privacy-Preserving based Secure Task Allocation(LRPP-STA) scheme. First, we design a secure distance computation method to obtain the distance from the workers to the tasks under location privacy preserving. Second, considering fixed reward for the task, we propose a Fixed Rewarding Secure Task Allocation(FR-STA) scheme, where a secure utility calculation method is proposed to calculate the encrypted utility of the worker upon completing tasks under rewards privacy preserving, along with the path planning for workers to maximize the total utility of the system through an Extended Maximum-Utility Flow model(EMUF). Third, considering the situation of dynamic task reward adjusted by requesters based on the supply and demand relationship as well as the urgency of the task, we propose a Dynamic Rewarding Secure Task Allocation(DR-STA) scheme to optimize the task allocation for workers while improving requesters satisfaction. Finally, we theoretically analyze the security of location and reward privacy-preserving scheme, and conduct extensive experiments with real-world datasets to verify that the secure task allocation scheme is effective in improving the total utility of workers compared to other baseline online tasking schemes.},
  archive      = {J_TMC},
  author       = {Zhetao Li and Weifan Shi and Young-June Choi and Hiroo Sekiya and Qingyong Deng},
  doi          = {10.1109/TMC.2025.3564404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9951-9964},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Location and reward privacy-preserving based secure task allocation in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressed private aggregation for scalable and robust federated learning over massive networks. <em>TMC</em>, <em>24</em>(10), 9934-9950. (<a href='https://doi.org/10.1109/TMC.2025.3564390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging paradigm that allows a central server to train machine learning models using remote users’ data. Despite its growing popularity, Federated learning (FL) faces challenges in preserving the privacy of local datasets, its sensitivity to poisoning attacks by malicious users, and its communication overhead, especially in large-scale networks. These limitations are often individually mitigated by local differential privacy (LDP) mechanisms, robust aggregation, compression, and user selection techniques, which typically come at the cost of accuracy. In this work, we present compressed private aggregation (CPA), allowing massive deployments to simultaneously communicate at extremely low bit rates while achieving privacy, anonymity, and resilience to malicious users. CPA randomizes a codebook for compressing the data into a few bits using nested lattice quantizers, while ensuring anonymity and robustness, with a subsequent perturbation to hold LDP. CPA-aided FL is proven to converge in the same asymptotic rate as FL without privacy, compression, and robustness considerations, while satisfying both anonymity and LDP requirements. These analytical properties are empirically confirmed in a numerical study, where we demonstrate the performance gains of CPA compared with separate mechanisms for compression and privacy, as well as its robustness in mitigating the harmful effects of malicious users.},
  archive      = {J_TMC},
  author       = {Natalie Lang and Nir Shlezinger and Rafael G. L. D’Oliveira and Salim El Rouayheb},
  doi          = {10.1109/TMC.2025.3564390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9934-9950},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compressed private aggregation for scalable and robust federated learning over massive networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-aware incentive mechanism for multi-server federated learning in edge-enabled wireless networks with differential privacy. <em>TMC</em>, <em>24</em>(10), 9919-9933. (<a href='https://doi.org/10.1109/TMC.2025.3564301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a distributed machine learning method, federated learning (FL) can collaboratively train a global model with multiple devices without sharing the original data, thus protecting certain privacy. However, due to the strong heterogeneity of edge nodes (ENs) participating in FL, the quality of data uploaded to the parameter server (PS) varies significantly. Without an appropriate incentive mechanism, low-quality contributors may receive disproportionately high rewards, while high-quality contributors may lack sufficient motivation, leading to inefficient participation and suboptimal global model performance. Consequently, it is critical to develop an effective incentive mechanism to promote fairness for the FL process. To address the issues of existing FL incentive mechanisms lacking privacy protection performance analysis, we propose a fairness-aware incentive mechanism for multi-server FL in edge-enabled wireless differential privacy (DP) networks. Specifically, the wireless channel noise is used to provide DP protection for the local model gradients uploaded by ENs. Next, the interaction between the PSs and ENs is modeled as a Stackelberg game. Furthermore, we solve the Stackelberg game process using backward induction and theoretically propose optimal strategies for both the PSs and ENs. Finally, extensive numerical simulations using real datasets demonstrate the superior performance of our theoretical analysis of the proposed scheme.},
  archive      = {J_TMC},
  author       = {Yu Yang and Kai Peng and Shangguang Wang and Xiaolong Xu and Peiyun Xiao and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3564301},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9919-9933},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fairness-aware incentive mechanism for multi-server federated learning in edge-enabled wireless networks with differential privacy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing convergence, privacy and fairness for wireless personalized federated learning: Quantization-assisted min-max fair scheduling. <em>TMC</em>, <em>24</em>(10), 9902-9918. (<a href='https://doi.org/10.1109/TMC.2025.3566421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) offers a solution to balancing personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). Little attention has been given to wireless PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is another challenge resulting from communication bottlenecks in WPFL. This paper exploits quantization errors to enhance the privacy of WPFL and proposes a novel quantization-assisted Gaussian differential privacy (DP) mechanism. We analyze the convergence upper bounds of individual PL models by considering the impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and imperfect communication channels on the FL of WPFL. By minimizing the maximum of the bounds, we design an optimal transmission scheduling strategy that yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by revealing the nested structure of this problem to decouple it into subproblems solved sequentially for the client selection, channel allocation, and power control, and for the learning rates and PL-FL weighting coefficients. Experiments validate our analysis and demonstrate that our approach substantially outperforms alternative scheduling strategies by 87.08%, 16.21%, and 38.37% in accuracy, the maximum test loss of participating clients, and fairness (Jain's index), respectively.},
  archive      = {J_TMC},
  author       = {Xiyu Zhao and Qimei Cui and Ziqiang Du and Wei Ni and Weicai Li and Xi Yu and Ji Zhang and Xiaofeng Tao and Ping Zhang},
  doi          = {10.1109/TMC.2025.3566421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9902-9918},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing convergence, privacy and fairness for wireless personalized federated learning: Quantization-assisted min-max fair scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sonicumos: An enhanced active face liveness detection system via ultrasonic and video signals. <em>TMC</em>, <em>24</em>(10), 9883-9901. (<a href='https://doi.org/10.1109/TMC.2025.3565689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonicumos is an enhanced behavior-based face liveness detection system that combines ultrasonic and video signals to sense the 3D head gestures. As face authentication becomes increasingly prevalent, the need for a reliable liveness detection system is paramount. Traditional behavior-based liveness detection methods (e.g., eye-blinking, nodding, etc.), which are widely deployed in mission-critical scenarios like finance and banking applications today, are prone to advanced media-based facial forgery attacks. Sonicumos aims to incorporate the traditional behavior-based method for active liveness detection without introducing extra user burden. By employing ultrasonic signals, Sonicumos capitalizes on the head gestures, significantly raising the security bar. Our approach utilizes the frequency-modulated continuous-wave (FMCW) ultrasonic radar for robust 3D gesture recognition compatible with face authentication. We also propose a new dual-feature fusion network that integrates audio and video features at the feature level to increase detection accuracy and resilience against numerous attacks. Our prototype has been tested on seven off-the-shelf Android/iOS smartphones, achieving an overall detection accuracy of 95.83% at an equal error rate (EER) of 4.96% when dealing with 3D impersonation attacks.},
  archive      = {J_TMC},
  author       = {Yihao Wu and Peipei Jiang and Jianhao Cheng and Lingchen Zhao and Chao Shen and Cong Wang and Qian Wang},
  doi          = {10.1109/TMC.2025.3565689},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9883-9901},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sonicumos: An enhanced active face liveness detection system via ultrasonic and video signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture-of-experts as continual knowledge adapter for mobile vision understanding. <em>TMC</em>, <em>24</em>(10), 9868-9882. (<a href='https://doi.org/10.1109/TMC.2025.3567179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual machine learning in the context of limited computational resources and data availability is critical in the connected digital world. Current intelligent applications predominantly rely on deep learning models requiring labor/computation-intensive training. These models often struggle to adapt effectively to new data while preserving performance on previously learned knowledge. In this paper, we introduce a lightweight method for continual knowledge adaptation that can address these challenges. To prevent disruption of the existing services, we propose a Mixture-of-Experts (MoE) adapter that integrates seamlessly with the existing vision model to encode new data. The weights of the original model are kept fixed during the adaptation process, ensuring the preservation of previously learned knowledge. The MoE technique enables scaling up the parameters of the adapter while maintaining a relatively low computation, making it fit for constrained devices in mobile computation scenarios. Furthermore, to enhance learning efficiency and accelerate convergence with new data, we implement a knowledge fusion mechanism that facilitates interaction between the existing knowledge and the information extracted from new data. The timing of employing the fusion module is further investigated. We find that it is conducive in scenarios where the task’s performance requirements are enhanced. The MoE adapter and knowledge fusion module are integrated at each stage with minimal trainable parameters, efficiently optimizing resource usage. Extensive experiments and ablation studies validate the effectiveness of the proposed method. Specifically, the proposed method prevents an accuracy drop of 43.02% on the previous data compared to the continual train method, while achieving an accuracy of 44.81% on the new data, which is even 0.34% higher than fully training a new model.},
  archive      = {J_TMC},
  author       = {Bicheng Guo and Conghao Zhou and Shibo He and Jiming Chen and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3567179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9868-9882},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixture-of-experts as continual knowledge adapter for mobile vision understanding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive search and collaborative offloading under device-to-device joint edge computing network. <em>TMC</em>, <em>24</em>(10), 9852-9867. (<a href='https://doi.org/10.1109/TMC.2025.3567549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) and Device-to-Device (D2D) peer offloading are two promising paradigms in the mobile Internet of Things (IoT). In this paper, we study the collaborative task offloading with redundant data and codes in large-scale IoT networks, where computing resource-starved IoT devices can offload their tasks to MEC servers via cellular links or to nearby peer devices (PDs) with idle resources through D2D links for execution. IoT tasks usually consist of a series of dependent and parallel subtasks, and the difficulties in current research are (i) how to eliminate redundancy in data or codes between subtasks, and (ii) how to leverage previous experience to adaptively search a set of collaborative MEC servers and PDs for matching offloading of dependent and parallel subtasks. From this, we propose a redundancy-aware adaptive search offloading (RASO) method based on the deep Q-network (DQN). Specifically, we first design a fine-grained task recombination scheme by judging the consistency of subtask data and codes. After that, we organize the global devices into a spatial index MP-tree to reduce the search solution space, and propose a fast adaptive search method based on the DQN combined with MP-tree, where optimal path-guiding parameters training of inner and outer layers is involved to efficiently help achieve collaborative devices to complete specific tasks with the same type. After finding the collaborative MEC servers and PDs along MP-tree for a certain task, a centralized stable matching algorithm is further developed to give a decision of offloading each of its divided dependent and parallel subtasks to the matched one, thereby optimizing offloading delay and energy consumption. Extensive simulation results show that compared to other counterpart solutions, our proposed method has improved task offloading performance in terms of delay and energy consumption.},
  archive      = {J_TMC},
  author       = {Jine Tang and Wentao Zhao and Jiahao Jin and Yong Xiang and Xiaofei Wang and Zhangbing Zhou},
  doi          = {10.1109/TMC.2025.3567549},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9852-9867},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive search and collaborative offloading under device-to-device joint edge computing network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical index retrieval-driven wireless network intent translation with LLM. <em>TMC</em>, <em>24</em>(10), 9837-9851. (<a href='https://doi.org/10.1109/TMC.2025.3564937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intent-Based Networking (IBN) represents an emerging network management concept that is designed to fulfill user service requirements through automation. At its core, IBN is capable of translating user intent into network policies, thereby enabling automated configuration and management. However, the application of IBN has been limited by challenges associated with automation and intelligence. The recent widespread adoption of Large Language Model (LLM) has partially mitigated these issues. Nonetheless, hardware heterogeneity and high dynamic networks remain significant challenges for IBN: (i) Devices from different vendors are challenging to manage uniformly; (ii) Aligning service demands with rapidly changing network status is difficult. To address these challenges, we propose LIT, a framework of LLM-empowered Intent Translation with manual guidance. LIT incorporates Retrieval-Augmented Generation (RAG) to reference hardware manuals and enhance the generation results of LLMs. To reduce noise from retrieval results, we optimized the general RAG process. Additionally, LIT introduces MoE (Mixture of Experts) to adjust parameter values according to network status by synthesizing results from multiple expert models. Experiments demonstrate that LIT alleviates the challenges faced by IBN, achieving a 57.5% improvement in F1 score compared to the baseline.},
  archive      = {J_TMC},
  author       = {Jingyu Wang and Lingqi Guo and Jianyu Wu and Caijun Yan and Haifeng Sun and Lei Zhang and Zirui Zhuang and Qi Qi and Jianxin Liao},
  doi          = {10.1109/TMC.2025.3564937},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9837-9851},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical index retrieval-driven wireless network intent translation with LLM},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented semantic communication in large multimodal models-based vehicle networks. <em>TMC</em>, <em>24</em>(10), 9822-9836. (<a href='https://doi.org/10.1109/TMC.2025.3564543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12 dB and 33.1% at 10 dB, respectively.},
  archive      = {J_TMC},
  author       = {Baoxia Du and Hongyang Du and Dusit Niyato and Ruidong Li},
  doi          = {10.1109/TMC.2025.3564543},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9822-9836},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task-oriented semantic communication in large multimodal models-based vehicle networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RadarODE: An ODE-embedded deep learning model for contactless ECG reconstruction from millimeter-wave radar. <em>TMC</em>, <em>24</em>(10), 9806-9821. (<a href='https://doi.org/10.1109/TMC.2025.3563945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar-based cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses purely data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model that considers the fine-grained cardiac feature sensed by radar, and a novel deep learning framework called radarODE is designed to extract both temporal and morphological features for generating ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the dataset, the proposed radarODE achieves better performance compared with the benchmark in terms of missed detection rate, root mean square error, Pearson correlation coefficient with improvements of 9%, 16% and 19%, respectively. The validation results imply that radarODE is capable of recovering ECG signals from radar signals with high fidelity and can potentially be implemented in real-life scenarios.},
  archive      = {J_TMC},
  author       = {Yuanyuan Zhang and Runwei Guan and Lingxiao Li and Rui Yang and Yutao Yue and Eng Gee Lim},
  doi          = {10.1109/TMC.2025.3563945},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9806-9821},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RadarODE: An ODE-embedded deep learning model for contactless ECG reconstruction from millimeter-wave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AgentsCoMerge: Large language model empowered collaborative decision making for ramp merging. <em>TMC</em>, <em>24</em>(10), 9791-9805. (<a href='https://doi.org/10.1109/TMC.2025.3564163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent’s own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflection guided training paradigm to further enhance the decision-making capability of the framework. Extensive experiments are conducted to evaluate the performance of our proposed method, demonstrating its superior efficiency and effectiveness for multi-agent collaborative decision-making under various ramp merging scenarios.},
  archive      = {J_TMC},
  author       = {Senkang Hu and Zhengru Fang and Zihan Fang and Yiqin Deng and Xianhao Chen and Yuguang Fang and Sam Tak Wu Kwong},
  doi          = {10.1109/TMC.2025.3564163},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9791-9805},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AgentsCoMerge: Large language model empowered collaborative decision making for ramp merging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive transport with high link utilization using opportunistic packets in cloud data centers. <em>TMC</em>, <em>24</em>(10), 9774-9790. (<a href='https://doi.org/10.1109/TMC.2025.3563182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the stringent demanding low latency and high throughput of cloud datacenter applications, recent receiver-driven transport protocols transmit only one packet once receiving each credit packet from the receiver to achieve ultra-low queueing delay. However, the round-trip time variation and the highly dynamic background traffic significantly deteriorate the performance of receiver-driven transport protocols, resulting in under-utilized bandwidth. This article designs a simple yet effective solution called RPO, which retains the advantages of receiver-driven transmission while efficiently utilizing the available bandwidth. Specifically, RPO rationally uses low-priority opportunistic packets to ensure high network utilization without increasing the queueing delay of high-priority normal packets. Furthermore, to tackle the queueing buildup due to line-rate transmission in the first RTT, we design a selective dropping mechanism called SDM to help the majority of small flows complete within only one RTT by prioritizing the first-RTT bursty packets over the packets triggered by grants. We implement RPO in Linux hosts with DPDK. The experimental results show that RPO significantly improves the network utilization by up to 35% over the state-of-the-art schemes, without introducing additional queueing delay. Moreover, RPO integrated with SDM reduces the AFCT of small flows by up to 45% compared with RPO integrated with Aeolus.},
  archive      = {J_TMC},
  author       = {Jinbin Hu and Jiawei Huang and Zhaoyi Li and Yijun Li and Shuying Rao and Wenchao Jiang and Kai Chen and Jianxin Wang and Tian He},
  doi          = {10.1109/TMC.2025.3563182},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9774-9790},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive transport with high link utilization using opportunistic packets in cloud data centers},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedACS: An adaptive client selection framework for communication-efficient federated graph learning. <em>TMC</em>, <em>24</em>(10), 9760-9773. (<a href='https://doi.org/10.1109/TMC.2025.3563404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated graph learning (FGL) has been proposed to collaboratively train the increasing graph data with graph neural networks (GNNs) in a recommendation system. Nevertheless, implementing an efficient recommendation system with FGL still faces two primary challenges, i.e., limited communication bandwidth and non-IID local graph data. Existing works typically reduce communication frequency or transmission amount, which may suffer significant performance degradation under non-IID settings. Furthermore, some researchers propose to share the underlying structure among clients, which brings massive communication cost. To this end, we propose an efficient FGL framework, named FedACS, which adaptively selects a subset of clients for model training, to alleviate communication overhead and non-IID issues simultaneously. In FedACS, the global GNN model learns significant hidden edges and the structure of graph data among selected clients, enhancing recommendation efficiency. This capability distinguishes it from the traditional FL client selection methods. To optimize the client selection process, we introduce a multi-armed bandit (MAB) based algorithm to select participating clients according to the resource budgets and the training performance (i.e., RMSE). Experimental results indicate that FedACS improves RMSE by 5.4% over baselines with the same resource budget and reduces communication costs by up to 70.7% to achieve the same RMSE performance.},
  archive      = {J_TMC},
  author       = {Hongli Xu and Xianjun Gao and Jianchun Liu and Qianpiao Ma and Liusheng Huang},
  doi          = {10.1109/TMC.2025.3563404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9760-9773},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedACS: An adaptive client selection framework for communication-efficient federated graph learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid data-driven SSM for interpretable and label-free mmWave channel prediction. <em>TMC</em>, <em>24</em>(10), 9743-9759. (<a href='https://doi.org/10.1109/TMC.2025.3564260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of mmWave time-varying channels is essential for mitigating the issue of channel aging in highly dynamic scenarios. Existing channel prediction methods have limitations: classical model-based methods often struggle to track highly nonlinear channel dynamics due to limited expert knowledge, while emerging data-driven methods typically require substantial labeled data for effective training and often lack interpretability. To address these issues, this paper proposes a novel hybrid method that integrates a data-driven neural network into a conventional model-based workflow based on a state-space model (SSM), implicitly tracking complex channel dynamics from data without requiring precise expert knowledge. Additionally, a novel unsupervised learning strategy is developed to train the embedded neural network solely with unlabeled data. Theoretical analyses and ablation studies are conducted to interpret the enhanced benefits gained from the hybrid integration. Numerical simulations based on the 3GPP mmWave channel model corroborate the superior prediction accuracy of the proposed method, compared to state-of-the-art methods that are either purely model-based or data-driven. Furthermore, extensive experiments validate its robustness against various challenging factors, including among others severe channel variations.},
  archive      = {J_TMC},
  author       = {Yiyong Sun and Jiajun He and Zhidi Lin and Wenqiang Pu and Feng Yin and Hing Cheung So},
  doi          = {10.1109/TMC.2025.3564260},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9743-9759},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hybrid data-driven SSM for interpretable and label-free mmWave channel prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trusted clustering based federated learning in edge networks. <em>TMC</em>, <em>24</em>(10), 9726-9742. (<a href='https://doi.org/10.1109/TMC.2025.3566492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is integral to advancing edge intelligence by enabling collaborative machine learning. In FL-empowered edge networks, computing nodes first train local models and then send them to an or multiple aggregation node(s) for global model collaboration. However, the trustworthiness of both local and global models in conventional FL frameworks is compromised due to inadequate model security and transparency. Distributed ledger technique (DLT) can address this issue by leveraging multi-nodes trust capabilities to support distributed consensus. However, model training and consensus performance of DLT may significantly degrade due to instability and resource constraints of edge networks. Sharding technique provides an effective approach by dividing the ledger into smaller and manageable shards. In this paper, to improve model training and consensus performance, we propose a trusted FL framework by incorporating sharding DLT into FL frameworks. We construct a theoretical model to investigate the relationship between model training performance, consensus efficiency, and capacity of edge nodes regarding storage, computing and communications. Based on the theoretical model, we propose a trusted clustering scheme to aggregate local models. Numerical results show that our proposed scheme significantly improves network throughput for transmitting models while guaranteeing model learning performance in comparison with some classical baselines.},
  archive      = {J_TMC},
  author       = {Yi-Jing Liu and Long Zhang and Xiaoqian Li and Hongyang Du and Gang Feng and Shuang Qin and Jiacheng Wang},
  doi          = {10.1109/TMC.2025.3566492},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9726-9742},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trusted clustering based federated learning in edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated unlearning with fast recovery. <em>TMC</em>, <em>24</em>(10), 9709-9725. (<a href='https://doi.org/10.1109/TMC.2025.3563265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent federated unlearning studies mainly focus on removing the target client's contributions from the global model permanently. However, the requirement for accommodating temporary user exits or additions in federated learning has been neglected. In this paper, we propose a novel recoverable federated unlearning scheme, named RFUL, which allows users to remove or add their local model to the global one at any time easily and quickly. It mainly consists of two main components, i.e., knowledge unlearning and knowledge recovery. In knowledge unlearning, the target contributions can be eliminated by training with mislabeled target data, while preserving the non-target contributions through distillation using the original model. In knowledge recovery, the forgotten contributions can be restored by training the target data using classification loss, while the non-target contributions are maintained through feature distillation and parameter freezing on the classifier. Both knowledge unlearning and recovery processes only require the participation of target data, guaranteeing the algorithm's practicality in federated learning systems. Extensive experiments demonstrate the significant efficacy of RFUL. For knowledge unlearning, RFUL matches state-of-the-art methods using only target data, achieving a runtime speedup of 3.3 to 8.7 times compared to retraining across various datasets. For knowledge recovery, RFUL exceeds state-of-the-art incremental learning methods by 5.02% to 29.97% in accuracy and achieves a runtime speedup of 1.8 to 4.4 times compared to retraining on different datasets.},
  archive      = {J_TMC},
  author       = {Changjun Zhou and Chenglin Pan and Minglu Li and Pengfei Wang},
  doi          = {10.1109/TMC.2025.3563265},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9709-9725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated unlearning with fast recovery},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward universal personalization in federated learning via collaborative foundation generative models. <em>TMC</em>, <em>24</em>(10), 9695-9708. (<a href='https://doi.org/10.1109/TMC.2025.3564880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) enhances the performance of customized client models through collaborative training without compromising data privacy and ownership. Some previous PFL methods rely on rich prior knowledge about the types of data heterogeneity (such as class imbalance or feature skew), which greatly limits their application ranges. In this paper, we study the Universal Personalization in Federated Learning (UniPFL), the problem that has no prior knowledge about the types of data heterogeneity. In real-world PFL scenarios, UniPFL is potential because the data distributions of clients are usually heterogeneous and unknown to the server, where quantity imbalance, class imbalance, feature skew, or hybrid heterogeneity are possible contingencies. To address UniPFL, we propose FedFD, a novel framework with local data augmentation and global concept fusion, which is based on the recent advances in the foundation generative models (e.g., diffusion models, BLIP-2). On the client side, FedFD utilizes a diffusion model to assist local training by generating augmented data samples, and is then efficiently fine-tuned to be personalized. On the server side, we customize the aggregation strategies based on model similarities to learn both personalized models and diverse feature concepts. Extensive experiments show that FedFD reaches the state-of-the-art on (1) CIFAR-10 and CIFAR-100 for class imbalance; (2) DomainNet and Office-10 for feature skew, and (3) hybrid heterogeneity with both class and feature shifts.},
  archive      = {J_TMC},
  author       = {Chenrui Wu and Zexi Li and Fangxin Wang and Hongyang Chen and Jiajun Bu and Haishuai Wang},
  doi          = {10.1109/TMC.2025.3564880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9695-9708},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward universal personalization in federated learning via collaborative foundation generative models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salix-leaf: Find main veins of signal clusters for practical parallel decoding. <em>TMC</em>, <em>24</em>(10), 9683-9694. (<a href='https://doi.org/10.1109/TMC.2025.3562590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel decoding of backscatter improves communication throughput by enabling concurrent transmission of backscatter tags. In practical applications of parallel decoding, it is extremely difficult to distinguish collided signals in superclusters where multiple signal clusters overlap. Existing methods are usually effective for superclusters with uniformly distributed signals. Nevertheless, there are many more scenarios in which signals in superclusters tend to gather unevenly, and existing methods cannot work. Such uneven clustering of signals occurs due to the following two possible causes: (1) signal-strength-differences (SSDs) among tags; or (2) cluster drifting (CD) driven by interferences from other objects within communication environments. This paper proposes a novel scheme called Salix-Leaf, which aims to identify the main veins of signal clusters to address this problem of superclusters with unevenly distributed signals. Salix-Leaf identifies the main vein of each signal cluster for fine-grained clustering so that the direction of the main veins can be used to verify the accuracy of clustering. In addition, Salix-Leaf employs a supercluster decomposer that divides signals into different segments for clustering analysis, enhancing robustness and practicability. Experimental results show that Salix-Leaf achieves a 1.2-fold increase in throughput and a 25% reduction in bit error rate (BER) compared to the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Yajun Li and Jumin Zhao and Dengao Li and Hejun Wu and Shuang Xu and Ruiqin Bai},
  doi          = {10.1109/TMC.2025.3562590},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9683-9694},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Salix-leaf: Find main veins of signal clusters for practical parallel decoding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-aware offloading and resource allocation for MEC-empowered AIGC services. <em>TMC</em>, <em>24</em>(10), 9664-9682. (<a href='https://doi.org/10.1109/TMC.2025.3563027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence-Generated Content (AIGC) has emerged as a transformative paradigm, enabling the autonomous creation of diverse content. By offloading model inference tasks to the network edge that is closer to mobile users (MUs), Mobile Edge Computing (MEC) has the potential to significantly enhance the performance of AIGC services. In practice, however, it is challenging to optimally manage MEC-empowered AIGC services, due to the lack of well-defined AIGC-specific metrics, as well as the dynamic workload and computation-intensive nature of AIGC services. In this paper, we first define a novel AIGC metric based on extensive real data experiments, and then study the joint task offloading and resource allocation problem in a generic MEC-empowered AIGC network, where MUs can offload model inference tasks to local or remote Base Stations (BSs), aiming at maximizing their Quality of Experience (QoE). The problem is challenging due to the fast and randomly changing of environments, as well as the necessity for real-time, asynchronous decision-making. To tackle these challenges, we propose two deep reinforcement learning algorithms based on the Proximal Policy Optimization (PPO) framework: Single-Layer PPO (SL-PPO) and Multi-Layer PPO (ML-PPO), designed for slow-changing and fast-changing environments, respectively. In the SL-PPO algorithm, both task offloading and resource allocation decisions are made simultaneously when tasks arrive. In the ML-PPO algorithm, the task offloading decision is made immediately when tasks arrive, while the resource allocation decision is deferred until tasks are scheduled for processing or transmission in the corresponding queues. Simulation results show that (i) both algorithms outperform existing methods in the literature, and can increase the average utility by up to 47% and 48.8%; (ii) both algorithms can effectively manage the trade-off between latency and energy consumption.},
  archive      = {J_TMC},
  author       = {Jiaqi Wu and Xinyi Zhuang and Ming Tang and Lin Gao},
  doi          = {10.1109/TMC.2025.3563027},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9664-9682},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-aware offloading and resource allocation for MEC-empowered AIGC services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Participant recruitment of vehicular crowdsensing along freeways for traffic accident detection. <em>TMC</em>, <em>24</em>(10), 9650-9663. (<a href='https://doi.org/10.1109/TMC.2025.3562565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular crowdsensing provides a new approach for freeway traffic accident detection. However, the uncertainty on traffic accidents and Mobile Users (MUs) brings great challenges for participant recruitment in constructing the deterministic representation of sensing tasks and estimating the participants. To address the challenges, a participant recruitment method for freeway traffic accident detection is proposed. In the method, to deal with the non-deterministic sensing tasks and MUs, the temporal-spatial distribution of accident risk is estimated by optimal transport theory to represent sensing tasks, and the probability distributions of MUs’ trip distance and requested rewards are used to estimate MUs. Then the participant recruitment problem is converted into an optimal coverage problem for accident risk under the macro statistical characteristics of MUs. The participant recruitment model is established to determine the participants by maximizing the coverage rate of accident risk with the budget constraint. And a greedy heuristic strategy is used to solve the model. Simulation experiments are carried out to validate the proposed method. The results show the proposed method is effective and reliable in freeway traffic accident detection.},
  archive      = {J_TMC},
  author       = {Qian Cao and Zhihui Li and Haitao Li and Shirui Zhou and Yunxiang Zhang},
  doi          = {10.1109/TMC.2025.3562565},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9650-9663},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Participant recruitment of vehicular crowdsensing along freeways for traffic accident detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-aware intelligence information sharing requests scheduling in IoV: CPO-based modeling and solution. <em>TMC</em>, <em>24</em>(10), 9636-9649. (<a href='https://doi.org/10.1109/TMC.2025.3565898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the accelerated development of autonomous driving and large language model, blockchain-supported data interaction and artificial intelligence (AI)-assisted performance optimization is the current mainstream research in the Internet of Vehicles (IoV). However, the trial-and-error behavior of the AI algorithm during the training process is a threat to road safety. Therefore, this paper proposes a general constrained policy optimization (CPO)-based modeling and solution for high-dimensional constrained optimization problems. We focus on intelligent driving information sharing in blockchain-enhanced IoV and optimize the service rewards in the sharing requests scheduling problem while ensuring the frequency resource limitation, service quality constraint, and road safety constraint. The constrained state space (CSS) is innovatively proposed to abstract the environment mathematically with the definition of constraint hyperplanes and distance. Accordingly, the constrained Markov Decision process (CMDP) and the optimization problem are formulated. With the practical implementation of the CPO theory, the constrained sharing requests scheduling (CSRS) algorithm is proposed. Ablation experiments are deep reinforcement learning-based methods without using the CSS-based constraint modeling or without using the CPO-based constrained problem solving process. Results show the effectiveness of CSS and CSRS algorithm in improving the policy training efficiency, and the testing results shows excellent generalization ability.},
  archive      = {J_TMC},
  author       = {Yang Gao and Wenjun Wu and Ao Sun and Yang Sun and Teng Sun and Pengbo Si},
  doi          = {10.1109/TMC.2025.3565898},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9636-9649},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-aware intelligence information sharing requests scheduling in IoV: CPO-based modeling and solution},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing edge-cloud collaboration with blockchain-assisted digital twin intelligence offloading scheme. <em>TMC</em>, <em>24</em>(10), 9619-9635. (<a href='https://doi.org/10.1109/TMC.2025.3562189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Edge-Cloud Collaborative (ECC) has emerged as an efficient and promising technique to empower various computation-intensive applications in Digital Twin Network (DTN). The integration of ECC with JointCloud and DTN serves to bridge the gap between data analysis and physical states. In ECC, a reliable and optimal task offloading scheme is required to maximize resource utilization and provide satisfying services to End Users (EU). However, existing offloading schemes still face significant challenges, such as the instability and complexity of network topologies, the intricacies of massive data, and the lack of trust among EU. In this paper, we propose an enhancinG edge-clOud collaboraTion wiTh blockchain-assistEd digital twin intelligence offloadiNg scheme (GOTTEN) which transmits large-scale tasks generated by DTs to Edge Station (ES) or Cloud Station (CS) in dynamic DTN scenarios. We first formulate this resource allocation and task offloading problem and provide an appropriate initial solution which guarantees that tasks generated by DTs can be accurately mapped to physical entities, while optimizing block allocation and reducing the decision space of task offloading. Then, we employ the Lagrange Multiplier based Distributed Island model-enhanced Genetic Algorithm (LM-DIGA) to transform our formulated problem into a convex form and achieve an optimal resource allocation under a specific scheme. Additionally, our proposed architecture also leverages blockchain verification mechanisms to enhance system stability, strengthening privacy protection for DT data as well. Finally, extensive simulation results demonstrate that, compared with seven baselines, our proposed scheme achieves a 10 percent the total system delay and privacy overhead with regard to other schemes in ECC.},
  archive      = {J_TMC},
  author       = {Tianyu Li and Xingwei Wang and Rongfei Zeng and Liang Zhao and Ammar Hawbani and Yuxin Zhang and Min Huang},
  doi          = {10.1109/TMC.2025.3562189},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9619-9635},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing edge-cloud collaboration with blockchain-assisted digital twin intelligence offloading scheme},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedRAV: Hierarchically federated region-learning for traffic object classification of personalized autonomous vehicles with guaranteed efficiency. <em>TMC</em>, <em>24</em>(10), 9599-9618. (<a href='https://doi.org/10.1109/TMC.2025.3564402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging federated learning enables distributed autonomous vehicles to train equipped deep learning models collaboratively without exposing their raw data, providing great potential for utilizing explosively growing autonomous driving data. However, considering the complicated traffic environments and driving scenarios, deploying federated learning for autonomous vehicles is inevitably challenged by non-independent and identically distributed (Non-IID) data of vehicles, which may lead to failed convergence and low training accuracy. In this paper, we propose a novel hierarchically Federated Region-learning framework of Autonomous Vehicles (FedRAV) that adaptively divides a large area containing vehicles into sub-regions based on the defined region-wise distance, and achieves personalized vehicular models and regional models. Specifically, the architecture employs a designated hypernetwork to learn personalized mask vectors per vehicle used in the linear combination of models shared by vehicles in the same region. This approach ensures that the updated vehicular model adopts the beneficial models while discarding the unprofitable ones. We validate our FedRAV framework against existing federated learning algorithms on four real-world autonomous driving datasets in various heterogeneous settings. Extensive experiment results demonstrate that FedRAV framework achieves superior performance than the state-of-the-art algorithms, and improves the accuracy by 9.36%.},
  archive      = {J_TMC},
  author       = {Pengzhan Zhou and Yijun Zhai and Yuepeng He and Fang Qu and Zhida Qin and Xianlong Jiao and Fulin Luo and Chao Chen and Songtao Guo},
  doi          = {10.1109/TMC.2025.3564402},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9599-9618},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedRAV: Hierarchically federated region-learning for traffic object classification of personalized autonomous vehicles with guaranteed efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint positioning and computation offloading in multi-UAV MEC for low latency applications: A proximal policy optimization approach. <em>TMC</em>, <em>24</em>(10), 9584-9598. (<a href='https://doi.org/10.1109/TMC.2025.3562806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) has emerged as a proven solution for reducing communication latency and enhancing user experience in delay-sensitive applications by offloading computation-intensive tasks to edge servers. In future networks, uncrewed aerial vehicles (UAVs), with their flexible deployment and reliable communication capabilities, have the potential to be deployed as aerial MEC servers in areas lacking cellular infrastructure. However, the joint optimization of UAV placement and task offloading poses significant challenges due to the interdependence between communication latency, computational demands, and the resource limitations of UAVs. In this paper, we propose a novel joint optimization framework utilizing proximal policy optimization (PPO) to simultaneously address UAV placement and computation offloading in UAV-enabled MEC networks. The framework dynamically adapts to changing network conditions, minimizing end-to-end latency while balancing computational loads and energy consumption. Extensive simulations demonstrate that the proposed PPO-based approach achieves superior performance compared to conventional optimization methods, with significant improvements in system latency, resource utilization, and network resilience. This work contributes scalable, adaptive solutions for UAV-assisted MEC networks in dynamic environments, enabling robust support for mission-critical and latency-sensitive applications.},
  archive      = {J_TMC},
  author       = {Yuhui Wang and Junaid Farooq and Hakim Ghazzai and Gianluca Setti},
  doi          = {10.1109/TMC.2025.3562806},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9584-9598},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint positioning and computation offloading in multi-UAV MEC for low latency applications: A proximal policy optimization approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HeadMon$^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math>: Domain adaptive head dynamic-based riding maneuver prediction. <em>TMC</em>, <em>24</em>(10), 9570-9583. (<a href='https://doi.org/10.1109/TMC.2025.3562179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-mobility has become a vital means of transportation in recent years, however, it has also resulted in a rise in traffic incidents. Timely tracking and predicting riders’ maneuvers hold the potential to ensure active protection and allow for sufficient time to avert accidents by issuing timely warnings and interventions. We contend that the rider's head dynamics can provide valuable information regarding their subsequent maneuvers. Riders’ traveling habits, however diverse, not to mention the rapidly varying riding environment. The above factors contribute to significant disruptions in the data source, and various micro-mobility forms further exacerbate the issue. We accordingly present HeadMon$^{+}$, which predicts the rider's subsequent maneuver by examining their head dynamics, and it can effectively adapt to various riding conditions and individuals. The system incorporates a deep learning framework with an advanced domain adversarial network. By single-time pre-training, HeadMon$^{+}$ is capable of adapting to new data domains, including human subjects, and riding conditions for robust maneuver prediction. Based on our evaluation, we have found that the maneuver prediction of HeadMon$^{+}$ has an overall precision of 94% with a prediction time gap of 4 seconds. HeadMon$^{+}$'s low cost and rapid response capability make it easily deployed and then contribute to enhancing safe riding.},
  archive      = {J_TMC},
  author       = {Zengyi Han and En Wang and Mohan Yu and Jie Wang and Yuuki Nishiyama and Kaoru Sezaki},
  doi          = {10.1109/TMC.2025.3562179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9570-9583},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HeadMon$^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math>: Domain adaptive head dynamic-based riding maneuver prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tackling resource allocation for decentralized federated learning: A GNN-based approach. <em>TMC</em>, <em>24</em>(10), 9554-9569. (<a href='https://doi.org/10.1109/TMC.2025.3562834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized federated learning (DFL) enables clients to train a neural network model in a device-to-device (D2D) manner without central coordination. In practical systems, DFL faces challenges due to dynamic topology changes, time-varying channel conditions, and limited computational capability of the clients. These factors can affect the learning performance and efficiency of DFL. To address the aforementioned challenges, in this paper, we propose a graph neural network (GNN)–based algorithm to minimize the total delay and energy consumption on training and improve the learning performance of DFL in D2D wireless networks. In our proposed GNN, a multi-head graph attention mechanism is used to capture different features of clients and wireless channels. We design a neighbor selection module which enables each client to select a subset of its neighbors for the participation of model aggregation. We develop a decoder that enables each client to determine its transmit power and computational resource. Experimental results show that our proposed algorithm achieves a lower total delay and energy consumption on training when compared with five baseline schemes. Furthermore, by properly selecting a subset of neighbors for each client, our proposed algorithm achieves similar testing accuracy to the full participation scheme.},
  archive      = {J_TMC},
  author       = {Chuiyang Meng and Ming Tang and Mehdi Setayesh and Vincent W.S. Wong},
  doi          = {10.1109/TMC.2025.3562834},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9554-9569},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Tackling resource allocation for decentralized federated learning: A GNN-based approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of beamforming and trajectory for UAV-RIS-assisted MU-MISO systems using GNN and SD3. <em>TMC</em>, <em>24</em>(10), 9539-9553. (<a href='https://doi.org/10.1109/TMC.2025.3563072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In urban environments, direct communication links between a base station (BS) and user equipment (UEs) are often obstructed by buildings. To mitigate these blockages, we integrate uncrewed aerial vehicles (UAVs) and reconfigurable intelligent surfaces (RISs) to enhance system flexibility and improve transmission efficiency. This paper investigates an RIS-assisted multi-user multiple-input single-output (MU-MISO) downlink system, where the RIS is mounted on a UAV. To maximize the system rate while minimizing the UAV’s energy consumption and flight duration, we formulate a multi-objective optimization problem. To address this problem, we propose a hybrid algorithm that integrates the soft deep deterministic policy gradient (SD3) algorithm with a graph neural network (GNN) architecture, named SD3-GNN-RIS. The original problem is decomposed into two subproblems: joint active beamforming at the BS and passive beamforming at the RIS, optimized via a GNN-based approach, and three-dimensional (3D) UAV trajectory optimization, formulated as a Markov decision process and solved using the SD3 algorithm. Simulation results demonstrate the superior performance of the proposed algorithm compared to baseline methods in terms of system rate, energy efficiency, and UAV trajectory optimization.},
  archive      = {J_TMC},
  author       = {Shumo Wang and Xiaoqin Song and Tiecheng Song and Yang Yang},
  doi          = {10.1109/TMC.2025.3563072},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9539-9553},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of beamforming and trajectory for UAV-RIS-assisted MU-MISO systems using GNN and SD3},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiCG: In-body cardiac motion sensing based on a mix-medium wi-fi fresnel zone model. <em>TMC</em>, <em>24</em>(10), 9524-9538. (<a href='https://doi.org/10.1109/TMC.2025.3564843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) are a leading cause of mortality worldwide, highlighting the critical need for accurate and continuous heart health monitoring. Electrocardiograms (ECG), considered as the golden standard for diagnosing and monitoring heart-related conditions, offer precise measurements but require direct skin contact, limiting their practicality for long-term and everyday use. On the other hand, existing RF sensing techniques that analyze signals reflected off the skin struggle to distinguish micro cardiac motions of the heart due to weak motion amplitude and respiration interference at the chest wall. To overcome these limitations, we introduce WiCG, a novel contact-less cardiac motion monitoring system that employs 2.4 GHz Wi-Fi signals to penetrate the chest and detect subtle cardiac movements. A mix-medium Wi-Fi Fresnel zone model is developed to explain the enhanced phase sensitivity of in-body Wi-Fi signals, which is crucial for accurately detecting cardiac motions. By strategically positioning antennas near the heart, WiCG captures ventricular motions effectively. A novel cardiac Doppler method is proposed to suppress phase noise and interference from static paths and extract the time interval between the systole and diastole of the ventricular. Extensive experiments demonstrate that the proposed system can robustly estimate the R-R and Q-T intervals of human cardiac cycles across 21 subjects and different environments with an average accuracy of 99.22% and 92.8%, achieving performance comparable to ECG.},
  archive      = {J_TMC},
  author       = {Pei Wang and Anlan Yu and Xujun Ma and Rong Zheng and Jingfu Dong and Zhaoxin Chang and Duo Zhang and Djamal Zeghlache and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3564843},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9524-9538},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiCG: In-body cardiac motion sensing based on a mix-medium wi-fi fresnel zone model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiCast: Parallel cross-technology transmission for connecting heterogeneous IoT devices. <em>TMC</em>, <em>24</em>(10), 9506-9523. (<a href='https://doi.org/10.1109/TMC.2025.3564340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Technology Communication (CTC) is an emerging technique that enables direct interconnection among incompatible wireless technologies. However, for the downlink from WiFi to multiple IoT technologies, serially emulating and transmitting the data of each IoT technology has extremely low spectrum efficiency. In this paper, we propose WiCast, a parallel CTC that uses IEEE 802.11ax to emulate a composite signal that can be received by commodity BLE, ZigBee, and LoRa devices. By taking advantage of OFDMA in 802.11ax, WiCast uses a single Resource Unit (RU) for parallel CTC and sets other RUs free for high-rate WiFi users. But such a sophisticated composite signal is very easily distorted by emulation imperfections, dynamic channel noises, cyclic prefix, and center frequency offset. We propose a CTC link model that jointly models the emulation errors and channel distortions. Then we carve the emulated signal with elaborate compensations in both time and frequency domains. Based on the proposed CTC scheme, a unified Media Access Control approach is introduced to discover and synchronize the heterogeneous IoT devices. We implement a prototype of WiCast using USRP N210 platform along with commodity ZigBee, BLE, and LoRa devices. The extensive experiments demonstrate WiCast can achieve an efficient parallel transmission with the aggregated goodput up to $ 390.24 \;\text{kbps}$.},
  archive      = {J_TMC},
  author       = {Dan Xia and Xiaolong Zheng and Liang Liu and Shanguo Huang and Huadong Ma},
  doi          = {10.1109/TMC.2025.3564340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9506-9523},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiCast: Parallel cross-technology transmission for connecting heterogeneous IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the robustness: Hierarchical federated learning framework for object detection of UAV cluster. <em>TMC</em>, <em>24</em>(10), 9489-9505. (<a href='https://doi.org/10.1109/TMC.2025.3562812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of Unmanned Aerial Vehicle (UAV) cluster is an available solution for object detection missions. In the harsh environment, UAV cluster could suffer from some significant threats (e.g., forest fire hazards, electromagnetic interference, and ground-to-air attacks), which could lead to the destruction of UAVs and loss of data. To this end, we propose a Hierarchical Federated Learning Framework for Object Detection (HFL-OD) to enhance the robustness of UAV cluster conducting object detection missions. In HFL-OD, UAVs are grouped through a Three-Dimensional (3D) graph coloring method, and an intragroup backup mechanism is provided to prevent the data loss caused by the destruction of UAVs. Besides, a dynamic server selection mechanism deals with the potential destruction of servers (cluster server and group servers) by adaptively reassigning the server roles. To further improve the robustness and mission efficiency of UAV cluster, a two-tier federated learning framework is introduced to make a proper trade-off between object detection accuracy and communication/computational overhead. This framework is built on the concept of hierarchical federated learning by implementing both intragroup parameter aggregation and global parameter aggregation. Extensive simulations and comparisons demonstrate the superior performance of our proposed HFL-OD, i.e., the robustness of UAV cluster conducting object detection missions can be significantly improved, and the communication/computational overhead is effectively reduced.},
  archive      = {J_TMC},
  author       = {Xingyu Li and Wenzhe Zhang and Linfeng Liu and Jia Xu},
  doi          = {10.1109/TMC.2025.3562812},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9489-9505},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring the robustness: Hierarchical federated learning framework for object detection of UAV cluster},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SWIPTNet: A unified deep learning framework for SWIPT based on GNN and transfer learning. <em>TMC</em>, <em>24</em>(10), 9477-9488. (<a href='https://doi.org/10.1109/TMC.2025.3563892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the deep learning based approaches for simultaneous wireless information and power transfer (SWIPT). The quality-of-service (QoS) constrained sum-rate maximization problems are, respectively, formulated for power-splitting (PS) receivers and time-switching (TS) receivers and solved by a unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet). To improve the performance of SWIPTNet, we first propose a single-type output method to reduce the learning complexity and facilitate the satisfaction of QoS constraints, and then, utilize the Laplace transform to enhance input features with the structural information. Besides, we adopt the multi-head attention and layer connection to enhance feature extracting. Furthermore, we present the implementation of transfer learning to the SWIPTNet between PS and TS receivers. Ablation studies show the effectiveness of key components in the SWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in achieving near-optimal performance with millisecond-level inference speed which is much faster than the traditional optimization algorithms. We also show the effectiveness of transfer learning via fast convergence and expressive capability improvement.},
  archive      = {J_TMC},
  author       = {Hong Han and Yang Lu and Zihan Song and Ruichen Zhang and Wei Chen and Bo Ai and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3563892},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9477-9488},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SWIPTNet: A unified deep learning framework for SWIPT based on GNN and transfer learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EchoExpress: Facial expression recognition in the wild via acoustic sensing on smart glasses. <em>TMC</em>, <em>24</em>(10), 9458-9476. (<a href='https://doi.org/10.1109/TMC.2025.3566341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately recognizing facial expressions and emotions at any time and in any place can significantly improve people’s quality of life and mental well-being. However, existing methods lack the convenient capability for long-term monitoring in the wild environment. In this paper, we introduce EchoExpress, an in-the-wild emotion-related facial expression recognition system that works in an unobtrusive, low-power, and privacy-friendly way. EchoExpress uses two speakers and two microphones mounted on a glass-frame for transmitting and receiving mutually orthogonal wave signals. Concurrently, a unique attention mechanism dynamically extracts crucial features, enabling the capture of nuanced facial expressions and emotions. Furthermore, we introduce an open-set filtering mechanism with a specially designed loss function, which effectively filters out irrelevant actions, thereby reducing the risk of misidentification. Finally, a semi-supervised training method is employed to address the significant variability in wild expressions across different individuals. In extensive testing, EchoExpress achieves an accuracy of 84% in a laboratory environment and over 75% in real-world conditions. We believe that EchoExpress can serve as an unobtrusive and reliable way to monitor facial expressions.},
  archive      = {J_TMC},
  author       = {Kaiyi Guo and Qian Zhang and Dong Wang},
  doi          = {10.1109/TMC.2025.3566341},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9458-9476},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EchoExpress: Facial expression recognition in the wild via acoustic sensing on smart glasses},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AegisRAN: A fair and energy-efficient computing resource allocation framework for vRANs. <em>TMC</em>, <em>24</em>(10), 9441-9457. (<a href='https://doi.org/10.1109/TMC.2025.3564116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The virtualization of Radio Access Networks (vRAN) is rapidly becoming a reality, driven by the increasing need for flexible, scalable, and cost-effective mobile network solutions. To mitigate energy efficiency concerns in vRAN deployments, two approaches are gaining attention: ($i$) sharing computing infrastructure among multiple virtualized base stations (vBSs); and ($ii$) relying upon general-purpose, low-cost CPUs. However, effectively realizing these approaches poses several challenges. In this paper, we first conduct a comprehensive experimental campaign on a vRAN platform to characterize the impact of computing and radio resource allocation on energy consumption and performance across various network contexts. This analysis reveals several key issues. First, determining the optimal allocation of computing resources is difficult because it depends on the context of each vBS (e.g., traffic load, channel quality) in a non-trivial and non-linear manner. Second, suboptimal resource assignment can lead to increased energy consumption or, even worse, degradation of users’ Quality of Service. Third, the high dimensionality of the solution space hinders the effectiveness of traditional optimization or learning methods. To tackle these challenges, we propose AegisRAN, a framework for optimizing computing resource allocation in vRAN. AegisRAN addresses the dual objective of minimizing energy consumption while maintaining high system reliability. Moreover, when computing resources are overbooked, our solution ensures a fair resource partition based on vBS performance. AegisRAN leverages a discrete soft actor-critic algorithm combined with several techniques, including multi-step decision-making, action masking, digital twin-based training, and a tailored reward signal that mitigates feedback sparsity. Our evaluations demonstrate that AegisRAN achieves near-optimal performance and offers high flexibility across diverse network contexts and varying numbers of vBSs, with up to 25% improvement in energy savings compared to baseline solutions in medium-scale scenarios.},
  archive      = {J_TMC},
  author       = {Ethan Sanchez Hidalgo and Jose A. Ayala-Romero and Josep Xavier Salvat Lozano and Andres Garcia-Saavedra and Xavier Costa Perez},
  doi          = {10.1109/TMC.2025.3564116},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9441-9457},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AegisRAN: A fair and energy-efficient computing resource allocation framework for vRANs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REPLAY: Modeling time-varying temporal regularities of human mobility for location prediction over sparse trajectories. <em>TMC</em>, <em>24</em>(10), 9428-9440. (<a href='https://doi.org/10.1109/TMC.2025.3562669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-location prediction aims to forecast which location a user is most likely to visit given the user’s historical data. As a sequence modeling problem by nature, it has been widely addressed using Recurrent Neural Networks (RNNs). To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by integrating them into the RNN units as additional information, or utilizing them to search for informative historical hidden states to improve prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other time periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Under this circumstance, we propose REPLAY, learning to capture the time-varying temporal regularities for location prediction based on general RNN architecture. Specifically, REPLAY is designed on top of a flashback mechanism, where the spatiotemporal distances in sparse trajectories are used to search for the informative past hidden states; to accommodate the time-varying temporal regularities, REPLAY incorporates smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. We conduct a comprehensive evaluation, comparing REPLAY against a wide range of state-of-the-art methods. Experimental results show REPLAY significantly and consistently outperforms state-of-the-art methods by 7.7%–10.5% in the location prediction task, and the learnt bandwidths reveal interesting patterns of the time-varying temporal regularities.},
  archive      = {J_TMC},
  author       = {Bangchao Deng and Bingqing Qu and Pengyang Wang and Dingqi Yang and Benjamin Fankhauser and Philippe Cudre-Mauroux},
  doi          = {10.1109/TMC.2025.3562669},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9428-9440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REPLAY: Modeling time-varying temporal regularities of human mobility for location prediction over sparse trajectories},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-enhanced representation learning for road networks with temporal dynamics. <em>TMC</em>, <em>24</em>(10), 9413-9427. (<a href='https://doi.org/10.1109/TMC.2025.3562656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of mobile devices and positioning technology has resulted in the generation of massive urban data, offering great opportunities to improve analytical abilities for urban infrastructure components. In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage mobile trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.},
  archive      = {J_TMC},
  author       = {Yile Chen and Xiucheng Li and Gao Cong and Zhifeng Bao and Cheng Long},
  doi          = {10.1109/TMC.2025.3562656},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9413-9427},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semantic-enhanced representation learning for road networks with temporal dynamics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dynamic scaling and request routing optimization in the multi-edge cluster collaboration. <em>TMC</em>, <em>24</em>(10), 9395-9412. (<a href='https://doi.org/10.1109/TMC.2025.3562209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of mobile devices, a growing number of intelligent applications are being deployed at the network edge, placing immense strain on the processing capabilities of edge computing. Therefore, resource-constrained edge servers frequently experience overload due to highly dynamic workloads. To address this, one approach involves forwarding user requests to the cloud or other edge servers, albeit at the cost of increased transmission latency. Alternatively, dynamic scaling of edge clusters can be employed to enhance processing capacity, thereby mitigating latency but at the expense of additional service configuration and hosting expenses. By integrating their complementary benefits, we study the joint optimization problem of dynamic scaling and request routing within a multi-edge cluster collaborative framework, which fully exploits cluster resources to manage the temporal and spatial varying edge workloads. This collaborative framework aims to minimize overall request latency while satisfying an acceptable time-averaged budget cost. However, the complex coupling between scaling and routing decisions, along with the uncertainty of future system information (e.g., user request workloads) impedes the derivation of an optimal offline policy over the long term. Thus, considering the different decision granularities, we employ the two-timescale Lyapunov optimization technique to decouple the original problem into a series of independent online optimization problems with the current system state. In particular, we make cluster scaling decisions in each large timescale and request routing decisions in each small timescale. Given that the decoupled large-timescale subproblems involve NP-hard mixed-integer linear programming, we design an edge resource-aware greedy rounding algorithm to efficiently produce approximate optimal solutions. Finally, both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the superiority of our proposed algorithm over its counterparts.},
  archive      = {J_TMC},
  author       = {Pu Wang and Tao Ouyang and Jie Gong and Chao Hong and Xu Chen},
  doi          = {10.1109/TMC.2025.3562209},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9395-9412},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive dynamic scaling and request routing optimization in the multi-edge cluster collaboration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SignCRF: Scalable channel-agnostic data-driven radio authentication system. <em>TMC</em>, <em>24</em>(10), 9383-9394. (<a href='https://doi.org/10.1109/TMC.2025.3564556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (“fingerprint”) the device itself based on variations introduced in the transmitted waveform. Key impediments in developing robust and scalable Radio Frequency Fingerprinting through Deep Learning (RFFDL) techniques that are practical in dynamic and mobile environments are the non-stationary behavior of the wireless channel and other impairments introduced by the propagation conditions. To date, the existing RFFDL-based techniques have only been able to demonstrate a desirable performance when the training and testing environment remains the same, which makes the solutions impractical. SignCRF brings to the RFFDL landscape what it has been missing so far: a scalable, channel-agnostic data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments that is independent of the dynamic nature of the environment or channel irregularities caused by mobility. SignCRF consists of: (i) a classifier developed in a base-environment with minimum channel dynamics, and finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator that is carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific “signature”; and (iii) a Max Rule module that selects the highest precision authentication technique between the baseline classifier and the environment translator per radio. We design, train, and validate the performance of SignCRF for multiple technologies in dynamic environments and at scale (100 LoRa and 20 WiFi devices, the largest datasets available in the literature). We assess the scalability of SignCRF across various testbed scales by validating our system using small, medium, and large-scale testbeds, with sizes of 5, 20, and 100 devices, respectively. We demonstrate that SignCRF can significantly improve the RFFDL performance by achieving as high as 100% correct authentication for WiFi devices and 80% correctly authenticated LoRa devices, a 5x and 8x improvement when compared to the state-of-the-art respectively. Furthermore, we show that SignCRF is resilient to adversarial actions by reducing the device recognition accuracy from 73% to 6%, which translates into zero mis-authentication of adversary radios that try to impersonate legitimate devices, which has not been achieved by any prior RFFDL techniques.},
  archive      = {J_TMC},
  author       = {Amani Al-Shawabka and Philip Pietraski and Sudhir B Pattar and Pedram Johari and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3564556},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9383-9394},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SignCRF: Scalable channel-agnostic data-driven radio authentication system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BAT: A versatile bipartite attention-based approach for comprehensive truth inference in mobile crowdsourcing. <em>TMC</em>, <em>24</em>(10), 9368-9382. (<a href='https://doi.org/10.1109/TMC.2025.3563345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of smart mobile devices has catalyzed the growth of Mobile CrowdSourcing (MCS) as a distributed problem-solving paradigm. MCS platforms heavily rely on advanced truth inference techniques to extract reliable information from diverse and potentially noisy crowd-contributed data. Existing truth inference models often made simplified assumptions about workers or tasks, employing complex Bayesian models or stringent data aggregation methods. These approaches tend to be task-specific, primarily limited to categorical labeling, making adaptations to other mobile computing scenarios labor-intensive. To address these limitations, we introduce the Bipartite Attention-driven Truth (BAT), a versatile approach tailored for mobile computing environments. BAT utilizes an Attributed Bipartite Graph (ABG) to holistically model the MCS process, with workers and tasks as nodes connected by edges representing answer-specific attributes. The approach employs a bipartite graph neural network with an innovative attention mechanism to assess the importance of different answers. BAT extends beyond categorical tasks to support numerical ones by incorporating novel feature representations and model extensions. Theoretical analyses clarify the link between answer similarity and worker expertise. Extensive experiments using diverse real-world datasets demonstrate BAT's superior performance compared to state-of-the-art categorical and numerical truth inference models, highlighting its effectiveness in mobile computing scenarios.},
  archive      = {J_TMC},
  author       = {Jiacheng Liu and Feilong Tang and Hao Liu and Long Chen and Yichuan Yu and Yanmin Zhu and Jiadi Yu and Xiaofeng Hou and Pheng-Ann Heng},
  doi          = {10.1109/TMC.2025.3563345},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9368-9382},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BAT: A versatile bipartite attention-based approach for comprehensive truth inference in mobile crowdsourcing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical split federated learning: Convergence analysis and system optimization. <em>TMC</em>, <em>24</em>(10), 9352-9367. (<a href='https://doi.org/10.1109/TMC.2025.3565509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI models expand in size, it has become increasingly challenging to deploy federated learning (FL) on resource-constrained edge devices. To tackle this issue, split federated learning (SFL) has emerged as an FL framework with reduced workload on edge devices via model splitting; it has received extensive attention from the research community in recent years. Nevertheless, most prior works on SFL focus only on a two-tier architecture without harnessing multi-tier cloud-edge computing resources. In this paper, we intend to analyze and optimize the learning performance of SFL under multi-tier systems. Specifically, we propose the hierarchical SFL (HSFL) framework and derive its convergence bound. Based on the theoretical results, we formulate a joint optimization problem for model splitting (MS) and model aggregation (MA). To solve this rather hard problem, we then decompose it into MS and MA sub-problems that can be solved via an iterative descending algorithm. Simulation results demonstrate that the tailored algorithm can effectively optimize MS and MA in multi-tier systems and significantly outperform existing schemes.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Wei Wei and Zhe Chen and Chan-Tong Lam and Xianhao Chen and Yue Gao and Jun Luo},
  doi          = {10.1109/TMC.2025.3565509},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9352-9367},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical split federated learning: Convergence analysis and system optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IvyCross: A privacy-preserving and concurrency control framework for blockchain interoperability. <em>TMC</em>, <em>24</em>(10), 9334-9351. (<a href='https://doi.org/10.1109/TMC.2025.3562875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interoperability is a fundamental challenge for long-envisioned blockchain applications. A mainstream approach is using Trusted Execution Environment (TEE) to support interoperable off-chain execution. However, this incurs multiple TEE configured with non-trivial storage capabilities running on fragile concurrent processing environments, rendering current strategies based on TEE far from being practical. This paper aims to fill this gap and design a practical interoperability mechanism with simplified TEE as the underlying architecture. Specifically, we present IvyCross, a TEE-based framework that achieves low-cost, privacy-preserving, and race-free blockchain interoperability. IvyCross allows running arbitrary smart contracts across heterogeneous blockchains atop two distributed TEE-powered hosts. We design an incentive scheme based on smart contracts to stimulate the honest behavior of two hosts, bypassing the requirement of the number of TEE and large memory need. We examine the conditions to guarantee the uniqueness of Nash Equilibrium via Game Theory. Furthermore, an extended optimistic concurrency control protocol is designed to ensure the correctness of concurrent contracts execution. We formally prove the security of IvyCross in the Universal Composability (UC) framework and implement a prototype atop Bitcoin, Ethereum, and FISCO BOCS. Extensive experimental results on end-to-end performance and concurrency control demonstrate the efficiency and practicality of IvyCross.},
  archive      = {J_TMC},
  author       = {Ming Li and Jian Weng and Jiasi Weng and Yi Li and Yongdong Wu and Dingcheng Li and Guowen Xu and Robert H. Deng},
  doi          = {10.1109/TMC.2025.3562875},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9334-9351},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IvyCross: A privacy-preserving and concurrency control framework for blockchain interoperability},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergy: Towards on-body AI via tiny AI accelerator collaboration on wearables. <em>TMC</em>, <em>24</em>(10), 9319-9333. (<a href='https://doi.org/10.1109/TMC.2025.3564314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0× improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.},
  archive      = {J_TMC},
  author       = {Taesik Gong and SiYoung Jang and Utku Günay Acer and Fahim Kawsar and Chulhong Min},
  doi          = {10.1109/TMC.2025.3564314},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9319-9333},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Synergy: Towards on-body AI via tiny AI accelerator collaboration on wearables},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPViT: Accelerate vision transformer inference on mobile devices via adaptive splitting and offloading. <em>TMC</em>, <em>24</em>(10), 9303-9318. (<a href='https://doi.org/10.1109/TMC.2025.3562721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vision Transformer (ViT), which benefits from utilizing self-attention mechanisms, has demonstrated superior accuracy compared to CNNs. However, due to the expensive computational costs, deploying and inferring ViTs on resource-constrained mobile devices has become a challenge. To resolve this challenge, we conducted an empirical analysis to identify performance bottlenecks in deploying ViTs on mobile devices and explored viable solutions. In this paper, we propose SPViT, an adaptive split and offloading method that accelerates ViT inference on mobile devices. SPViT executes collaborative inference of ViT across available edge devices. We introduce a fine-grained splitting technique for the vision transformer structure. Furthermore, we propose an algorithm based on the Auto Regression model to predict partition latency and adaptive offload partitions. Finally, we design offline and online optimization methods to minimize the computational and communication overhead on each device. Based on real-world prototype experiments, SPViT effectively reduces inference latency by 2.2x to 3.3x across four state-of-the-art models.},
  archive      = {J_TMC},
  author       = {Sifan Zhao and Tongtong Liu and Hai Jin and Dezhong Yao},
  doi          = {10.1109/TMC.2025.3562721},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9303-9318},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SPViT: Accelerate vision transformer inference on mobile devices via adaptive splitting and offloading},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for task offloading in crowd-edge computing. <em>TMC</em>, <em>24</em>(10), 9289-9302. (<a href='https://doi.org/10.1109/TMC.2025.3531793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Crowd-edge (CE) computing paradigm facilitates the utilization of the computational resources through simultaneously relying the edge computing and the collaboration among various mobile devices (MDs). Most existing works, focusing on offloading tasks from device to edge servers by centralized solutions, are unable to distribute tasks to massive MDs in CE. Meanwhile, designing a decentralized task offloading solution enabling task subscribers to individually make offloading decisions can be challenging given the randomness of crowd resource provisioning and limited knowledge of global status variations. In this paper, we propose a decentralized crowd-edge task offloading solution that enables users to optimally offload tasks to the CE in a distributed manner. Specifically, we formulate the corresponding problem as a stochastic optimization with partially observable status. By observing network and process delays at the crowd side, we further reform the optimization forms and provide a novel approximation policy, enabling users to optimize their offloading strategy based on local observations without interaction with each other. We then solve this task offloading problem by developing a Mixed Multi-Agent Proxy Policy Optimization algorithm (mixed MAPPO). Extensive testing, including numerical and system-level simulations, was conducted to validate the performance of the proposed algorithm in terms of task delay (including the processing delay and transmission delay), load rate, and resource utilization.},
  archive      = {J_TMC},
  author       = {Su Yao and Mu Wang and Ju Ren and Tianyu Xia and Weiqiang Wang and Ke Xu and Mingwei Xu and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3531793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9289-9302},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent reinforcement learning for task offloading in crowd-edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
