<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 270</h2>
<ul>
<li><details>
<summary>
(2025). Exponential dissimilarity-dispersion family for domain-specific representation learning. <em>TIP</em>, <em>34</em>, 6110-6125. (<a href='https://doi.org/10.1109/TIP.2025.3608661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new domain-specific representation learning method, exponential dissimilarity-dispersion family (EDDF), a novel distribution family that includes a dissimilarity function and a global dispersion parameter. In generative models, variational autoencoders (VAEs) has a solid theoretical foundation based on variational inference in visual representation learning and are also used as one of core components of other generative models. This paper addresses the issue where conventional VAEs, with the commonly adopted Gaussian settings, tend to experience performance degradation in generative modeling for high-dimensional data. This degradation is often caused by their excessively limited model family. To tackle this problem, we propose EDDF, a new domain-specific method introducing a novel distribution family with a dissimilarity function and a global dispersion parameter. A decoder using this family employs dissimilarity functions for the evidence lower bound (ELBO) reconstruction loss, leveraging domain-specific knowledge to enhance high-dimensional data modeling. We also propose an ELBO optimization method for VAEs with EDDF decoders that implicitly approximates the stochastic gradient of the normalizing constant using log-expected dissimilarity. Empirical evaluations of the generative performance show the effectiveness of our model family and proposed method. Our framework can be integrated into any VAE-based generative models in representation learning. The code and model are available at https://github.com/ganmodokix/eddf-vae},
  archive      = {J_TIP},
  author       = {Ren Togo and Nao Nakagawa and Takahiro Ogawa and Miki Haseyama},
  doi          = {10.1109/TIP.2025.3608661},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6110-6125},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exponential dissimilarity-dispersion family for domain-specific representation learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptually-guided VR style transfer. <em>TIP</em>, <em>34</em>, 6083-6097. (<a href='https://doi.org/10.1109/TIP.2025.3607611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) makes it possible to provide immersive multimedia content composed of omnidirectional videos (ODVs). Towards enabling more immersive and satisfying VR content, methods are needed to manipulate VR scenes, taking into account perceptual factors related to viewers’ quality of experience (QoE). For example, style transfer methods can be applied to VR content, allowing users to create artistic or surreal effects in their immersive environments. Here, we study perceptual factors that affect the sensation of stylized immersiveness, including color dynamics and spatio-temporal consistency. To do this, we introduce an immersiveness sensitivity model of luminance and color perception, and use it to measure the color dynamics and spatio-temporal consistency of stylized VR contents. We subsequently use this model to construct a perceptually-guided VR style transfer model called VR Style Transfer GAN (VRST-GAN). VRST-GAN learns to transfer a desired style into VR to enhance immersiveness by considering color dynamics while preserving spatio-temporal consistency. We demonstrate the effectiveness of VRST-GAN via qualitative and quantitative experiments. We also develop a VR Immersiveness Predictor (VR-IP) that is able to predict the sensation of immersiveness using the perceptual model. In our experiments, VR-IP predicts immersiveness with an accuracy of 91%.},
  archive      = {J_TIP},
  author       = {Seonghwa Choi and Jungwoo Huh and Sanghoon Lee and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2025.3607611},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6083-6097},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptually-guided VR style transfer},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUNTNet: Homomorphic unified nexus topology for camouflaged object detection. <em>TIP</em>, <em>34</em>, 6068-6082. (<a href='https://doi.org/10.1109/TIP.2025.3607635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is challenging for both human and computer vision, as targets often blend into the background by sharing similar color, texture, or shape. While many feature enhancement techniques exist, single-view methods tend to overemphasize certain Recognizing that camouflaged objects exhibit different concealment strategies under varying observational perspectives, we propose HUNTNet, a network that establishes a dynamic detection mechanism to decouple target features from RGB images and perform topological decamouflage across multiple homomorphic feature spaces through a unified feature focusing architecture. We adopt PVTv2 as the backbone to extract multi-perspective spatial features. Detail representation is enhanced via a feature module that integrates Dual-Channel Recursive (DCR), Wavelet-Gabor Transform (WGT), and Anisotropic Gradient Responding (AGR), which together improve boundary discrimination and edge contour detection. To further boost performance, the Simplicial Feature Integration (SFI) module recursively fuses multi-layer features, enabling high-resolution focus on target regions. Experiments show that HUNTNet surpasses state-of-the-art methods in both accuracy and generalization, offering a robust solution for COD and improving segmentation in complex scenes. Our code is available at https://github.com/HaolinJi817/HUNTNet},
  archive      = {J_TIP},
  author       = {Haolin Ji and Fengying Xie and Linpeng Pan and Yushan Zheng and Zhenwei Shi},
  doi          = {10.1109/TIP.2025.3607635},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6068-6082},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HUNTNet: Homomorphic unified nexus topology for camouflaged object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial clustering guided two-view multi-structural deterministic geometric model fitting. <em>TIP</em>, <em>34</em>, 6016-6028. (<a href='https://doi.org/10.1109/TIP.2025.3610248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the two-view geometric model fitting problem on the multi-structural data with severe outliers for providing reliable and consistent fitting results. The key idea is to adopt spatial clustering to guide deterministically sample minimum subsets. Specifically, we firstly improve the effectiveness of spatial clustering with good neighbors that preserve the consensus of neighborhood elements and neighborhood topology, for enhancing the quality of sampled minimum subsets. Then we further design a multi-scale fusion strategy, which not only boosts more high-quality minimum subsets, but also enables our method to cover all model instances in data. Moreover, we propose a simple and effective model selection algorithm to estimate the parameters of model instances in data. The final proposed method is able to guarantee fast, accurate and stable model fitting results for the multi-structural data. In addition, we construct two large labeled datasets, for homography and fundamental matrix estimation, respectively. Experimental results on real images from six datasets show the significant superiority of the proposed method on both accuracy and speed over several state-of-the-art alternatives. Especially for the MS-COCO-F and YFCC100M-F datasets, the proposed method yields a performance boost of over three times on segmentation error, parameter error and the CPU time.},
  archive      = {J_TIP},
  author       = {Guobao Xiao},
  doi          = {10.1109/TIP.2025.3610248},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6016-6028},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial clustering guided two-view multi-structural deterministic geometric model fitting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COLA: Context-aware language-driven test-time adaptation. <em>TIP</em>, <em>34</em>, 6002-6015. (<a href='https://doi.org/10.1109/TIP.2025.3607634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing “distribution shift” issue while simultaneously protecting data privacy. However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability. In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels. This is achieved by using a pre-trained vision-language model (VLM), e.g., CLIP, that can recognize images through matching with class descriptions. While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain. To that end, we propose a novel method – Context-aware Language-driven TTA (COLA). The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively. It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency. Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance. We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks. The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA},
  archive      = {J_TIP},
  author       = {Aiming Zhang and Tianyuan Yu and Liang Bai and Jun Tang and Yanming Guo and Yirun Ruan and Yun Zhou and Zhihe Lu},
  doi          = {10.1109/TIP.2025.3607634},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6002-6015},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {COLA: Context-aware language-driven test-time adaptation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSP: Multimodal self-attention prompt learning. <em>TIP</em>, <em>34</em>, 5978-5988. (<a href='https://doi.org/10.1109/TIP.2025.3607613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal prompt learning has emerged as an effective strategy for adapting vision-language models such as CLIP to downstream tasks. However, conventional approaches typically operate at the input level, forcing learned prompts to propagate through a sequence of frozen Transformer layers. This indirect adaptation introduces cumulative geometric distortions, a limitation that we formalize as the indirect learning dilemma (ILD), leading to overfitting of the base class and reduced generalization to novel classes. To overcome this challenge, we propose the Multimodal Self-Attention Prompt (MSP) framework, which shifts adaptation into the semantic core of the model by injecting learnable prompts directly into the key and value sequences of attention blocks. This direct modulation preserves the pretrained embedding geometry while enabling more precise downstream adaptation. MSP further incorporates distance-aware optimization to maintain semantic consistency with CLIP’s original representation space, and partial prompt learning via stochastic dimension masking to improve robustness and prevent over-specialization. Extensive evaluations across 11 benchmarks demonstrate the effectiveness of MSP. It achieves a state-of-the-art harmonic mean accuracy of 80.67%, with 77.32% accuracy on novel classes—representing a 2.18% absolute improvement over prior methods—while requiring only 0.11M learnable parameters. Notably, MSP surpasses CLIP’s zero-shot performance on 10 out of 11 datasets, establishing a new paradigm for efficient and generalizable prompt-based adaptation. Our implementation is available at https://github.com/laixinyi023/Multimodal-Self-Attention-Prompt},
  archive      = {J_TIP},
  author       = {Xinyi Lai and Xiao Ke and Huangbiao Xu and Shanghui Wu and Wenzhong Guo},
  doi          = {10.1109/TIP.2025.3607613},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5978-5988},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSP: Multimodal self-attention prompt learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion and appearance decoupling representation for event cameras. <em>TIP</em>, <em>34</em>, 5964-5977. (<a href='https://doi.org/10.1109/TIP.2025.3607632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, with high temporal resolution and high dynamic range, have shown great potential under extreme scenarios such as high-speed movement and low illumination. However, previous event representation methods typically aggregate event data into a single dense tensor, often overlooking the dynamic changes of events within a given time unit. This limitation can introduce historical artifacts and semantic inconsistencies, ultimately degrading model performance. Inspired by human visual prior, we propose a motion and appearance decoupling (MAD) event representation to disentangle the mixed spatial-temporal event tensor into two independent branches. This bio-inspired design helps the network extract discriminative temporal (i.e., motion) and spatial (i.e., appearance) information, thus reducing the network’s learning burden toward complex high-level interpretation tasks. In our method, the event motion guided attention module (EMGA) is designed to achieve temporal and spatial feature interaction and fusion sequentially. Based on EMGA, three specially designed decoder heads are proposed for several representative event-based tasks (i.e., object detection, semantic segmentation, and human pose estimation). Experimental results demonstrate that our method achieves state-of-the-art performance on the above three tasks, which reveals that our method is an easy-to-implement replacement for currently event-based methods. Our code is available at: https://github.com/ChenYichen9527/MAD-representation},
  archive      = {J_TIP},
  author       = {Nuo Chen and Boyang Li and Yingqian Wang and Xinyi Ying and Longguang Wang and Chushu Zhang and Yulan Guo and Miao Li and Wei An},
  doi          = {10.1109/TIP.2025.3607632},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5964-5977},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion and appearance decoupling representation for event cameras},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free object detection with detection transformer. <em>TIP</em>, <em>34</em>, 5948-5963. (<a href='https://doi.org/10.1109/TIP.2025.3607621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: 1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; 2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; 3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and 4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pre-trained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.},
  archive      = {J_TIP},
  author       = {Huizai Yao and Sicheng Zhao and Shuo Lu and Hui Chen and Yangyang Li and Guoping Liu and Tengfei Xing and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi          = {10.1109/TIP.2025.3607621},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5948-5963},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Source-free object detection with detection transformer},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prospective layout-guided multi-modal online hashing. <em>TIP</em>, <em>34</em>, 5935-5947. (<a href='https://doi.org/10.1109/TIP.2025.3607626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, the data usually appears in a streaming fashion. To achieve remarkable retrieval performance in such scenarios, online multi-modal hashing has drawn great research attention due to its high retrieval speed and low storage cost. However, existing online multi-modal hashing methods still fail to achieve satisfactory retrieval performance in the scenarios where the new streaming datapoints all belong to the new classes. Therefore, to further improve the retrieval performance in these scenarios, we propose a novel Prospective Layout-Guided Multi-modal Online Hashing, termed PLG-MOH. Specifically, PLG-MOH first establishes the layout of the Hamming space by generating a series of hashing centers to split the space. Each hashing center will be gradually assigned to a new appearing class, and these assigned centers correspond one-to-one with the classes. Moreover, we propose a novel prospective layout-guided loss, which leverages all the hashing centers, including those not yet assigned to the classes, to supervise the training of hashing model. As the unassigned hashing centers will be designated to the new classes emerging in the future, it signifies that during each round of training, PLG-MOH has already considered the forthcoming data from new classes in the future rounds. Consequently, PLG-MOH can effectively adapt its hashing functions to address the new arriving samples and learn semantic similarity-preserved hash codes for them, meanwhile it can effectively retain the information learned from the old data. Extensive experiments on two public datasets demonstrate that the proposed PLG-MOH achieves better retrieval performance than state-of-the-art baselines on online scenarios.},
  archive      = {J_TIP},
  author       = {Rong-Cheng Tu and Xian-Ling Mao and Jin-Yu Liu and Zi-Ao Ma and Tian Lan and Heyan Huang},
  doi          = {10.1109/TIP.2025.3607626},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5935-5947},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prospective layout-guided multi-modal online hashing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast video recoloring via curve-based palettes. <em>TIP</em>, <em>34</em>, 5920-5934. (<a href='https://doi.org/10.1109/TIP.2025.3607584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color grading, as a crucial step in film post-production, plays an important role in emotional expression and artistic enhancement. Recently, a geometric palette-based approach to video recoloring has been introduced with impressive results. It offers an intuitive interface that allows users to alter the color of a video by manipulating a limited set of representative colors. However, this method has two primary limitations. Firstly, palette extraction is computationally expensive, often taking more than one hour to generate palettes even for medium-length videos, which significantly limits the practical application of color editing for longer videos. Secondly, the palette colors are less representative, and some primary colors may be omitted from the resulting palettes during topological simplification, making it less intuitive in color editing. To overcome these limitations, in this paper, we propose a novel approach to video recoloring. The core of our method is a set of Bézier curves that connect the dominant colors throughout the input video. By slicing these Bézier curves in RGBT space, per-frame palette can be naturally derived. During recoloring, users can select several frames of interest and modify their corresponding palettes to change the color of the video. Our method is simple and intuitive, enabling compelling time-varying recoloring results. Compared to existing methods, our approach is more efficient in palette extraction and can effectively capture the dominant colors of the video. Extensive experiments demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Zheng-Jun Du and Jia-Wei Zhou and Kang Li and Jian-Yu Hao and Zi-Kang Huang and Kun Xu},
  doi          = {10.1109/TIP.2025.3607584},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5920-5934},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast video recoloring via curve-based palettes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multimodal knowledge for image compression via large foundation models. <em>TIP</em>, <em>34</em>, 5904-5919. (<a href='https://doi.org/10.1109/TIP.2025.3607616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge is an abstraction of factual principles of the physical world. Large foundation models encapsulate extensive multimodal knowledge into the parameters and thus invoke machine intelligence on various tasks. How to invoke the knowledge in these models to facilitate image compression lacks in-depth exploration. In this work, we aim to harness multimodal knowledge into ultra-low bitrate compression and propose Multimodal Knowledge-aware Image Compression (MKIC). Our key insight is that under the context of ultra-low bitrate compression, where the encoded representation is too sparse to represent enough information of the input signal, knowledge from the physical world is required to be incorporated into the compression. Thus, more shared patterns can be stored in the model together with sparse unique features also embedded into the bitstream. In light of two kinds of knowledge, namely natural visual knowledge and human language knowledge, we propose a novel Alternating Rate-Distortion Optimization to enhance the accuracy and compactness of global semantic text representation extraction, extract the local feature map that captures visual details, and integrate these multimodal representations into a large generative foundation model to achieve high-quality reconstruction. The proposed method relights the path of learned image coding, leveraging decoupled knowledge from large foundation models. Extensive experiments show that our proposed method achieves superior comprehensive performance compared to various methods and shows great potential for ultra-low bitrate image compression.},
  archive      = {J_TIP},
  author       = {Junlong Gao and Zhimeng Huang and Qi Mao and Siwei Ma and Chuanmin Jia},
  doi          = {10.1109/TIP.2025.3607616},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5904-5919},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring multimodal knowledge for image compression via large foundation models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised text-based person search. <em>TIP</em>, <em>34</em>, 5888-5903. (<a href='https://doi.org/10.1109/TIP.2025.3607637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description. Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning. This presents a substantial practical challenge, given the difficulty in obtaining annotated texts for person images. This work undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations. We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS. The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images. Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data. Crucially, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data. The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process. PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision. NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning. Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting.},
  archive      = {J_TIP},
  author       = {Daming Gao and Yang Bai and Min Cao and Hao Dou and Mang Ye and Min Zhang},
  doi          = {10.1109/TIP.2025.3607637},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5888-5903},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised text-based person search},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware generative point cloud compression for visual perception. <em>TIP</em>, <em>34</em>, 5873-5887. (<a href='https://doi.org/10.1109/TIP.2025.3607630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a rapid growth in applications that rely on point clouds to represent the 3D world, driven by the increasing demand for immersive and other related scenarios. However, compressing the large and high-precision point cloud data efficiently while maintaining high perceptual quality for human vision remains a challenge. To solve the problem, we propose a new structure-aware generative point cloud compression framework for human vision. In the encoder, we focus on information that is more sensitive to the human vision and obtain this type of information from different scale. This allows us to capture structural importance information from global scale and local scale, which are more difficult to reconstruct. For the decoder, we introduce a progressive generative reconstruction approach that utilizes acquired information from the encoder to guide the generation of point cloud surfaces. Moreover, we propose a novel probability cloud-based discriminator. Instead of directly assessing the authenticity of the generated point clouds, our discriminator evaluates the probability distribution of the existence of points within the generated point cloud. This approach reduces the difficulty of discrimination while effectively improving the accuracy of the generator in generating probability distributions. According to the correct probability, we can obtain a high accuracy point cloud by pruning the points with low probability. Through comprehensive experiments, we demonstrate the effectiveness and superiority of our proposed framework in terms of encoding efficiency, high perceptual quality, and generation quality.},
  archive      = {J_TIP},
  author       = {Yichen Zhou and Xinfeng Zhang and Yingzhan Xu and Kai Zhang and Li Zhang and Qingming Huang},
  doi          = {10.1109/TIP.2025.3607630},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5873-5887},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-aware generative point cloud compression for visual perception},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging. <em>TIP</em>, <em>34</em>, 5860-5872. (<a href='https://doi.org/10.1109/TIP.2025.3607633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic functional brain network (DFBN) can flexibly describe the time-varying topological connectivity patterns of the brain, and show great potential in brain disease diagnosis. However, most of the existing DFBN analysis methods focus on capturing the dynamic interaction at the brain region level, ignoring the spatio-temporal topological evolution across time windows. Moreover, they are difficult to suppress interfering connections in DFBNs, which leads to a diminished capacity for discerning the intrinsic structures that are intimately linked to brain disorders. To address these issues, we propose a topological evolution graph learning model to capture disease-related spatio-temporal topological features in DFBNs. Specifically, we first take the hubness of adjacent DFBN as the source domain and the target domain in turn, and then use Wasserstein distance (WD) and Gromov-Wasserstein distance (GWD) to capture the brain’s evolution law at the node and edge levels, respectively. Furthermore, we introduce the principle of relevant information to guide the topology evolution graph to learn the structures that are most relevant to brain diseases yet least redundant information between adjacent DFBNs. On this basis, we develop a high-order spatio-temporal model with multi-hop graph convolution to collaboratively extract long-range spatial and temporal dependencies from the topological evolution graph. Extensive experiments show that the proposed method outperforms the current state-of-the-art methods, and can effectively reveal the information evolution mechanism between brain regions across windows.},
  archive      = {J_TIP},
  author       = {Shengrong Li and Qi Zhu and Chunwei Tian and Li Zhang and Bo Shen and Chuhang Zheng and Daoqiang Zhang and Wei Shao},
  doi          = {10.1109/TIP.2025.3607633},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5860-5872},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective. <em>TIP</em>, <em>34</em>, 5847-5859. (<a href='https://doi.org/10.1109/TIP.2025.3607612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) shows great potential for improving generalization to unseen classes in real-world scenarios. However, most GZSL methods depend on benchmark datasets with per-class attribute annotations, which creates a large semantic gap and worsens the domain shift problem in the visual-semantic space. To address these challenges, instance-level attributes offer an intuitive solution, but they require expensive manual annotation. In this paper, we propose a simple yet effective approach called per-instance attribute synthesis (PIAS) to generate diverse semantic representations for each instance. Our method first uses the Vision Transformer (ViT) model to extract visual features and then generates per-instance attributes. The patch splitting, positional embedding, and multi-head self-attention mechanisms in ViT improve the discriminability of both visual and semantic representations. Next, we define the generated attributes of class-average images as class anchor points. These anchor points are calibrated in the semantic space by minimizing the cosine similarity between the anchor points and per-class attribute annotations. Finally, we improve the diversity of generated per-instance attributes by aligning the topological structure between per-class attribute annotations and synthesized per-instance attributes with that between class-average visual features and per-instance visual features. We conduct comprehensive experiments on three challenging ZSL datasets: AWA2, CUB, and SUN. The results show that PIAS significantly outperforms state-of-the-art methods under both ZSL and GZSL settings. We further demonstrate the generalization ability of PIAS by applying it to attribute-based zero-shot image retrieval tasks.},
  archive      = {J_TIP},
  author       = {Chenwei Tang and Ying Wang and Wei Xie and Qianjun Zhang and Rong Xiao and Zhenan He and Jiancheng Lv},
  doi          = {10.1109/TIP.2025.3607612},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5847-5859},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harmonized domain enabled alternate search for infrared and visible image alignment. <em>TIP</em>, <em>34</em>, 5832-5846. (<a href='https://doi.org/10.1109/TIP.2025.3607585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image alignment is essential and critical to the fusion and multi-modal perception applications. It addresses discrepancies in position and scale caused by spectral properties and environmental variations, ensuring precise pixel correspondence and spatial consistency. Existing manual calibration requires regular maintenance and exhibits poor portability, challenging the adaptability of multi-modal application in dynamic environments. In this paper, we propose a harmonized representation based infrared and visible image alignment, achieving both high accuracy and scene adaptability. Specifically, with regard to the disparity between multi-modal images, we develop an invertible translation process to establish a harmonized representation domain that effectively encapsulates the feature intensity and distribution of both infrared and visible modalities. Building on this, we design a hierarchical framework to correct deformations inferred from the harmonized domain in a coarse-to-fine manner. Our framework leverages advanced perception capabilities alongside residual estimation to enable accurate regression of sparse offsets, while an alternate correlation search mechanism ensures precise correspondence matching. Furthermore, we propose the first ground truth available misaligned infrared and visible image benchmark for evaluation. Extensive experiments validate the effectiveness of the proposed method against the state-of-the-arts, advancing the subsequent applications further. Code and dataset are available at https://github.com/Jzy2017/HR4IR},
  archive      = {J_TIP},
  author       = {Zhiying Jiang and Zengxi Zhang and Jinyuan Liu},
  doi          = {10.1109/TIP.2025.3607585},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5832-5846},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harmonized domain enabled alternate search for infrared and visible image alignment},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DFACENet: 3D facial attractiveness computation and enhancement network. <em>TIP</em>, <em>34</em>, 5819-5831. (<a href='https://doi.org/10.1109/TIP.2025.3607629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of facial editing, virtual makeup, AR/VR technologies and 3D games applications underscore the need for advanced 3D facial attractiveness research. However, due to the lack of 3D beauty face data and the complexity of handling 3D face data, 3D facial aesthetics research remains largely unexplored. To fill this gap, we propose 3DFACENet, an innovative system designed for the computation and enhancement of 3D facial attractiveness. Our approach employs a 3D facial reconstruction encoder to generate encoded vectors from images and a render module to obtain 3D face models. To minimize computational load, we innovatively propose an attractiveness computation module which leverages 3D shape and texture coefficients rather than 3D mesh models to access facial attractiveness, achieving state-of-the-art results. To balance aesthetic enhancement and identity preservation, we design a controllable beautification decoder. For the first time, we introduce the concept of “attractive centers”, demonstrating that an individual’s distance to these centers is significantly negatively correlated with their beauty scores. Our beautification decoder edits 3D facial coefficients towards these centers, achieving a significant and controllable enhancement in facial attractiveness. Extensive experiments on the SCUT-FBP5500 and MEBeauty dataset validate the effectiveness and feasibility of 3DFACENet.},
  archive      = {J_TIP},
  author       = {Yuan Xie and Tianhao Peng and Mu Li and Baoyuan Wu and David Zhang},
  doi          = {10.1109/TIP.2025.3607629},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5819-5831},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3DFACENet: 3D facial attractiveness computation and enhancement network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). URFusion: Unsupervised unified degradation-robust image fusion network. <em>TIP</em>, <em>34</em>, 5803-5818. (<a href='https://doi.org/10.1109/TIP.2025.3607628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dealing with low-quality source images, existing image fusion methods either fail to handle degradations or are restricted to specific degradations. This study proposes an unsupervised unified degradation-robust image fusion network, termed as URFusion, in which various types of degradations can be uniformly eliminated during the fusion process, leading to high-quality fused images. URFusion is composed of three core modules: intrinsic content extraction, intrinsic content fusion, and appearance representation learning and assignment. It first extracts degradation-free intrinsic content features from images affected by various degradations. These content features then provide feature-level rather than image-level fusion constraints for optimizing the fusion network, effectively eliminating degradation residues and reliance on ground truth. Finally, URFusion learns the appearance representation of images and assigns the statistical appearance representation of high-quality images to the content-fused result, producing the final high-quality fused image. Extensive experiments on multi-exposure image fusion and multi-modal image fusion tasks demonstrate the advantages of URFusion in fusion performance and suppression of multiple types of degradations. The code is available at https://github.com/hanna-xu/URFusion},
  archive      = {J_TIP},
  author       = {Han Xu and Xunpeng Yi and Chen Lu and Guangcan Liu and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3607628},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5803-5818},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {URFusion: Unsupervised unified degradation-robust image fusion network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry. <em>TIP</em>, <em>34</em>, 5792-5802. (<a href='https://doi.org/10.1109/TIP.2025.3607619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase unwrapping is a critical step in fringe projection profilometry, essential for achieving accurate and efficient three-dimension (3D) imaging. Temporal phase unwrapping is the most widely utilized to improve robustness and the reconstruction quality. Unfortunately, due to abrupt phase discontinuities at boundaries, misalignment between the wrapped phases, and unreliable shadow regions, fringe order errors may occur. To address these challenges, this study presents a generalized multi-feature-guided progressive order correction algorithm (GMP-OCA) for high-quality 3D imaging. The algorithm integrates global coarse detection, incremental line-wise optimization, and regional precision scanning to progressively correct fringe orders. Static and dynamic experimental results demonstrate that GMP-OCA effectively eliminates the systematic errors inherent in various phase unwrapping methods, producing high-quality 3D imaging results.},
  archive      = {J_TIP},
  author       = {Jiayi Qin and Yansong Jiang and Yiping Cao},
  doi          = {10.1109/TIP.2025.3607619},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5792-5802},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance-adaptive spatial-temporal enhancement for efficient video compression. <em>TIP</em>, <em>34</em>, 5776-5791. (<a href='https://doi.org/10.1109/TIP.2025.3602648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently compressing HD/UHD content has long been challenging due to high bitrate costs. Instance-adaptive enhancement methods try to tackle this issue by compressing a video at reduced resolution and enhancing it using a neural model specifically overfitted for this video. However, existing methods focus solely on spatial super-resolution (SR) and under-utilize the videos’ temporal redundancy. Their limited management of the model’s updated parameters also causes excessive overfitting overheads. Therefore, this paper introduces IASTE, the first instance-adaptive enhancement method based on spatial-temporal enhancement (STE), and incorporates low-rank adaptation (LoRA) for efficient model overfitting. Specifically, we downscale videos spatially and temporally to reduce the data volume and achieve efficient video compression. Then, we overfit a specific STE model for each video and use it to enhance the decoded video’s spatiotemporal resolution. Leveraging the video swin transformer’s strong capability in capturing spatiotemporal correlations, we design a lightweight and efficient model to implement video STE. The model is overfitted for each video using LoRA. By freezing the pre-trained model and selectively updating a few low-rank matrices, the bitrate overhead for model storage can be mitigated. Experiments prove that compared to directly compressing high-frame-rate (HFR), high-resolution (HR) videos, our method achieves around 30% BD-Rate gains on the CTC and UVG datasets, about 15% gains on the YoutubeUGC dataset, and about 10% gains on the ultra-long videos in the Xiph dataset.},
  archive      = {J_TIP},
  author       = {Yan Zhao and Zhengxue Cheng and Jiangchuan Li and Donghui Feng and Qunshan Gu and Qi Wang and Guo Lu and Li Song},
  doi          = {10.1109/TIP.2025.3602648},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5776-5791},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Instance-adaptive spatial-temporal enhancement for efficient video compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation. <em>TIP</em>, <em>34</em>, 5761-5775. (<a href='https://doi.org/10.1109/TIP.2025.3607615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D medical image segmentation is crucial for diagnosis and treatment. Diffusion models demonstrate promising performance in medical image segmentation tasks due to the progressive nature of the generation process and the explicit modeling of data distributions. However, the weak guidance of conditional information and insufficient feature extraction in diffusion models lead to the loss of fine-grained features and structural consistency in the segmentation results, thereby affecting the accuracy of medical image segmentation. To address this challenge, we propose a Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation. We extract multilevel semantic features from the original images using an encoder and tightly integrate them with the denoising process of the diffusion model through a Semantic Hierarchical Embedding (SHE) mechanism, to capture the intricate relationship between the noisy label and image data. Meanwhile, we design a Global-Slice Perception Mamba (GSPM) layer, which integrates multi-dimensional perception mechanisms to endow the model with comprehensive spatial reasoning and feature extraction capabilities. Experimental results show that our proposed MambaDiff achieves more competitive performance compared to prior arts with substantially fewer parameters on four public medical image segmentation datasets including BraTS 2021, BraTS 2024, LiTS and MSD Hippocampus. The source code of our method is available at https://github.com/yuliu316316/MambaDiff},
  archive      = {J_TIP},
  author       = {Yu Liu and Yan Feng and Juan Cheng and Haolin Zhan and Zhiqin Zhu},
  doi          = {10.1109/TIP.2025.3607615},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5761-5775},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streaming view classification with noisy label. <em>TIP</em>, <em>34</em>, 5750-5760. (<a href='https://doi.org/10.1109/TIP.2025.3607610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many image processing tasks, e.g., 3D reconstruction of dynamic scenes, different types of descriptions, a.k.a., views, of an object are emerging in a streaming way. Streaming view learning provides an effective solution to this dynamic view problem. In this paradigm, existing streaming view learning methods typically assume that all labels are accurate. However, in many real-world applications, the initial views may be not good enough for characterizing, leading to noisy labels that degrade classification performance. How to learn a model for simultaneous view evolving and label ambiguity is critical yet unexplored. In this paper, we propose a novel method called Streaming View Classification with Noisy Label (SVCNL). We calibrate noisy labels according to the emerging of new views, thereby reflecting the dynamic changes in the data more accurately. Leveraging the sequential and non-revisitable nature of views, the method tunes existing models to inherit information from previous stages by utilizing current-stage data. It reconstructs noisy labels through a label transition matrix and establishes relationships between true labels and samples using a graph embedding strategy, progressively correcting noisy labels. Together with the theoretical analyses about generalization bounds, extensive experiments demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Xiao Ouyang and Ruidong Fan and Hong Tao and Chenping Hou},
  doi          = {10.1109/TIP.2025.3607610},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5750-5760},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Streaming view classification with noisy label},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOPE: Enhanced position image priors via high-order implicit representations. <em>TIP</em>, <em>34</em>, 5737-5749. (<a href='https://doi.org/10.1109/TIP.2025.3607582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Image Prior (DIP) has shown that networks with stochastic initialization and custom architectures can effectively address inverse imaging challenges. Despite its potential, DIP requires significant computational resources, whereas the lighter Implicit Neural Positional Image Prior (PIP) often yields overly smooth solutions due to exacerbated spectral bias. Research on lightweight, high-performance solutions for inverse imaging remains limited. This paper proposes a novel framework, Enhanced Positional Image Priors through High-Order Implicit Representations (HOPE), incorporating high-order interactions between layers within a conventional cascade structure. This approach reduces the spectral bias commonly seen in PIP, enhancing the model’s ability to capture both low- and high-frequency components for optimal inverse problem performance. We theoretically demonstrate that HOPE’s expanded representational space, narrower convergence range, and improved Neural Tangent Kernel (NTK) diagonal properties enable more precise frequency representations than PIP. Comprehensive experiments across tasks such as signal representation (audio, image, volume) and inverse image processing (denoising, super-resolution, CT reconstruction, inpainting) confirm that HOPE establishes new benchmarks for recovery quality and training efficiency.},
  archive      = {J_TIP},
  author       = {Yang Chen and Ruituo Wu and Junhui Hou and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TIP.2025.3607582},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5737-5749},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOPE: Enhanced position image priors via high-order implicit representations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OSFormer: One-step transformer for infrared video small object detection. <em>TIP</em>, <em>34</em>, 5725-5736. (<a href='https://doi.org/10.1109/TIP.2025.3598426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared video small object detection is pivotal in numerous security and surveillance applications. However, existing deep learning-based methods, which typically rely on a two-step paradigm of frame-by-frame detection followed by temporal refinement, struggle to effectively utilize temporal information. This is particularly challenging when detecting small objects against complex backgrounds. To address these issues, we introduce the One-Step Transformer (OSFormer), a novel method that pioneeringly integrates a small-object-friendly transformer with a one-step detection paradigm. Unlike traditional methods, OSFormer processes the video sequence only through a single inference, encoding the sequence into cube format data and tracking object motion trajectories. Additionally, we propose the Varied-Size Patch Attention (VPA) module, which generates patches of varying sizes to capture adaptive attention features, bridging the gap between transformer architectures and small object detection. To further enhance detection accuracy, OSFormer incorporates a Doppler Adaptive Filter, which integrates traditional filtering techniques into an end-to-end neural network to suppress background noise and accentuate small objects. OSFormer outperforms YOLOv8-s on both the AntiUAV dataset (+ $3.1\%~\text {mAP}_{50}$ , - $35.1\%~\text {Params}$ ) and the InfraredUAV dataset (+ $4.0\%~\text {mAP}_{50-95}$ , - $51.0\%~\text {FLOPs}$ ), demonstrating superior efficiency and effectiveness in small object detection. The code is available on https://github.com/q2479036243/OSFormer.},
  archive      = {J_TIP},
  author       = {Haolin Qin and Tingfa Xu and Yuan Tang and Fengxiang Xu and Jianan Li},
  doi          = {10.1109/TIP.2025.3598426},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5725-5736},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OSFormer: One-step transformer for infrared video small object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic prompting learning for human-object interaction detection. <em>TIP</em>, <em>34</em>, 5710-5724. (<a href='https://doi.org/10.1109/TIP.2025.3607614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection, as a foundational task in human-centric understanding, aims to detect interactive triplets in real-world scenarios. To better distinguish diverse HOIs within an open-world context, current HOI detectors utilize pre-trained Visual-Language Models (VLMs) to extract prior knowledge through textual prompts (i.e., descriptive texts for each HOI instance). However, relying on predetermined descriptive texts, such approaches only acquire a fixed set of textual knowledge for HOI prediction, consequently resulting in inferior performance and limited generalization. To remedy this, we propose a novel VLM-based method, which jointly performs prompting learning from both visual and textual perspectives and synergizes visual-textual prompting for HOI detection. Initially, we design a hierarchical adaptation architecture to perform progressive prompting: visual prompting is facilitated through gradual token migration from VLM’s image encoder, while textual prompting is initialized with progressively leveled interaction descriptions. In addition, to synergize the visual-textual prompting learning, a text-supervising and image-tuning loop is introduced, in which the text-supervising stage guides visual prompting learning through contrastive learning and the image-tuning stage refines textual prompting by modal matching. Finally, we employ an interaction-aware knowledge merging mechanism to effectively transfer visual-textual knowledge encapsulated within synergistic prompting for HOI detection. Extensive experiments on two benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings.},
  archive      = {J_TIP},
  author       = {Jinguo Luo and Weihong Ren and Zhiyong Wang and Xi’ai Chen and Huijie Fan and Zhi Han and Honghai Liu},
  doi          = {10.1109/TIP.2025.3607614},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5710-5724},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Synergistic prompting learning for human-object interaction detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking variational graph representation inference for video summarization. <em>TIP</em>, <em>34</em>, 5697-5709. (<a href='https://doi.org/10.1109/TIP.2025.3602649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of short video content, efficient video summarization techniques for extracting key information have become crucial. However, existing methods struggle to capture the global temporal dependencies and maintain the semantic coherence of video content. Additionally, these methods are also influenced by noise during multi-channel feature fusion. We propose a Spiking Variational Graph (SpiVG) Network, which enhances information density and reduces computational complexity. First, we design a keyframe extractor based on Spiking Neural Networks (SNN), leveraging the event-driven computation mechanism of SNNs to learn keyframe features autonomously. To enable fine-grained and adaptable reasoning across video frames, we introduce a Dynamic Aggregation Graph Reasoner, which decouples contextual object consistency from semantic perspective coherence. We present a Variational Inference Reconstruction Module to address uncertainty and noise arising during multi-channel feature fusion. In this module, we employ Evidence Lower Bound Optimization (ELBO) to capture the latent structure of multi-channel feature distributions, using posterior distribution regularization to reduce overfitting. Experimental results show that SpiVG surpasses existing methods across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes and pre-trained models are available at https://github.com/liwrui/SpiVG},
  archive      = {J_TIP},
  author       = {Wenrui Li and Wei Han and Liang-Jian Deng and Ruiqin Xiong and Xiaopeng Fan},
  doi          = {10.1109/TIP.2025.3602649},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5697-5709},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spiking variational graph representation inference for video summarization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faces blind your eyes: Unveiling the content-irrelevant synthetic artifacts for deepfake detection. <em>TIP</em>, <em>34</em>, 5686-5696. (<a href='https://doi.org/10.1109/TIP.2025.3592576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data synthesis methods have shown promising results in general deepfake detection tasks. This is attributed to the inherent blending process in deepfake creation, which leaves behind distinct synthetic artifacts. However, the existence of content-irrelevant artifacts has not been explicitly explored in the deepfake synthesis. Unveiling content-irrelevant synthetic artifacts helps uncover general deepfake features and enhances the generalization capability of detection models. To capture the content-irrelevant synthetic artifacts, we propose a learning framework incorporating a synthesis process for diverse contents and specially designed learning strategies that encourage using content-irrelevant forgery information across deepfake images. From the data perspective, we disentangle the blending operation from face data and propose a universal synthetic module that generates images from various classes with common synthetic artifacts. From the learning perspective, a domain-adaptive learning head is introduced to filter out forgery-irrelevant features and optimize the decision on deepfake face detection. To efficiently learn the content-irrelevant artifacts for detection with a large sampling space, we propose a batch-wise sample selection strategy that actively mines the hard samples based on their effect on the adaptive decision boundary. Extensive cross-dataset experiments show that our method achieves state-of-the-art performance in general deepfake detection.},
  archive      = {J_TIP},
  author       = {Xinghe Fu and Benzun Fu and Shen Chen and Taiping Yao and Yiting Wang and Shouhong Ding and Xiubo Liang and Xi Li},
  doi          = {10.1109/TIP.2025.3592576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5686-5696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Faces blind your eyes: Unveiling the content-irrelevant synthetic artifacts for deepfake detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous feature representation for camouflaged object detection. <em>TIP</em>, <em>34</em>, 5672-5685. (<a href='https://doi.org/10.1109/TIP.2025.3602657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to discover objects that are seamlessly embedded in the environment. Existing COD methods have made significant progress by typically representing features in a discrete way with arrays of pixels. However, limited by discrete representation, these methods need to align features of different scales during decoding, which causes some subtle discriminative clues to become blurred. This is a huge blow to the task of identifying camouflaged objects from clear subtle clues. To address this issue, we propose a novel continuous feature representation network (CFRN), which aims to represent features of different scales as a continuous function for COD. Specifically, a Swin transformer encoder is first exploited to explore the global context between camouflaged objects and the background. Then, an object-focusing module (OFM) deployed layer by layer is designed to deeply mine subtle discriminative clues, thereby highlighting the body of camouflaged objects and suppressing other distracting objects at different scales. Finally, a novel frequency-based implicit feature decoder (FIFD) is proposed, which directly decodes the predictions at arbitrary coordinates in the continuous function with implicit neural representations, thus propagating clearer discriminative clues. Extensive experiments on four challenging COD benchmarks demonstrate that our method significantly outperforms state-of-the-art methods. The source code will be available at https://github.com/SongZeHNU/CFRN},
  archive      = {J_TIP},
  author       = {Ze Song and Xudong Kang and Xiaohui Wei and Jinyang Liu and Zheng Lin and Shutao Li},
  doi          = {10.1109/TIP.2025.3602657},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5672-5685},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Continuous feature representation for camouflaged object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ColorAssist: Perception-based recoloring for color vision deficiency compensation. <em>TIP</em>, <em>34</em>, 5658-5671. (<a href='https://doi.org/10.1109/TIP.2025.3602643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement methods have been widely studied to improve the visual quality of diverse images, implicitly assuming that all human observers have normal vision. However, a large population around the world suffers from Color Vision Deficiency (CVD). Enhancing images to compensate for their perceptions remains a challenging issue. Existing CVD compensation methods have two drawbacks: first, the available datasets and validations have not been rigorously tested by CVD individuals; second, these methods struggle to strike an optimal balance between contrast enhancement and naturalness preservation, which often results in suboptimal outcomes for individuals with CVD. To address these issues, we develop the first large-scale, CVD-individual-labeled dataset called FZU-CVDSet and a CVD-friendly recoloring algorithm called ColorAssist. In particular, we design a perception-guided feature extraction module and a perception-guided diffusion transformer module that jointly achieve efficient image recoloring for individuals with CVD. Comprehensive experiments on both FZU-CVDSet and subjective tests in hospitals demonstrate that the proposed ColorAssist closely aligns with the visual perceptions of individuals with CVD, achieving superior performance compared with the state-of-the-arts. The source code is available at https://github.com/xsx-fzu/ColorAssist.},
  archive      = {J_TIP},
  author       = {Liqun Lin and Shangxi Xie and Yanting Wang and Bolin Chen and Ying Xue and Xiahai Zhuang and Tiesong Zhao},
  doi          = {10.1109/TIP.2025.3602643},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5658-5671},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ColorAssist: Perception-based recoloring for color vision deficiency compensation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPC-SPU: Nonlinear phase coding-based stereo phase unwrapping for efficient 3D measurement. <em>TIP</em>, <em>34</em>, 5642-5657. (<a href='https://doi.org/10.1109/TIP.2025.3602644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D imaging based on phase-shifting structured light is widely used in industrial measurement due to its non-contact nature. However, it typically requires a large number of additional images (multi-frequency heterodyne (M-FH) method) or introduces intensity features that compromise accuracy (space domain modulation phase-shifting (SDM-PS) method) for phase unwrapping, and it remains sensitive to motion. To overcome these issues, this article proposes a nonlinear phase coding-based stereo phase unwrapping (NPC-SPU) method that requires no additional patterns while maintaining measurement accuracy. In the encoding stage, a novel nonlinear distortion feature is introduced, while the signal-to-noise ratio of the phase codeword is preserved. In the decoding stage, a local phase unwrapping method that does not require additional auxiliary information is first proposed, closely associating the distortion information in the local wrapped phase. Then, a pre-calibrated stereo constraint system is used to filter potential matching phases, significantly reducing phase ambiguity and computational costs. Finally, to avoid the time-consuming and complex intensity kernel matching used in traditional methods, we propose a local phase correlation matching (LPCM) technique that enables lightweight and robust phase unwrapping. Experimental results demonstrate that this algorithm significantly enhances 3D reconstruction performance in scenarios with large depth, large disparity, complex colored structures, and dynamic scenes. Specifically, in dynamic environments (20mm/s), the proposed method achieves a lower measurement error rate (0.7829% vs. 6.4962%) with only 3 patterns, compared to the traditional three-frequency heterodyne (T-FH) method (using 9 patterns). Additionally, its measurement accuracy outperforms the advanced SDM-PS method, which also uses 3 patterns (0.1102 mm vs. 0.3232 mm).},
  archive      = {J_TIP},
  author       = {Ruiming Yu and Hongshan Yu and Wei Sun and Yaonan Wang and Naveed Akhtar and Kemao Qian},
  doi          = {10.1109/TIP.2025.3602644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5642-5657},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NPC-SPU: Nonlinear phase coding-based stereo phase unwrapping for efficient 3D measurement},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TripleNet: Exploiting complementary features and pseudo-labels for semi-supervised salient object detection. <em>TIP</em>, <em>34</em>, 5628-5641. (<a href='https://doi.org/10.1109/TIP.2025.3601334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited output categories, semi-supervised salient object detection faces challenges in adapting conventional semi-supervised strategies. To address this limitation, we propose a multi-branch architecture that extracts complementary features from labeled data. Specifically, we introduce TripleNet, a three-branch network architecture designed for contour, content, and holistic saliency prediction. The supervision signals for the contour and content branches are derived by decomposing the limited ground truths. After training on the labeled data, the model produces pseudo-labels for unlabeled images, including contour, content, and salient objects. By leveraging the complementarity between the contour and content branches, we construct coupled pseudo-saliency labels by integrating the pseudo-contour and pseudo-content labels, which differ from the model-inferred pseudo-saliency labels. We further develop an enhanced pseudo-labeling mechanism that generates enhanced pseudo-saliency labels by combining reliable regions from both pseudo-saliency labels. Moreover, we incorporate a partial binary cross-entropy loss function to guide the learning of the saliency branch to focus on effective regions within the enhanced pseudo-saliency labels, which are identified through our adaptive thresholding approach. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance using only 329 labeled training images.},
  archive      = {J_TIP},
  author       = {Liyuan Chen and Ming-Hsuan Yang and Jian Pu and Zhonglong Zheng},
  doi          = {10.1109/TIP.2025.3601334},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5628-5641},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TripleNet: Exploiting complementary features and pseudo-labels for semi-supervised salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FocalTransNet: A hybrid focal-enhanced transformer network for medical image segmentation. <em>TIP</em>, <em>34</em>, 5614-5627. (<a href='https://doi.org/10.1109/TIP.2025.3602739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CNNs have demonstrated superior performance in medical image segmentation. To overcome the limitation of only using local receptive field, previous work has attempted to integrate Transformers into convolutional network components such as encoders, decoders, or skip connections. However, these methods can only establish long-distance dependencies for some specific patterns and usually neglect the loss of fine-grained details during downsampling in multi-scale feature extraction. To address the issues, we present a novel hybrid Transformer network called FocalTransNet. Specifically, we construct a focal-enhanced (FE) Transformer module by introducing dense cross-connections into a CNN-Transformer dual-path structure and deploy the FE Transformer throughout the entire encoder. Different from existing hybrid networks that employ embedding or stacking strategies, the proposed model allows for a comprehensive extraction and deep fusion of both local and global features at different scales. Besides, we propose a symmetric patch merging (SPM) module for downsampling, which can retain the fine-grained details by establishing a specific information compensation mechanism. We evaluated the proposed method on four different medical image segmentation benchmarks. The proposed method outperforms previous state-of-the-art convolutional networks, Transformers, and hybrid networks. The code for FocalTransNet is publicly available at https://github.com/nemanjajoe/FocalTransNet},
  archive      = {J_TIP},
  author       = {Miao Liao and Ruixin Yang and Yuqian Zhao and Wei Liang and Junsong Yuan},
  doi          = {10.1109/TIP.2025.3602739},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5614-5627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FocalTransNet: A hybrid focal-enhanced transformer network for medical image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-world animation super-resolution benchmark with color degradation and multi-scale multi-frequency alignment. <em>TIP</em>, <em>34</em>, 5598-5613. (<a href='https://doi.org/10.1109/TIP.2025.3599946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animation super-resolution (SR) aims to generate high-resolution (HR) animation frames from degraded low-resolution (LR) inputs, constituting an important task in real-world SR. Existing animation SR methods typically follow a photorealistic real-world SR computational paradigm. However, digital animation frames commonly suffer from compression and transmission-related degradation, distinct from degradations in camera-captured real-world images. In this paper, we introduce a novel real-world animation super-resolution benchmark designed explicitly for animation frames, named ADASR, featuring both 2D and modern 3D animation content to facilitate industry applications. Additionally, we propose a Color-Aware Animation Super-Resolution (CAASR) method. CAASR, for the first time, incorporates a color degradation simulation mechanism tailored for animations, addressing color banding, blocking, and color shift. Furthermore, we develop a multi-scale multi-frequency alignment mechanism to robustly extract degradation-invariant features. Extensive experiments conducted on both the existing AVC dataset and our newly constructed ADASR dataset demonstrate that our proposed CAASR achieves state-of-the-art performance in restoring HR frames for both 2D and 3D animations. Code and data are available at https://github.com/huangyang-666/CAASR.},
  archive      = {J_TIP},
  author       = {Yu Jiang and Yongji Zhang and Siqi Li and Yang Huang and Yuehang Wang and Yutong Yao and Yue Gao},
  doi          = {10.1109/TIP.2025.3599946},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5598-5613},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A real-world animation super-resolution benchmark with color degradation and multi-scale multi-frequency alignment},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-text-image knowledge transfer for lifelong person re-identification with hybrid clothing states. <em>TIP</em>, <em>34</em>, 5584-5597. (<a href='https://doi.org/10.1109/TIP.2025.3602745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous expansion of intelligent surveillance networks, lifelong person re-identification (LReID) has received widespread attention, pursuing the need of self-evolution across different domains. However, existing LReID studies accumulate knowledge with the assumption that people would not change their clothes. In this paper, we propose a more practical task, namely lifelong person re-identification with hybrid clothing states (LReID-Hybrid), which takes a series of cloth-changing and same-cloth domains into account during lifelong learning. To tackle the challenges of knowledge granularity mismatch and knowledge presentation mismatch in LReID-Hybrid, we take advantage of the consistency and generalization capabilities of the text space, and propose a novel framework, dubbed Teata, to effectively align, transfer, and accumulate knowledge in an “image-text-image” closed loop. Concretely, to achieve effective knowledge transfer, we design a Structured Semantic Prompt (SSP) learning to decompose the text prompt into several structured pairs to distill knowledge from the image space with a unified granularity of text description. Then, we introduce a Knowledge Adaptation and Projection (KAP) strategy, which tunes text knowledge via a slow-paced learner to adapt to different tasks without catastrophic forgetting. Extensive experiments demonstrate the superiority of our proposed Teata for LReID-Hybrid as well as on conventional LReID benchmarks over advanced methods.},
  archive      = {J_TIP},
  author       = {Qizao Wang and Xuelin Qian and Bin Li and Yanwei Fu and Xiangyang Xue},
  doi          = {10.1109/TIP.2025.3602745},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5584-5597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image-text-image knowledge transfer for lifelong person re-identification with hybrid clothing states},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-based multi-modal feature learning for cloth-changing person re-identification. <em>TIP</em>, <em>34</em>, 5570-5583. (<a href='https://doi.org/10.1109/TIP.2025.3602641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive Language-Image Pre-training (CLIP) has achieved remarkable results in the field of person re-identification (ReID) due to its excellent cross-modal understanding ability and high scalability. Since the text encoder of CLIP mainly focuses on easy-to-describe attributes such as clothing, and clothing is the main interference factor that reduces the recognition accuracy in cloth-changing person ReID (CC ReID). Consequently, directly applying CLIP to cloth-changing scenario may be difficult to adapt to such dynamic feature changes, thereby affecting the precision of identification. To solve this challenge, we propose a CLIP-based multi-modal feature learning framework (CMFF) for CC ReID. Specifically, we first design a pose-aware identity enhancement module (PIE) to enhance the model’s perception of identity-intrinsic information. In this branch, to weaken the interference of clothing information, we apply a ranking loss to minimize the difference between appearance and pose in the feature space. Secondly, we propose a global-local hybrid attention module (GLHA), which fuses head and global features through a cross-attention mechanism, enhancing the global recognition ability of key head information. Finally, considering that existing CLIP-based methods often ignore the potential importance of shallow features, we propose a graph-based multi-layer interactive enhancement module (GMIE), which groups and integrates multi-layer features of the image encoder, aiming to enhance the contextual awareness of multi-scale features. Extensive experiments on multiple popular pedestrian datasets validate the outstanding performance of our proposed CMFF.},
  archive      = {J_TIP},
  author       = {Guoqing Zhang and Jieqiong Zhou and Lu Jiang and Yuhui Zheng and Weisi Lin},
  doi          = {10.1109/TIP.2025.3602641},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5570-5583},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLIP-based multi-modal feature learning for cloth-changing person re-identification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust labeling and invariance modeling for unsupervised cross-resolution person re-identification. <em>TIP</em>, <em>34</em>, 5557-5569. (<a href='https://doi.org/10.1109/TIP.2025.3601443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-resolution person re-identification (CR-ReID) aims to match low-resolution (LR) and high-resolution (HR) images of the same individual. To reduce the cost of manual annotation, existing unsupervised CR-ReID methods typically rely on cross-resolution fusion to obtain pseudo-labels and resolution-invariant features. However, the fusion process requires two encoders and a fusion module, which significantly increases computational complexity and reduces efficiency. To address this issue, we propose a robust labeling and invariance modeling (RLIM) framework, which utilizes a single encoder to tackle the unsupervised CR-ReID problem. To obtain pseudo-labels robust to resolution gaps, we develop cross-resolution robust labeling (CRL), which utilizes two clustering criteria to encourage cross-resolution positive pairs to cluster together and exploit the reliable relationships between images. We also introduce random texture augmentation (TexA) to enhance the model’s robustness to noisy textures related to artifacts and backgrounds by randomly adjusting texture strength. During the optimization process, we introduce the resolution-cluster consistency loss, which promotes resolution-invariant feature learning by aligning inter-resolution distances with intra-cluster distances. Experimental results on multiple datasets demonstrate that RLIM not only surpasses existing unsupervised methods, but also achieves performance close to some supervised CR-ReID methods. Code is available at https://github.com/zqpang/RLIM},
  archive      = {J_TIP},
  author       = {Zhiqi Pang and Lingling Zhao and Yang Liu and Chunyu Wang and Gaurav Sharma},
  doi          = {10.1109/TIP.2025.3601443},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5557-5569},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust labeling and invariance modeling for unsupervised cross-resolution person re-identification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware cross-training for semi-supervised medical image segmentation. <em>TIP</em>, <em>34</em>, 5543-5556. (<a href='https://doi.org/10.1109/TIP.2025.3599783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code and segmentation maps will be released at https://github.com/taozh2017/UCSeg},
  archive      = {J_TIP},
  author       = {Kaiwen Huang and Tao Zhou and Huazhu Fu and Yizhe Zhang and Yi Zhou and Xiao-Jun Wu},
  doi          = {10.1109/TIP.2025.3599783},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5543-5556},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty-aware cross-training for semi-supervised medical image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-customized domain adaptation: Unlock each customer-specific class with single annotation. <em>TIP</em>, <em>34</em>, 5527-5542. (<a href='https://doi.org/10.1109/TIP.2025.3597036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model customization mitigates the issues of inadequate performance, resource wastage, and privacy risks associated with using general-purpose models in specialized domains and well-defined tasks. However, achieving customization at a low annotation cost still poses a challenge. Existing domain adaptation research has addressed cases where all customized classes are present in the labeled database, yet scenarios involving customer-specific classes are still unresolved. Therefore, this paper proposes a novel Class-Customized Domain Adaptation (CCDA) method, addressing the latter scenario with just one additional annotation for each customer-specific class. CCDA adopts the classic adaptation training framework and comprises two innovative techniques. Firstly, to ensure the shared class knowledge from the database and the private class knowledge from additional annotations are transferred and propagated to the correct regions within the target domain, we design the partial-feature alignment strategy, based on the mechanical properties of feature alignment. Second, we propose soft-balanced sampling to tackle the long-tail distribution problem in labeled data, preventing the model from overfitting to the labeled samples of customer-specific classes. The effectiveness of CCDA has been validated across 48 tasks simulated on domain adaptation benchmarks and two real-world customization scenarios, consistently showing excellent performance. Additionally, extensive analytical experiments illustrate the contributions of two innovative techniques. The code is available at https://github.com/CHEN-kx/ClassCustomizedDA},
  archive      = {J_TIP},
  author       = {Kaixin Chen and Huiying Chang and Mengqiu Xu and Ruoyi Du and Ming Wu and Zhanyu Ma and Chuang Zhang},
  doi          = {10.1109/TIP.2025.3597036},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5527-5542},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Class-customized domain adaptation: Unlock each customer-specific class with single annotation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UFPF: A universal feature perception framework for microscopic hyperspectral images. <em>TIP</em>, <em>34</em>, 5513-5526. (<a href='https://doi.org/10.1109/TIP.2025.3594151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has shown immense promise in advancing medical hyperspectral imaging diagnostics at the microscopic level. Despite this progress, most existing research models remain constrained to single-task or single-scene applications, lacking robust collaborative interpretation of microscopic hyperspectral features and spatial information, thereby failing to fully explore the clinical value of hyperspectral data. In this paper, we propose a microscopic hyperspectral universal feature perception framework (UFPF), which extracts high-quality spatial-spectral features of hyperspectral data, providing a robust feature foundation for downstream tasks. Specifically, this innovative framework captures different sequential spatial nearest-neighbor relationships through a hierarchical corner-to-center mamba structure. It incorporates the concept of “progressive focus towards the center”, starting by emphasizing edge information and gradually refining attention from the edges towards the center. This approach effectively integrates richer spatial-spectral information, boosting the model’s feature extraction capability. On this basis, a dual-path spatial-spectral joint perception module is developed to achieve the complementarity of spatial and spectral information and fully explore the potential patterns in the data. In addition, a Mamba-attention Mix-alignment is designed to enhance the optimized alignment of deep semantic features. The experimental results on multiple datasets have shown that this framework significantly improves classification and segmentation performance, supporting the clinical application of medical hyperspectral data. The code is available at: https://github.com/Qugeryolo/UFPF},
  archive      = {J_TIP},
  author       = {Geng Qin and Huan Liu and Wei Li and Xueyu Zhang and Yuxing Guo and Xiang-Gen Xia},
  doi          = {10.1109/TIP.2025.3594151},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5513-5526},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UFPF: A universal feature perception framework for microscopic hyperspectral images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the two-stream framework for efficient visual tracking. <em>TIP</em>, <em>34</em>, 5500-5512. (<a href='https://doi.org/10.1109/TIP.2025.3598934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical deployments, especially on resource-limited edge devices, necessitate high speed for visual object trackers. To meet this demand, we introduce a new efficient tracker with a Two-Stream architecture, named ToS. While the recent one-stream tracking framework, employing a unified backbone for simultaneous processing of both the template and search region, has demonstrated exceptional efficacy, we find the conventional two-stream tracking framework, which employs two separate backbones for the template and search region, offers inherent advantages. The two-stream tracking framework is more compatible with advanced lightweight backbones and can efficiently utilize benefits from large templates. We demonstrate that the two-stream setup can exceed the one-stream tracking model in both speed and accuracy through strategic designs. Our methodology rejuvenates the two-stream tracking paradigm with lightweight pre-trained backbones and the proposed three efficient strategies: 1) A feature-aggregation module that improves the representation capability of the backbone, 2) A channel-wise approach for feature fusion, presenting a more effective and lighter alternative to spatial concatenation techniques, and 3) An expanded template strategy to boost tracking accuracy with negligible additional computational cost. Extensive evaluations across multiple tracking benchmarks demonstrate that the proposed method sets a new state-of-the-art performance in efficient visual tracking.},
  archive      = {J_TIP},
  author       = {Chengao Zong and Xin Chen and Jie Zhao and Yang Liu and Huchuan Lu and Dong Wang},
  doi          = {10.1109/TIP.2025.3598934},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5500-5512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing the two-stream framework for efficient visual tracking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HDRSL net for accurate high dynamic range imaging-based structured light 3D reconstruction. <em>TIP</em>, <em>34</em>, 5486-5499. (<a href='https://doi.org/10.1109/TIP.2025.3599934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fringe projection profilometry systems, accurately reconstructing 3D objects with varying surface reflectivity requires high dynamic range (HDR) imaging. However, the limited dynamic range of single-exposure cameras poses challenges for capturing HDR fringe patterns efficiently. This paper introduces a deep learning-based HDR structured light 3D reconstruction pipeline, comprising an HDR Fringe Generation Module and a Phase Calculation Module. The HDR Fringe Generation Module employs an end-to-end network with attention guidance and feature distillation to reconstruct HDR fringe images from short- and long-exposure low dynamic range (LDR) inputs. The Phase Calculation Module processes the phase information from HDR fringes to enable 3D reconstruction. On a metallic HDR dataset, the method achieved a phase error of 0.105, comparable to the 4-exposure 6-step Phase Shifting Profilometry (PSP) method (0.069), with only 8.3% of the projection time. Experimental results demonstrate the robustness of our approach under diverse object geometries, exposure levels, and challenging global illumination environments. In quantitative measurements, our method achieved accuracies of sub-50 $\mu $ m on ceramic spheres, flat plates and metal step object. Ablation experiments confirmed that feature distillation and attention module effectively enhance the HDR Fringe Generation Module, producing high-quality HDR fringe patterns critical for reconstructing objects with HDR surface reflectivity. Furthermore, we constructed an HDR imaging metal dataset comprising 1,700 samples of machined metal parts with diverse shapes, sizes, and materials, making it a benchmark in the field of HDR structured light measurement. Our method offers a general HDR imaging-based structured light 3D reconstruction approach, integrating the two modules into an efficient, end-to-end solution for objects with HDR reflective surfaces.},
  archive      = {J_TIP},
  author       = {Hao Wang and Chaobo Zhang and Xiang Qian and Xiaohao Wang and Weihua Gui and Wen Gao and Xiaojun Liang and Xinghui Li},
  doi          = {10.1109/TIP.2025.3599934},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5486-5499},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HDRSL net for accurate high dynamic range imaging-based structured light 3D reconstruction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CompletionMamba: Taming state space model for point cloud completion. <em>TIP</em>, <em>34</em>, 5473-5485. (<a href='https://doi.org/10.1109/TIP.2025.3597041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to reconstruct complete 3D shapes from partial scans. The long-range dependencies between points and shape perception are crucial for this task. While Transformers are effective due to their global processing ability, the quadratic complexity of their attention mechanism makes them unsuitable for long sequences when computational resources are constrained. As an alternative, State Space Models (SSMs) provide a memory-efficient solution for handling long-range dependencies, yet applying them directly to unordered point clouds presents challenges because of their intrinsic causality requirements. Existing methods attempt to address this by sorting points along a single axis. This, however, often overlooks complex causal relationships in 3D space since adjacency relationships based on Euclidean distance between points in the 3D space may not be preserved by this linear arrangement. To overcome this issue, we introduce CompletionMamba, a novel SSM-based network designed to harness SSMs for capturing both global and local dependencies within a point cloud. Initially, the input point cloud is causally structured by rearranging its coordinates. Then, a local SSM framework is proposed that defines neighborhood spaces around each point based on Euclidean distance, enhancing the causal structure. Although local SSM enhances relationships in short and long distance sequences, it still lacks full shape modeling of point cloud. To address this, we propose a novel shape-aware Mamba by integrating the shape code of each 3D shape into the model, enabling shape information propagation to all points. Our experiments show that CompletionMamba achieves state-of-the-art performance on both the MVP and PCN datasets.},
  archive      = {J_TIP},
  author       = {Zhiheng Fu and Jiehua Zhang and Longguang Wang and Lian Xu and Hamid Laga and Yulan Guo and Farid Boussaid and Mohammed Bennamoun},
  doi          = {10.1109/TIP.2025.3597041},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5473-5485},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CompletionMamba: Taming state space model for point cloud completion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multimodal learning via hierarchical fusion architecture search with inconsistency mitigation. <em>TIP</em>, <em>34</em>, 5458-5472. (<a href='https://doi.org/10.1109/TIP.2025.3599673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of effective multimodal feature fusion strategies is the key task for multimodal learning, which often requires huge computational costs with extensive expertise. In this paper, we seek to enhance multimodal learning via hierarchical fusion architecture search with inconsistency mitigation. Different from previous works, our Hierarchical Fusion Multimodal Neural Architecture Search (HF-MNAS) considers the inconsistency in modalities and labels, and fine-grained exploitation in multi-level fusion architectures. Specifically, it disentangles the hierarchical fusion problem into two-level (macro- and micro-level) search spaces. In the macro-level search space, the high-level and low-level features are extracted and then connected in a fine-grained way, where the inconsistency mitigation module is designed to minimize discrepancies between modalities and labels in cell outputs. In the micro-level search space, we find that different intermediate nodes in the cells exhibit different importance degrees. Then, we propose an importance-based node selection mechanism to form the optimal cells for feature fusion. We evaluate HF-MNAS on a series of multimodal classification tasks. Empirical evidence shows that HF-MNAS achieves competitive trade-off performance across accuracy, search time, and inference speed. In particular, HF-MNAS consumes minimal computational cost compared with state-of-the-art MNASs. Furthermore, we theoretically and experimentally verify that the modality-label inconsistency deteriorates the overall fusion performance of models such as accuracy and F1 score, and demonstrate that the proposed inconsistency mitigation module could effectively mitigate this phenomenon.},
  archive      = {J_TIP},
  author       = {Kaifang Long and Guoyang Xie and Lianbo Ma and Qing Li and Min Huang and Jianhui Lv and Zhichao Lu},
  doi          = {10.1109/TIP.2025.3599673},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5458-5472},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing multimodal learning via hierarchical fusion architecture search with inconsistency mitigation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric-aware low-light image and video enhancement via depth guidance. <em>TIP</em>, <em>34</em>, 5442-5457. (<a href='https://doi.org/10.1109/TIP.2025.3597046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-Light Enhancement (LLE) is aimed at improving the quality of photos/videos captured under low-light conditions. It is worth noting that most existing LLE methods do not take advantage of geometric modeling. We believe that incorporating geometric information can enhance LLE performance, as it provides insights into the physical structure of the scene that influences illumination conditions. To address this, we propose a Geometry-Guided Low-Light Enhancement Refine Framework (GG-LLERF) designed to assist low-light enhancement models in learning improved features by integrating geometric priors into the feature representation space. In this paper, we employ depth priors as the geometric representation. Our approach focuses on the integration of depth priors into various LLE frameworks using a unified methodology. This methodology comprises two key novel modules. First, a depth-aware feature extraction module is designed to inject depth priors into the image representation. Then, the Hierarchical Depth-Guided Feature Fusion Module (HDGFFM) is formulated with a cross-domain attention mechanism, which combines depth-aware features with the original image features within LLE models. We conducted extensive experiments on public low-light image and video enhancement benchmarks. The results illustrate that our framework significantly enhances existing LLE methods. The source code and pre-trained models are available at https://github.com/Estheryingqi/GG-LLERF},
  archive      = {J_TIP},
  author       = {Yingqi Lin and Xiaogang Xu and Jiafei Wu and Yan Han and Zhe Liu},
  doi          = {10.1109/TIP.2025.3597046},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5442-5457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometric-aware low-light image and video enhancement via depth guidance},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral information extraction with full resolution from arbitrary photographs. <em>TIP</em>, <em>34</em>, 5429-5441. (<a href='https://doi.org/10.1109/TIP.2025.3597038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because optical spectrometers capture abundant molecular, biological, and physical information beyond images, ongoing efforts focus on both algorithmic and hardware approaches to obtain detailed spectral information. Spectral reconstruction from red-green-blue (RGB) values acquired by conventional trichromatic cameras has been an active area of study. However, the resultant spectral profile is often affected not only by the unknown spectral properties of the sample itself, but also by light conditions, device characteristics, and image file formats. Existing machine learning models for spectral reconstruction are further limited in generalizability due to their reliance on task-specific training data or fixed models. Advanced spectrometer hardware employing sophisticated nanofabricated components also constrains scalability and affordability. Here we introduce a general computational framework, co-designed with spectrally incoherent color reference charts, to recover the spectral information of an arbitrary sample from a single-shot photo in the visible range. The mutual optimization of reference color selection and the computational algorithm eliminates the need for training data or pretrained models. In transmission mode, altered RGB values of reference colors are used to recover the spectral intensity of the sample, achieving spectral resolution comparable to that of scientific spectrometers. In reflection mode, a spectral hypercube of the sample can be constructed from a single-shot photo, analogous to hyperspectral imaging. The reported computational photography spectrometry has the potential to make optical spectroscopy and hyperspectral imaging accessible using off-the-shelf smartphones.},
  archive      = {J_TIP},
  author       = {Semin Kwon and Sang Mok Park and Yuhyun Ji and Haripriya Sakthivel and Jung Woo Leem and Young L. Kim},
  doi          = {10.1109/TIP.2025.3597038},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5429-5441},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral information extraction with full resolution from arbitrary photographs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised medical hyperspectral image segmentation using adversarial consistency constraint learning and cross indication network. <em>TIP</em>, <em>34</em>, 5414-5428. (<a href='https://doi.org/10.1109/TIP.2025.3598499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging technology is considered a new paradigm for high-precision pathological image segmentation due to its ability to obtain spatial and spectral information of the detected object simultaneously. However, due to the time-consuming and laborious manual annotation, precise annotation of medical hyperspectral images is difficult to obtain. Therefore, there is an urgent need for a semi-supervised learning framework that can fully utilize unlabeled data for medical hyperspectral image segmentation. In this work, we propose an adversarial consistency constraint learning cross indication network (ACCL-CINet), which achieves accurate pathological image segmentation through adversarial consistency constraint learning training strategies. The ACCL-CINet comprises a contextual and structural encoder to form the spatial-spectral feature encoding part. The contextual and structural indications are aggregated into features through a cross indication attention module and finally decoded by a pixel decoder to generate prediction results. For the semi-supervised training strategy, a pixel perceptual consistency module encourages the two models to generate consistent and low-entropy predictions. Secondly, a pixel maximum neighborhood probability adversarial constraint strategy is designed, which produces high-quality pseudo labels for cross supervision training. The proposed ACCL-CINet has been rigorously evaluated on both public and private datasets, with experimental results demonstrating that it outperforms state-of-the-art semi-supervised methods. The code is available at: https://github.com/Qugeryolo/ACCL-CINet},
  archive      = {J_TIP},
  author       = {Geng Qin and Huan Liu and Xueyu Zhang and Wei Li and Yuxing Guo and Chuanbin Guo},
  doi          = {10.1109/TIP.2025.3598499},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5414-5428},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised medical hyperspectral image segmentation using adversarial consistency constraint learning and cross indication network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UpGen: Unleashing potential of foundation models for training-free camouflage detection via generative models. <em>TIP</em>, <em>34</em>, 5400-5413. (<a href='https://doi.org/10.1109/TIP.2025.3599101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) aims to segment objects resembling their environment. To address the challenges of extensive annotations and complex optimizations in supervised learning, recent prompt-based segmentation methods excavate insightful prompts from Large Vision-Language Models (LVLMs) and refine them using various foundation models. These are subsequently fed into the Segment Anything Model (SAM) for segmentation. However, due to the hallucinations of LVLMs and insufficient image-prompt interactions during the refinement stage, these prompts often struggle to capture well-established class differentiation and localization of camouflaged objects, resulting in performance degradation. To provide SAM with more informative prompts, we present UpGen, a pipeline that prompts SAM with generative prompts without requiring training, marking a novel integration of generative models with LVLMs. Specifically, we propose the Multi-Student-Single-Teacher (MSST) knowledge integration framework to alleviate hallucinations of LVLMs. This framework integrates insights from multiple sources to enhance the classification of camouflaged objects. To enhance interactions during the prompt refinement stage, we are the first to leverage generative models on real camouflage images to produce SAM-style prompts without fine-tuning. By capitalizing on the unique learning mechanism and structure of generative models, we effectively enable image-prompt interactions and generate highly informative prompts for SAM. Our extensive experiments demonstrate that UpGen outperforms weakly-supervised models and its SAM-based counterparts. We also integrate our framework into existing weakly-supervised methods to generate pseudo-labels, resulting in consistent performance gains. Moreover, with minor adjustments, UpGen shows promising results in open-vocabulary COD, referring COD, salient object detection, marine animal segmentation, and transparent object segmentation.},
  archive      = {J_TIP},
  author       = {Ji Du and Jiesheng Wu and Desheng Kong and Weiyun Liang and Fangwei Hao and Jing Xu and Bin Wang and Guiling Wang and Ping Li},
  doi          = {10.1109/TIP.2025.3599101},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5400-5413},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UpGen: Unleashing potential of foundation models for training-free camouflage detection via generative models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rate-distortion-complexity optimized framework for multi-model image compression. <em>TIP</em>, <em>34</em>, 5385-5399. (<a href='https://doi.org/10.1109/TIP.2025.3598916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned Image Compression (LIC) has experienced rapid growth with the emergence of diverse frameworks. However, the variability in model design and training datasets poses a challenge for the universal application of a single coding model. To address this problem, this paper introduces a pioneering multi-model image coding framework that integrates various image codecs to overcome these limitations. By dynamically allocating codecs to different image regions, our framework optimizes reconstruction quality within the constraints of limited bitrate and decoding time, offering a high-performance, ubiquitous solution for the rate-distortion-complexity trade-off. Our framework features a detailed codec assignment algorithm based on the Simulated Annealing (SA) method, selected for its proven efficacy in managing the discrete and intricate nature of codec assignment optimization. We have implemented a coarse-to-fine strategy, which significantly enhances efficiency. Notably, our framework maintains compatibility with all standard image codecs without necessitating structural modifications. Empirical results indicate that our framework establishes a new standard in LIC, advancing the Pareto frontier for performance-complexity trade-offs. It achieves a significant 70% reduction in decoding time compared to current state-of-the-art methods, without compromising reconstruction quality. Furthermore, under comparable conditions, our approach not only outperforms but significantly eclipses existing Rate-Distortion-Complexity (RDC) optimized codecs, with decoding speeds up to 30 times faster.},
  archive      = {J_TIP},
  author       = {Xinyu Hang and Ziqing Ge and Hongfei Fan and Chuanmin Jia and Siwei Ma and Wen Gao},
  doi          = {10.1109/TIP.2025.3598916},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5385-5399},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rate-distortion-complexity optimized framework for multi-model image compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving video summarization by exploring the coherence between corresponding captions. <em>TIP</em>, <em>34</em>, 5369-5384. (<a href='https://doi.org/10.1109/TIP.2025.3598709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization aims to generate a compact summary of the original video by selecting and combining the most representative parts. Most existing approaches only focus on recognizing key video segments to generate the summary, which lacks holistic considerations. The transitions between selected video segments are usually abrupt and inconsistent, making the summary confusing. Indeed, the coherence of video summaries is crucial to improve the quality and user viewing experience. However, the coherence between video segments is hard to measure and optimize from a pure vision perspective. To this end, we propose a Language-guided Segment Coherence-Aware Network (LS-CAN), which integrates entire coherence considerations into the key segment recognition. The main idea of LS-CAN is to explore the coherence of corresponding text modality to facilitate the entire coherence of the video summary, which leverages the natural property in the language that contextual coherence is easy to measure. In terms of text coherence measures, specifically, we propose the multi-graph correlated neural network module (MGCNN), which constructs a graph for each sentence based on three key components, i.e., subject, attribute, and action words. For each sentence pair, the node features are then discriminatively learned by incorporating neighbors of its own graph and information of its dual graph, reducing the error of synonyms or reference relationships in measuring the correlation between sentences, as well as the error caused by considering each component separately. In doing so, MGCNN utilizes subject agreement, attribute coherence, and action succession to measure text coherence. Besides, with the help of large language models, we augment the original text coherence annotations, improving the ability of MGCNN to judge coherence. Extensive experiments on three challenging datasets demonstrate the superiority of our approach and each proposed module, especially improving the latest records by +3.8%, +14.2% and +12% w.r.t. F1 scores, $\tau $ and $\rho $ metrics on the BLiSS dataset.},
  archive      = {J_TIP},
  author       = {Cheng Ye and Weidong Chen and Bo Hu and Lei Zhang and Yongdong Zhang and Zhendong Mao},
  doi          = {10.1109/TIP.2025.3598709},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5369-5384},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving video summarization by exploring the coherence between corresponding captions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FusionINV: A diffusion-based approach for multimodal image fusion. <em>TIP</em>, <em>34</em>, 5355-5368. (<a href='https://doi.org/10.1109/TIP.2025.3593775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared images exhibit a significantly different appearance compared to visible counterparts. Existing infrared and visible image fusion (IVF) methods fuse features from both infrared and visible images, producing a new “image” appearance not inherently captured by any existing device. From an appearance perspective, infrared, visible, and fused images belong to different data domains. This difference makes it challenging to apply fused images because their domain-specific appearance may be difficult for downstream systems, e.g., pre-trained segmentation models. Therefore, accurately assessing the quality of the fused image is challenging. To address those problem, we propose a novel IVF method, FusionINV, which produces fused images with an appearance similar to visible images. FusionINV employs the pre-trained Stable Diffusion (SD) model to invert infrared images into the noise feature space. To inject visible-style appearance information into the infrared features, we leverage the inverted features from visible images to guide this inversion process. In this way, we can embed all the information of infrared and visible images in the noise feature space, and then use the prior of the pre-trained SD model to generate visually friendly images that align more closely with the RGB distribution. Specially, to generate the fused image, we design a tailored fusion rule within the denoising process that iteratively fuses visible-style infrared and visible features. In this way, the fused image falls into the visible domain and can be directly applied to existing downstream machine systems. Thanks to advancements in image inversion, FusionINV can directly produce fused images in a training-free manner. Extensive experiments demonstrate that FusionINV achieves outstanding performance in both human visual evaluation and machine perception tasks. The code is available at https://github.com/erfect2020/FusionINV},
  archive      = {J_TIP},
  author       = {Pengwei Liang and Junjun Jiang and Qing Ma and Chenyang Wang and Xianming Liu and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3593775},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5355-5368},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FusionINV: A diffusion-based approach for multimodal image fusion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware transformer for referring camouflaged object detection. <em>TIP</em>, <em>34</em>, 5341-5354. (<a href='https://doi.org/10.1109/TIP.2025.3587579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring camouflaged object detection (Ref-COD) is a recently proposed task, aiming to segment specified camouflaged objects by leveraging visual reference, i.e., a small set of referring images with salient target objects. Ref-COD poses a considerable challenge due to the difficulty of discerning camouflaged objects from their highly similar backgrounds, as well as the significant feature differences between the camouflaged objects and the provided visual reference. To tackle the above dilemma, we propose a novel uncertainty-aware transformer for the Ref-COD task, termed UAT. UAT first utilizes a cross-attention mechanism to align and integrate visual reference to guide camouflaged feature learning, and then models dependencies between patches in a probabilistic manner to learn predictive uncertainty and excavate discriminative camouflaged features. Specifically, we first design a referring feature aggregation (RFA) module to align and incorporate referring features with camouflaged features, guiding targeted specific feature learning within the feature space of camouflaged images. Then, to enhance multi-level feature extraction, we develop a cross-attention encoder (CAE) to integrate global information and multi-scale semantics between adjacent layers to excavate critical camouflage cues. More importantly, we propose a transformer probabilistic decoder (TPD) to model the dependencies between patches as Gaussian random variables to capture uncertainty-aware camouflaged features. Extensive experiments on the golden Ref-COD benchmark demonstrate the superiority of UAT over existing state-of-the-art competitors. The proposed UAT also achieves competitive performance on several conventional COD datasets, further demonstrating its scalability. The source code is available at https://github.com/CVL-hub/UAT},
  archive      = {J_TIP},
  author       = {Ranwan Wu and Tian-Zhu Xiang and Guo-Sen Xie and Rongrong Gao and Xiangbo Shu and Fang Zhao and Ling Shao},
  doi          = {10.1109/TIP.2025.3587579},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5341-5354},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty-aware transformer for referring camouflaged object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternating direction unfolding with a cross spectral attention prior for dual-camera compressive hyperspectral imaging. <em>TIP</em>, <em>34</em>, 5325-5340. (<a href='https://doi.org/10.1109/TIP.2025.3597775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coded Aperture Snapshot Spectral Imaging (CASSI) multiplexes 3D Hyperspectral Images (HSIs) into a 2D sensor to capture dynamic spectral scenes, which, however, sacrifices the spatial information. Dual-Camera Compressive Hyperspectral Imaging (DCCHI) enhances CASSI by incorporating a Panchromatic (PAN) camera to compensate for the loss of spatial information in CASSI. However, the dual-camera structure of DCCHI disrupts the diagonal property of the product of the sensing matrix and its transpose, making it difficult to efficiently and accurately solve the data subproblem in closed-form and thereby hindering the application of model-based methods and Deep Unfolding Networks (DUNs) that rely on such a closed-form solution. To address this issue, we propose an Alternating Direction DUN, named ADRNN, which decouples the imaging model of DCCHI into a CASSI subproblem and a PAN subproblem. The ADRNN alternately solves data terms analytically and a joint prior term in these subproblems. Additionally, we propose a Cross Spectral Transformer (XST) to exploit the joint prior. The XST utilizes cross spectral attention to exploit the correlation between the compressed HSI and the PAN image, and incorporates Grouped-Query Attention (GQA) to alleviate the burden of parameters and computational cost brought by impartially treating the compressed HSI and the PAN image. Furthermore, we built a real DCCHI system and captured large-scale indoor and outdoor scenes for future academic research. Extensive experiments on both simulation and real datasets demonstrate that the proposed method achieves state-of-the-art (SOTA) performance. The code and datasets have been open-sourced at: https://github.com/ShawnDong98/ADRNN-XST},
  archive      = {J_TIP},
  author       = {Yubo Dong and Dahua Gao and Danhua Liu and Yanli Liu and Guangming Shi},
  doi          = {10.1109/TIP.2025.3597775},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5325-5340},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Alternating direction unfolding with a cross spectral attention prior for dual-camera compressive hyperspectral imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Color spike camera reconstruction via long short-term temporal aggregation of spike signals. <em>TIP</em>, <em>34</em>, 5312-5324. (<a href='https://doi.org/10.1109/TIP.2025.3595368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of emerging computer vision applications, the demand for capturing dynamic scenes with high-speed motion has increased. A kind of neuromorphic sensor called spike camera shows great potential in this aspect since it generates a stream of binary spikes to describe the dynamic light intensity with a very high temporal resolution. Color spike camera (CSC) was recently invented to capture the color information of dynamic scenes via a color filter array (CFA) on the sensor. This paper proposes a long short-term temporal aggregation strategy of spike signals. First, we utilize short-term temporal correlation to adaptively extract temporal features of each time point. Then we align the features and aggregate them to exploit long-term temporal correlation, suppressing undesired motion blur. To implement the strategy, we design a CSC reconstruction network. Based on adaptive short-term temporal aggregation, we propose a spike representation module to extract temporal features of each color channel, leveraging multiple temporal scales. Considering the long-term temporal correlation, we develop an alignment module to align the temporal features. In particular, we perform motion alignment of red and blue channels with the guidance of the higher-sampling-rate green channel, leveraging motion consistency among color channels. Besides, we propose a module to aggregate the aligned temporal features for the restored color image, which exploits color channel correlation. We have also developed a CSC simulator for data generation. Experimental results demonstrate that our method can restore color images with fine texture details, achieving state-of-the-art CSC reconstruction performance.},
  archive      = {J_TIP},
  author       = {Yanchen Dong and Ruiqin Xiong and Jing Zhao and Xiaopeng Fan and Xinfeng Zhang and Tiejun Huang},
  doi          = {10.1109/TIP.2025.3595368},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5312-5324},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color spike camera reconstruction via long short-term temporal aggregation of spike signals},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA3Attacker: A diffusion-based attacker against aesthetics-oriented black-box models. <em>TIP</em>, <em>34</em>, 5300-5311. (<a href='https://doi.org/10.1109/TIP.2025.3594068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adage “Beautiful Outside But Ugly Inside” resonates with the security and explainability challenges encountered in image aesthetics assessment (IAA). Although deep neural networks (DNNs) have demonstrated remarkable performance in various IAA tasks, how to probe, explain, and enhance aesthetics-oriented “black-box” models has not yet been investigated to our knowledge. This lack of investigation has significantly impeded the commercial application of IAA. In this paper, we investigate the susceptibility of current IAA models to adversarial attacks and aim to elucidate the underlying mechanisms that contribute to their vulnerabilities. To address this, we propose a novel diffusion-based framework as an attacker (DA3Attacker), capable of generating adversarial examples (AEs) to deceive diverse black-box IAA models. DA3Attacker employs a dedicated Attack Diffusion Transformer, equipped with modular aesthetics-oriented filters. By undergoing two unsupervised training stages, it constructs a latent space to generate AEs and facilitates two distinct yet controllable attack modes: restricted and unrestricted. Extensive experiments on 26 baseline models demonstrate that our method effectively explores the vulnerabilities of these IAA models, while also providing multi-attribute explanations for their feature dependencies. To facilitate further research, we contribute the evaluation tools and four metrics for measuring adversarial robustness, as well as a dataset of 60,000 re-labeled AEs for fine-tuning IAA models. The resources are available here.},
  archive      = {J_TIP},
  author       = {Shuai He and Shuntian Zheng and Anlong Ming and Yanni Wang and Huadong Ma},
  doi          = {10.1109/TIP.2025.3594068},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5300-5311},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DA3Attacker: A diffusion-based attacker against aesthetics-oriented black-box models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Propagating sparse depth via depth foundation model for out-of-distribution depth completion. <em>TIP</em>, <em>34</em>, 5285-5299. (<a href='https://doi.org/10.1109/TIP.2025.3597047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is a pivotal challenge in computer vision, aiming at reconstructing the dense depth map from a sparse one, typically with a paired RGB image. Existing learning-based models rely on carefully prepared but limited data, leading to significant performance degradation in out-of-distribution (OOD) scenarios. Recent foundation models have demonstrated exceptional robustness in monocular depth estimation through large-scale training, and using such models to enhance the robustness of depth completion models is a promising solution. In this work, we propose a novel depth completion framework that leverages depth foundation models to attain remarkable robustness without large-scale training. Specifically, we leverage a depth foundation model to extract environmental cues, including structural and semantic context, from RGB images to guide the propagation of sparse depth information into missing regions. We further design a dual-space propagation approach, without any learnable parameters, to effectively propagate sparse depth in both 3D and 2D spaces to maintain geometric structure and local consistency. To refine the intricate structure, we introduce a learnable correction module to progressively adjust the depth prediction towards the real depth. We train our model on the NYUv2 and KITTI datasets as in-distribution datasets and extensively evaluate the framework on 16 other datasets. Our framework performs remarkably well in the OOD scenarios and outperforms existing state-of-the-art depth completion methods. Our models are released in https://github.com/shenglunch/PSD.},
  archive      = {J_TIP},
  author       = {Shenglun Chen and Xinzhu Ma and Hong Zhang and Haojie Li and Zhihui Wang},
  doi          = {10.1109/TIP.2025.3597047},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5285-5299},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Propagating sparse depth via depth foundation model for out-of-distribution depth completion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confound controlled multimodal neuroimaging data fusion and its application to developmental disorders. <em>TIP</em>, <em>34</em>, 5271-5284. (<a href='https://doi.org/10.1109/TIP.2025.3597045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion provides multiple benefits over single modality analysis by leveraging both shared and complementary information from different modalities. Notably, supervised fusion enjoys extensive interest for capturing multimodal co-varying patterns associated with clinical measures. A key challenge of brain data analysis is how to handle confounds, which, if unaddressed, can lead to an unrealistic description of the relationship between the brain and clinical measures. Current approaches often rely on linear regression to remove covariate effects prior to fusion, which may lead to information loss, rather than pursue the more global strategy of optimizing both fusion and covariates removal simultaneously. Thus, we propose “CR-mCCAR” to jointly optimize for confounds within a guided fusion model, capturing co-varying multimodal patterns associated with a specific clinical domain while also discounting covariate effects. Simulations show that CR-mCCAR separate the reference and covariate factors accurately. Functional and structural neuroimaging data fusion reveals co-varying patterns in attention deficit/hyperactivity disorder (ADHD, striato-thalamo-cortical and salience areas) and in autism spectrum disorder (ASD, salience and fronto-temporal areas) that link with core symptoms but uncorrelate with age and motion. These results replicate in an independent cohort. Downstream classification accuracy between ADHD/ASD and controls is markedly higher for CR-mCCAR compared to fusion and regression separately. CR-mCCAR can be extended to include multiple targets and multiple covariates. Overall, results demonstrate CR-mCCAR can jointly optimize for target components that correlate with the reference(s) while removing nuisance covariates. This approach can improve the meaningful detection of reliable phenotype-linked multimodal biomarkers for brain disorders.},
  archive      = {J_TIP},
  author       = {Chuang Liang and Rogers F. Silva and Tülay Adali and Rongtao Jiang and Daoqiang Zhang and Shile Qi and Vince D. Calhoun},
  doi          = {10.1109/TIP.2025.3597045},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5271-5284},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Confound controlled multimodal neuroimaging data fusion and its application to developmental disorders},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized restoration via dual-pivot tuning. <em>TIP</em>, <em>34</em>, 5257-5270. (<a href='https://doi.org/10.1109/TIP.2025.3586141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models can serve as priors, ensuring that image restoration solutions adhere to natural image manifolds. For facial images, however, personalized priors are essential to accurately reconstruct individual-specific facial features. We propose Dual-Pivot Tuning — a simple yet effective two-stage approach to personalize blind restoration systems while preserving general prior integrity. Our key observation is that for efficient personalization, the diffusion model should be tuned around a fixed textual pivot in the first step, while in the second step a guiding network should be tuned in a generic (non-personalized) manner, using the personalized diffusion model as a fixed “pivot”. This approach ensures that personalization does not interfere with the restoration process, producing results with a natural appearance that show high fidelity to both identity and degraded image attributes. We conducted extensive experiments with images of widely recognized individuals, evaluating our approach both qualitatively and quantitatively against relevant baselines. Notably, our personalized prior not only achieves superior identity fidelity, but also outperforms state-of-the-art generic priors in terms of overall image quality. Project webpage is https://personalized-restoration.github.io/ and code is available at https://github.com/personalized-restoration/personalized-restoration},
  archive      = {J_TIP},
  author       = {Pradyumna Chari and Sizhuo Ma and Daniil Ostashev and Achuta Kadambi and Gurunandan Krishnan and Jian Wang and Kfir Aberman},
  doi          = {10.1109/TIP.2025.3586141},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5257-5270},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Personalized restoration via dual-pivot tuning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parts2Whole: Generalizable multi-part portrait customization. <em>TIP</em>, <em>34</em>, 5241-5256. (<a href='https://doi.org/10.1109/TIP.2025.3597037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-part portrait customization aims to generate realistic human images by assembling specified body parts from multiple reference images, with significant applications in digital human creation. Existing customization methods typically follow two approaches: 1) test-time fine-tuning, which learn concepts effectively but is time-consuming and struggles with multi-part composition; 2) generalizable feed-forward methods, which offer efficiency but lack fine control over appearance specifics. To address these limitations, we present Parts2Whole, a diffusion-based generalizable portrait generator that harmoniously integrates multiple reference parts into high-fidelity human images by our proposed multi-reference mechanism. To adequately characterize each part, we propose a detail-aware appearance encoder, which is initialized and inherits powerful image priors from the pre-trained denoising U-Net, enabling the encoding of detailed information from reference images. The extracted features are incorporated into the denoising U-Net by a shared self-attention mechanism, enhanced by mask information for precise part selection. Additionally, we integrate pose map conditioning to control the target posture of generated portraits, facilitating more flexible customization. Extensive experiments demonstrate the superiority of our approach over existing methods and applicability to related tasks like pose transfer and pose-guided human image generation, showcasing its versatile conditioning. Our project is available at https://huanngzh.github.io/Parts2Whole/},
  archive      = {J_TIP},
  author       = {Hongxing Fan and Zehuan Huang and Lipeng Wang and Haohua Chen and Li Yin and Lu Sheng},
  doi          = {10.1109/TIP.2025.3597037},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5241-5256},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parts2Whole: Generalizable multi-part portrait customization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PalmDiff: When palmprint generation meets controllable diffusion model. <em>TIP</em>, <em>34</em>, 5228-5240. (<a href='https://doi.org/10.1109/TIP.2025.3593974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its distinctive texture and intricate details, palmprint has emerged as a critical modality in biometric identity recognition. The absence of large-scale public palmprint datasets has substantially impeded the advancement of palmprint research, resulting in inadequate accuracy in commercial palmprint recognition systems. However, existing generative methods exhibit insufficient generalization, as the images they generate differ in specific ways from the conditional images. This paper proposes a method for generating palmprint images using a controllable diffusion model (PalmDiff), which addresses the issue of insufficient datasets by generating palmprint data, improving the accuracy of palmprint recognition. We introduce a diffusion process that effectively tackles the problems of excessive noise and loss of texture details commonly encountered in diffusion models. A linear attention mechanism is employed to enhance the backbone’s expressive capacity and reduce the computational complexity. To this end, we proposed an ID loss function to enable the diffusion model to generate palmprint images under the same identical space consistently. PalmDiff is compared with other generation methods in terms of both image quality and the enhancement of palmprint recognition performance. Experiments show that PalmDiff performs well in image generation, with an FID score of 13.311 on MPD and 18.434 on Tongji. Besides, PalmDiff has significantly improved various backbones for palmprint recognition compared to other generation methods.},
  archive      = {J_TIP},
  author       = {Long Tang and Tingting Chai and Zheng Zhang and Miao Zhang and Xiangqian Wu},
  doi          = {10.1109/TIP.2025.3593974},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5228-5240},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PalmDiff: When palmprint generation meets controllable diffusion model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupling discriminative attributes for few-shot fine-grained recognition. <em>TIP</em>, <em>34</em>, 5215-5227. (<a href='https://doi.org/10.1109/TIP.2025.3573498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot fine-tuning of pre-trained vision-language models (VLMs) for downstream tasks has gained widespread attention for reducing data annotation efforts while maintaining high performance. However, we observe that VLMs excel in excluding most incorrect classes in fine-grained recognition tasks, but struggles with a small set of confusing categories, which are typically highly similar subspecies. Existing few-shot fine-tuning methods attempt to directly recognize the correct category among all predefined classes, limiting their ability to capture discriminative features for those confusing categories. This raises an intriguing question: Can we specifically extract useful information from confusing classes to enhance fine-grained recognition performance? Based on this insight, we propose a hierarchical few-shot fine-tuning framework to address the severe confusion problem while ensuring the interpretability, namely Attribute-Decoupled Discriminator (AttrDD). Instead of thinking once among all classes, AttrDD employs a two-stage recognition, “think through” then “think smart”. Specifically, in the first phase, a representative VLM, CLIP, is fine-tuned to select the Top-K confusing classes. In the second phase, we leverage the knowledge of large language models (LLMs) to generate fixed format descriptions of attribute differences between these confusing classes via in-context learning. Attribute-decoupled classifications are then conducted to capture fine-grained discriminative features. To achieve parameter-efficient fine-tuning, we introduce a lightweight attention adapter for each phase to align image features with task-specific textual features and LLM-generated textual features. Extensive experiments on 9 fine-grained recognition benchmarks demonstrate that AttrDD consistently outperforms existing baselines by wide margins.},
  archive      = {J_TIP},
  author       = {Yehao Lu and Chaoxiang Cai and Wei Su and Guangcong Zheng and Wenjie Wang and Xuewei Li and Xi Li},
  doi          = {10.1109/TIP.2025.3573498},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5215-5227},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Decoupling discriminative attributes for few-shot fine-grained recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CloCap-GS: Clothed human performance capture with 3D gaussian splatting. <em>TIP</em>, <em>34</em>, 5200-5214. (<a href='https://doi.org/10.1109/TIP.2025.3592534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the human body and clothing from videos has obtained significant progress in recent years, but several challenges remain to be addressed. Previous methods reconstruct the 3D bodies and garments from videos with self-rotating human motions or capture the body and clothing separately based on neural implicit fields. However, the reconstruction methods for self-rotating motions may cause instable tracking on dynamic videos with arbitrary human motions, while implicit fields based methods are limited to inefficient rendering and low quality synthesis. To solve these problems, we propose a new method, called CloCap-GS, for clothed human performance capture with 3D Gaussian Splatting. Specifically, we align 3D Gaussians with the deforming geometries of body and clothing, and leverage photometric constraints formed by matching Gaussians renderings with input video frames to recover temporal deformations of the dense template geometry. The geometry deformations and Gaussians properties of both the body and clothing are optimized jointly, achieving both dense geometry tracking and novel-view synthesis. In addition, we introduce a physics-aware material-varying cloth model to preserve physically-plausible cloth dynamics and body-clothing interactions that is pre-trained in a self-supervised manner without preparing training data. Compared with the existing methods, our method improves the accuracy of dense geometry tracking and quality of novel-view synthesis for a variety of daily garment types (e.g., loose clothes). Extensive experiments in both quantitative and qualitative evaluations demonstrate the effectiveness of CloCap-GS on real sparse-view or monocular videos.},
  archive      = {J_TIP},
  author       = {Kangkan Wang and Chong Wang and Jian Yang and Guofeng Zhang},
  doi          = {10.1109/TIP.2025.3592534},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5200-5214},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CloCap-GS: Clothed human performance capture with 3D gaussian splatting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gating model for bias calibration in generalized zero-shot learning. <em>TIP</em>, <em>34</em>, 5172-5183. (<a href='https://doi.org/10.1109/TIP.2022.3153138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) aims at training a model that can generalize to unseen class data by only using auxiliary information. One of the main challenges in GZSL is a biased model prediction toward seen classes caused by overfitting on only available seen class data during training. To overcome this issue, we propose a two-stream autoencoder-based gating model for GZSL. Our gating model predicts whether the query data is from seen classes or unseen classes, and utilizes separate seen and unseen experts to predict the class independently from each other. This framework avoids comparing the biased prediction scores for seen classes with the prediction scores for unseen classes. In particular, we measure the distance between visual and attribute representations in the latent space and the cross-reconstruction space of the autoencoder. These distances are utilized as complementary features to characterize unseen classes at different levels of data abstraction. Also, the two-stream autoencoder works as a unified framework for the gating model and the unseen expert, which makes the proposed method computationally efficient. We validate our proposed method in four benchmark image recognition datasets. In comparison with other state-of-the-art methods, we achieve the best harmonic mean accuracy in SUN and AWA2, and the second best in CUB and AWA1. Furthermore, our base model requires at least 20% less number of model parameters than state-of-the-art methods relying on generative models.},
  archive      = {J_TIP},
  author       = {Gukyeong Kwon and Ghassan AlRegib},
  doi          = {10.1109/TIP.2022.3153138},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5172-5183},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A gating model for bias calibration in generalized zero-shot learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetric and discrete self-representation enhancement hashing for cross-domain retrieval. <em>TIP</em>, <em>34</em>, 5158-5171. (<a href='https://doi.org/10.1109/TIP.2025.3594140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the characteristics of low storage requirement and high retrieval efficiency, hashing-based retrieval has shown its great potential and has been widely applied for information retrieval. However, retrieval tasks in real-world applications are usually required to handle the data from various domains, leading to the unsatisfactory performances of existing hashing-based methods, as most of them assuming that the retrieval pool and the querying set are similar. Most of the existing works overlooked the self-representation that containing the modality-specific semantic information, in the cross-modal data. To cope with the challenges mentioned above, this paper proposes an asymmetric and discrete self-representation enhancement hashing (ADSEH) for cross-domain retrieval. Specifically, ADSEH aligns the mathematical distribution with domain adaptation for cross-domain data, by exploiting the correlation of minimizing the distribution mismatch to reduce the heterogeneous semantic gaps. Then, ADSEH learns the self-representation which is embedded into the generated hash codes, for enhancing the semantic relevance, improving the quality of hash codes, and boosting the generalization ability of ADSEH. Finally, the heterogeneous semantic gaps are further reduced by the log-likelihood similarity preserving for the cross-domain data. Experimental results demonstrate that ADSEH can outperform some SOTA baseline methods on four widely used datasets.},
  archive      = {J_TIP},
  author       = {Jiaxing Li and Lin Jiang and Xiaozhao Fang and Shengli Xie and Yong Xu},
  doi          = {10.1109/TIP.2025.3594140},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5158-5171},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Asymmetric and discrete self-representation enhancement hashing for cross-domain retrieval},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing text-based person retrieval by combining fused representation and reciprocal learning with adaptive loss refinement. <em>TIP</em>, <em>34</em>, 5147-5157. (<a href='https://doi.org/10.1109/TIP.2025.3594880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person retrieval is defined as the challenging task of searching for people’s images based on given textual queries in natural language. Conventional methods primarily use deep neural networks to understand the relationship between visual and textual data, creating a shared feature space for cross-modal matching. The absence of awareness regarding variations in feature granularity between the two modalities, coupled with the diverse poses and viewing angles of images corresponding to the same individual, may lead to overlooking significant differences within each modality and across modalities, despite notable enhancements. Furthermore, the inconsistency in caption queries in large public datasets presents an additional obstacle to cross-modality mapping learning. Therefore, we introduce 3RTPR, a novel text-based person retrieval method that integrates a representation fusing mechanism and an adaptive loss refinement algorithm into a dual-encoder branch architecture. Moreover, we propose training two independent models simultaneously, which reciprocally support each other to enhance learning effectiveness. Consequently, our approach encompasses three significant contributions: (i) proposing a fused representation method to generate more discriminative representations for images and captions; (ii) introducing a novel algorithm to adjust loss and prioritize samples that contain valuable information; and (iii) proposing reciprocal learning involving a pair of independent models, which allows us to enhance general retrieval performance. In order to validate our method’s effectiveness, we also demonstrate superior performance over state-of-the-art methods by performing rigorous experiments on three well-known benchmarks: CUHK-PEDES, ICFG-PEDES, and RSTPReid.},
  archive      = {J_TIP},
  author       = {Anh D. Nguyen and Hoa N. Nguyen},
  doi          = {10.1109/TIP.2025.3594880},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5147-5157},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing text-based person retrieval by combining fused representation and reciprocal learning with adaptive loss refinement},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAN: Cascade augmentations against noise for image restoration. <em>TIP</em>, <em>34</em>, 5131-5146. (<a href='https://doi.org/10.1109/TIP.2025.3595374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration aims to recover the latent clean image from a degraded counterpart. In general, the prevailing state-of-the-art image restoration methods concentrate on solving only a specific degradation type according to the task, e.g., deblurring or deraining. However, if the corresponding well-trained frameworks confront other real-world image corruptions, i.e., the corruptions are not covered in the training phase, and state-of-the-art restoration models will suffer from a lack of generalization ability. We have observed that an image restoration model can be easily confused by noise corruption. Towards improving the robustness of image restoration networks, in this paper, we focus on alleviating the corruption of noise in various image restoration tasks, which is almost inevitable in real-world scenes. To this end, we devise a novel Cascade Augmentation strategy against Noise (CAN) to enhance the robustness of specific image restoration. Specifically, the given degraded images are sequentially augmented from different perspectives, i.e., noise-aware augmentation and model-aware augmentation. The noise-aware augmentation is proposed to enrich the samples by introducing various noise operations. Moreover, to adapt to more unknown corruptions, we propose a novel model-aware augmentation mechanism, which enhances the scalability by exploring useful both spatial and frequency clues with the help of model randomness. It is worth noting that the proposed augmentation scheme is model-agnostic, and it can plug and play into arbitrary state-of-the-art image restoration architectures. In addition, we construct noise corruption benchmark datasets, derived from the validation set of standard image restoration datasets, to assist us in evaluating the robustness of restoration networks. Extensive quantitative and qualitative evaluations demonstrate that the proposed method has strong generalization capability, which can enhance the robustness of various image restoration frameworks when facing diverse noises.},
  archive      = {J_TIP},
  author       = {Yanyang Yan and Siyuan Yao and Wenqi Ren and Rui Zhang and Qi Guo and Xiaochun Cao},
  doi          = {10.1109/TIP.2025.3595374},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5131-5146},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CAN: Cascade augmentations against noise for image restoration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference hashing for long-tailed image retrieval. <em>TIP</em>, <em>34</em>, 5099-5114. (<a href='https://doi.org/10.1109/TIP.2025.3588054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hashing-based long-tailed image retrieval, the dominance of data-rich head classes often hinders the learning of effective hash codes for data-poor tail classes due to inherent long-tailed bias. Interestingly, this bias also contains valuable prior knowledge by revealing inter-class dependencies, which can be beneficial for hash learning. However, previous methods have not thoroughly analyzed this tangled negative and positive effects of long-tailed bias from a causal inference perspective. In this paper, we propose a novel hash framework that employs causal inference to disentangle detrimental bias effects from beneficial ones. To capture good bias in long-tailed datasets, we construct hash mediators that conserve valuable prior knowledge from class centers. Furthermore, we propose a de-biased hash loss To enhance the beneficial bias effects while mitigating adverse ones, leading to more discriminative hash codes. Specifically, this loss function leverages the beneficial bias captured by hash mediators to support accurate class label prediction, while mitigating harmful bias by blocking its causal path to the hash codes and refining predictions through backdoor adjustment. Extensive experimental results on four widely used datasets demonstrate that the proposed method improves retrieval performance against the state-of-the-art methods by large margins. The source code is available at https://github.com/IMAG-LuJin/CIH},
  archive      = {J_TIP},
  author       = {Lu Jin and Zhengyun Lu and Zechao Li and Yonghua Pan and Longquan Dai and Jinhui Tang and Ramesh Jain},
  doi          = {10.1109/TIP.2025.3588054},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5099-5114},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Causal inference hashing for long-tailed image retrieval},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplifying scalable subspace clustering and its multi-view extension by anchor-to-sample kernel. <em>TIP</em>, <em>34</em>, 5084-5098. (<a href='https://doi.org/10.1109/TIP.2025.3593057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we all known, sparse subspace learning can provide good input for spectral clustering, thereby producing high-quality cluster partitioning. However, it employs complete samples as the dictionary for representation learning, resulting in non-negligible computational costs. Therefore, replacing the complete samples with representative ones (anchors) as the dictionary has become a more popular choice, giving rise to a series of related works. Unfortunately, although these works are linear with respect to the number of samples, they are often quadratic or even cubic with respect to the number of anchors. In this paper, we derive a simpler problem to replace the original scalable subspace clustering, whose properties are utilized. This new problem is linear with respect to both the number of samples and anchors, further enhancing scalability and providing more efficient operations. Furthermore, thanks to the new problem formulation, we can adopt a separate fusion strategy for multi-view extensions. This strategy can better measure the inter-view difference and avoid alternate optimization, so as to achieve more robust and efficient multi-view clustering. Finally, comprehensive experiments demonstrate that our methods not only significantly reduce time overhead but also exhibit superior performance.},
  archive      = {J_TIP},
  author       = {Zhoumin Lu and Feiping Nie and Linru Ma and Rong Wang and Xuelong Li},
  doi          = {10.1109/TIP.2025.3593057},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5084-5098},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simplifying scalable subspace clustering and its multi-view extension by anchor-to-sample kernel},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Escaping modal interactions: An efficient DESANet for multi-modal object re-identification. <em>TIP</em>, <em>34</em>, 5068-5083. (<a href='https://doi.org/10.1109/TIP.2025.3592575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal object Re-ID aims to leverage the complementary information provided by multiple modalities to overcome challenging conditions and achieve high-quality object matching. However, existing multi-modal methods typically rely on various modality interaction modules for information fusion, which can reduce the efficiency of real-time monitoring systems. Additionally, practical challenges such as low-quality multi-modal data or missing modalities further complicate the application of object Re-ID. To address these issues, we propose the Complementary Data Enhancement and Modal-Aware Soft Alignment Network (DESANet), which is designed to be independent of interactive networks and adaptable to scenarios with missing modalities. This approach ensures a simple-yet-effective, and efficient multi-modal object Re-ID. DESANet consists of three key components: Firstly, the Dual-Color Space Data Enhancement (DCDE) module, which enhances multi-modal data by performing patch rotation in the RGB space and improving image quality in the HSV space. Secondly, the Salient Feature ReConstruction (SFRC) module, which addresses the issue of missing modalities by reconstructing features from one modality using the other two. Thirdly, the Modal-Aware Soft Alignment (MASA) module, which integrates multi-source data to avoid the blind fusion of features and prevents the propagation of noise from reconstructed modalities. Our approach achieves state-of-the-art performances on both person and vehicle datasets. Source code is available at https://github.com/DWJ11/DESANet},
  archive      = {J_TIP},
  author       = {Wenjiao Dong and Xi Yang and De Cheng and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3592575},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5068-5083},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Escaping modal interactions: An efficient DESANet for multi-modal object re-identification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contour flow constraint: Preserving global shape similarity for deep learning-based image segmentation. <em>TIP</em>, <em>34</em>, 5054-5067. (<a href='https://doi.org/10.1109/TIP.2025.3592545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For effective image segmentation, it is crucial to employ constraints informed by prior knowledge about the characteristics of the areas to be segmented to yield favorable segmentation outcomes. However, the existing methods have primarily focused on priors of specific properties or shapes, lacking consideration of the general global shape similarity from a Contour Flow perspective. Furthermore, naturally integrating this contour flow prior image segmentation model into the activation functions of deep convolutional networks through mathematical methods is currently unexplored. In this paper, we establish a concept of global shape similarity based on the premise that two shapes exhibit comparable contours. Furthermore, we mathematically derive a contour flow constraint that ensures the preservation of global shape similarity. We propose two implementations to integrate the constraint with deep neural networks. Firstly, the constraint is converted to a shape loss, which can be seamlessly incorporated into the training phase for any learning-based segmentation framework. Secondly, we add the constraint into a variational segmentation model and derive its iterative schemes for solution. The scheme is then unrolled to get the architecture of the proposed CFSSnet. Validation experiments on diverse datasets are conducted on classic benchmark deep network segmentation models. The results indicate a great improvement in segmentation accuracy and shape similarity for the proposed shape loss, showcasing the general adaptability of the proposed loss term regardless of specific network architectures. CFSSnet shows robustness in segmenting noise-contaminated images, and inherent capability to preserve global shape similarity.},
  archive      = {J_TIP},
  author       = {Shengzhe Chen and Zhaoxuan Dong and Jun Liu},
  doi          = {10.1109/TIP.2025.3592545},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5054-5067},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contour flow constraint: Preserving global shape similarity for deep learning-based image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fourier boundary features network with wider catchers for glass segmentation. <em>TIP</em>, <em>34</em>, 5038-5053. (<a href='https://doi.org/10.1109/TIP.2025.3592522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glass largely blurs the boundary between the real world and the reflection. The special transmittance and reflectance quality have confused the semantic tasks related to machine vision. Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass. We propose the Fourier Boundary Features Network with Wider Catchers (FBWC), which might represent the first attempt to utilize sufficiently wide horizontal shallow branches without vertical deepening for guiding the fine granularity segmentation boundary through primary glass semantic information. Specifically, we design the Wider Coarse-Catchers (WCC) for anchoring large area segmentation and reducing excessive extraction from a structural perspective. We embed fine-grained features by Cross Transpose Attention (CTA), which is introduced to avoid the incomplete area within the boundary caused by reflection noise. For excavating glass features and balancing high-low layers context, a learnable Fourier Convolution Controller (FCC) is proposed to regulate information integration robustly. The proposed method is validated on three different public glass segmentation datasets. Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art (SOTA) methods in glass image segmentation.},
  archive      = {J_TIP},
  author       = {Xiaolin Qin and Jiacen Liu and Qianlei Wang and Shaolin Zhang and Fei Zhu and Zhang Yi},
  doi          = {10.1109/TIP.2025.3592522},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5038-5053},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fourier boundary features network with wider catchers for glass segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard EXIF: Protecting image authorship through metadata, hardware, and content. <em>TIP</em>, <em>34</em>, 5023-5037. (<a href='https://doi.org/10.1109/TIP.2025.3593911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of digital image content and advancements in image editing technologies, the protection of digital image authorship has become an increasingly important issue. Traditional methods for authorship protection include registering authorship through certification organization, utilizing image metadata such as Exchangeable Image File Format (EXIF) data, and employing watermarking techniques to prove ownership. In recent years, blockchain-based technologies have also been introduced to enhance authorship protection further. However, these approaches face challenges in balancing four key attributes: strong legal validity, high security, low cost, and high usability. Authorship registration is often cumbersome, EXIF metadata can be easily extracted and tampered with, watermarking techniques are vulnerable to various forms of attack, and blockchain technology is complex to implement and requires long-term maintenance. In response to these challenges, this paper introduces a new framework Hard EXIF, designed to balance these multiple attributes while delivering improved performance. The proposed method integrates metadata with physically unclonable functions (PUFs) for the first time, creating unique device fingerprints and embedding them into images using watermarking techniques. By leveraging the security and simplicity of hash functions and PUFs, this method enhances EXIF security while minimizing costs. Experimental results demonstrate that the Hard EXIF framework achieves an average peak signal-to-noise ratio (PSNR) of 42.89 dB, with a similarity of 99.46% between the original and watermarked images, and the extraction error rate is only 0.0017. These results show that the Hard EXIF framework balances legal validity, security, cost, and usability, promising authorship protection with great potential for wider application.},
  archive      = {J_TIP},
  author       = {Yushu Zhang and Bowen Shi and Shuren Qi and Xiangli Xiao and Ping Wang and Wenying Wen},
  doi          = {10.1109/TIP.2025.3593911},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5023-5037},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hard EXIF: Protecting image authorship through metadata, hardware, and content},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial domain adaptation via importance sampling-based shift correction. <em>TIP</em>, <em>34</em>, 5009-5022. (<a href='https://doi.org/10.1109/TIP.2025.3593115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial domain adaptation (PDA) is a challenging task in real-world machine learning scenarios. It aims to transfer knowledge from a labeled source domain to a related unlabeled target domain, where the support set of the source label distribution subsumes the target one. Previous PDA works managed to correct the label distribution shift by weighting samples in the source domain. However, the simple reweighing technique cannot explore the latent structure and sufficiently use the labeled data, and then models are prone to over-fitting on the source domain. In this work, we propose a novel importance sampling-based shift correction (IS2C) method, where new labeled data are sampled from a built sampling domain, whose label distribution is supposed to be the same as the target domain, to characterize the latent structure and enhance the generalization ability of the model. We provide theoretical guarantees for IS2C by proving that the generalization error can be sufficiently dominated by IS2C. In particular, by implementing sampling with the mixture distribution, the extent of shift between source and sampling domains can be connected to generalization error, which provides an interpretable way to build IS2C. To improve knowledge transfer, an optimal transport-based independence criterion is proposed for conditional distribution alignment, where the computation of the criterion can be adjusted to reduce the complexity from $\mathcal {O}(n^{3})$ to $\mathcal {O}(n^{2})$ in realistic PDA scenarios. Extensive experiments on PDA benchmarks validate the theoretical results and demonstrate the effectiveness of our IS2C over existing methods.},
  archive      = {J_TIP},
  author       = {Cheng-Jun Guo and Chuan-Xian Ren and You-Wei Luo and Xiao-Lin Xu and Hong Yan},
  doi          = {10.1109/TIP.2025.3593115},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5009-5022},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Partial domain adaptation via importance sampling-based shift correction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep ensemble model for quantitative optical property and chromophore concentration images of biological tissues. <em>TIP</em>, <em>34</em>, 4999-5008. (<a href='https://doi.org/10.1109/TIP.2025.3593071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to quantify widefield tissue optical properties (OPs, i.e., absorption and scattering) has major implications on the characterization of various physiological and disease processes. However, conventional image processing methods for tissue optical properties are either limited to qualitative analysis, or have tradeoffs in speed and accuracy. The key to quantification of optical properties is the extraction of amplitude maps from reflectance images under sinusoidal illumination of different spatial frequencies. Conventional three-phase demodulation (TPD) method has been demonstrated for the mapping of OPs, but it requires as many as 14 measurement images for accurate OP extraction, which leads to limited throughput and hinders practical translation. Although single-phase demodulation (SPD) method has been proposed to map OPs with a single measurement image, it is typically subject to image artifacts and decreased measurement accuracy. To tackle those challenges, here we develop a deep ensemble model (DEM) that can map tissue optical properties with high accuracy in a single snapshot, increasing the measurement speed by $14\times $ compared to conventional TPD method. The proposed method was validated with measurements on an array of optical phantoms, ex vivo tissues, and in vivo tissues. The errors for OP extraction were $0.83~\pm ~5.0$ % for absorption and $0.40~\pm ~1.9$ % for reduced scattering, dramatically lower than that of the state-of-the-art SPD method ( $2.5~\pm ~15$ % for absorption and - $1.2~\pm ~11$ % for reduced scattering). It was further demonstrated that while trained with data from a single wavelength, the DEM can be directly applied to other wavelengths and effectively obtain optical property and chromophore concentration images of biological tissues. Together, these results highlight the potential of DEM to enable new capabilities for quantitative monitoring of tissue physiological and disease processes.},
  archive      = {J_TIP},
  author       = {Bingbao Yan and Bowen Song and Chang Ge and Xinman Yin and Wenchao Jia and Gen Mu and Yanyu Zhao},
  doi          = {10.1109/TIP.2025.3593071},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4999-5008},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep ensemble model for quantitative optical property and chromophore concentration images of biological tissues},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subjective and objective quality assessment of banding artifacts on compressed videos. <em>TIP</em>, <em>34</em>, 4983-4998. (<a href='https://doi.org/10.1109/TIP.2025.3592543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there have been notable advancements in video compression technologies in recent years, banding artifacts remain a serious issue affecting the quality of compressed videos, particularly on smooth regions of high-definition videos. Noticeable banding artifacts can severely impact the perceptual quality of videos viewed on a high-end HDTV or high-resolution screen. Hence, there is a pressing need for a systematic investigation of the banding video quality assessment problem for advanced video codecs. Given that the existing publicly available datasets for studying banding artifacts are limited to still picture data only, which cannot account for temporal banding dynamics, we have created a first-of-a-kind open video dataset, dubbed LIVE-YT-Banding, which consists of 160 videos generated by four different compression parameters using the AV1 video codec. A total of 7,200 subjective opinions are collected from a cohort of 45 human subjects. To demonstrate the value of this new resources, we tested and compared a variety of models that detect banding occurrences, and measure their impact on perceived quality. Among these, we introduce an effective and efficient new no-reference (NR) video quality evaluator which we call CBAND. CBAND leverages the properties of the learned statistics of natural images expressed in the embeddings of deep neural networks. Our experimental results show that the perceptual banding prediction performance of CBAND significantly exceeds that of previous state-of-the-art models, and is also orders of magnitude faster. Moreover, CBAND can be employed as a differentiable loss function to optimize video debanding models. The LIVE-YT-Banding database, code, and pre-trained model are all publically available at https://github.com/uniqzheng/CBAND.},
  archive      = {J_TIP},
  author       = {Qi Zheng and Li-Heng Chen and Chenlong He and Neil Birkbeck and Yilin Wang and Balu Adsumilli and Alan C. Bovik and Yibo Fan and Zhengzhong Tu},
  doi          = {10.1109/TIP.2025.3592543},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4983-4998},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Subjective and objective quality assessment of banding artifacts on compressed videos},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-to-image bayesian flow networks with structurally informative priors. <em>TIP</em>, <em>34</em>, 4968-4982. (<a href='https://doi.org/10.1109/TIP.2025.3592546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models represented by diffusion models have recently shown great potential in image generation. They usually use a reverse iteration process to map noise into the data. However, for many real-world applications such as image restoration and translation, the model input comes from a distribution that is not random noise, making it difficult for these models to adapt directly to these tasks. In this paper, we introduce Image-to-Image Bayesian Flow Networks (I2I-BFNs), a novel framework for general-purpose image-to-image translation (I2I) that operates within the parameter space of distributions. This method upholds Gaussian distributions over pixel intensities, refining distribution parameters through closed-form Bayesian inference, steered by the network’s predictions for the target image. An essential aspect of our approach is the utilization of the conditional image as a robust prior parameter, initializing the translation process from a deterministic, clean image to reduce variance and produce interpretable generation. Additionally, we introduce a skip sampling technique that enhances the efficiency of I2I-BFNs, facilitating rapid translation in diverse image restoration and general I2I tasks. Our experimental evaluations showcase the model’s competitive edge in various settings, underscoring its efficacy and adaptability. This work contributes new insights and opportunities for the large-scale development of efficient conditional generation systems.},
  archive      = {J_TIP},
  author       = {Hongkun Dou and Jinyang Du and Xingyu Jiang and Hongjue Li and Wen Yao and Yue Deng},
  doi          = {10.1109/TIP.2025.3592546},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4968-4982},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image-to-image bayesian flow networks with structurally informative priors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive contour detection of sparse-structured objects in the alpha-shape scale space. <em>TIP</em>, <em>34</em>, 4955-4967. (<a href='https://doi.org/10.1109/TIP.2025.3592862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce cognitive contour, a novel image attribute that encapsulates the global shape perceived from sparsely distributed, identical or similar objects—such as drone swarms or flocks of geese—collectively termed sparse-structured objects. Unlike traditional contour analysis that delineates the boundaries of individual objects, cognitive contours reflect a gestalt-inspired perception of the overall structure formed by the ensemble, capturing higher-level visual organization. Detecting cognitive contours is challenging due to the sparsity and multiplicity of constituent elements. To tackle this, we propose a scale-space method that integrates alpha shapes into a scale-space framework. An alpha-shape scale space is constructed for the sparse-structured object, and the optimal scale is adaptively selected to extract cognitively meaningful contours with appropriate structural detail. Extensive experiments validate the effectiveness and robustness of the proposed method, enhancing visual inference and offering flexibility across diverse image-based applications. Code and data are available at: https://github.com/CookiC/Sparse},
  archive      = {J_TIP},
  author       = {Yuxiang Shen and Baojiang Zhong and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2025.3592862},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4955-4967},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cognitive contour detection of sparse-structured objects in the alpha-shape scale space},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active contour model driven by non-local feature fitting energy function with scalable normalization. <em>TIP</em>, <em>34</em>, 4939-4954. (<a href='https://doi.org/10.1109/TIP.2025.3585682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging for active contour models (ACMs) to segment weak-edge and noisy images efficiently and accurately. To solve this problem, a novel ACM is proposed in this work. The proposed ACM achieves high-precision segmentation for weak-edge and noisy images using a non-local feature fitting energy function and a scalable normalization method. The non-local feature fitting energy function is constructed based on the distances calculated by Jeffreys divergence between non-local weighted fitting images and the image processed by the non-local means (NLM) algorithm. The non-local weighted fitting images include the fitting foreground and background with image edge features. The images processed by the NLM algorithm is used to reduce the influence of noise. The data-driven term, obtained by minimizing the non-local feature fitting energy function, is computed before the level set iteration, which improves the computation speed. In addition, a scalable normalization method is proposed to normalize the data-driven term. The ability to distinguish the targets from the background for different types of images is enhanced by adjusting a scaling factor, improving the robustness and accuracy of the proposed model. Experimental results demonstrate the advantages of the proposed model.},
  archive      = {J_TIP},
  author       = {Qianqian Bu and Bin Dong and Zicong Zhu and Jingen Ni},
  doi          = {10.1109/TIP.2025.3585682},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4939-4954},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Active contour model driven by non-local feature fitting energy function with scalable normalization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prior image guided snapshot high-resolution spectral imaging in near infrared. <em>TIP</em>, <em>34</em>, 4923-4938. (<a href='https://doi.org/10.1109/TIP.2025.3581464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-Infrared (NIR) hyperspectral imaging opens up numerous possibilities for wide applications. Despite Compressive Spectral Imaging (CSI) being a promising technique, which enables the acquisition of three-dimensional (3D) spatio-spectral information from dynamic scenes, applying it to the NIR spectrum remains challenging. The bottleneck lies in the high cost and limited resolution of InGaAs Focal Plane Arrays (FPAs), which further degrade the high-frequency information of the compressed measurements. Here we demonstrate a novel Effective Prior Image-guided Spectral imager, termed EpiSpec, towards high-resolution spectral imaging in the NIR. Our key observation is that the tail response of low-cost silicon-based sensors tends to capture similar image, offering high spatial resolution guidance for retrieving details. Hence, the degraded measurement of hyperspectral scene, guided by the prior image, is capable of obtaining high-quality reconstructions. Since the prior image integrates only a partial spectrum of the target scene, introducing content-aware chromatic errors, we propose the Prior Image Guided Deep Unfolding Framework (PIUF) for high-fidelity spectral reconstruction. This framework implicitly models the underlying non-linear relationship between the degraded measurements and the Non-Panchromatic (NPA) prior image. We also introduce a new NIR Spectral Images Dataset (NISID), which features a broad selection of real-world NIR spectral interesting scenes. Based on the dataset in hand, we evaluate the sparse structure of such spectra, which can serve as a guide for efficient CSI sensing matrices design. Extensive evaluations on representative CSI systems demonstrate the effectiveness of the proposed EpiSpec framework. Subsequently, lab prototypes are built for real-world imaging validation, further supporting the viability of high-resolution spectral imaging in the NIR.},
  archive      = {J_TIP},
  author       = {Zhan Shi and Zhicheng Xu and Lijing Cai and Hao Ye and Linsen Chen and Qiu Shen and Xun Cao},
  doi          = {10.1109/TIP.2025.3581464},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4923-4938},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prior image guided snapshot high-resolution spectral imaging in near infrared},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Client-unbiased skeletal action recognizer in federated learning. <em>TIP</em>, <em>34</em>, 4908-4922. (<a href='https://doi.org/10.1109/TIP.2025.3586511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge sensor devices generate vast amounts of user data, but centralized processing poses privacy risks. Federated Learning addresses this by decentralizing training. However, applying Federated Learning directly to skeleton videos fails to preserve motion dynamics and suffers from client heterogeneity bias. To address these limitations, we propose CSAR—a Client-Unbiased Skeletal Action Recognizer for Federated Learning—which tackles two core challenges: motion dynamics preservation and classifier bias mitigation. Specifically, CSAR employs a Model Calibration Loss during client training to align client-server representations and reduce drift. On the server, it generates class-balanced spatiotemporal federated features through Prototypical Gaussian Sampling, subsequently refined via a Motion-aware Differential Loss to capture kinematic properties. These features enable retraining of a globally debiased recognizer that achieves accuracy comparable to real-data-trained models. Further stabilization is achieved through Knowledge Matching, which enhances global understanding. Experiments under natural and label heterogeneity confirm that CSAR outperforms state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xingyu Zhu and Xiangbo Shu and Jinhui Tang},
  doi          = {10.1109/TIP.2025.3586511},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4908-4922},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Client-unbiased skeletal action recognizer in federated learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain generalization for learned lossless volumetric biomedical image compression. <em>TIP</em>, <em>34</em>, 4896-4907. (<a href='https://doi.org/10.1109/TIP.2025.3592549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned lossless compression methods for volumetric biomedical images have achieved significant performance improvements compared with the traditional ones. However, they often perform poorly when applied to unseen domains due to domain gap issues. To address this problem, we propose a multi-source domain generalization method to handle two main sources of domain gap issues: modality and structure differences. To address modality differences, we develop an adaptive modality transfer (AMT) module, which predicts a set of modality-specific parameters from the original image and embeds them into the bit stream. These parameters control the weights of a mixture of experts to create a dynamic convolution, which is then used for entropy coding to facilitate modality transfer. To address structure differences, we design an adaptive structure transfer (AST) module, which decomposes the high dynamic range biomedical images into least significant bits (LSB) and most significant bits (MSB) in the wavelet domain. The MSB information, which is unique to the test image, is then used to predict an additional set of dynamic convolutions to enable structure transfer. Experimental results show that our approach reduces performance degradation caused by the domain gap to within 3% across various volumetric biomedical modalities. This paves the way for the practical end-to-end biomedical image compression.},
  archive      = {J_TIP},
  author       = {Dongmei Xue and Siqi Wu and Li Li and Dong Liu and Zhu Li},
  doi          = {10.1109/TIP.2025.3592549},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4896-4907},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-source domain generalization for learned lossless volumetric biomedical image compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic personalized federated learning for cross-spectral palmprint recognition. <em>TIP</em>, <em>34</em>, 4885-4895. (<a href='https://doi.org/10.1109/TIP.2025.3592508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint recognition has recently garnered attention due to its high accuracy, strong robustness, and high security. Existing deep learning-based palmprint recognition methods usually require large amounts of data for centralized training, facing the challenge of privacy disclosure. In addition, the non-independent and identically distributed (non-IID) issue in the multi-spectral palmprint images generally leads to the degradation of recognition performance. To tackle these problems, this paper proposes a dynamic personalized federated learning model for cross-spectral palmprint recognition, called DPFed-Palm. Specifically, for each client’s local training, we present a new combination of loss functions to enforce the constraints of local models and effectively enhance the feature representation capability of models. Subsequently, DPFed-Palm aggregates the above-trained local models by using the combined aggregation strategies of the Federated Averaging (FedAvg) and Personalized Federated Learning (PFL) to obtain the best personalized global model of each client. For the selection of the best personalized global model, we develop a dynamic weight selection strategy to obtain the optimal weights of the local and global models by cross-spectral (cross-client) testing. Extensive experimental results on three public PolyU multispectral, IITD, and CASIA datasets show that the proposed method outperforms the existing techniques in privacy-preserving and recognition performance.},
  archive      = {J_TIP},
  author       = {Shuyi Li and Jianian Hu and Bob Zhang and Xin Ning and Lifang Wu},
  doi          = {10.1109/TIP.2025.3592508},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4885-4895},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic personalized federated learning for cross-spectral palmprint recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image denoising using green channel prior. <em>TIP</em>, <em>34</em>, 4869-4884. (<a href='https://doi.org/10.1109/TIP.2025.3592550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is an appealing and challenging task, in that noise statistics of real-world observations may vary with local image contents and different image channels. Specifically, the green channel usually has twice the sampling rate in raw data. To handle noise variances and leverage such channel-wise prior information, we propose a simple and effective green channel prior-based image denoising (GCP-ID) method, which integrates GCP into the classic patch-based denoising framework. Briefly, we exploit the green channel to guide the search for similar patches, which aims to improve the patch grouping quality and encourage sparsity in the transform domain. The grouped image patches are then reformulated into RGGB arrays to explicitly characterize the density of green samples. Furthermore, to enhance the adaptivity of GCP-ID to various image contents, we cast the noise estimation problem into a classification task and train an effective estimator based on convolutional neural networks (CNNs). Experiments on real-world datasets demonstrate the competitive performance of the proposed GCP-ID method for image and video denoising applications in both raw and sRGB spaces. Our code is available at https://github.com/ZhaomingKong/GCP-ID},
  archive      = {J_TIP},
  author       = {Zhaoming Kong and Fangxi Deng and Xiaowei Yang},
  doi          = {10.1109/TIP.2025.3592550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4869-4884},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image denoising using green channel prior},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level contextual prototype modulation for compositional zero-shot learning. <em>TIP</em>, <em>34</em>, 4856-4868. (<a href='https://doi.org/10.1109/TIP.2025.3592560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object compositions by leveraging prior knowledge of known primitives. However, real-world visual features of attributes and objects are often entangled, causing distribution shifts between seen and unseen combinations. Existing methods often ignore intrinsic variations and interactions among primitives, leading to poor feature discrimination and biased predictions. To address these challenges, we propose Multi-level Contextual Prototype Modulation (MCPM), a transformer-based framework with a hierarchical structure that effectively integrates attributes and objects to generate richer visual embeddings. At the feature level, we apply contrastive learning to improve discriminability across compositional tasks. At the prototype level, a subclass-driven modulator captures fine-grained attribute-object interactions, enabling better adaptation to long-tail distributions. Additionally, we introduce a Minority Attribute Enhancement (MAE) strategy that synthesizes virtual samples by mixing attribute classes, further mitigating data imbalance. Experiments on four benchmark datasets (MIT-States, C-GQA, UT-Zappos, and VAW-CZSL) show that MCPM brings significant performance improvements, verifying its effectiveness in complex composition scenes.},
  archive      = {J_TIP},
  author       = {Yang Liu and Xinshuo Wang and Xinbo Gao and Jungong Han and Ling Shao},
  doi          = {10.1109/TIP.2025.3592560},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4856-4868},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-level contextual prototype modulation for compositional zero-shot learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaAugment: A tuning-free and adaptive approach to enhance data augmentation. <em>TIP</em>, <em>34</em>, 4843-4855. (<a href='https://doi.org/10.1109/TIP.2025.3592538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation (DA) is widely employed to improve the generalization performance of deep models. However, most existing DA methods employ augmentation operations with fixed or random magnitudes throughout the training process. While this fosters data diversity, it can also inevitably introduce uncontrolled variability in augmented data, which could potentially cause misalignment with the evolving training status of the target models. Both theoretical and empirical findings suggest that this misalignment increases the risks of both underfitting and overfitting. To address these limitations, we propose AdaAugment, an innovative and tuning-free adaptive augmentation method that leverages reinforcement learning to dynamically and adaptively adjust augmentation magnitudes for individual training samples based on real-time feedback from the target network. Specifically, AdaAugment features a dual-model architecture consisting of a policy network and a target network, which are jointly optimized to adapt augmentation magnitudes in accordance with the model’s training progress effectively. The policy network optimizes the variability within the augmented data, while the target network utilizes the adaptively augmented samples for training. These two networks are jointly optimized and mutually reinforce each other. Extensive experiments across benchmark datasets and deep architectures demonstrate that AdaAugment consistently outperforms other state-of-the-art DA methods in effectiveness while maintaining remarkable efficiency. Code is available at https://github.com/Jackbrocp/AdaAugment.},
  archive      = {J_TIP},
  author       = {Suorong Yang and Peijia Li and Xin Xiong and Furao Shen and Jian Zhao},
  doi          = {10.1109/TIP.2025.3592538},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4843-4855},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AdaAugment: A tuning-free and adaptive approach to enhance data augmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Associate everything detected: Facilitating tracking-by-detection to the unknown. <em>TIP</em>, <em>34</em>, 4830-4842. (<a href='https://doi.org/10.1109/TIP.2025.3592524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything Detected (AED), that simultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf detector and supports unknown categories. Different from existing tracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g., motion cues) and relies solely on highly robust feature learning to handle complex trajectories in OV-MOT tasks while keeping excellent performance in CV-MOT tasks. Specifically, we model the association task as a similarity decoding problem and propose a sim-decoder with an association-centric learning mechanism. The sim-decoder calculates similarities in three aspects: spatial, temporal, and cross-clip. Subsequently, association-centric learning leverages these threefold similarities to ensure that the extracted features are appropriate for continuous tracking and robust enough to generalize to unknown categories. Compared with existing powerful OV-MOT and CV-MOT methods, AED achieves superior performance on TAO, SportsMOT, and DanceTrack without any prior knowledge. Our code is available at https://github.com/balabooooo/AED},
  archive      = {J_TIP},
  author       = {Zimeng Fang and Chao Liang and Xue Zhou and Shuyuan Zhu and Xi Li},
  doi          = {10.1109/TIP.2025.3592524},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4830-4842},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Associate everything detected: Facilitating tracking-by-detection to the unknown},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph mamba reasoning-based social relation recognition. <em>TIP</em>, <em>34</em>, 4814-4829. (<a href='https://doi.org/10.1109/TIP.2025.3592551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing social relations from images is crucial for improving machine perception of social interactions. Current studies mainly focus on exploring single-type relation reasoning frameworks, such as the relation between father, mother and son in a family. However, real-world scenarios often involve complex hybrid relations, such as friendships and professional relations, which pose a challenge for current methods due to the difficulty of establishing robust logical connections between these relations. In fact, in this hybrid social relation recognition setting, the interactions extend beyond dyadic to multipartite structures. To effectively explore these multipartite interactions, we propose a novel Hypergraph Mamba (HGM) framework. Specifically, we construct two hypergraphs, i.e., Person-Person Hypergraphs (PPH) and Person-Object Hypergraphs (POH), to model these high-order multipartite interactions. The HGM module performs social relation reasoning within these hypergraph structures, which includes a Vertex Selection Algorithm to mitigate inference confusion by filtering out confounders, and a Vertex Interaction Operator to find optimal global vertex neighborhoods by capturing long-range vertex dependencies. In addition, a Multilevel Transformer is proposed to adaptively align the PPH and POH inferred knowledge and visual signals to facilitate information fusion. We validate the effectiveness of our proposed HGM model on several public datasets and perform extensive ablation studies to elucidate the reasons contributing to its superior performance. Experimental results indicate that our HGM model achieves superior accuracy in predicting social relations compared to the state-of-the-art methods. Codes and datasets are available at: https://github.com/tw-repository/HGM-SRR},
  archive      = {J_TIP},
  author       = {Wang Tang and Linbo Qing and Pingyu Wang and Lindong Li and Ce Zhu},
  doi          = {10.1109/TIP.2025.3592551},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4814-4829},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hypergraph mamba reasoning-based social relation recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized nonconvex low-rank matrix recovery. <em>TIP</em>, <em>34</em>, 4806-4813. (<a href='https://doi.org/10.1109/TIP.2025.3588719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the low-rank matrix recovery problem, algorithms that directly manipulate the low-rank matrix typically require computing the top singular values/vectors of the matrix and thus are computationally expensive. Matrix factorization is a computationally efficient nonconvex approach for low-rank matrix recovery, utilizing an alternating minimization or a gradient descent algorithm, and its theoretical properties have been investigated in recent years. However, the behavior of the factorization-based matrix recovery problem in the decentralized setting is still unknown when data are distributed on multiple nodes. In this paper, we consider the distributed gradient descent algorithm and establish its (local) linear convergence up to the approximation error. Numerical results are also presented to illustrate the convergence of the algorithm over a general network.},
  archive      = {J_TIP},
  author       = {Junzhuo Gao and Heng Lian},
  doi          = {10.1109/TIP.2025.3588719},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4806-4813},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Decentralized nonconvex low-rank matrix recovery},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selective cross-view topology for deep incomplete multi-view clustering. <em>TIP</em>, <em>34</em>, 4792-4805. (<a href='https://doi.org/10.1109/TIP.2025.3587586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering has gained significant attention due to the prevalence of incomplete multi-view data in real-world scenarios. However, existing methods often overlook the critical role of inter-view relationships. In unsupervised settings, selectively leveraging cross-view topological relationships can effectively guide view completion and representation learning. To address this challenge, we propose a novel framework called Selective Cross-View Topology Incomplete Multi-View Clustering (SCVT). Our approach constructs a view topology graph using the Optimal Transport (OT) distance between view. This graph helps identify neighboring views for those with missing data, enabling the inference of topological relationships and accurate completion of missing samples. Additionally, we introduce the Max View Graph Contrastive Alignment module to facilitate information transfer and alignment across neighboring views. Furthermore, we propose the View Graph Weighted Intra-View Contrastive Learning module, which enhances representation learning by pulling representations of samples within the same cluster closer, while applying varying degrees of enhancement across different views based on the view graph. Our method achieves state-of-the-art performance on seven benchmark datasets, significantly outperforming existing methods for incomplete multi-view clustering and demonstrating its effectiveness.},
  archive      = {J_TIP},
  author       = {Zhibin Dong and Dayu Hu and Jiaqi Jin and Siwei Wang and Xinwang Liu and En Zhu},
  doi          = {10.1109/TIP.2025.3587586},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4792-4805},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Selective cross-view topology for deep incomplete multi-view clustering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InstantGroup: Instant template generation for scalable group of brain MRI registration. <em>TIP</em>, <em>34</em>, 4778-4791. (<a href='https://doi.org/10.1109/TIP.2025.3587597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Template generation is a critical step in groupwise image registration, which involves aligning a group of subjects into a common space. While existing methods can generate high-quality template images, they often incur substantial time costs or are limited by fixed group scales. In this paper, we present InstantGroup, an efficient groupwise template generation framework based on variational autoencoder (VAE) models that leverage latent representations’ arithmetic properties, enabling scalability to groups of any size. InstantGroup features a Dual VAE backbone with shared-weight twin networks to handle pairs of inputs and incorporates a Displacement Inversion Module (DIM) to maintain template unbiasedness and a Subject-Template Alignment Module (STAM) to improve template quality and registration accuracy. Experiments on 3D brain MRI scans from the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces runtime, generating templates within seconds for various group sizes while maintaining superior performance compared to state-of-the-art baselines on quantitative metrics, including unbiasedness and registration accuracy.},
  archive      = {J_TIP},
  author       = {Ziyi He and Albert C. S. Chung},
  doi          = {10.1109/TIP.2025.3587597},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4778-4791},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {InstantGroup: Instant template generation for scalable group of brain MRI registration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FP-age: Leveraging face parsing attention for facial age estimation in the wild. <em>TIP</em>, <em>34</em>, 4767-4777. (<a href='https://doi.org/10.1109/TIP.2022.3155944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based age estimation aims to predict a person’s age from facial images. It is used in a variety of real-world applications. Although end-to-end deep models have achieved impressive results for age estimation on benchmark datasets, their performance in-the-wild still leaves much room for improvement due to the challenges caused by large variations in head pose, facial expressions, and occlusions. To address this issue, we propose a simple yet effective method to explicitly incorporate facial semantics into age estimation, so that the model would learn to correctly focus on the most informative facial components from unaligned facial images regardless of head pose and non-rigid deformation. To this end, we design a face parsing-based network to learn semantic information at different scales and a novel face parsing attention module to leverage these semantic features for age estimation. To evaluate our method on in-the-wild data, we also introduce a new challenging large-scale benchmark called IMDB-Clean. This dataset is created by semi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained clustering method. Through comprehensive experiment on IMDB-Clean and other benchmark datasets, under both intra-dataset and cross-dataset evaluation protocols, we show that our method consistently outperforms all existing age estimation methods and achieves a new state-of-the-art performance. To the best of our knowledge, our work presents the first attempt of leveraging face parsing attention to achieve semantic-aware age estimation, which may be inspiring to other high level facial analysis tasks.},
  archive      = {J_TIP},
  author       = {Yiming Lin and Jie Shen and Yujiang Wang and Maja Pantic},
  doi          = {10.1109/TIP.2022.3155944},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4767-4777},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FP-age: Leveraging face parsing attention for facial age estimation in the wild},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Content-decoupled contrastive learning-based implicit degradation modeling for blind image super-resolution. <em>TIP</em>, <em>34</em>, 4751-4766. (<a href='https://doi.org/10.1109/TIP.2025.3558442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit degradation modeling-based blind super-resolution (SR) has attracted more increasing attention in the community due to its excellent generalization to complex degradation scenarios and wide application range. How to extract more discriminative degradation representations and fully adapt them to specific image features is the key to this task. In this paper, we propose a new Content-decoupled Contrastive Learning-based blind image super-resolution (CdCL) framework following the typical blind SR pipeline. This framework introduces negative-free contrastive learning technique for the first time to model the implicit degradation representation, in which a new cyclic shift sampling strategy is designed to ensure decoupling between content features and degradation features from the data perspective, thereby improving the purity and discriminability of the learned implicit degradation space. In addition, we propose a detail-aware implicit degradation adapting module that can better adapt degradation representations to specific LR features by enhancing the basic adaptation unit’s perception of image details, significantly reducing the overall SR model complexity. Extensive experiments on synthetic and real data show that our method achieves highly competitive quantitative and qualitative results in various degradation settings while obviously reducing parameters and computational costs, validating the feasibility of designing practical and lightweight blind SR tools. Codes and models will be available at https://github.com/Fieldhunter/CdCL.},
  archive      = {J_TIP},
  author       = {Jiang Yuan and Ji Ma and Bo Wang and Weiming Hu},
  doi          = {10.1109/TIP.2025.3558442},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4751-4766},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Content-decoupled contrastive learning-based implicit degradation modeling for blind image super-resolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learned spherical image compression with spherical convolution-self-attention and transformer context model. <em>TIP</em>, <em>34</em>, 4736-4750. (<a href='https://doi.org/10.1109/TIP.2025.3585721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging virtual reality (VR) applications bring significant challenges to spherical image compression. Spherical images are first converted into planar images using projections like the equirectangular projection (ERP) to facilitate compression. Methods based on deep neural networks (DNNs) have achieved optimal rate-distortion (R-D) performance in planar image compression. However, the non-uniform sampling of ERP makes the R-D optimization process inefficient when using DNN-based planar compression methods. To address this problem, we propose spherical DNNs for learning based spherical image compression using uniform sampling and ordered rooted tree based index of the Spherical Measure-Based Spherical Image Representation (SMSIR). Specifically, we first define basic spherical operations under the ordered rooted tree based index, including spherical convolution and window transformer, to exploit both local and non-local correlations on the sphere, respectively. We then construct a spherical convolution and a self-attention integrated transformer module named SMixFormer, which simultaneously considers both the enlargement of the receptive fields of local windows and the capture of local and non-local correlations. Furthermore, we introduce a spherical transformer context model with an ordering following the ordered rooted tree based index to enhance the accuracy of the entropy model. To optimize our model, we collect a high-resolution and high-quality spherical image dataset from the Internet. Experimental results demonstrate that our approach outperforms traditional image compression standards, including JPEG, JPEG2000, and BPG. Compared to the learning-based hyperprior planar image compression model, our method achieves a bitrate reduction of over 16%.},
  archive      = {J_TIP},
  author       = {Hui Hu and Yunhui Shi and Jin Wang and Dong Liu and Nam Ling and Baocai Yin},
  doi          = {10.1109/TIP.2025.3585721},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4736-4750},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learned spherical image compression with spherical convolution-self-attention and transformer context model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AU-net: Adaptive unified network for joint multi-modal image registration and fusion. <em>TIP</em>, <em>34</em>, 4721-4735. (<a href='https://doi.org/10.1109/TIP.2025.3586507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint multi-modal image registration and fusion (JMIRF) typically follows a register-first, fuse-later paradigm. It has a registration module to align parallax images and a fusion module to fuse registered images. Existing research typically focuses on the mutual enhancement between the two modules, but this is essentially a straightforward combination rather than an efficient, unified network. Moreover, executing the two modules separately may cause inefficiency, as the total runtime is merely the sum of both steps without investigating potential shared structures. In this paper, we propose an Adaptive Unified Network (AU-Net) following a novel end-to-end paradigm called Feature-Level Joint Training (FLJT). Firstly, AU-Net learns registration and fusion within a unified network through shared structure and hierarchical semantic interaction. A multi-level dynamic fusion module is designed to adaptively fuse input features from different scales and modalities. Secondly, the image-to-image translation based on Denoising Diffusion Probabilistic Models (DDPMs) is introduced to train AU-Net using simple and reliable single-modal metrics. Unlike previous unidirectional translation, we explore bidirectional translation to provide additional implicit branch supervision. Furthermore, a cache-like scheme is proposed to elegantly circumvent the additional computational overhead caused by the iterative denoising of DDPMs. Finally, our method was validated on two publicly available datasets, demonstrating advantages over state-of-the-art methods in terms of qualitative evaluation, quantitative evaluation, and computational complexity analysis. The code will be publically available at https://github.com/luming1314/AU-Net},
  archive      = {J_TIP},
  author       = {Ming Lu and Min Jiang and Xuefeng Tao and Jun Kong},
  doi          = {10.1109/TIP.2025.3586507},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4721-4735},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AU-net: Adaptive unified network for joint multi-modal image registration and fusion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReviveDiff: A universal diffusion model for restoring images in adverse weather conditions. <em>TIP</em>, <em>34</em>, 4706-4720. (<a href='https://doi.org/10.1109/TIP.2025.3587578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in challenging environments–such as nighttime, smoke, rainy weather, and underwater–often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed “ReviveDiff”, which can address various degradations and restore images to their original quality by enhancing and restoring their details. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually.},
  archive      = {J_TIP},
  author       = {Wenfeng Huang and Guoan Xu and Wenjing Jia and Stuart Perry and Guangwei Gao},
  doi          = {10.1109/TIP.2025.3587578},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4706-4720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ReviveDiff: A universal diffusion model for restoring images in adverse weather conditions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniEmoX: Cross-modal semantic-guided large-scale pretraining for universal scene emotion perception. <em>TIP</em>, <em>34</em>, 4691-4705. (<a href='https://doi.org/10.1109/TIP.2025.3587577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual emotion analysis holds significant research value in both computer vision and psychology. However, existing methods for visual emotion analysis suffer from limited generalizability due to the ambiguity of emotion perception and the diversity of data scenarios. To tackle this issue, we introduce UniEmoX, a cross-modal semantic-guided large-scale pretraining framework. Inspired by psychological research emphasizing the inseparability of the emotional exploration process from the interaction between individuals and their environment, UniEmoX integrates scene-centric and person-centric low-level image spatial structural information, aiming to derive more nuanced and discriminative emotional representations. By exploiting the similarity between paired and unpaired image-text samples, UniEmoX distills rich semantic knowledge from the CLIP model to enhance emotional embedding representations more effectively. To the best of our knowledge, this is the first large-scale pretraining framework that integrates psychological theories with contemporary contrastive learning and masked image modeling techniques for emotion analysis across diverse scenarios. Additionally, we develop a visual emotional dataset titled Emo8. Emo8 samples cover a range of domains, including cartoon, natural, realistic, science fiction and advertising cover styles, covering nearly all common emotional scenes. Comprehensive experiments conducted on seven benchmark datasets across two downstream tasks validate the effectiveness of UniEmoX. The source code is available at https://github.com/chincharles/u-emo},
  archive      = {J_TIP},
  author       = {Chuang Chen and Xiao Sun and Zhi Liu},
  doi          = {10.1109/TIP.2025.3587577},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4691-4705},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UniEmoX: Cross-modal semantic-guided large-scale pretraining for universal scene emotion perception},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view disparity estimation using the gradient consistency model. <em>TIP</em>, <em>34</em>, 4676-4690. (<a href='https://doi.org/10.1109/TIP.2025.3588322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational approaches to disparity estimation typically use a linearised brightness constancy constraint, which only applies in smooth regions and over small distances. Accordingly, current variational approaches rely on a schedule to progressively include image data. This paper proposes the use of Gradient Consistency information to assess the validity of the linearisation; this information is used to determine the weights applied to the data term as part of an analytically inspired Gradient Consistency Model. The Gradient Consistency Model penalises the data term for view pairs that have a mismatch between the spatial gradients in the source view and the spatial gradients in the target view. Instead of relying on a tuned or learned schedule, the Gradient Consistency Model is self-scheduling, since the weights evolve as the algorithm progresses. We show that the Gradient Consistency Model outperforms standard coarse-to-fine schemes and the recently proposed progressive inclusion of views approach in both rate of convergence and accuracy.},
  archive      = {J_TIP},
  author       = {James Lyndon Gray and Aous Thabit Naman and David S. Taubman},
  doi          = {10.1109/TIP.2025.3588322},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4676-4690},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view disparity estimation using the gradient consistency model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind image quality assessment by gaussian mixture distribution. <em>TIP</em>, <em>34</em>, 4660-4675. (<a href='https://doi.org/10.1109/TIP.2025.3586512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image quality assessment (IQA), researchers have been studying the mean opinion score (MOS) of image quality for decades. They focus on developing IQA methods with the help of MOS without using the potential of the distribution of opinion scores (DOS). We find that the Gaussian mixture distribution (GMD) can more accurately describe the DOS of image quality on SJTU IQSD and KonIQ-10K databases compared to some traditional distributions. Therefore, this paper proposes a blind IQA method that predicts the MOS of image quality by learning the GMD-based image quality. The proposed method consists of a visual feature learning module and a GMD learning module. The visual feature learning module uses a multi-stage Swin Transformer model and a CLIP feature extractor to extract visual features from an image. The GMD learning module then maps the extracted visual features to the GMD-based image quality using a mixture density network, where the mean of the GMD represents the MOS of image quality. We not only use the MOS of image quality to train the proposed method, but also employ the DOS of image quality for auxiliary training to improve the prediction performance of the proposed method. To address the lack of DOS in some existing IQA databases, we introduce a pseudo DOS generation strategy to generate the DOS of image quality for training, which significantly improves the applicability of the proposed method. Numerous analyses show that the proposed method is superior to most state-of-the-art IQA methods in predicting both the MOS and the DOS, thus facilitating a deeper investigation into the DOS of image quality in IQA.},
  archive      = {J_TIP},
  author       = {Yixuan Gao and Xiongkuo Min and Yuqin Cao and Weisi Lin and Bu Sung Lee and Guangtao Zhai},
  doi          = {10.1109/TIP.2025.3586512},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4660-4675},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind image quality assessment by gaussian mixture distribution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of alpha-trees for hierarchical segmentation by horizontal cuts. <em>TIP</em>, <em>34</em>, 4646-4659. (<a href='https://doi.org/10.1109/TIP.2025.3588250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alpha trees, and derived $\alpha $ - $\omega $ -hierarchies are powerful tools for hierarchical image representation in computer vision. However, the quality of $\alpha $ - $\omega $ -hierarchies has not been fully evaluated, limiting their further development and application. In our study, an algorithm for evaluating the quality of $\alpha $ - $\omega $ -hierarchies based on horizontal cut filters is proposed. With the aim to automatically select optimal parameters and dissimilarity measures for $\alpha $ - $\omega $ -hierarchy constructions, key factors including maximum accuracy, construction complexity, and efficiency of $\alpha $ - $\omega $ -hierarchies are systematically considered. Notably, remote sensing images based experiments were conducted to demonstrate the usefulness of this algorithm. In addition, our algorithm can be potentially extended to qualify other types of hierarchical trees, making it useful for the automatic selection of optimal hierarchical segmentation methods.},
  archive      = {J_TIP},
  author       = {Xiaoxuan Zhang and Michael H. F. Wilkinson},
  doi          = {10.1109/TIP.2025.3588250},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4646-4659},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Evaluation of alpha-trees for hierarchical segmentation by horizontal cuts},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SQLNet: Scale-modulated query and localization network for few-shot class-agnostic counting. <em>TIP</em>, <em>34</em>, 4631-4645. (<a href='https://doi.org/10.1109/TIP.2025.3588255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class-agnostic counting (CAC) task has recently been proposed to solve the problem of counting all objects of an arbitrary class with several exemplars given in the input image. To address this challenging task, existing leading methods all resort to density map regression, which renders them impractical for downstream tasks that require object locations and restricts their ability to well explore the scale information of exemplars for supervision. Meanwhile, they generally model the interaction between the input image and the exemplars in an exemplar-by-exemplar way, which is inefficient and may not fully synthesize information from all exemplars. To address these limitations, we propose a novel localization-based CAC approach, termed Scale-modulated Query and Localization Network (SQLNet). It fully explores the scales of exemplars in both the query and localization stages and achieves effective counting by accurately locating each object and predicting its approximate size. Specifically, during the query stage, rich discriminative representations of the target class are acquired by the Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few exemplars through multi-scale exemplar cooperation with equifrequent size prompt embedding. These representations are then fed into the Exemplars-Unified Query Correlation (EUQC) module to interact with the query features in a unified manner and produce the correlated query tensor. In the localization stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query tensor to predict the confidence, location, and size of each potential object. Moreover, a scale-aware localization loss is introduced, which exploits flexible location associations and exemplar scales for supervision to optimize the model performance. Extensive experiments demonstrate that SQLNet outperforms state-of-the-art methods on popular CAC benchmarks, achieving excellent performance not only in counting accuracy but also in localization and bounding box generation.},
  archive      = {J_TIP},
  author       = {Hefeng Wu and Yandong Chen and Lingbo Liu and Tianshui Chen and Keze Wang and Liang Lin},
  doi          = {10.1109/TIP.2025.3588255},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4631-4645},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SQLNet: Scale-modulated query and localization network for few-shot class-agnostic counting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED4: Explicit data-level debiasing for deepfake detection. <em>TIP</em>, <em>34</em>, 4618-4630. (<a href='https://doi.org/10.1109/TIP.2025.3588323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning intrinsic bias from limited data has been considered the main reason for the failure of deepfake detection with generalizability. Apart from the discovered content and specific-forgery bias, we reveal a novel spatial bias, where detectors inertly anticipate observing structural forgery clues appearing at the image center, also can lead to the poor generalization of existing methods. We present ED4, a simple and effective strategy, to address aforementioned biases explicitly at the data level in a unified framework rather than implicit disentanglement via network design. In particular, we develop ClockMix to produce facial structure preserved mixtures with arbitrary samples, which allows the detector to learn from an exponentially extended data distribution with much more diverse identities, backgrounds, local manipulation traces, and the co-occurrence of multiple forgery artifacts. We further propose the Adversarial Spatial Consistency Module (AdvSCM) to prevent extracting features with spatial bias, which adversarially generates spatial-inconsistent images and constrains their extracted feature to be consistent. As a model-agnostic debiasing strategy, ED4 is plug-and-play: it can be integrated with various deepfake detectors to obtain significant benefits. We conduct extensive experiments to demonstrate its effectiveness and superiority over existing deepfake detection approaches. Code is available at https://github.com/beautyremain/ED4.},
  archive      = {J_TIP},
  author       = {Jikang Cheng and Ying Zhang and Qin Zou and Zhiyuan Yan and Chao Liang and Zhongyuan Wang and Chen Li},
  doi          = {10.1109/TIP.2025.3588323},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4618-4630},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ED4: Explicit data-level debiasing for deepfake detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot skeleton-based action recognition with prototype-guided feature alignment. <em>TIP</em>, <em>34</em>, 4602-4617. (<a href='https://doi.org/10.1109/TIP.2025.3586487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot skeleton-based action recognition aims to classify unseen skeleton-based human actions without prior exposure to such categories during training. This task is extremely challenging due to the difficulty in generalizing from known to unknown actions. Previous studies typically use two-stage training: pre-training skeleton encoders on seen action categories using cross-entropy loss and then aligning pre-extracted skeleton and text features, enabling knowledge transfer to unseen classes through skeleton-text alignment and language models’ generalization. However, their efficacy is hindered by 1) insufficient discrimination for skeleton features, as the fixed skeleton encoder fails to capture necessary alignment information for effective skeleton-text alignment; 2) the neglect of alignment bias between skeleton and unseen text features during testing. To this end, we propose a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive training framework to improve skeleton-text alignment, ensuring sufficient discrimination for skeleton features. Additionally, we introduce a prototype-guided text feature alignment strategy to mitigate the adverse impact of the distribution discrepancy during testing. We provide a theoretical analysis to support our prototype-guided text feature alignment strategy and empirically evaluate our overall PGFA on three well-known datasets. Compared with the top competitor SMIE method, our PGFA achieves absolute accuracy improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD datasets, respectively.},
  archive      = {J_TIP},
  author       = {Kai Zhou and Shuhai Zhang and Zeng You and Jinwu Hu and Mingkui Tan and Fei Liu},
  doi          = {10.1109/TIP.2025.3586487},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4602-4617},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot skeleton-based action recognition with prototype-guided feature alignment},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust palmprint recognition via multi-stage noisy label selection and correction. <em>TIP</em>, <em>34</em>, 4591-4601. (<a href='https://doi.org/10.1109/TIP.2025.3588040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based palmprint recognition methods take performance to the next level. However, most current methods rely on samples with clean labels. Noisy labels are difficult to avoid in practical applications and may affect the reliability of models, which poses a big challenge. In this paper, we propose a novel Multi-stage Noisy Label Selection and Correction (MNLSC) framework to address this issue. Three stages are proposed to improve the robustness of palmprint recognition. Clean simple samples are firstly selected based on self-supervised learning. A Fourier-based module is constructed to select clean hard samples. A pototype-based module is further introduced for selecting noisy labels from the remaining samples and correcting them. Finally, the model is trained by using clean and corrected labels to improve the performance. Experiments are conducted on several constrained and unconstrained palmprint databases. The results demonstrate the superiority of our method over other methods in dealing with different noise rates. Compared with the baseline method, the accuracy can be improved by up to 33.45% when there are 60% noisy labels.},
  archive      = {J_TIP},
  author       = {Huikai Shao and Siyu Shi and Xuefeng Du and Dan Zeng and Dexing Zhong},
  doi          = {10.1109/TIP.2025.3588040},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4591-4601},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust palmprint recognition via multi-stage noisy label selection and correction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FullLoRA: Efficiently boosting the robustness of pretrained vision transformers. <em>TIP</em>, <em>34</em>, 4580-4590. (<a href='https://doi.org/10.1109/TIP.2025.3587598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Vision Transformer (ViT) model has gradually become mainstream in various computer vision tasks, and the robustness of the model has received increasing attention. However, existing large models tend to prioritize performance during training, potentially neglecting the robustness, which may lead to serious security concerns. In this paper, we establish a new challenge: exploring how to use a small number of additional parameters for adversarial finetuning to quickly and effectively enhance the adversarial robustness of a standardly trained model. To address this challenge, we develop novel LNLoRA module, incorporating a learnable layer normalization before the conventional LoRA module, which helps mitigate magnitude differences in parameters between the adversarial and standard training paradigms. Furthermore, we propose the FullLoRA framework by integrating the learnable LNLoRA modules into all key components of ViT-based models while keeping the pretrained model frozen, which can significantly improve the model robustness via adversarial finetuning in a parameter-efficient manner. Extensive experiments on several datasets demonstrate the superiority of our proposed FullLoRA framework. It achieves comparable robustness with full finetuning while only requiring about 5% of the learnable parameters. This also effectively addresses concerns regarding extra model storage space and enormous training time caused by adversarial finetuning.},
  archive      = {J_TIP},
  author       = {Zheng Yuan and Jie Zhang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TIP.2025.3587598},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4580-4590},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FullLoRA: Efficiently boosting the robustness of pretrained vision transformers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight class incremental semantic segmentation without catastrophic forgetting. <em>TIP</em>, <em>34</em>, 4566-4579. (<a href='https://doi.org/10.1109/TIP.2025.3588065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental semantic segmentation (CISS) aims to progressively segment newly introduced classes while preserving the memory of previously learned ones. Traditional CISS methods directly employ advanced semantic segmentation models (e.g., Deeplab-v3) as continual learners. However, these methods require substantial computational and memory resources, limiting their deployment on edge devices. In this paper, we propose a Lightweight Class Incremental Semantic Segmentation (LISS) model tailored for resource-constrained scenarios. Specifically, we design an automatic knowledge-preservation pruning strategy based on the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which automatically compresses the CISS model by searching for global penalty coefficients. Nonetheless, reducing model parameters exacerbates catastrophic forgetting during incremental learning. To mitigate this challenge, we develop a clustering-based pseudo labels generator to obtain high-quality pseudo labels by considering the feature space structure of old classes. It adjusts predicted probabilities from the old model according to the feature proximity to nearest sub-cluster centers for each class. Additionally, we introduce a customized soft labels module that distills the semantic relationships between classes separately. It decomposes soft labels into target probabilities, background probabilities, and other probabilities, thereby maintaining knowledge of previously learned classes in a fine-grained manner. Extensive experiments on two benchmark datasets demonstrate that our LISS model outperforms state-of-the-art approaches in both effectiveness and efficiency.},
  archive      = {J_TIP},
  author       = {Wei Cong and Yang Cong and Yu Ren},
  doi          = {10.1109/TIP.2025.3588065},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4566-4579},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lightweight class incremental semantic segmentation without catastrophic forgetting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from vision foundation models for cross-domain remote sensing image segmentation. <em>TIP</em>, <em>34</em>, 4553-4565. (<a href='https://doi.org/10.1109/TIP.2025.3588041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain image segmentation plays a crucial role in the field of remote sensing. Current approaches often rely on a mean-teacher model that is integrated from student models to guide the training of the student model itself. However, the feature space of the mean-teacher model exhibits significant domain discrepancy and considerable class overlap, which results in suboptimal performance. Motivated by the idea of learning from stronger teachers, we introduce a robust domain adaptation method called LFMDA. This novel approach is the first to explicitly enhance cross-domain semantic segmentation performance by leveraging vision foundation models (VFMs) within remote sensing applications. Specifically, we propose a prototypical contrastive knowledge distillation loss (PCD) that enables the student model to produce domain-invariant yet category-discriminative features by distilling knowledge from a domain-generalized VFM teacher. Additionally, we introduce a local region homogenization strategy (LRH) to generate high-quality and high-quantity pseudo-labels by incorporating a Segment Anything Model (SAM). Extensive empirical evaluations demonstrate that our method outperforms existing approaches, setting a new state-of-the-art (SOTA) method in domain-adaptive remote sensing image segmentation. The code is available at https://github.com/StuLiu/LFMDA},
  archive      = {J_TIP},
  author       = {Wang Liu and Puhong Duan and Zhuojun Xie and Xudong Kang and Shutao Li},
  doi          = {10.1109/TIP.2025.3588041},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4553-4565},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning from vision foundation models for cross-domain remote sensing image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRENet: Saliency-based lighting enhancement network. <em>TIP</em>, <em>34</em>, 4541-4552. (<a href='https://doi.org/10.1109/TIP.2025.3587588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lighting enhancement is a classical topic in low-level image processing. Existing studies mainly focus on global illumination optimization while overlooking local semantic objects, and this limits the performance of exposure compensation. In this paper, we introduce SRENet, a novel lighting enhancement network guided by saliency information. It adopts a two-step strategy of foreground-background separation optimization to achieve a balance between global and local illumination. In the first step, we extract salient regions and implement the local illumination enhancement that ensures the exposure quality of salient objects. Next, we utilize a fusion module to process global lighting optimization based on local enhanced results. With the two-step strategy, the proposed SRENet yield better lighting enhancement for local illumination while preserving the globally optimal results. Experimental results demonstrate that our method obtains more effective enhancement results for various tasks of exposure correction and lighting quality improvement. The source code and pre-trained models are available at https://github.com/PlanktonQAQ/SRENet},
  archive      = {J_TIP},
  author       = {Yuming Fang and Chen Peng and Chenlei Lv and Weisi Lin},
  doi          = {10.1109/TIP.2025.3587588},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4541-4552},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRENet: Saliency-based lighting enhancement network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sharing task-relevant information in visual prompt tuning by cross-layer dynamic connection. <em>TIP</em>, <em>34</em>, 4527-4540. (<a href='https://doi.org/10.1109/TIP.2025.3587587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress has shown great potential of visual prompt tuning (VPT) when adapting pre-trained vision transformers to various downstream tasks. However, most existing solutions independently optimize prompts at each layer, thereby neglecting the usage of task-relevant information encoded in prompt tokens across layers. Additionally, existing prompt structures are prone to interference from task-irrelevant noise in input images, which can adversely affect the sharing of task-relevant information. In this paper, we propose a novel VPT approach, SVPT. It innovatively incorporates a cross-layer dynamic connection (CDC) for input prompt tokens from adjacent layers, enabling effective sharing of task-relevant information. Furthermore, we design a dynamic aggregation (DA) module that facilitates selective sharing of information between layers. The combination of CDC and DA enhances the flexibility of the attention process within the VPT framework. Building upon these foundations, SVPT introduces an attentive enhancement (AE) mechanism that automatically identifies salient image tokens and refines them with prompt tokens in an additive manner. Extensive experiments on 24 image classification and semantic segmentation benchmarks clearly demonstrate the advantages of the proposed SVPT, compared to the state-of-the-art counterparts.},
  archive      = {J_TIP},
  author       = {Nan Zhou and Jiaxin Chen and Di Huang},
  doi          = {10.1109/TIP.2025.3587587},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4527-4540},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sharing task-relevant information in visual prompt tuning by cross-layer dynamic connection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy visual-textual retrieval. <em>TIP</em>, <em>34</em>, 4515-4526. (<a href='https://doi.org/10.1109/TIP.2025.3587575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-textual retrieval, as a link between computer vision and natural language processing, aims at jointly learning visual-semantic relevance to bridge the heterogeneity gap across visual and textual spaces. Existing methods conduct retrieval only relying on the ranking of pairwise similarities, but they cannot self-evaluate the uncertainty of retrieved results, resulting in unreliable retrieval and hindering interpretability. To address this problem, we propose a novel Trust-Consistent Learning framework (TCL) to endow visual-textual retrieval with uncertainty evaluation for trustworthy retrieval. More specifically, TCL first models the matching evidence according to cross-modal similarity to estimate the uncertainty for cross-modal uncertainty-aware learning. Second, a simple yet effective consistency module is presented to enforce the subjective opinions of bidirectional learning to be consistent for high reliability and accuracy. Finally, extensive experiments are conducted to demonstrate the superiority and generalizability of TCL on six widely-used benchmark datasets, i.e., Flickr30K, MS-COCO, MSVD, MSR-VTT, ActivityNet, and DiDeMo. Furthermore, some qualitative experiments are carried out to provide comprehensive and insightful analyses for trustworthy visual-textual retrieval, verifying the reliability and interoperability of TCL. The code is available in https://github.com/QinYang79/TCL},
  archive      = {J_TIP},
  author       = {Yang Qin and Lifu Huang and Dezhong Peng and Bohan Jiang and Joey Tianyi Zhou and Xi Peng and Peng Hu},
  doi          = {10.1109/TIP.2025.3587575},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4515-4526},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Trustworthy visual-textual retrieval},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal contrastive masked AutoEncoder for compressed video pre-training. <em>TIP</em>, <em>34</em>, 4500-4514. (<a href='https://doi.org/10.1109/TIP.2025.3583168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Transformer based approach, namely Cross-modal Contrastive Masked AutoEncoder (C2MAE), to Self-Supervised Learning (SSL) on compressed videos. A unified Transformer encoder is employed to discover relationships of visual tokens from RGBs, motion vectors and residuals. A hybrid SSL framework is proposed, which combines the complementary advantages of Masked Image Modeling (MIM) and Contrastive Learning (CL) pretext tasks, for powerful representation learning. The MIM branch extends VideoMAE by a new Fine-Grained Motion-aware Masking (FGMM) strategy and a modified Multi-modal Reconstruction (MR) task, where FGMM computes motion saliency maps as motion priors to guide the masks so that it well fits for the data properties in the compressed domain and the MR task highlights the reconstruction of raw videos by joint representations from corresponding compressed videos in addition to that in each single modality. The CL branch introduces the Contrastive Cross-modal Learning (CCL) module, and the features from a compressed video clip and the ones from its raw video counterpart are compared instead of widely used augmented data. Due to these designs, C2MAE significantly enhances interactions across modalities to compensate the sparsity of I-frames and the coarse and noisy nature of P-frames, thus delivering much stronger pre-trained models. Extensive experiments are conducted on the UCF-101, HMDB-51 and Kinetics-400 benchmarks with state-of-the-art results reported, demonstrating its effectiveness.},
  archive      = {J_TIP},
  author       = {Bing Li and Jiaxin Chen and Guohao Li and Dongming Zhang and Xiuguo Bao and Di Huang},
  doi          = {10.1109/TIP.2025.3583168},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4500-4514},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal contrastive masked AutoEncoder for compressed video pre-training},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptual quality assessment of 360° images based on generative scanpath representation. <em>TIP</em>, <em>34</em>, 4485-4499. (<a href='https://doi.org/10.1109/TIP.2025.3583181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite substantial efforts dedicated to the design of heuristic models for omnidirectional (i.e., 360°) image quality assessment (OIQA), a conspicuous gap remains due to the lack of consideration for the diversity of viewing behaviors that leads to the varying perceptual quality of 360° images. Two critical aspects underline this oversight: the neglect of viewing conditions that significantly sway user gaze patterns and the overreliance on a single viewport sequence from the 360° image for quality inference. To address these issues, we introduce a unique generative scanpath representation (GSR) for effective quality inference of 360° images, which aggregates varied perceptual experiences of multi-hypothesis users under a predefined viewing condition. More specifically, given a viewing condition characterized by the starting point of viewing and exploration time, a set of scanpaths consisting of dynamic visual fixations can be produced using an apt scanpath generator. Following this vein, we use the scanpaths to convert the 360° image into the unique GSR, which provides a global overview of gazed-focused contents derived from scanpaths. As such, the quality inference of the 360° image is swiftly transformed to that of GSR. We then propose an efficient OIQA computational framework by learning the quality maps of GSR. Comprehensive experimental results validate that the predictions of the proposed framework are highly consistent with human perception in the spatiotemporal domain, especially in the challenging context of locally distorted 360° images under varied viewing conditions. The code will be released at https://github.com/xiangjieSui/GSR},
  archive      = {J_TIP},
  author       = {Xiangjie Sui and Hanwei Zhu and Xuelin Liu and Yuming Fang and Shiqi Wang and Zhou Wang},
  doi          = {10.1109/TIP.2025.3583181},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4485-4499},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptual quality assessment of 360° images based on generative scanpath representation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALSP+: Fast scene recovery via ambient light similarity prior. <em>TIP</em>, <em>34</em>, 4470-4484. (<a href='https://doi.org/10.1109/TIP.2025.3586514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absorption and scattering of light in different turbid media cause images to suffer from poor visibility and contrast, which severely affects the performance of many computer vision tasks. To address this issue, we propose a fast scene recovery method based on the Ambient light similarity prior (ALSP). In this method, the ambient light similarity metric is designed from both magnitude and orientation, which is embedded into the optical imaging model, and the estimation of scene transmission is derived by simplification and approximation. The estimation of the transmission map is very simple, and its time complexity is O(N), where N is the size of the input image. Moreover, we propose a progressive manner to determine the ambient light for both the near and far regions separately, which can effectively improve the brightness and color saturation of the restored image. Experiments performed in different scenes demonstrate that our method outperforms several state-of-the-art competitors in terms of efficiency and scene recovery performance.},
  archive      = {J_TIP},
  author       = {Lei He and Zunhui Yi and Jinshi Liu and Chaoyang Chen and Ming Lu and Zhipeng Chen},
  doi          = {10.1109/TIP.2025.3586514},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4470-4484},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ALSP+: Fast scene recovery via ambient light similarity prior},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing feature alignment and uniformity for few-shot classification. <em>TIP</em>, <em>34</em>, 4456-4469. (<a href='https://doi.org/10.1109/TIP.2023.3328475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Few-Shot Learning (FSL), the objective is to correctly recognize new samples from novel classes with only a few available samples per class. Existing methods in FSL primarily focus on learning transferable knowledge from base classes by maximizing the information between feature representations and their corresponding labels. However, this approach may suffer from the “supervision collapse” issue, which arises due to a bias towards the base classes. In this paper, we propose a solution to address this issue by preserving the intrinsic structure of the data and enabling the learning of a generalized model for the novel classes. Following the InfoMax principle, our approach maximizes two types of mutual information (MI): between the samples and their feature representations, and between the feature representations and their class labels. This allows us to strike a balance between discrimination (capturing class-specific information) and generalization (capturing common characteristics across different classes) in the feature representations. To achieve this, we adopt a unified framework that perturbs the feature embedding space using two low-bias estimators. The first estimator maximizes the MI between a pair of intra-class samples, while the second estimator maximizes the MI between a sample and its augmented views. This framework effectively combines knowledge distillation between class-wise pairs and enlarges the diversity in feature representations. By conducting extensive experiments on popular FSL benchmarks, our proposed approach achieves comparable performances with state-of-the-art competitors. For example, we achieved an accuracy of 69.53% on the miniImageNet dataset and 77.06% on the CIFAR-FS dataset for the 5-way 1-shot task.},
  archive      = {J_TIP},
  author       = {Yunlong Yu and Dingyi Zhang and Zhong Ji and Xi Li and Jungong Han and Zhongfei Zhang},
  doi          = {10.1109/TIP.2023.3328475},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4456-4469},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Balancing feature alignment and uniformity for few-shot classification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast partial-modal online cross-modal hashing. <em>TIP</em>, <em>34</em>, 4440-4455. (<a href='https://doi.org/10.1109/TIP.2025.3586504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Modal Hashing (CMH) has become a powerful technique for large-scale cross-modal retrieval, offering benefits like fast computation and efficient storage. However, most CMH models struggle to adapt to streaming multimodal data in real-time once deployed. Although recent online CMH studies have made progress in this area, they often overlook two key challenges: 1) learning effectively from streaming partial-modal multimodal data, and 2) avoiding the high costs associated with frequent hash function re-training and large-scale updates to database hash codes. To address these issues, we propose Fast Partial-modal Online Cross-Modal Hashing (FPO-CMH), the first approach to tackle online cross-modal hash learning with partial-modal data. This marks a significant shift from previous methods that rely on fully-available multimodal data. Specifically, our approach introduces a multimodal dual-tier anchor bank, initialized using offline training data, which allows offline-trained CMH models to adapt seamlessly to partial-modal data while progressively updating the anchor bank. By leveraging gradient accumulation and asynchronous optimization, FPO-CMH facilitates efficient online cross-modal hash learning. Additionally, an initial-anchor rehearsal strategy is employed to prevent model catastrophic forgetting during online optimization, ensuring the code invariance of database hash codes and eliminating the need for frequent hash function re-training. Extensive experiments validate the superiority of FPO-CMH, especially in handling streaming partial-modal multimodal data, a more realistic scenario. The source codes and datasets are available at https://github.com/DandelionWow/FPO-CMH},
  archive      = {J_TIP},
  author       = {Fengling Li and Yang Sun and Tianshi Wang and Lei Zhu and Xiaojun Chang},
  doi          = {10.1109/TIP.2025.3586504},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4440-4455},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast partial-modal online cross-modal hashing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAP: Masked adversarial perturbation for boosting black-box attack transferability. <em>TIP</em>, <em>34</em>, 4426-4439. (<a href='https://doi.org/10.1109/TIP.2025.3586485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transferability of adversarial examples is vital for black-box attacks, as it enables the adversary to deceive the target model without knowing its internals. Despite numerous methods focusing on transferability, they still struggle with transferring across models with distinct architectural components (e.g., CNNs and ViTs). In this work, we argue that the limited adversarial perturbation diversity leads to overfitting of the surrogate model, which acts as a key factor in reducing transferability. To this end, we propose a Masked Adversarial Perturbation (MAP) method to boost adversarial transferability across various architectures from a novel perspective of diversifying perturbation. Specifically, MAP randomly masks perturbation patches during iterations and compels the remaining ones to retain the attack effect, which diversifies perturbations to mitigate their overfitting to the surrogate model. Naturally, MAP spreads perturbation over local patches to alleviate their co-adaptation and prevent perturbations from overly relying on specific patterns. Consequently, it can deceive convolution operation and self-attention mechanism indiscriminately by attacking their basic input units, i.e., a single patch, showing superior transferability over previous methods. Extensive experiments illustrate that MAP consistently and significantly boosts diverse black-box attacks to achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Kaige Li and Maoxian Wan and Qichuan Geng and Weimin Shi and Xiaochun Cao and Zhong Zhou},
  doi          = {10.1109/TIP.2025.3586485},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4426-4439},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MAP: Masked adversarial perturbation for boosting black-box attack transferability},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTMLNet: Multi-task mutual learning network for infrared small target detection and segmentation. <em>TIP</em>, <em>34</em>, 4414-4425. (<a href='https://doi.org/10.1109/TIP.2025.3587576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection has been extensively studied due to its wide range of applications. Most studies treat infrared small target detection as an independent task, either as a detection-based or a segmentation-based, failing to fully leverage the supervisory information from different annotation forms. To address this issue, we propose a multi-task mutual learning network (MTMLNet) specifically designed for infrared small targets, aiming to enhance both detection and segmentation performance by effectively utilizing various forms of supervisory information. Specifically, we design a multi-stage feature aggregation (MFA) module capable of capturing features with varying gradients and receptive fields simultaneously. Additionally, a hybrid pooling down-sampling (HPDown) module is proposed to mitigate information loss during the down-sampling process of infrared small targets. Finally, the hierarchical feature fusion (HFF) module is designed to adaptively select and fuse features from different semantic layers, learning the optimal way to fuse features across semantic layers. The results on IRSTD-1k and SIRST-V2 datasets show that our proposed MTMLNet achieves state-of-the-art (SOTA) performance in both detection-based and segmentation-based methods. The codes are available at https://github.com/YangBo0411/MTMLNet},
  archive      = {J_TIP},
  author       = {Bo Yang and Fengqian Li and Songliang Zhao and Wei Wang and Jun Luo and Huayan Pu and Mingliang Zhou and Yangjun Pi},
  doi          = {10.1109/TIP.2025.3587576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4414-4425},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MTMLNet: Multi-task mutual learning network for infrared small target detection and segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An alpha-tree algorithm for massively parallel architectures. <em>TIP</em>, <em>34</em>, 4402-4413. (<a href='https://doi.org/10.1109/TIP.2025.3586495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alpha-tree, also known as the quasi-flat zone hierarchy is a widely used representation of images in Mathematical Morphology. This structure organizes the regions according to a similarity criterion into a tree, that eases the multiscale analysis of images. Many alpha-tree algorithms exist and computing this structure efficiently is still an active field of research. Indeed, the alpha-tree is commonly used in remote sensing where there is an urge for fast processing of large terabytes images. In this paper, we propose the first massively parallel alpha-tree algorithm that leverages concurrent union-find data structures to exploit the SIMT (Single Instruction Multiple Threads) programming model of GPUs. Our algorithm outperforms the State-of-the-Art parallel CPU algorithms by a factor of 10 on average on desktop computers and servers. It also opens new perspectives for using Mathematical Morphology methods on GPU pipelines.},
  archive      = {J_TIP},
  author       = {Edwin Carlinet and Quentin Kaci and Nicolas Blin},
  doi          = {10.1109/TIP.2025.3586495},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4402-4413},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An alpha-tree algorithm for massively parallel architectures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFTER: Attention-based fusion router for RGBT tracking. <em>TIP</em>, <em>34</em>, 4386-4401. (<a href='https://doi.org/10.1109/TIP.2025.3586467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal feature fusion as a core investigative component of RGBT tracking emerges numerous fusion studies in recent years. However, existing RGBT tracking methods widely adopt fixed fusion structures to integrate multi-modal feature, which are hard to handle various challenges in dynamic scenarios. To address this problem, this work presents a novel Attention-based Fusion router called AFTER, which optimizes the fusion structure to adapt to the dynamic challenging scenarios, for robust RGBT tracking. In particular, we design a fusion structure space based on the hierarchical attention network, each attention-based fusion unit corresponding to a fusion operation and a combination of these attention units corresponding to a fusion structure. Through optimizing the combination of attention-based fusion units, we can dynamically select the fusion structure to adapt to various challenging scenarios. Unlike complex search of different structures in neural architecture search algorithms, we develop a dynamic routing algorithm, which equips each attention-based fusion unit with a router, to predict the combination weights for efficient optimization of the fusion structure. Extensive experiments on five mainstream RGBT tracking datasets demonstrate the superior performance of the proposed AFTER against state-of-the-art RGBT trackers. We release the code in https://github.com/Alexadlu/AFter},
  archive      = {J_TIP},
  author       = {Andong Lu and Wanyu Wang and Chenglong Li and Jin Tang and Bin Luo},
  doi          = {10.1109/TIP.2025.3586467},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4386-4401},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AFTER: Attention-based fusion router for RGBT tracking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DVMark: A deep multiscale framework for video watermarking. <em>TIP</em>, <em>34</em>, 4371-4385. (<a href='https://doi.org/10.1109/TIP.2023.3251737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video watermarking embeds a message into a cover video in an imperceptible manner, which can be retrieved even if the video undergoes certain modifications or distortions. Traditional watermarking methods are often manually designed for particular types of distortions and thus cannot simultaneously handle a broad spectrum of distortions. To this end, we propose a robust deep learning-based solution for video watermarking that is end-to-end trainable. Our model consists of a novel multiscale design where the watermarks are distributed across multiple spatial-temporal scales. Extensive evaluations on a wide variety of distortions show that our method outperforms traditional video watermarking methods as well as deep image watermarking models by a large margin. We further demonstrate the practicality of our method on a realistic video-editing application.},
  archive      = {J_TIP},
  author       = {Xiyang Luo and Yinxiao Li and Huiwen Chang and Ce Liu and Peyman Milanfar and Feng Yang},
  doi          = {10.1109/TIP.2023.3251737},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4371-4385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DVMark: A deep multiscale framework for video watermarking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StreakNet-arch: An anti-scattering network-based architecture for underwater carrier LiDAR-radar imaging. <em>TIP</em>, <em>34</em>, 4357-4370. (<a href='https://doi.org/10.1109/TIP.2025.3586431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce StreakNet-Arch, a real-time, end-to-end binary-classification framework based on our self-developed Underwater Carrier LiDAR-Radar (UCLR) that embeds Self-Attention and our novel Double Branch Cross Attention (DBC-Attention) to enhance scatter suppression. Under controlled water tank validation conditions, StreakNet-Arch with Self-Attention or DBC-Attention outperforms traditional bandpass filtering and achieves higher $F_{1}$ scores than learning-based MP networks and CNNs at comparable model size and complexity. Real-time benchmarks on an NVIDIA RTX 3060 show a constant Average Imaging Time (54 to 84 ms) regardless of frame count, versus a linear increase (58 to 1,257 ms) for conventional methods. To facilitate further research, we contribute a publicly available streak-tube camera image dataset contains 2,695,168 real-world underwater 3D point cloud data. More importantly, we validate our UCLR system in a South China Sea trial, reaching an error of 46mm for 3D target at 1,000 m depth and 20 m range. Source code and data are available at https://github.com/BestAnHongjun/StreakNet},
  archive      = {J_TIP},
  author       = {Xuelong Li and Hongjun An and Haofei Zhao and Guangying Li and Bo Liu and Xing Wang and Guanghua Cheng and Guojun Wu and Zhe Sun},
  doi          = {10.1109/TIP.2025.3586431},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4357-4370},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {StreakNet-arch: An anti-scattering network-based architecture for underwater carrier LiDAR-radar imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint visual compression and perception framework for neuromorphic spiking camera. <em>TIP</em>, <em>34</em>, 4343-4356. (<a href='https://doi.org/10.1109/TIP.2025.3581372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Neuromorphic spike cameras has garnered significant attention for their ability to capture continuous motion with unparalleled temporal resolution. However, this imaging attribute necessitates considerable resources for binary spike data storage and transmission. In light of compression and spike-driven intelligent applications, we present the notion of Spike Coding for Intelligence (SCI), wherein spike sequences are compressed and optimized for both bit-rate and task performance. Drawing inspiration from the mammalian vision system, we propose a dual-pathway architecture for separate processing of spatial semantics and motion information, which is then merged to produce features for compression. A refinement scheme is also introduced to ensure consistency between decoded features and motion vectors. We further propose a temporal regression approach that integrates various motion dynamics, capitalizing on the advancements in warping and deformation simultaneously. Comprehensive experiments demonstrate our scheme achieves state-of-the-art (SOTA) performance for spike compression and analysis. We achieve an average 17.25% BD-rate reduction compared to SOTA codecs and a 4.3% accuracy improvement over SpiReco for spike-based classification, with 88.26% complexity reduction and 42.41% inference time saving on the encoding side.},
  archive      = {J_TIP},
  author       = {Kexiang Feng and Chuanmin Jia and Siwei Ma and Wen Gao},
  doi          = {10.1109/TIP.2025.3581372},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4343-4356},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A joint visual compression and perception framework for neuromorphic spiking camera},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification for incomplete multi-view data using divergence measures. <em>TIP</em>, <em>34</em>, 4328-4342. (<a href='https://doi.org/10.1109/TIP.2025.3579987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-view classification and clustering methods typically improve task accuracy by leveraging and fusing information from different views. However, ensuring the reliability of multi-view integration and final decisions is crucial, particularly when dealing with noisy or corrupted data. Current methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty of network predictions, ignoring domain gaps between different modalities. To address this issue, KPHD-Net, based on Hölder divergence, is proposed for multi-view classification and clustering tasks. Generally, our KPHD-Net employs a variational Dirichlet distribution to represent class probability distributions, models evidences from different views, and then integrates it with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation effects. Our theoretical analysis demonstrates that Proper Hölder divergence offers a more effective measure of distribution discrepancies, ensuring enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence theory, recognized for its superior performance in multi-view fusion tasks, is introduced and combined with the Kalman filter to provide future state estimations. This integration further enhances the reliability of the final fusion results. Extensive experiments show that the proposed KPHD-Net outperforms the current state-of-the-art methods in both classification and clustering tasks regarding accuracy, robustness, and reliability, with theoretical guarantees.},
  archive      = {J_TIP},
  author       = {Zhipeng Xue and Yan Zhang and Ming Li and Chun Li and Yue Liu and Fei Yu},
  doi          = {10.1109/TIP.2025.3579987},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4328-4342},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty quantification for incomplete multi-view data using divergence measures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft neighbors supported contrastive clustering. <em>TIP</em>, <em>34</em>, 4315-4327. (<a href='https://doi.org/10.1109/TIP.2025.3583194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep clustering methods leverage contrastive or non-contrastive learning to facilitate downstream tasks. Most contrastive-based methods typically learn representations by comparing positive pairs (two views of the same sample) against negative pairs (views of different samples). However, we spot that this hard treatment of samples ignores inter-sample relationships, leading to class collisions and degrade clustering performances. In this paper, we propose a soft neighbor supported contrastive clustering method to address this issue. Specifically, we first introduce a concept called perception radius to quantify similarity confidence between a sample and its neighbors. Based on this insight, we design a two-level soft neighbor loss that captures both local and global neighborhood relationships. Additionally, a cluster-level loss enforces compact and well-separated cluster distributions. Finally, we conduct a pseudo-label refinement strategy to mitigate false negative samples. Extensive experiments on benchmark datasets demonstrate the superiority of our method. The code is available at https://github.com/DuannYu/soft-neighbors-supported-clustering},
  archive      = {J_TIP},
  author       = {Yu Duan and Huimin Chen and Runxin Zhang and Rong Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TIP.2025.3583194},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4315-4327},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Soft neighbors supported contrastive clustering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coarse-to-fine multi-hypothesis method for ambiguous hand pose estimation. <em>TIP</em>, <em>34</em>, 4302-4314. (<a href='https://doi.org/10.1109/TIP.2025.3578256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hand pose estimation, challenges such as occlusion often result in partial observation of a human hand, making it difficult to uniquely determine the hand pose, thus leading to ambiguity in certain hand regions. Heatmap-based methods may struggle with locating ambiguous joints and end up violating physiological constraints in their predictions. Parametric model based single-solution methods often fail to adequately address this ambiguity issue due to the inherent one-to-many mappings between input and output, resulting in unstable regression. While some existing multi-hypothesis methods have improved diversity by directly modeling the distribution of ambiguous hypotheses, their localization accuracy still falls short compared to the recent single-solution methods. To achieve quality results in both diversity and accuracy, we propose a novel multi-hypothesis approach for hand pose estimation, by progressively integrating heatmap information into the distribution of ambiguous poses using a RANSAC-like strategy. It starts with a conditional-flow model to provide an initial estimate of a coarse distribution over ambiguous joint poses. This is followed by randomly sampling multiple hypotheses, projecting each of them onto 2D heatmap plane, and employing consensus checks to identify unambiguous joints that adhere to skeletal constraints. Joint features are then resampled, with mismatches due to incorrect estimations being eliminated. Finally, we refine the distribution of ambiguous poses using graph neural networks and attention mechanisms. Extensive empirical experiments are carried out, where our approach are carefully examined both qualitatively and quantitatively. It is shown to not only produce more diverse & feasible pose hypotheses than existing multi-hypothesis methods, but also achieves accurate localization results comparable to the state-of-the-art single-solution methods.},
  archive      = {J_TIP},
  author       = {Yuting Ge and Chi Xu and Li Cheng},
  doi          = {10.1109/TIP.2025.3578256},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4302-4314},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A coarse-to-fine multi-hypothesis method for ambiguous hand pose estimation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCTC: Lightweight convolutional thresholding sparse coding network prior for compressive hyperspectral imaging. <em>TIP</em>, <em>34</em>, 4286-4301. (<a href='https://doi.org/10.1109/TIP.2025.3583951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive spectral imaging has garnered significant attention for its ability to effectively enhance the captured spatial and spectral information. Predominant methods, based on compressive sensing, typically formulate the imaging task as a constrained optimization problem and rely on hand-crafted priors to model the sparsity of spectral images. However, these approaches often suffer from suboptimal performance due to the inherent difficulty of identifying an appropriate transform space where spectral images exhibit sparsity. To overcome this limitation, we propose a novel convolutional sparse coding-inspired untrained network prior for fast and adaptive identification of the sparse transform domain and compressible signal. Specifically, a Lightweight Convolutional Thresholding sparse Coding (LCTC) network is designed as the sparse transform domain, with its inputs interpreted as sparse coefficients. Crucially, both the transform domain and its coefficients are solved in a self-supervised learning manner. Furthermore, we demonstrate that LCTC prior can be seamlessly incorporated into the iterative optimization algorithm as a Plug-and-Play (PnP) regularization. Both the LCTC and PnP-LCTC exhibit superior performance compared to previous methods. Experiments under various scenarios validate the effectiveness and efficiency of our approach.},
  archive      = {J_TIP},
  author       = {Yurong Chen and Yaonan Wang and Xiaodong Wang and Xin Yuan and Hui Zhang},
  doi          = {10.1109/TIP.2025.3583951},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4286-4301},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LCTC: Lightweight convolutional thresholding sparse coding network prior for compressive hyperspectral imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dispersal and collaborative clustering for few-shot unsupervised domain adaptation. <em>TIP</em>, <em>34</em>, 4273-4285. (<a href='https://doi.org/10.1109/TIP.2025.3581007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is mainly focused on the tasks of transferring knowledge from a fully-labeled source domain to an unlabeled target domain. However, in some scenarios, the labeled data are expensive to collect, which cause an insufficient label issue in the source domain. To tackle this issue, some works have focused on few-shot unsupervised domain adaptation (FUDA), which transfers predictive models to an unlabeled target domain through a source domain that only contains a few labeled samples. Yet the relationship between labeled and unlabeled source domains are not well exploited in generating pseudo-labels. Additionally, the few-shot setting further prevents the transfer tasks as an excessive domain gap is introduced between the source and target domains. To address these issues, we newly proposed an adaptive dispersal and collaborative clustering (ADCC) method for FUDA. Specifically, for the shortage of the labeled source data, a collaborative clustering algorithm is constructed that expands the labeled source data to obtain more distribution information. Furthermore, to alleviate the negative impact of domain-irrelevant information, we construct an adaptive dispersal strategy that introduces an intermediate domain and pushes both the source and target domains to this intermediate domain. Extensive experiments on the Office31, Office-Home, miniDomainNet, and VisDA-2017 datasets showcase the superior performance of ADCC compared to the state-of-the-art FUDA methods.},
  archive      = {J_TIP},
  author       = {Yuwu Lu and Haoyu Huang and Wai Keung Wong and Xue Hu and Zhihui Lai and Xuelong Li},
  doi          = {10.1109/TIP.2025.3581007},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4273-4285},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive dispersal and collaborative clustering for few-shot unsupervised domain adaptation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCPI-depth: Explicitly infusing dense correspondence prior to unsupervised monocular depth estimation. <em>TIP</em>, <em>34</em>, 4258-4272. (<a href='https://doi.org/10.1109/TIP.2025.3581422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion. A key challenge in this field is achieving robust and accurate depth estimation in regions with weak textures or where dynamic objects are present. This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints. The first novel contribution is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels. The second novel contribution arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient. A differential property correlation loss is therefore designed to refine depth estimation with a specific emphasis on local variations. The third novel contribution is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses. DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts. Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness. Our source code is publicly available at https://mias.group/DCPI-Depth.},
  archive      = {J_TIP},
  author       = {Mengtan Zhang and Yi Feng and Qijun Chen and Rui Fan},
  doi          = {10.1109/TIP.2025.3581422},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4258-4272},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DCPI-depth: Explicitly infusing dense correspondence prior to unsupervised monocular depth estimation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse target and contribution scheduling for domain generalization. <em>TIP</em>, <em>34</em>, 4242-4257. (<a href='https://doi.org/10.1109/TIP.2025.3581012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalization under distribution shifts has been a great challenge in computer vision. The prevailing practice of directly employing the one-hot labels as the training targets in domain generalization (DG) can lead to gradient conflicts, making it insufficient for capturing the intrinsic class characteristics and hard to increase the intra-class variation. Besides, existing methods in DG mostly overlook the distinct contributions of source (seen) domains, resulting in uneven learning from these domains. To address these issues, we first present a theoretical and empirical analysis on the existence of gradient conflicts in DG, unveiling the previously unexplored relationship between distribution shifts and gradient conflicts during optimization process. In this paper, we present a novel perspective of DG from the empirical source domain’s risk, and propose a new paradigm for DG called Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution Balance (DCB), with the aim of addressing the limitations associated with the common utilization of one-hot labels and equal contributions for source domains in DG. In specific, DTS employs distinct soft labels as training targets to account for various feature distributions across domains and thereby mitigates the gradient conflicts, and DCB dynamically balances the contributions of source domains by ensuring a fair decline in losses of different source domains. Extensive experiments with analysis on four benchmark datasets show that the proposed method achieves a competitive performance in comparison with the state-of-the-art approaches, demonstrating the effectiveness and advantages of the proposed DTCS. The source code will be available at https://github.com/longshaocong/DTCS},
  archive      = {J_TIP},
  author       = {Shaocong Long and Qianyu Zhou and Chenhao Ying and Lizhuang Ma and Yuan Luo},
  doi          = {10.1109/TIP.2025.3581012},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4242-4257},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diverse target and contribution scheduling for domain generalization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLASH: Complementary learning with neural architecture search for gait recognition. <em>TIP</em>, <em>34</em>, 4230-4241. (<a href='https://doi.org/10.1109/TIP.2024.3360870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition, which aims at identifying individuals by their walking patterns, has achieved great success based on silhouette. The binary silhouette sequence encodes the walking pattern within the sparse boundary representation. Therefore, most pixels in the silhouette are under-sensitive to the walking pattern since the sparse boundary lacks dense spatial-temporal information, which is suitable to be represented with dense texture. To enhance the sensitivity to the walking pattern while maintaining the robustness of recognition, we present a Complementary Learning with neural Architecture SearcH (CLASH) framework, consisting of walking pattern sensitive gait descriptor named dense spatial-temporal field (DSTF) and neural architecture search based complementary learning (NCL). Specifically, DSTF transforms the representation from the sparse binary boundary into the dense distance-based texture, which is sensitive to the walking pattern at the pixel level. Further, NCL presents a task-specific search space for complementary learning, which mutually complements the sensitivity of DSTF and the robustness of the silhouette to represent the walking pattern effectively. Extensive experiments demonstrate the effectiveness of the proposed methods under both in-the-lab and in-the-wild scenarios. On CASIA-B, we achieve rank-1 accuracy of 98.8%, 96.5%, and 89.3% under three conditions. On OU-MVLP, we achieve rank-1 accuracy of 91.9%. Under the latest in-the-wild datasets, we outperform the latest silhouette-based methods by 16.3% and 19.7% on Gait3D and GREW, respectively.},
  archive      = {J_TIP},
  author       = {Huanzhang Dou and Pengyi Zhang and Yuhan Zhao and Lu Jin and Xi Li},
  doi          = {10.1109/TIP.2024.3360870},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4230-4241},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLASH: Complementary learning with neural architecture search for gait recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NSB-H2GAN: “Negative sample”-boosted hierarchical heterogeneous graph attention network for interpretable classification of whole-slide images. <em>TIP</em>, <em>34</em>, 4215-4229. (<a href='https://doi.org/10.1109/TIP.2025.3583127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gigapixel whole-slide image (WSI) prediction and region-of-interest localization present considerable challenges due to the diverse range of features both across different slides and within individual slides. Most current methods rely on weakly supervised learning using homogeneous graphs to establish context-aware relevance within slides, often neglecting the rich diversity of heterogeneous information inherent in pathology images. Inspired by the negative sampling strategy of the Determinantal Point Process (DPP) and the hierarchical structure of pathology slides, we introduce the Negative Sample Boosted Hierarchical Heterogeneous Graph Attention Network (NSB-H2GAN). This model addresses the over-smoothing issue typically encountered in classical Graph Convolutional Networks (GCNs) when applied to pathology slides. By incorporating “negative samples” at multiple scales and utilizing hierarchical, heterogeneous feature discrimination, NSB-H2GAN more effectively captures the unique features of each patch, leading to an improved representation of gigapixel WSIs. We evaluated the performance of NSB-H2GAN on three publicly available datasets: CAMELYON16, TCGA-NSCLC and TCGA-COAD. The results show that NSB-H2GAN significantly outperforms existing state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, NSB-H2GAN generates more detailed and interpretable heatmaps, allowing for precise localization of tiny lesions as small as $200\mu m\times 200\mu m$ that are often missed by the human eye. The robust performance of NSB-H2GAN offers a new paradigm for computer-aided pathology diagnosis and holds great potential for advancing the clinical applications of computational pathology.},
  archive      = {J_TIP},
  author       = {Meiyan Liang and Shupeng Zhang and Xikai Wang and Bo Li and Muhammad Hamza Javed and Xiaojun Jia and Lin Wang},
  doi          = {10.1109/TIP.2025.3583127},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4215-4229},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NSB-H2GAN: “Negative sample”-boosted hierarchical heterogeneous graph attention network for interpretable classification of whole-slide images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view clustering with incremental instances and views. <em>TIP</em>, <em>34</em>, 4203-4214. (<a href='https://doi.org/10.1109/TIP.2025.3583122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) has attracted increasing attention with the emergence of various data collected from multiple sources. In real-world dynamic environment, instances are continually gathered, and the number of views expands as new data sources become available. Learning for such simultaneous increment of instances and views, particularly in unsupervised scenarios, is crucial yet underexplored. In this paper, we address this problem by proposing a novel MVC method with Incremental Instances and Views, MVC-IIV for short. MVC-IIV contains two stages, an initial stage and an incremental stage. In the initial stage, a basic latent multi-view subspace clustering model is constructed to handle existing data, which can be viewed as traditional static MVC. In the incremental stage, the previously trained model is reused to guide learning for newly arriving instances with new views, transferring historical knowledge while avoiding redundant computations. In specific, we design and reuse two modules, i.e., multi-view embedding module for low-dimensional representation learning, and consensus centroids module for cluster probability learning. By adding consistency regularization on the two modules, the knowledge acquired from previous data is used, which not only enhances the exploration within current data batch, but also extracts the between-batch data correlations. The proposed model can be efficiently solved with linear space and time complexity. Extensive experiments demonstrate the effectiveness and efficiency of our method compared with the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Chao Zhang and Zhi Wang and Xiuyi Jia and Zechao Li and Chunlin Chen and Huaxiong Li},
  doi          = {10.1109/TIP.2025.3583122},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4203-4214},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view clustering with incremental instances and views},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDCoT++: Improved static-dynamic co-teaching for class-incremental 3D object detection. <em>TIP</em>, <em>34</em>, 4188-4202. (<a href='https://doi.org/10.1109/TIP.2024.3518774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have demonstrated high effectiveness in 3D object detection tasks. However, they often suffer from a notable drop in performance on the previously trained classes when learning new classes incrementally without revisiting the old data. This is the “catastrophic forgetting” phenomenon which impedes 3D object detection in real-world scenarios, where intelligent machines must continuously learn to detect previously unseen categories. Furthermore, frequent co-occurrences of old and new classes in scenes exacerbate catastrophic forgetting and cause model confusion. To address these challenges, we propose a novel static-dynamic co-teaching approach. Our framework involves a student model and two teacher models: a static teacher with fixed weights which imparts preserved old knowledge to the student, and a dynamic teacher with continuously updated weights which transfers underlying knowledge from new data to the student. To mitigate the issue of co-occurrence, we generate pseudo labels for base (i.e. old) classes from both static and dynamic sources during incremental learning. Additionally, to mitigate the negative impact of varying occurrence frequencies of classes on fixed thresholding during the selection of pseudo labels, we calibrate the probabilities of base classes to attain more balanced class probabilities. Moreover, our static-dynamic co-teaching framework is backbone-agnostic, making it compatible with different detection architectures. We demonstrate its backbone-agnostic nature by adapting three representative 3D object detectors: VoteNet, 3DETR and CAGroup3D. Extensive experiments showcase the superior performance of our proposed method compared to baseline approaches across indoor and outdoor benchmark datasets and applicability with different backbone models.},
  archive      = {J_TIP},
  author       = {Na Zhao and Peisheng Qian and Fang Wu and Xun Xu and Xulei Yang and Gim Hee Lee},
  doi          = {10.1109/TIP.2024.3518774},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4188-4202},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SDCoT++: Improved static-dynamic co-teaching for class-incremental 3D object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curriculum dataset distillation. <em>TIP</em>, <em>34</em>, 4176-4187. (<a href='https://doi.org/10.1109/TIP.2025.3579228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most dataset distillation methods struggle to accommodate large-scale datasets due to their substantial computational and memory requirements. Recent research has begun to explore scalable disentanglement methods. However, there are still performance bottlenecks and room for optimization in this direction. In this paper, we present a curriculum-based dataset distillation framework aiming to harmonize performance and scalability. This framework strategically distills synthetic images, adhering to a curriculum that transitions from simple to complex. By incorporating curriculum evaluation, we address the issue of previous methods generating images that tend to be homogeneous and simplistic, doing so at a manageable computational cost. Furthermore, we introduce adversarial optimization towards synthetic images to further improve their representativeness and safeguard against their overfitting to the neural network involved in distilling. This enhances the generalization capability of the distilled images across various neural network architectures and also increases their robustness to noise. Extensive experiments demonstrate that our framework sets new benchmarks in large-scale dataset distillation, achieving substantial improvements of 11.1% on Tiny-ImageNet, 9.0% on ImageNet-1K, and 7.3% on ImageNet-21K. Our distilled datasets and code are available at https://github.com/MIV-XJTU/CUDD},
  archive      = {J_TIP},
  author       = {Zhiheng Ma and Anjia Cao and Funing Yang and Yihong Gong and Xing Wei},
  doi          = {10.1109/TIP.2025.3579228},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4176-4187},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Curriculum dataset distillation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S4Fusion: Saliency-aware selective state space model for infrared and visible image fusion. <em>TIP</em>, <em>34</em>, 4161-4175. (<a href='https://doi.org/10.1109/TIP.2025.3583132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preservation and the enhancement of complementary features between modalities are crucial for multi-modal image fusion and downstream vision tasks. However, existing methods are limited to local receptive fields (CNNs) or lack comprehensive utilization of spatial information from both modalities during interaction (transformers), which results in the inability to effectively retain useful information from both modalities in a comparative manner. Consequently, the fused images may exhibit a bias towards one modality, failing to adaptively preserve salient targets from all sources. Thus, a novel fusion framework (S4Fusion) based on the Saliency-aware Selective State Space is proposed. S4Fusion introduces the Cross-Modal Spatial Awareness Module (CMSA), which is designed to simultaneously capture global spatial information from all input modalities and promote effective cross-modal interaction. This enables a more comprehensive representation of complementary features. Furthermore, to guide the model in adaptively preserving salient objects, we propose a novel perception-enhanced loss function. This loss aims to enhance the retention of salient features by minimizing ambiguity or uncertainty, as measured at a pre-trained model’s decision layer, within the fused images. The code is available at https://github.com/zipper112/S4Fusion},
  archive      = {J_TIP},
  author       = {Haolong Ma and Hui Li and Chunyang Cheng and Gaoang Wang and Xiaoning Song and Xiao-Jun Wu},
  doi          = {10.1109/TIP.2025.3583132},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4161-4175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {S4Fusion: Saliency-aware selective state space model for infrared and visible image fusion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperTaFOR: Task-adaptive few-shot open-set recognition with spatial-spectral selective transformer for hyperspectral imagery. <em>TIP</em>, <em>34</em>, 4148-4160. (<a href='https://doi.org/10.1109/TIP.2025.3555069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set recognition (OSR) aims to accurately classify known categories while effectively rejecting unknown negative samples. Existing methods for OSR in hyperspectral images (HSI) can be generally divided into two categories: reconstruction-based and distance-based methods. Reconstruction-based approaches focus on analyzing reconstruction errors during inference, whereas distance-based methods determine the rejection of unknown samples by measuring their distance to each prototype. However, these techniques often require a substantial amount of training data, which can be both time-consuming and expensive to gather, and they require manual threshold setting, which can be difficult for different tasks. Furthermore, effectively utilizing spectral-spatial information in HSI remains a significant challenge, particularly in open-set scenarios. To tackle these challenges, we introduce a few-shot OSR framework for HSI named HyperTaFOR, which incorporates a novel spatial-spectral selective transformer (S3Former). This framework employs a meta-learning strategy to implement a negative prototype generation module (NPGM) that generates task-adaptive rejection scores, allowing flexible categorization of samples into various known classes and anomalies for each task. Additionally, the S3Former is designed to extract spectral-spatial features, optimizing the use of central pixel information while reducing the impact of irrelevant spatial data. Comprehensive experiments conducted on three benchmark hyperspectral datasets show that our proposed method delivers competitive classification and detection performance in open-set environments when compared to state-of-the-art methods. The code is available online at https://github.com/B-Xi/TIP_2025_HyperTaFOR.},
  archive      = {J_TIP},
  author       = {Bobo Xi and Wenjie Zhang and Jiaojiao Li and Rui Song and Yunsong Li},
  doi          = {10.1109/TIP.2025.3555069},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4148-4160},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HyperTaFOR: Task-adaptive few-shot open-set recognition with spatial-spectral selective transformer for hyperspectral imagery},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to learn transferable generative attack for person re-identification. <em>TIP</em>, <em>34</em>, 4134-4147. (<a href='https://doi.org/10.1109/TIP.2025.3558434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based person re-identification (re-id) models are widely employed in surveillance systems and inevitably inherit the vulnerability of deep networks to adversarial attacks. Existing attacks merely consider cross-dataset and cross-model transferability, ignoring the cross-test capability to perturb models trained in different domains. To powerfully examine the robustness of real-world re-id models, the Meta Transferable Generative Attack (MTGA) method is proposed, which adopts meta-learning optimization to promote the generative attacker producing highly transferable adversarial examples by learning comprehensively simulated transfer-based cross-model&dataset&test black-box meta attack tasks. Specifically, cross-model&dataset black-box attack tasks are first mimicked by selecting different re-id models and datasets for meta-train and meta-test attack processes. As different models may focus on different feature regions, the Perturbation Random Erasing module is further devised to prevent the attacker from learning to only corrupt model-specific features. To boost the attacker learning to possess cross-test transferability, the Normalization Mix strategy is introduced to imitate diverse feature embedding spaces by mixing multi-domain statistics of target models. Extensive experiments show the superiority of MTGA, especially in cross-model&dataset and cross-model&dataset&test attacks, our MTGA outperforms the SOTA methods by 20.0% and 11.3% on mean mAP drop rate, respectively. The source codes are available at https://github.com/yuanbianGit/MTGA},
  archive      = {J_TIP},
  author       = {Yuan Bian and Min Liu and Xueping Wang and Yunfeng Ma and Yaonan Wang},
  doi          = {10.1109/TIP.2025.3558434},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4134-4147},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to learn transferable generative attack for person re-identification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SiamTITP: Incorporating temporal information and trajectory prediction siamese network for satellite video object tracking. <em>TIP</em>, <em>34</em>, 4120-4133. (<a href='https://doi.org/10.1109/TIP.2025.3573527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is considered as a template matching task. Traditional and deep learning-based methods have achieved high performance in satellite video object tracking (SVOT). However, existing methods still suffer from insufficiently discriminative features, complex approaches to handling occlusion, and excessive hyperparameters. In response to these issues, we propose a simple, yet effective Siamese network, termed SiamTITP. A temporal information (TI) submodule is developed, which integrates temporal cues by dynamically updating the template to enhance discriminative features. Furthermore, we propose a structurally simple trajectory prediction (TP) submodule, which solely utilizes polynomial function for fitting historical results to assist the network in addressing occlusion. In an effort to reduce hyperparameters, we forgo feature fusion steps and weighted results, while we propose an adaptive occlusion judgment metrics based on the target size. To validate the efficacy of our approach, we conducted extensive experiments on three large satellite video datasets, namely the SatSOT, SV248S and OOTB datasets. Code and train models are publicly available at https://github.com/jiawei-zhou/SiamTITP},
  archive      = {J_TIP},
  author       = {Jiawei Zhou and Yanni Dong and Bo Du},
  doi          = {10.1109/TIP.2025.3573527},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4120-4133},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SiamTITP: Incorporating temporal information and trajectory prediction siamese network for satellite video object tracking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive conditional latent diffusion for audio-visual segmentation. <em>TIP</em>, <em>34</em>, 4108-4119. (<a href='https://doi.org/10.1109/TIP.2025.3580269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual Segmentation (AVS) is conceptualized as a conditional generation task, where audio is considered as the conditional variable for segmenting the sound producer(s). In this case, audio should be extensively explored to maximize its contribution for the final segmentation task. We propose a contrastive conditional latent diffusion model for audio-visual segmentation (AVS) to thoroughly investigate the impact of audio, where the correlation between audio and the final segmentation map is modeled to guarantee the strong correlation between them. To achieve semantic-correlated representation learning, our framework incorporates a latent diffusion model. The diffusion model learns the conditional generation process of the ground-truth segmentation map, resulting in ground-truth aware inference during the denoising process at the test stage. As our model is conditional, it is vital to ensure that the conditional variable contributes to the model output. We thus extensively model the contribution of the audio signal by minimizing the density ratio between the conditional probability of the multimodal data, e.g. conditioned on the audio-visual data, and that of the unimodal data, e.g. conditioned on the audio data only. In this way, our latent diffusion model via density ratio optimization explicitly maximizes the contribution of audio for AVS, which can then be achieved with contrastive learning as a constraint, where the diffusion part serves as the main objective to achieve maximum likelihood estimation, and the density ratio optimization part imposes the constraint. By adopting this latent diffusion model via contrastive learning, we effectively enhance the contribution of audio for AVS. The effectiveness of our solution is validated through experimental results on the benchmark dataset. Code and results are online via our project page: https://github.com/OpenNLPLab/DiffusionAVS},
  archive      = {J_TIP},
  author       = {Yuxin Mao and Jing Zhang and Mochu Xiang and Yunqiu Lv and Dong Li and Yiran Zhong and Yuchao Dai},
  doi          = {10.1109/TIP.2025.3580269},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4108-4119},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contrastive conditional latent diffusion for audio-visual segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Background matters: A cross-view bidirectional modeling framework for semi-supervised medical image segmentation. <em>TIP</em>, <em>34</em>, 4092-4107. (<a href='https://doi.org/10.1109/TIP.2025.3573516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data to reduce reliance on manually annotated images. However, current SOTA approaches predominantly focus on foreground-oriented modeling (i.e., segmenting only the foreground region) and have largely overlooked the potential benefits of explicitly modeling the background region. Our study theoretically and empirically demonstrates that highly certain predictions in background modeling enhance the confidence of corresponding foreground modeling. Building on this insight, we propose the Cross-view Bidirectional Modeling (CVBM) framework, which introduces a novel perspective by incorporating background modeling to improve foreground modeling performance. Within CVBM, background modeling serves as an auxiliary perspective, providing complementary supervisory signals to enhance the confidence of the foreground model. Additionally, CVBM introduces an innovative bidirectional consistency mechanism, which ensures mutual alignment between foreground predictions and background-guided predictions. Extensive experiments demonstrate that our approach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets. Notably, on the Pancreas dataset, CVBM outperforms fully supervised methods (i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data. Our code is publicly available at https://github.com/caoluyang0830/CVBM.git},
  archive      = {J_TIP},
  author       = {Luyang Cao and Jianwei Li and Yinghuan Shi},
  doi          = {10.1109/TIP.2025.3573516},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4092-4107},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Background matters: A cross-view bidirectional modeling framework for semi-supervised medical image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ring artifacts removal based on implicit neural representation of sinogram data. <em>TIP</em>, <em>34</em>, 4080-4091. (<a href='https://doi.org/10.1109/TIP.2025.3581003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inconsistent responses of X-ray detector elements lead to stripe artifacts within the sinogram data, which subsequently manifest as ring artifacts in the reconstructed computed tomography (CT) images, severely degrading image quality. This paper presents a novel method for correcting stripe artifacts in the sinogram data by separating the sinogram into an Ideal Sinogram (IS) and Stripe Artifacts (SA), with both components parameterized through Implicit Neural Representations (INR). The proposed method leverages INR to correct defective pixel response values using implicit continuous functions while simultaneously learning stripe features in the angular direction of the sinogram data. These two components, IS and SA, are combined within an optimization constraint framework, achieving unsupervised iterative correction of stripe artifacts in the projection domain. Experimental results demonstrate that the proposed method significantly outperforms current state-of-the-art techniques in effectively removing ring artifacts while maintaining the clarity and fidelity of CT images, thereby enhancing the overall diagnostic quality of CT imaging.},
  archive      = {J_TIP},
  author       = {Ligen Shi and Xu Jiang and Yunze Liu and Chang Liu and Ping Yang and Shifeng Guo and Xing Zhao},
  doi          = {10.1109/TIP.2025.3581003},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4080-4091},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ring artifacts removal based on implicit neural representation of sinogram data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiobjective evolutionary multiscale transformer incorporating fractal features for steel materials quality analytics. <em>TIP</em>, <em>34</em>, 4067-4079. (<a href='https://doi.org/10.1109/TIP.2025.3580320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface quality of steel materials is significantly influenced by processing conditions, which may result in roughness, flatness deviations, and various surface defects. However, the diversity of defect types and the limited size of labeled datasets pose challenges for accurate and efficient defect identification. To address these challenges, this paper proposes a multiobjective evolutionary multiscale Transformer incorporating fractal features for surface quality analytics of steel materials. Specifically, a multiscale Transformer is constructed, consisting of the convolutional tokenization architecture embedded with the multiscale attention module (MAM) and stacked Transformer encoders, enabling the model to effectively capture both morphological patterns and local defect details. In addition, a novel fractal dimension feature fusion module (FDFFM) is introduced to describe the irregularity of defect textures, enhancing feature representation. To achieve a balance between recognition accuracy and model complexity, a multiobjective evolutionary algorithm (MOEA) is employed, with the final model selected based on a knee point selection strategy to support decision-making. Experimental results validate the superior performance and efficiency of MOEA-FM-Trans compared to state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Kainan Zhang and Chang Liu and Lixin Tang},
  doi          = {10.1109/TIP.2025.3580320},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4067-4079},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multiobjective evolutionary multiscale transformer incorporating fractal features for steel materials quality analytics},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WS-SAM: Generalizing SAM to weakly supervised object detection with category label. <em>TIP</em>, <em>34</em>, 4052-4066. (<a href='https://doi.org/10.1109/TIP.2025.3581729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building an effective object detector usually depends on large well-annotated training samples. While annotating such dataset is extremely laborious and costly, where box-level supervision which contains both accurate classification category and localization coordinate is required. Compared to above box-level supervised annotation, those weakly supervised learning manners (e.g, category, point and scribble) need relatively less laborious annotation cost, and provide a feasible way to mitigate the reliance on the dataset. Because of the lack of sufficient supervised information, current weakly supervised methods cannot achieve satisfactory detection performance. Recently, Segment Anything Model (SAM) has appeared as a task-agnostic foundation model and shown promising performance improvement in many related works due to its powerful generalization and data processing abilities. The properties of the SAM inspire us to adopt such basic benchmark to weakly supervised object detection field to compensate the deficiencies in supervised information. However, directly deploying SAM on weakly supervised object detection task meets with two issues. Firstly, SAM needs meticulously-designed prompts, and such expert-level prompts restrict their applicability and practicality. Besides, SAM is a category unawareness model, and it cannot assign the category labels to the generated predictions. To solve above issues, we propose WS-SAM, which generalizes Segment Anything Model (SAM) to weakly supervised object detection with category label. Specifically, we design an adaptive prompt generator to take full advantages of the spatial and semantic information from the prompt. It employs in a self-prompting manner by taking the output of SAM from the previous iteration as the prompt input to guide the next iteration, where the prompts can be adaptively generated based on the classification activation map. We also develop a segmentation mask refinement module and formulate the label assignment process as a shortest path optimization problem by considering the similarity between each location and prompts. Furthermore, a bidirectional adapter is also implemented to resolve the domain discrepancy by incorporating domain-specific information. We evaluate the effectiveness of our method on several detection datasets (e.g., PASCAL VOC and MS COCO), and the experiment results show that our proposed method can achieve clear improvement over state-of-the-art methods, while performing favorably against state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Hao Wang and Tong Jia and Qilong Wang and Wangmeng Zuo},
  doi          = {10.1109/TIP.2025.3581729},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4052-4066},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {WS-SAM: Generalizing SAM to weakly supervised object detection with category label},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wavelet-guided deep unfolding network for single image reflection removal. <em>TIP</em>, <em>34</em>, 4040-4051. (<a href='https://doi.org/10.1109/TIP.2025.3581418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing unwanted reflections from images is a fundamental yet challenging problem in low-level computer vision. Recent deep learning-based Single Image Reflection Removal (SIRR) methods have made significant progress. However, separating reflections from transmission content remains difficult, particularly in complex scenes where the two exhibit high visual similarity. Upon careful analysis, we find that reflections predominantly reside in the high-frequency components of an image. These reflections tend to distort fine details in the high-frequency range, while the low-frequency information remains relatively less affected. This observation motivates us to explore a frequency-aware approach for SIRR by leveraging the Discrete Wavelet Transform (DWT). The wavelet decomposition enables us to distinguish and isolate reflective artifacts in the frequency domain while preserving the transmission information. Building on this insight, we propose a novel Wavelet-guided Deep Unfolding Network (WDUNet) that leverages the strengths of wavelet decomposition and deep unfolding techniques to improve interpretability and generalization in SIRR. Specifically, we formulate an optimization-based reflection removal model using DWT and convolutional dictionaries. The proposed model is optimized via a proximal gradient algorithm and then unfolded into a neural network architecture, where all parameters are learned end-to-end during training. By combining wavelet domain analysis with deep unfolding, WDUNet enhances both the interpretability and generalization of SIRR methods. Additionally, we design and integrate the Low-frequency Parameter Estimation Module (LPEM) and High-frequency Parameter Estimation Module (HPEM) modules into WDUNet, allowing the network to automatically learn and optimize the models’ hyperparameters. Extensive experiments conducted on four benchmark datasets demonstrate that WDUNet consistently outperforms existing state-of-the-art methods in both objective evaluation metrics and subjective visual quality.},
  archive      = {J_TIP},
  author       = {Ya-Nan Zhang and Qiufu Li and Xu Wu and Nan Mu and Xiaoning Li and Linlin Shen},
  doi          = {10.1109/TIP.2025.3581418},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4040-4051},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A wavelet-guided deep unfolding network for single image reflection removal},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCLNet: Double collaborative learning network on stationary-dynamic functional brain network for brain disease classification. <em>TIP</em>, <em>34</em>, 4026-4039. (<a href='https://doi.org/10.1109/TIP.2025.3579991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stationary functional brain networks (sFBNs) and dynamic functional brain networks (dFBNs) derived from resting-state functional MRI characterize the complex interactions of the human brain from different aspects and could offer complementary information for brain disease analysis. Most current studies focus on sFBN or dFBN analysis, thus limiting the performance of brain network analysis. A few works have explored integrating sFBN and dFBN to identify brain diseases, and achieved better performance than conventional methods. However, these studies still ignore some valuable discriminative information, such as the distribution information of subjects between and within categories. This paper presents a Double Collaborative Learning Network (DCLNet), which takes advantage of both collaborative encoder and collaborative contrastive learning, to learn complementary information of sFBN and dFBN and distribution information of subjects between inter- and intra-categories for brain disease classification. Specifically, we first construct sFBN and dFBN using traditional correlation-based methods with rs-fMRI data, respectively. Then, we build a collaborative encoder to extract brain network features at different levels (i.e., connectivity-based, brain-region-based, and brain-network-based features), and design a prune-graft transformer module to embed the complementary information of the features at each level between two kinds of FBNs. We also develop a collaborative contrastive learning module to capture the distribution information of subjects between and within different categories, thereby learning the more discriminative features of brain networks. We evaluate the DCLNet on two real brain disease datasets with rs-fMRI data, with experimental results demonstrating the superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Jie Zhou and Biao Jie and Zhengdong Wang and Zhixiang Zhang and Weixin Bian and Yang Yang and Hongwei Li and Fengyun Sun and Mingxia Liu},
  doi          = {10.1109/TIP.2025.3579991},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4026-4039},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DCLNet: Double collaborative learning network on stationary-dynamic functional brain network for brain disease classification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstruction reordered intra block copy for screen content coding. <em>TIP</em>, <em>34</em>, 4011-4025. (<a href='https://doi.org/10.1109/TIP.2025.3579988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pursuit of achieving further coding gains beyond the versatile video coding (VVC) standard, the enhanced compression model (ECM) has been initiated by the Joint Video Exploration Team (JVET) with the aim of developing next generation video coding techniques. In ECM, novel coding tools are studied to improve the coding efficiency for both camera-captured content and screen content. Intra block copy (IBC) has been included as a fundamental coding tool in both VVC and ECM, yielding significant improvement in compression efficiency for screen content. This paper presents a method of reconstruction reordered IBC (RR-IBC) to further improve the compression efficiency for screen content, by taking advantage of the symmetry property inherent in screen content sequences. The reconstruction block is flipped horizontally or vertically to restore the characteristics of samples in the original block. A flip-aware adjustment is performed to regulate block vector candidates of the RR-IBC block according to the types of symmetry. Similarly, the reference template of the template-based reordering method for the RR-IBC block is adjusted accordingly to accommodate the geometry property. A motion constraint is applied to restrict the block vector of an RR-IBC coded block to a single direction displacement perpendicular to the flip axis. An RR-IBC flip mode index is signalled to specify how to flip the reconstruction block. Experimental results show that the proposed RR-IBC can provide an average Bjontegaard delta rate (BD-rate) saving of 1.61%/1.79%/1.76% and 3.90%/3.63%/3.63% on Y/Cb/Cr components for class F and class TGM sequences, respectively, with a negligible change on the runtime, compared with ECM-5.0 in all intra configurations. RR-IBC has been adopted into ECM.},
  archive      = {J_TIP},
  author       = {Zhipin Deng and Kai Zhang and Li Zhang},
  doi          = {10.1109/TIP.2025.3579988},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4011-4025},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reconstruction reordered intra block copy for screen content coding},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dynamic prompts for all-in-one image restoration. <em>TIP</em>, <em>34</em>, 3997-4010. (<a href='https://doi.org/10.1109/TIP.2025.3567205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All-in-one image restoration, which seeks to handle multiple types of degradation within a unified model, has become a prominent research topic in computer vision. While existing deep learning models have achieved remarkable success in specific restoration tasks, extending these models to heterogenous degradations presents significant challenges. Current all-in-one methods predominantly concentrate on extracting degradation priors, often employing learned and fixed task prompts to guide the restoration process. However, these static prompts are inclined to generate an average distribution characteristics of degradations, unable to accurately depict the unique attribute of the given input, consequently providing suboptimal restoration results. To tackle these challenges, we propose a novel dynamic prompt approach called Degradation Prototype Assignment and Prompt Distribution Learning (DPPD). Our approach decouples the degradation prior extraction into two novel components: Degradation Prototype Assignment (DPA) and Prompt Distribution Learning (PDL). DPA anchors the degradation representations to predefined prototypes, providing discriminative and scalable representations. In addition, PDL models prompts as distributions rather than fixed parameters, facilitating dynamic and adaptive prompt sampling. Extensive experiments demonstrate that our DPPD framework can achieve significant performance improvement on different image restoration tasks. Codes are available at our project page https://github.com/Aitical/DPPD},
  archive      = {J_TIP},
  author       = {Gang Wu and Junjun Jiang and Kui Jiang and Xianming Liu and Liqiang Nie},
  doi          = {10.1109/TIP.2025.3567205},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3997-4010},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dynamic prompts for all-in-one image restoration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial frequency modulation network for efficient image dehazing. <em>TIP</em>, <em>34</em>, 3982-3996. (<a href='https://doi.org/10.1109/TIP.2025.3579148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, two main research lines in efficient context modeling for image dehazing are tailoring effective feature modulation mechanisms and utilizing the Fourier transform more precisely. The former is usually based on self-scale features that ignore complementary cross-scale/level features, and the latter tends to overlook regions with pronounced haze degradation and intricate structures. This paper introduces a novel spatial and frequency modulation perspective to synergistically investigate contextual feature modeling for efficient image dehazing. Specifically, we delicately develop a Spatial Frequency Modulator (SFM) equipped with a Cross-Scale Modulator (CSM) and Frequency Modulator (FM) to implement intra-block feature modulation. The CSM progressively aggregates hierarchical features across different scales, employing them for spatial self-modulation, and the FM subsequently adopts a dual-branch design to focus more on the crucial areas with severe haze and complex structures for reconstruction. Further, we propose a Cross-Level Modulator (CLM) to facilitate inter-block feature mutual modulation, enhancing seamless interaction between features at different depths and layers. Integrating the above-developed modules into the U-Net architecture, we construct a two-stage spatial frequency modulation network (SFMN). Extensive quantitative and qualitative evaluations showcase the superior performance and efficiency of the proposed SFMN over recent state-of-the-art image dehazing methods. The source code can be found in https://github.com/it-hao/SFMN.},
  archive      = {J_TIP},
  author       = {Hao Shen and Henghui Ding and Yulun Zhang and Zhong-Qiu Zhao and Xudong Jiang},
  doi          = {10.1109/TIP.2025.3579148},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3982-3996},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial frequency modulation network for efficient image dehazing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video coding with cross-component sample offset. <em>TIP</em>, <em>34</em>, 3971-3981. (<a href='https://doi.org/10.1109/TIP.2025.3579992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beyond the exploration of traditional spatial, temporal and subjective visual signal redundancy in image and video compression, recent research has focused on leveraging cross-color component redundancy to enhance coding efficiency. Cross-component coding approaches are motivated by the statistical correlations among different color components, such as those in the Y’CbCr color space, where luma (Y) color component typically exhibits finer details than chroma (Cb/Cr) color components. Inspired by previous cross-component coding algorithms, this paper introduces a novel in-loop filtering approach named Cross-Component Sample Offset (CCSO). CCSO utilizes co-located and neighboring luma samples to generate correction signals for both luma and chroma reconstructed samples. It is a multiplication-free, non-linear mapping process implemented using a look-up-table. The input to the mapping is a group of reconstructed luma samples, and the output is an offset value applied on the center luma or co-located chroma sample. Experimental results demonstrate that the proposed CCSO can be applied to both image and video coding, resulting in improved coding efficiency and visual quality. The method has been adopted into an experimental next-generation video codec beyond AV1 developed by the Alliance for Open Media (AOMedia), demonstrating average -0.81% and -0.69% coding gain on PSNR and VMAF quality metric, respectively, under random access configuration. Additionally, CCSO notably improves the subjective visual quality.},
  archive      = {J_TIP},
  author       = {Han Gao and Xin Zhao and Tianqi Liu and Shan Liu},
  doi          = {10.1109/TIP.2025.3579992},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3971-3981},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video coding with cross-component sample offset},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable coding for high-resolution, high-compression ratio snapshot compressive video. <em>TIP</em>, <em>34</em>, 3960-3970. (<a href='https://doi.org/10.1109/TIP.2025.3579208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-speed cameras are crucial for capturing fast events beyond human perception, although challenges in terms of storage, bandwidth, and cost hinder their widespread use. As an alternative, snapshot compressive video can overcome these challenges by exploiting the principles of compressed sensing to capture compressive projections of dynamic scenes into a single image, which is then used to recover the underlying video by solving an ill-posed inverse problem. However, scalability in terms of spatial and temporal resolution is limited for both acquisition and reconstruction. In this work, we leverage time-division multiplexing to design a versatile scalable coded aperture approach that allows unseen spatio-temporal scalability for snapshot compressive video, offering on-the-fly, high-compression ratios with minimal computational burden and low memory requirements. The proposed sampling scheme is universal and compatible with any compressive temporal imaging sampling matrices and reconstruction algorithm aimed for low spatio-temporal resolutions. Simulations validated with a series of experimental results confirm that we can compress up to 512 frames of 2K $\times 2$ K resolution into a single snapshot, equivalent to a compression ratio of 0.2%, delivering an overall reconstruction quality exceeding 30 dB in PSNR for conventional reconstruction algorithms, and often surpassing 36 dB when utilizing the latest state-of-the-art deep learning reconstruction algorithms. The results presented in this paper can be reproduced in the following GitHub repository: https://github.com/FOGuzman/All-scalable-CACTI},
  archive      = {J_TIP},
  author       = {Felipe Guzmán and Nelson Díaz and Bastián Romero and Esteban Vera},
  doi          = {10.1109/TIP.2025.3579208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3960-3970},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scalable coding for high-resolution, high-compression ratio snapshot compressive video},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From haziness to clarity: A novel iterative memory-retrospective emergence model for omnidirectional image saliency prediction. <em>TIP</em>, <em>34</em>, 3944-3959. (<a href='https://doi.org/10.1109/TIP.2025.3578264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve saliency prediction in omnidirectional images (ODIs), the majority of prior works typically adopt the convolutional neural networks (CNNs)-based saliency models to extract semantic features to predict prominent regions in ODIs. Albeit achieving substantially performance gains, these works all employed purely visual computing paradigms and ignore to explore the nature of human visual attention mechanisms. In other words, existing saliency prediction works for ODIs are insufficient to capture the biological characteristics of the visual attention mechanism in the human brain. To establish a more explicit link between saliency prediction performance and brain-like visual attention mechanism, we simulate the mechanism of human retrospective memory in neuropsychology and propose IMRE model, a novel iterative memory-retrospective emergence model can predict and infer the salient features by recalling previously learned information. In IMRE model, we introduce four key modules to simulate the visual attention mechanism for predicting human fixations in the human brain. Firstly, the visual stimulus response module is designed to effectively extract semantic features and capture the intricate relationship between these features, acting as the human visual cortex. Secondly, the retrospective integration module serves to distill valuable information from a fuzzy memory ensemble, resembling the role of the basal ganglia in the neural system. Thirdly, the memory bank module explicitly records and stores subconscious response information and learned knowledge, acting like the hippocampus in neural system. Lastly, the prospective inference module accurately infers saliency maps from the refined useful information, resembling the role of the prefrontal cortex. During prediction, we utilize the introduced memory bank to retrieve and recall previously learned information, which simulates the process of memory emergence from haziness to clarity. Such a process aligns with the retrospective memory mechanism of the human brain. To validate the superiority of the proposed model in ODIs saliency prediction tasks, we conduct extensive experiments on two benchmark datasets. Experiments show impressive performances that IMRE model outperforms other state-of-the-art methods across all benchmark datasets. Importantly, experiments also highlight the IMRE model’s ability to trace back to specific instances during prediction, thereby reducing model inference costs and enhancing interpretability.},
  archive      = {J_TIP},
  author       = {Dandan Zhu and Kaiwei Zhang and Xiongkuo Min and Guangtao Zhai and Xiaokang Yang},
  doi          = {10.1109/TIP.2025.3578264},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3944-3959},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {From haziness to clarity: A novel iterative memory-retrospective emergence model for omnidirectional image saliency prediction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast 3D room layout estimation based on compact high-level representation. <em>TIP</em>, <em>34</em>, 3930-3943. (<a href='https://doi.org/10.1109/TIP.2025.3578785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D room layout estimation aims to reconstruct the holistic 3D structure from an indoor RGB image. For most of the deep learning-based methods, layout inference is guided by a kind of learned 2D mid-level representation such as pixel-wise surface labels. However, learning such high-resolution 2D representation might suffer from information redundancy and memory consumption, and will increase the runtime of estimation and deployment cost for practical applications. In this paper, we attempt to learn a compact high-level representation with only 29 real numbers for estimating the 3D layout using general regression networks. The learned compact high-level representation contains three components: instance-wise plane parameters, camera intrinsic parameters, and plane location indicators. With the learned representation, the inverse depth map of each plane can be calculated to reconstruct the 3D layout. We further design a set of order-agnostic loss functions to restrict the produced inverse depth maps, with which the model can be trained with either weak 2D layout labels or full 3D layout supervision. Moreover, by jointly learning the plane parameters and locations, the model is benefited from 3D reasoning. Experimental results show that our method is much faster than the existing layout estimation methods and obtains competitive performance on benchmark datasets, showing its potential for real-time applications.},
  archive      = {J_TIP},
  author       = {Weidong Zhang and Yulei Qiao and Ying Liu and Ran Song and Wei Zhang},
  doi          = {10.1109/TIP.2025.3578785},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3930-3943},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast 3D room layout estimation based on compact high-level representation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOD-PCAC: Level-of-detail-based deep lossless point cloud attribute compression. <em>TIP</em>, <em>34</em>, 3918-3929. (<a href='https://doi.org/10.1109/TIP.2025.3578760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud attribute compression is a challenging issue in efficiently compressing large volumes of attributes. Despite notable advancements in lossy point cloud compression using deep learning, progress in lossless compression remains limited. Some methods have employed octree- or voxel-based partitioning techniques derived from geometric compression, achieving success on dense point clouds. However, these voxel-based approaches struggle with sparse or unevenly distributed point clouds, leading to performance degradation. In this work, we introduce a novel framework for learning-based lossless point cloud attribute compression, named LOD-PCAC, which leverages a Level-of-Detail (LOD) structure to ensure density-robust compression. Specifically, the input point cloud is divided into multiple detail levels, and vertices from these levels are selected to construct a Reference Set as context, which effectively captures multi-level information. Then we propose the Bit-level Residual Coder for efficient attribute compression. Instead of directly compressing attributes, our method first predicts attribute values and organizes the residual bits into a Bit Matrix as another context, simplifying predictions and fully exploiting channel correlations. Finally, a neural network with specialized encoders processes the context to estimate the probability of each residual bit. Experimental results demonstrate that the proposed method outperforms both traditional and learning-based approaches across various point clouds, exhibiting strong generalization across datasets and robustness to varying densities.},
  archive      = {J_TIP},
  author       = {Wenbo Zhao and Wei Gao and Dingquan Li and Jing Wang and Guoqing Liu},
  doi          = {10.1109/TIP.2025.3578760},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3918-3929},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LOD-PCAC: Level-of-detail-based deep lossless point cloud attribute compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pushing the boundaries of salient object detection: A denoising-driven approach. <em>TIP</em>, <em>34</em>, 3903-3917. (<a href='https://doi.org/10.1109/TIP.2025.3576993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient Object Detection (SOD) aims to identify the most attention-grabbing regions in an image and focuses on distinguishing salient objects from their backgrounds. Current SOD methods primarily use a discriminative approach, which works well for clear images but struggles in complex scenes with similar colors and textures between objects and backgrounds. To address these limitations, we introduce the diffusion-based salient object detection model (DiffSOD), which leverages a noise-to-image denoising process within a diffusion framework, enhancing saliency detection in both RGB and RGB-D images. Unlike conventional fusion-based SOD methods that directly merge RGB and depth information, we treat RGB and depth as distinct conditions, i.e., the appearance condition and the structure condition, respectively. These conditions serve as controls within the diffusion UNet architecture, guiding the denoising process. To facilitate this guidance, we employ two specialized control adapters: the appearance control adapter and the structure control adapter. Moreover, conventional denoising UNet models may struggle when handling low-quality depth maps, potentially introducing detrimental cues into the denoising process. To mitigate the impact of low-quality depth maps, we introduce a quality-aware filter. This filter selectively processes only high-quality depth data, ensuring that the denoising process is based on reliable information. Comparative evaluations on benchmark datasets have shown that DiffSOD substantially surpasses existing RGB and RGB-D saliency detection methods, improving average performance by 1.5% and 1.2% respectively, thus setting a new benchmark for diffusion-based dense prediction models in visual saliency detection.},
  archive      = {J_TIP},
  author       = {Mengke Song and Luming Li and Xu Yu and Chenglizhao Chen},
  doi          = {10.1109/TIP.2025.3576993},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3903-3917},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pushing the boundaries of salient object detection: A denoising-driven approach},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoopSparseGS: Loop-based sparse-view friendly gaussian splatting. <em>TIP</em>, <em>34</em>, 3889-3902. (<a href='https://doi.org/10.1109/TIP.2025.3574929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the photorealistic novel view synthesis (NVS) performance achieved by the original 3D Gaussian splatting (3DGS), its rendering quality significantly degrades with sparse input views. This performance drop is mainly caused by the limited number of initial points generated from the sparse input, lacking reliable geometric supervision during the training process, and inadequate regularization of the oversized Gaussian ellipsoids. To handle these issues, we propose the LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis task. In specific, we propose a loop-based Progressive Gaussian Initialization (PGI) strategy that could iteratively densify the initialized point cloud using the rendered pseudo images during the training process. Then, the sparse and reliable depth from the Structure from Motion, and the window-based dense monocular depth are leveraged to provide precise geometric supervision via the proposed Depth-alignment Regularization (DAR). Additionally, we introduce a novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian ellipsoids leading to large pixel errors. Comprehensive experiments on four datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art methods for sparse-input novel view synthesis, across indoor, outdoor, and object-level scenes with various image resolutions. Code is available at: https://github.com/pcl3dv/LoopSparseGS},
  archive      = {J_TIP},
  author       = {Zhenyu Bao and Guibiao Liao and Kaichen Zhou and Kanglin Liu and Qing Li and Guoping Qiu},
  doi          = {10.1109/TIP.2025.3574929},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3889-3902},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LoopSparseGS: Loop-based sparse-view friendly gaussian splatting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RECISTSurv: Hybrid multi-task transformer for hepatocellular carcinoma response and survival evaluation. <em>TIP</em>, <em>34</em>, 3873-3888. (<a href='https://doi.org/10.1109/TIP.2025.3579200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transarterial Chemoembolization (TACE) is a widely applied alternative treatment for patients with hepatocellular carcinoma who are not eligible for liver resection or transplantation. However, the clinical outcomes after TACE are highly heterogeneous. There remains an urgent need for effective and efficient strategies to accurately assess tumor response and predict long-term outcomes using longitudinal and multi-center datasets. To address this challenge, we here introduce RECISTSurv, a novel response-driven Transformer model that integrates multi-task learning with a response-driven co-attention mechanism to simultaneously perform liver and tumor segmentation, predict tumor response to TACE, and estimate overall survival based on longitudinal Computed Tomography (CT) imaging. The proposed Response-driven Co-attention layer models the interactions between pre-TACE and post-TACE features guided by the treatment response embedding. This design enables the model to capture complex relationships between imaging features, treatment response, and survival outcomes, thereby enhancing both prediction accuracy and interpretability. In a multi-center validation study, RECISTSurv-predicted prognosis has demonstrated superior precision than state-of-the-art methods with C-indexes ranging from 0.595 to 0.780. Furthermore, when integrated with multi-modal data, RECISTSurv has emerged as an independent prognostic factor in all three validation cohorts, with hazard ratio (HR) ranging from 1.693 to 20.7 ( $\text {P = 0.001-0.042}$ ). Our results highlight the potential of RECISTSurv as a powerful tool for personalized treatment planning and outcome prediction in hepatocellular carcinoma patients undergoing TACE. The experimental code is made publicly available at https://github.com/rushier/RECISTSurv},
  archive      = {J_TIP},
  author       = {Rushi Jiao and Qiuping Liu and Yao Zhang and Bangzheng Pu and Bingsen Xue and Yi Cheng and Kailan Yang and Xisheng Liu and Jinrong Qu and Cheng Jin and Ya Zhang and Yanfeng Wang and Yu-Dong Zhang},
  doi          = {10.1109/TIP.2025.3579200},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3873-3888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RECISTSurv: Hybrid multi-task transformer for hepatocellular carcinoma response and survival evaluation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural similarity-inspired unfolding for lightweight image super-resolution. <em>TIP</em>, <em>34</em>, 3861-3872. (<a href='https://doi.org/10.1109/TIP.2025.3578753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major efforts in data-driven image super-resolution (SR) primarily focus on expanding the receptive field of the model to better capture contextual information. However, these methods are typically implemented by stacking deeper networks or leveraging transformer-based attention mechanisms, which consequently increases model complexity. In contrast, model-driven methods based on the unfolding paradigm show promise in improving performance while effectively maintaining model compactness through sophisticated module design. Based on these insights, we propose a Structural Similarity-Inspired Unfolding (SSIU) method for efficient image SR. This method is designed through unfolding an SR optimization function constrained by structural similarity, aiming to combine the strengths of both data-driven and model-driven approaches. Our model operates progressively following the unfolding paradigm. Each iteration consists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse Attention Module (ESAM). The former implements comprehensive constraints on features, including a structural similarity constraint, while the latter aims to achieve sparse activation. In addition, we design a Mixture-of-Experts-based Feature Selector (MoE-FS) that fully utilizes multi-level feature information by combining features from different steps. Extensive experiments validate the efficacy and efficiency of our unfolding-inspired network. Our model outperforms current state-of-the-art models, boasting lower parameter counts and reduced memory consumption. Our code will be available at: https://github.com/eezkni/SSIU},
  archive      = {J_TIP},
  author       = {Zhangkai Ni and Yang Zhang and Wenhan Yang and Hanli Wang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TIP.2025.3578753},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3861-3872},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structural similarity-inspired unfolding for lightweight image super-resolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating light field from stereo images for AR display with matched angular sampling structure and minimal retinal error. <em>TIP</em>, <em>34</em>, 3849-3860. (<a href='https://doi.org/10.1109/TIP.2025.3575333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-eye light field displays offer natural 3D visual experiences for AR/VR users by projecting light rays onto retina as if the light rays were emanated from a real object. Such displays normally take four-dimensional light field data as input. Given that sizeable existing 3D contents are in the form of stereo images, we propose a practical approach that generates light field data from such contents at minimal computational cost while maintaining a reasonable image quality. The perceptual quality of light field is ensured by making the baseline of light field subviews consistent with that of the micro-projectors of the light field display and by compensating for the optical artifact of the light field display through digital rectification. The effectiveness and efficiency of the proposed approach is verified through both quantitative and qualitative experiments. The results demonstrate that our light field converter works for real-world light field displays.},
  archive      = {J_TIP},
  author       = {Yi-Chou Chen and Homer H. Chen},
  doi          = {10.1109/TIP.2025.3575333},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3849-3860},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generating light field from stereo images for AR display with matched angular sampling structure and minimal retinal error},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spherical patch generative adversarial net for unconditional panoramic image generation. <em>TIP</em>, <em>34</em>, 3833-3848. (<a href='https://doi.org/10.1109/TIP.2025.3578257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in virtual reality (VR) and augmented reality (AR) have popularised the emerging panoramic content for the immersive visual experience. The difficulty in acquisition and display of 360° format further highlights the necessity of unconditional panoramic image generation. Existing methods essentially generate planar images mapped from panoramic images, and fail to address the deformation and closed-loop characteristics when inverted back to the panoramic images. Thus leading to the generation of pseudo-panoramic content. This paper aims to directly generate spherical content, in a patch-by-patch style; besides computation friendly, this promises the anywhere continuity on the panoramic image and proper accommodation of panoramic deformation. More specifically, we first propose a novel spherical patch convolution (SPConv) that operates on the local spherical patch, which naturally addresses the deformation of panoramic content. We then propose our spherical patch generative adversarial net (SP-GAN) that consists of spherical local embedding (SLE) and spherical content synthesiser (SCS) modules, which seamlessly incorporate our SPConv so as to generate continuous panoramic patches. To the best of our knowledge, the proposed SP-GAN is the first successful attempt to accommodate the spherical distortion for closed-loop panoramic image generation in a patch-by-patch manner. The experimental results, with human-rated evaluations, have verified the consistently superior performances for unconditional panoramic image generation, from the perspectives of generation quality, computational memory, and generalisation to various resolutions. Codes are publicly available at https://github.com/chronos123/SP-GAN},
  archive      = {J_TIP},
  author       = {Mai Xu and Xiancheng Sun and Shengxi Li and Lai Jiang and Jingyuan Xia and Xin Deng},
  doi          = {10.1109/TIP.2025.3578257},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3833-3848},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spherical patch generative adversarial net for unconditional panoramic image generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCM-PDA: A generative compensation model for progressive difference attenuation in spatiotemporal fusion of remote sensing images. <em>TIP</em>, <em>34</em>, 3817-3832. (<a href='https://doi.org/10.1109/TIP.2025.3576992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution satellite imagery with dense temporal series is crucial for long-term surface change monitoring. Spatiotemporal fusion seeks to reconstruct remote sensing image sequences with both high spatial and temporal resolutions by leveraging prior information from multiple satellite platforms. However, significant radiometric discrepancies and large spatial resolution variations between images acquired from different satellite sensors, coupled with the limited availability of prior data, present major challenges to accurately reconstructing missing data using existing methods. To address these challenges, this paper introduces GCM-PDA, a novel generative compensation model with progressive difference attenuation for spatiotemporal fusion of remote sensing images. The proposed model integrates multi-scale image decomposition within a progressive fusion framework, enabling the efficient extraction and integration of information across scales. Additionally, GCM-PDA employs domain adaptation techniques to mitigate radiometric inconsistencies between heterogeneous images. Notably, this study pioneers the use of style transformation in spatiotemporal fusion to achieve spatial-spectral compensation, effectively overcoming the constraints of limited prior image information. Experimental results demonstrate that GCM-PDA not only achieves competitive fusion performance but also exhibits strong robustness across diverse conditions.},
  archive      = {J_TIP},
  author       = {Kai Ren and Weiwei Sun and Xiangchao Meng and Gang Yang},
  doi          = {10.1109/TIP.2025.3576992},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3817-3832},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GCM-PDA: A generative compensation model for progressive difference attenuation in spatiotemporal fusion of remote sensing images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multi-view contrastive clustering via graph structure awareness. <em>TIP</em>, <em>34</em>, 3805-3816. (<a href='https://doi.org/10.1109/TIP.2025.3573501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) aims to exploit the latent relationships between heterogeneous samples in an unsupervised manner, which has served as a fundamental task in the unsupervised learning community and has drawn widespread attention. In this work, we propose a new deep multi-view contrastive clustering method via graph structure awareness (DMvCGSA) by conducting both instance-level and cluster-level contrastive learning to exploit the collaborative representations of multi-view samples. Unlike most existing deep multi-view clustering methods, which usually extract only the attribute features for multi-view representation, we first exploit the view-specific features while preserving the latent structural information between multi-view data via a GCN-embedded autoencoder, and further develop a similarity-guided instance-level contrastive learning scheme to make the view-specific features discriminative. Moreover, unlike existing methods that separately explore common information, which may not contribute to the clustering task, we employ cluster-level contrastive learning to explore the clustering-beneficial consistency information directly, resulting in improved and reliable performance for the final multi-view clustering task. Extensive experimental results on twelve benchmark datasets clearly demonstrate the encouraging effectiveness of the proposed method compared with the state-of-the-art models.},
  archive      = {J_TIP},
  author       = {Lunke Fei and Junlin He and Qi Zhu and Shuping Zhao and Jie Wen and Yong Xu},
  doi          = {10.1109/TIP.2025.3573501},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3805-3816},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep multi-view contrastive clustering via graph structure awareness},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer for multitemporal hyperspectral image unmixing. <em>TIP</em>, <em>34</em>, 3790-3804. (<a href='https://doi.org/10.1109/TIP.2025.3577394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitemporal hyperspectral image unmixing (MTHU) holds significant importance in monitoring and analyzing the dynamic changes of surface. However, compared to single-temporal unmixing, the multitemporal approach demands comprehensive consideration of information across different phases, rendering it a greater challenge. To address this challenge, we propose the Multitemporal Hyperspectral Image Unmixing Transformer (MUFormer), an end-to-end unsupervised deep learning model. To effectively perform multitemporal hyperspectral image unmixing, we introduce two key modules: the Global Awareness Module (GAM) and the Change Enhancement Module (CEM). The GAM computes self-attention across all phases, facilitating global weight allocation. On the other hand, the CEM dynamically learns local temporal changes by capturing differences between adjacent feature maps. The integration of these modules enables the effective capture of multitemporal semantic information related to endmember and abundance changes, significantly improving the performance of multitemporal hyperspectral image unmixing. We conducted experiments on one real dataset and two synthetic datasets, demonstrating that our model significantly enhances the effect of multitemporal hyperspectral image unmixing.},
  archive      = {J_TIP},
  author       = {Hang Li and Qiankun Dong and Xueshuo Xie and Xia Xu and Tao Li and Zhenwei Shi},
  doi          = {10.1109/TIP.2025.3577394},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3790-3804},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Transformer for multitemporal hyperspectral image unmixing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reference-based iterative interaction with p2-matching for stereo image super-resolution. <em>TIP</em>, <em>34</em>, 3779-3789. (<a href='https://doi.org/10.1109/TIP.2025.3577538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo Image Super-Resolution (SSR) holds great promise in improving the quality of stereo images by exploiting the complementary information between left and right views. Most SSR methods primarily focus on the inter-view correspondences in low-resolution (LR) space. The potential of referencing a high-quality SR image of one view benefits the SR for the other is often overlooked, while those with abundant textures contribute to accurate correspondences. Therefore, we propose Reference-based Iterative Interaction (RIISSR), which utilizes reference-based iterative pixel-wise and patch-wise matching, dubbed $P^{2}$ -Matching, to establish cross-view and cross-resolution correspondences for SSR. Specifically, we first design the information perception block (IPB) cascaded in parallel to extract hierarchical contextualized features for different views. Pixel-wise matching is embedded between two parallel IPBs to exploit cross-view interaction in LR space. Iterative patch-wise matching is then executed by utilizing the SR stereo pair as another mutual reference, capitalizing on the cross-scale patch recurrence property to learn high-resolution (HR) correspondences for SSR performance. Moreover, we introduce the supervised side-out modulator (SSOM) to re-weight local intra-view features and produce intermediate SR images, which seamlessly bridge two matching mechanisms. Experimental results demonstrate the superiority of RIISSR against existing state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Runmin Cong and Rongxin Liao and Feng Li and Ronghui Sheng and Huihui Bai and Renjie Wan and Sam Kwong and Wei Zhang},
  doi          = {10.1109/TIP.2025.3577538},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3779-3789},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reference-based iterative interaction with p2-matching for stereo image super-resolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing generalizable remote physiological measurement through the integration of explicit and implicit prior knowledge. <em>TIP</em>, <em>34</em>, 3764-3778. (<a href='https://doi.org/10.1109/TIP.2025.3576490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) is a promising technology for capturing physiological signals from facial videos, with potential applications in medical health, affective computing, and biometric recognition. The demand for rPPG tasks has evolved from achieving high performance in intra-dataset testing to excelling in cross-dataset testing (i.e., domain generalization). However, most existing methods have overlooked the incorporation of prior knowledge specific to rPPG, leading to limited generalization capabilities. In this paper, we propose a novel framework that effectively integrates both explicit and implicit prior knowledge into the rPPG task. Specifically, we conduct a systematic analysis of noise sources (e.g., variations in cameras, lighting conditions, skin types, and motion) across different domains and embed this prior knowledge into the network design. Furthermore, we employ a two-branch network to disentangle physiological feature distributions from noise through implicit label correlation. Extensive experiments demonstrate that the proposed method not only surpasses state-of-the-art approaches in RGB cross-dataset evaluation but also exhibits strong generalization from RGB datasets to NIR datasets. The code is publicly available at https://github.com/keke-nice/Greip},
  archive      = {J_TIP},
  author       = {Yuting Zhang and Hao Lu and Xin Liu and Yingcong Chen and Kaishun Wu},
  doi          = {10.1109/TIP.2025.3576490},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3764-3778},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing generalizable remote physiological measurement through the integration of explicit and implicit prior knowledge},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S4R: Separated self-supervised spectral regression for hyperspectral histopathology image diagnosis. <em>TIP</em>, <em>34</em>, 3748-3763. (<a href='https://doi.org/10.1109/TIP.2025.3575183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) offer great potential for computational pathology. But, limited by the lack of adequate annotated data and the high spectral redundancy of HSIs, traditional supervised learning techniques are usually bottlenecked. To exploit the structural properties of HSIs and learn representations with good transferability, we propose Separated Self-Supervised Spectral Regression (S4R). Concretely, we find one spectral band can be represented by a linear combination of the remaining bands. Regressing the distribution of the linear coefficients learns the inherent properties of HSIs and pathological information about the tissue. Besides, reconstructing the missing band, especially the tissue boundaries makes the model learn pathology details that are critical to downstream tasks. Coupling these two pretext tasks makes the self-supervised model understand spectral structures of HSIs w.r.t. pathological semantics and spatial micro details. Furthermore, we design two brand-new architectures to avoid the interference of extraneous signal based on S4R: S4R-CLS and S4R-SEG for HSI classification and segmentation, respectively. Two downstream tasks are incorporated into a unified framework, which first encodes different bands from HSIs via a depthwise separable encoder, and then selectively aggregates band features to generate final predictions. In S4R-SEG, we propose to pick the best matching bands with the guidance of a classification paradigm. Extensive experiments show S4R performs much better than competitors on both tasks. Theoretical analysis and clinical discussion also indicate the great potential for further medical applications. The code and pre-trained checkpoints are available at https://github.com/DeepMed-Lab-ECNU/S4R},
  archive      = {J_TIP},
  author       = {Yan Wang and Xingran Xie and Lili Gao and Benyan Zhang and Chunhua Zhou and Duowu Zou and Le Lu and Qingli Li},
  doi          = {10.1109/TIP.2025.3575183},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3748-3763},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {S4R: Separated self-supervised spectral regression for hyperspectral histopathology image diagnosis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ‘Disengage AND integrate’: Personalized causal network for gaze estimation. <em>TIP</em>, <em>34</em>, 3733-3747. (<a href='https://doi.org/10.1109/TIP.2025.3575238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze estimation task aims to predict a 3D gaze direction or a 2D gaze point given a face or eye image. To improve generalization of gaze estimation models to unseen new users, existing methods either disentangle personalized information of all subjects from their gaze features, or integrate unrefined personalized information into blended embeddings. Their methodologies are not rigorous whose performance is still unsatisfactory. In this paper, we put forward a comprehensive perspective named ‘Disengage AND Integrate’ to deal with personalized information, which elaborates that for specified users, their irrelevant personalized information should be discarded while relevant one should be considered. Accordingly, a novel Personalized Causal Network (PCNet) for generalizable gaze estimation has been proposed. The PCNet adopts a two-branch framework, which consists of a subject-deconfounded appearance sub-network (SdeANet) and a prototypical personalization sub-network (ProPNet). The SdeANet aims to explore causalities among facial images, gazes, and personalized information and extract a subject-invariant appearance-aware feature of each image by means of causal intervention. The ProPNet aims to characterize customized personalization-aware features of arbitrary users with the help of a prototype-based subject identification task. Furthermore, our whole PCNet is optimized in a hybrid episodic training paradigm, which further improve its adaptability to new users. Experiments on three challenging datasets over within-domain and cross-domain gaze estimation tasks demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Yi Tian and Xiyun Wang and Sihui Zhang and Wanru Xu and Yi Jin and Yaping Huang},
  doi          = {10.1109/TIP.2025.3575238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3733-3747},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {‘Disengage AND integrate’: Personalized causal network for gaze estimation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHI: Bridging domain shift in long-term action quality assessment via progressive hierarchical instruction. <em>TIP</em>, <em>34</em>, 3718-3732. (<a href='https://doi.org/10.1109/TIP.2025.3574938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative performance of actions in long videos. However, existing methods face challenges due to domain shifts between the pre-trained large-scale action recognition backbones and the specific AQA task, thereby hindering their performance. This arises since fine-tuning resource-intensive backbones on small AQA datasets is impractical. We address this by identifying two levels of domain shift: task-level, regarding differences in task objectives, and feature-level, regarding differences in important features. For feature-level shifts, which are more detrimental, we propose Progressive Hierarchical Instruction (PHI) with two strategies. First, Gap Minimization Flow (GMF) leverages flow matching to progressively learn a fast flow path that reduces the domain gap between initial and desired features across shallow to deep layers. Additionally, a temporally-enhanced attention module captures long-range dependencies essential for AQA. Second, List-wise Contrastive Regularization (LCR) facilitates coarse-to-fine alignment by comprehensively comparing batch pairs to learn fine-grained cues while mitigating domain shift. Integrating these modules, PHI offers an effective solution. Experiments demonstrate that PHI achieves state-of-the-art performance on three representative long-term AQA datasets, proving its superiority in addressing the domain shift for long-term AQA.},
  archive      = {J_TIP},
  author       = {Kanglei Zhou and Hubert P. H. Shum and Frederick W. B. Li and Xingxing Zhang and Xiaohui Liang},
  doi          = {10.1109/TIP.2025.3574938},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3718-3732},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PHI: Bridging domain shift in long-term action quality assessment via progressive hierarchical instruction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous experts and hierarchical perception for underwater salient object detection. <em>TIP</em>, <em>34</em>, 3703-3717. (<a href='https://doi.org/10.1109/TIP.2025.3572760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing underwater salient object detection (USOD) methods design fusion strategies to integrate multimodal information, but lack exploration of modal characteristics. To address this, we separately leverage the RGB and depth branches to learn disentangled representations, formulating the heterogeneous experts and hierarchical perception network (HEHP). Specifically, to reduce modal discrepancies, we propose the hierarchical prototype guided interaction (HPI), which achieves fine-grained alignment guided by the semantic prototypes, and then refines with complementary modalities. We further design the mixture of frequency experts (MoFE), where experts focus on modeling high- and low-frequency respectively, collaborating to explicitly obtain hierarchical representations. To efficiently integrate diverse spatial and frequency information, we formulate the four-way fusion experts (FFE), which dynamically selects optimal experts for fusion while being sensitive to scale and orientation. Since depth maps with poor quality inevitably introduce noises, we design the uncertainty injection (UI) to explore high uncertainty regions by establishing pixel-level probability distributions. We further formulate the holistic prototype contrastive (HPC) loss based on semantics and patches to learn compact and general representations across modalities and images. Finally, we employ varying supervision based on branch distinctions to implicitly construct difference modeling. Extensive experiments on two USOD datasets and four relevant underwater scene benchmarks validate the effect of the proposed method, surpassing state-of-the-art binary detection models. Impressive results on seven natural scene benchmarks further demonstrate the scalability.},
  archive      = {J_TIP},
  author       = {Mingfeng Zha and Guoqing Wang and Yunqiang Pei and Tianyu Li and Xiongxin Tang and Chongyi Li and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TIP.2025.3572760},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3703-3717},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Heterogeneous experts and hierarchical perception for underwater salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing continual semantic segmentation via uncertainty and class balance re-weighting. <em>TIP</em>, <em>34</em>, 3689-3702. (<a href='https://doi.org/10.1109/TIP.2025.3576477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Semantic Segmentation (CSS) primarily aims to continually learn new semantic segmentation categories while avoiding catastrophic forgetting. In semantic segmentation tasks, images can comprise both familiar old categories and novel unseen categories and they are treated as background in the incremental stage. Therefore, it is necessary to utilize the old model to generate pseudo-labels. However, the quality of these pseudo-labels significantly influences the model’s forgetting of the old categories. Erroneous pseudo-labels can introduce harmful gradients, thus exacerbating model forgetting. In addition, the issue of class imbalance poses a significant challenge within the realm of CSS. Although traditional methods frequently diminish the emphasis placed on new classes to address this imbalance, we discover that the imbalance extends beyond the distinction between old and new classes. In this paper, we specifically address two previously overlooked problems in CSS: the impact of erroneous pseudo-labels on model forgetting and the confusion induced by class imbalance. We propose an Uncertainty and Class Balance Re-weighting approach (UCB) that assigns higher weights to pixels with pseudo-labels exhibiting lower uncertainty and to categories with smaller proportions during the training process. Our proposed approach enhances the impact of essential pixels during the continual learning process, thereby reducing model forgetting and dynamically balancing category weights based on the dataset. Our method is simple yet effective and can be applied to any method that uses pseudo-labels. Extensive experiments on the Pascal-VOC and ADE20K datasets demonstrate the efficacy of our approach in improving model performance across three state-of-the-art methods. The code will be available at https://github.com/JACK-Chen-2019/UCB},
  archive      = {J_TIP},
  author       = {Zichen Liang and Yusong Hu and Fei Yang and Xialei Liu},
  doi          = {10.1109/TIP.2025.3576477},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3689-3702},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing continual semantic segmentation via uncertainty and class balance re-weighting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class incremental learning via contrastive complementary augmentation. <em>TIP</em>, <em>34</em>, 3663-3673. (<a href='https://doi.org/10.1109/TIP.2025.3574930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental learning (CIL) endeavors to acquire new knowledge continuously from an unending data stream while retaining previously acquired knowledge. Since the amount of new data is significantly smaller than that of old data, existing methods struggle to strike a balance between acquiring new knowledge and retaining previously learned knowledge, leading to substantial performance degradation. To tackle such a dilemma, in this paper, we propose the Contrastive Complementary Augmentation Learning (CoLA) method, which mitigates the aliasing of distributions in incremental tasks. Specifically, we introduce a novel yet effective supervised contrastive learning module with instance- and class-level augmentation during base training. For the instance-level augmentation method, we spatially segment the image at different scales, creating spatial pyramid contrastive pairs to obtain more robust feature representations. Meanwhile, the class-level augmentation method randomly mixes images within the mini-batch, facilitating the learning of compact and more easily adaptable decision boundaries. In this way, we only need to train the classifier to maintain competitive performance during the incremental phases. Furthermore, we also propose CoLA+ to further enhance the proposed method with relaxed limitations on data storage. Extensive experiments demonstrate that our method achieves state-of-the-art performance on different benchmarks.},
  archive      = {J_TIP},
  author       = {Xi Wang and Xu Yang and Kun Wei and Yanan Gu and Cheng Deng},
  doi          = {10.1109/TIP.2025.3574930},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3663-3673},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Class incremental learning via contrastive complementary augmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity distribution alignment for cross-domain crowd counting. <em>TIP</em>, <em>34</em>, 3648-3662. (<a href='https://doi.org/10.1109/TIP.2025.3571312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation enables the transfer of knowledge from a labeled source domain to an unlabeled target domain, and its application in crowd counting is gaining momentum. Current methods typically align distributions across domains to address inter-domain disparities at a global level. However, these methods often struggle with significant intra-domain gaps caused by domain-agnostic factors such as density, surveillance angles, and scale, leading to inaccurate alignment and unnecessary computational burdens, especially in large-scale training scenarios. To address these challenges, we propose the Multi-Granularity Optimal Transport (MGOT) distribution alignment framework, which aligns domain-agnostic factors across domains at different granularities. The motivation behind multi-granularity is to capture fine-grained domain-agnostic variations within domains. Our method proceeds in three phases: first, clustering coarse-grained features based on intra-domain similarity; second, aligning the granular clusters using an optimal transport framework and constructing a mapping from cluster centers to finer patch levels between domains; and third, re-weighting the aligned distribution for model refinement in domain adaptation. Extensive experiments across twelve cross-domain benchmarks show that our method outperforms existing state-of-the-art methods in adaptive crowd counting. The code will be available at https://github.com/HopooLinZ/MGOT},
  archive      = {J_TIP},
  author       = {Xian Zhong and Lingyue Qiu and Huilin Zhu and Jingling Yuan and Shengfeng He and Zheng Wang},
  doi          = {10.1109/TIP.2025.3571312},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3648-3662},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-granularity distribution alignment for cross-domain crowd counting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic spherical gaussians lighting priors for indoor environment map estimation. <em>TIP</em>, <em>34</em>, 3635-3647. (<a href='https://doi.org/10.1109/TIP.2025.3575902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) environment lighting is essential for augmented reality and visual editing applications, enabling realistic object relighting and seamless scene composition. However, the acquisition of accurate HDR environment maps remains resource-intensive, often requiring specialized devices such as light probes or 360° capture systems, and necessitating stitching during postprocessing. Existing deep learning-based methods attempt to estimate global illumination from partial-view images but often struggle with complex lighting conditions, particularly in indoor environments with diverse lighting variations. To address this challenge, we propose a novel method for estimating indoor HDR environment maps from single standard images, leveraging Anisotropic Spherical Gaussians (ASG) to model intricate lighting distributions as priors. Unlike traditional Spherical Gaussian (SG) representations, ASG can better capture anisotropic lighting properties, including complex shape, rotation, and spatial extent. Our approach introduces a transformer-based network with a two-stage training scheme to predict ASG parameters effectively. To leverage these predicted lighting priors for environment map generation, we introduce a novel generative projector that synthesizes environment maps with high-frequency textures. To train the generative projector, we propose a parameter-efficient adaptation method that transfers knowledge from SG-based guidance to ASG, enabling the model to preserve the generalizability of SG (e.g., spatial distribution and dominance of light sources) while enhancing its capacity to capture fine-grained anisotropic lighting characteristics. Experimental results demonstrate that our method yields environment maps with more precise lighting conditions and environment textures, facilitating the realistic rendering of lighting effects. The implementation code for ASG extraction can be found at https://github.com/junhong-jennifer-zhao/ASG-lighting},
  archive      = {J_TIP},
  author       = {Junhong Zhao and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TIP.2025.3575902},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3635-3647},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Anisotropic spherical gaussians lighting priors for indoor environment map estimation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCPM: Uncertainty-guided cross-modal retrieval with partially mismatched pairs. <em>TIP</em>, <em>34</em>, 3622-3634. (<a href='https://doi.org/10.1109/TIP.2025.3574918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manual annotation of perfectly aligned labels for cross-modal retrieval (CMR) is incredibly labor-intensive. As an alternative, the collection of co-occurring data pairs from the Internet is a remarkably cost-effective way, but which, inevitably induces the Partially Mismatched Pairs (PMPs) and therefore significantly degrades the retrieval performance without particular treatment. Previous efforts often utilize the pair-wise similarity to filter out the mismatched pairs, and such operation is highly sensitive to mismatched or ambiguous data and thus leads to sub-optimal performance. To alleviate these concerns, we propose an efficient approach, termed UCPM, i.e., Uncertainty-guided Cross-modal retrieval with Partially Mismatched pairs, which can significantly reduce the adverse impact of mismatched data pairs. Specifically, a novel Uncertainty Guided Division (UGD) strategy is sophisticatedly designed to divide the corrupted training data into confident matched (clean), easily-identifiable mismatched (noisy) and hardly-determined hard subsets, and the derived uncertainty can simultaneously guide the informative pair learning while reducing the negative impact of potential mismatched pairs. Meanwhile, an effective Uncertainty Self-Correction (USC) mechanism is concurrently presented to accurately identify and rectify the fluctuated uncertainty during the training process, which further improves the stability and reliability of the estimated uncertainty. Besides, a Trusted Margin Loss (TML) is newly designed to enhance the discriminability between those hard pairs, by dynamically adjusting their soft margins to amplify the positive contributions of matched pairs while suppressing the negative impacts of mismatched pairs. Extensive experiments on three widely-used benchmark datasets, verify the effectiveness and reliability of UCPM compared with the existing SOTA approaches, and significantly improve the robustness in both synthetic and real-world PMPs. The code is available at: https://github.com/qxzha/UCPM},
  archive      = {J_TIP},
  author       = {Quanxing Zha and Xin Liu and Yiu-Ming Cheung and Shu-Juan Peng and Xing Xu and Nannan Wang},
  doi          = {10.1109/TIP.2025.3574918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3622-3634},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UCPM: Uncertainty-guided cross-modal retrieval with partially mismatched pairs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional probabilistic multi-graph learning and decomposition for multi-view clustering. <em>TIP</em>, <em>34</em>, 3609-3621. (<a href='https://doi.org/10.1109/TIP.2025.3574924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view clustering has attracted remarkable attention due to its impressive performance. However, the typical framework consisting of graph learning and indicator generation may fail to align learned graphs with the underlying data structure due to the unidirectional pipeline from refined graphs to indicator generation. Another common problem is the inadequate prior information in graph learning methods. This paper proposes a Bidirectional Probabilistic Multi-graph Learning and Decomposition (BPMLD) method by establishing an explicit bidirectional pipeline between graph learning and indicator generation for multi-view clustering. Specifically, we design a confidence term based on clustering probability indicators and fuse it with graph learning to form clustering confidence driven graph learning. Meanwhile, graph tensor learning is introduced to recover the high-order correlations among the refined graphs. We further propose a multi-graph probability decomposition module to adaptively produce cluster indicators with probability representation from the refined graphs. The seamless integration between graph learning and indicator generation enables them to interact directly and enhance each other. To solve the proposed model, we design an effective optimization algorithm. Extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art methods. The code is available at: https://github.com/W-Xinxin/BPMLD.},
  archive      = {J_TIP},
  author       = {Xinxin Wang and Yongshan Zhang and Yicong Zhou},
  doi          = {10.1109/TIP.2025.3574924},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3609-3621},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bidirectional probabilistic multi-graph learning and decomposition for multi-view clustering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIFNet: Learning modality-invariant features for generalizable multimodal image matching. <em>TIP</em>, <em>34</em>, 3593-3608. (<a href='https://doi.org/10.1109/TIP.2025.3574937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many keypoint detection and description methods have been proposed for image matching or registration. While these methods demonstrate promising performance for single-modality image matching, they often struggle with multimodal data because the descriptors trained on single-modality data tend to lack robustness against the non-linear variations present in multimodal data. Extending such methods to multimodal image matching often requires well-aligned multimodal data to learn modality-invariant descriptors. However, acquiring such data is often costly and impractical in many real-world scenarios. To address this challenge, we propose a modality-invariant feature learning network (MIFNet) to compute modality-invariant features for keypoint descriptions in multimodal image matching using only single-modality training data. Specifically, we propose a novel latent feature aggregation module and a cumulative hybrid aggregation module to enhance the base keypoint descriptors trained on single-modality data by leveraging pre-trained features from Stable Diffusion models. We validate our method with recent keypoint detection and description methods in three multimodal retinal image datasets (CF-FA, CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is able to learn modality-invariant feature for multimodal image matching without accessing the targeted modality and has good zero-shot generalization ability. The code will be released at https://github.com/lyp-deeplearning/MIFNet},
  archive      = {J_TIP},
  author       = {Yepeng Liu and Zhichao Sun and Baosheng Yu and Yitian Zhao and Bo Du and Yongchao Xu and Jun Cheng},
  doi          = {10.1109/TIP.2025.3574937},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3593-3608},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MIFNet: Learning modality-invariant features for generalizable multimodal image matching},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CKD: Contrastive knowledge distillation from a sample-wise perspective. <em>TIP</em>, <em>34</em>, 3578-3592. (<a href='https://doi.org/10.1109/TIP.2025.3573474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a simple yet effective contrastive knowledge distillation framework that achieves sample-wise logit alignment while preserving semantic consistency. Conventional knowledge distillation approaches exhibit over-reliance on feature similarity per sample, which risks overfitting, and contrastive approaches focus on inter-class discrimination at the expense of intra-sample semantic relationships. Our approach transfers “dark knowledge” through teacher-student contrastive alignment at the sample level. Specifically, our method first enforces intra-sample alignment by directly minimizing teacher-student logit discrepancies within individual samples. Then, we utilize inter-sample contrasts to preserve semantic dissimilarities across samples. By redefining positive pairs as aligned teacher-student logits from identical samples and negative pairs as cross-sample logit combinations, we reformulate these dual constraints into an InfoNCE loss framework, reducing computational complexity lower than sample squares while eliminating dependencies on temperature parameters and large batch sizes. We conduct comprehensive experiments across three benchmark datasets, including the CIFAR-100, ImageNet-1K, and MS COCO datasets, and experimental results clearly confirm the effectiveness of the proposed method on image classification, object detection, and instance segmentation tasks.},
  archive      = {J_TIP},
  author       = {Wencheng Zhu and Xin Zhou and Pengfei Zhu and Yu Wang and Qinghua Hu},
  doi          = {10.1109/TIP.2025.3573474},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3578-3592},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CKD: Contrastive knowledge distillation from a sample-wise perspective},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prototypical distribution divergence loss for image restoration. <em>TIP</em>, <em>34</em>, 3563-3577. (<a href='https://doi.org/10.1109/TIP.2025.3572818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have achieved significant advances in the field of image restoration and much research has focused on designing new architectures for convolutional neural networks (CNNs) and Transformers. The choice of loss functions, despite being a critical factor when training image restoration networks, has attracted little attention. The existing losses are primarily based on semantic or hand-crafted representations. Recently, discrete representations have demonstrated strong capabilities in representing images. In this work, we explore the loss of discrete representations for image restoration. Specifically, we propose a Local Residual Quantized Variational AutoEncoder (Local RQ-VAE) to learn prototype vectors that represent the local details of high-quality images. Then we propose a Prototypical Distribution Divergence (PDD) loss that measures the Kullback-Leibler divergence between the prototypical distributions of the restored and target images. Experimental results demonstrate that our PDD loss improves the restored images in both PSNR and visual quality for state-of-the-art CNNs and Transformers on several image restoration tasks, including image super-resolution, image denoising, image motion deblurring, and defocus deblurring.},
  archive      = {J_TIP},
  author       = {Jialun Peng and Jingjing Fu and Dong Liu},
  doi          = {10.1109/TIP.2025.3572818},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3563-3577},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prototypical distribution divergence loss for image restoration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral video tracking with Spectral–Spatial fusion and memory enhancement. <em>TIP</em>, <em>34</em>, 3547-3562. (<a href='https://doi.org/10.1109/TIP.2025.3569479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral video (HSV) provides rich spectral-spatial-temporal information, enabling the capture of complex object dynamics beyond the limitations of conventional single- and multi-modal tracking. However, current HSV tracking methods face challenges such as data scarcity, band gaps, spectral fragmentation, temporal underutilization, and high computational load, which constrain performance. In this article, we present SpectralTrack, a novel HSV tracking framework with spectral-spatial fusion and memory enhancement. SpectralTrack incorporates an explicit visual prompting module to mitigate band gaps and spectral fragmentation. We further introduce an extraction-matching-interaction module, which leverages a template-bridging search adapter and a multi-layer perceptron adapter within a multi-modal Transformer architecture for efficient cross-modal feature extraction-matching-interaction. Additionally, a memory perception module enhances state reasoning by injecting temporal prompts to refine spectral and spatial cues. SpectralTrack follows parameter-efficient fine-tuning and feature-level fusion to alleviate data scarcity and reduce computational overhead. We instantiate two variants, SpectralTrack and SpectralTrack+, across nine HSV tracking datasets, demonstrating superior effectiveness over extensive trackers. Implementations and results will be available at https://github.com/YZCU/SpectralTrack},
  archive      = {J_TIP},
  author       = {Yuzeng Chen and Qiangqiang Yuan and Hong Xie and Yuqi Tang and Yi Xiao and Jiang He and Renxiang Guan and Xinwang Liu and Liangpei Zhang},
  doi          = {10.1109/TIP.2025.3569479},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3547-3562},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral video tracking with Spectral–Spatial fusion and memory enhancement},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UncTrack: Reliable visual object tracking with uncertainty-aware prototype memory network. <em>TIP</em>, <em>34</em>, 3533-3546. (<a href='https://doi.org/10.1109/TIP.2025.3559796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based trackers have achieved promising success and become the dominant tracking paradigm because of their accuracy and efficiency. Despite the substantial progress, most of the existing approaches handle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been largely overlooked, which hampers trackers’ ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer-based tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack uses a transformer encoder to perform feature interactions between the template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then, the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, which makes the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code is available at https://github.com/ManOfStory/UncTrack},
  archive      = {J_TIP},
  author       = {Siyuan Yao and Yang Guo and Yanyang Yan and Wenqi Ren and Xiaochun Cao},
  doi          = {10.1109/TIP.2025.3559796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3533-3546},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UncTrack: Reliable visual object tracking with uncertainty-aware prototype memory network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSF-net: Spatial-spectral fusion network with spectral angle awareness for hyperspectral object tracking. <em>TIP</em>, <em>34</em>, 3518-3532. (<a href='https://doi.org/10.1109/TIP.2025.3572812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal information simultaneously, making it highly suitable for handling challenges such as background clutter and visual similarity in object tracking. However, existing methods primarily focus on band regrouping and rely on RGB trackers for feature extraction, resulting in limited exploration of spectral information and difficulties in achieving complementary representations of object features. In this paper, a spatial-spectral fusion network with spectral angle awareness (SSF-Net) is proposed for hyperspectral (HS) object tracking. Firstly, to address the issue of insufficient spectral feature extraction in existing networks, a spatial-spectral feature backbone ( $S^{2}$ FB) is designed. With the spatial and spectral extraction branch, a joint representation of texture and spectrum is obtained. Secondly, a spectral attention fusion module (SAFM) is presented to capture the intra- and inter-modality correlation to obtain the fused features from the HS and RGB modalities. It can incorporate the visual information into the HS context to form a robust representation. Thirdly, to ensure a more accurate response to the object position, a spectral angle awareness module (SAAM) is designed to investigate the region-level spectral similarity between the template and search images during the prediction stage. Furthermore, a novel spectral angle awareness loss (SAAL) is developed to offer guidance for the SAAM based on similar regions. Finally, to obtain the robust tracking results, a weighted prediction method is considered to combine the HS and RGB predicted motions of objects to leverage the strengths of each modality. Extensive experiments on the HOTC-2020, HOTC-2024, and BihoT datasets demonstrate the effectiveness of the proposed SSF-Net compared with state-of-the-art trackers. The source code will be available at https://github.com/hzwyhc/hsvt},
  archive      = {J_TIP},
  author       = {Hanzheng Wang and Wei Li and Xiang-Gen Xia and Qian Du and Jing Tian},
  doi          = {10.1109/TIP.2025.3572812},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3518-3532},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SSF-net: Spatial-spectral fusion network with spectral angle awareness for hyperspectral object tracking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing zero-shot digital human quality assessment through text-prompted evaluation. <em>TIP</em>, <em>34</em>, 3503-3517. (<a href='https://doi.org/10.1109/TIP.2025.3570597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zero-shot performance. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D},
  archive      = {J_TIP},
  author       = {Zicheng Zhang and Wei Sun and Yingjie Zhou and Haoning Wu and Chunyi Li and Xiongkuo Min and Xiaohong Liu and Guangtao Zhai and Weisi Lin},
  doi          = {10.1109/TIP.2025.3570597},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3503-3517},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing zero-shot digital human quality assessment through text-prompted evaluation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing environmental robustness in few-shot learning via conditional representation learning. <em>TIP</em>, <em>34</em>, 3489-3502. (<a href='https://doi.org/10.1109/TIP.2025.3572762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) has recently been extensively utilized to overcome the scarcity of training data in domain-specific visual recognition. In real-world scenarios, environmental factors such as complex backgrounds, varying lighting conditions, long-distance shooting, and moving targets often cause test images to exhibit numerous incomplete targets or noise disruptions. However, current research on evaluation datasets and methodologies has largely ignored the concept of “environmental robustness”, which refers to maintaining consistent performance in complex and diverse physical environments. This neglect has led to a notable decline in the performance of FSL models during practical testing compared to their training performance. To bridge this gap, we introduce a new real-world multi-domain few-shot learning (RD-FSL) benchmark, which includes four domains and six evaluation datasets. The test images in this benchmark feature various challenging elements, such as camouflaged objects, small targets, and blurriness. Our evaluation experiments reveal that existing methods struggle to utilize training images effectively to generate accurate feature representations for challenging test images. To address this problem, we propose a novel conditional representation learning network (CRLNet) that integrates the interactions between training and testing images as conditional information in their respective representation processes. The main goal is to reduce intra-class variance or enhance inter-class variance at the feature representation level. Finally, comparative experiments reveal that CRLNet surpasses the current state-of-the-art methods, achieving performance improvements ranging from 6.83% to 16.98% across diverse settings and backbones. The source code and dataset are available at https://github.com/guoqianyu-alberta/Conditional-Representation-Learning},
  archive      = {J_TIP},
  author       = {Qianyu Guo and Jingrong Wu and Tianxing Wu and Haofen Wang and Weifeng Ge and Wenqiang Zhang},
  doi          = {10.1109/TIP.2025.3572762},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3489-3502},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing environmental robustness in few-shot learning via conditional representation learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiview point cloud registration via optimization in an autoencoder latent space. <em>TIP</em>, <em>34</em>, 3475-3488. (<a href='https://doi.org/10.1109/TIP.2025.3565998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.},
  archive      = {J_TIP},
  author       = {Luc Vedrenne and Sylvain Faisan and Denis Fortun},
  doi          = {10.1109/TIP.2025.3565998},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3475-3488},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiview point cloud registration via optimization in an autoencoder latent space},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-video retrieval with global-LocalSemantic consistent learning. <em>TIP</em>, <em>34</em>, 3463-3474. (<a href='https://doi.org/10.1109/TIP.2025.3574925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting large-scale image-text pre-training models, e.g., CLIP, to the video domain represents the current state-of-the-art for text-video retrieval. The primary approaches involve transferring text-video pairs to a common embedding space and leveraging cross-modal interactions on specific entities for semantic alignment. Though effective, these paradigms entail prohibitive computational costs, leading to inefficient retrieval. To address this, we propose a simple yet effective method, Global-Local Semantic Consistent Learning (GLSCL), which capitalizes on latent shared semantics across modalities for text-video retrieval. Specifically, we introduce a parameter-free global interaction module to explore coarse-grained alignment. Then, we devise a shared local interaction module that employs several learnable queries to capture latent semantic concepts for learning fine-grained alignment. Furthermore, an Inter-Consistency Loss (ICL) is devised to accomplish the concept alignment between the visual query and corresponding textual query, and an Intra-Diversity Loss (IDL) is developed to repulse the distribution within visual (textual) queries to generate more discriminative concepts. Extensive experiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC, and ActivityNet) substantiate the superior effectiveness and efficiency of the proposed method. Remarkably, our method achieves comparable performance with SOTA as well as being nearly 220 times faster in terms of computational cost. Code is available at: https://github.com/zchoi/GLSCL},
  archive      = {J_TIP},
  author       = {Haonan Zhang and Pengpeng Zeng and Lianli Gao and Jingkuan Song and Yihang Duan and Xinyu Lyu and Heng Tao Shen},
  doi          = {10.1109/TIP.2025.3574925},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3463-3474},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Text-video retrieval with global-LocalSemantic consistent learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How does audio influence visual attention in omnidirectional videos? database and model. <em>TIP</em>, <em>34</em>, 3447-3462. (<a href='https://doi.org/10.1109/TIP.2025.3567842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and predicting viewer attention in omnidirectional videos (ODVs) is crucial for enhancing user engagement in virtual and augmented reality applications. Although both audio and visual modalities are essential for saliency prediction in ODVs, the joint exploitation of these two modalities has been limited, primarily due to the absence of large-scale audio-visual saliency databases and comprehensive analyses. This paper comprehensively investigates audio-visual attention in ODVs from both subjective and objective perspectives. Specifically, we first introduce a new audio-visual saliency database for omnidirectional videos, termed AVS-ODV database, containing 162 ODVs and corresponding eye movement data collected from 60 subjects under three audio modes including mute, mono, and ambisonics. Based on the constructed AVS-ODV database, we perform an in-depth analysis of how audio influences visual attention in ODVs. To advance the research on audio-visual saliency prediction for ODVs, we further establish a new benchmark based on the AVS-ODV database by testing numerous state-of-the-art saliency models, including visual-only models and audio-visual models. In addition, given the limitations of current models, we propose an innovative omnidirectional audio-visual saliency prediction network (OmniAVS), which is built based on the U-Net architecture, and hierarchically fuses audio and visual features from the multimodal aligned embedding space. Extensive experimental results demonstrate that the proposed OmniAVS model outperforms other state-of-the-art models on both ODV AVS prediction and traditional AVS prediction tasks. The AVS-ODV database and the OmniAVS model are available at: https://github.com/IntMeGroup/AVS-ODV.},
  archive      = {J_TIP},
  author       = {Yuxin Zhu and Huiyu Duan and Kaiwei Zhang and Yucheng Zhu and Xilei Zhu and Long Teng and Xiongkuo Min and Guangtao Zhai},
  doi          = {10.1109/TIP.2025.3567842},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3447-3462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {How does audio influence visual attention in omnidirectional videos? database and model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-CAM: Context-level weight-based CAM with sequential denoising to generate high-quality class activation maps. <em>TIP</em>, <em>34</em>, 3431-3446. (<a href='https://doi.org/10.1109/TIP.2025.3573509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class activation mapping (CAM) methods have garnered considerable research attention because they can be used to interpret the decision-making of deep convolutional neural network (CNN) models and provide initial masks for weakly supervised semantic segmentation (WSSS) tasks. However, the class activation maps generated by most CAM methods usually have two limitations: 1) a lack of the ability to cover the whole object when using low-level features; and 2) introducing background noise. To mitigate these issues, an innovative Context-level weights-based CAM (Context-CAM) method is proposed, which guarantees: 1) the non-discriminative regions that have similar appearances and are located close to the discriminative regions can also be highlighted by the newly designed Region-Enhanced Mapping (REM) module using context-level weights; and 2) the background noises are gradually eliminated via a newly proposed Semantic-guided Reverse Sequence Fusion (SRSF) strategy that can sequentially denoise and fuse the region-enhanced maps from the last layer to the first layer. Extensive experimental results show that our Context-CAM can generate higher-quality class activation maps than classic and state-of-the-art (SOTA) CAM methods in terms of the Energy-Based Pointing Game (EBPG) score, and the improvements are up to 35.49% when compared to the second-best method. Moreover, for WSSS tasks, our Context-CAM can directly replace the CAM method used in existing WSSS methods without any architectural modification to further improve the segmentation performance. Our code is available at https://github.com/cwb0611/Context-CAM.},
  archive      = {J_TIP},
  author       = {Jie Du and Wenbing Chen and Chi-Man Vong and Peng Liu and Tianfu Wang},
  doi          = {10.1109/TIP.2025.3573509},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3431-3446},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Context-CAM: Context-level weight-based CAM with sequential denoising to generate high-quality class activation maps},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrivacyHFR: Visual privacy preserving for heterogeneous face recognition. <em>TIP</em>, <em>34</em>, 3417-3430. (<a href='https://doi.org/10.1109/TIP.2025.3572793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has achieved remarkable progress and is widely deployed in real-world scenarios. Recently more and more attention has been given to individual privacy protection, due to unauthorized sensitive image leakage by malicious attackers. Multi-modality face images captured by diverse sensors, also called heterogeneous faces, bring in more challenges in face privacy protection while lacking related research. In this paper, we propose a novel visual Privacy preserving method for Heterogeneous Face Recognition (Privacy-HFR) to protect perceptual visual information and maintain essential identity information in multi-modality face analysis scenarios. Frequency domain analysis is a vital strategy to bridge the inevitable modality gap for heterogeneous face images. Meanwhile, recent theoretical insights also inspire us to design a suitable frequency component adjustment to balance human visual sensitivity and identity discriminative information. In addition, the ability to defend against recovery attacks has emerged as an essential criterion for privacy preserving face recognition. Noting that there seems to exist a dilemma that reducing accessible information by the attack model will affect the extracted identity information for recognition. It is because these two kinds of information are mutually blended in the frequency domain, which makes it a challenge to simultaneously maintain visual privacy and identity distinguishability. Thus, we provide a novel perspective to leverage the randomly optimal solutions and design the specific adversarial perturbations against the recovery attack. Experiments on several large-scale heterogeneous face datasets (CASIA NIR-VIS 2.0, LAMP-HQ, Tufts Face and CUFSF datasets) prove that the proposed method outperforms existing privacy-preserving face recognition methods in terms of recognition accuracy and privacy protection capability. The code is available in https://github.com/xiyin11/Privacy-HFR},
  archive      = {J_TIP},
  author       = {Decheng Liu and Weizhao Yang and Chunlei Peng and Nannan Wang and Ruimin Hu and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3572793},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3417-3430},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PrivacyHFR: Visual privacy preserving for heterogeneous face recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the potential of pooling techniques for universal image restoration. <em>TIP</em>, <em>34</em>, 3403-3416. (<a href='https://doi.org/10.1109/TIP.2025.3572788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration involves recovering a clean image from its degraded counterpart. In recent years, we have witnessed a paradigm shift from convolutional neural networks to Transformers, which have quadratic complexity with respect to the input size. Instead of designing more complex modules based on recent techniques, this paper presents an efficient and effective mechanism for image restoration by exploring the potential of ubiquitous pooling techniques. We leverage different pooling operators as tools for implicit dual-domain representation learning. Specifically, the average and max pooling can be used as extractors for implicit low- and high-frequency signals, respectively. Then, we utilize lightweight learnable parameters to modulate the resulting frequency components. Furthermore, the intermediate high-frequency features can serve as attention maps to highlight the spatial edge information. Our pooling module is built by incorporating the aforementioned dual-domain modulation across multiple scales and various shapes. We demonstrate the effectiveness of our module in single-degradation, composite-degradation, and all-in-one image restoration tasks. Extensive experimental results show that the resulting network achieves state-of-the-art performance on 15 datasets for five single-degradation and two composite-degradation image restoration tasks by deploying our module. Moreover, our method can be extended to all-in-one scenarios and performs favorably against state-of-the-art all-in-one algorithms under two settings. The code is available at https://github.com/c-yn/PoolNet},
  archive      = {J_TIP},
  author       = {Yuning Cui and Wenqi Ren and Alois Knoll},
  doi          = {10.1109/TIP.2025.3572788},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3403-3416},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring the potential of pooling techniques for universal image restoration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPU+: Dimension folding for semantic point cloud upsampling. <em>TIP</em>, <em>34</em>, 3389-3402. (<a href='https://doi.org/10.1109/TIP.2025.3571680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic Point Cloud Upsampling (SPU) aims to reconstruct a high-resolution (dense) 3D point cloud from a low-resolution (sparse) one, ensuring that the upsampled point cloud is easily recognizable by downstream tasks. Conventional upsampling architectures typically represent point clouds using high-dimensional feature vectors. However, we observe a dimensional bottleneck, where simply increasing the feature dimensionality does not necessarily improve performance on semantic tasks. This insight motivates us to explore more effective feature representations within upsampling networks. In this paper, we propose a novel SPU method called SPU+, which introduces dimension folding as an alternative strategy for handling high-dimensional features. Specifically, SPU+ decomposes each high-dimensional feature into several g-dimensional packages, allowing interactions among packages within the feature space. Guided by the principle of maximizing feature diversity, we determine that setting the package dimension to 3 yields optimal performance. To enable convolutional operations over these 3D packages, we present a 3D Residual Graph Convolution Block (3D-RGCB) that achieves high computational efficiency. Based on 3D-RGCBs, we design an upsampling network that incorporates three structural modes: pre-mode, middle-mode, and end-mode. Additionally, for large-scale upsampling, we develop a scaling-and-shuffling strategy that adaptively adjusts the spatial size of each 3D package. Finally, we analyze the covering number of the 3D package representation and compare it to traditional high-dimensional feature representations. Experiments on publicly available datasets demonstrate not only the effectiveness of dimension folding but also the state-of-the-art performance achieved by SPU+. Code is available at: https://github.com/lizhuangzi/SPU_plus},
  archive      = {J_TIP},
  author       = {Zhuangzi Li and Thomas H. Li and Shan Liu and Ge Li},
  doi          = {10.1109/TIP.2025.3571680},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3389-3402},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SPU+: Dimension folding for semantic point cloud upsampling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PLGS: Robust panoptic lifting with 3D gaussian splatting. <em>TIP</em>, <em>34</em>, 3377-3388. (<a href='https://doi.org/10.1109/TIP.2025.3573524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous methods utilize the Neural Radiance Field (NeRF) for panoptic lifting, while their training and rendering speed are unsatisfactory. In contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due to its rapid training and rendering speed. However, unlike NeRF, the conventional 3DGS may not satisfy the basic smoothness assumption as it does not rely on any parameterized structures to render (e.g., MLPs). Consequently, the conventional 3DGS is, in nature, more susceptible to noisy 2D mask supervision. In this paper, we propose a new method called PLGS that enables 3DGS to generate consistent panoptic segmentation masks from noisy 2D segmentation masks while maintaining superior efficiency compared to NeRF-based methods. Specifically, we build a panoptic-aware structured 3D Gaussian model to introduce smoothness and design effective noise reduction strategies. For the semantic field, instead of initialization with structure from motion, we construct reliable semantic anchor points to initialize the 3D Gaussians. We then use these anchor points as smooth regularization during training. Additionally, we present a self-training approach using pseudo labels generated by merging the rendered masks with the noisy masks to enhance the robustness of PLGS. For the instance field, we project the 2D instance masks into 3D space and match them with oriented bounding boxes to generate cross-view consistent instance masks for supervision. Experiments on various benchmarks demonstrate that our method outperforms previous state-of-the-art methods in terms of both segmentation quality and speed.},
  archive      = {J_TIP},
  author       = {Yu Wang and Xiaobao Wei and Ming Lu and Guoliang Kang},
  doi          = {10.1109/TIP.2025.3573524},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3377-3388},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PLGS: Robust panoptic lifting with 3D gaussian splatting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized category discovery with unknown sample generation. <em>TIP</em>, <em>34</em>, 3366-3376. (<a href='https://doi.org/10.1109/TIP.2025.3572767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning uses labeled and unlabeled data from known classes for training, assuming the test data contains only those classes. However, in real-world scenarios, new classes can appear. Generalized Category Discovery (GCD) extends SSL to handle unlabeled samples that may belong to both known and unknown categories. The challenge arises from the lack of prior information about the unknown categories. We propose to generate unknown samples to address the GCD problem, called Generalized Category Discovery with Unknown Sample Generation (GCDUSG). Since the number of unknown categories is uncertain, we propose a prototype alignment method to estimate both the class numbers and pseudo-labels for unlabeled samples, thereby enabling us to learn the unknown prototypes. We have developed a process for generating realistic and discriminative unknown samples based on the known-unknown relationships between known and unknown prototypes. We generate realistic and discriminative unknown samples leveraging the known-unknown relationships. We achieve this by minimizing the class-wise Maximum Mean Discrepancy distance between the generated samples and the selected unknown samples. To account for the pseudo-labels assigned to unlabeled samples, we train a classifier using all samples, incorporating a pseudo-label supervision loss to mitigate the impact of potentially erroneous labels. This comprehensive training equips the classifier to effectively handle both known and unknown classes during testing. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our approach.},
  archive      = {J_TIP},
  author       = {Xiao Li and Min Fang and Haixiang Li},
  doi          = {10.1109/TIP.2025.3572767},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3366-3376},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized category discovery with unknown sample generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Restoration of images taken through a dirty window using optics-guided transformer. <em>TIP</em>, <em>34</em>, 3352-3365. (<a href='https://doi.org/10.1109/TIP.2025.3573500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking photographs through windows is an inevitable scenario in the real world, but glass windows are not ideally clean in most cases. Although there exists various raindrop removal methods, the occlusion of dirt, as another dirty window case, has not been well valued. The vital reasons include i) the limitation of the optical imaging model proposed in previous methods, and ii) the shortage of a practical dataset for sufficient types of dirty glass windows. To fill this research gap, in this paper, we first propose a general optical imaging model that fits widely used dirty window cases. Following this, training and testing synthetic datasets are generated, and real-world dirty window data are collected to evaluate the effectiveness of our imaging model and synthetic data. For the methodology part, we propose an optics-guided Transformer network to solve this special image restoration problem, i.e., the dirt removal for images taken through a dirty window. Experimental results demonstrate that our imaging model is effective and robust. Our proposed network leads to higher performance than existing methods on both synthetic and real-world dirty window images. Code and data are available at https://github.com/Zongliang-Wu/ReDNet},
  archive      = {J_TIP},
  author       = {Zongliang Wu and Juzheng Zhang and Ying Fu and Yulun Zhang and Xin Yuan},
  doi          = {10.1109/TIP.2025.3573500},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3352-3365},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Restoration of images taken through a dirty window using optics-guided transformer},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring invariance matters for domain generalization. <em>TIP</em>, <em>34</em>, 3336-3351. (<a href='https://doi.org/10.1109/TIP.2025.3568747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) aims to solve the problem of significant performance degradation when target domain data collected from the Out-Of-Distribution (O.O.D). Previous efforts try to exploit invariant features in the source domain through CNN networks. However, inspired by causal mechanisms, we find that the complex spurious-invariant information is still hidden in this view invariant features, and the impact of domain and class discrepancies on extracting invariance has not been effectively mitigated. To alleviate these issues, we propose a self-weighted multi-view mining invariance domain generalization framework (SMIDG). On the one hand, to make up for the insufficiency of traditional single-view convolutional feature extraction networks, we propose to mine features from another frequency view and use the self-adaptive adversarial masks to eliminate some spurious correlations, ensuring causal invariance in the coarse-grained generalization. However, due to inconsistencies in discriminative information between inter-domain and intra-domain samples, as well as inter-class and intra-class samples, the coarse-grained elimination of spurious associations does not fully resolve this issue. On the other hand, we also consider the fine-grained generalization from two aspects. Firstly, to better tackle the domain discrepancies, we propose a novel progressive contrastive learning strategy that learns the underlying specific features of samples while gradually mitigating domain discrepancies, thereby ensuring domain invariance in fine-grained generalization. Secondly, due to the issue of feature inconsistency, we adopt a self-adaptive hard sample mining method with information gain to ensure that the model pays more attention on hard disentangled samples, thus maintaining feature invariance. Extensive experiments on five benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. Our code is available at https://github.com/bihhm/SMIDG},
  archive      = {J_TIP},
  author       = {Shanshan Wang and Houmeng He and Xun Yang and Zhipu Liu and Yuanhong Zhong and Xingyi Zhang and Meng Wang},
  doi          = {10.1109/TIP.2025.3568747},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3336-3351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring invariance matters for domain generalization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-resolution natural image matting by refining low-resolution alpha mattes. <em>TIP</em>, <em>34</em>, 3323-3335. (<a href='https://doi.org/10.1109/TIP.2025.3573620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution natural image matting plays an important role in image editing, film-making and remote sensing due to its ability of accurately extract the foreground from a natural background. However, due to the complexity brought about by the proliferation of resolution, the existing image matting methods cannot obtain high-quality alpha mattes on high-resolution images in reasonable time. To overcome this challenge, we introduce a high-resolution image matting framework based on alpha matte refinement from low-resolution to high-resolution (HRIMF-AMR). The proposed framework transforms the complex high-resolution image matting problem into low-resolution image matting problem and high-resolution alpha matte refinement problem. While the first problem is solved by adopting an existing image matting method, the latter is addressed by applying the Detail Difference Feature Extractor (DDFE) designed as a part of our work. The DDFE extracts detail difference features from high-resolution images by measuring the image feature difference between high-resolution images and low-resolution images. The low-resolution alpha matte is refined according to the extracted detail difference feature, providing the high-resolution alpha matte. In addition, the Matte Detail Resolution Difference (MDRD) loss is introduced to train the DDFE, which imposes an additional constraint on the extraction of detail difference features with mattes. Experimental results show that integrating HRIMF-AMR significantly enhances the performance of existing matting methods on high-resolution images of Transparent-460 and Alphamatting. Project page: https://github.com/yexianmin/HRAMR-Matting},
  archive      = {J_TIP},
  author       = {Xianmin Ye and Yihui Liang and Mian Tan and Fujian Feng and Lin Wang and Han Huang},
  doi          = {10.1109/TIP.2025.3573620},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3323-3335},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-resolution natural image matting by refining low-resolution alpha mattes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning guided implicit depth function with scale-aware feature fusion. <em>TIP</em>, <em>34</em>, 3309-3322. (<a href='https://doi.org/10.1109/TIP.2025.3570571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the single image super-resolution based on implicit image function is a hot topic, which learns a universal model for arbitrary upsampling scales. By contrast, color-guided depth map super-resolution is less explored based on implicit function learning. The related research faces three questions. First, is it also necessary and applicable to fuse the depth feature and the color feature in the encoder with continuous upsampling scales? Second, is the scale information in the encoder as important as that in the decoder? Third, how to efficiently and effectively model the affinity of location distance and content similarity within cross domains in the decoder? This paper proposes a transformer-based network to answer the above questions, which includes a depth super-resolution branch and a guidance extraction branch. Specifically, in the encoder, the effective implicit cross transformer is designed to fuse the guidance from the color feature with continuous coordinate mapping. In addition, the unrelated guidance is filtered out by correlation evaluation in the high-dimension feature space. Unlike the scale only introduced in the decoder, this paper additionally embeds the scale into the position encoding and the feed-forward network in the encoder to learn the scale-aware feature representation. In the decoder, the high-resolution depth feature is reconstructed by using the internal prior and the external guidance. The internal prior is implemented by implicit self-attention in the depth super-resolution branch, and the external guidance is exploited via implicit cross-attention between both branches. Finally, the above decoded features are complementary to generate the high-resolution depth map. The sufficient experiments on the synthetic and real datasets for in-distribution and out-of-distribution upsampling scales validate the improved performance. The code and the models are public via https://github.com/NaNRan13/GIDF},
  archive      = {J_TIP},
  author       = {Yifan Zuo and Yuqi Hu and Yaping Xu and Zhi Wang and Yuming Fang and Jiebin Yan and Wenhui Jiang and Yuxin Peng and Yan Huang},
  doi          = {10.1109/TIP.2025.3570571},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3309-3322},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning guided implicit depth function with scale-aware feature fusion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCT-CCDiff: Context-aware contrastive diffusion model with mediator-bridging cross-modal transformer for image change captioning. <em>TIP</em>, <em>34</em>, 3294-3308. (<a href='https://doi.org/10.1109/TIP.2025.3573471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in diffusion models (DMs) have showcased superior capabilities in generating images and text. This paper first introduces DMs for image change captioning (ICC) and proposes a novel Context-aware Contrastive Diffusion model with Mediator-bridging Cross-modal Transformer (MCT-CCDiff) to accurately predict visual difference descriptions conditioned on two similar images. Technically, MCT-CCDiff develops a Text Embedding Contrastive Loss (TECL) that leverages both positive and negative samples to more effectively distinguish text embeddings, thus generating more discriminative text representations for ICC. To accurately predict visual difference descriptions, MCT-CCDiff introduces a Mediator-bridging Cross-modal Transformer (MCTrans) designed to efficiently explore the cross-modal correlations between visual differences and corresponding text by using a lightweight mediator, mitigating interference from visual redundancy and reducing interaction overhead. Additionally, it incorporates context-augmented denoising to further understand the contextual relationships within caption words implemented by a revised diffusion loss, which provides a tighter optimization bound, leading to enhanced optimization effects for high-quality text generation. Extensive experiments conducted on four benchmark datasets clearly demonstrate that our MCT-CCDiff significantly outperforms state-of-the-art methods in the field of ICC, marking a notable advancement in the generation of precise visual difference descriptions.},
  archive      = {J_TIP},
  author       = {Jinhong Hu and Guojin Zhong and Jin Yuan and Wenbo Pan and Xiaoping Wang},
  doi          = {10.1109/TIP.2025.3573471},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3294-3308},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MCT-CCDiff: Context-aware contrastive diffusion model with mediator-bridging cross-modal transformer for image change captioning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MindGPT: Interpreting what you see with non-invasive brain recordings. <em>TIP</em>, <em>34</em>, 3281-3293. (<a href='https://doi.org/10.1109/TIP.2025.3572784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed MindGPT, which interprets perceived visual stimuli into natural languages from functional Magnetic Resonance Imaging (fMRI) signals in an end-to-end manner. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism. By the collaborative use of data augmentation techniques, this architecture permits us to guide latent neural representations towards a desired language semantic direction in a self-supervised fashion. Through doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The source code for the MindGPT model is publicly available at https://github.com/JxuanC/MindGPT.},
  archive      = {J_TIP},
  author       = {Jiaxuan Chen and Yu Qi and Yueming Wang and Gang Pan},
  doi          = {10.1109/TIP.2025.3572784},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3281-3293},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MindGPT: Interpreting what you see with non-invasive brain recordings},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eyes on islanded nodes: Better reasoning via structure augmentation and feature co-training on bi-level knowledge graphs. <em>TIP</em>, <em>34</em>, 3268-3280. (<a href='https://doi.org/10.1109/TIP.2025.3572825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) represent known entities and their relationships using triplets, but this method cannot represent relationships between facts, limiting their expressiveness. Recently, the Bi-level Knowledge Graph (Bi-level KG) has addressed this issue by modeling facts as nodes and establishing relationships between these facts, introducing two new tasks: triplet prediction and conditional link prediction. Existing methods enhance triplets through data augmentation method and represent facts using entity representations. However, these methods do not address the isolated nodes at the structure level, nor do they effectively capture the information of facts at the feature level. To address these two issues, we design a data augmentation method that identifies islanded node by detecting anomalous structures and features in the graph. Subsequently, we perform similar subgraph matching for each isolated node to construct potential facts. To enrich the features of facts, we design a weighted combination initialization method for facts and introduce a new relation $\widetilde {R}$ , to connect facts with related entities. This approach allows for the co-training of fact and entity representations during the training process. Extensive experiments validate the effectiveness of our data augmentation and co-training methods. Our model achieves optimal performance in triplet prediction and conditional link prediction tasks.},
  archive      = {J_TIP},
  author       = {Hao Li and Ke Liang and Wenjing Yang and Lingyuan Meng and Yaohua Wang and Sihang Zhou and Xinwang Liu},
  doi          = {10.1109/TIP.2025.3572825},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3268-3280},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Eyes on islanded nodes: Better reasoning via structure augmentation and feature co-training on bi-level knowledge graphs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-concept learning for scene graph generation. <em>TIP</em>, <em>34</em>, 3253-3267. (<a href='https://doi.org/10.1109/TIP.2025.3540296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model’s performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.},
  archive      = {J_TIP},
  author       = {Xinyu Lyu and Lianli Gao and Junlin Xie and Pengpeng Zeng and Yulu Tian and Jie Shao and Heng Tao Shen},
  doi          = {10.1109/TIP.2025.3540296},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3253-3267},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-concept learning for scene graph generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Irregular tensor low-rank representation for hyperspectral image representation. <em>TIP</em>, <em>34</em>, 3239-3252. (<a href='https://doi.org/10.1109/TIP.2025.3571669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral variations pose a common challenge in analyzing hyperspectral images (HSI). To address this, low-rank tensor representation has emerged as a robust strategy, leveraging inherent correlations within HSI data. However, the spatial distribution of ground objects in HSIs is inherently irregular, existing naturally in tensor format, with numerous class-specific regions manifesting as irregular tensors. Current low-rank representation techniques are designed for regular tensor structures and overlook this fundamental irregularity in real-world HSIs, leading to performance limitations. To tackle this issue, we propose a novel model for irregular tensor low-rank representation tailored to efficiently model irregular 3D cubes. By incorporating a non-convex nuclear norm to promote low-rankness and integrating a global negative low-rank term to enhance the discriminative ability, our proposed model is formulated as a constrained optimization problem and solved using an alternating augmented Lagrangian method. Experimental validation conducted on four public datasets demonstrates the superior performance of our method compared to existing state-of-the-art approaches. The code is publicly available at https://github.com/hb-studying/ITLRR},
  archive      = {J_TIP},
  author       = {Bo Han and Yuheng Jia and Hui Liu and Junhui Hou},
  doi          = {10.1109/TIP.2025.3571669},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3239-3252},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Irregular tensor low-rank representation for hyperspectral image representation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal molecular representation learning via structure awareness. <em>TIP</em>, <em>34</em>, 3225-3238. (<a href='https://doi.org/10.1109/TIP.2025.3570604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model’s generalization ability. Compared to existing multi-modal approaches, MMSA can be seamlessly integrated with any graph-based method and supports multiple molecular data modalities, ensuring both versatility and compatibility. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods.},
  archive      = {J_TIP},
  author       = {Rong Yin and Ruyue Liu and Xiaoshuai Hao and Xingrui Zhou and Yong Liu and Can Ma and Weiping Wang},
  doi          = {10.1109/TIP.2025.3570604},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3225-3238},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modal molecular representation learning via structure awareness},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant feature extraction functions for UME-based point cloud detection and registration. <em>TIP</em>, <em>34</em>, 3209-3224. (<a href='https://doi.org/10.1109/TIP.2025.3570628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are unordered sets of coordinates in 3D with no functional relation imposed on them. The Rigid Transformation Universal Manifold Embedding (RTUME) is a mapping of volumetric or surface measurements on a 3D object to matrices, such that when two observations on the same object are related by a rigid transformation, this relation is preserved between their corresponding RTUME matrices, thus providing linear and robust solution to the registration and detection problems. To make the RTUME framework of 3D object detection and registration applicable for processing point cloud observations, there is a need to define a function that assigns each point in the cloud with a value (feature vector), invariant to the action of the transformation group. Since existing feature extraction functions do not achieve the desired level of invariance to rigid transformations, to the variability of sampling patterns, and to model mismatches, we present a novel approach for designing dense feature extraction functions, compatible with the requirements of the RTUME framework. One possible implementation of the approach is to adapt existing feature extracting functions, whether learned or analytic, designed for the estimation of point correspondences, to the RTUME framework. The novel feature-extracting function design employs integration over $SO(3)$ to marginalize the pose dependency of extracted features, followed by projecting features between point clouds using nearest neighbor projection to overcome other sources of model mismatch. In addition, the non-linear functions that define the RTUME mapping are optimized using an MLP model, trained to minimize the RTUME registration errors. The overall RTUME registration performance is evaluated using standard registration benchmarks, and is shown to outperform existing SOTA methods.},
  archive      = {J_TIP},
  author       = {Amit Efraim and Yuval Haitman and Joseph M. Francos},
  doi          = {10.1109/TIP.2025.3570628},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3209-3224},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Invariant feature extraction functions for UME-based point cloud detection and registration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recursive confidence training for pseudo-labeling calibration in semi-supervised few-shot learning. <em>TIP</em>, <em>34</em>, 3194-3208. (<a href='https://doi.org/10.1109/TIP.2025.3569196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Few-Shot Learning (SSFSL) aims to address the data scarcity in few-shot learning by leveraging both a few labeled support data and abundant unlabeled data. In SSFSL, a classifier trained on scarce support data is often biased and thus assigns inaccurate pseudo-labels to the unlabeled data, which will mislead downstream learning tasks. To combat this issue, we introduce a novel method called Certainty-Aware Recursive Confidence Training (CARCT). CARCT hinges on the insight that selecting pseudo-labeled data based on confidence levels can yield more informative support data, which is crucial for retraining an unbiased classifier to achieve accurate pseudo-labeling—a process we term pseudo-labeling calibration. We observe that accurate pseudo-labels typically exhibit smaller certainty entropy, indicating high-confidence pseudo-labeling compared to those of inaccurate pseudo-labels. Accordingly, CARCT constructs a joint double-Gaussian model to fit the certainty entropies collected across numerous SSFSL tasks. Thereby, A semi-supervised Prior Confidence Distribution (ssPCD) is learned to aid in distinguishing between high-confidence and low-confidence pseudo-labels. During an SSFSL task, ssPCD guides the selection of both high-confidence and low-confidence pseudo-labeled data to retrain the classifier that then assigns more accurate pseudo-labels to the low-confidence pseudo-labeled data. Such recursive confidence training continues until the low-confidence ones are exhausted, terminating the pseudo-labeling calibration. The unlabeled data all receive accurate pseudo-labels to expand the few support data to generalize the downstream learning task, which in return meta-refines the classifier, named self-training, to boost the pseudo-labeling in subsequent tasks. Extensive experiments on basic and extended SSFSL setups showcase the superiority of CARCT versus state-of-the-art methods, and comprehensive ablation studies and visualizations justify our insight. The source code is available at https://github.com/Klein-JING/CARCT},
  archive      = {J_TIP},
  author       = {Kunlei Jing and Hebo Ma and Chen Zhang and Lei Wen and Zhaorui Zhang},
  doi          = {10.1109/TIP.2025.3569196},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3194-3208},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Recursive confidence training for pseudo-labeling calibration in semi-supervised few-shot learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label space-induced pseudo label refinement for multi-source black-box domain adaptation. <em>TIP</em>, <em>34</em>, 3181-3193. (<a href='https://doi.org/10.1109/TIP.2025.3570220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional unsupervised domain adaptation (UDA) requires access to source data and/or source model parameters, prohibiting its practical application in terms of privacy, security, and intellectual property. Recent black-box UDA (BDA) reduces such constraints by defining a pseudo label from a single encapsulated source application programming interface (API) prediction, which allows for self-training of the target model. Nonetheless, existing methods have limited consideration for multi-source settings, in which multiple source domain APIs are available to generate pseudo labels. In this work, we introduce a novel training framework for multi-source BDA (MSBDA), dubbed Label Space-Induced Pseudo Label Refinement (LPR). Specifically, LPR incorporates a Pseudo label Refinery Network (PRN) that learns the relationship among source domains conditioned by the target domain only utilizing source API’s prediction. The target model is adapted by our dual phases PRN. First, a warm-up phase targets to avoid failure due to noisy samples and provide an initial pseudo-label, which is followed by a label refinement phase with domain relationship exploration. We provide theoretical support for the mechanism of the LPR. Experimental results on four benchmark datasets demonstrate that MSBDA using LPR achieves competitive performance compared to state-of-the-art approaches with different DA settings.},
  archive      = {J_TIP},
  author       = {Chaehwa Yoo and Xiaofeng Liu and Fangxu Xing and Jonghye Woo and Je-Won Kang},
  doi          = {10.1109/TIP.2025.3570220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3181-3193},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Label space-induced pseudo label refinement for multi-source black-box domain adaptation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STPNet: Scale-aware text prompt network for medical image segmentation. <em>TIP</em>, <em>34</em>, 3169-3180. (<a href='https://doi.org/10.1109/TIP.2025.3571672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of lesions plays a critical role in medical image analysis and diagnosis. Traditional segmentation approaches that rely solely on visual features often struggle with the inherent uncertainty in lesion distribution and size. To address these issues, we propose STPNet, a Scale-aware Text Prompt Network that leverages vision-language modeling to enhance medical image segmentation. Our approach utilizes multi-scale textual descriptions to guide lesion localization and employs retrieval-segmentation joint learning to bridge the semantic gap between visual and linguistic modalities. Crucially, STPNet retrieves relevant textual information from a specialized medical text repository during training, eliminating the need for text input during inference while retaining the benefits of cross-modal learning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and Kvasir-SEG. Experimental results show that our vision-language approach outperforms state-of-the-art segmentation methods, demonstrating the effectiveness of incorporating textual semantic knowledge into medical image analysis. The code has been made publicly on https://github.com/HUANGLIZI/STPNet},
  archive      = {J_TIP},
  author       = {Dandan Shan and Zihan Li and Yunxiang Li and Qingde Li and Jie Tian and Qingqi Hong},
  doi          = {10.1109/TIP.2025.3571672},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3169-3180},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {STPNet: Scale-aware text prompt network for medical image segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive prototype learning for weakly-supervised temporal action localization. <em>TIP</em>, <em>34</em>, 3154-3168. (<a href='https://doi.org/10.1109/TIP.2024.3431915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised Temporal Action Localization (WTAL) aims to localize action instances with only video-level labels during training, where two primary issues are localization incompleteness and background interference. To relieve these two issues, recent methods adopt an attention mechanism to activate action instances and simultaneously suppress background ones, which have achieved remarkable progress. Nevertheless, we argue that these two issues have not been well resolved yet. On the one hand, the attention mechanism adopts fixed weights for different videos, which are incapable of handling the diversity of different videos, thus deficient in addressing the problem of localization incompleteness. On the other hand, previous methods only focus on learning the foreground attention and the attention weights usually suffer from ambiguity, resulting in difficulty of suppressing background interference. To deal with the above issues, in this paper we propose an Adaptive Prototype Learning (APL) method for WTAL, which includes two key designs: 1) an Adaptive Transformer Network (ATN) to explicitly model background and learn video-adaptive prototypes for each specific video; 2) an OT-based Collaborative (OTC) training strategy to guide the learning of prototypes and remove the ambiguity of the foreground-background separation by introducing an Optimal Transport (OT) algorithm into the collaborative training scheme between RGB and FLOW streams. These two key designs can work together to learn video-adaptive prototypes and solve the above two issues, achieving robust localization. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our proposed APL performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wang Luo and Huan Ren and Tianzhu Zhang and Wenfei Yang and Yongdong Zhang},
  doi          = {10.1109/TIP.2024.3431915},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3154-3168},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive prototype learning for weakly-supervised temporal action localization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDSFusion: A semantic-aware infrared and visible image fusion network for degraded scenes. <em>TIP</em>, <em>34</em>, 3139-3153. (<a href='https://doi.org/10.1109/TIP.2025.3571339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single-modal infrared or visible image offers limited representation in scenes with lighting degradation or extreme weather. We propose a multi-modal fusion framework, named SDSFusion, for all-day and all-weather infrared and visible image fusion. SDSFusion exploits the commonality in image processing to achieve enhancement, fusion, and semantic task interaction in a unified framework guided by semantic awareness and multi-scale features and losses. To address the disparity between infrared and visible images in degraded scenes, we differentiate modal features in a unified fusion model. Unlike existing joint fusion methods, we propose an adversarial generative network that refines the reconstruction of low-light images by embedding fused features. It provides feature-level brightness supplementation and image reconstruction to refine brightness and contrast. Extensive experiments in degraded scenes confirm that our approach is superior to state-of-the-art approaches in visual quality and performance, demonstrating the effectiveness of interaction improvement. The code will be posted at: https://github.com/Liling-yang/SDSFusion.},
  archive      = {J_TIP},
  author       = {Jun Chen and Liling Yang and Wei Yu and Wenping Gong and Zhanchuan Cai and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3571339},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3139-3153},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SDSFusion: A semantic-aware infrared and visible image fusion network for degraded scenes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HLDD: Hierarchically learned detector and descriptor for robust image matching. <em>TIP</em>, <em>34</em>, 3123-3138. (<a href='https://doi.org/10.1109/TIP.2025.3568310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching is a critical task in computer vision research, focusing on aligning two or more images with similar features. Feature detection and description constitute the core of image matching. Handcrafted detectors are capable of obtaining distinctive points but these points may not be repeatable on the image pairs especially those with dramatic appearance changes. On the contrary, the learned detectors can extract a large number of repeatable points but many of them tend to be ambiguous points with low distinctiveness. Moreover, in the scenarios of dramatic appearance change, commonly used contrast or triplet loss in the training of descriptors employ the hard negative mining strategy, which may obtain overly challenging negative samples by global sampling, resulting in sluggish convergence or even overfitting. Those learned descriptors may not guarantee that the corresponding points enjoy larger similarities than unmatched ones, leading to inaccurate matches. To address those issues, we propose a hierarchically learned detector and descriptor (HLDD) for robust image matching, which contains three modules: a handcrafted-learned detector, a hierarchically learned descriptor, and a coarse-to-fine matching strategy. The handcrafted-learned detector integrates the advantages of handcrafted and learned detectors. It extracts distinctive feature points from a learned repeatability map robust to image changes and eliminates the ambiguous ones according to a learned distinctiveness map. The descriptor is trained by a proposed hierarchical triplet loss, which employs a dual window strategy. It can obtain the hardest negative samples in local windows, which are comparatively easier over global sampling, ensuring the effective training of descriptors. The coarse-to-fine matching strategy performs global and local mutual nearest neighbor matching on the coarse and fine descriptor maps respectively to improve the matching accuracy progressively. By comparing with other matching methods, experimental results demonstrate the superiority of the proposed method in the task of image matching, homography estimation, visual localization, and relative pose estimation. Moreover, ablation studies illustrate the effectiveness of the three proposed modules.},
  archive      = {J_TIP},
  author       = {Maoqing Hu and Bin Sun and Fuhua Zhang and Shutao Li},
  doi          = {10.1109/TIP.2025.3568310},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3123-3138},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HLDD: Hierarchically learned detector and descriptor for robust image matching},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized non-convex surrogated framework for anomaly detection on blurred hyperspectral images. <em>TIP</em>, <em>34</em>, 3108-3122. (<a href='https://doi.org/10.1109/TIP.2025.3568745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging is endowed with outstanding discriminability between different land types by its comprehensive sensing of the spectrum, thus favored applying to anomaly detection. However, blurring effect, as a critical cause for quality deterioration of hyperspectral imaging, has been omitted by previous hyperspectral anomaly detection models. On one hand, given that anomalies are sparsely distributed in nature, such blurring effect entangling neighboring pixels severely weighs those detection models down. On the other hand, abnormal objects jeopardize the low-dimensional structure of the image, thus deblurring those images with anomalies is more challenging than normal ones. Hence, it is of much significance to investigate anomaly detection using blurred hyperspectral images. To this end, this paper proposes a generalized non-convex surrogated tensor framework that is able to perform anomaly detection robustly to blurring effects on hyperspectral images. The proposed framework is featured to be a unified paradigm which guarantees convergence for a broad class of non-convex surrogates. Through treating the spatial and spectral low-rankness adaptively via Block Term Decomposition, the unevenness in the multi-linear low-rankness of hyperspectral image is comprehensively considered, which together with the non-convex surrogates results in a tighter modeling of the low-dimensional prior of hyperspectral images. Extensive experiments demonstrate the superiority of the proposed method compared with the state-of-the-art methods on both hyperspectral image deblurring and anomaly detection.},
  archive      = {J_TIP},
  author       = {Yinjian Wang and Wei Li and Yuanyuan Gui and Haijun Xie and Lianbo Zhang},
  doi          = {10.1109/TIP.2025.3568745},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3108-3122},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A generalized non-convex surrogated framework for anomaly detection on blurred hyperspectral images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OccNeRF: Advancing 3D occupancy prediction in LiDAR-free environments. <em>TIP</em>, <em>34</em>, 3096-3107. (<a href='https://doi.org/10.1109/TIP.2025.3567828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupancy prediction reconstructs 3D structures of surrounding environments. It provides detailed information for autonomous driving planning and navigation. However, most existing methods heavily rely on the LiDAR point clouds to generate occupancy ground truth, which is not available in the vision-based system. In this paper, we propose an OccNeRF method for training occupancy networks without 3D ground truth. Different from previous works which consider a bounded scene, we parameterize the reconstructed occupancy fields and reorganize the sampling strategy to align with the cameras’ infinite perceptive range. The neural rendering is adopted to convert occupancy fields to multi-camera depth maps, supervised by multi-frame photometric consistency. Moreover, for semantic occupancy prediction, we design several strategies to polish the prompts and filter the outputs of a pretrained open-vocabulary 2D segmentation model. Extensive experiments for both self-supervised depth estimation and 3D occupancy prediction tasks on nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our method. The code is available at https://github.com/LinShan-Bin/OccNeRF},
  archive      = {J_TIP},
  author       = {Chubin Zhang and Juncheng Yan and Yi Wei and Jiaxin Li and Li Liu and Yansong Tang and Yueqi Duan and Jiwen Lu},
  doi          = {10.1109/TIP.2025.3567828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3096-3107},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OccNeRF: Advancing 3D occupancy prediction in LiDAR-free environments},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised semantic segmentation via alternate self-dual teaching. <em>TIP</em>, <em>34</em>, 3086-3095. (<a href='https://doi.org/10.1109/TIP.2023.3343112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) is a challenging yet important research field in vision community. In WSSS, the key problem is to generate high-quality pseudo segmentation masks (PSMs). Existing approaches mainly depend on the discriminative object part to generate PSMs, which would inevitably miss object parts or involve surrounding image background, as the learning process is unaware of the full object structure. In fact, both the discriminative object part and the full object structure are critical for deriving of high-quality PSMs. To fully explore these two information cues, we build a novel end-to-end learning framework, alternate self-dual teaching (ASDT), based on a dual-teacher single-student network architecture. The information interaction among different network branches is formulated in the form of knowledge distillation (KD). Unlike the conventional KD, the knowledge of the two teacher models would inevitably be noisy under weak supervision. Inspired by the Pulse Width (PW) modulation, we introduce a PW wave-like selection signal to alleviate the influence of the imperfect knowledge from either teacher model on the KD process. Comprehensive experiments on the PASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of the proposed ASDT framework, and new state-of-the-art results are achieved.},
  archive      = {J_TIP},
  author       = {Dingwen Zhang and Hao Li and Wenyuan Zeng and Chaowei Fang and Lechao Cheng and Ming-Ming Cheng and Junwei Han},
  doi          = {10.1109/TIP.2023.3343112},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3086-3095},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly supervised semantic segmentation via alternate self-dual teaching},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward transparent deep image aesthetics assessment with tag-based content descriptors. <em>TIP</em>, <em>34</em>, 3070-3085. (<a href='https://doi.org/10.1109/TIP.2023.3308852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches for Image Aesthetics Assessment (IAA) have shown promising results in recent years, but the internal mechanisms of these models remain unclear. Previous studies have demonstrated that image aesthetics can be predicted using semantic features, such as pre-trained object classification features. However, these semantic features are learned implicitly, and therefore, previous works have not elucidated what the semantic features are representing. In this work, we aim to create a more transparent deep learning framework for IAA by introducing explainable semantic features. To achieve this, we propose Tag-based Content Descriptors (TCDs), where each value in a TCD describes the relevance of an image to a human-readable tag that refers to a specific type of image content. This allows us to build IAA models from explicit descriptions of image contents. We first propose the explicit matching process to produce TCDs that adopt predefined tags to describe image contents. We show that a simple MLP-based IAA model with TCDs only based on predefined tags can achieve an SRCC of 0.767, which is comparable to most state-of-the-art methods. However, predefined tags may not be sufficient to describe all possible image contents that the model may encounter. Therefore, we further propose the implicit matching process to describe image contents that cannot be described by predefined tags. By integrating components obtained from the implicit matching process into TCDs, the IAA model further achieves an SRCC of 0.817, which significantly outperforms existing IAA methods. Both the explicit matching process and the implicit matching process are realized by the proposed TCD generator. To evaluate the performance of the proposed TCD generator in matching images with predefined tags, we also labeled 5101 images with photography-related tags to form a validation set. And experimental results show that the proposed TCD generator can meaningfully assign photography-related tags to images.},
  archive      = {J_TIP},
  author       = {Jingwen Hou and Weisi Lin and Yuming Fang and Haoning Wu and Chaofeng Chen and Liang Liao and Weide Liu},
  doi          = {10.1109/TIP.2023.3308852},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3070-3085},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward transparent deep image aesthetics assessment with tag-based content descriptors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIPT: Latency-aware image processing transformer. <em>TIP</em>, <em>34</em>, 3056-3069. (<a href='https://doi.org/10.1109/TIP.2025.3567832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer is leading a trend in the field of image processing. While existing lightweight image processing transformers have achieved notable success, they primarily focus on reducing FLOPs (floating-point operations) or the number of parameters, rather than on practical inference acceleration. In this paper, we present a latency-aware image processing transformer, termed LIPT. We devise the low-latency proportion LIPT block that substitutes memory-intensive operators with the combination of self-attention and convolutions to achieve practical speedup. Specifically, we propose a novel non-volatile sparse masking self-attention (NVSM-SA) that utilizes a pre-computing sparse mask to capture contextual information from a larger window with no extra computation overload. Besides, a high-frequency reparameterization module (HRM) is proposed to make LIPT block reparameterization friendly, enhancing the model’s ability to reconstruct fine details. Extensive experiments on multiple image processing tasks (e.g., image super-resolution (SR), JPEG artifact reduction, and image denoising) demonstrate the superiority of LIPT on both latency and PSNR. LIPT achieves real-time GPU inference with state-of-the-art performance on multiple image SR benchmarks. The source codes are released at https://github.com/Lucien66/LIPT},
  archive      = {J_TIP},
  author       = {Junbo Qiao and Wei Li and Haizhen Xie and Hanting Chen and Jie Hu and Shaohui Lin and Jungong Han},
  doi          = {10.1109/TIP.2025.3567832},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3056-3069},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LIPT: Latency-aware image processing transformer},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximately invertible neural network for learned image compression. <em>TIP</em>, <em>34</em>, 3041-3055. (<a href='https://doi.org/10.1109/TIP.2025.3567830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned image compression has attracted considerable interests in recent years. An analysis transform and a synthesis transform, which can be regarded as coupled transforms, are used to encode an image to latent feature and decode the feature after quantization to reconstruct the image. Inspired by the success of invertible neural networks in generative modeling, invertible modules can be used to construct the coupled analysis and synthesis transforms. Considering the noise introduced in the feature quantization invalidates the invertible process, this paper proposes an Approximately Invertible Neural Network (A-INN) framework for learned image compression. It formulates the rate-distortion optimization in lossy image compression when using INN with quantization, which differentiates from using INN for generative modelling. Generally speaking, A-INN can be used as the theoretical foundation for any INN based lossy compression method. Based on this formulation, A-INN with a progressive denoising module (PDM) is developed to effectively reduce the quantization noise in the decoding. Moreover, a Cascaded Feature Recovery Module (CFRM) is designed to learn high-dimensional feature recovery from low-dimensional ones to further reduce the noise in feature channel compression. In addition, a Frequency-enhanced Decomposition and Synthesis Module (FDSM) is developed by explicitly enhancing the high-frequency components in an image to address the loss of high-frequency information inherent in neural network based image compression, thereby enhancing the reconstructed image quality. Extensive experiments demonstrate that the proposed A-INN framework achieves better or comparable compression efficiency than the conventional image compression approach and state-of-the-art learned image compression methods.},
  archive      = {J_TIP},
  author       = {Yanbo Gao and Shuai Li and Meng Fu and Chong Lv and Zhiyuan Yang and Xun Cai and Hui Yuan and Mao Ye},
  doi          = {10.1109/TIP.2025.3567830},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3041-3055},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Approximately invertible neural network for learned image compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoVis: When 3D object detection is like human monocular vision. <em>TIP</em>, <em>34</em>, 3025-3040. (<a href='https://doi.org/10.1109/TIP.2025.3544880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection has garnered significant attention for its outstanding cost effectiveness compared with multi-sensor systems. However, previous work mainly acquires object 3D properties in a heuristic way, with less emphasis on the cues between objects. Inspired by the mechanisms of monocular vision, we propose MoVis, an innovative 3D object detection framework that skillfully combines object hierarchy and color sequence cues. Specifically, a decoupled Spatial Relationship Encoder (SRE) is designed to effectively feed back the high-level encoding results with object hierarchical relationships to low-level features. This method not only effectively reduces the computational overhead of multi-scale coding, but also significantly improves the detection accuracy of occluded objects by incorporating the hierarchical relationship between objects into multi-scale features. Moreover, to obtain more precise object depth information, an Object-level Depth Modulator (ODM) based on the concept of conditional random fields is designed, which employs color sequences. Ultimately, the results of the SRE and ODM are efficiently fused by our Spatial Context Processor (SCP) to accurately perceive the 3D attributes of the objects. Extensive experiments on the KITTI and Rope3D benchmarks show that MoVis achieves state-of-the-art performance. Our MoVis represents a progressive approach that emulates how human monocular vision utilizes monocular cues to perceive 3D scenes.},
  archive      = {J_TIP},
  author       = {Zijie Wang and Jizheng Yi and Aibin Chen and Guangjie Han},
  doi          = {10.1109/TIP.2025.3544880},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3025-3040},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MoVis: When 3D object detection is like human monocular vision},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CmdVIT: A voluntary facial expression recognition model for complex mental disorders. <em>TIP</em>, <em>34</em>, 3013-3024. (<a href='https://doi.org/10.1109/TIP.2025.3567825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) is a critical method for evaluating the emotional states of patients with mental disorders, playing a significant role in treatment monitoring. However, due to privacy constraints, facial expression data from patients with mental disorders is severely limited. Additionally, the more complex inter-class and intra-class similarities compared to healthy individuals make accurate recognition of facial expressions challenging. Therefore, we propose a Voluntary Facial Expression Mimicry (VFEM) experiment, which collected facial expression data from schizophrenia, depression, and anxiety. This experiment establishes the first dataset designed for facial expression recognition tasks exclusively composed of patients with mental disorders. Simultaneously, based on VFEM, we propose a Vision Transformer FER model tailored for Complex mental disorder patients (CmdVIT). CmdVIT integrates crucial facial expression features through both explicit and implicit mechanisms, including explicit visual center positional encoding and implicit sparse attention center loss function. These two key components enhance positional information and minimize the facial feature space distance between conventional attention and critical attention, effectively suppressing inter-class and intra-class similarities. In various FER tasks for different mental disorders in VFEM, CmdVIT achieves more competitive performance compared to contemporary benchmark models. Our works are available at https://github.com/yjy-97/CmdVIT.},
  archive      = {J_TIP},
  author       = {Jiayu Ye and Yanhong Yu and Qingxiang Wang and Guolong Liu and Wentao Li and An Zeng and Yiqun Zhang and Yang Liu and Yunshao Zheng},
  doi          = {10.1109/TIP.2025.3567825},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3013-3024},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CmdVIT: A voluntary facial expression recognition model for complex mental disorders},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-source frequency transform for cross-scene classification of hyperspectral image. <em>TIP</em>, <em>34</em>, 3000-3012. (<a href='https://doi.org/10.1109/TIP.2025.3568749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the research on cross-scene classification of hyperspectral image (HSI) based on domain generalization (DG) has received wider attention. The majority of the existing methods achieve cross-scene classification of HSI via data manipulation that generates more feature-rich samples. The insufficient mining of complex features of HSIs in these methods leads to limiting the effectiveness of the newly generated HSI samples. Therefore, in this paper, we propose a novel single-source frequency transform (SFT), which realizes domain generalization by transforming the frequency features of samples, mainly including frequency transform (FT) and balanced attentional consistency (BAC). Firstly, FT is designed to learn dynamic attention maps in the frequency space of samples filtering frequency components to improve the diversity of features in new samples. Moreover, BAC is designed based on the class activation map to improve the reliability of newly generated samples. Comprehensive experiments on three public HSI datasets demonstrate that the proposed method outperforms the state-of-the-art method, with accuracy at most 5.14% higher than the second place.},
  archive      = {J_TIP},
  author       = {Xizeng Huang and Yanni Dong and Yuxiang Zhang and Bo Du},
  doi          = {10.1109/TIP.2025.3568749},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3000-3012},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single-source frequency transform for cross-scene classification of hyperspectral image},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive multi-granularity graph representation of image via granular-ball computing. <em>TIP</em>, <em>34</em>, 2986-2999. (<a href='https://doi.org/10.1109/TIP.2025.3565212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) encounter challenges in establishing deep structures and managing a large number of parameters effectively to learn node features comprehensively. Consequently, in vision tasks, GNNs often struggle to achieve high classification accuracy compared to convolutional neural networks. Nonetheless, GNNs retain crucial advantages and potential, particularly in lightweight network scale and efficient, reliable decision-making. Thus, improving GNN performance in vision tasks remains a significant research endeavor, with numerous important works exploring the application of GNN models in such contexts, where the graph representation of images poses a key challenge. Existing methods often fall short in adaptively generating blocks of different sizes and their corresponding edges to form graph representations according to graph semantics. To address this issue, we propose a novel method to convert images into graphical forms using granular-ball computing. Our approach does not rely on manual annotation or other learning methods, yet it can dynamically generate block nodes of varying sizes and corresponding edges. Compared to other state-of-the-art methods, our approach better captures semantic information within the graph. Despite having fewer parameters, our method significantly enhances accuracy. Overall, our work holds substantial implications for improving the performance of graph neural networks in vision tasks.},
  archive      = {J_TIP},
  author       = {Dawei Dai and Fan Chen and Shuyin Xia and Long Yang and Guan Wang and Guoyin Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3565212},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2986-2999},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An adaptive multi-granularity graph representation of image via granular-ball computing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal causal representation learning for radiology report generation. <em>TIP</em>, <em>34</em>, 2970-2985. (<a href='https://doi.org/10.1109/TIP.2025.3568746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiology Report Generation (RRG) is essential for computer-aided diagnosis and medication guidance, which can relieve the heavy burden of radiologists by automatically generating the corresponding radiology reports according to the given radiology image. However, generating accurate lesion descriptions remains challenging due to spurious correlations from visual-linguistic biases and inherent limitations of radiological imaging, such as low resolution and noise interference. To address these issues, we propose a two-stage framework named Cross-Modal Causal Representation Learning (CMCRL), consisting of the Radiological Cross-modal Alignment and Reconstruction Enhanced (RadCARE) pre-training and the Visual-Linguistic Causal Intervention (VLCI) fine-tuning. In the pre-training stage, RadCARE introduces a degradation-aware masked image restoration strategy tailored for radiological images, which reconstructs high-resolution patches from low-resolution inputs to mitigate noise and detail loss. Combined with a multiway architecture and four adaptive training strategies (e.g., text postfix generation with degraded images and text prefixes), RadCARE establishes robust cross-modal correlations even with incomplete data. In the VLCI phase, we deploy causal front-door intervention through two modules: the Visual Deconfounding Module (VDM) disentangles local-global features without fine-grained annotations, while the Linguistic Deconfounding Module (LDM) eliminates context bias without external terminology databases. Experiments on IU-Xray and MIMIC-CXR show that our CMCRL pipeline significantly outperforms state-of-the-art methods, with ablation studies confirming the necessity of both stages. Code and models are available at https://github.com/WissingChen/CMCRL.},
  archive      = {J_TIP},
  author       = {Weixing Chen and Yang Liu and Ce Wang and Jiarui Zhu and Guanbin Li and Cheng-Lin Liu and Liang Lin},
  doi          = {10.1109/TIP.2025.3568746},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2970-2985},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal causal representation learning for radiology report generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-level multi-instance distillation for self-supervised fine-grained visual categorization. <em>TIP</em>, <em>34</em>, 2954-2969. (<a href='https://doi.org/10.1109/TIP.2025.3567834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality annotation of fine-grained visual categories demands great expert knowledge, which is taxing and time consuming. Alternatively, learning fine-grained visual representation from enormous unlabeled images (e.g., species, brands) by self-supervised learning becomes a feasible solution. However, recent investigations find that existing self-supervised learning methods are less qualified to represent fine-grained categories. The bottleneck lies in that the pre-trained class-agnostic representation is built from every patch-wise embedding, while fine-grained categories are only determined by several key patches of an image. In this paper, we propose a Cross-level Multi-instance Distillation (CMD) framework to tackle this challenge. Our key idea is to consider the importance of each image patch in determining the fine-grained representation by multiple instance learning. To comprehensively learn the relation between informative patches and fine-grained semantics, the multi-instance knowledge distillation is implemented on both the region/image crop pairs from the teacher and student net, and the region-image crops inside the teacher / student net, which we term as intra-level multi-instance distillation and inter-level multi-instance distillation. Extensive experiments on several commonly used datasets, including CUB-200-2011, Stanford Cars and FGVC Aircraft, demonstrate that the proposed method outperforms the contemporary methods by up to 10.14% and existing state-of-the-art self-supervised learning approaches by up to 19.78% on both top-1 accuracy and Rank-1 retrieval metric. Source code is available at https://github.com/BiQiWHU/CMD},
  archive      = {J_TIP},
  author       = {Qi Bi and Wei Ji and Jingjun Yi and Haolan Zhan and Gui-Song Xia},
  doi          = {10.1109/TIP.2025.3567834},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2954-2969},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-level multi-instance distillation for self-supervised fine-grained visual categorization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPAC: Sampling-based progressive attribute compression for dense point clouds. <em>TIP</em>, <em>34</em>, 2939-2953. (<a href='https://doi.org/10.1109/TIP.2025.3565214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end attribute compression method for dense point clouds. The proposed method combines a frequency sampling module, an adaptive scale feature extraction module with geometry assistance, and a global hyperprior entropy model. The frequency sampling module uses a Hamming window and the Fast Fourier Transform to extract high-frequency components of the point cloud. The difference between the original point cloud and the sampled point cloud is divided into multiple sub-point clouds. These sub-point clouds are then partitioned using an octree, providing a structured input for feature extraction. The feature extraction module integrates adaptive convolutional layers and uses offset-attention to capture both local and global features. Then, a geometry-assisted attribute feature refinement module is used to refine the extracted attribute features. Finally, a global hyperprior model is introduced for entropy encoding. This model propagates hyperprior parameters from the deepest (base) layer to the other layers, further enhancing the encoding efficiency. At the decoder, a mirrored network is used to progressively restore features and reconstruct the color attribute through transposed convolutional layers. The proposed method encodes base layer information at a low bitrate and progressively adds enhancement layer information to improve reconstruction accuracy. Compared to the best anchor of the latest geometry-based point cloud compression (G-PCC) standard that was proposed by the Moving Picture Experts Group (MPEG), the proposed method can achieve an average Bjøntegaard delta bitrate of -24.58% for the Y component (resp. -21.23% for YUV components) on the MPEG Category Solid dataset and -22.48% for the Y component (resp. -17.19% for YUV components) on the MPEG Category Dense dataset. This is the first instance that a learning-based attribute codec outperforms the G-PCC standard on these datasets by following the common test conditions specified by MPEG. Our source code will be made publicly available on https://github.com/sduxlmao/SPAC},
  archive      = {J_TIP},
  author       = {Xiaolong Mao and Hui Yuan and Tian Guo and Shiqi Jiang and Raouf Hamzaoui and Sam Kwong},
  doi          = {10.1109/TIP.2025.3565214},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2939-2953},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SPAC: Sampling-based progressive attribute compression for dense point clouds},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in predictive RAHT for geometric point cloud compression. <em>TIP</em>, <em>34</em>, 2926-2938. (<a href='https://doi.org/10.1109/TIP.2025.3565992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud compression is critical for the success of immersive multimedia applications. For attribute compression in geometric point cloud compression (G-PCC), Region Adaptive Hierarchical Transform (RAHT) is the preferred coding method. This paper presents several advances to predictive coding with RAHT: 1) Sample Domain Prediction: Prediction in RAHT is done in transform domain. This introduces undesirable distortion to the prediction signal because of fixed-point computations and leads to increased decoding complexity. We address this by naturally applying prediction in sample domain. The method opens door to skip the transform stage altogether when all residues are quantized to zero, leading to a significantly light decoder. 2) Reference Node Resampling: Inter-prediction signal derived in RAHT could have a different occupancy and weight distribution compared to the current block, causing a mismatch. To address this, we resample the reference node and align the occupancy and weight distribution. 3) Temporal Filtering: During inter-prediction, the reference node is simply copied as the prediction signal. This assumes a correlation coefficient of unity, which is barely true. We introduce a temporal filtering mechanism conditioned on the sub-band, that emulates a low-pass filtering and achieves improved prediction. 4) Inter-Eligibility: During AC inter-prediction, both encoder and decoder have access to the DC of the current and the reference nodes. We use this information to derive an inter-eligibility criterion. Experimental results show considerable gains and reduced complexity that demonstrate the utility of the proposed methods. All the presented methods have been adopted to the second version of G-PCC.},
  archive      = {J_TIP},
  author       = {Bharath Vishwanath and Kai Zhang and Li Zhang},
  doi          = {10.1109/TIP.2025.3565992},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2926-2938},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advances in predictive RAHT for geometric point cloud compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive face video coding: A generative compression framework. <em>TIP</em>, <em>34</em>, 2910-2925. (<a href='https://doi.org/10.1109/TIP.2025.3563762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework for Interactive Face Video Coding (IFVC), which allows humans to interact with the intrinsic visual representations instead of the signals. The proposed solution enjoys several distinct advantages, including ultra-compact representation, low delay interaction, and vivid expression/headpose animation. In particular, we propose the Internal Dimension Increase (IDI) based representation, greatly enhancing the fidelity and flexibility in rendering the appearance while maintaining reasonable representation cost. By leveraging strong statistical regularities, the visual signals can be effectively projected into controllable semantics in the three dimensional space (e.g., mouth motion, eye blinking, head rotation, head translation and head location), which are compressed and transmitted. The editable bitstream, which naturally supports the interactivity at the semantic level, can synthesize the face frames via the strong inference ability of the deep generative model. Experimental results have demonstrated the performance superiority and application prospects of our proposed IFVC scheme. In particular, the proposed scheme not only outperforms the state-of-the-art video coding standard Versatile Video Coding (VVC) and the latest generative compression schemes in terms of rate-distortion performance for face videos, but also enables the interactive coding without introducing additional manipulation processes. Furthermore, the proposed framework is expected to shed lights on the future design of the digital human communication in the metaverse. The project page can be found at https://github.com/Berlin0610/Interactive_Face_Video_Coding},
  archive      = {J_TIP},
  author       = {Bolin Chen and Zhao Wang and Binzhe Li and Shurun Wang and Shiqi Wang and Yan Ye},
  doi          = {10.1109/TIP.2025.3563762},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2910-2925},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interactive face video coding: A generative compression framework},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayes image restoration with compressive autoencoders. <em>TIP</em>, <em>34</em>, 2896-2909. (<a href='https://doi.org/10.1109/TIP.2025.3564829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play (PnP) methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a second contribution, we introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which performs latent estimation within the framework of variational inference. Thanks to a simple yet efficient parameterization of the variational posterior, VBLE allows for fast and easy (approximate) posterior sampling. Experimental results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar performance as state-of-the-art PnP methods, while being able to quantify uncertainties significantly faster than other existing posterior sampling techniques. The code associated to this paper is available in https://github.com/MaudBqrd/VBLE},
  archive      = {J_TIP},
  author       = {Maud Biquard and Marie Chabert and Florence Genin and Christophe Latry and Thomas Oberlin},
  doi          = {10.1109/TIP.2025.3564829},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2896-2909},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational bayes image restoration with compressive autoencoders},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointFormer: Keypoint-guided transformer for simultaneous nuclei segmentation and classification in multi-tissue histology images. <em>TIP</em>, <em>34</em>, 2883-2895. (<a href='https://doi.org/10.1109/TIP.2025.3565184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic nuclei segmentation and classification (NSC) is a fundamental prerequisite in digital pathology analysis as it enables the quantification of biomarkers and histopathological features for precision medicine. Nuclei appear to be small, however, global spatial distribution and brightness contrast, or color correlation between the nucleus and background, have been recognized as key rationales for accurate nuclei segmentation in actual clinical practice. Although recent great breakthroughs in medical image segmentation have been achieved by Transformer-based methods, the adaptability of segmenting and classifying nuclei from histopathological images is rarely investigated. Also, the severe overlap of nuclei and the large intra-class variability are common in clinical wild data. Prevailing methods based on polygonal representations or distance maps are limited by empirically designed post-processing strategies, resulting in ineffective segmentation of large irregular nuclei instances. To address these challenges, we propose a keypoint-guided tri-decoder Transformer (PointFormer) for NSC simultaneously. Specifically, the overall NSC task is decoupled to a multi-task learning problem, where a tri-decoder structure is employed for decoding nuclei instance, edges, and types, respectively. The nuclei detection and classification (NDC) subtask is reformulated as a semantic keypoint estimation problem. Meanwhile, introduces a novel attention-guiding strategy to capture strong inter-branch correlations and mitigate inconsistencies between multi-decoder predictions. Finally, a multi-local perception module is designed as the base building block of PointFormer to achieve local and global trade-offs and reduce model complexity. Comprehensive quantitative and qualitative experimental results on three datasets of different volumes have demonstrated the superiority of the proposed method over prevalent methods, especially for the PanNuke dataset with an achievement of 70.6% on bPQ.},
  archive      = {J_TIP},
  author       = {Jing Xu and Lei Shi and Shuxi Li and Yameng Zhang and Guohua Zhao and Yucheng Shi and Jie Li and Yufei Gao},
  doi          = {10.1109/TIP.2025.3565184},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2883-2895},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PointFormer: Keypoint-guided transformer for simultaneous nuclei segmentation and classification in multi-tissue histology images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). U-N2C: A dual memory-guided disentanglement framework for unsupervised system matrix denoising in magnetic particle imaging. <em>TIP</em>, <em>34</em>, 2867-2882. (<a href='https://doi.org/10.1109/TIP.2025.3564845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Magnetic Particle Imaging, an emerging functional imaging modality, has exhibited outstanding spatial-temporal resolution and sensitivity. The general reconstruction pipeline of Magnetic Particle Imaging involves calibrating a System Matrix and then solving an ill-posed inverse problem combined with the measured particle signals. However, the introduction of noise during the System Matrix calibration procedure is inevitable, which degrades the detailed information in the reconstructed images. Therefore, frequency selection methods based on signal-to-noise ratio are commonly adopted. However, these methods lead to a decrease in the available high-frequency components, which damages the spatial resolution. To address this problem, we propose an unsupervised memory-guided denoising framework with unpaired noisy-clean System Matrix components, called U-N2C. Specifically, we design a Pattern Memory Block to memorize System Matrix patterns, directed by a position-aware frequency index embedding. Meanwhile, we devise a Noise Memory Block to implicitly approximate noise distributions. With the guidance of our dual memory blocks, we can disentangle the noise and content of the System Matrix in the latent space. Furthermore, benefiting from the ability to model complex noise, our method can generate pseudo but high-quality noisy-clean pairs and further enhance our denoising capability. Experiments on both synthetic and real noise demonstrate that our U-N2C achieves cutting-edge performance compared to other methods. Moreover, we conduct extensive qualitative and quantitative ablation studies to verify the effectiveness of our method. Our code has been available at U-N2C.},
  archive      = {J_TIP},
  author       = {Wenxuan Zou and Gen Shi and Siao Lei and Guanghui Li and Guangxing Zhou and Yang Jing and Jie He and Zhenchao Tang and Yu An and Jie Tian},
  doi          = {10.1109/TIP.2025.3564845},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2867-2882},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {U-N2C: A dual memory-guided disentanglement framework for unsupervised system matrix denoising in magnetic particle imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit motion handling and interactive prompting for video camouflaged object detection. <em>TIP</em>, <em>34</em>, 2853-2866. (<a href='https://doi.org/10.1109/TIP.2025.3565879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflage poses notable challenges in distinguishing a static target, as it usually blends seamlessly with the background. However, any movement by the target can disrupt this disguise, making it detectable. Existing video camouflaged object detection (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive prompting way that is inspired by emerging visual prompt learning. Two learnable modules, i.e. the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation prompts, respectively, and enhance outputs of the both streams. The prompt fed to the motion stream is learned by supervising optical flow in a self-supervised manner. Furthermore, we show that long-term historical information can also be incorporated as a prompt into EMIP and achieve more robust results with temporal consistency. By leveraging promoting techniques based on EMIP, the proposed long-term model EMIP ${}^{\dagger }$ incurs lower training cost with only 8.5M trainable parameters (less than 8% of the total model parameters). Experimental results demonstrate that both EMIP and EMIP ${}^{\dagger }$ set new state-of-the-art records on popular VCOD benchmarks. Additionally, comparative evaluations against other video segmentation models on a wider range of video segmentation tasks demonstrate the robustness and superior generalization capabilities of EMIP. Our code is made publicly available at https://github.com/zhangxin06/EMIP},
  archive      = {J_TIP},
  author       = {Xin Zhang and Tao Xiao and Ge-Peng Ji and Xuan Wu and Keren Fu and Qijun Zhao},
  doi          = {10.1109/TIP.2025.3565879},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2853-2866},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Explicit motion handling and interactive prompting for video camouflaged object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free cross-modal knowledge transfer by unleashing the potential of task-irrelevant data. <em>TIP</em>, <em>34</em>, 2840-2852. (<a href='https://doi.org/10.1109/TIP.2025.3561670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns. A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap. However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality. To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer. Our work is buttressed by two key technical components. Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module. It translates the target modality data into the source-like images based on paired TI data and the guidance of the available source model to alleviate two key gaps: 1) inter-modality gap between the paired TI data; 2) intra-modality gap between TI and TR target data. We then propose a Task-irrelevant data-Guided Knowledge Transfer (TGKT) module that transfers knowledge from the source model to the target model by leveraging the paired TI data. Notably, due to the unavailability of labels for the TR target data and its less reliable prediction from the source model, our TGKT model incorporates a self-supervised pseudo-labeling approach to enable the target model to learn from its predictions. Extensive experiments show that our method achieves state-of-the-art performance on three datasets (RGB-to-depth and RGB-to-infrared).},
  archive      = {J_TIP},
  author       = {Jinjing Zhu and Yucheng Chen and Lin Wang},
  doi          = {10.1109/TIP.2025.3561670},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2840-2852},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Source-free cross-modal knowledge transfer by unleashing the potential of task-irrelevant data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamical threshold-based fractional anisotropic diffusion for speckle noise removal. <em>TIP</em>, <em>34</em>, 2826-2839. (<a href='https://doi.org/10.1109/TIP.2025.3561685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of effective methods for removing image speckle remains a significant challenge in image processing. In contrast to additive noise, speckle noise is a multiplicative noise whose intensity is proportional to the signal. This results in a noise distribution that exhibits a high dependence on the signal intensity throughout the image, rendering it difficult to remove. Therefore, we present a novel approach to speckle noise removal using dynamical threshold–based fractional anisotropic diffusion (named as DTFAD) in this study. The method simultaneously considers both gradient and gray scale information in the image. In addition, the fractional derivative is integrated with anisotropic diffusion in the DTFAD model, which enhances the image denoising effect to preserve the fundamental features and edges of the image. The design of a dynamic threshold function in the diffusion coefficient enables the diffusion pattern and intensity to adaptively change according to image information, thus effectively removing speckle noise. We establish the well–posedness of the DTFAD model and implement it using an explicit finite difference scheme. Extensive experiments demonstrate that the DTFAD model outperforms traditional anisotropic diffusion techniques, and achieves a superior balance between denoising performance and texture preservation. This evidence demonstrates that the DTFAD model has the potential to be applied in practical engineering.},
  archive      = {J_TIP},
  author       = {Jiali Wei and Xiaofeng Liao},
  doi          = {10.1109/TIP.2025.3561685},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2826-2839},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamical threshold-based fractional anisotropic diffusion for speckle noise removal},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ray-aided quadruple affiliation network for calculating tumor-stroma ratios in breast cancers. <em>TIP</em>, <em>34</em>, 2811-2825. (<a href='https://doi.org/10.1109/TIP.2025.3561679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tumor-stroma ratio (TSR), which is the area ratio between two components within tumor beds, namely tumor cells and tumor stroma, has been suggested as a promising prognostic feature in breast cancers. However, due to imperfect datasets, and the similarity between tumor stroma and non-tumor stroma, previous algorithms struggle to delineate tumor beds, especially those of histomorphologies with a fibrotic focus. To overcome these limitations, we propose a novel ray-aided quadruple affiliation network (RQA-Net) for calculating TSRs in breast cancers. RQA-Net uses quadruple branches to segment tumor cells and tumor beds simultaneously, where a crisscross task subtraction module (CTS-Module) is designed to locate tumor stroma, grounded on its affiliation relationships with tumor beds. Moreover, we propose an affiliation loss (Aff-Loss) to force identified tumor beds to incorporate tumor cells to enhance their affiliation relationships. Furthermore, we propose a ray-based hypothesis testing (RH-Testing) to obtain line segments from ray equations in tumor beds that can decorate identified tumor beds by overlapping. In summary, RQA-Net precisely predicts tumor cells and tumor beds, and thus supports the calculation of TSRs. We also create a cancerous dataset (CrD-Set) containing 100 slides with an average resolution of $50,000\times 50,000$ pixels from real breast cancer cases, which is the first dataset with pixel-wise tumor bed annotations. Experimental results on existing datasets and CrD-Set demonstrate that compared with previous methods, RQA-Net better calculates breast cancer TSRs by precisely identifying tumor cells and tumor beds. The created CrD-Set and codes in this work will be available online at https://github.com/Kunpingyang1992/Breast-Cancer-TSR-Calculation},
  archive      = {J_TIP},
  author       = {Kunping Yang and Linying Chen and Xi Zheng and Xuanping Li and Junhui Lan and Yi Wu and Julia Y. S. Tsang and Gary M. Tse},
  doi          = {10.1109/TIP.2025.3561679},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2811-2825},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ray-aided quadruple affiliation network for calculating tumor-stroma ratios in breast cancers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TwinsTNet: Broad-view twins transformer network for bi-modal salient object detection. <em>TIP</em>, <em>34</em>, 2796-2810. (<a href='https://doi.org/10.1109/TIP.2025.3564821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring complementary information between RGB and thermal/depth modalities is crucial for bi-modal salient object detection (BSOD). However, the distinct characteristics of different modalities often lead to large differences in information distributions. Existing models, which rely on convolutional operations or plug-and-play attention mechanisms, struggle to address this issue. To overcome this challenge, we rethink the relationship between information complementarity and long-range relevance, and propose a uniform broad-view Twins Transformer Network (TwinsTNet) for accurate BSOD. Specifically, to efficiently fuse bi-modal information, we first design the Cross-Modal Federated Attention (CMFA), which mines complementary cues across modalities through element-wise global dependency. Second, to ensure accurate modality fusion, we propose the Semantic Consistency Attention Loss, which supervises the co-attention feature in CMFA using the ground-truth-generated attention map. Additionally, existing BSOD models lack the exploration of inter-layer interactions, for which we propose the Cross-Scale Retracing Attention (CSRA), which retrieves query-relevant information from stacked features of all previous layers, enabling flexible cross-layer interactions. The cooperation between CMFA and CSRA mitigates inductive bias in both modality and layer dimensions, enhancing TwinsTNet’s representational capability. Extensive experiments demonstrate that TwinsTNet outperforms twenty-two existing state-of-the-art models on ten BSOD benchmark datasets. The code is available at: https://github.com/JoshuaLPF/TwinsTNet.},
  archive      = {J_TIP},
  author       = {Pengfei Lyu and Xiaosheng Yu and Jianning Chi and Hao Wu and Chengdong Wu and Jagath C. Rajapakse},
  doi          = {10.1109/TIP.2025.3564821},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2796-2810},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TwinsTNet: Broad-view twins transformer network for bi-modal salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PreCM: The padding-based rotation equivariant convolution mode for semantic segmentation. <em>TIP</em>, <em>34</em>, 2781-2795. (<a href='https://doi.org/10.1109/TIP.2025.3558425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various convolutional neural networks have been proposed for pixel-level classification and segmentation tasks. In practical scenarios, however, imaging angles are often arbitrary, encompassing instances such as water body images from remote sensing and capillary and polyp images in the medical domain, where prior orientation information is typically unavailable to guide these networks to extract more effective features. In this case, learning features from objects with diverse orientation information poses a significant challenge, as the majority of CNN-based semantic segmentation networks lack rotation equivariance to resist the disturbance from orientation information. To address this challenge, this paper first constructs a universal convolution-group framework aimed at more fully utilizing orientation information and equipping the network with rotation equivariance. Subsequently, we mathematically design a padding-based rotation equivariant convolution mode (PreCM), which is not only applicable to multi-scale images and convolutional kernels but can also serve as a replacement component for various types of convolutions, such as dilated convolutions, transposed convolutions, and asymmetric convolution. To quantitatively assess the impact of image rotation in semantic segmentation tasks, we also propose a new evaluation metric, Rotation Difference (RD). The replacement experiments related to six existing semantic segmentation networks on three datasets (i.e., Satellite Images of Water Bodies, DRIVE, and Floodnet) show that, the average Intersection Over Union (IOU) of their PreCM-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%, 8.33% compared to their original versions in terms of random angle rotation. And the average RD values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%, 3.43% respectively. The code can be download from https://github.com/XinyuXu414},
  archive      = {J_TIP},
  author       = {Xinyu Xu and Huazhen Liu and Tao Zhang and Huilin Xiong and Wenxian Yu},
  doi          = {10.1109/TIP.2025.3558425},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2781-2795},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PreCM: The padding-based rotation equivariant convolution mode for semantic segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCSS: 3D keypoint detection for point clouds using structural saliency. <em>TIP</em>, <em>34</em>, 2765-2780. (<a href='https://doi.org/10.1109/TIP.2025.3565380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D keypoint detection is of great interest to researchers in computer vision and graphics because it is an integral part of realizing many tasks, such as object tracking, 3D reconstruction, and shape registration. However, it is challenging to detect 3D keypoints quickly and stably due to the ambiguity of the keypoints and the presence of noise, density changes, and geometric distortions in the 3D point cloud. This paper proposes a novel 3D keypoint detection method based on point cloud structural saliency (PCSS) to realize stable and efficient 3D keypoint detection. First, we propose an effective point cloud feature descriptor called local spatial geometric feature, which can effectively combine spatial and geometric information to improve feature distinguishability. Second, we define a point cloud structural saliency representation that effectively characterizes the structured information in the point cloud. Finally, we generate 3D keypoints based on point cloud structural saliency using a non-maximum suppression method. We evaluate our method on five 3D keypoint benchmark datasets, and the experimental results demonstrate that it achieves state-of-the-art performance in 3D keypoint detection. Comparing it with previous keypoint detection methods further demonstrates the effectiveness and superiority of our method.},
  archive      = {J_TIP},
  author       = {Chengzhuan Yang and Xin Zhao and Xiaohan Li and Qian Yu and Hui Wei and Yunliang Jiang and Zhonglong Zheng},
  doi          = {10.1109/TIP.2025.3565380},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2765-2780},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PCSS: 3D keypoint detection for point clouds using structural saliency},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-aware crowd counting network with annotation error modeling. <em>TIP</em>, <em>34</em>, 2750-2764. (<a href='https://doi.org/10.1109/TIP.2025.3555116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional crowd-counting networks suffer from information loss when feature maps are reduced by pooling layers, leading to inaccuracies in counting crowds at a distance. Existing methods often assume correct annotations during training, disregarding the impact of noisy annotations, especially in crowded scenes. Furthermore, using a fixed Gaussian density model does not account for the varying pixel distribution of the camera distance. To overcome these challenges, we propose a Scale-Aware Crowd Counting Network (SACC-Net) that introduces a scale-aware loss function with error-compensation capabilities of noisy annotations. For the first time, we simultaneously model labeling errors (mean) and scale variations (variance) by spatially varying Gaussian distributions to produce fine-grained density maps for crowd counting. Furthermore, the proposed scale-aware Gaussian density model can be dynamically approximated with a low-rank approximation, leading to improved convergence efficiency with comparable accuracy. To create a smoother scale-aware feature space, this paper proposes a novel Synthetic Fusion Module (SFM) and an Intra-block Fusion Module (IFM) to generate fine-grained heat maps for better crowd counting. The lightweight version of our model, named SACC-LW, enhances the computational efficiency while retaining accuracy. The superiority and generalization properties of scale-aware loss function are extensively evaluated for different backbone architectures and performance metrics on six public datasets: UCF-QNRF, UCF CC 50, NWPU, ShanghaiTech A, ShanghaiTech B, and JHU. Experimental results also demonstrate that SACC-Net outperforms all state-of-the-art methods, validating its effectiveness in achieving superior crowd-counting accuracy. The source code is available at https://github.com/Naughty725.},
  archive      = {J_TIP},
  author       = {Yi-Kuan Hsieh and Jun-Wei Hsieh and Xin Li and Yu-Ming Zhang and Yu-Chee Tseng and Ming-Ching Chang},
  doi          = {10.1109/TIP.2025.3555116},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2750-2764},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scale-aware crowd counting network with annotation error modeling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal hashing via diverse instances matching. <em>TIP</em>, <em>34</em>, 2737-2749. (<a href='https://doi.org/10.1109/TIP.2025.3561659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing is a highly effective technique for searching relevant data across different modalities, owing to its low storage costs and fast similarity retrieval capability. While significant progress has been achieved in this area, prior investigations predominantly concentrate on a one-to-one feature alignment approach, where a singular feature is derived for similarity retrieval. However, the singular feature in these methods fails to adequately capture the varied multi-instance information inherent in the original data across disparate modalities. Consequently, the conventional one-to-one methodology is plagued by a semantic mismatch issue, as the rigid one-to-one alignment inhibits effective multi-instance matching. To address this issue, we propose a novel Diverse Instances Matching for Cross-modal Hashing (DIMCH), which explores the relevance between multiple instances in different modalities using a multi-instance learning algorithm. Specifically, we design a novel diverse instances learning module to extract a multi-feature set, which enables our model to capture detailed multi-instance semantics. To evaluate the similarity between two multi-feature sets, we adopt the smooth chamfer distance function, which enables our model to incorporate the conventional similarity retrieval structure. Moreover, to sufficiently exploit the supervised information from the semantic label, we adopt the weight cosine triplet loss as the objective function, which incorporates the multilevel similarity among the multi-labels into the training procedure and enables the model to mine the multi-label correlation effectively. Extensive experiments demonstrate that our diverse hashing embedding method achieves state-of-the-art performance in supervised cross-modal hashing retrieval tasks.},
  archive      = {J_TIP},
  author       = {Junfeng Tu and Xueliang Liu and Zhen Huang and Yanbin Hao and Richang Hong and Meng Wang},
  doi          = {10.1109/TIP.2025.3561659},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2737-2749},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal hashing via diverse instances matching},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergence model of perception with global-contour precedence based on gestalt theory and primary visual cortex. <em>TIP</em>, <em>34</em>, 2721-2736. (<a href='https://doi.org/10.1109/TIP.2025.3562054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perceptual edge grouping is a technique for organizing the cluttered edge pixels into meaningful structures and further serves high-level vision tasks, which has long been a basic and critical task in computer vision. Existing methods usually have a poor performance when coping with the junctions caused by occlusion and noise in natural images. In this paper, we present GPGrouper, a perceptual edge grouping model based on gestalt theory and the primary visual cortex (V1). Different from the existing methods, GPGrouper leverages the edge representation and grouping matrix (ERGM), a functional structure inspired by V1 mechanisms, to represent edges in a way that can effectively reduce grouping errors caused by occlusion between objects. ERGM is trained with natural image contours and further provides a priori guidance for the construction of the edge connection graph (ECG) that is useful to minimize the impact of noise on grouping. In the experiment, we compared GPGrouper and the state-of-the-art (SOTA) method of perceptual grouping on the visual psychology pathfinder challenge. The results demonstrate that GPGrouper outperforms the SOTA method in grouping performance. Furthermore, in the grouping experiments involving line segments with varying lengths detected by the Line Segment Detector (LSD), as well as those involving superpixel segmentation results with significant levels of interfering noise using the SLIC algorithm, GPGrouper was superior to the existing methods in terms of grouping effect and robustness. Moreover, the results of applying the grouping results to the vision tasks objectness demonstrate that GPGrouper can contribute significantly to high-level visual tasks.},
  archive      = {J_TIP},
  author       = {Jingmeng Li and Hui Wei},
  doi          = {10.1109/TIP.2025.3562054},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2721-2736},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Emergence model of perception with global-contour precedence based on gestalt theory and primary visual cortex},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detector with classifier2: An end-to-end multi-stream feature aggregation network for fine-grained object detection in remote sensing images. <em>TIP</em>, <em>34</em>, 2707-2720. (<a href='https://doi.org/10.1109/TIP.2025.3563708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained object detection (FGOD) fundamentally comprises two primary tasks: object detection and fine-grained classification. In natural scenes, most FGOD methods benefit from higher instance resolution and fewer environmental variation, attributing more commonly associated with the latter task. In this paper, we propose a unified paradigm named Detector with Classifier2 (DC2), which provides a holistic paradigm by explicitly considering the end-to-end integration of object detection and fine-grained classification tasks, rather than prioritizing one aspect. Initially, our detection sub-network is restricted to only determining whether the proposal is a coarse-category and does not delve into the specific sub-categories. Moreover, in order to reduce redundant pixel-level calculation, we propose an instance-level feature enhancement (IFE) module to model the semantic similarities among proposals, which poses great potential for locating more instances in remote sensing images (RSIs). After obtaining the coarse detection predictions, we further construct a classification sub-network, which is built on top of the former branch to determine the specific sub-categories of the aforementioned predictions. Importantly, the detection network is performed on the complete image, while the classification network conducts secondary modeling for the detected regions. These operations can be denoted as the global contextual information and local intrinsic cues extractions for each instance. Therefore, we propose a multi-stream feature aggregation (MSFA) module to integrate global-stream semantic information and local-stream discriminative cues. Our whole DC2 network follows an end-to-end learning fashion, which effectively excavates the internal correlation between detection and fine-grained classification networks. We evaluate the performance of our DC2 network on two benchmarks SAT-MTB and HRSC2016 datasets. Importantly, our method achieves the new state-of-the-art results compared with recent works (approximately 7% mAP gains on SAT-MTB) and improves baseline by a significant margin (43.2% $v.s.~36.7$ %) without any complicated post-processing strategies. Source codes of the proposed methods are available at https://github.com/zhengshangdong/DC2},
  archive      = {J_TIP},
  author       = {Shangdong Zheng and Zebin Wu and Yang Xu and Chengxun He and Zhihui Wei},
  doi          = {10.1109/TIP.2025.3563708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2707-2720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Detector with classifier2: An end-to-end multi-stream feature aggregation network for fine-grained object detection in remote sensing images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive invariant causal feature learning for single domain generalization. <em>TIP</em>, <em>34</em>, 2694-2706. (<a href='https://doi.org/10.1109/TIP.2025.3563772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single domain generalization (SDG) aims to transfer models trained on a single source domain to multiple unseen target domains while against the unknown domain shifts. The main challenge lies in learning the domain-invariant features to mitigate the domain shift impact. To address this challenge, we reconsider SDG from a causal perspective to capture the domain-invariant features accurately. Specifically, we present a Progressive Invariant Causal Feature Learning (PICF) method that leverages front-door adjustment to gradually obtain the invariant causal features for SDG. First, we introduce a foreground feature filter, which removes object-irrelevant confounders in a cyclical manner to extract the object-related causal features. Subsequently, to further enhance the causal feature invariance, we propose to train with augmented causal features by combining them with randomly-sampled styles from the object-irrelevant feature distribution boundary. As a result, our model bridges the gap between one seen domain and multiple unseen ones by capturing the invariant causal features, which largely enhances the model’s generalization ability in SDG. In experiments, our method can be plugged into multiple state-of-the-art methods, and the significant performance improvements on multiple datasets demonstrate the superiority of our method. In particular, on the PACS dataset, our method achieves an accuracy improvement of 4.7%.},
  archive      = {J_TIP},
  author       = {Yuxuan Wang and Muli Yang and Aming Wu and Cheng Deng},
  doi          = {10.1109/TIP.2025.3563772},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2694-2706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive invariant causal feature learning for single domain generalization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to see low-light images via feature domain adaptation. <em>TIP</em>, <em>34</em>, 2680-2693. (<a href='https://doi.org/10.1109/TIP.2025.3563775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw low-light image enhancement (LLIE) has achieved much better performance than the sRGB domain enhancement methods due to the merits of raw data. However, the ambiguity between noisy to clean and raw to sRGB mappings may mislead the single-stage enhancement networks. The two-stage networks avoid ambiguity by step-by-step or decoupling the two mappings but usually have large computing complexity. To solve this problem, we propose a single-stage network empowered by Feature Domain Adaptation (FDA) to decouple the denoising and color mapping tasks in raw LLIE. The denoising encoder is supervised by the clean raw image, and then the denoised features are adapted for the color mapping task by an FDA module. We propose a Lineformer to serve as the FDA, which can well explore the global and local correlations with fewer line buffers (friendly to the line-based imaging process). During inference, the raw supervision branch is removed. In this way, our network combines the advantage of a two-stage enhancement process with the efficiency of single-stage inference. Experiments on four benchmark datasets demonstrate that our method achieves state-of-the-art performance with fewer computing costs (60% FLOPs of the two-stage method DNF). Our codes will be released after the acceptance of this work.},
  archive      = {J_TIP},
  author       = {Qirui Yang and Qihua Cheng and Huanjing Yue and Le Zhang and Yihao Liu and Jingyu Yang},
  doi          = {10.1109/TIP.2025.3563775},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2680-2693},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to see low-light images via feature domain adaptation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-level modality de-biasing for RGB-T tracking. <em>TIP</em>, <em>34</em>, 2667-2679. (<a href='https://doi.org/10.1109/TIP.2025.3562077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracking aims to effectively leverage the complement ability of visual (RGB) and infrared (TIR) modalities to achieve robust tracking performance in various scenarios. Existing RGB-T tracking methods typically adopt backbone networks pre-trained on large-scale RGB datasets, which can lead to a predisposition toward RGB image patterns. RGB and TIR modalities also exhibit inconsistent responses to regions with diverse properties, resulting in imbalances in tracking decisions. We refer to these issues as feature-level and decision-level biases in the TIR modality. In this paper, we propose a novel dual-level modality de-biasing framework for RGB-T tracking to eliminate the inherent feature and decision-level biases. Specifically, we propose a joint infrared-fusion adapter, comprising an infrared-aware adapter and a cross-fusion adapter, designed to adaptively mitigate feature-level biases and utilize complementary information between the two modalities. In addition to implicit feature-level adjustment, we propose a response-decoupled distillation strategy to explicitly alleviate decision-level biases, aiming to achieve consistently accurate decision-making between the RGB and TIR modalities. Extensive experiments on several popular RGB-T tracking benchmarks validate the effectiveness of our proposed method.},
  archive      = {J_TIP},
  author       = {Yufan Hu and Zekai Shao and Bin Fan and Hongmin Liu},
  doi          = {10.1109/TIP.2025.3562077},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2667-2679},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-level modality de-biasing for RGB-T tracking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Session-guided attention in continuous learning with few samples. <em>TIP</em>, <em>34</em>, 2654-2666. (<a href='https://doi.org/10.1109/TIP.2025.3559463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class-incremental learning (FSCIL) aims to learn from a sequence of incremental data sessions with a limited number of samples in each class. The main issues it encounters are the risk of forgetting previously learned data when introducing new data classes, as well as not being able to adapt the old model to new data due to limited training samples. Existing state-of-the-art solutions normally utilize pre-trained models with fixed backbone parameters to avoid forgetting old knowledge. While this strategy preserves previously learned features, the fixed nature of the backbone limits the model’s ability to learn optimal representations for unseen classes, which compromises performance on new class increments. In this paper, we propose a novel SEssion-Guided Attention framework (SEGA) to tackle this challenge. SEGA exploits the class relationships within each incremental session by assessing how test samples relate to class prototypes. This allows accurate incremental session identification for test data, leading to more precise classifications. In addition, an attention module is introduced for each incremental session to further utilize the feature from the fixed backbone. As the session of the testing image is determined, we can fine-tune the feature with the corresponding attention module to better cluster the sample within the selected session. Our approach adopts the fixed backbone strategy to avoid forgetting the old knowledge while achieving novel data adaptation. Experimental results on three FSCIL datasets consistently demonstrate the superior adaptability of the proposed SEGA framework in FSCIL tasks. The code is available at: https://github.com/zichengpan/SEGA.},
  archive      = {J_TIP},
  author       = {Zicheng Pan and Xiaohan Yu and Yongsheng Gao},
  doi          = {10.1109/TIP.2025.3559463},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2654-2666},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Session-guided attention in continuous learning with few samples},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Webly supervised fine-grained classification by integrally tackling noises and subtle differences. <em>TIP</em>, <em>34</em>, 2641-2653. (<a href='https://doi.org/10.1109/TIP.2025.3562740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Webly-supervised fine-grained visual classification (WSL-FGVC) aims to learn similar sub-classes from cheap web images, which suffers from two major issues: label noises in web images and subtle differences among fine-grained classes. However, existing methods for WSL-FGVC only focus on suppressing noise at image-level, but neglect to mine cues at pixel-level to distinguish the subtle differences among fine-grained classes. In this paper, we propose a bag-level top-down attention framework, which could tackle label noises and mine subtle cues simultaneously and integrally. Specifically, our method first extracts high-level semantic information from a bag of images belonging to the same class, and then uses the bag-level information to mine discriminative regions in various scales of each image. Besides, we propose to derive attention weights from attention maps to weight the bag-level fusion for a robust supervision. We also propose an attention loss on self-bag attention and cross-bag attention to facilitate the learning of valid attention. Extensive experiments on four WSL-FGVC datasets, i.e., Web-Aircraft, Web-Bird, Web-Car, and WebiNat-5089, demonstrate the effectiveness of our method against the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Junjie Chen and Jiebin Yan and Yuming Fang and Li Niu},
  doi          = {10.1109/TIP.2025.3562740},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2641-2653},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Webly supervised fine-grained classification by integrally tackling noises and subtle differences},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-free deep multi-modal clustering with reliable contrastive learning. <em>TIP</em>, <em>34</em>, 2628-2640. (<a href='https://doi.org/10.1109/TIP.2025.3562083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep multi-modal clustering (DMC) expects to improve clustering performance by exploiting abundant information available from multiple modalities. However, different modalities usually have heterogeneous distribution with uneven quality. This may lead to limited performance, especially for contrastive multi-modal clustering, which inevitably performs contrastive learning between high-quality and low-quality modalities. To tackle this challenge, we propose a novel framework named parameter-free deep multi-modal clustering with reliable contrastive learning (PDMC-RCL). Specifically, the reliable contrastive learning quantifies the relationship between contrastive modality pairs with weight values that will promote the discriminative features learning from useful modality pairs and slow down or even prevent the learning from unreliable modality pairs. Moreover, the reliable contrastive learning is imposed simultaneously at both the feature-level and cluster-level in this framework so that the feature representation learning can benefit from multi-level contrastive learning. It is worth noting that our PDMC-RCL method is parameter-free, which can achieve promising performance without additional hyperparameter tuning. Experimental results on various datasets show the effectiveness of our method over typical state-of-the-art compared DMCs. The source code is available on https://github.com/ShizheHu},
  archive      = {J_TIP},
  author       = {Zhengzheng Lou and Hang Xue and Yanzheng Wang and Chaoyang Zhang and Xin Yang and Shizhe Hu},
  doi          = {10.1109/TIP.2025.3562083},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2628-2640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parameter-free deep multi-modal clustering with reliable contrastive learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency-queried transformer for audio-visual segmentation. <em>TIP</em>, <em>34</em>, 2616-2627. (<a href='https://doi.org/10.1109/TIP.2025.3563076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual segmentation (AVS) aims to segment objects in audio-visual content. The effective interaction between audio and visual features has garnered significant attention from the multimodal domain. Despite significant advancements, most existing AVS methods are hampered by multimodal inconsistencies. These inconsistencies primarily manifest as a mismatch between audio and visual information guided by audio cues, wherein visual features often dominate audio modality. To address this issue, we propose the Consistency-Queried Transformer (CQFormer), a novel framework for AVS tasks that leverages the transformer architecture. This framework features a Consistency Query Generator (CQG) and a Query-Aligned Matching (QAM) module. The Noise Contrastive Estimation (NCE) loss function enhances modality matching and consistency by minimizing the distributional differences between audio and visual features, facilitating effective fusion and interaction between these features. Additionally, introducing the consistency query during the decoding stage enhances consistency constraints and object-level semantic information, further improving the accuracy and stability of audio-visual segmentation. Extensive experiments on the popular benchmark of the audio-visual segmentation dataset demonstrate that the proposed CQFormer achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ying Lv and Zhi Liu and Xiaojun Chang},
  doi          = {10.1109/TIP.2025.3563076},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2616-2627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistency-queried transformer for audio-visual segmentation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled noisy correspondence learning. <em>TIP</em>, <em>34</em>, 2602-2615. (<a href='https://doi.org/10.1109/TIP.2025.3559457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval is crucial in understanding latent correspondences across modalities. However, existing methods implicitly assume well-matched training data, which is impractical as real-world data inevitably involves imperfect alignments, i.e., noisy correspondences. Although some works explore similarity-based strategies to address such noise, they suffer from sub-optimal similarity predictions influenced by modality-exclusive information (MEI), e.g., background noise in images and abstract definitions in texts. This issue arises as MEI is not shared across modalities, thus aligning it in training can markedly mislead similarity predictions. Moreover, although intuitive, directly applying previous cross-modal disentanglement methods suffers from limited noise tolerance and disentanglement efficacy. Inspired by the robustness of information bottlenecks against noise, we introduce DisNCL, a novel information-theoretic framework for feature Disentanglement in Noisy Correspondence Learning, to adaptively balance the extraction of modality-invariant information (MII) and MEI with certifiable optimal cross-modal disentanglement efficacy. DisNCL then enhances similarity predictions in modality-invariant subspace, thereby greatly boosting similarity-based alleviation strategy for noisy correspondences. Furthermore, DisNCL introduces soft matching targets to model noisy many-to-many relationships inherent in multi-modal inputs for noise-robust and accurate cross-modal alignment. Extensive experiments confirm DisNCL’s efficacy by 2% average recall improvement. Mutual information estimation and visualization results show that DisNCL learns meaningful MII/MEI subspaces, validating our theoretical analyses.},
  archive      = {J_TIP},
  author       = {Zhuohang Dang and Minnan Luo and Jihong Wang and Chengyou Jia and Haochen Han and Herun Wan and Guang Dai and Xiaojun Chang and Jingdong Wang},
  doi          = {10.1109/TIP.2025.3559457},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2602-2615},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Disentangled noisy correspondence learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating the space of reflectance spectra. <em>TIP</em>, <em>34</em>, 2588-2601. (<a href='https://doi.org/10.1109/TIP.2025.3558443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color imaging algorithms - such as color correction, spectral estimation and color constancy - are developed and validated with spectral reflectance data. However, the choice of the reflectance data set - used in development and tuning - not only affects the results of these algorithms but it also changes the ranking of the different approaches. We propose that this fragility is because it is difficult to measure/sample enough data to statistically represent the large number of degrees of freedom apparent in spectral reflectances. In this paper, we propose that the space of reflectance data should not be sampled but, rather, integrated. Specifically, we advocate that the convex closure of a reflectance data set - all convex combinations of all spectra - should be used instead of discrete reflectance samples. To make the integration computation tractable, we approximate these convex closures by their enclosing hyper-cube in a privileged coordinate system. We use color correction as an exemplar color imaging problem to demonstrate the utility of our approach.},
  archive      = {J_TIP},
  author       = {Graham D. Finlayson and Javier Vazquez-Corral and Fufu Fang},
  doi          = {10.1109/TIP.2025.3558443},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2588-2601},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Integrating the space of reflectance spectra},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeRF-det++: Incorporating semantic cues and perspective-aware depth supervision for indoor multi-view 3D detection. <em>TIP</em>, <em>34</em>, 2575-2587. (<a href='https://doi.org/10.1109/TIP.2025.3560240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-Aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3) Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP $\text{@}0.25$ and +3.5% in mAP $\text{@}0.50$ . The code will be publicly available at https://github.com/mrsempress/NeRF-Detplusplus},
  archive      = {J_TIP},
  author       = {Chenxi Huang and Yuenan Hou and Weicai Ye and Di Huang and Xiaoshui Huang and Binbin Lin and Deng Cai},
  doi          = {10.1109/TIP.2025.3560240},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2575-2587},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NeRF-det++: Incorporating semantic cues and perspective-aware depth supervision for indoor multi-view 3D detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel computational photography for soft-focus effect in automatic post production. <em>TIP</em>, <em>34</em>, 2560-2574. (<a href='https://doi.org/10.1109/TIP.2025.3562071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The well-known soft-focus effect, which relies on either special optical filters or manual post-production techniques, has been intriguing and powerful in photography for quite a while. Nonetheless, how to impose the soft-focus effect automatically simply using sophisticated image-processing (computational photography) algorithms has never been addressed in the literature to the best of our knowledge. In this work, we would like to make the first-ever attempt to design an automatic, optical-filter-free approach to create the appropriate soft-focus effects desired by individual users. Our approach is first to investigate the physical optical filter, namely Kenko Black Mist No. 5, and estimate the corresponding kernel matrix (i.e., the system impulse response matrix) using our proposed novel irradiance-domain kernel-matrix estimation framework. Furthermore, we demonstrate that it is not feasible to find a kernel matrix that precisely characterizes the soft-focus effect by just using a pixel-value-domain image (a regular photo) in post production. To combat the aforementioned problem, we establish a novel pixel-value-to-pseudo-irradiance map such that the pseudo irradiance-domain image can be obtained directly from any pixel-value-domain image. Finally the soft-focus effect can be created from the two-dimensional convolution between the pseudo irradiance-domain image and the estimated kernel. To evaluate our proposed automatic scheme for soft-focus effect, we compare the results from our proposed new scheme and the physical optical filter in terms of the DCT-KLD (Kullback-Leibler divergence of discrete cosine transform) and the conventional PSNR (peak-signal-to-noise ratio). Experiments show that our proposed new scheme can achieve very small DCT-KLDs and very large PSNRs over the ground truth, namely the results from the physical optical filter.},
  archive      = {J_TIP},
  author       = {Hao-Yu Tsai and Morris C.-H. Tsai and Scott C.-H. Huang and Hsiao-Chun Wu},
  doi          = {10.1109/TIP.2025.3562071},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2560-2574},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Novel computational photography for soft-focus effect in automatic post production},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CharacterFactory: Sampling consistent characters with GANs for diffusion models. <em>TIP</em>, <em>34</em>, 2544-2559. (<a href='https://doi.org/10.1109/TIP.2025.3558668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in text-to-image models have opened new frontiers in human-centric generation. However, these models cannot be directly employed to generate images with consistent newly coined identities. In this work, we propose CharacterFactory, a framework that allows sampling new characters with consistent identities in the latent space of GANs for diffusion models. More specifically, we consider the word embeddings of celeb names as ground truths for the identity-consistent generation task and train a GAN model to learn the mapping from a latent space to the celeb embedding space. In addition, we design a context-consistent loss to ensure that the generated identity embeddings can produce identity-consistent images in various contexts. Remarkably, the whole model only takes 10 minutes for training, and can sample infinite characters end-to-end during inference. Extensive experiments demonstrate excellent performance of the proposed CharacterFactory on character creation in terms of identity consistency and editability. Furthermore, the generated characters can be seamlessly combined with the off-the-shelf image/video/3D diffusion models. We believe that the proposed CharacterFactory is an important step for identity-consistent character generation. Code and Gradio demo are available at: https://qinghew.github.io/CharacterFactory/},
  archive      = {J_TIP},
  author       = {Qinghe Wang and Baolu Li and Xiaomin Li and Bing Cao and Liqian Ma and Huchuan Lu and Xu Jia},
  doi          = {10.1109/TIP.2025.3558668},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2544-2559},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CharacterFactory: Sampling consistent characters with GANs for diffusion models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight RGB-D salient object detection from a speed-accuracy tradeoff perspective. <em>TIP</em>, <em>34</em>, 2529-2543. (<a href='https://doi.org/10.1109/TIP.2025.3560488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. For modality fusion, we propose a Decoupled Attention Module (DAM) to explore the consistency within and between modalities. Here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. For feature representation, we develop a Dual Information Representation Module (DIRM) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. DIRM models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. Finally, we design a Dual Feature Aggregation Module (DFAM) in the decoder to aggregate texture and saliency features. Extensive experiments on five public RGB-D SOD datasets indicate that the proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and achieves a lightweight framework with 5.2 M parameters and 415 FPS. The code is available at https://github.com/duan-song/SATNet},
  archive      = {J_TIP},
  author       = {Songsong Duan and Xi Yang and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3560488},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2529-2543},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lightweight RGB-D salient object detection from a speed-accuracy tradeoff perspective},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised range-nullspace learning prior for multispectral images reconstruction. <em>TIP</em>, <em>34</em>, 2513-2528. (<a href='https://doi.org/10.1109/TIP.2025.3560430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot Spectral Imaging (SSI) techniques, with the ability to capture both spectral and spatial information in a single exposure, have been found useful in a wide range of applications. SSI systems generally operate within the ‘encoding-decoding’ framework, leveraging the synergism of optical hardware and reconstruction algorithms. Typically, reconstructing desired spectral images from SSI measurements is an ill-posed and challenging problem. Existing studies utilize either model-based or deep learning-based methods, but both have their drawbacks. Model-based algorithms suffer from high computational costs, while supervised learning-based methods rely on large paired training data. In this paper, we propose a novel Unsupervised range-Nullspace learning (UnNull) prior for spectral image reconstruction. UnNull explicitly models the data via subspace decomposition, offering enhanced interpretability and generalization ability. Specifically, UnNull considers that the spectral images can be decomposed into the range and null subspaces. The features projected onto the range subspace are mainly low-frequency information, while features in the nullspace represent high-frequency information. Comprehensive multispectral demosaicing and reconstruction experiments demonstrate the superior performance of our proposed algorithm.},
  archive      = {J_TIP},
  author       = {Yurong Chen and Yaonan Wang and Hui Zhang},
  doi          = {10.1109/TIP.2025.3560430},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2513-2528},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised range-nullspace learning prior for multispectral images reconstruction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuseumMaker: Continual style customization without catastrophic forgetting. <em>TIP</em>, <em>34</em>, 2499-2512. (<a href='https://doi.org/10.1109/TIP.2025.3553024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trainedlarge text-to-image (T2I) models with an appropriate text prompt has attracted growing interests in customized image generation fields. However, catastrophic forgetting issue makes it hard to continually synthesize new user-provided styles while retaining the satisfying results amongst learned styles. In this paper, we propose MuseumMaker, a method that enables the synthesis of images by following a set of customized styles in a never-end manner, and gradually accumulates these creative artistic works as a Museum. When facing with a new customization style, we develop a style distillation loss module to extract and learn the styles of the training data for new image generation task. It can minimize the learning biases caused by content of new training images, and address the catastrophic overfitting issue induced by few-shot images. To deal with catastrophic forgetting issue amongst past learned styles, we devise a dual regularization for shared-LoRA module to optimize the direction of model update, which could regularize the diffusion model from both weight and feature aspects, respectively. Meanwhile, to further preserve historical knowledge from past styles and address the limited representability of LoRA, we design a task-wise token learning module where a unique token embedding is learned to denote a new style. As any new user-provided style come, our MuseumMaker can capture the nuances of the new styles while maintaining the details of learned styles. Experimental results on diverse style datasets validate the effectiveness of our proposed MuseumMaker method, showcasing its robustness and versatility across various scenarios.},
  archive      = {J_TIP},
  author       = {Chenxi Liu and Gan Sun and Wenqi Liang and Jiahua Dong and Can Qin and Yang Cong},
  doi          = {10.1109/TIP.2025.3553024},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2499-2512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MuseumMaker: Continual style customization without catastrophic forgetting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contrastive-learning framework for unsupervised salient object detection. <em>TIP</em>, <em>34</em>, 2487-2498. (<a href='https://doi.org/10.1109/TIP.2025.3558674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing unsupervised salient object detection (USOD) methods usually rely on low-level saliency priors, such as center and background priors, to detect salient objects, resulting in insufficient high-level semantic understanding. These low-level priors can be fragile and lead to failure when the natural images do not satisfy the prior assumptions, e.g., these methods may fail to detect those off-center salient objects causing fragmented objects in the segmentation. To address these problems, we propose to eliminate the dependency on flimsy low-level priors, and extract high-level saliency from natural images through a contrastive learning framework. To this end, we propose a Contrastive Saliency Network (CSNet), which is a prior-free and label-free saliency detector, with two novel modules: 1) a Contrastive Saliency Extraction (CSE) module to extract high-level saliency cues, by mimicking the human attention mechanism within an instance discriminative task through a contrastive learning framework, and 2) a Feature Re-Coordinate (FRC) module to recover spatial details, by calibrating high-level features with low-level features in an unsupervised fashion. In addition, we introduce a novel local appearance triplet (LAT) loss to assist the training process by encouraging similar saliency scores for regions with homogeneous appearances. Extensive experiments show that our approach is effective and outperforms state-of-the-art methods on popular SOD benchmarks.},
  archive      = {J_TIP},
  author       = {Huankang Guan and Jiaying Lin and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2025.3558674},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2487-2498},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A contrastive-learning framework for unsupervised salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSMT: Dual-stage multiscale transformer for hyperspectral snapshot compressive imaging. <em>TIP</em>, <em>34</em>, 2473-2486. (<a href='https://doi.org/10.1109/TIP.2025.3556520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) compresses a 3D hyperspectral image (HSI) into a 2D measurement, significantly improving imaging efficiency while preserving the spatial and spectral information inherent in HSI. However, reconstructing high-quality HSIs from compressed measurements remains a core challenge due to the complexity of the inverse problem. Transformer-based methods have recently shown promising performance in HSI reconstruction. Nonetheless, effectively capturing local information, long-range dependencies, and multi-scale features within a reasonable computational cost remains a significant challenge. In this paper, we propose a dual-stage multiscale Transformer (DSMT) tailored for HSI reconstruction, which adopts a coarse-to-fine framework to enhance reconstruction accuracy and network generalization. Specifically, we design a novel U-Net architecture with a dual-branch encoder, where two separate branches process distinct features and are fused to achieve more refined reconstruction results. Full-scale skip connections are introduced to strengthen feature fusion across different stages. To further improve performance, we develop a novel self-attention mechanism called dual-window multiscale multi-head self-attention (DWM-MSA). By utilizing two differently sized windows, DWM-MSA captures long-range dependencies and local information at multiple scales, significantly boosting reconstruction quality. Additionally, we introduce a hybrid positional embedding method, conditional/relative positional embedding (CRPE), which dynamically models both spatial and spectral dependencies, effectively enhancing the Transformer’s capacity for HSI reconstruction. Extensive quantitative and qualitative experiments on both the simulated and the real data are conducted to demonstrate the superior performance, stability, and generalization ability of our DSMT. Code of this project is at https://github.com/chenx2000/DSMT.},
  archive      = {J_TIP},
  author       = {Fulin Luo and Xi Chen and Tan Guo and Xiuwen Gong and Lefei Zhang and Ce Zhu},
  doi          = {10.1109/TIP.2025.3556520},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2473-2486},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DSMT: Dual-stage multiscale transformer for hyperspectral snapshot compressive imaging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double oracle neural architecture search for game theoretic deep learning models. <em>TIP</em>, <em>34</em>, 2463-2472. (<a href='https://doi.org/10.1109/TIP.2025.3558420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new approach to train deep learning models using game theory concepts including Generative Adversarial Networks (GANs) and Adversarial Training (AT) where we deploy a double-oracle framework using best response oracles. GAN is essentially a two-player zero-sum game between the generator and the discriminator. The same concept can be applied to AT with attacker and classifier as players. Training these models is challenging as a pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium is difficult as training algorithms for both GAN and AT have a large-scale strategy space. Extending our preliminary model DO-GAN, we propose the methods to apply the double oracle framework concept to Adversarial Neural Architecture Search (NAS for GAN) and Adversarial Training (NAS for AT) algorithms. We first generalize the players’ strategies as the trained models of generator and discriminator from the best response oracles. We then compute the meta-strategies using a linear program. For scalability of the framework where multiple network models of best responses are stored in the memory, we prune the weakly-dominated players’ strategies to keep the oracles from becoming intractable. Finally, we conduct experiments on MNIST, CIFAR-10 and TinyImageNet for DONAS-GAN. We also evaluate the robustness under FGSM and PGD attacks on CIFAR-10, SVHN and TinyImageNet for DONAS-AT. We show that all our variants have significant improvements in both subjective qualitative evaluation and quantitative metrics, compared with their respective base architectures.},
  archive      = {J_TIP},
  author       = {Aye Phyu Phyu Aung and Xinrun Wang and Ruiyu Wang and Hau Chan and Bo An and Xiaoli Li and J. Senthilnath},
  doi          = {10.1109/TIP.2025.3558420},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2463-2472},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Double oracle neural architecture search for game theoretic deep learning models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt-based modality alignment for effective multi-modal object re-identification. <em>TIP</em>, <em>34</em>, 2450-2462. (<a href='https://doi.org/10.1109/TIP.2025.3556531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical challenge for multi-modal Object Re-Identification (ReID) is the effective aggregation of complementary information to mitigate illumination issues. State-of-the-art methods typically employ complex and highly-coupled architectures, which unavoidably result in heavy computational costs. Moreover, the significant distribution gap among different image spectra hinders the joint representation of multi-modal features. In this paper, we propose a framework named as PromptMA to establish effective communication channels between different modality paths, thereby aggregating modal complementary information and bridging the distribution gap. Specifically, we inject a series of learnable multi-modal prompts into the Image Encoder and introduce a prompt exchange mechanism to enable the prompts to alternately interact with different modal token embeddings, thus capturing and distributing multi-modal features effectively. Building on top of the multi-modal prompts, we further propose Prompt-based Token Selection (PBTS) and Prompt-based Modality Fusion (PBMF) modules to achieve effective multi-modal feature fusion while minimizing background interference. Additionally, due to the flexibility of our prompt exchange mechanism, our method is well-suited to handle scenarios with missing modalities. Extensive evaluations are conducted on four widely used benchmark datasets and the experimental results demonstrate that our method achieves state-of-the-art performances, surpassing the current benchmarks by over 15% on the challenging MSVR310 dataset and by 6% on the RGBNT201. The code is available at https://github.com/FHR-L/PromptMA},
  archive      = {J_TIP},
  author       = {Shizhou Zhang and Wenlong Luo and De Cheng and Yinghui Xing and Guoqiang Liang and Peng Wang and Yanning Zhang},
  doi          = {10.1109/TIP.2025.3556531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2450-2462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prompt-based modality alignment for effective multi-modal object re-identification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NP-hand: Novel perspective hand image synthesis guided by normals. <em>TIP</em>, <em>34</em>, 2435-2449. (<a href='https://doi.org/10.1109/TIP.2025.3560241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing multi-view images that are geometrically consistent with a given single-view image is one of the hot issues in AIGC in recent years. Existing methods have achieved impressive performance on objects with symmetry or rigidity, but they are inappropriate for the human hand. Because an image-captured human hand has more diverse poses and less attractive textures. In this paper, we propose NP-Hand, a framework that elegantly combines the diffusion model and generative adversarial network: The multi-step diffusion is trained to synthesize low-resolution novel perspective, while the single-step generator is exploited to further enhance synthesis quality. To maintain the consistency between inputs and synthesis, we creatively introduce normal maps into NP-Hand to guide the whole synthesizing process. Comprehensive evaluations have demonstrated that the proposed framework is superior to existing state-of-the-art models and more suitable for synthesizing hand images with faithful structures and realistic appearance details. The code will be released on our website.},
  archive      = {J_TIP},
  author       = {Binghui Zuo and Wenqian Sun and Zimeng Zhao and Xiaohan Yuan and Yangang Wang},
  doi          = {10.1109/TIP.2025.3560241},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2435-2449},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NP-hand: Novel perspective hand image synthesis guided by normals},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal knowledge diffusion-based generation for difference-aware medical VQA. <em>TIP</em>, <em>34</em>, 2421-2434. (<a href='https://doi.org/10.1109/TIP.2025.3558446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal medical applications have garnered considerable attention due to their potential to offer comprehensive and robust support for medical assistance. Specifically, within this domain, difference-aware medical Visual Question Answering (VQA) has emerged as a topic of increasing interest that enables the recognition of changes in physical conditions over time when compared to previous states and provides customized suggestions accordingly. However, it is challenging because samples usually exhibit characteristics of complexity, diversity, and inherent noise. Besides, there is a need for multimodal knowledge understanding of the medical domain. The difference-aware setting requiring image comparison further intensifies these situations. To this end, we propose a cross-Modal knowlEdge diffusioN-baseD gEneration netwoRk (MENDER), where the diffusion mechanism with multi-step denoising and knowledge injection from global to local level are employed to tackle the aforementioned challenges, respectively. The diffusion process is to gradually generate answers with the sequence input of questions, random noises for the answer masks and virtual vision prompts of images. The strategy of answer nosing and knowledge cascading is specifically tailored for this task and is implemented during forward and reverse diffusion processes. Moreover, the visual and structure knowledge injection are proposed to learn virtual vision prompts to guide the diffusion process, where the former is realized using a pre-trained medical image-text network and the latter is modeled with spatial and semantic graph structures processed by the heterogeneous graph Transformer models. Experiment results demonstrate the effectiveness of MENDER for difference-aware medical VQA. Furthermore, it also exhibits notable performance in the low-resource setting and conventional medical VQA tasks.},
  archive      = {J_TIP},
  author       = {Qika Lin and Kai He and Yifan Zhu and Fangzhi Xu and Erik Cambria and Mengling Feng},
  doi          = {10.1109/TIP.2025.3558446},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2421-2434},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal knowledge diffusion-based generation for difference-aware medical VQA},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALPS: An auto-labeling and pre-training scheme for remote sensing segmentation with segment anything model. <em>TIP</em>, <em>34</em>, 2408-2420. (<a href='https://doi.org/10.1109/TIP.2025.3556344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fast-growing field of Remote Sensing (RS) image analysis, the gap between massive unlabeled datasets and the ability to fully utilize these datasets for advanced RS analytics presents a significant challenge. To fill the gap, our work introduces an innovative auto-labeling framework named ALPS (Automatic Labeling for Pre-training in Segmentation), which leverages the Segment Anything Model (SAM) to predict precise pseudo-labels for RS images without necessitating prior annotations or additional prompts. The proposed pipeline significantly reduces the labor and resource demands traditionally associated with annotating RS datasets. By constructing two comprehensive pseudo-labeled RS datasets via ALPS for pre-training purposes, our approach enhances the performance of downstream tasks across various benchmarks, including iSAID and ISPRS Potsdam. Experiments demonstrate the effectiveness of our framework, showing its ability to generalize well across multiple tasks even under the scarcity of extensively annotated datasets, offering a scalable solution to automatic segmentation and annotation challenges in the field. In addition, the proposed pipeline is flexible and can be applied to medical image segmentation, remarkably increasing the performance. Note that ALPS utilizes pre-trained SAM to semi-automatically annotate RS images without additional manual annotations. Although every component in the pipeline has been well explored, integrating clustering algorithms with SAM and novel pseudo-label alignment significantly enhances RS segmentation, as an off-the-shelf tool for pre-training data preparation. Our source code is available at: https://github.com/StriveZs/ALPS.},
  archive      = {J_TIP},
  author       = {Song Zhang and Qingzhong Wang and Junyi Liu and Haoyi Xiong},
  doi          = {10.1109/TIP.2025.3556344},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2408-2420},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ALPS: An auto-labeling and pre-training scheme for remote sensing segmentation with segment anything model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate and robust three-intersection-chord-invariant ellipse detection. <em>TIP</em>, <em>34</em>, 2392-2407. (<a href='https://doi.org/10.1109/TIP.2025.3559409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ellipse detection is of great significance in the fields of image processing and computer vision. Accurate, stable and direct ellipse detection in real-world images has always been a key issue. Therefore, an ellipse detection method is proposed on the basis of the constructed three-intersection-chord-invariant. First, in the inflexion point detection, the PCA minimum bounding box considering the distribution characteristics of edge points is studied to achieve the more refined line segment screening. Second, a multi-scale inflexion point detection method is proposed to effectively avoid over-segmentation of small arc segments, providing assurance for more reasonable and reliable arc segment combinations. Then, the 20 precisely classified arc segment combinations are refined into 4 combinations. A number of non-homologous arc segment combinations can be quickly removed to reduce incorrect combinations by the constructed midpoint distance constraint and quadrant constraint. Moreover, in order to accurately reflect the strict arc segment combination constraints of geometric features of ellipses, a three-intersection-chord-invariant model of ellipses is established with strong constraint of relative distances among five constraint points, by which a more robust initial ellipse set of homologous arc segment combinations is further obtained. Finally, ellipse validation and clustering are performed on the initial set of ellipses to obtain the high-precision ellipses. The algorithm accuracy of the ellipse detection method is experimentally validated on 6 publicly available datasets and 2 established wheel rim datasets.},
  archive      = {J_TIP},
  author       = {Guan Xu and Yunkun Wang and Fang Chen and Hui Shen and Xiaotao Li},
  doi          = {10.1109/TIP.2025.3559409},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2392-2407},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate and robust three-intersection-chord-invariant ellipse detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eigenpose: Occlusion-robust 3D human mesh reconstruction. <em>TIP</em>, <em>34</em>, 2379-2391. (<a href='https://doi.org/10.1109/TIP.2025.3559788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new approach for occlusion-robust 3D human mesh reconstruction from a single image is introduced in this paper. Since occlusion has emerged as a major problem to be resolved in this field, there have been meaningful efforts to deal with various types of occlusions (e.g., person-to-person occlusion, person-to-object occlusion, self-occlusion, etc.). Although many recent studies have shown the remarkable progress, previous regression-based methods still have respective limitations to handle occlusion problems due to the lack of the appearance information. To address this problem, we propose a novel method for human mesh reconstruction based on the pose-relevant subspace analysis. Specifically, we first generate a set of eigenvectors, so-called eigenposes, by conducting the singular value decomposition (SVD) of the pose matrix, which contains diverse poses sampled from the training set. These eigenposes are then linearly combined to construct a target body pose according to fusing coefficients, which are learned through the proposed network. Such combination of principal body postures (i.e., eigenposes) in a global manner gives a great help to cope with partial ambiguities by occlusions. Furthermore, we also propose to exploit a joint injection module that efficiently incorporates the spatial information of visible joints into the encoded feature during the estimation process of fusing coefficients. Experimental results on benchmark datasets demonstrate the ability of the proposed method to robustly reconstruct the human mesh under various occlusions occurring in real-world scenarios. The code and model are publicly available at: https://github.com/DCVL-3D/Eigenpose_release.},
  archive      = {J_TIP},
  author       = {Mi-Gyeong Gwon and Gi-Mun Um and Won-Sik Cheong and Wonjun Kim},
  doi          = {10.1109/TIP.2025.3559788},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2379-2391},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Eigenpose: Occlusion-robust 3D human mesh reconstruction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning sparse tensor neural networks enables deep learning for 3D ultrasound localization microscopy. <em>TIP</em>, <em>34</em>, 2367-2378. (<a href='https://doi.org/10.1109/TIP.2025.3552198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose the use of sparse tensor neural networks to enable deep learning-based 3D ULM by improving memory scalability with increased dimensionality. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.},
  archive      = {J_TIP},
  author       = {Brice Rauby and Paul Xing and Jonathan Porée and Maxime Gasse and Jean Provost},
  doi          = {10.1109/TIP.2025.3552198},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2367-2378},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pruning sparse tensor neural networks enables deep learning for 3D ultrasound localization microscopy},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRCL: Causal representation consistency learning for anomaly detection in surveillance videos. <em>TIP</em>, <em>34</em>, 2351-2366. (<a href='https://doi.org/10.1109/TIP.2025.3558089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Anomaly Detection (VAD) remains a fundamental yet formidable task in the video understanding community, with promising applications in areas such as information forensics and public safety protection. Due to the rarity and diversity of anomalies, existing methods only use easily collected regular events to model the inherent normality of normal spatial-temporal patterns in an unsupervised manner. Although such methods have made significant progress benefiting from the development of deep learning, they attempt to model the statistical dependency between observable videos and semantic labels, which is a crude description of normality and lacks a systematic exploration of its underlying causal relationships. Previous studies have shown that existing unsupervised VAD models are incapable of label-independent data offsets (e.g., scene changes) in real-world scenarios and may fail to respond to light anomalies due to the overgeneralization of deep neural networks. Inspired by causality learning, we argue that there exist causal factors that can adequately generalize the prototypical patterns of regular events and present significant deviations when anomalous instances occur. In this regard, we propose Causal Representation Consistency Learning (CRCL) to implicitly mine potential scene-robust causal variable in unsupervised video normality learning. Specifically, building on the structural causal models, we propose scene-debiasing learning and causality-inspired normality learning to strip away entangled scene bias in deep representations and learn causal video normality, respectively. Extensive experiments on benchmarks validate the superiority of our method over conventional deep representation learning. Moreover, ablation studies and extension validation show that the CRCL can cope with label-independent biases in multi-scene settings and maintain stable performance with only limited training data available.},
  archive      = {J_TIP},
  author       = {Yang Liu and Hongjin Wang and Zepu Wang and Xiaoguang Zhu and Jing Liu and Peng Sun and Rui Tang and Jianwei Du and Victor C. M. Leung and Liang Song},
  doi          = {10.1109/TIP.2025.3558089},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2351-2366},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CRCL: Causal representation consistency learning for anomaly detection in surveillance videos},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multispectral snapshot image registration using learned cross spectral disparity estimation and a deep guided occlusion reconstruction network. <em>TIP</em>, <em>34</em>, 2338-2350. (<a href='https://doi.org/10.1109/TIP.2025.3556602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral imaging aims at recording images in different spectral bands. This is extremely beneficial in diverse discrimination applications, for example in agriculture, recycling or healthcare. One approach for snapshot multispectral imaging, which is capable of recording multispectral videos, is by using camera arrays, where each camera records a different spectral band. Since the cameras are at different spatial positions, a registration procedure is necessary to map every camera to the same view. In this paper, we present a multispectral snapshot image registration with three novel components. First, a cross spectral disparity estimation network is introduced, which is trained on a popular stereo database using pseudo spectral data augmentation. Subsequently, this disparity estimation is used to accurately detect occlusions by warping the disparity map in a layer-wise manner. Finally, these detected occlusions are reconstructed by a learned deep guided neural network, which leverages the structure from other spectral components. It is shown that each element of this registration process as well as the final result is superior to the current state of the art. In terms of PSNR, our registration achieves an improvement of over 3 dB. At the same time, the runtime is decreased by a factor of over 3 on a CPU. Additionally, the registration is executable on a GPU, where the runtime can be decreased by a factor of 113. The source code and the data is available at https://github.com/FAU-LMS/MSIR.},
  archive      = {J_TIP},
  author       = {Frank Sippel and Jürgen Seiler and André Kaup},
  doi          = {10.1109/TIP.2025.3556602},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2338-2350},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multispectral snapshot image registration using learned cross spectral disparity estimation and a deep guided occlusion reconstruction network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pluralistic salient object detection. <em>TIP</em>, <em>34</em>, 2325-2337. (<a href='https://doi.org/10.1109/TIP.2025.3557563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce pluralistic salient object detection (PSOD), a novel task aimed at generating multiple plausible salient segmentation results for a given input image. Unlike conventional SOD methods that produce a single segmentation mask for salient objects, this new setting recognizes the inherent complexity of real-world images, comprising multiple objects, and the ambiguity in defining salient objects due to different user intentions. To study this task, we present two new SOD datasets “DUTS-MM” and “DUTS-MQ”, along with newly designed evaluation metrics. DUTS-MM builds upon the DUTS dataset but enriches the ground-truth mask annotations from three aspects which 1) improves the mask quality especially for boundary and fine-grained structures; 2) alleviates the annotation inconsistency issue; and 3) provides multiple ground-truth masks for images with saliency ambiguity. DUTS-MQ consists of approximately 100K image-mask pairs with human-annotated preference scores, enabling the learning of real human preferences in measuring mask quality. Building upon these two datasets, we propose a simple yet effective pluralistic SOD baseline based on a Mixture-of-Experts (MOE) design. Equipped with two prediction heads, it simultaneously predicts multiple masks using different query prompts and predicts human preference scores for each mask candidate. Extensive experiments and analyses underscore the significance of our proposed datasets and affirm the effectiveness of our PSOD framework.},
  archive      = {J_TIP},
  author       = {Xuelu Feng and Yunsheng Li and Dongdong Chen and Chunming Qiao and Junsong Yuan and Lu Yuan and Gang Hua},
  doi          = {10.1109/TIP.2025.3557563},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2325-2337},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pluralistic salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Keep and extent: Unified knowledge embedding for few-shot image generation. <em>TIP</em>, <em>34</em>, 2315-2324. (<a href='https://doi.org/10.1109/TIP.2025.3557578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training Generative Adversarial Networks (GANs) with few-shot data has been a challenging task, which is prevalently solved by adapting a deep generative model pre-trained on the large-scale data in a source domain to small target domains with limited training data. In practice, most of the existing methods focus on designing task-specific fine-tuning strategies or regularization terms to select and preserve compatible knowledge across the source and target domain. However, the compatible knowledge greatly depends on the target domain and is entangled with the incompatible one. For the few-shot image generation task, without accurate compatible knowledge as prior, the generated images will strongly overfit the scarce target images. From a different perspective, we propose a unified learning paradigm for better knowledge transfer, i.e., keep and extent (KAE). Specifically, we orthogonally decompose the latent space of GANs, where the resting direction that has an unnoticeable impact on the generated images is adopted to extend the new target latent subspace while the remaining directions keep intact to reconstruct the source latent subspace. In this way, the whole source domain knowledge is included in the source latent subspace and the compatible knowledge will be automatically transferred to the target domain along the resting direction, rather than manually selecting. Extensive experimental results on several benchmark datasets demonstrate the superiority of our method.},
  archive      = {J_TIP},
  author       = {Chenghao Xu and Jiexi Yan and Cheng Deng},
  doi          = {10.1109/TIP.2025.3557578},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2315-2324},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Keep and extent: Unified knowledge embedding for few-shot image generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-guided refinement for fine-grained salient object detection. <em>TIP</em>, <em>34</em>, 2301-2314. (<a href='https://doi.org/10.1109/TIP.2025.3557562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty guidance learning approach to SOD, intended to enhance the model’s perception of uncertain regions. Specifically, we design a novel Uncertainty Guided Refinement Attention Network (UGRAN), which incorporates three important components, i.e., the Multilevel Interaction Attention (MIA) module, the Scale Spatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement Attention (URA) module. Unlike conventional methods dedicated to enhancing features, the proposed MIA facilitates the interaction and perception of multilevel features, leveraging the complementary characteristics among multilevel features. Then, through the proposed SSCA, the salient information across diverse scales within the aggregated features can be integrated more comprehensively and integrally. In the subsequent steps, we utilize the uncertainty map generated from the saliency prediction map to enhance the model’s perception capability of uncertain regions, generating a highly-saturated fine-grained saliency prediction map. Additionally, we devise an adaptive dynamic partition (ADP) mechanism to minimize the computational overhead of the URA module and improve the utilization of uncertainty guidance. Experiments on seven benchmark datasets demonstrate the superiority of the proposed UGRAN over the state-of-the-art methodologies. Codes will be released at https://github.com/I2-Multimedia-Lab/UGRAN},
  archive      = {J_TIP},
  author       = {Yao Yuan and Pan Gao and Qun Dai and Jie Qin and Wei Xiang},
  doi          = {10.1109/TIP.2025.3557562},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2301-2314},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty-guided refinement for fine-grained salient object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction and reference quality adaptation for learned video compression. <em>TIP</em>, <em>34</em>, 2285-2300. (<a href='https://doi.org/10.1109/TIP.2025.3555401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal prediction is one of the most important technologies for video compression. Various prediction coding modes are designed in traditional video codecs. Traditional video codecs will adaptively to decide the optimal coding mode according to the prediction quality and reference quality. Recently, learned video codecs have made great progress. However, they did not effectively address the problem of prediction and reference quality adaptation, which limits the effective utilization of temporal prediction and reduction of reconstruction error propagation. Therefore, in this paper, we first propose a confidence-based prediction quality adaptation (PQA) module to provide explicit discrimination for the spatial and channel-wise prediction quality difference. With this module, the prediction with low quality will be suppressed and that with high quality will be enhanced. The codec can adaptively decide which spatial or channel location of predictions to use. Then, we further propose a reference quality adaptation (RQA) module and an associated repeat-long training strategy to provide dynamic spatially variant filters for diverse reference qualities. With these filters, our codec can adapt to different reference qualities, making it easier to achieve the target reconstruction quality and reduce the reconstruction error propagation. Experimental results verify that our proposed modules can effectively help our codec achieve a higher compression performance.},
  archive      = {J_TIP},
  author       = {Xihua Sheng and Li Li and Dong Liu and Houqiang Li},
  doi          = {10.1109/TIP.2025.3555401},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2285-2300},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prediction and reference quality adaptation for learned video compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGC-net: Dynamic graph contrastive network for video object detection. <em>TIP</em>, <em>34</em>, 2269-2284. (<a href='https://doi.org/10.1109/TIP.2025.3551158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is a challenging task in computer vision since it needs to handle the object appearance degradation problem that seldom occurs in the image domain. Off-the-shelf video object detection methods typically aggregate multi-frame features at one stroke to alleviate appearance degradation. However, these existing methods do not take supervision knowledge into consideration and thus still suffer from insufficient feature aggregation, resulting in the false detection problem. In this paper, we take a different perspective on feature aggregation, and propose a dynamic graph contrastive network (DGC-Net) for video object detection, including three improvements against existing methods. First, we design a frame-level graph contrastive module to aggregate frame features, enabling our DGC-Net to fully exploit discriminative contextual feature representations to facilitate video object detection. Second, we develop a proposal-level graph contrastive module to aggregate proposal features, making our DGC-Net sufficiently learn discriminative semantic feature representations. Third, we present a graph transformer to dynamically adjust the graph structure by pruning the useless nodes and edges, which contributes to improving accuracy and efficiency as it can eliminate the geometric-semantic ambiguity and reduce the graph scale. Furthermore, inherited from the framework of DGC-Net, we develop DGC-Net Lite to perform real-time video object detection with a much faster inference speed. Extensive experiments conducted on the ImageNet VID dataset demonstrate that our DGC-Net outperforms the performance of current state-of-the-art methods. Notably, our DGC-Net obtains 86.3%/87.3% mAP when using ResNet-101/ResNeXt-101.},
  archive      = {J_TIP},
  author       = {Qiang Qi and Hanzi Wang and Yan Yan and Xuelong Li},
  doi          = {10.1109/TIP.2025.3551158},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2269-2284},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DGC-net: Dynamic graph contrastive network for video object detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A swiss army knife for tracking by natural language specification. <em>TIP</em>, <em>34</em>, 2254-2268. (<a href='https://doi.org/10.1109/TIP.2025.3553290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking by natural language specification requires trackers to jointly perform grounding and tracking tasks. Existing methods either use separate models or a single shared network, failing to account for the link and diversity between tasks jointly. In this paper, we propose a novel framework that performs dynamic task switching to customize its network path routing for each task within a unified model. For this purpose, we design a task-switchable attention module, which enables the acquisition of modal relation patterns with different dominant modalities for each task via dynamic task switching. In addition, to alleviate the inconsistency between the static language description and the dynamic target appearance during tracking, we propose a language renovation mechanism that renovates the initial language online via visual-context-aware linguistic prompting. Extensive experimental results on five datasets demonstrate that the proposed method performs favorably against state-of-the-art approaches for both grounding and tracking. Our project will be available at: https://github.com/mkg1204/SAKTrack.},
  archive      = {J_TIP},
  author       = {Kaige Mao and Xiaopeng Hong and Xiaopeng Fan and Wangmeng Zuo},
  doi          = {10.1109/TIP.2025.3553290},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2254-2268},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A swiss army knife for tracking by natural language specification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-spatial complementation: Unified channel-specific style attack for cross-domain few-shot learning. <em>TIP</em>, <em>34</em>, 2242-2253. (<a href='https://doi.org/10.1109/TIP.2025.3553781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Learning (CD-FSL) addresses the challenges of recognizing targets with out-of-domain data when only a few instances are available. Many current CD-FSL approaches primarily focus on enhancing the generalization capabilities of models in spatial domain, which neglects the role of the frequency domain in domain generalization. To take advantage of frequency domain in processing global information, we propose a Frequency-Spatial Complementation (FSC) model, which combines frequency domain information with spatial domain information to learn domain-invariant information from attacked data style. Specifically, we design a Frequency and Spatial Fusion (FusionFS) module to enhance the ability of the model to capture style-related information. Besides, we propose two attack strategies, i.e., the Gradient-guided Unified Style Attack (GUSA) strategy and the Channel-specific Attack Intensity Calculation (CAIC) strategy, which conduct targeted attacks on different channels to provide more diversified style data during the training phase, especially in single-source domain scenarios where the source domain data style is homogeneous. Extensive experiments across eight target domains demonstrate that our method significantly improves the model’s performance under various styles.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Zhilong Wang and Xiyao Liu and Yunlong Yu and Yanwei Pang and Jungong Han},
  doi          = {10.1109/TIP.2025.3553781},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2242-2253},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Frequency-spatial complementation: Unified channel-specific style attack for cross-domain few-shot learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Onet: Twin U-net architecture for unsupervised binary semantic segmentation in radar and remote sensing images. <em>TIP</em>, <em>34</em>, 2161-2172. (<a href='https://doi.org/10.1109/TIP.2025.3530816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting objects from cluttered backgrounds in single-channel images, such as marine radar echoes, medical images, and remote sensing images, poses significant challenges due to limited texture, color information, and diverse target types. This paper proposes a novel solution: the Onet, an O-shaped assembly of twin U-Net deep neural networks, designed for unsupervised binary semantic segmentation. The Onet, trained with an intensity-complementary image pair and without the need for annotated labels, maximizes the Jensen-Shannon divergence (JSD) between the densely localized features and the class probability maps. By leveraging the symmetry of U-Net, Onet subtly strengthens the dependence between dense local features, global features, and class probability maps during the training process. The design of the complementary input pair aligns with the theoretical requirement that optimizing JSD needs the class probability of negative samples to accurately estimate the marginal distribution. Compared to the current leading unsupervised segmentation methods, the Onet demonstrates superior performance in target segmentation in marine radar frames and cloud segmentation in remote sensing images. Notably, we found that Onet’s foreground prediction significantly enhances the signal-to-noise ratio (SNR) of targets amidst marine radar clutter. Onet’s source code is publicly accessible at https://github.com/joeyee/Onet.},
  archive      = {J_TIP},
  author       = {Yi Zhou and Hang Su and Tian Wang and Qing Hu},
  doi          = {10.1109/TIP.2025.3530816},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2161-2172},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Onet: Twin U-net architecture for unsupervised binary semantic segmentation in radar and remote sensing images},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
