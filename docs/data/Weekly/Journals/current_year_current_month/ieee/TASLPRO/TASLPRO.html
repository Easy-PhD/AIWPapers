<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TASLPRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taslpro">TASLPRO - 299</h2>
<ul>
<li><details>
<summary>
(2025). Towards understanding of frequency dependence on sound event detection. <em>TASLPRO</em>, <em>33</em>, 3948-3960. (<a href='https://doi.org/10.1109/TASLPRO.2025.3603891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we conduct an in-depth analysis of two frequency-dependent methods for sound event detection (SED): FilterAugment and frequency dynamic convolution (FDY conv). The goal is to better understand their characteristics and behaviors in the context of SED. While SED has been rapidly advancing through the adoption of various deep learning techniques from other pattern recognition fields, such adopted techniques are often not suitable for SED. To address this issue, two frequency-dependent SED methods were previously proposed: FilterAugment, a data augmentation randomly weighting frequency bands, and FDY conv, an architecture applying frequency adaptive convolution kernels. These methods have demonstrated superior performance in SED, and we aim to further analyze their detailed effectiveness and characteristics in SED. We compare class-wise performance to find out specific pros and cons of FilterAugment and FDY conv. We apply Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights time-frequency region that is more inferred by the model, on SED models with and without frequency masking and two types of FilterAugment to observe their detailed characteristics. We propose simpler frequency dependent convolution methods and compare them with FDY conv to further understand which components of FDY conv affects SED performance. Lastly, we apply PCA to show how FDY conv adapts dynamic kernel across frequency dimensions on different sound event classes. The results and discussions demonstrate that frequency dependency plays a significant role in sound event detection and further confirms the effectiveness of frequency dependent methods on SED.},
  archive  = {J},
  author   = {Hyeonuk Nam and Seong-Hu Kim and Deokki Min and Byeong-Yun Ko and Yong-Hwa Park},
  doi      = {10.1109/TASLPRO.2025.3603891},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3948-3960},
  title    = {Towards understanding of frequency dependence on sound event detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing contextual clues and role correlations for enhancing document-level event argument extraction. <em>TASLPRO</em>, <em>33</em>, 3932-3947. (<a href='https://doi.org/10.1109/TASLPRO.2025.3598644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Document-level event argument extraction is a crucial yet challenging task within the field of information extraction. Current mainstream approaches primarily focus on the information interaction between event triggers and their arguments, facing two limitations: insufficient context interaction and the ignorance of event correlations. Here, we introduce a novel framework named CARG (Contextual Aggregation of Clues and Role-based Latent Guidance), comprising two innovative components: the Contextual Clues Aggregation (CCA) and the Role-based Latent Information Guidance (RLIG) The CCA module leverages the attention weights derived from a pre-trained encoder to adaptively assimilate broader contextual information, while the RLIG module aims to capture the semantic correlations among event roles. We then instantiate the CARG framework into two variants based on two types of mainstream EAE approaches. Notably, our CARG framework introduces less than 1% new parameters yet significantly improves the performance. Comprehensive experiments across the RAMS, WikiEvents, MLEE and ACE 2005 datasets confirm the superiority of CARG, showing significant superiority in terms of both performance and inference speed compared to major benchmarks. Further analyses demonstrate the effectiveness of the proposed modules.},
  archive  = {J},
  author   = {Wanlong Liu and Dingyi Zeng and Li Zhou and Wenyu Chen and Malu Zhang and Dan Liu and Xiaodong He and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3598644},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3932-3947},
  title    = {Utilizing contextual clues and role correlations for enhancing document-level event argument extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRSF: Generating and refining LLM knowledge for cross-domain zero-shot slot filling. <em>TASLPRO</em>, <em>33</em>, 3921-3931. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Slot filling plays an important role in task-oriented dialogue systems. Though slot filling has made significant progress in the single-domain learning paradigm, facilitating learning in unknown domains remains a challenge. Previous works have mostly focused on supplementing sequence labeling models with slot meta-information or metric learning, lacking domain-specific knowledge and performing poorly on unseen slots. By further pretraining or employing larger-parameter generative models, other methods simply introduce implicit general knowledge, which proves challenging in effectively compensating for the performance deficiencies of the model across different slots. In this paper, as knowledge becomes increasingly abundant within LLMs (large language models), we propose a novel framework that generates and refines LLMs knowledge for cross-domain zero-shot slot filling (GRSF). Specifically, based on in-context learning, we introduce a knowledge generation module to fully leverage the knowledge of LLMs. Moreover, based on the transferability of different slots, we train a knowledge evaluator to transfer the factuality of knowledge to refine knowledge. Finally, the refined knowledge is integrated with the original text and fed into the model to compensate for its knowledge gaps. Extensive experiments on three public datasets demonstrate that our approach achieves superior performance compared to the baselines.},
  archive  = {J},
  author   = {Weizhen Li and Peijie Huang and Yuhong Xu and Junbao Huang and Jiekun Fan},
  doi      = {10.1109/TASLPRO.2025.3597441},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3921-3931},
  title    = {GRSF: Generating and refining LLM knowledge for cross-domain zero-shot slot filling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active impulsive noise control with adaptive robust kernels. <em>TASLPRO</em>, <em>33</em>, 3908-3920. (<a href='https://doi.org/10.1109/TASLPRO.2025.3611274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional active noise control (ANC) algorithms suffer significant performance degradation when addressing impulsive noise. Recent advancements. in robust ANC algorithms have focused on the design and application of effective cost functions. A class of ANC algorithms based on the maximum correntropy criterion (MCC), which uses entropy as a robust adaptive cost function, has shown promising capability in handing heavy-tailed impulsive noise. However, the performance of these algorithms heavily relies on empirically manual parameter tuning, which poses challenges for practical applications. To address this, this paper proposes a filtered-x generalized adaptive robust function (FxGARF) algorithm. The robustness is quantified by a continuous parameter that can adaptively adjust based on the residual distribution. Additionally, to enhance the control precision of the algorithm, the scale parameter involved in the generalized robust kernel family is determined by the mean value of the noise signal filtered through Hampel filter. Thus, the proposed algorithm can adapt to noise characteristics, eliminating the need for manual kernel parameter tuning. Moreover, the mean convergence and mean-square performance of the FxGARF algorithm are analyzed. Finally, simulation results demonstrate that the proposed algorithm adaptively selects kernel parameters for impulsive noise of varying characteristics and achieves superior noise reduction performance compared to benchmark algorithms.},
  archive  = {J},
  author   = {Yang Zhou and Haiquan Zhao and Dongxu Liu},
  doi      = {10.1109/TASLPRO.2025.3611274},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3908-3920},
  title    = {Active impulsive noise control with adaptive robust kernels},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangling speech representations learning with latent diffusion for speaker verification. <em>TASLPRO</em>, <em>33</em>, 3896-3907. (<a href='https://doi.org/10.1109/TASLPRO.2025.3610023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Disentangled speech representation learning for speaker verification aims to separate spoken content and speaker timbre into distinct representations. However, existing variational autoencoder (VAE)–based methods for speech disentanglement rely on latent variables that lack semantic meaning, limiting their effectiveness for speaker verification. To address this limitation, we propose a diffusion-based method that disentangles and separates speaker features and speech content in the latent space. Building upon the VAE framework, we employ a speaker encoder to learn latent variables representing speaker features while using frame-specific latent variables to capture content. Unlike previous sequential VAE approaches, our method utilizes a conditional diffusion model in the latent space to derive speaker-aware representations. Experiments on the VoxCeleb and CN-Celeb datasets demonstrate that our method effectively isolates speaker features from speech content using pre-trained speech representations. The learned embeddings are robust to language mismatches since the speaker embeddings become content-invariant after content removal. Additionally, we design contrastive learning experiments showing that our training objective can enhance the learning of speaker-discriminative embeddings without relying on classification-based loss.},
  archive  = {J},
  author   = {Zhe Li and Man-Wai Mak and Jen-Tzung Chien and Mert Pilanci and Zezhong Jin and Helen Meng},
  doi      = {10.1109/TASLPRO.2025.3610023},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3896-3907},
  title    = {Disentangling speech representations learning with latent diffusion for speaker verification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot class-incremental audio classification using pseudo-incrementally trained embedding learner and continually updated stochastic classifier. <em>TASLPRO</em>, <em>33</em>, 3880-3895. (<a href='https://doi.org/10.1109/TASLPRO.2025.3610050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot Class-incremental Audio Classification (FCAC) aims to progressively recognize incremental classes with few tagged samples and meanwhile memorize base classes. To achieve satisfactory FCAC performance, the model needs to have high stability (memorizing base classes) and strong plasticity (adapting to incremental classes). In this work, we design a model which can be decoupled into two independent modules, namely an embedding learner and a stochastic classifier. The former is the backbone of a residual convolutional network, while the latter is composed of distributions and each distribution consists of a mean vector and a variance vector for representing one class. After being trained in the base session, the embedding learner is not updated in each incremental session and thus can memorize the knowledge of base classes. To make the embedding learner possess strong representation ability for incremental classes, we propose a strategy to pseudo-incrementally train the embedding learner using data augmentation in the base session. On the other hand, the stochastic classifier is continually updated in each incremental session and thus can adapt to incremental classes. Our model which consists of a pseudo-incrementally trained embedding learner and a continually updated stochastic classifier can increasingly identify incremental classes without forgetting base classes. Three datasets (FSC-89, NSynth-100 and LS-100) are used to verify the effectiveness of our method. Experiments show that our method exceeds the comparison methods in accuracy, and has lower complexity than most of the comparison methods.},
  archive  = {J},
  author   = {Yanxiong Li and Wenchang Cao and Jiaxin Tan and Qianqian Li and Guoqing Chen},
  doi      = {10.1109/TASLPRO.2025.3610050},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3880-3895},
  title    = {Few-shot class-incremental audio classification using pseudo-incrementally trained embedding learner and continually updated stochastic classifier},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General quality evaluation metric towards demonstrations for in-context learning. <em>TASLPRO</em>, <em>33</em>, 3866-3879. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, Large Language Models (LLMs) have demonstrated strong capabilities to perform multiple tasks through in-context learning (ICL), which is a new paradigm allowing LLMs to learn by observing a few demonstrations. Previous studies have revealed that ICL is highly variable to a range of factors, such as the quality, the quantity and the order of the demonstrations. Though they conduct qualitative analyses, they still lack comprehensive quantitative analyses to evaluate the quality of demonstrations. Previous studies assume that demonstrations exhibiting high similarity to the test query indicate high quality. We argue that high-quality demonstrations should also align with the preferences of the inference model. However, there lacks a comprehensive method to define how good a demonstration is to the inference model. Aiming at this, we propose a General quality Evaluation Metric of demonstrations to the inference model for In-Context learning, named Gemic, without any human-annotation or extra training cost. To verify the effectiveness of Gemic, we take demonstration retrieval task as an illustrative case. Experiments are conducted on a variety of datasets under different LLMs with multiple scales. The results with a relative improvement of up to 56.7% highlight the effectiveness of Gemic.},
  archive  = {J},
  author   = {Huazheng Wang and Jinming Wu and Haifeng Sun and Zixuan Xia and Daixuan Cheng and Jingyu Wang and Qi Qi and Jianxin Liao},
  doi      = {10.1109/TASLPRO.2025.3606230},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3866-3879},
  title    = {General quality evaluation metric towards demonstrations for in-context learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-sinkhorn teacher knowledge aggregation framework for adaptive audio anti-spoofing. <em>TASLPRO</em>, <em>33</em>, 3850-3865. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio anti-spoofing algorithms are widely deployed to defend against spoofing attacks, yet they often fail to detect unseen attacks. Although unsupervised domain adaptation (UDA) offers the potential to address this challenge, existing methods struggle with the large intra-class variability and complex distribution structures in target domains caused by the diversity of speech and attack types. In contrast, optimal transport (OT) leverages the geometric structure of intra-class distributions to measure discrepancies between probability distributions. The effectiveness of OT relies on the discriminability of data within target domains. However, in real-world scenarios involving multiple target domains, these domains often overlap in feature space, leading to the negative transport problem in OT. To overcome these domain mismatches in anti-spoofing, we propose the Multi-Sinkhorn Teacher Knowledge Aggregation (MSTKA) framework. Initially, to avoid interference between target domains during alignment, we use OT to adapt the source model to each target domain independently, thereby reducing negative transport. This adaptation involves constructing an OT cost matrix based on sentence-level representations of cross-domain samples and training an expert model for each target domain. Subsequently, we aggregate the knowledge from these expert models into a unified student model, enabling it to generalize across multiple target domains. Since spoofing cues could be distributed across different temporal scales, we align the student model’s representations at multiple time scales with the teacher model’s sentence-level representations to enhance the effectiveness of knowledge distillation. Multi-target adaptation experiments on eleven data sets demonstrate that our framework achieves state-of-the-art performance in audio anti-spoofing.},
  archive  = {J},
  author   = {Ruiteng Zhang and Jianguo Wei and Xugang Lu and Lin Zhang and Di Jin and Wenhuan Lu and Junhai Xu},
  doi      = {10.1109/TASLPRO.2025.3606191},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3850-3865},
  title    = {Multi-sinkhorn teacher knowledge aggregation framework for adaptive audio anti-spoofing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-channel speech enhancement guided by learning-based $A$ $Posteriori$ speech presence probability estimation. <em>TASLPRO</em>, <em>33</em>, 3838-3849. (<a href='https://doi.org/10.1109/TASLPRO.2025.3599782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, a new deep neural network (DNN)-guided multi-channel speech enhancement approach is proposed to achieve noise reduction, dereverberation, and speech restoration. Different from the end-to-end methods, a DNN model is employed to estimate the key parameters to guide the beamformer (BF) and post-filter (PF). Since the $a$ $posteriori$ speech presence probability (SPP) can softly decide whether speech is present or absent in the short-time Fourier transform domain, the SPP estimate is derived by the DNN to guide the statistics estimation in both PF and BF stages. In the first stage, the multi-channel SPP (MC-SPP) estimate is used to update the estimates of the power density (PSD) matrices of the noise and clean signals. The steering vector is obtained using the covariance subtraction method to implement the BF. In the second stage, the output from the first stage and the observed signal of the reference microphone are integrated as the input of the second DNN to estimate the single-channel SPP of the observed signal from the reference microphone. With the single-channel SPP estimate, the noise PSD is updated using the minimum mean-squared error method without time frame smoothing. Finally, with the statistics estimate, one commonly used BF and PF are employed to extract the target speech from the observed signal. In this paper, the BF and PF are the minimum variance distortionless response and the log spectral amplitude estimator, respectively. One small DNN model for multi-channel speech enhancement is used to estimate SPP in both two stages which aims to improve the model adoption and effectiveness. The experimental results demonstrate that, compared to the end-to-end approaches, our proposed method achieved a better performance in terms of noise attenuation and speech distortion while maintaining a lower model complexity, measured in terms of both the number of parameters and Multiply-ACcumulate operations per second.},
  archive  = {J},
  author   = {Shuai Tao and Pejman Mowlaee and Jesper Rindom Jensen and Mads Græsbøoll Christensen},
  doi      = {10.1109/TASLPRO.2025.3599782},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3838-3849},
  title    = {Multi-channel speech enhancement guided by learning-based $A$ $Posteriori$ speech presence probability estimation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new method for speech enhancement based on harmonic distortion. <em>TASLPRO</em>, <em>33</em>, 3826-3837. (<a href='https://doi.org/10.1109/TASLPRO.2025.3608964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A near-end listening enhancement (NELE) method that combines harmonic (non-linear) distortion with linear distortion for improving speech intelligibility in noisy conditions without altering the signal-to-noise ratio (SNR) is proposed. Parameters were optimized across noise types, SNRs, and reverberation using a Genetic Algorithm with Speech Intelligibility in Bits (SIIB) as the loss function. Subjective experiments confirmed that, on average, the odds of correctly recognizing a word treated with the proposed method were \boldmath 18.8 higher relative to plain speech when presented at $-9$ dB SNR, averaged over the results of Cafeteria and Speech-Shaped Noise (SSN). In Cafeteria noise, the odds of correct recognition were 16.4 times higher compared to plain speech, while in SSN they were about 8.4 times higher, averaged across SNRs ranging from $-9$ to $-3$ dB. Objective evaluation further confirmed its superiority to alternative approaches in reverberant maskers. These results demonstrate that combining harmonic and linear distortions can substantially improve intelligibility in adverse listening conditions.},
  archive  = {J},
  author   = {Julián Villegas},
  doi      = {10.1109/TASLPRO.2025.3608964},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3826-3837},
  title    = {A new method for speech enhancement based on harmonic distortion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video-foley: Two-stage video-to-sound generation via temporal event condition for foley sound. <em>TASLPRO</em>, <em>33</em>, 3813-3825. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Foley sound synthesis is crucial for multimedia production, enhancing user experience by synchronizing audio and video both temporally and semantically. Recent studies on automating this labor-intensive process through video-to-sound generation face significant challenges. Systems lacking explicit temporal features suffer from poor alignment and controllability, while timestamp-based models require costly and subjective human annotation. We propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an intuitive condition with semantic timbre prompts (audio or text). RMS, a frame-level intensity envelope closely related to audio semantics, acts as a temporal event feature to guide audio generation from video. The annotation-free self-supervised learning framework consists of two stages, Video2RMS and RMS2Sound, incorporating mu-law scaled RMS discretization and RMS-ControlNet with a pretrained text-to-audio model. Our extensive evaluation shows that Video-Foley achieves state-of-the-art performance in audio-visual alignment and controllability for sound timing, intensity, timbre, and nuance.},
  archive  = {J},
  author   = {Junwon Lee and Jaekwon Im and Dabin Kim and Juhan Nam},
  doi      = {10.1109/TASLPRO.2025.3597477},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3813-3825},
  title    = {Video-foley: Two-stage video-to-sound generation via temporal event condition for foley sound},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IslamicPCQA: A dataset for persian multi-hop complex question answering in islamic text resources. <em>TASLPRO</em>, <em>33</em>, 3801-3812. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Today, one of the most significant challenges faced by Islamic question-answering systems is responding to multi-hop complex questions using various information sources. Multi-hop questions, which include bridge and comparative questions, are types of complex queries that require multi-step reasoning and the use of at least two paragraphs from different documents to answer. On the other hand, in recent years, most research in the field of answering multi-hop complex questions has focused on the English language, and limited effort has been made for low-resource languages, including Persian. To improve the efficiency of question-answering systems in supporting low-resource languages such as Persian, and to enhance the quality of responses to multi-hop complex questions, this paper introduces the Persian dataset “IslamicPCQA.” This dataset consists of 12,282 question-answer pairs and is the first Persian dataset for answering multi-hop complex questions, created solely using content from unstructured information sources, specifically nine Islamic encyclopedias. To create this dataset, a unique framework was developed, including stages of data collection, preprocessing, constructing a content recommendation graph infrastructure or hyperlinks graph, and annotating the data, which can be utilized for creating similar datasets in other low-resource languages for the task of answering multi-hop complex questions. To evaluate and benchmark this dataset, some of the best pre-trained language models that support Persian were fine-tuned using this dataset, and the results were reported based on two metrics: F1 score and Exact Match. The findings indicate that this dataset significantly enhances Persian question-answering systems, with the XLM-RoBERTa model achieving an F1 score of 80.44% and an Exact Match of 67.33%, outperforming other baseline models.},
  archive  = {J},
  author   = {Arash Ghafouri and Mohammad Aghajani Asl and Hasan Naderi and Mahdi Firouzmandi},
  doi      = {10.1109/TASLPRO.2025.3587450},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3801-3812},
  title    = {IslamicPCQA: A dataset for persian multi-hop complex question answering in islamic text resources},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning speech-text representations via contrastive modality translation. <em>TASLPRO</em>, <em>33</em>, 3787-3800. (<a href='https://doi.org/10.1109/TASLPRO.2025.3603910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advances in automatic speech recognition (ASR) have led to substantial improvements in system accuracy and robustness, particularly in converting speech signals into text sequences. Among prevailing ASR paradigms, both acoustic encoder architectures and end-to-end encoder-decoder frameworks have demonstrated strong performance across a variety of downstream tasks. However, challenges remain in achieving robust generalization, especially for the models trained solely on acoustic inputs. These limitations are exacerbated in scenarios involving complex linguistic content and variable acoustic conditions. To mitigate these issues, recent research has explored joint modeling of speech and text, aiming to embed both modalities into a shared representation space. While promising, such approaches often suffer from modality interference and suboptimal alignment, particularly under domain shifts. Notably, most existing evaluations have been conducted on neutral speech, with limited attention to robustness under emotional or affective speech conditions. This study proposes a contrastive modality translation framework to improve the alignment between speech and text representations within a unified embedding space. By leveraging cross-modal contrastive learning, the proposed method enhances ASR performance, especially in emotion-influenced speech scenarios. Empirical results demonstrate that our approach significantly improves recognition accuracy across both neutral and emotionally expressive speech, highlighting its effectiveness in addressing cross-modality and domain-shift challenges in ASR.},
  archive  = {J},
  author   = {Jen-Tzung Chien and Pin-Yen Liu},
  doi      = {10.1109/TASLPRO.2025.3603910},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3787-3800},
  title    = {Aligning speech-text representations via contrastive modality translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of catastrophic forgetting in large language models during continual fine-tuning. <em>TASLPRO</em>, <em>33</em>, 3776-3786. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs’ knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1 b to 7 b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model scale range, which may result from the much more significant initial performance in the larger LLM. The finding is also observed by the experiment of Qwen-2.5-Inst from 3 B to 14 B. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.},
  archive  = {J},
  author   = {Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
  doi      = {10.1109/TASLPRO.2025.3606231},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3776-3786},
  title    = {An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constituency parsing using LLMs. <em>TASLPRO</em>, <em>33</em>, 3762-3775. (<a href='https://doi.org/10.1109/TASLPRO.2025.3600867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Constituency parsing is a fundamental yet unsolved challenge in natural language processing. In this paper, we examine the potential of recent large language models (LLMs) to address this challenge. We reformat constituency parsing as a sequence-to-sequence generation problem and evaluate the performance of a diverse range of LLMs under zero-shot, few-shot, and supervised fine-tuning learning paradigms. We observe that while LLMs achieve acceptable improvements, they still encounter substantial limitations, due to the absence of mechanisms to guarantee the validity and faithfulness of the generated constituent trees. Motivated by this observation, we propose two strategies to guide LLMs to generate more accurate constituent trees by learning from erroneous samples and refining outputs in a multi-agent collaboration way, respectively. The experimental results demonstrate that our methods effectively reduce the occurrence of invalid and unfaithful trees, thereby enhancing overall parsing performance and achieving promising results across different learning paradigms.},
  archive  = {J},
  author   = {Xuefeng Bai and Jialong Wu and Yulong Chen and Zhongqing Wang and Kehai Chen and Min Zhang and Yue Zhang},
  doi      = {10.1109/TASLPRO.2025.3600867},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3762-3775},
  title    = {Constituency parsing using LLMs},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAMGFusion:A joint entity and relation extraction model based on hierarchical attention and feature fusion. <em>TASLPRO</em>, <em>33</em>, 3747-3761. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Joint entity and relation extraction remains a fundamental yet challenging task in natural language processing (NLP), often constrained by insufficient information fusion and severe class imbalance, especially under complex semantic structures. To address these issues, we propose HAMGFusion, a unified framework that leverages a Hierarchical Semantic Encoder to capture multi-level contextual dependencies and a Dynamic Feature Fusion Decoder to adaptively integrate fine-grained semantic features across layers. To further alleviate class imbalance, a Class-Aware Loss Fusion mechanism is introduced to dynamically calibrate loss weights and better align predictions with imbalanced label distributions. Extensive experiments on three benchmark datasets—NYT, WebNLG, and ADE—demonstrate that HAMGFusion achieves state-of-the-art F1 scores of 93.7%, 94.8%, and 86.4%, respectively. Ablation studies validate the contribution of each module, and visualizations of fusion dynamics offer insights into the model’s adaptive behavior. The proposed method shows strong potential for real-world applications, such as knowledge graph construction and biomedical information extraction, and contributes to advancing learning-based approaches for joint extraction tasks.},
  archive  = {J},
  author   = {Zhijie Li and Ke Wang and Changhua Li and Jiahui Zhang and Jinyong Chang and Wei Dong and Yuan Gao},
  doi      = {10.1109/TASLPRO.2025.3606190},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3747-3761},
  title    = {HAMGFusion:A joint entity and relation extraction model based on hierarchical attention and feature fusion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A finite difference framework for differential beamforming with planar microphone arrays. <em>TASLPRO</em>, <em>33</em>, 3732-3746. (<a href='https://doi.org/10.1109/TASLPRO.2025.3604660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Differential microphone arrays (DMAs) have demonstrated great advantages in high-fidelity acoustic signal acquisition and enhancement due to their capability to capture both the acoustic pressure and differential fields. While considerable progress has been made in differential beamforming with planar DMAs, most existing methods are rooted in beampattern approximation. In such approaches, the connection between differential beamformers and physical differential operations is usually implicit or confined to specific array topologies. To address this, this paper presents a finite difference framework for the analysis and design of planar DMAs. The main contributions of this work are threefold. First, we revisit the fundamentals of DMAs from an acoustic physics perspective, demonstrating how finite differences can be applied to planar arrays to approximate the directional derivatives of the acoustic pressure field. Second, based on this finite difference approximation, we extend the spatial difference operator (SDO) for linear DMAs to the two-dimensional (2D) case. The resulting 2D SDO is applied to the observation signal vector to extract differential signals, which are then filtered to optimize various performance metrics, enabling the design of different differential beamformers. Finally, we show that the 2D SDO can be obtained by solving a linear system of equations, offering a clear and systematic design procedure.},
  archive  = {J},
  author   = {Xudong Zhao and Gongping Huang and Jingdong Chen and Jacob Benesty and Zoran Cvetković},
  doi      = {10.1109/TASLPRO.2025.3604660},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3732-3746},
  title    = {A finite difference framework for differential beamforming with planar microphone arrays},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the robustness of contextual ASR to varying biasing information volumes through purified semantic correlation joint modeling. <em>TASLPRO</em>, <em>33</em>, 3720-3731. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, cross-attention-based contextual automatic speech recognition (ASR) models have made notable advancements in recognizing personalized biasing phrases. However, the effectiveness of cross-attention is affected by variations in biasing information volume, especially when the length of the biasing list increases significantly. We find that, regardless of the length of the biasing list, only a limited amount of biasing information is most relevant to a specific ASR intermediate representation. Therefore, by identifying and integrating the most relevant biasing information rather than the entire biasing list, we can alleviate the effects of variations in biasing information volume for contextual ASR. To this end, we propose a purified semantic correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and calculate three semantic correlations between the ASR intermediate representations and biasing information from coarse to fine: list-level, phrase-level, and token-level. Then, the three correlations are jointly modeled to produce their intersection, so that the most relevant biasing information across various granularities is highlighted and integrated for contextual recognition. In addition, to reduce the computational cost introduced by the joint modeling of three semantic correlations, we also propose a purification mechanism based on a grouped-and-competitive strategy to filter out irrelevant biasing phrases. Compared with baselines, our PSC-Joint approach achieves average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46% on KeSpeech, across biasing lists of varying lengths.},
  archive  = {J},
  author   = {Yue Gu and Zhihao Du and Ying Shi and Shiliang Zhang and Qian Chen and Jiqing Han},
  doi      = {10.1109/TASLPRO.2025.3606198},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3720-3731},
  title    = {Enhancing the robustness of contextual ASR to varying biasing information volumes through purified semantic correlation joint modeling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QHARMA-GAN: Quasi-harmonic neural vocoder based on autoregressive moving average model. <em>TASLPRO</em>, <em>33</em>, 3703-3719. (<a href='https://doi.org/10.1109/TASLPRO.2025.3603847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Vocoders, encoding speech signals into acoustic features and allowing for speech signal reconstruction from them, have been studied for decades. Recently, the rise of deep learning has particularly driven the development of neural vocoders to generate high-quality speech signals. On the other hand, the existing end-to-end neural vocoders suffer from a lack of interpretability nature that blinds the speech production mechanism and the intrinsic structure of speech, resulting in the ambiguity of separately modeling source excitation and resonance characteristics and the loss of flexibly synthesizing or modifying speech with high quality. Moreover, their sequence-wise waveform generation usually requires complicated networks, leading to substantial time consumption. In this work, inspired by the quasi-harmonic model (QHM) that represents speech as sparse components, we combine the neural network and QHM synthesis process to propose a novel framework for the neural vocoder. Accordingly, speech signals can be encoded into autoregressive moving average (ARMA) functions to model the resonance characteristics, yielding accurate estimates of the amplitudes and phases of quasi-harmonics at any frequency. Subsequently, the speech can be resynthesized and arbitrarily modified in terms of pitch shifting and time stretching with high quality, whereas the time consumption and network size decrease. The experiments indicate that the proposed method leverages the strengths of QHM, the ARMA model, and neural networks, leading to the outperformance of our methods over other methods in terms of generation speed, synthesis quality, and modification flexibility.},
  archive  = {J},
  author   = {Shaowen Chen and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2025.3603847},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3703-3719},
  title    = {QHARMA-GAN: Quasi-harmonic neural vocoder based on autoregressive moving average model},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to mimic linguistic features of transcripts of atypical speech. <em>TASLPRO</em>, <em>33</em>, 3690-3702. (<a href='https://doi.org/10.1109/TASLPRO.2025.3603921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is often infeasible and costly to obtain speech data from individuals who exhibit atypical linguistic patterns. Although there have been considerable advances in the field of text generation, concerns remain about the potential leakage of identifiable information when generating data from patient speech transcripts. In response to this challenge, we present a novel approach to mimic the linguistic patterns found in transcripts of atypical speech. Our method estimates the probability distributions of linguistic features, devoid of sensitive information and unique to atypical typical speech. With these probability distributions, we create a mimicked atypical transcript by modifying selected content of a transcript of typical speech. To evaluate this mimicry strategy, three testing scenarios were considered: baseline test, where a classifier was trained and tested on real data; cross-data test, where a classifier was trained on real data and tested on mimicked data; and synthetic-boost test, where a classifier was trained on mimicked data and tested on real data. The real data consisted of non-parallel annotated spoken transcripts of cognitively intact individuals (typical speech) and those with probable Alzheimer’s disease (AD; atypical speech). In the synthetic-boost test, a Bidirectional Encoder Representations from Transformers (BERT) model classified the real data with an accuracy of 80.25%, exceeding the 75.86% accuracy achieved in the baseline test. This finding indicates that the mimicked data significantly enhanced the model’s ability to detect atypical Alzheimer’s speech. Accuracy significantly decreased when excluding shortening (63%, F1 score = 42%) and communicator (61%, F1 score= 41%) features from the mimicry process, underscoring their essential role in AD speech detection. We additionally compared our mimicry strategy with a GPT-4-o prompt-engineering baseline. The LLM-generated data yielded 73.9 % classification accuracy, 6.3 percentage points lower than mimicry, while exhibiting occasional hallucinated names/locations.},
  archive  = {J},
  author   = {Mahya Mirgbagheri and Hamidreza Saghir and Tom Chau},
  doi      = {10.1109/TASLPRO.2025.3603921},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3690-3702},
  title    = {A novel approach to mimic linguistic features of transcripts of atypical speech},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging content and acoustic representations for speech emotion recognition. <em>TASLPRO</em>, <em>33</em>, 3678-3689. (<a href='https://doi.org/10.1109/TASLPRO.2025.3603853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER), the task of identifying the expression of emotion from spoken content, is challenging due to the difficulty in extracting representations that capture emotional attributes. The scarcity of labeled datasets further complicates the challenge where large models are prone to over-fitting. In this paper, we propose CARE (Content and Acoustic Representations of Emotions), where we design a dual encoding scheme which emphasizes semantic and acoustic factors of speech. While the semantic encoder is trained using distillation from utterance-level text representations, the acoustic encoder is trained to predict low-level frame-wise features of the speech signal. The proposed dual encoding scheme is a base-sized model trained only on unsupervised raw speech. With a simple light-weight classification model trained on the downstream task, we show that the CARE embeddings provide effective emotion recognition on a variety of datasets. We compare the proposal with several other self-supervised models as well as recent large-language model based approaches. In these evaluations, the proposed CARE is shown to be the best performing model based on average performance across 8 diverse datasets. We also conduct several ablation studies to analyze the importance of various design choices.},
  archive  = {J},
  author   = {Soumya Dutta and Sriram Ganapathy},
  doi      = {10.1109/TASLPRO.2025.3603853},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3678-3689},
  title    = {Leveraging content and acoustic representations for speech emotion recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theoretical framework for envelope-based time-frequency masks in cochlear implant noise reduction applications. <em>TASLPRO</em>, <em>33</em>, 3665-3677. (<a href='https://doi.org/10.1109/TASLPRO.2025.3599773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work presents a theoretical framework for designing envelope-based time-frequency masks (TFM) in cochlear implant (CI) noise-reduction applications. A cost function is proposed with the aim of achieving the ideal envelope of the desired speech from an estimate of the noise-corrupted speech envelope. Real-world envelope estimation inaccuracies arising from the truncation of the discrete Hilbert filter are explicitly considered. Two new optimal TFMs were derived. They reduce to the square root Wiener (SRW) and Chiea and Costa (C2F) masks under ideal conditions, providing a unifying theoretical framework. Objective metrics and psychoacoustic experiments with ten normal-hearing (NH) subjects and three CI users were employed to evaluate the performance of several TFMs under near real-world conditions, considering limitations in envelope estimation accuracy and processing latency. The proposed TFMs achieve lower mean envelope errors and improve objective intelligibility scores by up to 3.4% compared to SRW and C2F, where the maximum improvement occurred at a signal-to-noise ratio (SNR) of −5 dB. Psychoacoustic experiments reveal intelligibility gains of up to 6.7% for CI users, where the utmost betterment occurred at an SNR = 0 dB. The proposed theoretical framework led to optimal envelope-based TFMs that converge to the classic SRW and C2F methods under ideal signal envelope estimation. This work provides a compelling method for designers of high-performance noise reduction systems in CIs, potentially improving real-world intelligibility.},
  archive  = {J},
  author   = {Paulo Henrique Gubert and Bruno Catarino Bispo and Márcio Holsbach Costa},
  doi      = {10.1109/TASLPRO.2025.3599773},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3665-3677},
  title    = {A theoretical framework for envelope-based time-frequency masks in cochlear implant noise reduction applications},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuQ: Self-supervised music representation learning with mel residual vector quantization. <em>TASLPRO</em>, <em>33</em>, 3653-3664. (<a href='https://doi.org/10.1109/TASLPRO.2025.3602320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9 K hours of open-source pre-training data. Scaling up the data to over 160 K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset.},
  archive  = {J},
  author   = {Haina Zhu and Yizhi Zhou and Hangting Chen and Jianwei Yu and Ziyang Ma and Rongzhi Gu and Yi Luo and Wei Tan and Xie Chen},
  doi      = {10.1109/TASLPRO.2025.3602320},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3653-3664},
  title    = {MuQ: Self-supervised music representation learning with mel residual vector quantization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCF-DS: Deep cascade fusion of diarization and separation for speech recognition under realistic single-channel conditions. <em>TASLPRO</em>, <em>33</em>, 3638-3652. (<a href='https://doi.org/10.1109/TASLPRO.2025.3600913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end automatic speech recognition (ASR), combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art single-channel speech recognition performance.},
  archive  = {J},
  author   = {Shu-Tong Niu and Jun Du and Ruo-Yu Wang and Gao-Bin Yang and Tian Gao and Jia Pan and Yu Hu},
  doi      = {10.1109/TASLPRO.2025.3600913},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3638-3652},
  title    = {DCF-DS: Deep cascade fusion of diarization and separation for speech recognition under realistic single-channel conditions},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online sampling rate offset estimation via real part maximization. <em>TASLPRO</em>, <em>33</em>, 3623-3637. (<a href='https://doi.org/10.1109/TASLPRO.2025.3599776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Distributed acoustic sensor networks consist of independent acoustic nodes, each with its own clock, which introduces inherent clock skew issues, creating several challenges when processing acoustic and speech signals. Specifically, the sampling rate offset (SRO) caused by clock skew leads to time-varying phase drift between the output signals of different nodes. This phase drift disrupts signal coherence, impairing the performance of downstream algorithms such as echo cancellation, blind source separation, and beamforming. Despite substantial progress in SRO estimation and compensation, existing online SRO estimation methods still encounter limitations in low SNR or naturally alternating conversational environments. To address these challenges, we propose a novel online SRO estimation method, formulated on the fundamental two-node case, with the potential for extension to multi-node scenarios. First, we reconsider phase drift in the normalized cross-spectrum (NCS), taking into account noise and interfering signals, and investigate the statistical properties of the average secondary NCS (SNCS). Then, we formulate an objective function based on real part maximization, which attains its maximum when the estimated SRO aligns with the true value. To enable online estimation, we replace the averaged SNCS with a recursively smoothed version in the objective function, and employ Nesterov accelerated gradient for efficient optimization. Both simulations and experiments validate the effectiveness of the proposed method1.},
  archive  = {J},
  author   = {Shanzheng Guan and Jianyu Wang and Mou Wang and Jingdong Chen and Jacob Benesty},
  doi      = {10.1109/TASLPRO.2025.3599776},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3623-3637},
  title    = {Online sampling rate offset estimation via real part maximization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuro-fuzzy voice quality improvement for low-power, half-duplex, communication. <em>TASLPRO</em>, <em>33</em>, 3609-3622. (<a href='https://doi.org/10.1109/TASLPRO.2025.3599778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Half-duplex mobile radio systems are commonly used for speech communication, often in challenging environments where background noise may prevent speech intelligibility. These systems usually operate with modest battery capacity which strictly limits the available computational power. In this paper, we present a noise suppression approach that uses lightweight machine learning. The machine learning leverages a neuro-fuzzy logic-based neural network to create accurate noise estimation that is used adaptively to create a filter for noise reduction. The system buffers a number of noise samples, triggered by the half-duplex key press, allowing the solution to adapt to recent changes in the background auditory noise. The selection of a neuro-fuzzy logic-based neural network is driven by the necessity for a low-power implementation suitable for mobile, power-constrained terminals. The proposed system is compared with both traditional noise suppression methods as well as with a deep convolutional neural network (DCNN) variant of our system. The results demonstrate that our system significantly outperforms traditional methods in terms of both subjective and objective quality metrics. Further, they show that, although the DCNN variant achieves comparable performance, it has a high computational cost which renders it unsuitable for low-power digital personal radio systems. To validate its practicality, the proposed method is tested in a real-time system, demonstrating its compatibility with constrained devices.},
  archive  = {J},
  author   = {Joyraj Chakraborty and Martin Reed and Nikolaos Thomos and Geoff Pratt and Nigel Wilson},
  doi      = {10.1109/TASLPRO.2025.3599778},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3609-3622},
  title    = {Neuro-fuzzy voice quality improvement for low-power, half-duplex, communication},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech prediction in ANC headphones for improved attenuation: New methods and perceptual study. <em>TASLPRO</em>, <em>33</em>, 3594-3608. (<a href='https://doi.org/10.1109/TASLPRO.2025.3598570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In environments such as open offices and co-working spaces, the adverse effects of speech as a primary source of ambient noise extend beyond mere annoyance, contributing to reduced productivity, increased workload and stress. When dealing with speech, active noise control (ANC) systems have difficulties arising from constraints within the ANC system alongside the non-stationary nature of speech. These constraints require the optimal filters to be non-causal, creating a prediction problem. The non-causality is due to the delay incurred by, e.g., digital processing or acoustic propagation paths. To deal with this, we propose prediction-based feedforward ANC for headphone applications with improved voiced speech attenuation. The proposed methods leverage various aspects of speech characteristics and structure, including an optimal linear prediction (LP) scheme based on short- and long-term speech correlations, sparse LP modelling, harmonic and harmonic-chirp model-based prediction to account the non-stationarity of speech, and multiple-frequency ANC with harmonic decomposition of speech. Simulation results demonstrate the superior performance of the proposed methods compared to conventional adaptive feedforward ANC across a wide range of delays, spanning from 1 to 60 samples at a sampling frequency of 48 kHz (0.02 – 1.25 ms). The proposed methods achieve attenuation improvements of up to 8 dB and significantly extend attenuation to higher frequencies. Results from the perceptual study assessing subjective satisfaction with prediction-based ANC reveal that the artifacts from speech prediction have a more significant impact on satisfaction compared to attenuation.},
  archive  = {J},
  author   = {Yurii Iotov and Rasmus Elofsson and Sidsel Marie Nørholm and Peter John McCutcheon and Mads Græsbøll Christensen},
  doi      = {10.1109/TASLPRO.2025.3598570},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3594-3608},
  title    = {Speech prediction in ANC headphones for improved attenuation: New methods and perceptual study},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHMamba: Structured hyperbolic state space model for audio-visual question answering. <em>TASLPRO</em>, <em>33</em>, 3582-3593. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Audio-Visual Question Answering (AVQA) task holds significant potential for applications. Compared to traditional unimodal approaches, the multi-modal input of AVQA makes feature extraction and fusion processes more challenging. Euclidean space is difficult to effectively represent multi-dimensional relationships of data. Especially when extracting and processing data with a tree structure or hierarchical structure, Euclidean space is not suitable as an embedding space. Additionally, the self-attention mechanism in Transformers is effective in capturing the dynamic relationships between elements in a sequence. However, the self-attention mechanism’s limitations in window modeling and quadratic computational complexity reduce its effectiveness in modeling long sequences. To address these limitations, we propose SHMamba: Structured Hyperbolic State Space Model to integrate the advantages of hyperbolic geometry and state space models. Specifically, SHMamba leverages the intrinsic properties of hyperbolic space to represent hierarchical structures and complex relationships in audio-visual data. Meanwhile, the state space model captures dynamic changes over time by globally modeling the entire sequence. Furthermore, we introduce a cross fusion block to enhance the understanding of hierarchical structures and the dynamic exchange of cross-modal information. Extensive experiments demonstrate that SHMamba outperforms previous methods with fewer parameters and computational costs. Our learnable parameters are reduced by 78.12%, while the average performance improves by 2.53%. Experiments show that our method demonstrates superiority among all current major methods and is more suitable for practical application scenarios.},
  archive  = {J},
  author   = {Zhe Yang and Wenrui Li and Guanghui Cheng},
  doi      = {10.1109/TASLPRO.2025.3597461},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3582-3593},
  title    = {SHMamba: Structured hyperbolic state space model for audio-visual question answering},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LS-EEND: Long-form streaming end-to-end neural diarization with online attractor extraction. <em>TASLPRO</em>, <em>33</em>, 3568-3581. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work proposes a frame-wise online/streaming end-to-end neural diarization (EEND) method, which detects speaker activities in a frame-in-frame-out fashion. The proposed model mainly consists of a causal embedding encoder and an online attractor decoder. Speakers are modelled in the self-attention-based decoder along both the time and speaker dimensions, and frame-wise speaker attractors are automatically generated and updated for new speakers and existing speakers, respectively. Retention mechanism is employed and especially adapted for long-form diarization with a linear temporal complexity. A multi-step progressive training strategy is proposed for gradually learning from easy tasks to hard tasks in terms of the number of speakers and audio length. Finally, the proposed model (referred to as long-form streaming EEND, LS-EEND) is able to perform streaming diarization for a high (up to 8) and flexible number speakers and very long (say one hour) audio recordings. Experiments on various simulated and real-world datasets show that: 1) when not using oracle speech activity information, the proposed model achieves new state-of-the-art online diarization error rate on all datasets, including CALLHOME (12.11%), DIHARD II (27.58%), DIHARD III (19.61%), and AMI (20.76%); 2) Due to the frame-in-frame-out processing fashion and the linear temporal complexity, the proposed model achieves several times lower real-time-factor than comparison online diarization models.},
  archive  = {J},
  author   = {Di Liang and Xiaofei Li},
  doi      = {10.1109/TASLPRO.2025.3597446},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3568-3581},
  title    = {LS-EEND: Long-form streaming end-to-end neural diarization with online attractor extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical curriculum learning for dysarthric speech recognition via multi-level knowledge distillation. <em>TASLPRO</em>, <em>33</em>, 3553-3567. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic speech recognition (ASR) for dysarthric speech faces three major challenges: (1) data scarcity, (2) variability in speech patterns, and (3) imbalanced performance across different speech intelligibility groups. To primarily address the performance imbalance, we propose a Hierarchical Curriculum Learning (HCL) framework consisting of: (a) a hierarchical architecture that trains separate ASR models for each intelligibility group to reduce cross-group interference, (b) a multi-level knowledge distillation strategy that progressively transfers knowledge from higher to lower intelligibility groups, and (c) a fusion model that integrates predictions from group-specific ASR models. To further tackle data scarcity and variability, the proposed method incorporates meta-learning, intelligibility classification, and data augmentation tailored for dysarthric speech. Unlike prior work that adopts uniform training strategies, our approach explicitly models inter-group differences and enables effective knowledge sharing. Experiments on the UASpeech and Torgo corpus show that the proposed method achieves average WERs of 19.44% and 6.12%, respectively, with consistent and statistically significant improvements across all intelligibility groups.},
  archive  = {J},
  author   = {I-Ting Hsieh and Chung-Hsien Wu},
  doi      = {10.1109/TASLPRO.2025.3597438},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3553-3567},
  title    = {Hierarchical curriculum learning for dysarthric speech recognition via multi-level knowledge distillation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active learning-based alternative reinforcement contextual information fusion model for multimodal sentiment analysis. <em>TASLPRO</em>, <em>33</em>, 3537-3552. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal Sentiment Analysis (MSA) combines data from multiple modalities to accurately interpret human emotional states. Despite recent advancements, deep learning-based MSA models continue to face critical challenges: (1) high costs and resource demands associated with large-scale data annotation, (2) limited effectiveness in capturing global intra-modal context in single-modality feature extraction, and (3) underutilization of complementary cross-modal information in inter-modal interactions. To address these challenges, we propose an innovative model, the Active Learning-based Aternative Reinforcement Contextual Information Fusion (AL-ARCF) model, designed for MSA tasks. Our approach introduces two novel active learning criteria: an abundance criterion, derived from gradient magnitude, and an availability criterion, both aimed at minimizing labeling costs without compromising model performance. Inspired by human learning principles, we also incorporate curriculum learning, gradually increasing learning complexity by dynamically balancing sample difficulty and active learning effectiveness. For enhanced intra-modal context extraction, we propose the Enhanced Global Context Extraction (EGCE) module, which captures detailed spatio-temporal features within unimodal data. To optimize cross-modal interactions, we introduce an Alternative Feature Interaction (AFI) module, leveraging relevance-based feature classification and cross-modal multi-head attention to fully exploit low-level feature correlations across modalities. Extensive experiments on standard MSA benchmarks, the CMU-MOSI,the CMU-MOSEI, and the CH-SIMS datasets, demonstrate that AL-ARCF achieves superior performance compared to existing models, verifying the proposed framework’s effectiveness and robustness.},
  archive  = {J},
  author   = {Xiaojiang He and Yushan Pan and Yangbin Chen and Zuhe Li and Zhijie Xu and Chenguang Yang and Kaiwei Wang},
  doi      = {10.1109/TASLPRO.2025.3597474},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3537-3552},
  title    = {An active learning-based alternative reinforcement contextual information fusion model for multimodal sentiment analysis},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-input multi-output target-speaker voice activity detection for unified, flexible, and robust audio-visual speaker diarization. <em>TASLPRO</em>, <em>33</em>, 3522-3536. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio-visual learning has demonstrated promising results in many classical speech tasks (e.g., speech separation, automatic speech recognition, wake-word spotting). We believe that introducing visual modality will also benefit speaker diarization. To date, Target-Speaker Voice Activity Detection (TS-VAD) plays an important role in highly accurate speaker diarization. However, previous TS-VAD models take audio features and utilize the speaker’s acoustic footprint to distinguish his or her personal speech activities, which is easily affected by overlapped speech in multi-speaker scenarios. Although visual information naturally tolerates overlapped speech, it suffers from spatial occlusion, low resolution, etc. The potential modality-missing problem blocks TS-VAD towards an audio-visual approach. This paper proposes a novel Multi-Input Multi-Output Target-Speaker Voice Activity Detection (MIMO-TSVAD) framework for speaker diarization. The proposed method can take audio-visual input and leverage the speaker’s acoustic footprint or lip track to flexibly conduct audio-based, video-based, and audio-visual speaker diarization in a unified sequence-to-sequence framework. Experimental results show that the MIMO-TSVAD framework demonstrates state-of-the-art performance on the VoxConverse, DIHARD-III, and MISP 2022 datasets under corresponding evaluation metrics, obtaining the Diarization Error Rates (DERs) of 4.18%, 10.10%, and 8.15%, respectively. In addition, it can perform robustly in heavy lip-missing scenarios.},
  archive  = {J},
  author   = {Ming Cheng and Ming Li},
  doi      = {10.1109/TASLPRO.2025.3597450},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3522-3536},
  title    = {Multi-input multi-output target-speaker voice activity detection for unified, flexible, and robust audio-visual speaker diarization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing contextual speech recognition using vector quantization for efficient retrieval. <em>TASLPRO</em>, <em>33</em>, 3508-3521. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural contextual biasing allows speech recognition models to leverage contextually relevant information, leading to improved transcription accuracy. However, the biasing mechanism is typically based on a cross-attention module between the audio and a catalogue of biasing entries, which means computational complexity can pose severe practical limitations on the size of the biasing catalogue and consequently on accuracy improvements. This work proposes an approximation to cross-attention scoring based on vector quantization and enables compute- and memory-efficient use of large biasing catalogues. We propose to use this technique jointly with a retrieval based contextual biasing approach. First, we use an efficient quantized retrieval module to shortlist biasing entries by grounding them on audio. Then we use retrieved entries for biasing. Since the proposed approach is agnostic to the biasing method, we investigate using full cross-attention, LLM prompting, and a combination of the two. We show that retrieval based shortlisting allows the system to efficiently leverage biasing catalogues of several thousands of entries, resulting in up to 71% relative error rate reduction in personal entity recognition. At the same time, the proposed approximation algorithm reduces compute time by 20% and memory usage by 85-95%, for lists of up to one million entries, when compared to standard dot-product cross-attention.},
  archive  = {J},
  author   = {Nikolaos Flemotomos and Roger Hsiao and Pawel Swietojanski and Takaaki Hori and Dogan Can and Xiaodan Zhuang},
  doi      = {10.1109/TASLPRO.2025.3594955},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3508-3521},
  title    = {Optimizing contextual speech recognition using vector quantization for efficient retrieval},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving speaker verification robustness with multilingual phonetic information and feature decorrelation. <em>TASLPRO</em>, <em>33</em>, 3494-3507. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In automatic speaker verification (ASV) systems, the representational capability of speaker embeddings is critical to system performance. Existing studies have shown that introducing phonetic information during speaker embedding extraction can achieve phonetic alignment, thereby enhancing discriminative ability. However, current methods typically rely on monolingual phonetic information, which limits the effectiveness of phonetic alignment when training models on multilingual ASV datasets. Moreover, as the parameters of the ASV model increase, the extracted speaker embeddings may include both speaker-related features and speaker-independent features influenced by the training data. The correlations among these features can increase the risk of overfitting in the model. To address these challenges, this paper proposes a method that combines multilingual phonetic information (MuPI) and feature decorrelation (FD), termed MuPI-FD. Specifically, we analyze the language distribution in the training data and train three multilingual automatic speech recognition (ASR) models with different architectures. Using transfer learning, we incorporate multilingual phonetic information into ASV systems to achieve frame-level phonetic alignment. Additionally, to ensure that the extracted speaker embeddings focus more on speaker-related features, we introduce Random Fourier Features (RFF) and a sample reweighting method to reduce correlations among speaker embedding features. Experimental results demonstrate that ASV systems introducing multilingual phonetic information significantly outperform those using monolingual phonetic information on the VoxCeleb, SITW, and CN-Celeb datasets. Furthermore, ASV systems processed with feature decorrelation exhibit stronger generalization capabilities. Compared to the MFA-Conformer baseline system, MuPI-FD achieves an average relative improvement of 31.89% in EER across all test sets.},
  archive  = {J},
  author   = {Zhida Song and He Cai and Xin Chen and Liang He},
  doi      = {10.1109/TASLPRO.2025.3597456},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3494-3507},
  title    = {Improving speaker verification robustness with multilingual phonetic information and feature decorrelation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of geometrical acoustics model simplification on binaural reverberation. <em>TASLPRO</em>, <em>33</em>, 3480-3493. (<a href='https://doi.org/10.1109/TASLPRO.2025.3597463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Virtual reverberation rendering typically requires detailed information about room geometry and surface acoustic characteristics. While comprehensive modeling approaches can account for all aspects of an acoustic environment, they often incur high computational cost that may be perceptually unnecessary. Therefore, finding a trade-off between perceptual authenticity and model complexity becomes a relevant challenge. This study investigates such compromise through the use of geometrical acoustics to render Ambisonics-based binaural reverberation. The accuracy of the rendering is determined, among other factors, by its fidelity to the room’s geometry and to the acoustic properties of its materials. In particular, the model fidelity is varied by simplifying the room geometry and frequency resolution of the absorption coefficients. Several decimated models based on a single room were perceptually evaluated using a multi-stimulus comparison method. Additionally, these differences were numerically assessed through the comparison of acoustic parameters of the rendered reverberation. According to numerical and perceptual evaluations, lowering the frequency resolution of absorption coefficients can have a significant impact on the perception of reverberation, while a smaller impact was observed when decimating the geometry of the model.},
  archive  = {J},
  author   = {Vincent Martin and Isaac Engel and Lorenzo Picinali},
  doi      = {10.1109/TASLPRO.2025.3597463},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3480-3493},
  title    = {Effects of geometrical acoustics model simplification on binaural reverberation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring stability-plasticity trade-offs for continual named entity recognition. <em>TASLPRO</em>, <em>33</em>, 3465-3479. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continual Named Entity Recognition (CNER) is an evolving field that focuses on sequentially updating an existing model to incorporate new entity types. Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve prior knowledge and overcome catastrophic forgetting, strictly ensuring that the representations of old and new models remain consistent. Consequently, they often impart the model with excessive stability (i.e., retention of old knowledge) but limited plasticity (i.e., acquisition of new knowledge). To address this issue, we propose a Stability-Plasticity Trade-off (SPT) method for CNER that balances these aspects from both representation and weight perspectives. From the representation perspective, we introduce a pooling operation into the original KD, permitting a level of plasticity by consolidating representation dimensions. From the weight perspective, we dynamically merge the weights of old and new models, strengthening old knowledge while maintaining new knowledge. During this fusion, we implement a weight-guided selective mechanism to prioritize significant weights. Moreover, we develop a confidence-based pseudo-labeling approach for the current non-entity type, which predicts entity types using the old model to handle the semantic shift of the non-entity type, a challenge specific to CNER that has largely been ignored by previous methods. Extensive experiments across ten CNER settings on three benchmark datasets demonstrate that our SPT method surpasses previous CNER approaches, highlighting its effectiveness in achieving a suitable stability-plasticity trade-off.},
  archive  = {J},
  author   = {Duzhen Zhang and Chenxing Li and Jiahua Dong and Qi Liu and Dong Yu},
  doi      = {10.1109/TASLPRO.2025.3592317},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3465-3479},
  title    = {Exploring stability-plasticity trade-offs for continual named entity recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MT2KD: Toward a general-purpose encoder for speech, speaker, and audio events. <em>TASLPRO</em>, <em>33</em>, 3454-3464. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With advances in deep learning, the performance of end-to-end single-task models for speech and audio processing has been constantly improving. However, it is challenging to build a general-purpose model with high performance on multiple tasks, since different speech and audio processing tasks usually require different training data, input features, or model architectures to achieve optimal performance. In this work, MT2KD, a novel two-stage multi-task learning framework is proposed to build a general-purpose speech and audio encoder that jointly performs three fundamental tasks: automatic speech recognition (ASR), audio tagging (AT) and speaker verification (SV). In the first stage, multi-teacher knowledge distillation (KD) is applied to align the feature spaces of three single-task high-performance teacher encoders into a single student encoder using the same unlabelled data. In the second stage, multi-task supervised fine-tuning is carried out by initialising the model from the first stage and training on the separate labelled data of each single task. Experiments demonstrate that the proposed multi-task training pipeline significantly outperforms a baseline model trained with multi-task learning from scratch. The final system achieves good performance on ASR, AT and SV: with less than 4% relative word-error-rate increase on ASR, only 1.9 lower mean averaged precision on AT and 0.23% absolute higher equal error rate on SV compared to the best-performing single-task encoders, using only a total of 66 M model parameters.},
  archive  = {J},
  author   = {Xiaoyu Yang and Qiujia Li and Chao Zhang and Philip C. Woodland},
  doi      = {10.1109/TASLPRO.2025.3594300},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3454-3464},
  title    = {MT2KD: Toward a general-purpose encoder for speech, speaker, and audio events},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and accurate incomplete utterance rewriting. <em>TASLPRO</em>, <em>33</em>, 3443-3453. (<a href='https://doi.org/10.1109/TASLP.2023.3284519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Omission and reference frequently occur in dialogues, which complicate the semantic structure of the dialogue and hinder the understanding of dialogue systems. Facing the challenge, researchers propose a new task, Incomplete Utterance Rewriting (IUR). IUR systems supplement information for utterances in a dialogue based on the previous context. By utilizing the substitutive nature of IUR, action-based models have recently led to much development on IUR compared to conventional sequence-to-sequence models. The current state-of-the-art method models IUR in a connected region locating scenario. As a result, the heavy burden for training and decoding hinders the efficiency significantly, especially when the dialogue context length increases. Thus, towards faster and more accurate IUR, this task should be modeled more naturally in a pair locating form where pairwise scorers locate pairs representing span and substitution relations. Following this idea, we propose a Cross Scorer Sharing (XSS) model to initially support pair locating. We experiment with XSS on both English and Chinese IUR datasets, and results have shown that our model leads to comparable or better performance than the previous state-of-the-art. For efficiency, our method is 132% faster when training and 85.1% faster when predicting.},
  archive  = {J},
  author   = {Letian Peng and Zuchao Li and Hai Zhao},
  doi      = {10.1109/TASLP.2023.3284519},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3443-3453},
  title    = {Fast and accurate incomplete utterance rewriting},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal relation extraction enhanced by out-of-text visual information. <em>TASLPRO</em>, <em>33</em>, 3428-3442. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-modal relation extraction (MRE) aims to extract semantic relations between two textual entities with the help of visual information. Existing studies typically leverage visual information by directly detecting objects in images or retrieving additional information based on the target text or image. However, we demonstrate through extensive experiments that the key to performance improvement brought by most existing methods is not the visual content, but the dataset bias — such as inconsistent entity type annotations or imbalanced counts of visual objects across samples. In other words, the visual modality has not been effectively utilized in existing studies. To address this issue, in this paper, we propose a novel paradigm to efficiently leverage visual information and achieve stable performance gains. Different from existing methods, the core idea of the proposed paradigm is that the introduced visual information should supplement the key knowledge beyond the text. Specifically, we first use prior knowledge that is not explicitly mentioned in the text as key clues to retrieve relevant images that contain out-of-text information. Then, an image filtering mechanism is designed to ensure the diversity and effectiveness of visual information. Finally, we design a multimodal fusion method based on the attention mechanism to accurately extract relationships between entities. Experimental results on widely used datasets show that our proposed method can steadily improve the performance of textual extraction, and outperforms existing baselines.},
  archive  = {J},
  author   = {Yu-Ming Shang and Tao Wang and Xi Zhang},
  doi      = {10.1109/TASLPRO.2025.3589892},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3428-3442},
  title    = {Multi-modal relation extraction enhanced by out-of-text visual information},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic self-similarity based form labelling of classical-period piano sonata movements from audio recordings. <em>TASLPRO</em>, <em>33</em>, 3414-3427. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Musical form refers to the overall structure or organisation of a musical composition. It is a complex and high-level property of music that requires musical training to identify. A review of previous research in this field indicates that the focus has been on the task of detecting section boundaries and that automatic audio based form label recognition is a field of study that remains largely unexplored. This study explores the complex task of automatically determining musical form from audio. It demonstrates the ability of a novel methodology to label eight different form types that occur in the movements of Classical-period piano sonatas. The methodology makes use of self-similarity matrices, generated from features extracted from raw audio, as input to a convolutional neural network. The superiority of our approach was confirmed by evaluating it against a neural network model based on state-of-the-art features. We also report an evaluation of self-similarity matrices based on automatically transcribed piano rolls for the task of form recognition. Piano rolls are demonstrated to be superior for this application when compared to a range of other feature representations. Additionally, the performance of the model is shown to be robust in handling variations in performer choices. These range from different interpretations of the same score to actual deviations from the score where performers may elect to play or not to play notated repeats thus highlighting its ability to generalise across different performances of the same piece.},
  archive  = {J},
  author   = {Paul AD Burger and J Pieter Jacobs},
  doi      = {10.1109/TASLPRO.2025.3594301},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3414-3427},
  title    = {Automatic self-similarity based form labelling of classical-period piano sonata movements from audio recordings},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind localization of early room reflections based on microphone arrays and reverberant speech. <em>TASLPRO</em>, <em>33</em>, 3401-3413. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Blindly estimating the direction of arrival (DoA) of early room reflections without prior knowledge of the room impulse response or source signal is highly valuable in audio signal processing applications. The FF-PHALCOR (Frequency Focusing PHase ALigned CORrelation) method was recently developed for this purpose, extending the original PHALCOR method to work with arbitrary arrays rather than just spherical ones. Previous studies have provided only initial insights into its performance. This study offers a comprehensive analysis of the method’s performance and limitations, examining the performance of the FF-PHALCOR algorithm relative to the theoretical Cramér-Rao Lower Bound (CRLB) and a baseline Coherent Signal-Subspace MUSIC (CSS-MUSIC) algorithm, also examining how reflection characteristics such as delay, amplitude, and spatial density affect its effectiveness. The research also proposes improvements to overcome these limitations, enhancing detection quality and reducing false alarms. Furthermore, performance with real-world data was presented, while in addition, the study examined how spatial perception is affected by generating room impulse responses using estimated reflection information. The findings suggest a perceptual advantage of the proposed approach over the baseline, with particularly high perceptual quality when using the spherical array with 32 microphones. However, the quality is somewhat reduced when using a semi-circular array with only 6 microphones.},
  archive  = {J},
  author   = {Yogev Hadadi and Hanan Beit-On and Vladimir Tourbabin and Zamir Ben-Hur and David Lou Alon and Boaz Rafaely},
  doi      = {10.1109/TASLPRO.2025.3594297},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3401-3413},
  title    = {Blind localization of early room reflections based on microphone arrays and reverberant speech},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECQED: Emotion-cause quadruple extraction in dialogs. <em>TASLPRO</em>, <em>33</em>, 3391-3400. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The existing emotion-cause pair extraction (ECPE) task, unfortunately, ignores extracting the emotion type and cause type, while these fine-grained meta-information can be practically useful in real-world applications, i.e., chat robots and empathic dialog generation. Also the current ECPE is limited to the scenario of single text piece, while neglecting the studies at dialog level that should have more realistic values. In this paper, we extend the ECPE task with a broader definition and scenario, presenting a new task, Emotion-Cause Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause utterance pairs and emotion and cause types. We present an ECQED model based on a structural and semantic heterogeneous graph as well as a parallel grid tagging scheme, which advances in effectively incorporating the dialog context structure, meanwhile solving the challenging overlapped quadruple issue. Via experiments we show that introducing the fine-grained emotion and cause features evidently helps better dialog generation. Also our proposed ECQED system shows exceptional superiority over baselines on both the emotion-cause quadruple or pair extraction tasks, meanwhile being highly efficient.},
  archive  = {J},
  author   = {Li Zheng and Donghong Ji and Fei Li and Hao Fei and Shengqiong Wu and Jingye Li and Bobo Li and Chong Teng},
  doi      = {10.1109/TASLPRO.2025.3589863},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3391-3400},
  title    = {ECQED: Emotion-cause quadruple extraction in dialogs},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A diffusion model based on Hilbert–Huang reconstruction for pathological voice generation. <em>TASLPRO</em>, <em>33</em>, 3377-3390. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most existing speech generation models require substantial amounts of learning data, significantly limiting their effectiveness when working with limited pathological voice samples. In this study, we propose a Diffusion Transformer model based on Hilbert-Huang reconstruction tailored for pathological vowels with limited data. Our model takes the pitch of pathological voice samples as input to generate new samples with authentic pathological acoustic characteristics. We employ a diffusion model as the generation framework and use a Transformer model as the denoising network within the diffusion process, enhancing the quality of generated speech. To capture the nonlinear and non-stationary characteristics of pathological voices in generated samples, we introduce a Hilbert reconstruction Decoder. This decoder incorporates the Hilbert-Huang transform to learn and retain pathological features, enhancing the presence of nonlinear and non-stationary components in the generated samples, making them closely resemble real pathological voices. Additionally, we propose a Pitch Learning Strategy that decomposes an entire pathological voice sample into segments based on its pitch period, effectively augmenting the model’s learning data and addressing the scarcity of pathological voice samples. We conducted experiments on three international pathological voice datasets, and under various acoustic evaluation metrics, the generated data closely resembled the characteristics of real data. In the classification experiments with data augmentation, our method achieves an average accuracy that is 4.4 p.p. (percentage points) higher than the best-performing models among the baselines.},
  archive  = {J},
  author   = {Yuyang Jiang and Mingxuan Yan and Xiaojun Zhang and Zhi Tao},
  doi      = {10.1109/TASLPRO.2025.3589868},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3377-3390},
  title    = {A diffusion model based on Hilbert–Huang reconstruction for pathological voice generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling nonuniform energy decay through the modal decomposition of acoustic radiance transfer (MoD-ART). <em>TASLPRO</em>, <em>33</em>, 3363-3376. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modeling late reverberation in real-time interactive applications is a challenging task when multiple sound sources and listeners are present in the same environment. This is especially problematic when the environment is geometrically complex and/or features uneven energy absorption (e.g. coupled volumes), because in such cases the late reverberation is dependent on the sound sources’ and listeners’ positions, and therefore must be adapted to their movements in real time. We present a novel approach to the task, named modal decomposition of acoustic radiance transfer (MoD-ART), which can handle highly complex scenarios with efficiency. The approach is based on the geometrical acoustics method of acoustic radiance transfer, from which we extract a set of energy decay modes and their positional relationships with sources and listeners. In this paper, we describe the physical and mathematical significance of MoD-ART, highlighting its advantages and applicability to different scenarios. Through an analysis of the method’s computational complexity, we show that it compares very favorably with ray-tracing. We also present simulation results showing that MoD-ART can capture multiple decay slopes and flutter echoes.},
  archive  = {J},
  author   = {Matteo Scerbo and Sebastian J. Schlecht and Randall Ali and Lauri Savioja and Enzo De Sena},
  doi      = {10.1109/TASLPRO.2025.3592322},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3363-3376},
  title    = {Modeling nonuniform energy decay through the modal decomposition of acoustic radiance transfer (MoD-ART)},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-set speaker identification through efficient few-shot tuning with speaker reciprocal points and unknown samples. <em>TASLPRO</em>, <em>33</em>, 3347-3362. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces a novel framework for few-shot open-set speaker identification, aimed at real-world household wake-up and recognition scenarios. To address the limitations of current speaker models and classification methods, our approach combines a pretrained speaker foundation frontend with a few-shot tunable neural network backend. We employ an effective open-set recognition technique called Speaker Reciprocal Points Learning (SpeakerRPL) to enhance discrimination among target speakers while modeling “otherness.” Moreover, we propose SpeakerRPL+, which integrates unknown sample learning via speech-synthesized unknown samples, significantly boosting few-shot open-set speaker identification (OpenSID) performance. We also investigate optimal model-tuning strategies, zero-shot timbre-controllable synthesis methods, and training procedures for SpeakerRPL+, demonstrating its adaptability across various speaker foundation models. Comprehensive evaluations on multiple multi-language, primarily text-dependent speaker recognition datasets confirm the efficacy of our framework in complex household environments, yielding superior few-shot open-set speaker identification performance over several state-of-the-art speaker foundation models.},
  archive  = {J},
  author   = {Zhiyong Chen and Shuhang Wu and Xinnuo Li and Zhiqi Ai and Shugong Xu},
  doi      = {10.1109/TASLPRO.2025.3587591},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3347-3362},
  title    = {Open-set speaker identification through efficient few-shot tuning with speaker reciprocal points and unknown samples},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-intrusive speech quality evaluation framework for hearing aids based on speech label assistance and multi-task learning strategy. <em>TASLPRO</em>, <em>33</em>, 3332-3346. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate evaluation of hearing aid speech quality is crucial for optimizing the auditory experience of hearing-impaired people. Aiming at the shortcomings of existing methods that rely on clean reference signals and do not take into account the effects of differences in Prescription Formula (PF), this paper proposes a non-intrusive speech quality evaluation framework based on speech label assistance, and multi-task learning strategy, termed MTSE-LA. The framework effectively mitigates evaluation bias caused by PF variations and effectively improves the prediction accuracy of speech quality metrics. MTSE-LA consists of three core modules: a feature extraction module, a label classification module, and a score prediction module. The feature extraction module extracts deep frame-level features from speech using a joint Convolutional Neural Network and Bidirectional Long Short-term Memory network (CNN-BiLSTM) model. The label classification module, acting as a pre-trained network, identifies PF labels and embeds them into the extracted frame-level features, which are then fed into the speech quality prediction branch of the multi-task score prediction module. Effective prediction of speech intelligibility is achieved by introducing the output vectors of the modulation filter bank to the speech intelligibility prediction branch to ensure synergy in the multi-task learning process. Moreover, each prediction branch uses the multi-head self-attention mechanism to capture contextual information and model the importance of speech frames. Experimental results demonstrate that MTSE-LA considerably improves the prediction accuracy of the Hearing Aid Speech Quality Index (HASQI) under multiple PF configurations and different degrees of hearing loss conditions. Compared with existing cutting-edge methods, the proposed framework exhibits higher correlation and fitting accuracy, establishing its reliability and superiority in the field of non-intrusive speech quality evaluation for hearing aids.},
  archive  = {J},
  author   = {Yang Yang and Ruiyu Liang and Ye Ni and Yue Xie and Cairong Zou and Björn W. Schuller},
  doi      = {10.1109/TASLPRO.2025.3594293},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3332-3346},
  title    = {A non-intrusive speech quality evaluation framework for hearing aids based on speech label assistance and multi-task learning strategy},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved robust total logistic distance metrics algorithm for generalized gaussian noise and noisy input. <em>TASLPRO</em>, <em>33</em>, 3320-3331. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although the known maximum total generalized correntropy (MTGC) and generalized maximum blake–zisserman total correntropy (GMBZTC) algorithms can maintain good performance under the errors-in-variables (EIV) model disrupted by generalized Gaussian noise, their requirement for manual adjustment of parameters is excessive, greatly increasing the practical difficulty of use. To solve this problem, the total arctangent based on logical distance metrics (TACLDM) algorithm is proposed by utilizing the advantage of few parameters in logical distance metrics (LDM) theory and the convergence behavior is improved by the arctangent function. Compared with other competing algorithms, the TACLDM algorithm not only has fewer parameters, but also has better robustness to generalized Gaussian noise and significantly reduces the steady-state error. Furthermore, the analysis of the algorithm in the generalized Gaussian noise environment is analyzed in detail in this paper. Finally, computer simulations demonstrate the outstanding performance of the TACLDM algorithm and the rigorous theoretical deduction in this paper.},
  archive  = {J},
  author   = {Haiquan Zhao and Yi Peng and Zian Cao},
  doi      = {10.1109/TASLPRO.2025.3594248},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3320-3331},
  title    = {An improved robust total logistic distance metrics algorithm for generalized gaussian noise and noisy input},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QS-TTS: Towards semi-supervised text-to-speech synthesis via vector-quantized self-supervised speech representation learning. <em>TASLPRO</em>, <em>33</em>, 3307-3319. (<a href='https://doi.org/10.1109/TASLP.2024.3414342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.},
  archive  = {J},
  author   = {Haohan Guo and Fenglong Xie and Jiawen Kang and Yujia Xiao and Xixin Wu and Helen Meng},
  doi      = {10.1109/TASLP.2024.3414342},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3307-3319},
  title    = {QS-TTS: Towards semi-supervised text-to-speech synthesis via vector-quantized self-supervised speech representation learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced prompt-tuning with contrastive learning for emotion recognition in conversations. <em>TASLPRO</em>, <em>33</em>, 3292-3306. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion Recognition in Conversation (ERC) is essential for a wide range of applications, but it faces challenges arising from contextual influences. Imbalanced emotion distributions further complicate the ERC task, resulting in constrained learning for minority emotions and biased predictions for common emotions. To address these issues, we propose the Knowledge-Enhanced Prompt-Tuning with Contrastive Learning (KEPTCL) framework. KEPTCL leverages prompt-tuning to align the ERC task with the pre-training objective of the Pre-trained Language Model (PLM), thereby effectively harnessing the rich linguistic knowledge accumulated by the PLM from large-scale corpora. This approach effectively mitigates the data scarcity issue for minority emotions. Furthermore, we introduce a context-aware mixed prompt template and a knowledge subgraph-based label mapping strategy to aid the prompt-tuning process. Moreover, Supervised Contrastive Learning (SCL) is integrated to improve the PLM’s capability to distinguish utterances with different emotions, reducing biased predictions for common emotions. Experimental results demonstrate that KEPTCL outperforms the state-of-the-art methods on all three benchmark datasets and mitigates the challenges posed by the imbalanced emotion distributions.},
  archive  = {J},
  author   = {Qingqing Gao and Jiuxin Cao and Biwei Cao and Xin Guan and Bo Liu},
  doi      = {10.1109/TASLPRO.2025.3592325},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3292-3306},
  title    = {Knowledge-enhanced prompt-tuning with contrastive learning for emotion recognition in conversations},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangling segmental and prosodic factors to non-native speech comprehensibility. <em>TASLPRO</em>, <em>33</em>, 3281-3291. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current accent conversion (AC) systems do not disentangle the two main sources of non-native accent: segmental and prosodic characteristics. Being able to manipulate a non-native speaker’s segmental and/or prosodic channels independently is critical to quantify how these two channels contribute to speech comprehensibility and social attitudes. We present an AC system that not only decouples voice quality from accent, but also disentangles the latter into its segmental and prosodic characteristics. The system is able to generate accent conversions that combine (1) the segmental characteristics from a source utterance, (2) the voice characteristics from a target utterance, and (3) the prosody of a reference utterance. We show that vector quantization of acoustic embeddings and removal of consecutive duplicated codewords allows the system to transfer prosody and improve voice similarity. We conduct perceptual listening tests to quantify the individual contributions of segmental features and prosody on the perceived comprehensibility of non-native speech. Our results indicate that, contrary to prior research in non-native speech, segmental features have a larger impact on comprehensibility than prosody. The proposed AC system may also be used to study how segmental and prosody cues affect social attitudes towards non-native speech.},
  archive  = {J},
  author   = {Waris Quamer and Ricardo Gutierrez-Osuna},
  doi      = {10.1109/TASLPRO.2025.3592320},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3281-3291},
  title    = {Disentangling segmental and prosodic factors to non-native speech comprehensibility},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive load driven audio-visual speech enhancement using a neuro-fuzzy inference model and OpenMHA platform. <em>TASLPRO</em>, <em>33</em>, 3267-3280. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional hearing aids primarily amplify sound but often overlook factors critical for effective speech comprehension, such as listening effort and visual cues like lip movements. This article introduces an innovative, context and cognition-aware audio-visual speech enhancement (AVSE) framework for hearing aids, integrating adaptive and efficient edge implementation to improve user experience. Our approach includes adaptive speech enhancement model selection using neuro-fuzzy inference based on listening effort, audio signal-to-noise ratio (SNR), and visual cues. This work comprises four key components: integrating AVSE models with openMHA; using custom Radio Frequency (RF) sensors to estimate cognitive load; applying neuro-fuzzy inference for efficient speech enhancement selection; and developing hardware-accelerated Deep Neural Network (DNN)-based AVSE implementations. The framework includes a lightweight, customizable AVSE model optimized for real-time, power-efficient edge performance. The performance is comprehensively evaluated using metrics like STOI, PESQ, HASPI, MOS, latency, and power consumption, providing an in-depth analysis of trade-offs between speech intelligibility, perceptual quality, auditory perception, processing speed, and energy efficiency, with hardware-accelerated inference ensuring optimal performance. This research signifies a significant advance in AVSE technology, offering enhanced communication and a better quality of life for individuals with hearing impairments.},
  archive  = {J},
  author   = {Usman Anwar and Xianpo Ni and Adewale Adetomi and Kia Dashtipour and Mandar Gogate and Tughrul Arslan and Amir Hussain},
  doi      = {10.1109/TASLPRO.2025.3594296},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3267-3280},
  title    = {Cognitive load driven audio-visual speech enhancement using a neuro-fuzzy inference model and OpenMHA platform},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoAVT: A cognition-inspired unified audio-visual-text pre-training model for multimodal processing. <em>TASLPRO</em>, <em>33</em>, 3255-3266. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There has been a long-standing quest for a unified audio-visual-text model to enable various multimodal understanding tasks, which mimics the listening, seeing, and reading process of human beings. Humans tend to represent knowledge using two separate systems: one for representing verbal (textual) information and one for representing non-verbal (visual and auditory) information. These two systems can operate independently but can also interact with each other. Motivated by this understanding of human cognition, in this paper, we introduce CoAVT – a novel cognition-inspired Correlated Audio-Visual-Text pre-training model to connect the three modalities. It contains a joint audio-visual encoder that learns to encode audio-visual synchronization information together with the audio and visual content for non-verbal information, and a text encoder to handle textual input for verbal information. To bridge the gap between modalities, CoAVT employs a query encoder, which contains a set of learnable query embeddings and extracts the most informative audiovisual features of the corresponding text. Additionally, to leverage the correspondences between audio and vision with language respectively, we also establish the audio-text and visual-text bi-modal alignments upon the foundational audiovisual-text tri-modal alignment to enhance the multimodal representation learning. Finally, we jointly optimize CoAVT model with three multimodal objectives: contrastive loss, matching loss, and language modeling loss. Extensive experiments show that CoAVT can learn strong multimodal correlations and be generalized to various downstream tasks. CoAVT establishes new state-of-the-art performance on text-video retrieval tasks on AudioCaps for both zero-shot and fine-tuning settings, audio-visual event classification and audio-visual retrieval tasks on AudioSet and VGGSound. The results demonstrate the effectiveness and superiority of the proposed model for multimodal processing.},
  archive  = {J},
  author   = {Xianghu Yue and Xiaohai Tian and Malu Zhang and Zhizheng Wu and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3587467},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3255-3266},
  title    = {CoAVT: A cognition-inspired unified audio-visual-text pre-training model for multimodal processing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCKA: Continual cross-domain knowledge adaptation for multi-domain machine translation. <em>TASLPRO</em>, <em>33</em>, 3243-3254. (<a href='https://doi.org/10.1109/TASLPRO.2025.3594292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-domain machine translation (MDMT) has demonstrated remarkable performance in handling multiple domains within a single model and enhancing cross-domain knowledge. The major challenge lies in continually updating the original MDMT models as new domain data arrives. Previous works primarily focus on two separate aspects: mitigating catastrophic forgetting in the original model and acquiring incremental knowledge from new domains. However, addressing both issues simultaneously would better meet practical needs. Additionally, the cross-domain knowledge between the original and new domains remains underutilized. To this end, we propose a Continual Cross-domain Knowledge Adaptation for multi-domain neural machine translation framework (CCKA), our approach contains two learning stages that encourage original models to acquire cross-domain knowledge from new domain data, and preserves the model architecture without introducing parameters. Experimental results on the UM-Corpus and OPUS multi-domain datasets show superior performance of our proposed model compared to representative baselines.},
  archive  = {J},
  author   = {Zhibo Man and Yujie Zhang and Yuanmeng Chen and Yufeng Chen and Jinan Xu},
  doi      = {10.1109/TASLPRO.2025.3594292},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3243-3254},
  title    = {CCKA: Continual cross-domain knowledge adaptation for multi-domain machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probing fine-grained hierarchical concept comprehension and generation in large language models. <em>TASLPRO</em>, <em>33</em>, 3229-3242. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conceptual knowledge is fundamental to human cognition, and the unprecedented performance of large language models (LLMs) necessitates being tested for conceptual knowledge. However, many existing efforts focus on the overall assessment of conceptual knowledge, often overlooking fine-grained concept relationships, particularly the hypernym-hyponym relationship. In this work, we probe LLMs’ concept understand and generations by various granularities hypernyms. Specifically, we design two probing tasks, hypernym judgment and hypernym generation, to assess the comprehension and generation of hypernym relationships in LLMs. These tasks are evaluated using a novel metric, relative hypernym granularity, that quantifies the conceptual semantic distance to the query concepts. Additionally, We probe the semantic-enhanced prompts to enhance the reasoning abilities of LLMs, which is conducted by two structural hypernyms. Our experiments reveal that the latest language models, such as ChatGPT and Llama2, demonstrate consistent competence in managing fine-grained hypernyms. Notably, this research provides an in-depth analysis of hypernym hallucination, offering new perspectives and contributing meaningfully to the understanding of conceptual knowledge in LLMs. Our work delivers a comprehensive evaluation of concept granularity, that simultaneously covers (i) fine-grained hypernym judgement & generation, (ii) structured semantic prompting, and (iii) hypernym hallucination, providing the systematic evaluation of concept understanding capacity in LLMs.},
  archive  = {J},
  author   = {Kai Sun and Yushi Bai and Shangqing Tu and Juanzi Li and Lei Hou},
  doi      = {10.1109/TASLPRO.2025.3592339},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3229-3242},
  title    = {Probing fine-grained hierarchical concept comprehension and generation in large language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frame-wise steganalysis of AMR encoded speech streams using automatic codeword feature interaction mechanism. <em>TASLPRO</em>, <em>33</em>, 3215-3228. (<a href='https://doi.org/10.1109/TASLPRO.2025.3583463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Frame-wise steganalysis is crucial in AMR-encoded speech streams. By analyzing each frame individually, it becomes possible to precisely identify which frame has been inserted into confidential Information and easily alter the codewords in the frame to prevent the confidential information from being extracted correctly. However, no common interpretable theory has been proposed for frame-wise detection in AMR-encoded speech streams. Moreover, the existing frame-wise steganalysis methods cannot fully exploit the correlation between the codewords in each frame. Therefore, a frame-wise steganalysis method based on the automatic codeword feature interaction mechanism is proposed in this paper. The automatic codeword feature interaction mechanism can be achieved through multiple iterations. During these iterations, high-order codeword interaction features are obtained. Each iteration consists of two steps. The first step employs an element-wise codeword feature interaction mechanism to capture fine-grained, localized patterns between corresponding elements. The second step utilizes a vector-wise codeword feature interaction mechanism to capture global patterns across entire codeword vectors. To enhance the learning process and facilitate better information flow, a residual connection is added to the automatic codeword feature interaction mechanism. On numerous large steganography datasets, the experimental results reveal that our method has a significant improvement for the frame-wise steganalysis task, compared to the existing frame-wise steganalysis methods.},
  archive  = {J},
  author   = {Congcong Sun and Hui Tian and Peng Tian and Haizhou Li and Ching-Chun Chang and Chin-Chen Chang},
  doi      = {10.1109/TASLPRO.2025.3583463},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3215-3228},
  title    = {Frame-wise steganalysis of AMR encoded speech streams using automatic codeword feature interaction mechanism},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CleanMel: Mel-spectrogram enhancement for improving both speech quality and ASR. <em>TASLPRO</em>, <em>33</em>, 3202-3214. (<a href='https://doi.org/10.1109/TASLPRO.2025.3592333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. The proposed network takes as input the noisy and reverberant microphone recording and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to the speech waveform with a neural vocoder or directly used for ASR. The proposed network is composed of interleaved cross-band and narrow-band processing in the Mel-frequency domain, for learning the full-band spectral pattern and the narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the key advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results on five English and one Chinese datasets demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model.},
  archive  = {J},
  author   = {Nian Shao and Rui Zhou and Pengyu Wang and Xian Li and Ying Fang and Yujie Yang and Xiaofei Li},
  doi      = {10.1109/TASLPRO.2025.3592333},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3202-3214},
  title    = {CleanMel: Mel-spectrogram enhancement for improving both speech quality and ASR},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pretraining and fine-tuning techniques for electrolaryngeal speech enhancement based on sequence-to-sequence voice conversion. <em>TASLPRO</em>, <em>33</em>, 3189-3201. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We describe novel training methods based on sequence-to-sequence (seq2seq) voice conversion (VC) to address two practical issues in electrolaryngeal (EL)-speech-to-normal-speech conversion (EL2SP): 1) low-resource training data, and 2) the huge domain shift gap during transfer learning. Seq2seq VC is promising for EL2SP but suffers performance degradation without sufficiently high-quality, parallel training data. The common method utilizes transfer learning to address low-resource issue, following a direct pretraining–fine-tuning paradigm. However, in EL2SP, as huge domain shifts exist between upstream larger-scale normal corpora and the target EL2SP dataset, the common method cannot achieve effective transfer learning, limiting EL2SP performance. Therefore, we present training methods with multistage pretraining and fine-tuning techniques, particularly including an encoder adaptation training and a two-stage fine-tuning method, both leveraging low-quality synthetic data (SD), to improve transferability. During pretraining, aside from knowledge transfer from a more easily accessible TTS database into seq2seq VC pretraining, encoder adaptation training further minimizes the representation learning gap of the encoder in comprehending EL speech, facilitating smoother transfer for downstream EL2SP. Subsequently, the two-stage EL2SP fine-tuning finalizes a generalized and stable performance. Moreover, by effectively utilizing low-quality SD, our techniques relax training data demands and enhance practicality. Experimental results demonstrate our methods dramatically outperform a baseline using the common method regarding conversion quality and intelligibility. Comparative analyses confirm progressive performance gains with deeper system designs.},
  archive  = {J},
  author   = {Ding Ma and Lester Phillip Violeta and Kazuhiro Kobayashi and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2025.3577374},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3189-3201},
  title    = {Pretraining and fine-tuning techniques for electrolaryngeal speech enhancement based on sequence-to-sequence voice conversion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Word error rate definitions and algorithms for long-form multi-talker speech recognition. <em>TASLPRO</em>, <em>33</em>, 3174-3188. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The predominant metric for evaluating speech recognizers, the Word Error Rate (WER) has been extended in different ways to handle transcripts produced by long-form multi-talker speech recognizers.These systems process long transcripts containing multiple speakers and complex speaking patterns so that the classical WER cannot be applied.There are speaker-attributed approaches that count speaker confusion errors, such as the concatenated minimum-permutation WER cpWER and the time-constrained cpWER (tcpWER), and speaker-agnostic approaches, which aim to ignore speaker confusion errors, such as the Optimal Reference Combination WER (ORC-WER) and the MIMO-WER.These WERs evaluate different aspects and error types (e.g., temporal misalignment). A detailed comparison has not been made.We therefore present a unified description of the existing WERs and highlight when to use which metric.To further analyze how many errors are caused by speaker confusion, we propose the Diarization-invariant cpWER (DI-cpWER).It ignores speaker attribution errors and its difference to cpWER reflects the impact of speaker confusions on the WER.Since error types cannot reliably be classified automatically, we discuss ways to visualize sequence alignments between the reference and hypothesis transcripts to facilitate the spotting of errors by a human judge.Since some WER definitions have high computational complexity, we introduce a greedy algorithm to approximate the ORC-WER and DI-cpWER with high precision ($&lt; 0 .1 \% $ deviation in our experiments) and polynomial complexity instead of exponential.To improve the plausibility of the metrics, we also incorporate the time constraint from the tcpWER into ORC-WER and MIMO-WER, also significantly reducing the computational complexity.},
  archive  = {J},
  author   = {Thilo von Neumann and Christoph Boeddeker and Marc Delcroix and Reinhold Haeb-Umbach},
  doi      = {10.1109/TASLPRO.2025.3589862},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3174-3188},
  title    = {Word error rate definitions and algorithms for long-form multi-talker speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring speaking styles for conversational speech synthesis by learning contextual dependencies. <em>TASLPRO</em>, <em>33</em>, 3160-3173. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In addition to conventional text-to-speech (TTS) systems, conversational TTS systems need to infer the appropriate speaking style on the basis of the current conversational scenario and the dependencies within the context. These dependencies specifically include the interrelationships among speakers as well as the multimodal information present in the context at both the global scale (i.e., the utterance level) and the local scale (i.e., the word level). However, current context modeling techniques in conversational TTS focus only on global-scale modeling of textual information, disregarding the modeling of speaker dependencies and other fine-grained and multimodal features in context. In this paper, we propose several context modeling techniques to enhance context modeling and the inference of speaking styles in conversational TTS. We propose approaches inspired by DialogueRNN and DialogueGCN to capture inter- and intraspeaker dependencies through textual and acoustic features in conversations. We further propose an approach based on a multiscale relational graph convolutional network (MSRGCN) to learn the fine-grained dependencies at the local scale. The experimental results demonstrate the effectiveness of our proposed context modeling techniques compared with traditional methods in conversational TTS systems. The contributions of modeling multimodal information and multiscale dependencies are further demonstrated in ablation studies.},
  archive  = {J},
  author   = {Jingbei Li and Weihao Wu and Yi Meng and Luwen Zhang and Qiao Tian and Yuping Wang and Yuxuan Wang and Xixin Wu and Zhiyong Wu and Helen Meng},
  doi      = {10.1109/TASLPRO.2025.3589870},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3160-3173},
  title    = {Inferring speaking styles for conversational speech synthesis by learning contextual dependencies},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sparse softmax: An effective and efficient softmax variant. <em>TASLPRO</em>, <em>33</em>, 3148-3159. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Softmax with the cross entropy loss is the standard configuration for current neural classification models. The gold score for a target class is supposed to be 1, but it is never reachable under the softmax schema. Such a problem makes the training process continue forever and leads to overfitting. Moreover, the “target-approach-1” training goal forces the model to continuously learn all samples, leading to a waste of time in handling some samples which have already been classified correctly with high confidence, while the test goal simply requires the target class of each sample to hold the maximum score. To solve the above weaknesses, we propose the Adaptive Sparse softmax (AS-Softmax) which designs a reasonable and test-matching transformation on top of softmax. For more purposeful learning, we discard the classes with far smaller scores compared with the actual class during training. Then the model could focus on learning to distinguish the target class from its strong opponents, which is also the great challenge in test. In addition, since the training losses of easy samples will gradually drop to 0 in AS-Softmax, we develop an adaptive gradient accumulation strategy based on the masked sample ratio to speed up training. We verify the proposed AS-Softmax on a variety of text multi-class, text multi-label, text token classification, image classification and audio classification tasks with class sizes ranging from 5 to 5000+. The results show that AS-Softmax consistently outperforms softmax and its variants, and the loss of AS-Softmax is remarkably correlated with classification performance in validation. Furthermore, adaptive gradient accumulation strategy can bring about 1.2× training speedup comparing with the standard softmax while maintaining classification effectiveness.},
  archive  = {J},
  author   = {Qi Lv and Lei Geng and Ziqiang Cao and Min Cao and Sujian Li and Wenjie Li and Guohong Fu},
  doi      = {10.1109/TASLPRO.2025.3587436},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3148-3159},
  title    = {Adaptive sparse softmax: An effective and efficient softmax variant},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-direction aware multichannel selective fixed-filter active noise control based on multi-task learning. <em>TASLPRO</em>, <em>33</em>, 3137-3147. (<a href='https://doi.org/10.1109/TASLPRO.2025.3590289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The selective fixed-filter active noise control (SFANC) method has gained popularity due to its high computational efficiency and fast response. However, it solely accounts for noise frequency information, while neglecting its spatial information, which undoubtedly undermines the noise reduction performance, especially for direction-varied noises. To address this limitation, we proposed the Frequency-Direction Aware Multichannel SFANC (FD-MCSFANC) method, which incorporates both frequency and direction information of primary noises for accurate filter selection in the multichannel ANC (MCANC) system. In our approach, a Convolutional Neural Network (CNN) is designed to classify noises based on both frequency components and Direction-of-Arrival (DOA), with the combined classification results determining selected control filters. Furthermore, a joint loss function based on multi-task learning is utilized to implement end-to-end training of the CNN. Numerical simulations show that the FD-MCSFANC method effectively attenuates noises with different frequencies and incident angles. Moreover, it responds much faster than traditional adaptive algorithms and achieves better global noise control performance than the Multichannel SFANC (MCSFANC) method.},
  archive  = {J},
  author   = {Zhengding Luo and Dongyuan Shi and Xiruo Su and Woon-Seng Gan},
  doi      = {10.1109/TASLPRO.2025.3590289},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3137-3147},
  title    = {Frequency-direction aware multichannel selective fixed-filter active noise control based on multi-task learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MACE: Morphology aware cross-lingual embedding using contrastive learning. <em>TASLPRO</em>, <em>33</em>, 3124-3136. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The bilingual dictionary induction (BDI) task induced by cross-lingual word embedding usually performs poorly when the language pairs are structurally non-isomorphic, like English-Manipuri. The performance also degrades when one of the languages in the pair is morphologically rich. Earlier work shows that segmenting the morpheme (root) and suffix/prefix using a morphological analyzer slightly improves cross-lingual word embedding in the BDI task. In this work, we proposed a novel contrastive learning method that takes advantage of the rich morphological nature of a language without segmenting root words and suffixes/prefixes. The proposed contrastive learning pulls the source and target words together in a bilingual dictionary and brings the target word with another target word that shares the same root. From various experimental observations over four language pairs, it is evident that the proposed method outperforms the baseline methods in the BDI, Machine Translation (MT), and Cross-lingual Sentence Retrieval Task (CLSRT).},
  archive  = {J},
  author   = {Deepen Naorem and Sanasam Ranbir Singh and Telem Joyson Singh and Priyankoo Sarmah},
  doi      = {10.1109/TASLPRO.2025.3589860},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3124-3136},
  title    = {MACE: Morphology aware cross-lingual embedding using contrastive learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMG-SENet: Multi-modal speech enhancement with electromyography signals. <em>TASLPRO</em>, <em>33</em>, 3111-3123. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech enhancement (SE) is crucial for improving the intelligibility and quality of noisy speech in applications such as speech recognition and assistive technologies. While traditional SE methods often fail in complex noise environments, deep learning-based approaches and multi-modal techniques have shown significant progress. However, existing multi-modal SE methods, such as audio-visual approaches, are limited in scenarios where visual data cannot be captured. EMG-SENet-basic explores Electromyography (EMG) signals as a robust complementary modality for SE, leveraging their resistance to environmental noise and potential for non-invasive acquisition. In addition, we propose an improved multi-modal SE framework named EMG-SENet-improved featuring a frame-level cross-attention mechanism for dynamic feature fusion, an adaptive weighting mechanism to better integrate amplitude information from both modalities, and the Con-TF-Mamba architecture to enhance local feature extraction in time and frequency domains. Furthermore, Temporal Masking, used to simulate frame loss, improves robustness under real-world conditions. Experimental results demonstrate the proposed method’s superior performance in enhancing SE quality and intelligibility, addressing key challenges in practical applications with degraded or incomplete data.},
  archive  = {J},
  author   = {Fuyuan Feng and Xuebin Zhang and Yan Feng and Zhangcheng Yang and Longting Xu},
  doi      = {10.1109/TASLPRO.2025.3587412},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3111-3123},
  title    = {EMG-SENet: Multi-modal speech enhancement with electromyography signals},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reply with sticker: New dataset and model for sticker retrieval. <em>TASLPRO</em>, <em>33</em>, 3099-3110. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Using stickers in online chatting is very prevalent on social media platforms, where the stickers used in the conversation can express someone’s intention/emotion/attitude in a vivid, tactful, and intuitive way. Existing sticker retrieval research typically retrieves stickers based on context and the current utterance delivered by the user. That is, the stickers serve as a supplement to the current utterance. However, in the real-world scenario, using stickers to express what we want to say rather than as a supplement to our words only is also important. Therefore, in this paper, we create a new dataset for sticker retrieval in conversation, called StickerInt, where stickers are used to reply to previous conversations or supplement our words.1 Based on the created dataset, we present a simple yet effective framework for sticker retrieval in conversation based on the learning of intention and the cross-modal relationships between conversation context and stickers, coined as Int-RA. Specifically, we first devise a knowledge-enhanced intention predictor to introduce the intention information into the conversation representations. Subsequently, a relation-aware sticker selector is devised to retrieve the response sticker via cross-modal relationships. Extensive experiments on two datasets show that the proposed model achieves state-of-the-art performance and generalization capability in sticker retrieval.},
  archive  = {J},
  author   = {Bin Liang and Bingbing Wang and Zhixin Bai and Qiwei Lang and Mingwei Sun and Kaiheng Hou and Lanjun Zhou and Ruifeng Xu and Kam-Fai Wong},
  doi      = {10.1109/TASLPRO.2025.3587415},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3099-3110},
  title    = {Reply with sticker: New dataset and model for sticker retrieval},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AnyEnhance: A unified generative model with prompt-guidance and self-critic for voice enhancement. <em>TASLPRO</em>, <em>33</em>, 3085-3098. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker’s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests.},
  archive  = {J},
  author   = {Junan Zhang and Jing Yang and Zihao Fang and Yuancheng Wang and Zehua Zhang and Zhuo Wang and Fan Fan and Zhizheng Wu},
  doi      = {10.1109/TASLPRO.2025.3587393},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3085-3098},
  title    = {AnyEnhance: A unified generative model with prompt-guidance and self-critic for voice enhancement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable DNN-based beamformer with postfilter. <em>TASLPRO</em>, <em>33</em>, 3070-3084. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces an explainable DNN-based beamformer with a postfilter (ExNet-BF+PF) for multichannel signal processing. Our approach combines the U-Net network with a beamformer structure to address this problem. The method involves a two-stage processing pipeline. In the first stage, time-invariant weights are applied to construct a multichannel spatial filter, namely a beamformer. In the second stage, a time-varying single-channel post-filter is applied at the beamformer output. Additionally, we incorporate an attention mechanism inspired by its successful application in noisy and reverberant environments to improve speech enhancement further. The proposed scheme obviates the necessity for prior knowledge of the speaker’s activity, which is required in classical beamforming designs. Furthermore, our study fills a gap in the existing literature by conducting a thorough spatial analysis of the network’s performance. Specifically, we examine how the network utilizes spatial information during processing. This analysis yields valuable insights into the network’s functionality, thereby enhancing our understanding of its overall performance. A thorough experimental study analyzes the performance of the proposed method in various scenarios and compares both the results and computational resource consumption with several baseline methods, including the classical Minimum Variance Distortionless Response (MVDR) beamformer, Deep Neural Network (DNN)-based spatial filters that preserve a beamforming structure, and deep end-to-end schemes.},
  archive  = {J},
  author   = {Adi Cohen and Daniel Wong and Jung-Suk Lee and Sharon Gannot},
  doi      = {10.1109/TASLPRO.2025.3581110},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3070-3084},
  title    = {Explainable DNN-based beamformer with postfilter},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Targeted adversarial examples for attacking end-to-end chinese automatic speech recognition systems. <em>TASLPRO</em>, <em>33</em>, 3056-3069. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently, automatic speech recognition (ASR) systems are developing rapidly and most of the systems are built based on neural network models. Recent studies have shown that they are susceptible to adversarial attacks. There have been some studies on adversarial attacks for English ASR systems, but there are no adversarial attacks research works against Chinese systems so far, to the best of our knowledge. There are significant differences between the Chinese and English ASR systems, including the output character spaces and linguistic phenomena, which lead to problems such as difficulty in convergence, reduced speech similarity, and vulnerability to local optimal solutions during adversarial attacks. This paper takes a pioneering step toward adversarial attacks on Chinese ASR by proposing CASR-ATTACK, a novel method that leverages non-dominated ordering and gradient estimation to generate Chinese adversarial examples. We conducted extensive experiments using a very popular Chinese ASR system, DeepSpeech2, and used three popular open-source Chinese datasets to verify the effectiveness of the method in this paper. Experimental results showed that our approach is more effective than the state-of-the-art baselines, which achieves a success rate of 92.2%, 91.2%, 92.8% and keeps an similarity of 96.55%, 96.61%, 96.77% on short phrase attacks. The attack on long Chinese sentences also achieves an average success rate of 89.00%. Our research contributes to the study of adversarial attack research and defense of Chinese ASR systems.},
  archive  = {J},
  author   = {Qinglong Huang and Haizhou Wang and Yiyao Yang},
  doi      = {10.1109/TASLPRO.2025.3587395},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3056-3069},
  title    = {Targeted adversarial examples for attacking end-to-end chinese automatic speech recognition systems},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting LLM’s continual sentiment understanding for low-resource languages. <em>TASLPRO</em>, <em>33</em>, 3042-3055. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The emergence of Large Language Models has demonstrated remarkable performance in various sentiment understanding tasks. However, their sentiment understanding performance degrades sharply for low-resource languages (e.g. Acehnese) due to the primary training on high-resource languages (e.g. English). To this end, this paper proposes a new task, namely LLM’s Continual Sentiment understanding for Low-resource Language (CS4L), seeking to boost the sentiment continual learning ability of LLMs across various low-resource languages. In particular, this paper poses two key challenges in CS4L: 1) Efficient sentiment retention for intra-language, aiming to mitigate the sentiment forgetting for each language efficiently. 2) Efficient sentiment alignment for inter-language, aiming to align the sentiment information across various languages efficiently. In this way, this paper proposes a new Efficient Sentiment Neuron Modifying (ESNM) approach to CS4L. Specifically, ESNM designs two novel blocks to address the two challenges above respectively: 1) Masked Retentive Updating block via merely one shared LoRA. 2) Precise Aligned Editing block via the efficient neuron editing. Comprehensive experimental evaluations on our constructed multiple CS4L datasets demonstrate the great advantages of our proposed ESNM approach. Compared to the the state-of-the-art baseline (i.e., O-LoRA), the accuracy of sentiment classification improved by an average of 2.18% and the forgetting rate decreased by 5.36% . This justifies the importance of the proposed CS4L task and the effectiveness of our ESNM approach in enhancing continual sentiment understanding of LLMs for low-resource languages.},
  archive  = {J},
  author   = {Han Zhang and Jingjing Wang and Jiamin Luo and Min Zhang and Guodong Zhou},
  doi      = {10.1109/TASLPRO.2025.3581018},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3042-3055},
  title    = {Boosting LLM’s continual sentiment understanding for low-resource languages},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MKD-FSV: Multi-layer knowledge distillation for far-field speaker verification. <em>TASLPRO</em>, <em>33</em>, 3028-3041. (<a href='https://doi.org/10.1109/TASLPRO.2025.3579306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing Automatic Speaker Verification (ASV) systems show inferior performance in dealing with the cross-domain challenges posed by far-field utterances. To mitigate domain mismatch issues in far-field scenarios, this paper proposes a novel but promising Multi-layer Knowledge Distillation-based Far-field Speaker Verification (MKD-FSV) system. MKD-FSV employs a teacher-student framework with a well-thought-out Multi-layer Knowledge Distillation Strategy (MKDS) to transfer domain-invariant knowledge from close-talking utterances, guiding the student model to effectively learn far-field data. MKDS elaborately incorporates Feature Knowledge Distillation (FKD) with Decoupled Knowledge Distillation (DKD) in a complementary manner to ensure a more comprehensive knowledge transfer and significantly enhance the system’s performance. Specifically, FKD captures domain-invariant speaker characteristics more effectively, transferring crucial features from the teacher to the student model to develop a more robust, generalized speaker embedding space, while DKD enhances speaker feature discriminability by balancing information between target and non-target classes, reducing interference, and improving accuracy in cross-domain mismatch scenarios. This dual approach ensures that the student model inherits robust speaker embeddings while refining its decision-making, leading to improved verification performance. Moreover, a reparameterization technique is utilized to reduce model complexity and enhance inference efficiency. Extensive experimental results demonstrate that MKD-FSV outperforms existing methods in far-field ASV tasks, achieving both higher verification accuracy and significantly improved inference efficiency through the reparameterization technique, making it highly applicable in complex real-world scenarios.},
  archive  = {J},
  author   = {Lingyun Xiang and Jinghan Zhou and Chengfu Ou and Zhili Zhou and Yongfeng Huang},
  doi      = {10.1109/TASLPRO.2025.3579306},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3028-3041},
  title    = {MKD-FSV: Multi-layer knowledge distillation for far-field speaker verification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFA-KWS: Effective keyword spotting with multi-head frame-asynchronous decoding. <em>TASLPRO</em>, <em>33</em>, 3014-3027. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Keyword spotting (KWS) is essential for voice-driven applications, demanding both accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy and beam search, explore the entire search space without explicitly prioritizing keyword detection, often leading to suboptimal performance. In this paper, we propose an effective keyword-specific KWS framework by introducing a streaming-oriented CTC-Transducer-combined frame-asynchronous system with multi-head frame-asynchronous decoding (MFA-KWS). Specifically, MFA-KWS employs keyword-specific phone-synchronous decoding for CTC and replaces conventional RNN-T with Token-and-Duration Transducer to enhance both performance and efficiency. Furthermore, we explore various score fusion strategies, including single-frame-based and consistency-based methods. Extensive experiments demonstrate the superior performance of MFA-KWS, which achieves state-of-the-art results on both fixed keyword and arbitrary keywords datasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting strong robustness in noisy environments. Among fusion strategies, the consistency-based CDC-Last method delivers the best performance. Additionally, MFA-KWS achieves a 47%–63% speed-up over the frame-synchronous baselines across various datasets, with a lightweight model size of approximately 3.3 M parameters, making it well-suited for on-device deployment.},
  archive  = {J},
  author   = {Yu Xi and Haoyu Li and Xiaoyu Gu and Yidi Jiang and Kai Yu},
  doi      = {10.1109/TASLPRO.2025.3587459},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {3014-3027},
  title    = {MFA-KWS: Effective keyword spotting with multi-head frame-asynchronous decoding},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\beta$-order energy-weighting modulation on spectral bins for replay speech detection. <em>TASLPRO</em>, <em>33</em>, 2999-3013. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The usage of recording and playback devices for the generation of replay speech introduces some variation into replay speech. This kind of variation penetrates into the short-time details of the replay speech signal in a way of energy distribution. Together with the environmental noise during the process of recording, the disparity between replay speech and genuine speech is discernible. In this paper, in order to emphasize the disparity of the short-time energy distribution between replay speech and genuine speech, we propose a $\beta$-order energy-weighting (EW) method to modulate the spectral-energy into spectral bins in short time frame-wise duration. Specifically, we apply the $\beta$-order EW modulation in different spectral domains, i.e., octave spectral domain, Mel spectral domain and linear spectral domain. Subsequently, we propose three novel features: constant-Q EW octave coefficient (CEOC), Mel frequency EW cepstral coefficient (MFECC), and linear frequency EW cepstral coefficient (LFECC). Moreover, with the three proposed features, relevant replay speech detection (RSD) systems are developed with respective ResNet and DNN as backend classifiers. With experiments conducted on respective ASVspoof 2017 version 2.0, BTAS 2016 physical access, ASVspoof 2019 physical access and ASVspoof 2021 physical access corpora, the effectiveness of the three proposed features is evaluated in terms of the RSD performance. The evaluation results demonstrate that the $\beta$-order EW modulation exhibits superior discriminative capability in detecting replay speech. Furthermore, through the experiments, it can be observed that our proposed RSD systems outperform many state-of-the-art RSD systems.},
  archive  = {J},
  author   = {Jichen Yang and Chang Huai You and Rohan Kumar Das},
  doi      = {10.1109/TASLPRO.2025.3587454},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2999-3013},
  title    = {$\beta$-order energy-weighting modulation on spectral bins for replay speech detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation of few-shot bioacoustic event detection in different environments. <em>TASLPRO</em>, <em>33</em>, 2986-2998. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot Bioacoustic Event Detection (FS-BAED) aims to monitor and analyse events with few labelled samples. Across different environments, the feature distribution of background events may diverge, which is considered a domain shift problem. This paper identifies two distinct forms of bias induced by such shifts: the familiar sampling bias of few-shot learning, and a under-recognised semantic feature bias, whereby background events in novel environments are systematically misclassified as target events. To address both, this work introduces a unified FS-BAED framework. First, a contrastive learning backbone augmented with a multi-class classification strategy mitigates sampling bias by enforcing instance-level feature separability. Second, an environment-shift detector flags and filters out background samples whose acoustic features diverge significantly from known distributions, thereby suppressing false positives caused by semantic feature bias. The proposed framework is evaluated on the DCASE 2022 Task 5 dataset under two settings: (i) a fixed environment benchmark common to existing works, and (ii) an altered environment scenario in which foreground events from DCASE dataset are tested with simulated backgrounds of ESC-50 dataset. Compared to prior methods, our framework yields a 1.47% F-measure gain in the fixed setting and over 21% in altered environments, validating the importance of explicitly modeling semantic feature bias.},
  archive  = {J},
  author   = {Yizhou Tan and Haojun Ai and Shengchen Li and György Fazekas},
  doi      = {10.1109/TASLPRO.2025.3587468},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2986-2998},
  title    = {Domain adaptation of few-shot bioacoustic event detection in different environments},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture of LoRA experts with multi-modal and multi-granularity LLM generative error correction for accented speech recognition. <em>TASLPRO</em>, <em>33</em>, 2973-2985. (<a href='https://doi.org/10.1109/TASLPRO.2025.3589856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite improvements in automatic speech, performance drops with accented speech. Generative error correction (GER) leverages the linguistic knowledge of large language models (LLMs), outperforming typical language model methods. However, it lacks specificity in accented speech scenarios. Accents represent deviations from standard pronunciation, making multi-granularity pronunciation and semantic information essential for accented speech recognition. Moreover, accents exhibit considerable diversity, with each accent possessing distinct characteristics. In this study, we leverage GER to improve transcription accuracy by addressing the two primary features. We propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level pronunciation information. These methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through low-rank adaptation (LoRA) fine-tuning. We employ a three-stage strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge mono-accent LoRA experts within a single multi-modal GER to overcome accent diversity challenges. Furthermore, multi-granularity GER leverages N-best word-level and phoneme-level hypotheses from the HDMoLE model to predict final transcriptions. Experiments on a multi-accent English dataset show that our methods reduce word error rate by 67.35% compared to the baseline vanilla Whisper-large-v3 model.},
  archive  = {J},
  author   = {Bingshen Mu and Kun Wei and Pengcheng Guo and Lei Xie},
  doi      = {10.1109/TASLPRO.2025.3589856},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2973-2985},
  title    = {Mixture of LoRA experts with multi-modal and multi-granularity LLM generative error correction for accented speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual information-enhanced contrastive learning with margin for maximal speaker separability. <em>TASLPRO</em>, <em>33</em>, 2961-2972. (<a href='https://doi.org/10.1109/TASLPRO.2025.3583485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contrastive learning across various augmentations of the same utterance can enhance speaker representations’ ability to distinguish new speakers. This paper introduces a supervised contrastive learning objective that optimizes a speaker embedding space using label information from training data. Besides augmenting different segments of an utterance to form a positive pair, our approach generates multiple positive pairs by augmenting various utterances from the same speaker. However, employing contrastive learning for speaker verification (SV) presents two challenges: (1) softmax loss is ineffective in reducing intra-class variation, and (2) previous research has shown that contrastive learning can share information across the augmented views of an object but could discard task-relevant nonshared information, suggesting that it is essential to keep nonshared speaker information across the augmented views when constructing a speaker representation space. To overcome the first challenge, we incorporate an additive angular margin in the contrastive loss. For the second challenge, we maximize the mutual information (MI) between the squeezed low-level features and speaker representations to extract the nonshared information. Evaluations on VoxCeleb, CN-Celeb, and CU-MARVEL validate that our new learning objective enables ECAPA-TDNN to identify an embedding space that exhibits robust speaker discrimination.},
  archive  = {J},
  author   = {Zhe Li and Man-Wai Mak and Mert Pilanci and Helen Meng},
  doi      = {10.1109/TASLPRO.2025.3583485},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2961-2972},
  title    = {Mutual information-enhanced contrastive learning with margin for maximal speaker separability},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-vocabulary sound event localization and detection with joint learning of CLAP embedding and activity-coupled cartesian DOA vector. <em>TASLPRO</em>, <em>33</em>, 2946-2960. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We aim for an open-vocabulary sound event localization and detection (SELD) system that detects and localizes sound events in any category described by prompt texts. An open-vocabulary SELD system can be applied to various SELD tasks by changing prompt texts. A simple approach is to combine a language-audio model, such as a contrastive language-audio pretraining (CLAP) model, and a direction-of-arrival (DOA) estimation model. However, this combining approach cannot tackle overlapping sound events because the language-audio models output only one audio embedding, even for an audio input containing multiple events. Also, such a naive combination of two models can be sub-optimal regarding joint localization and detection. In this study, we present an embed-ACCDOA model, which jointly learns to output an embedding and the corresponding activity-coupled Cartesian DOA (ACCDOA) vector of each event in a track-wise manner, thereby tackling overlapping events. Each event’s embedding is trained to align with its corresponding audio and text embeddings inferred by a pretrained language-audio model, distilling its knowledge into the embed-ACCDOA model. We evaluate the proposed embed-ACCDOA model using an original synthetic dataset and two external SELD datasets. The knowledge distillation using both audio and text embeddings performs better than distillation using only one of the embeddings. The embed-ACCDOA model outperforms the naive combination. Moreover, it performs better than the official baseline system trained on the fully annotated training data of the target categories.},
  archive  = {J},
  author   = {Kazuki Shimada and Kengo Uchida and Yuichiro Koyama and Takashi Shibuya and Shusuke Takahashi and Yuki Mitsufuji and Tatsuya Kawahara},
  doi      = {10.1109/TASLPRO.2025.3587455},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2946-2960},
  title    = {Open-vocabulary sound event localization and detection with joint learning of CLAP embedding and activity-coupled cartesian DOA vector},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised frameworks for speaker verification via bootstrapped positive sampling. <em>TASLPRO</em>, <em>33</em>, 2932-2945. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent developments in Self-Supervised Learning (SSL) have demonstrated significant potential for Speaker Verification (SV), but closing the performance gap with supervised systems remains an ongoing challenge. SSL frameworks rely on anchor-positive pairs, constructed from segments of the same audio utterance. Hence, positives have channel characteristics similar to those of their corresponding anchors, even with extensive data-augmentation. Therefore, this positive sampling strategy is a fundamental limitation as it encodes too much information regarding the recording source in the learned representations. This article introduces Self-Supervised Positive Sampling (SSPS), a bootstrapped technique for sampling appropriate and diverse positives in SSL frameworks for SV. SSPS samples positives close to their anchor in the representation space, assuming that these pseudo-positives belong to the same speaker identity but correspond to different recording conditions. This method consistently demonstrates improvements in SV performance on VoxCeleb benchmarks when applied to major SSL frameworks, including SimCLR, SwAV, VICReg, and DINO. Using SSPS, SimCLR and DINO achieve 2.57% and 2.53% EER on VoxCeleb1-O, respectively. SimCLR yields a 58% relative reduction in EER, getting comparable performance to DINO with a simpler training framework. Furthermore, SSPS lowers intra-class variance and reduces channel information in speaker representations while exhibiting greater robustness without data-augmentation.},
  archive  = {J},
  author   = {Theo Lepage and Reda Dehak},
  doi      = {10.1109/TASLPRO.2025.3587462},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2932-2945},
  title    = {Self-supervised frameworks for speaker verification via bootstrapped positive sampling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic aperture local conformal autoencoder for semi-supervised speaker’s DOA tracking. <em>TASLPRO</em>, <em>33</em>, 2918-2931. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we address the problem of tracking the direction of arrival (DOA) of a moving speaker in noisy and reverberant environments. We aim to achieve this in real-time, using as few measurements as possible. Toward this goal, we present a semi-supervised learning scheme that tracks the speaker using acoustic features only. Specifically, we extend a recently proposed method called local conformal autoencoder (LOCA), a deep neural network (DNN)-based dimensionality reduction technique that considers the local information from adjacent measurements. We design a unique training procedure for LOCA, which uses a synthetic aperture training paradigm to take advantage of the speaker’s movement during training. We also add an anchoring loss term to the unsupervised LOCA model, which improves training stability and establishes a connection between the encoder mapping and the real-world position of the speaker. Finally, we conduct a comprehensive simulation study to demonstrate the effectiveness of our proposed method in dynamic environments with various levels of noise and reverberation.},
  archive  = {J},
  author   = {Idan Cohen and Sharon Gannot and Ofir Lindenbaum},
  doi      = {10.1109/TASLPRO.2025.3587465},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2918-2931},
  title    = {Synthetic aperture local conformal autoencoder for semi-supervised speaker’s DOA tracking},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning evaluation models from large language models for sequence generation. <em>TASLPRO</em>, <em>33</em>, 2902-2917. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic evaluation of sequence generation, which has traditionally relied on metrics such as BLEU and ROUGE, often struggles to capture the semantic accuracy of generated text due to an overemphasis on n-gram overlap. A promising solution to this issue is the development of model-based metrics, such as BLEURT and COMET. However, these approaches are typically limited by the scarcity of labeled evaluation data, which is essential for training evaluation models. In this work, we address this challenge by proposing the Customized Sequence Evaluation Metric (CSEM), a three-stage model training method that leverages large language models to generate labeled data for metric development, eliminating the need for human-labeled data. Furthermore, we extend the capabilities of CSEM to support a range of evaluation types, including single-aspect, multi-aspect, reference-free, and reference-based evaluations. This flexibility allows for the customization of metrics to fit various real-world scenarios. Experimental results on the SummEval benchmark demonstrate that CSEM can effectively train an evaluation model without human-labeled data. Additional experiments in reinforcement learning and reranking show that metrics developed through CSEM outperform traditional evaluation metrics, leading to significant improvements in sequence quality, as assessed by both commonly used metrics and ChatGPT.},
  archive  = {J},
  author   = {Chenglong Wang and Hang Zhou and Kaiyan Chang and Tongran Liu and Chunliang Zhang and Murun Yang and Quan Du and Tong Xiao and Yue Zhang and Jingbo Zhu},
  doi      = {10.1109/TASLPRO.2025.3587460},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2902-2917},
  title    = {Learning evaluation models from large language models for sequence generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation-guided cascading generation for low-shot relational triplet extraction. <em>TASLPRO</em>, <em>33</em>, 2887-2901. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Relational triplet extraction aims at constructing knowledge graphs, in the format of entity-relation-entity, from unstructured texts. Previous works mainly focus on an ideal scenario with sufficient labeled data, but fail to consider the cases with only a few (few-shot) or even no (zero-shot) labeled samples. Recently, although several methods have considered the few-shot relational triplet extraction, they can only tackle the one-relational triplet extraction without taking the multiple-triplet extraction into consideration. Meanwhile, these approaches overlook the accumulation of errors in the entity and relation extraction. Moreover, the approaches for zero-shot triplet extraction ignore the impact of fully utilizing relation labels and integrating negative samples to benefit the model train. In this paper, we carefully analyze the problems existing in the low-shot relational triplet extraction task (including both few-shot and zero-shot settings) and explore how to extract the triplets from the generative perspective to address the severe limitation of most previous methods, which only extract one relational triplet per sentence. Specifically, we propose a relation-guided cascading generation framework (CasGen) to take full advantage of the relation information to guide the entity extraction and then leverage the extracted entities to facilitate the relation extraction. Besides, our proposed cascading strategy also involves negative samples to make the model learn fine-grained representations. The experimental results show that our CasGen achieves better performance than many state-of-the-art methods by 19.5 (44.7%$\uparrow$) F1 score in the few-shot setting and 7.6 (30.9%$\uparrow$) F1 score in the zero-shot setting on average.},
  archive  = {J},
  author   = {Chengmei Yang and Bowei He and Yuyan Chen and Lianghua He and Chen Ma},
  doi      = {10.1109/TASLPRO.2025.3578775},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2887-2901},
  title    = {Relation-guided cascading generation for low-shot relational triplet extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the design of robust linear differential microphone arrays against low-rank noise. <em>TASLPRO</em>, <em>33</em>, 2874-2886. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The existing robust design method for differential microphone arrays (DMAs) allows control over array performance from either the directivity factor (DF) or white noise gain (WNG) perspective through quadratic form inequality constraints. This approach enables DMAs to mitigate the impact of array imperfections, such as sensor mismatch and self-noise, thus enhancing their practicality in real-world applications. While the existing method provides a solution for robust design, it is only applicable in scenarios where the noise covariance matrix (NCM) is full rank. The robust design of DMAs in the presence of low-rank noise remains an unsolved problem. To address this challenge and advance the theory of robust DMA design, we extend the existing method to incorporate low-rank noise, deriving several new classes of differential beamformers accordingly. The key contributions of this work are twofold: 1) we derive an explicit closed-form solution to low-rank quadratic-constrained quadratic programming problems by transforming them into a quadratic eigenvalue problem; and 2) we propose several new classes of differential beamformers using this method, offering fresh perspectives on the practical applications of DMAs.},
  archive  = {J},
  author   = {Jilu Jin and Xueqin Luo and Gongping Huang and Jingdong Chen and Jacob Benesty},
  doi      = {10.1109/TASLPRO.2025.3587444},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2874-2886},
  title    = {On the design of robust linear differential microphone arrays against low-rank noise},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study and design of robust differential beamformers for maximum front-to-back ratio. <em>TASLPRO</em>, <em>33</em>, 2861-2873. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Differential microphone arrays (DMAs) are widely used to capture high-fidelity acoustic signals due to their compact size, broadband spatial response, and high directivity. However, achieving high directivity often makes DMAs vulnerable to imperfections within the array. As a result, robust DMA design has attracted significant attention. This paper focuses on the design of a particular class of DMAs, specifically for maximizing the front-to-back ratio (FBR) under robustness constraints. While existing studies have explored the robust design of DMAs with a maximum directivity factor (MDF), these methods are not directly applicable to robust-maximum-FBR (RMFBR) differential beamformers due to the unique nature of the optimization problem. This work aims to address this challenge by framing the design of RMFBR beamformers as a fractional programming problem with a quadratic constraint. We then convert this fractional programming problem into a quadratic eigenvalue problem and introduce an iterative method for robust design. The primary contribution of this study is the proposal of a general optimization-based method for the design of robust MFBR differential beamformers, which has significant implications in practice. The performance of the proposed RMFBR beamformers is evaluated through both simulations and experiments, whose results justify their properties.},
  archive  = {J},
  author   = {Jilu Jin and Xueqin Luo and Gongping Huang and Jingdong Chen and Jacob Benesty},
  doi      = {10.1109/TASLPRO.2025.3587398},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2861-2873},
  title    = {Study and design of robust differential beamformers for maximum front-to-back ratio},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSELDNets: Pre-trained neural networks on a large-scale synthetic dataset for sound event localization and detection. <em>TASLPRO</em>, <em>33</em>, 2845-2860. (<a href='https://doi.org/10.1109/TASLPRO.2025.3587446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sound event localization and detection (SELD) has seen substantial advancements through learning-based methods. These systems, typically trained from scratch on specific datasets, have shown considerable generalization capabilities. Recently, deep neural networks trained on large-scale datasets have achieved remarkable success in the sound event classification (SEC) field, prompting an open question of whether these advances can be extended to the development of SELD foundation models. In this paper, leveraging the power of pre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on a large-scale synthetic dataset. The synthetic dataset, generated by convolving sound events with simulated spatial room impulse responses (SRIRs), contains 1,167 hours of audio clips with an ontology of 170 sound classes. These PSELDNets are applied to various SELD scenarios. When we adapt PSELDNets to specific scenarios, particularly in cases of low-resource data, we introduce a data-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on synthetic-test-set using collected SRIRs from the TAU Spatial Room Impulse Response Database (TAU-SRIR DB) and achieve satisfactory performance. We also carried out experiments to validate the transferability of PSELDNets to three publicly available datasets and our own real-world recordings. The results demonstrate that PSELDNets surpass state-of-the-art systems across all publicly available datasets. Given the need for direction-of-arrival estimation, SELD generally relies on sufficient multi-channel audio clips. However, incorporating the AdapterBit, PSELDNets show more efficient adaptability to various scenarios using minimal multi-channel or even just monophonic audio clips, outperforming traditional fine-tuning approaches.},
  archive  = {J},
  author   = {Jinbo Hu and Yin Cao and Ming Wu and Fang Kang and Feiran Yang and Wenwu Wang and Mark D. Plumbley and Jun Yang},
  doi      = {10.1109/TASLPRO.2025.3587446},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2845-2860},
  title    = {PSELDNets: Pre-trained neural networks on a large-scale synthetic dataset for sound event localization and detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WavJourney: Compositional audio creation with large language models. <em>TASLPRO</em>, <em>33</em>, 2830-2844. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. In a real-world scenario, however, we often need to generate audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements, simply based on textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-to-audio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues.},
  archive  = {J},
  author   = {Xubo Liu and Zhongkai Zhu and Haohe Liu and Yi Yuan and Qiushi Huang and Meng Cui and Jinhua Liang and Yin Cao and Qiuqiang Kong and Mark D. Plumbley and Wenwu Wang},
  doi      = {10.1109/TASLPRO.2025.3574867},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2830-2844},
  title    = {WavJourney: Compositional audio creation with large language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AudioSetCaps: An enriched audio-caption dataset using automated generation pipeline with large audio and language models. <em>TASLPRO</em>, <em>33</em>, 2817-2829. (<a href='https://doi.org/10.1109/TASLPRO.2025.3583354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the emergence of audio-language models, constructing large-scale paired audio-language datasets has become essential yet challenging for model development, primarily due to the time-intensive and labour-heavy demands involved. While large language models (LLMs) have improved the efficiency of synthetic audio caption generation, current approaches struggle to effectively extract and incorporate detailed audio information. In this paper, we propose an automated pipeline that integrates audio-language models for fine-grained content extraction, LLMs for synthetic caption generation, and a contrastive language-audio pretraining (CLAP) model-based refinement process to improve the quality of captions. Specifically, we employ prompt chaining techniques in the content extraction stage to obtain accurate and fine-grained audio information, while we use the refinement process to mitigate potential hallucinations in the generated captions. Leveraging the AudioSet dataset and the proposed approach, we create AudioSetCaps, a dataset comprising 1.9 million audio-caption pairs, the largest audio-caption dataset at the time of writing. The models trained with AudioSetCaps achieve state-of-the-art performance on audio-text retrieval with R@1 scores of 46.3% for text-to-audio and 59.7% for audio-to-text retrieval and automated audio captioning with the CIDEr score of 84.8. As our approach has shown promising results with AudioSetCaps, we create another dataset containing 4.1 million synthetic audio-language pairs based on the Youtube-8 M and VGGSound datasets.},
  archive  = {J},
  author   = {Jisheng Bai and Haohe Liu and Mou Wang and Dongyuan Shi and Wenwu Wang and Mark D. Plumbley and Woon-Seng Gan and Jianfeng Chen},
  doi      = {10.1109/TASLPRO.2025.3583354},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2817-2829},
  title    = {AudioSetCaps: An enriched audio-caption dataset using automated generation pipeline with large audio and language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STFT-domain least-distortion region-of-interest beamforming. <em>TASLPRO</em>, <em>33</em>, 2803-2816. (<a href='https://doi.org/10.1109/TASLPRO.2025.3580986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces a new method for maximizing array gain beamforming while focusing on a specific area in space. The main goal is to minimize the distortion of a desired signal when there is uncertainty about its actual direction of arrival. We develop a signal model and establish appropriate constraints and measures. We propose two beamformers that minimize distortion, each optimized for either white noise gain or directivity factor. Both beamformers are controlled by a design parameter that balances array gain and average signal distortion. We conduct extensive simulations to validate our approach. We examine the nature of this parameter across different areas of interest and evaluate its impact on the performance of the proposed beamformers using relevant measures. Finally, we conduct speech-signal experiments in various noisy and reverberant environments and consider different array calibration conditions. Our results show that the proposed approach outperforms recently proposed and traditional methods in terms of speech intelligibility, quality, and mean opinion scores, especially when there are significant deviations in the direction of arrival of the desired signal and when the reverberations are mild.},
  archive  = {J},
  author   = {Gal Itzhak and Israel Cohen},
  doi      = {10.1109/TASLPRO.2025.3580986},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2803-2816},
  title    = {STFT-domain least-distortion region-of-interest beamforming},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target speech detection with multimodal prompts. <em>TASLPRO</em>, <em>33</em>, 2788-2802. (<a href='https://doi.org/10.1109/TASLPRO.2025.3579304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional speaker diarization seeks to detect “who spoke when” according to speaker characteristics. Extending to target speech detection, we detect “when target speech event occurs” according to the semantic characteristics of speech. We propose a novel Multimodal Target Speech Detection (MM-TSD) framework, which accommodates diverse and multimodal prompts to specify target speech events in a flexible and user-friendly manner, including semantic language description, pre-enrolled speech, pre-registered face image, and audio-language logical prompts. We further propose a voice-face aligner module to project human voice and face representation into a shared space. We develop a multimodal dataset based on VoxCeleb2 for MM-TSD training and evaluation. Additionally, we conduct comparative analysis and ablation studies for each category of prompts to validate the efficacy of each component in the proposed framework. Furthermore, our framework demonstrates versatility in performing various signal processing tasks, including speaker diarization and overlap speech detection, using task-specific prompts. MM-TSD achieves robust and comparable performance as a unified system compared to specialized models. Moreover, MM-TSD shows capability to handle complex conversations for real-world dataset.},
  archive  = {J},
  author   = {Yidi Jiang and Ruijie Tao and Zhengyang Chen and Yanmin Qian and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3579304},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2788-2802},
  title    = {Target speech detection with multimodal prompts},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inplace frequency filtering and cepstral speech modeling in binaural speech enhancement. <em>TASLPRO</em>, <em>33</em>, 2775-2787. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents an improved Inplace Cepstral Convolutional Recurrent Neural Network (ICCRN+) model and evaluates it on the binaural speech enhancement task. The proposed ICCRN+ offers the following advantages: 1) By removing spectral downsampling, inplace neural networks preserve spatial cues and reverberation details as much as possible for better spatial filtering, dereverberation, and binaural cue preservation; 2) The neural cepstral modeling technique directly transforms frequency-domain neural network features to a cepstral space, thereby achieving effective harmonic restoration. This transformation utilizes the fast Fourier transform (FFT) to achieve full-band feature perception with $O(n\log {n})$ efficiency. 3) The consistent feature shape in inplace neural networks provides extra freedom to explore novel and efficient network topology designs in this paper. Comprehensive evaluations on dereverberation and denoising task demonstrate ICCRN+’s significant improvements in speech intelligibility and quality over baseline models, as well as the unique advantages of inplace neural networks in binaural cue preservation.},
  archive  = {J},
  author   = {Jinjiang Liu and Hao Li and Fei Chen and Zhiyong Wu and Xueliang Zhang},
  doi      = {10.1109/TASLPRO.2025.3577360},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2775-2787},
  title    = {Inplace frequency filtering and cepstral speech modeling in binaural speech enhancement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAT: Marker-lattice transformer for entity, relation and attribute extraction from chinese clinical text. <em>TASLPRO</em>, <em>33</em>, 2759-2774. (<a href='https://doi.org/10.1109/TASLPRO.2025.3579311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electronic Medical. Records (EMRs), extensively recognized as a significant repository of clinical experience and medical knowledge, often jeopardize their utility by being typically penned in free text, resulting in under-structured information. This lack of structural organization poses a major hurdle in fully capitalizing on the medical data embedded in these EMRs. Medical information extraction (IE) can fill this gap by converting the clinic text to structured data. In this paper, we propose Marker LAttice Transformer (MAT), a strong framework for medical IE. This framework is composed of three separate models, each designed for a specific task: medical entity recognition (MER), medical relation extraction (MRE), and medical attribute extraction (MAE). All the models are deeply based on markers embedded in the input text, with which the models compute representations from bottom to top layers. This allows the representations to encode deep semantic information, leading to better outputs. In addition, we enhance the models by lattice-style incorporation of medical dictionary information, further pre-training on large-scale EMRs, and auxiliary inputs of medical departments and EMR sections. We evaluated MAT using the HwaMei-500 dataset, the most comprehensive and current collection of Chinese electronic medical records. The framework demonstrated exceptional performance, achieving $F_{1}$ scores of 93.0%, 71.5%, and 88.9% for MER, MRE, and MAE tasks respectively. These scores outperform the baseline by large margins and also surpass the results of recent state-of-the-art relation extraction models. As a medical IE framework, we provide a practical example for other information extraction efforts, particularly those focusing on Chinese electronic medical record data.},
  archive  = {J},
  author   = {Sheng Wang and Enwei Zhu and Fangyuan Zhao and Dechao Bu and Jinpeng Li and Yi Zhao},
  doi      = {10.1109/TASLPRO.2025.3579311},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2759-2774},
  title    = {MAT: Marker-lattice transformer for entity, relation and attribute extraction from chinese clinical text},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning-based percussion transcription and $T\bar{a}la$ identification from low-resource audio. <em>TASLPRO</em>, <em>33</em>, 2749-2758. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study introduces a meta-learning-based approach for low-resource Tabla Stroke Transcription (TST) and $t\bar{a}la$ identification in Hindustani classical music. Using Model-Agnostic Meta-Learning (MAML), we address the challenges of limited annotated datasets and label heterogeneity, enabling rapid adaptation to new tasks with minimal data. The method is validated across various datasets, including tabla solo and concert recordings, demonstrating robustness in polyphonic audio scenarios. We propose two novel $t\bar{a}la$ identification techniques based on stroke sequences and rhythmic patterns. Additionally, the approach proves effective for Automatic Drum Transcription (ADT), showcasing its flexibility for Indian and Western percussion music. Experimental results show that the proposed method outperforms existing techniques in low-resource settings, significantly contributing to music transcription and studying musical traditions through computational tools.},
  archive  = {J},
  author   = {Rahul Bapusaheb Kodag and Vipul Arora},
  doi      = {10.1109/TASLPRO.2025.3581091},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2749-2758},
  title    = {Meta-learning-based percussion transcription and $T\bar{a}la$ identification from low-resource audio},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SANN-PSZ: Spatially adaptive neural network for head-tracked personal sound zones. <em>TASLPRO</em>, <em>33</em>, 2735-2748. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A deep learning framework for dynamically rendering personal sound zones (PSZs) with head tracking is presented, utilizing a spatially adaptive neural network (SANN) that inputs listeners’ head coordinates and outputs PSZ filter coefficients. The SANN model is trained using either simulated acoustic transfer functions (ATFs) with data augmentation for robustness in uncertain environments, or a mix of simulated and measured ATFs for customization under known conditions. Numerical experiments are conducted using an in-house PSZ rendering system with a linear loudspeaker array in the frequency range of 100-1500 Hz. It is found that augmenting room reflections in the training data improves model robustness more effectively than augmenting system imperfections, and that adding constraints related to filter compactness to the loss function does not significantly affect isolation performance. Comparisons of the best-performing model with traditional filter design methods show that, the SANN model achieves comparable or superior robustness in an unknown room environment without requiring explicit regularization. Furthermore, benchmark tests on a laptop demonstrate that the model offers greater data compression and computational efficiency than traditional rendering approaches, supporting SANN as a viable option for real-time applications.},
  archive  = {J},
  author   = {Yue Qiao and Edgar Choueiri},
  doi      = {10.1109/TASLPRO.2025.3581123},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2735-2748},
  title    = {SANN-PSZ: Spatially adaptive neural network for head-tracked personal sound zones},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequence-to-sequence neural diarization with automatic speaker detection and representation. <em>TASLPRO</em>, <em>33</em>, 2719-2734. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes a Sequence-to-Sequence Neural Diarization (S2SND) framework for both online and offline speaker diarization. Built upon a sequence-to-sequence architecture, S2SND integrates two novel components: (1) a masked speaker prediction mechanism that enables the model to detect unknown speakers without preextracted embeddings and (2) a target-voice speaker embedding extraction module that infers speaker representations using predicted voice activities as reference. This joint modeling approach eliminates the need for unsupervised clustering or permutation-invariant training. During inference, S2SND processes long-form audio in continuous blocks, leveraging a speaker-embedding buffer to maintain speaker consistency and enable low-latency prediction. It supports real-time detection of new speakers and efficient rescoring for improved offline performance. The entire diarization network is trained end-to-end, with binary cross-entropy and ArcFace loss guiding the detection and representation branches, respectively. Experimental resultsdemonstrate that S2SND achieves state-of-the-art diarization error rates (DERs) across multiple conditions. Specifically, it achieves DERs of 24.41% (online) and 21.95% (offline) on DIHARD-II, and 17.12% (online) and 15.13% (offline) on DIHARDIII, without using oracle voice activity detection. latex version:This paper proposes a Sequence-to-Sequence Neural Diarization (S2SND) framework for both online and offline speaker diarization. Built upon a sequence-to-sequence architecture, S2SND integrates two novel components: (1) a masked speaker prediction mechanism that enables the model to detect unknown speakers without pre-extracted embeddings and (2) a target-voice speaker embedding extraction module that infers speaker representations using predicted voice activities as reference. This joint modeling approach eliminates the need for unsupervised lustering or permutation-invariant training. During inference, S2SND processes longform audio in continuous blocks, leveraging a speaker-embedding buffer to maintain speaker consistency and enable low-latency prediction. It supports real-time detection of new speakers and efficient rescoring for improved offline performance. The entire diarization network is trained end-to-end, with binary cross-entropy and ArcFace loss guiding the detection and representation branches, respectively. Experimental results demonstrate that S2SND achieves state-of-the-art diarization error rates (DERs) across multiple conditions. Specifically, it achieves DERs of 24.41\% (online) and 21.95\% (offline) on DIHARD-II, and 17.12\% (online) and 15.13\% (offline) on DIHARD-III, without using oracle voice activity detection.},
  archive  = {J},
  author   = {Ming Cheng and Yuke Lin and Ming Li},
  doi      = {10.1109/TASLPRO.2025.3581032},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2719-2734},
  title    = {Sequence-to-sequence neural diarization with automatic speaker detection and representation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSM-iMagLS: ILD informed binaural signal matching for reproduction with head-mounted microphone arrays. <em>TASLPRO</em>, <em>33</em>, 2705-2718. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Headphone listening in applications such as augmented and virtual reality (AR and VR) relies on high-quality spatial audio to ensure immersion, making accurate binaural reproduction a critical component. As capture devices, wearable arrays with only a few microphones with irregular arrangement face challenges in achieving a reproduction quality comparable to that of arrays with a large number of microphones. Binaural signal matching (BSM) has recently been presented as a signal-independent approach for generating high-quality binaural signal using only a few microphones, which is further improved using magnitude-least squares (MagLS) optimization at high frequencies. This paper extends BSM with MagLS by introducing interaural level difference (ILD) into the MagLS, integrated into BSM (BSM-iMagLS). Using a deep neural network (DNN)-based solver, BSM-iMagLS achieves joint optimization of magnitude, ILD, and magnitude derivatives, improving spatial fidelity. Performance is validated through theoretical analysis, numerical simulations with diverse HRTFs and head-mounted array geometries, and listening experiments, demonstrating a substantial reduction in ILD errors while maintaining comparable magnitude accuracy to state-of-the-art solutions. The results highlight the potential of BSM-iMagLS to enhance binaural reproduction for wearable and portable devices.},
  archive  = {J},
  author   = {Or Berebi and Zamir Ben-Hur and David Lou Alon and Boaz Rafaely},
  doi      = {10.1109/TASLPRO.2025.3581096},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2705-2718},
  title    = {BSM-iMagLS: ILD informed binaural signal matching for reproduction with head-mounted microphone arrays},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring length generalization for transformer-based speech enhancement. <em>TASLPRO</em>, <em>33</em>, 2690-2704. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer network architecture has proven effective in speech enhancement. However, as its core module, self-attention suffers from quadratic complexity, making it infeasible for training on long speech utterances. In practical scenarios, speech enhancement models are often required to perform on noisy speech at run-time that is substantially longer than the training utterances. It remains a challenge how a Transformer-based speech enhancement model can generalize to long speech utterances. In this paper, extensive empirical studies are conducted to explore the model’s length generalization ability. In particular, we conduct speech enhancement experiments on four training objectives and evaluate with five metrics. Our studies establish that positional encoding is an effective instrument to dampen the effect of utterance length on speech enhancement. We first explore several existing positional encoding methods, and the results show that relative positional encoding methods exhibit a better length generalization property than absolute positional encoding methods. Additionally, we also explore a simpler and more effective positional encoding scheme, i.e. LearnLin, that uses only one trainable parameter for each attention head to scale the real relative position between time frames, which learns the different preferences on short- or long-term dependencies of these heads. The results demonstrate that our proposal exhibits excellent length generalization ability with comparable or superior performance than other state-of-the-art positional encoding strategies.},
  archive  = {J},
  author   = {Qiquan Zhang and Hongxu Zhu and Xinyuan Qian and Eliathamby Ambikairajah and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3578755},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2690-2704},
  title    = {Exploring length generalization for transformer-based speech enhancement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recurrent neural beamformer for multichannel speech enhancement under adverse noise condition. <em>TASLPRO</em>, <em>33</em>, 2674-2689. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech enhancement underlow signal-to-noise ratio conditions, such as for UAV applications, is challenging. Existing deep learning with beamforming approaches requires prior direction information or a sufficiently large number of estimated speech frames. We propose a recurrent neural beamformer (R-NBF) to enhance multichannel speech signals. The proposed R-NBF architecture comprises a set of beamformers followed by a multichannel complex spectral mapping twin-dilated U-net (TDU-net) and a feedback connection. This feedback connection serves as a conduit that facilitates frame-based adaptation between the output of the TDU net and the set of beamformers. We present an analysis framework based on Taylor’s first-order approximation and Wirtinger’s calculus that illustrates how the iterative optimization process results in a gradient that reduces the spatial covariance matrix error. The proposed approach is evaluated via simulation and on data recorded from a hexacopter hovering above an open field.},
  archive  = {J},
  author   = {Zhi-Wei Tan and Yuan Liu and Andy W. H. Khong and Anh H. T. Nguyen},
  doi      = {10.1109/TASLPRO.2025.3577345},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2674-2689},
  title    = {Recurrent neural beamformer for multichannel speech enhancement under adverse noise condition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPRSA: A plug-and-play language model with rational speech act inference for generating empathetic and engaging dialogue responses. <em>TASLPRO</em>, <em>33</em>, 2661-2673. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Developing an empathetic dialogue system has been a challenge for years. Although most system responses are grammatically correct and convey emotions to some extent, their quality still falls short of human standards. In this study, an integrated system called the Plug-and-Play Rational Speech Act (PPRSA) was developed by integrating a Transformer-based language model with two frameworks, namely a Plug-and-Play (PP) framework and a rational speech act (RSA) framework, to generate empathetic and engaging responses. The PP framework was used to control the generation through two attribute models, which ensure the empathetic intent and engagement of responses, thereby increasing empathy and relevance in the generated responses. The RSA framework was used to accelerate and improve response generation by reasoning the perturbation variation from the attribute models. It reduced the response generation time by 73.65% and exhibited enhanced performance compared to the Plug-and-Play language model (PPLM). EmpatheticDialogues was used as the evaluation dataset. In terms of objective evaluation, the PPRSA-based system accurately provided empathetic intent and engaging responses based on users’ emotions, thereby providing more comfort and support. The system achieved a BERTScore of 0.869, a Distinct-1 score of 5.19, a Distinct-2 score of 31.88, an empathetic intent accuracy of 41.84%, and an engagement accuracy of 72.24% . In terms of human evaluation, the system outperformed other baseline systems in terms of empathy, relevance, and fluency.},
  archive  = {J},
  author   = {Jeremy Chang and Chung-Hsien Wu},
  doi      = {10.1109/TASLPRO.2025.3578773},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2661-2673},
  title    = {PPRSA: A plug-and-play language model with rational speech act inference for generating empathetic and engaging dialogue responses},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task partially spoofed speech detection using a dual-view graph neural network assisted segment-level module. <em>TASLPRO</em>, <em>33</em>, 2647-2660. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Partially Spoofed Speech Detection (PSSD), as a multi-task learning problem, typically comprises segment- and utterance-level detection tasks, benefitting from diverse feature representations for effective classification. However, existing models for multi-tasks PSSD usually employ a shared feature processing module for the two tasks, which may lead to suboptimal performance compared with task-specific strategies. Further, most of existing works mainly capture segment-level information from a single view, which may result in poorly modeling local differences between fake and bonafide segments. In this regard, we propose a Dual-view Graph neural network Assisted segment-level Module (DGAM) for multi-task PSSD. The proposed approach contains three modules: Shared representation extracting, task-specific feature processing for the utterance-level task, and a Dual-View Graph Neural Network (D-GNN) with a dual-view consistency loss for the segment-level task through the graph attention mechanism with cosine similarity and heat kernel function with Euclidean distance as two different views, which capture semantic and Euclidean spatial relationships, respectively. Experimental evaluations on multiple spoofed-speech datasets demonstrate that, the proposed approach outperforms existing approaches in both segment- and utterance-level detection in terms of equal error rate, showcasing its effectiveness for the multi-task partially spoofed scenario.},
  archive  = {J},
  author   = {Zirui Ge and Xinzhou Xu and Haiyan Guo and Björn W. Schuller},
  doi      = {10.1109/TASLPRO.2025.3581019},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2647-2660},
  title    = {Multi-task partially spoofed speech detection using a dual-view graph neural network assisted segment-level module},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimpleSpeech 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models. <em>TASLPRO</em>, <em>33</em>, 2634-2646. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scaling Zero-shot Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale Zero-shot TTS models can be categorized into either Auto-regressive (AR) based (e.g., VALL-E) or Non-auto-regressive (NAR) based models (e.g., NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present (i) a detailed analysis of the influence of speech tokenizer and noisy label for Zero-shot TTS performance; (ii) four distinct types of sentence duration predictors; (iii) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets.},
  archive  = {J},
  author   = {Dongchao Yang and Rongjie Huang and Yuanyuan Wang and Haohan Guo and Dading Chong and Songxiang Liu and Xixin Wu and Helen Meng},
  doi      = {10.1109/TASLPRO.2025.3574847},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2634-2646},
  title    = {SimpleSpeech 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DYIN and dSWIPE: Differentiable variants of classical fundamental frequency estimators. <em>TASLPRO</em>, <em>33</em>, 2622-2633. (<a href='https://doi.org/10.1109/TASLPRO.2025.3581119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fundamental frequency (F0) estimation is a critical task in audio, speech, and music processing applications, such as speech analysis and melody extraction. F0 estimation algorithms generally fall into two paradigms: classical signal processing-based methods and neural network-based approaches. Classical methods, like YIN and SWIPE, rely on explicit signal models, offering interpretability and computational efficiency, but their non-differentiable components hinder integration into deep learning pipelines. Neural network-based methods, such as CREPE, are fully differentiable and flexible but often lack interpretability and require substantial computational resources. In this paper, we propose differentiable variants of two classical algorithms, dYIN and dSWIPE, which combine the strengths of both paradigms. These variants enable gradient-based optimization while preserving the efficiency and interpretability of the original methods. Through several case studies, we demonstrate their potential: First, we use gradient descent to reverse-engineer audio signals, showing that dYIN and dSWIPE produce smoother gradients compared to CREPE. Second, we design a two-stage vocal melody extraction pipeline that integrates music source separation with a differentiable F0 estimator, providing an interpretable intermediate representation. Finally, we optimize dSWIPE’s spectral templates for timbre-specific F0 estimation on violin recordings, demonstrating its enhanced adaptability over SWIPE. These case studies highlight that dYIN and dSWIPE successfully combine the flexibility of neural network-based methods with the interpretability and efficiency of classical algorithms, making them valuable tools for building end-to-end trainable and transparent systems.},
  archive  = {J},
  author   = {Sebastian Strahl and Meinard Müller},
  doi      = {10.1109/TASLPRO.2025.3581119},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2622-2633},
  title    = {DYIN and dSWIPE: Differentiable variants of classical fundamental frequency estimators},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reading when translating: Multi-modal document image machine translation with reading flow prediction. <em>TASLPRO</em>, <em>33</em>, 2606-2621. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Document Image Translation (DIT) aims to translate documents in images from one language to another. It is a multi-modal task that involves the cooperation of text, visual layout, and reading logical order. However, existing text-based or vision-based methods rely on solely textual or visual features. Layout-based methods are multi-modal but largely overlook the crucial reading logical order. To fully leverage the multi-modal information and exploit explicit modules to learn better reading logical order for DIT, this paper proposes the “reading-when-translating” guideline. It collaborates the translation process with an auxiliary “reading” process such that reading logical order directly contributes to translation. Following this guideline, we propose Document Reading and Translation Network (DocRTN), a novel unified framework that seamlessly integrates reading order into DIT, enabling the model to handle complex layouts by “reading” text in a human-like, coherent sequence. The unified framework comprises a reading flow decoder and a translation decoder. A novel feature decorator is proposed to harmonize the reading and translation channels, ensuring reading features are optimally adapted for translation. Extensive comparisons and analysis on 5 domains and 4 translation directions demonstrate DocRTN outperforms previous state-of-the-arts in all aspects.},
  archive  = {J},
  author   = {Zhiyang Zhang and Yaping Zhang and Yupu Liang and Cong Ma and Lu Xiang and Yang Zhao and Yu Zhou and Chengqing Zong},
  doi      = {10.1109/TASLPRO.2025.3578754},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2606-2621},
  title    = {Reading when translating: Multi-modal document image machine translation with reading flow prediction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-retention attack for continual named entity recognition. <em>TASLPRO</em>, <em>33</em>, 2591-2605. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continual Named Entity Recognition (CNER) aims to learn new entity types while preventing the catastrophic forgetting of previously learned types, expanding the flexibility of NER. However, the robustness of CNER models, which are typically evaluated through adversarial attacks, hasn’t been fully investigated due to two crucial challenges. Firstly, most works only focus on attacking NER models, in which the model is designed for a fixed dataset without considering the dynamic nature of real-world scenarios. These methods are not tailored to the challenges of the continual learning setting. Secondly, current textual attacks discretely change target words, which are widely used in sentence-level or document-level tasks. But in the word-level CNER task, even changes in one character may lead to a shift in the true label, making it difficult to reliably evaluate the results of the attack. Additionally, this type of attack is easily observed and lacks stealthiness. Thus, we propose a novel attack approach named Semantic-Retention Attack (SRA). To fit the continual learning tasks, SRA disrupts CNER models by enhancing catastrophic forgetting and knowledge confusion. To improve the reliability and stealthiness of the proposed attack, we perform a continuous transformation on the discrete texts and then apply a trainable SRA on them, ensuring the retention of the original semantics of the texts and avoiding changes in word-level ground truth labels. Experiments across ten CNER settings show our approach decreases performance to at most 51.63%, reaching the best degradation independent of the initial performance of CNER models while maintaining the best stealthiness, which exposes security vulnerabilities.1},
  archive  = {J},
  author   = {Yahan Yu and Zhengdong Yang and Fei Cheng and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3578765},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2591-2605},
  title    = {Semantic-retention attack for continual named entity recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage LoRA strategy for expanding language capabilities in multilingual ASR models. <em>TASLPRO</em>, <em>33</em>, 2576-2590. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Adapting multilingual automatic speech recognition (MASR) models to support new languages is crucial for enhancing global communication accessibility. However, extending these models often leads to catastrophic forgetting, impairing their ability to accurately process previously learned languages. To address this, we present a mixture-of-experts (MoE) strategy that employs Low-Rank Adaptation (LoRA) experts, each dedicated to a specific language. Unlike previous approaches that rely on a small neural network for gating, we utilize the language identification (LID) outputs of the MASR model itself to more precisely activate the appropriate LoRA experts. Our method involves training two sets of LoRAs for LID and ASR, respectively: the LID LoRAs adapt the MASR model for LID across both existing and new languages, and the ASR LoRAs consist of multiple language-specific LoRA experts for ASR. During inference, we first employ the LID LoRAs to identify the language, followed by the activation of the corresponding ASR LoRAs based on this identification. Unfortunately, this approach means LID errors also cause incorrect ASR LoRAs activation. We perform language-wise beam search to allow self-correction for such mistakes. When applied to the Whisper model and integrated with ten new languages from the Common Voice dataset, our approach achieves up to 13.8% and 10.4% relative improvements in word error rate (WER) for new and previously learned languages, respectively. This method effectively reduces catastrophic forgetting with less than 3% additional computation overhead compared to the standard LoRA implementation.},
  archive  = {J},
  author   = {Chin Yuen Kwok and Hexin Liu and Jia Qi Yip and Sheng Li and Eng Siong Chng},
  doi      = {10.1109/TASLPRO.2025.3578752},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2576-2590},
  title    = {A two-stage LoRA strategy for expanding language capabilities in multilingual ASR models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring cross-lingual transferability of multilingual transformers. <em>TASLPRO</em>, <em>33</em>, 2565-2575. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent studies have exhibited remarkable cross-lingual abilities of pre-trained multilingual Transformers, especially cross-lingual transferability. However, current methods do not measure cross-lingual transferability well, hindering the understanding of multilingual Transformers. In this paper, we propose IGap, a cross-lingual transferability metric for multilingual Transformers. IGap utilizes parallel data to measure cross-lingual transferability while taking training error into consideration, and can also estimate transferability without end-task data. Experimental results show that IGap outperforms baseline metrics for transferability measuring and transfer direction ranking. Besides, we conduct extensive systematic experiments where we compare transferability among various multilingual Transformers, fine-tuning algorithms, and transfer directions. More importantly, our results reveal three findings about cross-lingual transfer, which helps us to better understand multilingual Transformers.},
  archive  = {J},
  author   = {Heyan Huang and Zewen Chi and Xian-Ling Mao},
  doi      = {10.1109/TASLPRO.2025.3577329},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2565-2575},
  title    = {Measuring cross-lingual transferability of multilingual transformers},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised semantic retrieval via neural estimation of pointwise mutual information. <em>TASLPRO</em>, <em>33</em>, 2551-2564. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Semantic retrieval task aims to identify documents in a retrieval corpus that are semantically related to a query. Existing methods rely heavily on large-scale annotated training data, which are expensive to obtain. To relieve the reliance on annotated data, this paper investigates the semantic retrieval problem in an unsupervised setting, where a corpus of candidate documents is given without accessing the training data. In this setting, the utilization of deep learning techniques may be limited by the insufficient consideration of word-level correlations. To address this limitation, this study seeks to solve the unsupervised semantic retrieval problem by Pointwise Mutual Information (PMI). Specifically, we theoretically show that the solutions for semantic retrieval based on maximum a posteriori and maximum likelihood are equivalent to the maximumization of PMI, enabling us to tackle unsupervised semantic retrieval by PMI estimation. However, existing PMI estimation with the incidence matrix may suffer from the matrix-sparsity problem, which may cause the empirical distribution to be discordant with the ground truth distribution and lead to inaccurate PMI estimation. To tackle this problem, we propose a new unsupervised PMI estimation framework, i.e. UPMI, that leverages negative sampling to reformulate PMI estimation as a binary classification problem, whose optimal solution is exactly the PMI. We then employ neural networks equipped with attention mechanisms to realize the PMI function in the UPMI framework. Empirical studies on 13 benchmark datasets of three unsupervised semantic retrieval tasks confirm the effectiveness of our proposed model.},
  archive  = {J},
  author   = {Xiaohan Jiang and Richong Zhang and Yanzhao Zhang and Junfan Chen},
  doi      = {10.1109/TASLPRO.2025.3577349},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2551-2564},
  title    = {Unsupervised semantic retrieval via neural estimation of pointwise mutual information},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERVQ: Enhanced residual vector quantization with intra-and-inter-codebook optimization for neural audio codecs. <em>TASLPRO</em>, <em>33</em>, 2539-2550. (<a href='https://doi.org/10.1109/TASLPRO.2025.3579310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current neural audio codecs typically use residual vector quantization (RVQ) to discretize audio signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we propose Enhanced Residual Vector Quantization (ERVQ), a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available on the project page.},
  archive  = {J},
  author   = {Rui-Chen Zheng and Hui-Peng Du and Xiao-Hang Jiang and Yang Ai and Zhen-Hua Ling},
  doi      = {10.1109/TASLPRO.2025.3579310},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2539-2550},
  title    = {ERVQ: Enhanced residual vector quantization with intra-and-inter-codebook optimization for neural audio codecs},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning self-attention with local and syntactic dependencies for aspect sentiment triplet extraction. <em>TASLPRO</em>, <em>33</em>, 2527-2538. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment triple extraction (ASTE) is a demanding and emerging subtask of aspect-based sentiment analysis (ABSA). The primary objective of ASTE is to extract aspect and opinion terms and their corresponding sentiment polarities from a sentence. Existing methods stack additional graph convolutional networks (GCNs) on pretrained language models (PLMs) by propagating the representation according to the dependency relationship. Nevertheless, PLMs are pretrained on general language understanding tasks, introducing unnecessary relations in full connections in the Transformer encoder and disregarding both the local n-gram and syntactic dependency information. Moreover, the graph-based layers introduce additional computational loading when model convergence suffers from incorrect initialization. This study presents a lightweight PLM incorporating local n-gram and syntactic information for ASTE. Instead of stacking extra graph-based layers, we prune self-attention with local and syntactic dependency on the Transformer of the upper PLM layer to avoid inappropriate propagation between unrelated word pairs. A refining strategy is introduced to increase the final prediction distribution from the contextual prediction distribution. The experimental results on four benchmarks show that the proposed model outperforms the previously proposed methods.},
  archive  = {J},
  author   = {Li Yuan and Jin Wang and Liang-Chih Yu and Yi Cai and Xuejie Zhang},
  doi      = {10.1109/TASLPRO.2025.3577380},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2527-2538},
  title    = {Pruning self-attention with local and syntactic dependencies for aspect sentiment triplet extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic grid tagging scheme for event causality identification and classification. <em>TASLPRO</em>, <em>33</em>, 2516-2526. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a novel approach to event causality inference, a pivotal element in augmenting language comprehension across various applications, including question-answering, emotion extraction, and decision-making. Our study begins by addressing the complex task of identifying causal relationships in natural language, which are often subtly embedded across varied contextual settings. We outline four principal challenges in event causality analysis: extracting causality-related mentions, inferring causality within sentences, inferring causality across sentences, and determining the directionality of cause-effect relationships. We propose the Grid-Tagging Causality Inference (GCI) model to address these challenges. This model is designed to tag mentions, relations, and directional indicators of causality within an integrated framework. Utilizing a grid layout, the model enhances the tagging process for multiple relations and the discernment of cause-effect dynamics, employing a dynamic strategy to gradually extend the contextual scope, thus enabling a more refined comprehension of causality. The efficacy of the GCI model is assessed using the EventStoryLine benchmark, showcasing its superior capability in accurately predicting causality pairs, both within and across sentences, and establishing a new benchmark in determining the directional flow of causal relations. This research propels the field forward by tackling the previously unaddressed complexities in event causality inference, presenting a more effective and precise method for analyzing the intricate causal dynamics inherent in textual information.},
  archive  = {J},
  author   = {Hanxiu Chen},
  doi      = {10.1109/TASLPRO.2025.3577333},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2516-2526},
  title    = {Dynamic grid tagging scheme for event causality identification and classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PLEM: Prototype learning with evidence match for improving few-shot document-level relation extraction. <em>TASLPRO</em>, <em>33</em>, 2505-2515. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot document-level relation extraction (FSDLRE) aims to develop a model with the ability to generalize to new categories in the context of document-level relation extraction, using a small number of support samples. Among others, metric based meta-learning methods are widely used in FSDLRE, which involve constructing class prototypes using the contextual representation of the entire document and the representation of entity pairs for relation classification. However, in relation classification, only a subset of sentences in a document, known as evidence, is required to determine the relationship category of entity pairs. In this paper, we propose a prototype learning method with evidence match (PLEM). By introducing an evidence matching auxiliary task in the process of relation prototype construction, the model is guided to focus more on the semantics of evidence sentences when building prototypes, thereby enhancing the relation prototypes. We further design task-specific evidence prototypes, enabling the model to adapt to the evidence semantic space of different relation categories. Extensive experimental results demonstrate that PLEM outperforms the state-of-the-art methods, achieving an average improvement of 1.23% in Macro F1 across various settings of two FSDLRE benchmarks.},
  archive  = {J},
  author   = {Dinghao Pan and Yuanyuan Sun and Bo Xu and Jiru Li and Zhihao Yang and Ling Luo and Hongfei Lin and Jian Wang},
  doi      = {10.1109/TASLPRO.2025.3578781},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2505-2515},
  title    = {PLEM: Prototype learning with evidence match for improving few-shot document-level relation extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aggregation-free uncertainty estimation for CTC-based automatic speech recognition. <em>TASLPRO</em>, <em>33</em>, 2497-2504. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, deep neural networks (DNNs) are utilized in many assistance systems of various safety-critical relevance and have even established themselves in high-stakes decision making, such as medical purposes. Therefore, an accurate uncertainty estimation of the networks output is crucial for the system’s functional reliability in operational contexts. In this work, we investigate predictive uncertainty estimation for connectionist temporal classification (CTC) based automatic speech recognition models. We compute an uncertainty estimate based on the intermediate variables of the CTC Forward-Backward algorithm for any sub-sequence length, including token- and word-level, without the necessity to aggregate frame level uncertainties. Our approach outperforms prior state-of-the-art methods based on frame-level probability or entropy, as well as ensemble-based methods for prediction of recognition errors. We evaluate on TIMIT and Mozilla Common Voice (MCV) using pre-trained models of different model architectures. Our approach identifies 80% of a wav2vec2.0 model’s errors on MCV by selecting less than 9% of the tokens.},
  archive  = {J},
  author   = {Lars Rumberg and Christopher Gebauer and Jörn Ostermann},
  doi      = {10.1109/TASLPRO.2025.3578758},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2497-2504},
  title    = {Aggregation-free uncertainty estimation for CTC-based automatic speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speaker adaptive mixture of weight-decomposed LoRA experts for on-device end-to-end ASR. <em>TASLPRO</em>, <em>33</em>, 2485-2496. (<a href='https://doi.org/10.1109/TASLPRO.2025.3579303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {On-device speaker adaptation faces significant challenges in balancing recognition performance with computational efficiency. To address these challenges, Mixture-of-Experts (MoE) is a promising approach, which enhances model representation power and expands model capacity through a dynamic gating mechanism, thereby better handling speaker variability. However, conventional MoE models are often very large, making them challenging to deploy on resource-constrained edge devices. In this paper, we propose a novel speaker adaptive mixture of weight-decomposed LoRA experts (SAMD) approach, which uses low-rank adaptation (LoRA) modules as experts to reduce the number of trainable parameters in MoE, and further decomposes the experts into magnitude and directional components to enhance both the learning capacity and training stability of the experts. Specifically, SAMD is applied to the quantised and personalised end-to-end automatic speech recognition models, which combines test-time speaker adaptation to improve the performance of heavily compressed models in speaker-specific scenarios. Experiments have been performed on the LibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size, 31.6% and 33.8% relative word error rate reductions were achieved on the quantised Whisper model and Conformer-based attention-based encoder-decoder ASR model respectively, comparing to the original full precision models.},
  archive  = {J},
  author   = {Qiuming Zhao and Guangzhi Sun and Chao Zhang and Mingxing Xu and Thomas Fang Zheng},
  doi      = {10.1109/TASLPRO.2025.3579303},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2485-2496},
  title    = {Speaker adaptive mixture of weight-decomposed LoRA experts for on-device end-to-end ASR},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lifelong learning for entity linking. <em>TASLPRO</em>, <em>33</em>, 2471-2484. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Entity linking (EL), which seeks to align entity mentions in a text to standard entities in a knowledge base, is a vital task in various natural language processing applications. Recently, many approaches have been proposed for this task and have achieved satisfactory results. However, in real-world scenarios, as new entity domains frequently emerge, previous works need to re-train these EL models to ensure the effectiveness in both new entity domains and old ones. The process is often time-consuming and even becomes impractical as the number of entity domains increases. To tackle this problem, we introduce the lifelong learning setting into entity linking and propose the lifelong entity linking task, where entity domains are learned sequentially. We also propose a novel framework KEB-LEL for the lifelong entity linking, which exploits the weight-based pruning technique to update redundant parameters to learn new entity domains while retaining important parameters to preserve important knowledge previously learned in old entity domains. Moreover, we devise an additional regularization term to ensure stability by reducing over-fitting during the learning of new entity domains. The experimental results on 16 entity domains show that our proposed method significantly outperforms the state-of-the-art methods under our lifelong entity linking setting.},
  archive  = {J},
  author   = {Ying Zhang and Xuhui Sui and Kehui Song and Baohang Zhou and Wenya Guo and Xiaojie Yuan},
  doi      = {10.1109/TASLPRO.2025.3577342},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2471-2484},
  title    = {Lifelong learning for entity linking},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural TTS-based dynamic data augmentation for improved speech separation. <em>TASLPRO</em>, <em>33</em>, 2457-2470. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text-to-speech (TTS) synthetic data augmentation has been widely used in various speech processing tasks, but its effectiveness in speech separation remains understudied. In this paper, we present SpeakerAugment+ (SA+), a neural TTS-based dynamic data augmentation framework for speech separation. The SA+ framework consists of two modules: speaker module, which learns a Gaussian Mixture Model (GMM) to characterize the distribution over speaker embeddings in training data and samples unseen speaker embeddings during inference; speech module, which conditions speech synthesis using speaker embeddings with controllable speaker parameters. SA+ incorporates three augmentation techniques: speaker generation, parameter manipulation and utterance generation, enhancing speaker and utterance diversity from different perspectives. Following the SA+ framework, we design FS2-SA+ and Matcha-SA+, which are based on FastSpeech 2 and Matcha-TTS, respectively. We evaluate SA+ across multiple separation models and datasets, and the results demonstrate a substantial improvement in speech separation performance. Matcha-SA+ generates higher-quality speech and achieves better separation performance in intra-corpus tests. Conversely, FS2-SA+ supports a broad range of speaker parameter adjustments, leading to better generalization. Besides, the relationship between harmonicity and speech separation is a widely researched topic, and our findings indicate that speech lacking an explicit harmonic structure, when generated by neural TTS, can function as augmented data to improve speech separation. This research underscores the effectiveness of neural TTS-based data augmentation in speech separation tasks. We hope that our work can offer insights for future studies investigating data augmentation strategies within speech separation.},
  archive  = {J},
  author   = {Kai Wang and Cuicui Zhu and Lili Yin and Sheng Li and Madina Mansurova and Hao Huang},
  doi      = {10.1109/TASLPRO.2025.3578779},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2457-2470},
  title    = {Neural TTS-based dynamic data augmentation for improved speech separation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An orthogonality-based dual-memory framework for continual text classification. <em>TASLPRO</em>, <em>33</em>, 2444-2456. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continual text classification has played an increasingly important role in real-world applications, which focus on classifying the constantly emerging texts over time. Although many efforts have been made to overcome the major challenge of continual learning (CL), i.e., catastrophic forgetting (CF), the existing CL methods ignore the underlying interference among different tasks and still suffer from the CF issue. To get a deeper insight into CF, in this paper, we make the first attempt to study the relationship between forgetting and representation similarity in continual text classification tasks. Our thorough analyses of both real-world and pseudo datasets, for the pre-trained language model (PLM) based methods, demonstrate that forgetting can be effectively alleviated by reducing the similarity of the representations of different datasets. Based on our findings, we develop a novel Orthogonality-based Dual-memory Framework (Org-Frame), which is able to minimize the interference among different tasks via a two-stage learning process analogous to the human memory system. Org-Frame can also be combined with the LoRA fine-tuning method to enhance the continual learning capability of large language models (LLMs). Extensive experiments conducted on large-scale benchmarks using BERT and LLaMA demonstrate the effectiveness and robustness of our proposed framework in continual text classification tasks.},
  archive  = {J},
  author   = {Han Zhang and Yu Lei and Bin Liang and Hui Wang and Lin Gui and Ruifeng Xu},
  doi      = {10.1109/TASLPRO.2025.3577396},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2444-2456},
  title    = {An orthogonality-based dual-memory framework for continual text classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise and reverberation-controllable voice conversion. <em>TASLPRO</em>, <em>33</em>, 2430-2443. (<a href='https://doi.org/10.1109/TASLPRO.2025.3578761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent developments in voice conversion (VC) systems aim to improve speech quality and speaker similarity even under challenging background factors, such as noise or reverberation. Although these factors have traditionally been viewed as unwanted interference, they can also provide valuable information in certain applications like movie dubbing. In this paper, we present a new approach to VC that allows for better control over background factors like noise and reverberation. Our method is capable of generating both clean and noisy-reverberant converted speech. During inference, we first enhance interfered input speech data using speech enhancement models to reduce noise and reverberation. We also use a denoising model and a reverberation time (T60) estimator to extract important background information from the interfered speech data, which is then used alongside a target speaker’s code to generate converted speech with the background factors. Our experiments show that the use of speech data with various acoustic conditions during training leads to improved speech quality and speaker similarity, even when clean data are not available. Moreover, optimizing the model with clean and pseudo-clean data further enhances the results.},
  archive  = {J},
  author   = {Yeonjong Choi and Chao Xie and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2025.3578761},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2430-2443},
  title    = {Noise and reverberation-controllable voice conversion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-side context predictive completion for simultaneous machine translation. <em>TASLPRO</em>, <em>33</em>, 2418-2429. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Simultaneous machine translation (SiMT) initiates the translation process while continuously receiving input from a streaming source. Unlike full-sentence machine translation, SiMT faces the inherent issue of generating target words based on only partial source input, which typically limits the quality of translation. To address this issue, we propose the novel Predictive Source Completion Framework (PSCF) to predict and complete missing source-side information, thereby enhancing SiMT performance under the same latency. In particular, PSCF improves prediction accuracy through real supervisory signals and better completes the missing information through the representation information of the predicted words. Experimental results show that PSCF improves the translation performance over the strong baselines in regard to IWSLT14 De $\rightarrow$ En, IWSLT15 En $\rightarrow$ Vi, and WMT15 De $\rightarrow$ En.},
  archive  = {J},
  author   = {Andong Chen and Kehai Chen and Yang Xiang and Xuefeng Bai and Muyun Yang and Tiejun Zhao and Min Zhang},
  doi      = {10.1109/TASLPRO.2025.3577318},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2418-2429},
  title    = {Source-side context predictive completion for simultaneous machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PMF-CEC: Phoneme-augmented multimodal fusion for context-aware ASR error correction with error-specific selective decoding. <em>TASLPRO</em>, <em>33</em>, 2402-2417. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {End-to-end automatic speech recognition (ASR) models often struggle to accurately recognize rare words. Previously, we introduced an ASR postprocessing method called error detection and context-aware error correction (ED-CEC), which leverages contextual information such as named entities and technical terms to improve the accuracy of ASR transcripts. Although ED-CEC achieves a notable success in correcting rare words, its accuracy remains low when dealing with rare words that have similar pronunciations but different spellings. To address this issue, we proposed a phoneme-augmented multimodal fusion method for context-aware error correction (PMF-CEC) method on the basis of ED-CEC, which allowed for better differentiation between target rare words and homophones. Additionally, we observed that the previous ASR error detection module suffers from overdetection. To mitigate this, we introduced a retention probability mechanism to filter out editing operations with confidence scores below a set threshold, preserving the original operation to improve error detection accuracy. Experiments conducted on five datasets demonstrated that our proposed PMF-CEC maintains reasonable inference speed while further reducing the biased word error rate compared with ED-CEC, showing a stronger advantage in correcting homophones. Moreover, our method outperforms other contextual biasing methods, and remains valuable compared with LLM-based methods in terms of faster inference and better robustness under large biasing lists.},
  archive  = {J},
  author   = {Jiajun He and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2025.3577356},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2402-2417},
  title    = {PMF-CEC: Phoneme-augmented multimodal fusion for context-aware ASR error correction with error-specific selective decoding},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APG: Automatic prompt generation for improved document summarization. <em>TASLPRO</em>, <em>33</em>, 2388-2401. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, prompting has become a mainstream technique for querying language models in natural language generation tasks such as machine translation, question answering and document summarization. In document summarization in particular, prompts have been used to control aspects of the predicted summary (e.g., style and length) and generally improve its quality. However, all the prompt-based summarization approaches proposed to date rely on significant manual design effort or annotation of additional training data. For this reason, in this paper we propose a novel approach for document summarization – nicknamed automatic prompt generation (APG) – allowing the model to learn to simultaneously generate its own prompts and the predicted summaries without the need for any supplementary annotations. This capability has been achieved by augmenting the training data with sets of keywords automatically extracted from the input documents with an off-the-shelf, unsupervised keyword extractor, and generating the prompts directly from the hidden states of the model’s encoder. The proposed approach has been evaluated over five, diverse summarization datasets, showing that its performance has proved higher than that of many baseline models, including a state-of-the-art large language model, with increases of up to $+9.50$ ROUGE $R_{1}$ pp over BART-base, and $+4.02$ $F_{BERT}$ score pp over GPT-4o mini, as well as evidence of improved generalization.},
  archive  = {J},
  author   = {Jacob Parnell and Iñigo Jauregi Unanue and Scott Matthews and Massimo Piccardi},
  doi      = {10.1109/TASLPRO.2025.3577363},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2388-2401},
  title    = {APG: Automatic prompt generation for improved document summarization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Musical word embedding for music tagging and retrieval. <em>TASLPRO</em>, <em>33</em>, 2377-2387. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Word embedding has become an essential means for text-based information retrieval. Typically, word embeddings are learned from large quantities of general and unstructured text data. However, in the domain of music, word embedding may have difficulty understanding musical contexts or recognizing music-related entities like artists and tracks. To address this issue, we propose a new approach called Musical Word Embedding (MWE), which involves learning from various types of texts, including both everyday and music-related vocabulary. We integrate MWE into an audio-word joint representation framework for tagging and retrieving music, using words like tag, artist, and track that have different levels of musical specificity. Through extensive experiments, we demonstrate that the effectiveness of musical supervision varies by task - specifically, tag-level supervision improves tagging performance while track-level supervision enhances retrieval performance. This finding suggests that the choice of musical supervision in representation learning needs to be carefully considered based on the target task. To balance this compromise, we suggest multi-prototype training that uses words with different levels of musical specificity jointly. We evaluate both word embedding and audio-word joint embedding on four tasks (tag rank prediction, music tagging, query-by-tag, and query-by-track) across two datasets (Million Song Dataset and MTG-Jamendo). The results show that the suggested MWE is more efficient and effective for both in-domain and out-of-domain datasets compared to conventional word embedding.},
  archive  = {J},
  author   = {SeungHeon Doh and Jongpil Lee and Dasaem Jeong and Juhan Nam},
  doi      = {10.1109/TASLPRO.2025.3577408},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2377-2387},
  title    = {Musical word embedding for music tagging and retrieval},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNN-based geometry-invariant DOA estimation with microphone positional encoding and complexity gradual training. <em>TASLPRO</em>, <em>33</em>, 2360-2376. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent deep neural network (DNN)-based direction-of-arrival (DOA) estimation methods demonstrate greater robustness compared to conventional methods. However, most DNNs are designed for specific microphone arrays, requiring retraining for different geometries. Although some geometry-invariant methods employ conventional features, they often incur high computational costs and are prone to interference. This paper proposes a geometry-invariant DOA estimation network (GI-DOAEnet). It employs microphone positional encodings (MPEs) that modulate microphone spherical coordinates using sinusoidal functions to provide unique geometric information. Combining MPEs and channel-wise latent features, the network captures spatio-temporal correlations through geometry-invariant modules, ultimately producing spatial spectra. To train GI-DOAEnet effectively with diverse geometries, a complexity gradual training strategy is introduced, integrating deeply supervised curriculum learning with a novel multi-stage geometry learning method. This gradually increases task difficulty by training through varying soft labels and staged transitions from fixed to dynamic geometries. GI-DOAEnet achieves superior performance over baselines in terms of degree error and accuracy across diverse acoustic environments, while reducing FLOPS and inference time by eliminating pair-wise features and employing channel-wise aggregation.},
  archive  = {J},
  author   = {Min-Sang Baek and Joon-Hyuk Chang and Israel Cohen},
  doi      = {10.1109/TASLPRO.2025.3577336},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2360-2376},
  title    = {DNN-based geometry-invariant DOA estimation with microphone positional encoding and complexity gradual training},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multi-perspective document structures for news discourse profiling. <em>TASLPRO</em>, <em>33</em>, 2349-2359. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Learning the discourse structures of news articles is vital for obtaining valuable structural information around main events, which has been shown useful for many natural language processing applications. This paper proposes the extraction of graphs explicitly representing the discourse structure, coreference structure, and semantic similarities within a document. Concept embeddings are then updated using a weighted combination of feature embeddings, modeled after capsule networks, and resulting in improved context-dependent representation of the content of each news article. Experimental tests on the NewsDiscourse dataset demonstrate the effectiveness of our proposed method.},
  archive  = {J},
  author   = {Hang Yang and Dianbo Sui and Yubo Chen and Kang Liu and Jun Zhao},
  doi      = {10.1109/TASLPRO.2025.3574865},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2349-2359},
  title    = {Exploring multi-perspective document structures for news discourse profiling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging sound source trajectories for universal sound separation. <em>TASLPRO</em>, <em>33</em>, 2337-2348. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing methods utilizing spatial information for sound source separation require prior knowledge of the direction of arrival (DOA) of the source or utilize estimated but imprecise localization results, which impairs the separation performance, especially when the sound sources are moving. In fact, sound source localization and separation are interconnected problems, that is, sound source localization facilitates sound separation while sound separation contributes to refined source localization. This paper proposes a method utilizing the mutual facilitation mechanism between sound source localization and separation for moving sources. The proposed method comprises three stages. The first stage is initial tracking, which tracks each sound source from the audio mixture based on the source signal envelope estimation. These tracking results may lack sufficient accuracy. The second stage involves mutual facilitation: Sound separation is conducted using preliminary sound source tracking results. Subsequently, sound source tracking is performed on the separated signals, thereby refining the tracking precision. The refined trajectories further improve separation performance. This mutual facilitation process can be iterated multiple times. In the third stage, a neural beamformer estimates precise single-channel separation results based on the refined tracking trajectories and multi-channel separation outputs. Simulation experiments conducted under reverberant conditions and with moving sound sources demonstrate that the proposed method can achieve more accurate separation based on refined tracking results.},
  archive  = {J},
  author   = {Donghang Wu and Xihong Wu and Tianshu Qu},
  doi      = {10.1109/TASLPRO.2025.3577352},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2337-2348},
  title    = {Leveraging sound source trajectories for universal sound separation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical diffusion model for zero-shot singing voice synthesis with MIDI priors. <em>TASLPRO</em>, <em>33</em>, 2326-2336. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Singing voice synthesis systems have significantly advanced; however achieving high-quality singing voices in zero-shot tasks remains challenging. Traditional singing voice synthesis models face challenges in predicting the fundamental frequency (F0) of unseen speakers. In this study, we propose MIDI-Voice 2, which uses MIDI-driven priors to achieve high-quality singing voice synthesis, even in zero-shot tasks. We introduce a diffusion-based singing voice synthesis model that operates without F0. MIDI-Voice 2 consists of two diffusion models: a prior generator and a singing voice generator. The prior generator uses MIDI-driven priors, including accurate melody, to generate MIDI-style priors, and the singing voice generator uses these MIDI-style priors along with content and timbre information to generate singing voices. Disentangling the melody from timbre allows speaker adaptation without predicting the F0 of an unseen speaker. Additionally, we use a transformer-based diffusion to generate higher-quality audio in zero-shot tasks. Our experiments demonstrate that MIDI-Voice 2 improves speaker-adaptation without F0 and produces high-quality audio in zero-shot tasks.},
  archive  = {J},
  author   = {Dong-Min Byun and Seung-Bin Kim and Seong-Whan Lee},
  doi      = {10.1109/TASLPRO.2025.3577324},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2326-2336},
  title    = {Hierarchical diffusion model for zero-shot singing voice synthesis with MIDI priors},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A composite predictive-generative approach to monaural universal speech enhancement. <em>TASLPRO</em>, <em>33</em>, 2312-2325. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is promising to design a single model that can suppress various distortions and improve speech quality, i.e., universal speech enhancement (USE). Compared to supervised learning-based predictive methods, diffusion-based generative models have shown greater potential due to the generative capacities from degraded speech with severely damaged information. However, artifacts may be introduced in highly adverse conditions, and diffusion models often suffer from a heavy computational burden due to many steps for inference. In order to jointly leverage the superiority of prediction and generation and overcome the respective defects, in this work we propose a universal speech enhancement model called PGUSE by combining predictive and generative modeling. Our model consists of two branches: the predictive branch directly predicts clean samples from degraded signals, while the generative branch optimizes the denoising objective of diffusion models. We utilize the output fusion and truncated diffusion scheme to effectively integrate predictive and generative modeling, where the former directly combines results from both branches and the latter modifies the reverse diffusion process with initial estimates from the predictive branch. Extensive experiments on several datasets verify the superiority of the proposed model over state-of-the-art baselines, demonstrating the complementarity and benefits of combining predictive and generative modeling.},
  archive  = {J},
  author   = {Jie Zhang and Haoyin Yan and Xiaofei Li},
  doi      = {10.1109/TASLPRO.2025.3577387},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2312-2325},
  title    = {A composite predictive-generative approach to monaural universal speech enhancement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deep learning analysis for raga identification in indian art music. <em>TASLPRO</em>, <em>33</em>, 2302-2311. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Raga identification is an important problem within the domain of Indian Art music, as Ragas are fundamental to its composition and performance, playing a crucial role in music retrieval, preservation, and education. Few studies that have explored this task employ approaches such as signal processing, Machine Learning (ML), and more recently, Deep Learning (DL) based methods. However, a key question remains unanswered in all these works: do these ML/DL methods learn and interpret Ragas in a manner similar to human experts? Besides, a significant roadblock in this research is the unavailability of an ample supply of rich, labeled datasets, which drives these ML/DL-based methods. In this paper, firstly we curate a dataset comprising 191 hours of Hindustani Classical Music (HCM) recordings, annotate it for Raga and tonic labels, and train a CNN-LSTM model for the task of Automatic Raga Identification (ARI). We achieve a chunk-wise f1-measure of 0.89 for a subset of 12 Raga classes. Following this, we make one of the first attempts to employ model explainability techniques: SoundLIME and GradCAM++ for Raga identification, to evaluate whether the classifier’s predictions align with human understanding of Ragas. We compare the generated explanations with human expert annotations and further analyze individual test examples to understand the role of regions highlighted by explanations in making correct or incorrect predictions made by the model. Our results demonstrate a significant alignment of the model’s understanding with human understanding, and the thorough analysis validates the effectiveness of our approach.},
  archive  = {J},
  author   = {Parampreet Singh and Vipul Arora},
  doi      = {10.1109/TASLPRO.2025.3574839},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2302-2311},
  title    = {Explainable deep learning analysis for raga identification in indian art music},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Least-distortion maximum gain beamformer for time-domain region-of-interest beamforming. <em>TASLPRO</em>, <em>33</em>, 2286-2301. (<a href='https://doi.org/10.1109/TASLPRO.2025.3577388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Region-of-interest (ROI) beamformers are very useful in cases where precise information about the source position is unavailable, such as situations involving estimation errors or source movement. This paper presents an approach to ROI beamforming using convolutive filters in the time domain. The proposed beamformer can focus on a specific spatial region of interest while suppressing interference and noise from other directions. We formulate the signal model, considering a desired source signal propagating from a particular ROI and an array of sensors capturing a convolved version of the signal in some noise field. Appropriate performance measures are introduced to derive and analyze ROI-centric beamformers. The ROI beamformer is designed to maximize the gain in signal-to-noise ratio under a constraint on the signal distortion in the spatial region of interest. Additional parameters are introduced to balance the beamformer’s gain, distortion, and robustness. Simulations demonstrate the effectiveness of the proposed method in ROI beamforming and interference suppression.},
  archive  = {J},
  author   = {Ariel Frank and Israel Cohen},
  doi      = {10.1109/TASLPRO.2025.3577388},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2286-2301},
  title    = {Least-distortion maximum gain beamformer for time-domain region-of-interest beamforming},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Template-order driven feature integration with generative models for aspect sentiment triplet extraction. <em>TASLPRO</em>, <em>33</em>, 2275-2285. (<a href='https://doi.org/10.1109/TASLPRO.2025.3572352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment analysis (ABSA), which explores the nuanced sentiments individuals express toward specific services or products, has shown significant potential in practical applications. Recently, the aspect sentiment triplet extraction field has witnessed significant advancements through the prowess of generative models. However, existing methods face critical limitations: (1) fixed-order decoding (e.g., aspect term$\rightarrow$ opinion term$\rightarrow$ sentiment polarity) ignores interdependencies between elements; (2) token-by-token generation fails to model term boundaries, which struggles with multi-word terms or sentences containing multiple triplets. To address these challenges, we propose a Template-Order driven Feature Integration (TOFI) framework, which integrates two novel modules: Template-Order Prompt (TOP) and Feature Progressive Sequence Labeling (FPSL). TOP employs various orders of sentiment elements to prompt the model to generate sentiment tuples from different perspectives, each using a different element order. It then aggregates the sentiment tuples that appear across all orders. FPSL leverages sequence labeling to integrate the rich semantic information of aspect terms and opinion terms into the model, enhancing its ability to capture semantic nuances. Furthermore, the TOFI framework introduces unified multi-domain training, prefixing domain identifiers to inputs for multi-task learning across datasets. Extensive experiments on multiple public benchmarks demonstrate that the proposed framework consistently surpassed representative baseline models.},
  archive  = {J},
  author   = {Jiazhou Chen and Ruiqiang Guo},
  doi      = {10.1109/TASLPRO.2025.3572352},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2275-2285},
  title    = {Template-order driven feature integration with generative models for aspect sentiment triplet extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing predicted signal-to-distortion ratio: A new microphone selection criterion for beamforming in acoustic sensor networks. <em>TASLPRO</em>, <em>33</em>, 2259-2274. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper addresses the problem of selecting an effective subset of microphones in acoustic sensornetworks (ASNs) for speech enhancement applications. A basic approach to this problem is to select asubset of microphones such that the output signal-to-noise ratio (oSNR) of the beamforming output signalis maximized. However, oSNR does not necessarily correlate well with widely used signal quality measures such as the signal-to-distortion ratio (SDR), perceptual evaluation of speech quality (PESQ), and short-time objective intelligibility (STOI). We here introduce a new measure that predicts SDR for beamforming output signals. This measure, termed predicted SDR (pSDR), demonstrates a better correlation with SDR, PESQ, and STOI compared to oSNR. Whereas the original SDR requires an oracle reference signal for measurements, the proposed pSDR is defined in the short-time Fourier transform domain and can be estimated solely from source covariance matrices already used for obtaining beamformers. Therefore, our pSDR can be immediately used as a measure for selecting microphones instead of oSNR. We also extend conventional subset selection methods to jointly select a reference microphone of beamformers in a unified optimization problem. This extension is crucial because the choice of the reference microphone can significantly affect the quality of enhanced signals in ASNs. Numerical experiments suggest that, as a measure for selecting both a microphone subset and a reference microphone, pSDR is generally better than, or at least comparable to, oSNR in terms of SDR, PESQ, and STOI. We also demonstrate the effectiveness of selecting a microphone subset jointly with a reference microphone of beamformers.},
  archive  = {J},
  author   = {Rintaro Ikeshita and Tomohiro Nakatani and Tsubasa Ochiai and Shoko Araki},
  doi      = {10.1109/TASLPRO.2025.3574841},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2259-2274},
  title    = {Maximizing predicted signal-to-distortion ratio: A new microphone selection criterion for beamforming in acoustic sensor networks},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised blind joint dereverberation and room acoustics estimation with diffusion models. <em>TASLPRO</em>, <em>33</em>, 2244-2258. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents an unsupervised method for single-channel blind dereverberation and room impulse response (RIR) estimation, called BUDDy. The algorithm is rooted in Bayesian posterior sampling: it combines a likelihood model enforcing fidelity to the reverberant measurement, and an anechoic speech prior implemented by an unconditional diffusion model. We design a parametric filter representing the RIR, with exponential decay for each frequency subband. Room acoustics estimation and speech dereverberation are jointly carried out, as the filter parameters are iteratively estimated and the speech utterance refined along the reverse diffusion trajectory. In a blind scenario where the RIR is unknown, BUDDy successfully performs speech dereverberation in various acoustic scenarios, significantly outperforming other blind unsupervised baselines. Unlike supervised methods, which often struggle to generalize, BUDDy seamlessly adapts to different acoustic conditions. This paper extends our previous work by offering new experimental results and insights into the algorithm’s versatility. We demonstrate the robustness of our proposed method to new acoustic and speaker conditions, as well as its adaptability to high-resolution singing voice dereverberation, using both instrumental metrics and subjective listening evaluation. We study BUDDy’s performance for RIR estimation and observe it surpasses a state-of-the-art supervised DNN-based estimator on mismatched acoustic conditions. Finally, we investigate the sensitivity of informed dereverberation methods to RIR estimation errors, thereby motivating the joint acoustic estimation and dereverberation design.},
  archive  = {J},
  author   = {Jean-Marie Lemercier and Eloi Moliner and Simon Welker and Vesa Välimäki and Timo Gerkmann},
  doi      = {10.1109/TASLPRO.2025.3574988},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2244-2258},
  title    = {Unsupervised blind joint dereverberation and room acoustics estimation with diffusion models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-forecasting of time-varying spatial-frequency map for selective fixed-filter multichannel ANC based on dynamic factor graph. <em>TASLPRO</em>, <em>33</em>, 2232-2243. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Selective fixed-filter active noise control (SFANC) has emerged as a prominent and practical solution for noise suppression due to its fast response, high computational efficiency, and stability. However, its noise reduction performance still degrades when dealing with some dynamic noise scenarios where the filter switching cannot catch up with the variation of the primary noise source. To tackle this challenge, we proposed a novel approach to employ the dynamic factor graph (DFG) in predicting the direction and frequency bands of primary noise. Once the SFANC algorithm acquires this joint frequency and direction information in advance, it can generate a more suitable control filter for the coming noise in a shorter time. By involving the Bhattacharyya distance Matrix (BdM) to simultaneously render spatial and frequency information of the primary noise, the proposed method further facilitates the precision of the filter selection for multichannel ANC systems. The BdM can also be adaptive to non-uniformed array. Moreover, experimental results demonstrate the stability of BdM spectra in detecting joint frequency and directional information and showcase the method’s ability to accurately predict noise variation and respond promptly. Comparative analysis against other popular algorithms validates the superiority of our approach in dealing with dynamic and moving noise sources. Therefore, these theoretical investigations and numerical simulations show that this proposed method can potentially be applied to cope with the dynamic noise in real-world scenarios.},
  archive  = {J},
  author   = {Xiruo Su and Dongyuan Shi and Bin Wu and Lingyun Ye and Woon-Seng Gan},
  doi      = {10.1109/TASLPRO.2025.3570939},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2232-2243},
  title    = {Co-forecasting of time-varying spatial-frequency map for selective fixed-filter multichannel ANC based on dynamic factor graph},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERP: A blind estimator of room parameters for single-channel noisy speech signals. <em>TASLPRO</em>, <em>33</em>, 2215-2231. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Room acoustical parameters (RAPs), room geometrical parameters (RGPs) and instantaneous occupancy levels are essential metrics for parameterizing the room acoustical characteristics (RACs) of a sound field around a listener’s local environment, offering comprehensive indications for various applications. Current blind estimation methods either fail to cover a broad range of real-world acoustic environments in the context of real background noise or estimate only a few RAPs and RGPs from noisy single-channel speech signals. In addition, they are limited in their ability to estimate the instantaneous occupancy level. In this paper, we propose BERP, a new universal approach to blindly estimate RAPs, RGPs, and occupancy levels. It consists of two modules: one for RAPs and RGPs and another for occupancy levels. For the former task, we use a shared room feature encoder that combines attention mechanisms with convolutional layers to learn common features across room parameters, and multiple separate parametric predictors for continuous estimation of each parameter in parallel. The combination of attention and convolutions enables the model to capture acoustic features both locally and globally from speech, yielding more robust and multitask generalizable common features. Separate predictors allow the model to independently optimize for each room parameter to reduce task learning conflict and improve per-task performance. This architecture enables universal and efficient estimation of room parameters while maintaining satisfactory performance. For occupancy level estimation, we reuse the identical encoder with a classification head, exploiting the encoder’s strong sequencefeature extraction. To evaluate the effectiveness of the proposed approach, we compile a task-specific dataset from several publicly available datasets, including synthetic and real reverberant recordings. The results reveal that BERP achieves state-of-the-art (SOTA) performance and excellent adaptability to real-world scenarios.},
  archive  = {J},
  author   = {Lijun Wang and Yixian Lu and Ziyan Gao and Kai Li and Jianqiang Huang and Yuntao Kong and Shogo Okada},
  doi      = {10.1109/TASLPRO.2025.3574849},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2215-2231},
  title    = {BERP: A blind estimator of room parameters for single-channel noisy speech signals},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilingual lexical simplification via zero-shot paraphrasing. <em>TASLPRO</em>, <em>33</em>, 2200-2214. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Lexical simplification (LS) method based on pretrained language models is a straightforward yet powerful approach for generating potential substitutes for a complex word through analysis of its contextual surroundings. Nonetheless, these methods necessitate distinct pretrained models tailored to diverse languages, often overlooking the imperative task of preserving a sentence’s meaning. In this paper, we propose a novel multilingual LS method via zero-shot paraphrasing (LSPG), as paraphrases provide diversity in word selection while preserving the sentence’s meaning. We regard paraphrasing as a zero-shot translation task within multilingual neural machine translation that supports hundreds of languages. Once the input sentence is channeled into the paraphrasing, we embark on the generation of the substitutes. This endeavor is underpinned by a pioneering decoding strategy that concentrates exclusively on the lexical modifications of the complex word. To utilize the strong capabilities of large language models (LLM), we further introduce a novel approach PromLS that incorporates the results of LSPG to generate heuristic-enhanced context, enabling the LLM to generate diverse candidate substitutions. Experimental results demonstrate that LSPG surpasses BERT-based methods and zero-shot GPT3-based methods significantly in English, Spanish, and Portuguese. We also demonstrate a substantial improvement achieved by PromLS compared to the previous state-of-the-art LLM approach. LS approaches usually assume that complex words and their replacements are individual terms, concentrating on word-for-word substitutions. To tackle the more challenging task of multi-word lexical simplification, including phrase-to-phrase replacements, we extend LSPG and PromLS into MultiLSPG and MultiPromLS. MultiLSPG identifies multi-word expressions matched with their corresponding word counts in specific positions, while MultiPromLS, akin to PromLS, utilizes these candidates to generate a heuristic-enhanced prompt for GPT3. Our methods demonstrate exceptional performance in managing complex multi-word phrases, underscoring their wide-ranging applicability.},
  archive  = {J},
  author   = {Kang Liu and Jipeng Qiang and Yun Li and Yi Zhu and Yunhao Yuan and Kaixun Hua and Xindong Wu},
  doi      = {10.1109/TASLPRO.2025.3570945},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2200-2214},
  title    = {Multilingual lexical simplification via zero-shot paraphrasing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised fact-checking via recursively verifying presuppositions. <em>TASLPRO</em>, <em>33</em>, 2189-2199. (<a href='https://doi.org/10.1109/TASLPRO.2025.3572768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presuppositions are implicit assumptions that interlocutors take for granted. Verifying presuppositions is crucial for reasoning-based fact-checking that aims to reason about True or False, but existing pipelines have overlooked it. We first raise this critical issue with the community and propose a system that addresses the generation, verification, and reasoning of presuppositions through a recursive question-answering process. Importantly, our system automatically determines which knowledge entails which presuppositions, making it an unsupervised system for evidence retrieval, fact-checking, and justification production. We conduct a comprehensive experiment on the FEVER, Vitamin C, and FEVEROUS-S datasets. Our system outperforms competitive baselines, including LLM, and achieves state-of-the-art unsupervised performance. We highlight this critical issue to encourage further research on reasoning-based fact-checking.},
  archive  = {J},
  author   = {Xiucheng Lyu and Runcong Zhao and Jiazheng Li and Bin Liang and Min Yang and Lin Gui and Ruifeng Xu},
  doi      = {10.1109/TASLPRO.2025.3572768},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2189-2199},
  title    = {Unsupervised fact-checking via recursively verifying presuppositions},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A front-end adaptation network for improving speech recognition performance in packet loss and noisy environments. <em>TASLPRO</em>, <em>33</em>, 2175-2188. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robust automatic speech recognition (ASR) in packet loss and noisy environments remains a significant challenge. Large pretrained transformer models have made notable strides in improving ASR performance across diverse domains. However, considerable room remains for improvement, even in moderate packet loss and noise conditions. Enhancing these models is particularly difficult because retraining is computationally prohibitive, and fine-tuning introduces the risk of domain shift, which can degrade performance in other languages or environments. We introduce a novel method that leverages a front-end adaptation network to improve word error rate (WER) performance in scenarios with packet loss and noise. Our approach addresses the constraints of working with large pretrained ASR models while avoiding retraining or fine-tuning. We connect an adaptation network to a frozen ASR model, where the network is trained to modify corrupted input spectra using both the loss function of the ASR model and an enhancement loss. This strategy allows the system to adapt to packet loss and noise without compromising the performance of the original ASR model or generalization across domains. The method focuses on improving WER rather than signal quality or intelligibility, targeting it for ASR applications. We conduct a comprehensive set of experiments on various types of noise. Our results demonstrate that the adaptation network significantly reduces WER in all conditions while preserving the foundational performance of the pretrained ASR model.},
  archive  = {J},
  author   = {Yehoshua Dissen and Shiry Yonash and Israel Cohen and Joseph Keshet},
  doi      = {10.1109/TASLPRO.2025.3574840},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2175-2188},
  title    = {A front-end adaptation network for improving speech recognition performance in packet loss and noisy environments},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resampling filter design for multirate neural audio effect processing. <em>TASLPRO</em>, <em>33</em>, 2163-2174. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural networks have become ubiquitous in audio effects modelling, especially for guitar amplifiers and distortion pedals. One limitation of such models is that the sample rate of the training data is implicitly encoded in the model weights and therefore not readily adjustable at inference. Recent work explored modifications to recurrent neural network architecture to approximate a sample rate independent system, enabling audio processing at a rate that differs from the original training rate. This method works well for integer oversampling and can reduce aliasing caused by nonlinear activation functions. For small fractional changes in sample rate, fractional delay filters can be used to approximate sample rate independence, but in some cases this method fails entirely. Here, we explore the use of real-time signal resampling at the input and output of the neural network as an alternative solution. We investigate several resampling filter designs and show that a two-stage design consisting of a half-band IIR filter cascaded with a Kaiser window FIR filter can give similar or better results to the previously proposed model adjustment method with many fewer filtering operations per sample and less than one millisecond of latency at typical audio rates. Furthermore, we investigate interpolation and decimation filters for the task of integer oversampling and show that cascaded half-band IIR and FIR designs can be used in conjunction with the model adjustment method to reduce aliasing in a range of distortion effect models.},
  archive  = {J},
  author   = {Alistair Carson and Vesa Välimäki and Alec Wright and Stefan Bilbao},
  doi      = {10.1109/TASLPRO.2025.3574878},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2163-2174},
  title    = {Resampling filter design for multirate neural audio effect processing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic logic reasoning model for argumentation relation classification. <em>TASLPRO</em>, <em>33</em>, 2151-2162. (<a href='https://doi.org/10.1109/TASLPRO.2025.3574884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Argumentation relation classification (ARC) aims to identify the relations between argumentation component (AC) pairs. Existing studies have achieved impressive results on public datasets through the utilization of AC-level features, fine-grained attributes, and structural information. However, they overlook the logical relations and structure within AC pairs and fall short in performing dynamic logical reasoning for ARC. Therefore, in this paper, we propose a DynAmic Logic rEasoning model (DALE) with logic graph construction and dynamic logical reasoning pattern learning for ARC. Specifically, we first construct a logic graph to establish the logical structure within individual ACs and between AC pairs. Then we propose dynamic reasoning modules comprising four distinct types of graph cells to explore different logical reasoning operations. Each cell incorporates a dynamic router and interconnects others to establish a dynamic routing network. This design enables the model to autonomously adapt and select distinct operational paths based on the specific nature of the input instance. Our comprehensive experiments, conducted on three public datasets, reveal that our model outperforms strong baselines.},
  archive  = {J},
  author   = {Yang Sun and Caihua Yang and Jianzhu Bao and Bin Liang and Min Yang and Ruifeng Xu},
  doi      = {10.1109/TASLPRO.2025.3574884},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2151-2162},
  title    = {Dynamic logic reasoning model for argumentation relation classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bimodal sentiment analysis based on a pre-trained model and masked attention fusion. <em>TASLPRO</em>, <em>33</em>, 2139-2150. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, sentiment analysis based on deep learning and the fusion of multimodal data has become a research focus in the field of natural language processing (NLP). However, previous studies have proposed neural networks with complex internal designs but without a corresponding performance improvement. Additionally, the lack of effective fusion and interaction between modalities has limited the ability of these models to make sentiment decisions. In this study, we introduces a bimodal sentiment analysis model, GTA-BERT, which is based on pre-trained models and masked attention fusion to tackle these problems. This model leverages technologies such as RoBERTa, HuBERT, the graph convolutional network (GCN), and the bidirectional gated recurrent unit (BiGRU) to extract audio features and enhance text semantic features. Subsequently, it dynamically adjusts the attention weights for different modalities using masked attention to achieve optimal sentiment decisions. Finally, it combines the masked attention matrix with the sentence dependency parsing information retained by the GCN for classification. Experimental evaluations of this model are conducted on three publicly available Chinese and English datasets, and it is found to outperform mainstream models. The research findings can contribute to sentiment analysis decision-making in fields such as personalized recommendations, psychological counseling, and intelligent chat systems.},
  archive  = {J},
  author   = {Li Cai and Hongbin Li and Nan Zhang and Jing He},
  doi      = {10.1109/TASLPRO.2025.3564826},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2139-2150},
  title    = {Bimodal sentiment analysis based on a pre-trained model and masked attention fusion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward enhancing cross-lingual domain knowledge sharing and transferring for multilingual domain adaptation in NMT. <em>TASLPRO</em>, <em>33</em>, 2125-2138. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multilingual Neural Machine Translation (NMT) excels in sharing knowledge across languages and transferring insights from high-resource languages to improve performance in low-resource languages. However, its performance lags in specific domains such as legal and medical. Previous works have focused on adding language-specific and domain-specific adapters to achieve domain adaptation. Although effective, these adapter-based methods only use domain data to train additional parameters, limiting the performance of multilingual NMT. In this paper, we propose CDSTX, a novel approach that achieves robust multilingual domain adaptation without relying on adapters, solely leveraging multilingual models. Specifically, we utilize the pretrained model XLM-R and frozen embeddings to preserve its robust multilingual capabilities and design a two-stage training strategy for domain adaptation. This includes separate training for the decoder and fine-tuning of the entire model, ensuring that the model effectively acquires domain knowledge and correctly represents domain-specific text. Moreover, to better utilize the domain information conveyed implicitly by the training data, we devise special domain tokens at the beginning of the source and target sentences called Source&Target Domain Tags. In addition, back translation is employed to enhance the cross-lingual transfer ability of our approach. Our proposed method is evaluated on two datasets across three translation tasks: Single-domain Multilingual NMT, Multi-domain bilingual NMT, and Multilingual Multi-domain NMT. Notably, in the single-domain multilingual NMT task, CDSTX significantly enhances zero-shot domain translation performance, achieving improvements of up to +30 BLEU points through the utilization of back-translation techniques. Even in the bilingual multi-domain NMT task where specific domain data for the target translation direction is unavailable, our method consistently outperforms all SOTA methods. Moreover, in the most challenging Multilingual Multi-domain NMT task, CDSTX surpasses most baseline methods and attains a remarkable gain of +3.0 to +4.6 BLEU points.},
  archive  = {J},
  author   = {Zhengtao Yu and Jinlei Xu and Xiang Huang and Zhongfen Deng and Yonghua Wen and Li Sun and Yuxin Huang and Hao Peng and Philip S. Yu},
  doi      = {10.1109/TASLPRO.2025.3570946},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2125-2138},
  title    = {Toward enhancing cross-lingual domain knowledge sharing and transferring for multilingual domain adaptation in NMT},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USEF-TSE: Universal speaker embedding free target speaker extraction. <em>TASLPRO</em>, <em>33</em>, 2110-2124. (<a href='https://doi.org/10.1109/TASLPRO.2025.3572756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Target speaker extraction aims to separate the voice of a specific speaker from mixed speech. Traditionally, this process has relied on extracting a speaker embedding from a reference speech, in which a speaker recognition model is required. However, identifying an appropriate speaker recognition model can be challenging, and using the target speaker embedding as reference information may not be optimal for target speaker extraction tasks. This paper introduces a Universal Speaker Embedding-Free Target Speaker Extraction (USEF-TSE) framework that operates without relying on speaker embeddings. USEF-TSE utilizes a multi-head cross-attention mechanism as a frame-level target speaker feature extractor. This innovative approach allows mainstream speaker extraction solutions to bypass the dependency on speaker recognition models and better leverage the information available in the enrollment speech, including speaker characteristics and contextual details. Additionally, USEF-TSE can seamlessly integrate with other time-domain or time-frequency domain speech separation models to achieve effective speaker extraction. Experimental results show that our proposed method achieves state-of-the-art (SOTA) performance in terms of Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) on the WSJ0-2mix, WHAM!, and WHAMR! datasets, which are standard benchmarks for monaural anechoic, noisy and noisy-reverberant two-speaker speech separation and speaker extraction. The results on the LibriMix and the blind test set of the ICASSP2023 DNS Challenge demonstrate that the model performs well on more diverse and out-of-domain data.},
  archive  = {J},
  author   = {Bang Zeng and Ming Li},
  doi      = {10.1109/TASLPRO.2025.3572756},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2110-2124},
  title    = {USEF-TSE: Universal speaker embedding free target speaker extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving end-to-end speech-to-text translation with document-level context. <em>TASLPRO</em>, <em>33</em>, 2098-2109. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, end-to-end speech-to-text translation (E2E-ST) has emerged as a promising approach. Existing ST models mostly focus on learning from sentence-level speech while neglecting the valuable contextual information carried by document-level speech context. To leverage document-level context, in this paper we propose Context-Aware Speech-to-text Translation (CAST), a context-aware ST model which uses document-level context to enhance the encoding of the current speech sentence under a multi-task training framework. To better leverage the bimodal document-level context during training, on the one hand, we adopt a mixup strategy that mixes up the speech and text representations at sentence-level. On the other hand, we train the model to selectively utilize contextual information by employing a selective strategy. The experimental results from the MuST-C benchmark indicate that CAST significantly enhances the sentence-level baseline, yielding an average BLEU score of 30.4 and a COMET score of 78.7 across the eight translation directions.},
  archive  = {J},
  author   = {Xinyu Tian and Haoran Wei and Zhengxian Gong and Junhui Li and Jun Xie},
  doi      = {10.1109/TASLPRO.2025.3570951},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2098-2109},
  title    = {Improving end-to-end speech-to-text translation with document-level context},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRSF: Pitched-noise-robust score following based on multimodal learning. <em>TASLPRO</em>, <em>33</em>, 2084-2097. (<a href='https://doi.org/10.1109/TASLPRO.2025.3571294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Score following is an important task in the field of music information retrieval, which maps the real-time audio input of a musical performance to the corresponding musical score. In the real practice of score following, due to imperfect audio capture or the difficulty of digital score acquisition, the input audio may contain other pitched sounds beyond the score, such as background accompaniment or the sounds of other musical instruments in an ensemble. However, existing methods are limited in resisting this kind of noise, which may lead to unstable score following for the target instrument. To address this issue, this paper proposes a pitched-noise-robust score following (PRSF) method based on a cyclic feedback and early fusion mechanism that cyclically connects traditional and deep models. First, a joint local pitch inference (JLPI) deep model is proposed to address the problem that traditional models have difficulty in discriminating the target sound source under pitched noise. This model is designed to infer the pitch of the target instrument based on multimodal learning, which is accompanied by a specially designed training strategy. Second, in order to sufficiently utilize the audio and score information and achieve robust score following, a hidden Markov model (HMM)-based score position inference model is introduced. For the first time, our PRSF method solves the score following task in orchestral performance scenarios where only the score of the target musical instrument is informed. Using cross-dataset evaluation, our proposed method achieves an align rate of 75.5% within a threshold of 300 ms under the pitched noise environment of orchestral music.},
  archive  = {J},
  author   = {Zhicheng Lian and Haonan Cheng and Shulin Liu and Jiawan Zhang},
  doi      = {10.1109/TASLPRO.2025.3571294},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2084-2097},
  title    = {PRSF: Pitched-noise-robust score following based on multimodal learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agent-SiMT: Agent-assisted simultaneous translation with large language models. <em>TASLPRO</em>, <em>33</em>, 2074-2083. (<a href='https://doi.org/10.1109/TASLPRO.2025.3566220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Simultaneous Machine Translation (SiMT) generates target translations in real-time while reading the source sentence. It relies on a policy to determine the optimal timing for producing translations, aiming to achieve both low response latency and high translation quality. Existing SiMT methods typically utilize the traditional Transformer architecture, employing complicated dynamic programming algorithms to jointly optimize policy decisions and translation generation. While these methods excel at determining policies, their translation capabilities are suboptimal. In contrast, Large Language Models (LLMs), trained on extensive corpora, exhibit exceptional generation capabilities but struggle to learn translation policies through traditional Transformer-based training methods. To address these limitations, we introduce Agent-SiMT, a novel framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains a policy-decision agent and a translation agent. The policy-decision agent is managed by a SiMT model, which determines the policy using partial source sentences and translations. The translation agent, powered by an LLM, generates the target translation according to the policy decisions. These two agents work collaboratively to perform SiMT. Experiments demonstrate that our method attains state-of-the-art performance.},
  archive  = {J},
  author   = {Shoutao Guo and Shaolei Zhang and Zhengrui Ma and Min Zhang and Yang Feng},
  doi      = {10.1109/TASLPRO.2025.3566220},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2074-2083},
  title    = {Agent-SiMT: Agent-assisted simultaneous translation with large language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine anomalous sound detection using spectral-temporal modulation representations derived from machine-specific filterbanks. <em>TASLPRO</em>, <em>33</em>, 2059-2073. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Early detection of factory machinery malfunctions is crucial in industrial applications. In machine anomalous sound detection (ASD), different machines exhibit unique vibration-frequency ranges based on their physical properties. Meanwhile, the human auditory system is adept at tracking both temporal and spectral dynamics of machine sounds. Consequently, integrating the computational auditory models of the human auditory system with machine-specific properties can be an effective approach to machine ASD. We first quantified the frequency importances of four types of machines using the Fisher ratio (F-ratio). The quantified frequency importances were then used to design machine-specific non-uniform filterbanks (NUFBs), which extract the log non-uniform spectrum (LNS) feature. The designed NUFBs have a narrower bandwidth and higher filter distribution density in frequency regions with relatively high F-ratios. Finally, spectral and temporal modulation representations derived from the LNS feature were proposed. These proposed LNS feature and modulation representations are input into an autoencoder neural-network-based detector for ASD. The quantification results from the training set of the Malfunctioning Industrial Machine Investigation and Inspection dataset with a signal-to-noise (SNR) of 6 dB reveal that the distinguishing information between normal and anomalous sounds of different machines is encoded non-uniformly in the frequency domain. By highlighting these important frequency regions using NUFBs, the LNS feature can significantly enhance performance using the metric of AUC (area under the receiver operating characteristic curve) under various SNR conditions. Furthermore, modulation representations can further improve performance. Specifically, temporal modulation is effective for fans, pumps, and sliders, while spectral modulation is particularly effective for valves.},
  archive  = {J},
  author   = {Kai Li and Khalid Zaman and Xingfeng Li and Masato Akagi and Jianwu Dang and Masashi Unoki},
  doi      = {10.1109/TASLPRO.2025.3570956},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2059-2073},
  title    = {Machine anomalous sound detection using spectral-temporal modulation representations derived from machine-specific filterbanks},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot audio captioning using soft and hard prompts. <em>TASLPRO</em>, <em>33</em>, 2045-2058. (<a href='https://doi.org/10.1109/TASLPRO.2025.3567770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In traditional audio captioning methods, a model is usually trained in a fully supervised manner using a human-annotated dataset containing audio-text pairs and then evaluated on the test set from the same dataset. Such methods have two limitations. First, these methods are often data-hungry and require time-consuming and expensive human annotations to obtain audio-text pairs. Second, these models often suffer from performance degradation in cross-domain scenarios, i.e., when the input audio comes from a different domain than the training set, and this issue has received little attention. To address these issues, we propose a new zero-shot method for audio captioning. Our method is built on the contrastive language-audio pre-training (CLAP) model. During training, the model reconstructs the ground-truth caption using the CLAP text encoder. In the inference stage, the model generates text descriptions from the CLAP audio embeddings of given audio inputs. To enhance the ability of the model in transitioning from text-to-text generation to audio-to-text generation, we propose to use the mixed-augmentations-based soft prompt to learn more robust latent representations, leveraging instance replacement and embedding augmentation. Additionally, we introduce the retrieval-based acoustic-aware hard prompt to improve the cross-domain performance of the model by employing the domain-agnostic label information of sound events. Extensive experiments on AudioCaps and Clotho benchmarks show the effectiveness of our proposed method, which outperforms other zero-shot audio captioning approaches for in-domain scenarios and outperforms the compared methods for cross-domain scenarios, underscoring the generalization ability of our method.},
  archive  = {J},
  author   = {Yiming Zhang and Xuenan Xu and Ruoyi Du and Haohe Liu and Yuan Dong and Zheng-Hua Tan and Wenwu Wang and Zhanyu Ma},
  doi      = {10.1109/TASLPRO.2025.3567770},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2045-2058},
  title    = {Zero-shot audio captioning using soft and hard prompts},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HIFI-stego: A high-fidelity embedding audio steganography based on audio features decoupling. <em>TASLPRO</em>, <em>33</em>, 2032-2044. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The higher audio quality of steganography is directly correlated with the increased resistance to steganalysis tools. The advancement of generative AI technologies, particularly those that decouple style features from content, has shown promising developments by facilitating the creation of superior media content. This paper introduces the concept of audio decoupling and presents HIFI-Stego, an embedding audio steganography technique that aims to improve security while maintaining elevated stego audio quality. HIFI-Stego comprises a generator based on the encoder-decoder architecture and a secret message extractor. The encoder of the generator decouples the original audio, yielding the content vector, while the vocoder WORLD is employed to extract the style vector to preserve high-quality audio related features such as timbre and tone. Subsequently, the decoder embeds the secret vector into the decoupled content vector and then couples it with the style vector to generate high-fidelity stego audio. As embedding is not done in traditional time domain or frequency domain, existing analysis tools targeted at traditional steganographies fail to effectively detect the presence of the hidden message. The secret message extractor reuses the generator encoder and augments it with a single-layer convolutional neural network, resulting in a simplified structure suitable for lightweight deployment. Experimental results demonstrate that HIFI-Stego outperforms traditional generative and embedding steganographies in terms of audio quality, steganographic capacity, anti-analysis ability, and concealment.},
  archive  = {J},
  author   = {Sanfeng Zhang and Baiyu Tian and Yang Gao and Xingyu Liu and Wang Yang},
  doi      = {10.1109/TASLPRO.2025.3570942},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2032-2044},
  title    = {HIFI-stego: A high-fidelity embedding audio steganography based on audio features decoupling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving factual error correction for abstractive summarization via data distillation and conditional-generation cloze. <em>TASLPRO</em>, <em>33</em>, 2021-2031. (<a href='https://doi.org/10.1109/TASLPRO.2025.3567744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Improving factual consistency in abstractive summarization has been a focus of recent research. One promising approach is the post-editing method. However, previous works have yet to make sufficient use of factual factors in summaries and suffer from the negative effect of the training datasets. In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task. FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered. Then, we propose a data distillation method to generate a more faithful summarization dataset SummDSC via multiple-dimensional evaluation. We validate our method on both non-LLM and LLM-generated datasets. Besides BART and T5, we implement FactCloze using DeepSeek prompt. Finally, we examine the differences between LLM-based and traditional evaluation metrics for factual error correction.},
  archive  = {J},
  author   = {Yingqi Zhu and Yiyang Li and Lei Li and Dingxin Hu and Xueyi Hao and Dongsheng Chen and Xingyue Zhang and Zhejun Zhang and Yanquan Zhou and Marina Litvak and Natalia Vanetik},
  doi      = {10.1109/TASLPRO.2025.3567744},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2021-2031},
  title    = {Improving factual error correction for abstractive summarization via data distillation and conditional-generation cloze},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sound speed perturbation robust audio: Impulse response correction and sound zone control. <em>TASLPRO</em>, <em>33</em>, 2008-2020. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sound zone control (SZC) algorithms rely on pre-recorded, static impulse responses (IRs) to construct the control filters. Hence, their performance is compromised by perturbations within the acoustic environment. An important perturbation is changes in sound speed, which result primarily from temperature and humidity variations. In this paper, we first examine the impact of sound speed variations on IRs, and both empirically and theoretically show that changes in speed (either increase or decrease) leads to a corresponding compression or expansion of the IRs along the time axis. We then introduce a simple model, termed sinc interpolation-compression/expansion-resampling (SICER), to correct the IRs for sound speed change. Our simulation studies demonstrate that the corrected IRs closely match the actual IRs, for both increase and decrease in sound speed. Furthermore, we discuss several important considerations while correcting IRs for sound speed variations. In the context of sound zone control, using the proposed SICER model, IRs pre-measured at a certain sound speed can be corrected for any sound speed change and optimal control filters at the new sound speed can be re-computed, without the need of re-measuring the new IRs (which becomes impractical after SZC deployment). We integrate the proposed SICER IR correction method with the recently proposed variable span trade-off (VAST) framework for SZC, and propose a SICER-corrected VAST method for sound zone control, resilient to sound speed variations. Simulation studies carried out show that the proposed SICER-corrected VAST method significantly improves acoustic contrast and reduces signal distortion in the presence of sound speed changes.},
  archive  = {J},
  author   = {Sankha Subhra Bhattacharjee and Jesper Rindom Jensen and Mads Græsbøll Christensen},
  doi      = {10.1109/TASLPRO.2025.3570949},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {2008-2020},
  title    = {Sound speed perturbation robust audio: Impulse response correction and sound zone control},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech-based phonetic transcript metrics. <em>TASLPRO</em>, <em>33</em>, 1998-2007. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Grapheme-to-phoneme (G2P) transducers play a critical role in the training of modern phonetic neural speech systems, e.g., automatic speech recognition and text-to-speech synthesis systems, as they generate phonetic transcripts to train these systems. The primary metric to evaluate the generated phonetic transcripts is the phonetic error rate (PER), calculated as the ratio of errors in the transcripts to the total number of phones in the human-annotated ground-truth (GT) transcript or pronunciation lexicon. However, human-annotated phonetic transcripts are rare, while pronunciation lexicons typically permit multiple context-dependent variants of each word (or, worse, omit words entirely); these resource scarcity problems hinder evaluation of high-quality context-dependent G2P transducers. In this work, we explore alternative speech-based phonetic transcript metrics in replacement of PER, leveraging existing corpora's parallel text and speech data. We propose to train speech-to-phone (S2P) and phone-to-speech (P2S) systems using the speech and phonetic transcripts to be evaluated and use the error rates of S2P or P2S generation as the measure for the degree of matching between speech and phonetic transcripts, and, hence, for the quality of phonetic transcripts. We conduct experiments to compare the proposed metrics with PER and demonstrate their strong correlation. In particular, we find S2P error rates exhibit a stronger correlation with PER than P2S error rates. Furthermore, we experiment on G2PU, a recently proposed acoustically augmented sentence-level G2P model, and illustrate how S2P error rates can guide the selection of the optimal setting, thereby eliminating its need for GT transcripts.},
  archive  = {J},
  author   = {Heting Gao and Mark Hasegawa-Johnson and Chang D. Yoo},
  doi      = {10.1109/TASLPRO.2025.3564174},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1998-2007},
  title    = {Speech-based phonetic transcript metrics},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-robust speaker verification with attenuated speech restoration and consistency training. <em>TASLPRO</em>, <em>33</em>, 1987-1997. (<a href='https://doi.org/10.1109/TASLPRO.2025.3567758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Even though the performance of speaker verification (SV) has been significantly improved with deep learning approaches, it may degrade severely in the presence of background noises. Simple approaches to relieve this issue would be multi-condition training (MCT) and adopting a speech enhancement (SE) module as a pre-processor. However, whether joint-trained with the SV module or not, the SE module may occasionally incur speech attenuation which leads to the partial loss of speaker information. To address this problem, in this paper, we propose a noise-robust SV system with the SE front-end incorporating a speech restoration module using lost information aggregation and consistency training. In the speech restoration module, the lost information obtained from the noisy and enhanced latent representations processed with different sizes of the receptive field is aggregated to produce restored speech features using a loss function penalizing speech attenuation. Moreover, to further improve the robustness to the background noises and unseen data, we adopt consistency training to make the speaker embeddings for the noisy speech similar to those for the clean speech obtained by a pre-trained SV model. Our experimental results demonstrated that the proposed system significantly improved the performance of the speaker verification for the VoxCeleb dataset mixed with environmental noises, and exhibited the generalization capability in the experiment on the CHiME-4 dataset.},
  archive  = {J},
  author   = {Sangwook Han and Youngdo Ahn and Jong Won Shin},
  doi      = {10.1109/TASLPRO.2025.3567758},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1987-1997},
  title    = {Noise-robust speaker verification with attenuated speech restoration and consistency training},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving representation with intra- and inter-clause commonsense injection for emotion-cause pair extraction. <em>TASLPRO</em>, <em>33</em>, 1974-1986. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion–Cause Pair Extraction (ECPE) aims to simultaneously extract emotions and their corresponding causes, which is crucial for understanding human intent. Generally, machines can extract or infer emotion causes by incorporating commonsense knowledge with document contexts. However, most existing works lack integration of commonsense knowledge when analyzing human emotions and their triggering causes. To address this issue, this paper introduces a commonsense knowledge-injection framework (CokECPE), designed to enhance representation learning and thereby improve model performance. CokECPE enriches representation learning by incorporating commonsense knowledge at intra-clause and inter-clause levels. First, CokECPE retrieves concept-level and event-level knowledge from the commonsense conversational knowledge graph $\text{C}^{3}\text{KG}$. Next, it injects these commonsense features into a pretrained model, fusing them with multilevel textual information. Finally, CokECPE establishes dependencies between clauses based on commonsense reasoning, selecting the paths most similar to the emotion–cause pair candidates as edges, effectively embedding commonsense relations into the representation learning process. Experimental results on two public datasets demonstrate that CokECPE consistently outperforms existing methods, achieving enhanced state-of-the-art performance and validating its effectiveness.},
  archive  = {J},
  author   = {Guimin Hu and Jiayuan Xie},
  doi      = {10.1109/TASLPRO.2025.3525957},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1974-1986},
  title    = {Improving representation with intra- and inter-clause commonsense injection for emotion-cause pair extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sound radiation from a vibrating dome-shaped radiator surrounded by a rigid infinite baffle. <em>TASLPRO</em>, <em>33</em>, 1960-1973. (<a href='https://doi.org/10.1109/TASLPRO.2025.3567760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate modeling and analysis of baffled radiators are crucial for understanding their acoustic radiation characteristics, as these models serve as the foundation for evaluating the performance of real-world loudspeakers. This paper focuses on the radiation behavior of a dome-shaped radiator in an infinite baffle. A key theoretical insight is that the far-field sound pressure of a dome radiator can be expressed as a three-dimensional Fourier transform (3D-FT) integral of its surface velocity. Building on this foundation, we derive theoretical expressions for several typical acoustical quantities, including beampattern, directivity factor (DF), specific radiation impedance, and radiated sound power, under the far-field approximation and axisymmetric velocity distribution conditions. Next, we address the null problems observed in the DF of current velocity distributions, which arise due to the complex exponential integral term in the DF's numerator. A detailed analysis is performed to identify and understand the root causes of this issue. As a solution, we design an equalization filter based on the frequency spectral characteristics of an actual vibrating dome to smooth the DF across the entire frequency range. A performance comparison between several typical vibrating velocity distributions and the proposed distribution is presented, with evaluation results demonstrating that the proposed velocity distribution achieves a higher and smoother DF than existing models.},
  archive  = {J},
  author   = {Junqing Zhang and Wen Zhang and Jingdong Chen and Jacob Benesty},
  doi      = {10.1109/TASLPRO.2025.3567760},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1960-1973},
  title    = {Sound radiation from a vibrating dome-shaped radiator surrounded by a rigid infinite baffle},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subband adaptive filtering based on the squared sine error criterion: Analysis and applications. <em>TASLPRO</em>, <em>33</em>, 1949-1959. (<a href='https://doi.org/10.1109/TASLPRO.2025.3561576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we present a subband adaptive filtering (SAF) algorithm with good robustness for impulsive noise environments. The proposed squared sine normalized SAF (SS-NSAF) algorithm employs a squared sine function and is inspired by the idea of normalization. A statistical analysis of the proposed SS-NSAF algorithm is carried out under impulsive noise environments, which includes stability conditions for the convergence and analytical formulas to predict the mean-square deviation of the SS-NSAF algorithm. Numerical simulations for system-identification and echo cancellation applications are conducted to illustrate the performance of the SS-NSAF algorithm and competing techniques in impulsive noise environments.},
  archive  = {J},
  author   = {Zhikai Gong and Yingsong Li and Xinqi Huang and Liping Li and Zhixiang Huang and Rodrigo. C. de Lamare and Yingying Zhu},
  doi      = {10.1109/TASLPRO.2025.3561576},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1949-1959},
  title    = {Subband adaptive filtering based on the squared sine error criterion: Analysis and applications},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mamba in speech: Towards an alternative to self-attention. <em>TASLPRO</em>, <em>33</em>, 1933-1948. (<a href='https://doi.org/10.1109/TASLPRO.2025.3566210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and speech processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in speech signal processing. This paper explores solutions for applying Mamba to speech processing by discussing two typical speech processing tasks: speech recognition, which requires semantic and sequential information, and speech enhancement, which focuses primarily on sequential patterns. The experimental results confirm that bidirectional Mamba (BiMamba) consistently outperforms vanilla Mamba, highlighting the advantages of a bidirectional design for speech processing. Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in the Transformer model and its derivates, particularly for the semantic-aware task. The crucial technologies for transferring Mamba to speech are then summarized in ablation studies and the discussion section, offering insights for extending this research to a broader scope of tasks.},
  archive  = {J},
  author   = {Xiangyu Zhang and Qiquan Zhang and Hexin Liu and Tianyi Xiao and Xinyuan Qian and Beena Ahmed and Eliathamby Ambikairajah and Haizhou Li and Julien Epps},
  doi      = {10.1109/TASLPRO.2025.3566210},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1933-1948},
  title    = {Mamba in speech: Towards an alternative to self-attention},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing neural transducers for two-stage text-to-speech via semantic token prediction. <em>TASLPRO</em>, <em>33</em>, 1922-1932. (<a href='https://doi.org/10.1109/TASLPRO.2025.3566247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.},
  archive  = {J},
  author   = {Minchan Kim and Myeonghun Jeong and Byoung Jin Choi and Semin Kim and Joun Yeop Lee and Nam Soo Kim},
  doi      = {10.1109/TASLPRO.2025.3566247},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1922-1932},
  title    = {Utilizing neural transducers for two-stage text-to-speech via semantic token prediction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Only send what you need: Learning to communicate efficiently in federated multilingual machine translation. <em>TASLPRO</em>, <em>33</em>, 1907-1921. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) is a promising distributed machine learning paradigm that enables multiple clients to collaboratively train a global model. In this paper, we focus on a practical federated multilingual learning setup where clients with their own language-specific data aim to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. We propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget.},
  archive  = {J},
  author   = {Yun-Wei Chu and Dong-Jun Han and Christopher G. Brinton},
  doi      = {10.1109/TASLPRO.2025.3564854},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1907-1921},
  title    = {Only send what you need: Learning to communicate efficiently in federated multilingual machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast neural vocoder with fundamental frequency control using finite impulse response filters. <em>TASLPRO</em>, <em>33</em>, 1893-1906. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In practical use, a neural waveform generative model, which we call a neural vocoder, must be able to produce high-quality synthetic speech and perform real-time inference on a single CPU with flexible control of the fundamental frequency ($F_{0}$). To achieve this functionality, this paper proposes a novel neural vocoder called FIRNet, which is based on the source-filter model. The basic concept of FIRNet is that multiple finite impulse responses (FIRs) are predicted from acoustic features using neural networks, and the speech waveform is then generated by filtering an excitation signal with these multiple FIRs. In the first version of FIRNet, a mixed excitation signal is employed, and the speech waveform is produced by convolving the mixed excitation with residual and resonance FIR coefficients. Although this FIRNet can achieve fast inference and $F_{0}$ control, speech quality is not as high as that of other neural vocoders based on the source-filter model. To improve this, we apply three extensions to FIRNet: (1) a pitch-dependent convolutional network, (2) a structure separating the periodic and aperiodic components, and (3) data augmentation or multiple-speaker training. The experimental results show that the proposed method can improve speech quality while retaining flexible $F_{0}$ control and fast inference.},
  archive  = {J},
  author   = {Yamato Ohtani and Takuma Okamoto and Tomoki Toda and Hisashi Kawai},
  doi      = {10.1109/TASLPRO.2025.3564048},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1893-1906},
  title    = {Fast neural vocoder with fundamental frequency control using finite impulse response filters},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAAQI-net: A non-intrusive neural music audio quality assessment model for hearing aids. <em>TASLPRO</em>, <em>33</em>, 1877-1892. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces HAAQI-Net, a non-intrusive music audio quality assessment model for hearing aid users. Unlike traditional methods such as Hearing Aid Audio Quality Index (HAAQI), which requires intrusive reference signal comparisons, HAAQI-Net offers a more accessible and computationally efficient alternative. Leveraging a bidirectional long short-term memory architecture with attention mechanisms and features extracted from a pre-trained BEATs model, it can predict HAAQI scores directly from music audio clips and hearing loss patterns. The experimental results demonstrate that, compared to the traditional HAAQI as the reference, HAAQI-Net achieves a linear correlation coefficient (LCC) of 0.9368, a Spearman's rank correlation coefficient (SRCC) of 0.9486, and a mean squared error (MSE) of 0.0064, while significantly reducing the inference time from 62.52 seconds to 2.54 seconds. Furthermore, a knowledge distillation strategy was applied, reducing the parameters by 75.85% and inference time by 96.46%, while maintaining strong performance (LCC: 0.9071, SRCC: 0.9307, MSE: 0.0091). To expand its capabilities, HAAQI-Net was adapted to predict subjective human scores, mean opinion score (MOS), by fine-tuning. This adaptation significantly improved the prediction accuracy. Furthermore, the robustness of HAAQI-Net was evaluated under varying sound pressure level (SPL) conditions, revealing optimal performance at a reference SPL of 65 dB, with the accuracy gradually decreasing as SPL deviated from this point. The advancements in subjective score prediction, SPL robustness, and computational efficiency position HAAQI-Net as a reliable solution for music audio quality assessment, significantly contributing to the development of efficient and accurate models in audio signal processing and hearing aid technology.},
  archive  = {J},
  author   = {Dyah A. M. G. Wisnu and Stefano Rini and Ryandhimas E. Zezario and Hsin-Min Wang and Yu Tsao},
  doi      = {10.1109/TASLPRO.2025.3536156},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1877-1892},
  title    = {HAAQI-net: A non-intrusive neural music audio quality assessment model for hearing aids},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining X-vectors and bayesian batch active learning: Two-stage active learning pipeline for speech recognition. <em>TASLPRO</em>, <em>33</em>, 1862-1876. (<a href='https://doi.org/10.1109/TASLPRO.2025.3565216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces a novel two-stage active learning (AL) pipeline for automatic speech recognition (ASR), combining unsupervised and supervised AL methods. The first stage utilizes unsupervised AL by using x-vectors clustering for diverse sample selection from unlabeled speech data, thus establishing a robust initial dataset for the subsequent supervised AL. The second stage incorporates a supervised AL strategy, with a batch AL method specifically developed for ASR, aimed at selecting diverse and informative batches of samples. Here, sample diversity is also achieved using x-vectors clustering, while the most informative samples are identified using a Bayesian AL method tailored for ASR with an adaptation of Monte Carlo dropout to approximate Bayesian inference. This approach enables precise uncertainty estimation, thereby enhancing ASR model training with significantly reduced data requirements. Our method has shown superior performance compared to competing methods on homogeneous, heterogeneous, and OOD test sets, demonstrating that strategic sample selection and innovative Bayesian modeling can substantially optimize both labeling effort and data utilization in deep learning-based ASR applications.},
  archive  = {J},
  author   = {Ognjen Kundacina and Vladimir Vincan and Dragisa Miskovic},
  doi      = {10.1109/TASLPRO.2025.3565216},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1862-1876},
  title    = {Combining X-vectors and bayesian batch active learning: Two-stage active learning pipeline for speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VQ-CTAP: Cross-modal fine-grained sequence representation learning for speech processing. <em>TASLPRO</em>, <em>33</em>, 1849-1861. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called “Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)”, which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25 Hz from 24 kHz input waveforms, which is a 960-fold reduction in the sampling rate. The experimental results demonstrate that while VQ-CTAP outperforms baseline methods in TTS and VC tasks, its performance on ASR tasks is suboptimal.},
  archive  = {J},
  author   = {Chunyu Qiang and Wang Geng and Yi Zhao and Ruibo Fu and Tao Wang and Cheng Gong and Tianrui Wang and Qiuyu Liu and Jiangyan Yi and Zhengqi Wen and Chen Zhang and Hao Che and Longbiao Wang and Jianwu Dang and Jianhua Tao},
  doi      = {10.1109/TASLPRO.2025.3564168},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1849-1861},
  title    = {VQ-CTAP: Cross-modal fine-grained sequence representation learning for speech processing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multilingual dataset (MultiMWP) and benchmark for math word problem generation. <em>TASLPRO</em>, <em>33</em>, 1838-1848. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a multi-way parallel corpus of Math Word Problems (MWPs) in nine languages, including six low-resource languages. To date, this is the largest multilingual MWP dataset available. We utilize this dataset and show the viability of using pre-trained multilingual sequence-sequence language models (prMSLMs) for autoregressive MWP generation in both monolingual and multilingual setups, particularly for low-resource languages. We also integrate a math constraint satisfaction module with autoregressive text generation. Our extensive evaluations identify several factors that affect autoregressive text generation on prMSLMs. These include language representation in the model, model size, existence of similar languages in the model, and language script. Overall, our results reveal that autoregressive MWP generation on top of prMSLMs is very promising, even for low-resource languages.},
  archive  = {J},
  author   = {Omega Gamage and Surangika Ranathunga and Annie Lee and Xiao Sun and Aryaveer Singh and Marjana Prifti Skenduli and Mehreen Alam and Ajit Kumar Nayak and Haonan Gao and Barga Deori and Jingwen Ji and Qiyue Zhang and Yuchen Zeng and Muxin Tian and Yanke Mao and Endi Trico and Danja Nako and Sonila Shqezi and Sara Hoxha and Dezi Imami and Dea Doksani and Virat Kumar Pandey and Ananya Ananya and Nitisha Aggarwal and Naiyarah Hussain and Vandana Dwivedi and Rajkumari Monimala Sinha and Dhrubajyoti Kalita},
  doi      = {10.1109/TASLPRO.2025.3552936},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1838-1848},
  title    = {A multilingual dataset (MultiMWP) and benchmark for math word problem generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive residual extraction based pre-training for speech representation learning. <em>TASLPRO</em>, <em>33</em>, 1825-1837. (<a href='https://doi.org/10.1109/TASLPRO.2025.3564118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Self-supervised learning (SSL) has garnered significant attention in speech processing, particularly excelling in linguistic tasks such as speech recognition. However, improving the performance of pre-trained models across various downstream tasks—each requiring distinct types of speech information—remains a significant challenge. To address this, we propose a progressive residual extraction based SSL method, named ProgRE. Specifically, we introduce two lightweight, specialized task modules into an encoder-style SSL backbone to enhance its ability to extract pitch variation and speaker information from speech. Furthermore, to mitigate the incompatibility between the reinforced pitch variation and speaker information and the learning of content information, we employ residual extraction, leveraging the extracted representations as references or conditioning signals to guide the subsequent modules in more effectively learning content-related information under the supervision of HuBERT-based speech masking prediction. In this manner, we can incrementally extract pitch variation, speaker, and content representations from the input speech. Finally, these multiple representations, each capturing diverse speech information, are combined using different layer weights to produce task-specific representations for various downstream tasks. Experimental results demonstrate that our ProgRE achieves significant performance improvements across several tasks, such as speaker identification, speech recognition, emotion recognition, speech enhancement, and voice conversion, outperforming excellent SSL methods like wav2vec2.0, HuBERT, and WavLM.},
  archive  = {J},
  author   = {Tianrui Wang and Jin Li and Ziyang Ma and Rui Cao and Xie Chen and Longbiao Wang and Meng Ge and Xiaobao Wang and Yuguang Wang and Jianwu Dang and Nyima Tashi},
  doi      = {10.1109/TASLPRO.2025.3564118},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1825-1837},
  title    = {Progressive residual extraction based pre-training for speech representation learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). N-gram nearest neighbor machine translation. <em>TASLPRO</em>, <em>33</em>, 1813-1824. (<a href='https://doi.org/10.1109/TASLP.2024.3407537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nearest neighbor machine translation achieves non-parametric domain adaptation by augmenting the Autoregressive Translation (AT) with $k$-nearest-neighbor retrieval. The retrieval is conducted by comparing the similarity between the token-level context representations of the target tokens in the query and the datastore. However, the token-level representation may introduce noise when translating ambiguous words, or fail to provide accurate retrieval results when the representation generated by the model contains indistinguishable context information, e.g., Non-Autoregressive Translation (NAT) models. In this paper, we propose a novel $n$-gram nearest neighbor retrieval method that is model agnostic and applicable to both AT and NAT models. This method enhances the performance of AT models by reducing the ambiguous word problem. It also achieves impressive domain adaptation performance on NAT models and alleviates the multi-modality problem effectively, which is a well-known problem in NAT models. Specifically, we concatenate the adjacent $n$-gram hidden representations as the key, while the tuple of corresponding target tokens is the value. In inference, we propose tailored decoding algorithms for AT and NAT models respectively. We demonstrate that the proposed method consistently outperforms the token-level method on both AT and NAT models in general as well as on domain adaptation translation tasks. On domain adaptation, the proposed method brings $1.23 / 0.91$ and $2.25 / 1.75$ improvements regarding the average BLEU / ChrF score on AT and NAT models respectively. For the multi-modality problem, the repetition ratio for NAT models has been reduced by 2.64%.},
  archive  = {J},
  author   = {Rui Lv and Junliang Guo and Rui Wang and Xu Tan and Tao Qin and Qi Liu},
  doi      = {10.1109/TASLP.2024.3407537},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1813-1824},
  title    = {N-gram nearest neighbor machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pick the better and leave the rest: Leveraging multiple retrieved results to guide response generation. <em>TASLPRO</em>, <em>33</em>, 1801-1812. (<a href='https://doi.org/10.1109/TASLP.2023.3302231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural Response Generation (NRG) has achieved remarkable progress recently, though they suffer from the safe response problem. Some researchers proposed leveraging the retrieval-based chatbots' results to enhance the NRG models to generate diverse and informative responses. However, picking helpful information inside the multiple retrieved references and avoiding errors and noise brought from retrieval systems is still challenging. This paper proposes a variational neural response generating framework in which the validity of retrieved results is measured and those useful ones are taken as guidance explicitly. Moreover, if all the retrieved results fail to provide sufficient information, the framework also can let the model regress to a regular query-based NRG automatically. According to the thorough experimental comparisons with other retrieval-guided models, our proposed model can better utilize the useful information of retrieved results to generate appropriate and diverse responses.},
  archive  = {J},
  author   = {Bowen Wu and Yunhan Deng and Donghang Su and Jianyu Xiang and Chao Yang and Zongsheng Wang and Ying Li and Junhong Huang and Baoxun Wang},
  doi      = {10.1109/TASLP.2023.3302231},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1801-1812},
  title    = {Pick the better and leave the rest: Leveraging multiple retrieved results to guide response generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing speech emotion recognition with conditional emotion feature diffusion and progressive interleaved learning strategy. <em>TASLPRO</em>, <em>33</em>, 1787-1800. (<a href='https://doi.org/10.1109/TASLPRO.2025.3561606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER) aims to identify the speaker's emotional states in specific utterances accurately. However, existing methods still face feature confusion when attempting to recognize certain emotions because traditional acoustic feature extraction methods fail to capture dynamic emotional changes, blurring emotional boundaries. Additionally, existing classification networks (CNs) are constrained by fixed learning strategies, hindering their ability to capture subtle emotional nuances and resulting in label confusion. To address these two issues, we introduce 3D multiresolution modulation filtered cochleogram (MMCG) features by computing the deltas and delta-deltas of MMCG features to enhance the dynamic emotional changes and produce distinct emotional boundaries. We then customize a conditional emotion feature diffusion (CEFD) module, which progressively diffuses features based on emotional context to retain emotional nuances effectively and reduce reliance on conditioned information. In addition, a confidence filtering module is used to filter diffused features based on confidence-based posterior probabilities to ensure enhanced feature discrimination. We design a flexible training strategy named the progressive interleaved learning strategy (PILS) to learn further complex emotional nuances, which consists of two alternating stages: fine-tuning the CN parameters and supervising the CEFD output. Testing on the IEMOCAP, CASIA, and EMODB corpora demonstrates significant performance improvements in SER.},
  archive  = {J},
  author   = {Yang Liu and Xin Chen and Zhichao Peng and Yongwei Li and Xingfeng Li and Peng Song and Masashi Unoki and Zhen Zhao},
  doi      = {10.1109/TASLPRO.2025.3561606},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1787-1800},
  title    = {Enhancing speech emotion recognition with conditional emotion feature diffusion and progressive interleaved learning strategy},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone audition: On measurements and modeling of drone-related transfer functions. <em>TASLPRO</em>, <em>33</em>, 1775-1786. (<a href='https://doi.org/10.1109/TASLPRO.2025.3562006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With growing applications of drones, there is a demand for drone audition functionality that allows microphone arrays to be embedded into or attached to the body of drones for sound source localization and signal enhancement. In practical acoustical environments, the performance of drone audition is degraded due to the free-field sound propagation assumption. In this paper, we propose (i) a method for modeling the scattering and diffraction of a drone body beyond free-field propagation as a Drone-Related Transfer Function (DRTF), and (ii) a practical method to estimate the DRTF coefficients using circular loudspeaker arrays. The method incorporates the acoustic reciprocity principle and a finite order spherical harmonic expansion by utilizing a discrete set of measurements by multiple loudspeaker placements on circular arrays. We validate our DRTF model and parameterization method on a known rigid sphere scatterer in both simulation and real measurements using the em32 Eigenmike. The proposed method offers a low reconstruction error ($\leq -30$ dB) over speech frequencies. Additionally, we investigate the DRTF reconstruction error for a real drone. The proposed approach reveals a practically viable method for DRTF measurements compared to the traditional spherical loudspeaker array, and alleviates the free-field assumption from drone audition.},
  archive  = {J},
  author   = {Wageesha N. Manamperi and Thushara D. Abhayapala and Lachlan Brinie and Jihui Zhang and Prasanga N. Samarasinghe},
  doi      = {10.1109/TASLPRO.2025.3562006},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1775-1786},
  title    = {Drone audition: On measurements and modeling of drone-related transfer functions},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient CNNs via passive filter pruning. <em>TASLPRO</em>, <em>33</em>, 1763-1774. (<a href='https://doi.org/10.1109/TASLPRO.2025.3561589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Convolutional neural networks (CNNs) have shown state-of-the-art performance in various applications. However, CNNs are resource-hungry due to their requirement of high computational complexity and memory storage. Recent efforts toward achieving computational and memory efficiency in CNNs involve filter pruning methods that eliminate some of the filters in CNNs based on the “importance” of the filters. The majority of existing filter pruning methods are either “active”, which use a dataset and generate feature maps to quantify filter importance, or “passive”, which compute filter importance using entry-wise norm of the filters or by measuring similarity among filters without involving data. However, the existing passive filter pruning methods eliminate relatively smaller norm filters or similar filters without considering the significance of the filters in producing the node output, resulting in degradation in the performance. To address this, we present a passive filter pruning method where the least significant filters with relatively smaller contribution in producing output are pruned away by incorporating the operator norm of the filters. The proposed pruning method results in better performance across various CNNs compared to that of the existing passive filter pruning methods. In comparison to the existing active filter pruning methods, the proposed pruning method is more efficient and achieves similar performance as well. The efficacy of the proposed pruning method is evaluated on audio scene classification and audio tagging tasks using various CNNs architecture such as VGGish, DCASE21_Net and PANNs. The proposed pruning method reduces number of computations and parameters of the unrpuned CNNs by at least 40% and 50% respectively, enhancing inference latency while maintaining similar performance as obtained using the unpruned CNNs.},
  archive  = {J},
  author   = {Arshdeep Singh and Mark D. Plumbley},
  doi      = {10.1109/TASLPRO.2025.3561589},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1763-1774},
  title    = {Efficient CNNs via passive filter pruning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UnitCorrect: Unit-based mispronunciation correcting system with a DTW-based detection. <em>TASLPRO</em>, <em>33</em>, 1753-1762. (<a href='https://doi.org/10.1109/TASLPRO.2025.3559344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With recent globalization trends, the importance of language education for nonnative speakers has significantly increased. Research on pronunciation correction and speech editing continues to be actively explored to reduce the pronunciation gap between native and nonnative speakers. Traditional speech editing models frequently suffer from unnaturalness at the boundary of the corrected parts and demand substantial input information to effectively synthesize corrected speech. Furthermore, the lack of well-curated annotations for mispronounced speech presents a challenge for effective training. This paper introduces UnitCorrect, a mispronunciation correction system that leverages self-supervised unit representations for synthesizing natural-sounding speech and utilizing more phoneme information effectively. Additionally, we propose a mispronunciation detection approach based on dynamic time warping, which efficiently identifies mispronounced segments by aligning input audio with the target text, helping to mitigate the data scarcity issue. In addition, frame-level text is incorporated into the decoder's input to supplement the text information, and a conditional flow-matching decoder enables high-quality speech synthesis. The experimental results demonstrate that UnitCorrect outperforms existing models in terms of mispronunciation corrections and naturalness of speech.},
  archive  = {J},
  author   = {Hyun-Woo Bae and Hyung-Seok Oh and Seung-Bin Kim and Seong-Whan Lee},
  doi      = {10.1109/TASLPRO.2025.3559344},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1753-1762},
  title    = {UnitCorrect: Unit-based mispronunciation correcting system with a DTW-based detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank-then-imitate: Low-resourced event extraction via dynamic demonstration-based learning. <em>TASLPRO</em>, <em>33</em>, 1743-1752. (<a href='https://doi.org/10.1109/TASLPRO.2025.3557249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Event Extraction (EE) in low-resourced environments aims to derive structured representations of event information from limited annotations. Recent approaches have focused on ontology-agnostic parameterization to address this challenge, treating specific event ontology information as textual input instead of model parameters. However, these methods highly rely on semantic understanding ability to support cross-type transferability, while their fixed ontology modeling is essentially exclusive across seen and unseen types, their cross-type transferability would suffer during fine-tuning. Motivated by ideas of imitation learning in the educational field, we propose a Dynamic Demonstration-based Learning Framework for Event Extraction(D2E2), achieving event extraction in a rank-then-imitate manners. As imitation ability relies less on semantic understanding ability and can be jointly learned across types, it is supposed that D2E2 utilize limited annotations more efficiently. During training of D2E2, similar annotated examples are sampled based on BM25 algorithm, composing a series of positive or negative pairs. Then these pairs are used to fine-tune a semantic-based retriever, and demonstration-based learning extractor to extract events via imitation. At inference time, the semantic-based retriever first retrieves annotated examples similar with candidate events in the input text, then the extractor uses them as demonstrations to conceptualize and structure triggers and arguments in the input text via imitating those in annotations. Experimental evaluations conducted on public benchmarks ACE05 and FewFC demonstrate the effectiveness of our method. Results indicate significant performance improvements over existing approaches in low-resourced scenarios, while also achieving competitive or superior performance in high-resourced scenarios.},
  archive  = {J},
  author   = {Shudong Lu and Si Li and Jun Guo},
  doi      = {10.1109/TASLPRO.2025.3557249},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1743-1752},
  title    = {Rank-then-imitate: Low-resourced event extraction via dynamic demonstration-based learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beamforming in the short-time fourier transform domain via dimensionality reduction. <em>TASLPRO</em>, <em>33</em>, 1730-1742. (<a href='https://doi.org/10.1109/TASLPRO.2025.3559309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Beamforming is a fundamental technique for extracting a speech signal of interest from noisy observations and is widely used in speech processing, communication, and recognition applications. Typically, microphone array beamforming is performed in the short-time Fourier transform (STFT) domain, where a distinct beamformer is designed and applied to each STFT bin. This method offers great flexibility for handling nonstationary and broadband speech signals. However, it also requires a large number of beamformers to cover all STFT subbands, making it computationally demanding, particularly for adaptive beamformers. To address this challenge, this paper proposes a dimensionality-reduction-based beamforming approach in the STFT domain. By applying the singular value decomposition (SVD) to the data matrix of signals from all channels and STFT subbands, we achieve low-dimensional representations of the observation signals. This transformation processes the speech signal components into a smaller number of dimensions, significantly reducing the number of required beamformers as compared to traditional STFT-domain methods. We provide examples of designing beamformers in this transformed domain and show through simulations that the proposed method not only reduces computational complexity but also enhances performance compared to existing techniques.},
  archive  = {J},
  author   = {Wei Liu and Jacob Benesty and Gongping Huang and Jingdong Chen},
  doi      = {10.1109/TASLPRO.2025.3559309},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1730-1742},
  title    = {Beamforming in the short-time fourier transform domain via dimensionality reduction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the fundamental obstacle in speech-to-text modeling: Understanding and mitigating the granularity challenge. <em>TASLPRO</em>, <em>33</em>, 1719-1729. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech-to-text (S2T) generation tasks often struggle to achieve satisfactory convergence without relying on auxiliary data or models. We identify the core issue as the modeling granularity, where the fine-grained and lengthy characteristics of audio features pose obstacles in effectively allocating attention weights, particularly during encoder self-attention learning. In this paper, we investigate two well-established methods, Conformer and information aggregation, to mitigate the learning burden of the encoder from the aspects of intra-layer and inter-layer encoding. Conformer directly enhances modeling capability through architecture improvement, while aggregation generates coarser-grained representations, thus shaping text-like structures to fundamentally simplify attention learning. Extensive results demonstrate superior convergence and notable improvement on two representative S2T generation tasks, speech recognition and translation. In particular, we achieve a notable average BLEU score of 26.9 on MuST-C speech translation datasets without auxiliary resources and approaches. Furthermore, our finding suggests that the grown model capacity and sufficient training can effectively facilitate the granularity challenge.},
  archive  = {J},
  author   = {Chen Xu and Xiaoqian Liu and Yuhao Zhang and Anxiang Ma and Tong Xiao and Jingbo Zhu and Dapeng Man and Wu Yang},
  doi      = {10.1109/TASLPRO.2025.3555070},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1719-1729},
  title    = {Unveiling the fundamental obstacle in speech-to-text modeling: Understanding and mitigating the granularity challenge},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-speech: Straightened flow for zero-shot voice clone. <em>TASLPRO</em>, <em>33</em>, 1706-1718. (<a href='https://doi.org/10.1109/TASLPRO.2025.3557242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page.},
  archive  = {J},
  author   = {Xuyuan Li and Zengqiang Shang and Hua Hua and Peiyang Shi and Chen Yang and Li Wang and Pengyuan Zhang},
  doi      = {10.1109/TASLPRO.2025.3557242},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1706-1718},
  title    = {SF-speech: Straightened flow for zero-shot voice clone},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Homogeneous speaker features for on-the-fly dysarthric and elderly speaker adaptation and speech recognition. <em>TASLPRO</em>, <em>33</em>, 1689-1705. (<a href='https://doi.org/10.1109/TASLPRO.2025.3547217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The application of data-intensive automatic speech recognition (ASR) technologies to dysarthric and elderly adult speech is confronted by their mismatch against healthy and non-aged voices, data scarcity and large speaker-level variability. To this end, this paper proposes two novel data-efficient methods to learn homogeneous dysarthric and elderly speaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN and Conformer ASR models. These include: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that exploit a special regularization term to enforce homogeneity of speaker features in adaptation; and 2) feature-based learning hidden unit contributions (f-LHUC) transforms that are conditioned on VR-SBE features. Experiments are conducted on four benchmark tasks across two languages: the English UASpeech and TORGO dysarthric speech datasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora. The proposed on-the-fly speaker adaptation techniques consistently outperform baseline iVector and xVector adaptation by statistically significant word or character error rate reductions up to 2.85% absolute (8.06% relative) and batch-mode LHUC speaker adaptation by 2.00% absolute (8.21% relative), while operating with real-time factors speeding up to 33.6 times against xVectors during adaptation. The efficacy of the proposed adaptation techniques is demonstrated in a comparison against current ASR technologies including SSL pre-trained systems on UASpeech, where our best system produces a state-of-the-art WER of 23.33%. Analyses show that VR-SBE features and f-LHUC transforms are insensitive to speaker-level data quantity in test-time adaptation. T-SNE visualization reveals they have stronger speaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC transforms.},
  archive  = {J},
  author   = {Mengzhe Geng and Xurong Xie and Jiajun Deng and Zengrui Jin and Guinan Li and Tianzi Wang and Shujie Hu and Zhaoqing Li and Helen Meng and Xunying Liu},
  doi      = {10.1109/TASLPRO.2025.3547217},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1689-1705},
  title    = {Homogeneous speaker features for on-the-fly dysarthric and elderly speaker adaptation and speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards explainable medical machine reading comprehension with rationale generation. <em>TASLPRO</em>, <em>33</em>, 1675-1688. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine reading comprehension (MRC) has achieved remarkable progress in the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. Besides, the opacity of the large-scale neural language model also limits its practical utility. As an effort, we first collect a medical machine reading comprehension question dataset with experts' annotations for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model eKMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). Meanwhile, it improves the explainability of the model by leveraging the knowledge entailed in explanations as an additional distillation signal for more efficient learning. The experimental results demonstrate that our method outperforms existing competitive models with a large margin, which improves prediction accuracy on the test set and can generate reasonable free-text rationales for its predictions even with a small-scale training corpus.},
  archive  = {J},
  author   = {Dongfang Li and Shan He and Baotian Hu and Qingcai Chen},
  doi      = {10.1109/TASLPRO.2025.3555105},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1675-1688},
  title    = {Towards explainable medical machine reading comprehension with rationale generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUSA: Multi-lingual speaker anonymization via serial disentanglement. <em>TASLPRO</em>, <em>33</em>, 1664-1674. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speaker anonymization is an effective privacy protection solution designed to conceal the speaker's identity while preserving the linguistic content and para-linguistic information of the original speech. While most prior studies focus solely on a single language, an ideal speaker anonymization system should be capable of handling multiple languages. This paper proposes MUSA, a MUlti-lingual Speaker Anonymization approach that employs a serial disentanglement strategy to perform a step-by-step disentanglement from a global time-invariant representation to a temporal time-variant representation. By utilizing semantic distillation and self-supervised speaker distillation, the serial disentanglement strategy can avoid strong inductive biases and exhibit superior generalization performance across different languages. Meanwhile, we propose a straightforward anonymization strategy that employs empty embedding with zero values to simulate the speaker identity concealment process, eliminating the need for conversion to a pseudo-speaker identity and thereby reducing the complexity of speaker anonymization process. Experimental results on VoicePrivacy official datasets and multi-lingual datasets demonstrate that MUSA can effectively protect speaker privacy while preserving linguistic content and para-linguistic information.},
  archive  = {J},
  author   = {Jixun Yao and Qing Wang and Pengcheng Guo and Ziqian Ning and Yuguang Yang and Yu Pan and Lei Xie},
  doi      = {10.1109/TASLPRO.2025.3555115},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1664-1674},
  title    = {MUSA: Multi-lingual speaker anonymization via serial disentanglement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing language models via HTML DOM tree for text structure understanding. <em>TASLPRO</em>, <em>33</em>, 1653-1663. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Understanding text structure, which enables the automated system to parse long text structure, is crucial for various natural language processing applications such as information extraction, summarization, and question answering. Although previous methods have advanced text structure parsing effectively, they face challenges such as not leveraging the abundance of unlabelled data and focusing mainly on content-inferred information. To address this deficiency, this paper introduces a novel Text Structure Language Model (TSLM), an LM pre-training framework that employs ubiquitous HTML documents and considers the text structure among text units. HTML documents are composed by experts and their hierarchies can reflect the structure of documents. Our learning framework is designed to equip the LM with awareness of two complementary kinds of structures from HTML documents. It encourages the model to learn local structure which helps in understanding the immediate connection between two units by reconstructing the structure of DOM tree, and global structure which shapes the overall organization and thematic development by predicting the optimal content-fitting tree. Extensive experiments with structure-related downstream tasks, including text segmentation and table of contents generation, validate the effectiveness of TSLM.},
  archive  = {J},
  author   = {Hangdi Xing and Zirui Shao and Feiyu Gao and Jiajun Bu and Zhi Yu and Qi Zheng and Jingjun Gu and Xiaozhong Liu},
  doi      = {10.1109/TASLPRO.2025.3555098},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1653-1663},
  title    = {Enhancing language models via HTML DOM tree for text structure understanding},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Voice attribute editing with text prompt. <em>TASLPRO</em>, <em>33</em>, 1641-1652. (<a href='https://doi.org/10.1109/TASLPRO.2025.3557193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite recent advancements in speech generation with text prompt providing control over speech style, voice attributes in synthesized speech remain elusive and challenging to control. This paper introduces a novel task: voice attribute editing with text prompt, with the goal of making relative modifications to voice attributes according to the actions described in the text prompt. To solve this task, VoxEditor, an end-to-end generative model, is proposed. In VoxEditor, addressing the insufficiency of text prompt, a residual memory (ResMem) module is designed, that efficiently maps voice attributes and these descriptors into the shared feature space. Additionally, the ResMem module is enhanced with a voice attribute degree prediction (VADP) module to align voice attributes with corresponding descriptors, addressing the imprecision of text prompt caused by non-quantitative descriptions of voice attributes. We also establish the open-source VCTK-RVA dataset, which leads the way in manual annotations detailing voice characteristic differences among different speakers. Extensive experiments demonstrate the effectiveness and generalizability of our proposed method in terms of both objective and subjective metrics.},
  archive  = {J},
  author   = {Zheng-Yan Sheng and Li-Juan Liu and Yang Ai and Jia Pan and Zhen-Hua Ling},
  doi      = {10.1109/TASLPRO.2025.3557193},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1641-1652},
  title    = {Voice attribute editing with text prompt},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenEn-MNER: Enhancing nested chinese NER with multimodal fusion and alignment via speech-to-text generation. <em>TASLPRO</em>, <em>33</em>, 1628-1640. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, the academic community has increasingly focused on multimodal Chinese Named Entity Recognition (NER) that utilizes speech cues. Existing methods typically rely solely on the NER objective function to guide the alignment and fusion of speech and text, overlooking the inherent alignment within speech-text pairs. Furthermore, these approaches generally employ sequence labeling techniques, which are inadequate for handling nested entities. To address these limitations, we introduce GenEn-MNER, a novel multimodal nested Chinese NER approach that enhances fusion and alignment through speech-to-text generation. This method leverages natural alignment information obtained from the speech-to-text task, using a cross-modal Transformer to integrate and align modalities. Additionally, the table-filling module redefines nested NER by conceptualizing it as the prediction of token pair relationships. Experimental results, as indicated by F1 scores, on CNERTA flat version (80.83% ), CNERTA nest version (80.66% ), and AISHELL-NER (94.52% ) not only confirm the effectiveness of our approach but also demonstrate its superiority to existing state-of-the-art methods.},
  archive  = {J},
  author   = {Jinzhong Ning and Yuanyuan Sun and Zhihao Yang and Zhijun Wang and Ling Luo and Hongfei Lin and Yijia Zhang},
  doi      = {10.1109/TASLPRO.2025.3555106},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1628-1640},
  title    = {GenEn-MNER: Enhancing nested chinese NER with multimodal fusion and alignment via speech-to-text generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised end-to-end accented speech recognition under low-resource conditions. <em>TASLPRO</em>, <em>33</em>, 1616-1627. (<a href='https://doi.org/10.1109/TASLPRO.2025.3557248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accents, characterized by deviations from standard pronunciation, often lead to a sharp decline in the performance of speech recognition systems. This issue becomes even more serious when dealing with unsupervised accents under low-resource conditions. And the effective utilization of limited unsupervised accent speech data to enhance accent-robust ASR remains largely unexplored and presents significant challenges. In this study, we introduce a novel approach termed Joint Unsupervised and Supervised Data Training (JUSDT) to handle this issue. In JUSDT, the supervised ASR training and unsupervised representation learning are treated as two distinct tasks but jointly trained in a single training process. Building upon JUSDT, we further explore two additional variants: Phoneme-Quantizer JUSDT and JUSDT+, which respectively employ phoneme codebook learning to generate accent-invariant representations for accent normalization, and enable supervised training for unsupervised accent speech data to well-incorporate both textual and acoustic contextual information. Our experiments are performed on both the Librispeech dataset and accented English ASR tasks. Results demonstrate that the proposed methods outperform our strong baseline by relative 3.2% to 9.3% word error rate reductions across multiple test sets.},
  archive  = {J},
  author   = {Li Li and Yijie Li and Dongxing Xu and Yanhua Long},
  doi      = {10.1109/TASLPRO.2025.3557248},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1616-1627},
  title    = {Unsupervised end-to-end accented speech recognition under low-resource conditions},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-review-opinion generation for cross-domain aspect-based sentiment analysis. <em>TASLPRO</em>, <em>33</em>, 1604-1615. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment analysis (ABSA) aims to extract all aspect terms from review texts and classify their sentiment polarity. Although supervised methods exhibit excellent performance on the ABSA task, review texts in new domains typically lack labels, and it is time-consuming and costly to annotate for them. Unsupervised domain adaptation can transfer the knowledge learned from the source domain to the target domain to alleviate the problem of insufficient fine-grained labeled data. In this paper, we propose a novel label-review-opinion generative model (LRO-Gen) for cross-domain ABSA task. Specifically, we generate pseudo-labels for target domain review texts. Then, the model uses syntactic similarity rules alongside the pseudo-labels to generate new target-specific labeled review texts, which narrows the gap between domains. Finally, the generative extraction model is used in place of the traditional extraction model to improve domain generalization. In particular, we strengthen the correlation between sentiment elements by transforming generating a pair of sentiment elements into a pseudo-opinion sentence. Experimental results on four datasets demonstrate that our model is more effective than previous cross-domain ABSA models.},
  archive  = {J},
  author   = {Yinwei Bao and Xiangjie Kong and Qiuhua Yi and Chenwei Zhang and Linan Zhu and Guojiang Shen},
  doi      = {10.1109/TASLPRO.2025.3555107},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1604-1615},
  title    = {Label-review-opinion generation for cross-domain aspect-based sentiment analysis},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-resolution spectrums analysis-based DOA estimation using energy and sparsity joint mask. <em>TASLPRO</em>, <em>33</em>, 1590-1603. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {First Order Ambisonics has attracted significant attention for direction-of-arrival estimation. Combining with the time-frequency analysis techniques, existing methods could realize accurate localization results in most of acoustic scenarios. However, these methods encounter challenges such as energy leakage due to the uncertainty of linear time-frequency transformation, and insufficient localization cues in high-reverberant noisy acoustic environments. This paper addresses these challenges by integrating the localization cues from spectrums with varying time-frequency resolutions. Specifically, by applying analysis window with different sizes, the spectrums with non-redundant localization cues can be generated. Then, a joint mask is designed by combining Hoyer sparsity and inter-channel energy measurements to assess the localization contribution at each point within spectrums. The accurate direct-of-arrival estimation can be achieved by applying the masked DOA cues of T-F points from spectrums with different resolutions. Objective evaluations are conducted in both simulation and actual recording environments. Corresponding results prove that the proposed method could exhibit a superior localization performance than several existing localization estimators.},
  archive  = {J},
  author   = {Shang Gao and Maoshen Jia and Ruihai Dong},
  doi      = {10.1109/TASLPRO.2025.3552966},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1590-1603},
  title    = {Multi-resolution spectrums analysis-based DOA estimation using energy and sparsity joint mask},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of physiological vocal features from neck surface acceleration signals using probabilistic bayesian neural networks. <em>TASLPRO</em>, <em>33</em>, 1576-1589. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study presents a novel application of a Probabilistic Bayesian Neural Network (PBNN) for estimating vocal function variables and enhancing non-invasive ambulatory voice monitoring by addressing aleatoric and epistemic uncertainties in regression tasks. The proposed PBNN allows for estimating key physiological parameters including subglottal pressure, vocal fold contact pressure, thyroarytenoid, and cricothyroid muscle activations, from seven aerodynamic and acoustic features. The PBNN is trained on the Triangular Body-Cover Model (TBCM) of the vocal folds to produce a non-linear inverse mapping between its inputs and outputs. Furthermore, the selected aerodynamic and acoustic features can be obtained in ambulatory settings, thus enhancing the practical applicability of the proposed method. Transfer Learning is then applied to integrate real voice data into the initially synthetic-trained network to refine subglottal pressure estimations. The confidence intervals generated by the PBNN illustrate its ability to identify uncertain estimations, as the results show correlations between prediction errors and the estimated aleatoric and epistemic uncertainties. This correlation is advantageous because it shows that the network can effectively predict potential inaccuracies in its estimations. Increased uncertainty is mainly observed at operating points where the TBCM is likely to exhibit non-linear behaviors, at higher subglottal pressures. This suggests that the selected input features may not be robust enough for capturing the nonlinear effects in the TBCM. These results highlight the potential for future research to assess the viability of incorporating new features and additional measurements that could better capture non-linear responses.},
  archive  = {J},
  author   = {Joaquín Sepúlveda and Jesús A. Parra and Emiro J. Ibarra and Mauricio Araya and Patricio De La Cuadra and Matías Zañartu},
  doi      = {10.1109/TASLPRO.2025.3552938},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1576-1589},
  title    = {Estimation of physiological vocal features from neck surface acceleration signals using probabilistic bayesian neural networks},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-gradients distributed filtered reference least mean square algorithm – A robust distributed multichannel active noise control algorithm. <em>TASLPRO</em>, <em>33</em>, 1563-1575. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Distributed multichannel active noise control (DMCANC), which utilizes multiple individual processors to achieve a global noise reduction performance comparable to conventional centralized multichannel active noise control (MCANC), has become increasingly attractive due to its high computational efficiency. However, the majority of current DMCANC algorithms disregard the impact of crosstalk across nodes and impose the assumption of an ideal network devoid of communication limitations, which is an unrealistic assumption. Therefore, this work presents a robust DMCANC algorithm that employs the compensating filter to mitigate the impact of crosstalk. The proposed solution enhances the DMCANC system's flexibility and security by utilizing local gradients instead of local control filters to convey enhanced information, resulting in a mixed-gradients distributed filtered reference least mean square (MGDFxLMS) algorithm. The performance investigation demonstrates that the proposed approach performs well with the centralized method. Furthermore, to address the issue of communication delay in the distributed network, a practical strategy that auto-shrinks the step size value in response to the delayed samples is implemented to improve the system's resilience. The numerical simulation results demonstrate the efficacy of the proposed auto-shrink step size MGDFxLMS (ASSS-MGDFxLMS) algorithm across various communication delays, highlighting its practical value.},
  archive  = {J},
  author   = {Junwei Ji and Dongyuan Shi and Woon-Seng Gan},
  doi      = {10.1109/TASLPRO.2025.3552932},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1563-1575},
  title    = {Mixed-gradients distributed filtered reference least mean square algorithm – A robust distributed multichannel active noise control algorithm},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated incremental named entity recognition. <em>TASLPRO</em>, <em>33</em>, 1551-1562. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated Named Entity Recognition (FNER) boosts model training within each local client by aggregating the model updates of decentralized local clients, without sharing their private data. However, existing FNER methods assume fixed entity types and local clients in advance, leading to their ineffectiveness in practical applications. In a more realistic scenario, local clients receive new entity types continuously, while new local clients collecting novel data may irregularly join the global FNER training. This challenging setup, referred to here as Federated Incremental NER, renders the global model suffering from heterogeneous forgetting of old entity types from both intra-client and inter-client perspectives. To overcome these challenges, we propose a Local-Global Forgetting Defense (LGFD) model. Specifically, to address intra-client forgetting, we develop a structural knowledge distillation loss to retain the latent space's feature structure and a pseudo-label-guided inter-type contrastive loss to enhance discriminative capability over different entity types, effectively preserving previously learned knowledge within local clients. To tackle inter-client forgetting, we propose a task switching monitor that can automatically identify new entity types under privacy protection and store the latest old global model for knowledge distillation and pseudo-labeling. Experiments demonstrate significant improvement of our LGFD model over comparison methods.},
  archive  = {J},
  author   = {Duzhen Zhang and Yahan Yu and Chenxing Li and Jiahua Dong and Dong Yu},
  doi      = {10.1109/TASLPRO.2025.3555097},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1551-1562},
  title    = {Federated incremental named entity recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised reflective learning through self-distillation and online clustering for speaker representation learning. <em>TASLPRO</em>, <em>33</em>, 1535-1550. (<a href='https://doi.org/10.1109/TASLPRO.2025.3555132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speaker representation learning is crucial for voice recognition systems, with recent advances in self-supervised approaches reducing dependency on labeled data. Current two-stage iterative frameworks, while effective, suffer from significant computational overhead due to repeated rounds of clustering and training. They also struggle with noisy pseudo labels that can impair model learning. This paper introduces self-supervised reflective learning (SSRL), an improved framework that addresses these limitations by enabling continuous refinement of pseudo labels during training. Through a teacher-student architecture and online clustering mechanism, SSRL eliminates the need for iterative training rounds. To handle label noise, we incorporate noisy label modeling and pseudo label queues that maintain temporal consistency. Experiments on VoxCeleb show SSRL's superiority over current two-stage iterative approaches, surpassing the performance of a 5-round method in just a single training round. Ablation studies validate the contributions of key components like noisy label modeling and pseudo label queues. Moreover, consistent improvements in pseudo labeling and the convergence of cluster counts demonstrate SSRL's effectiveness in deciphering unlabeled data. This work marks an important advancement in efficient and accurate self-supervised speaker representation learning through the novel reflective learning paradigm.},
  archive  = {J},
  author   = {Danwei Cai and Zexin Cai and Ze Li and Ming Li},
  doi      = {10.1109/TASLPRO.2025.3555132},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1535-1550},
  title    = {Self-supervised reflective learning through self-distillation and online clustering for speaker representation learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAMP+: Retrieval-augmented MOS prediction with prior knowledge integration. <em>TASLPRO</em>, <em>33</em>, 1520-1534. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural network-based automatic Mean Opinion Score (MOS) prediction plays a pivotal role in assessing the perceptual quality of synthetic speech. Historically, this field has struggled with limited data resources, a problem not fully resolved by existing methodologies. On the one hand, self-supervised learning (SSL) models could enhance the representational power of the feature extractor by extensive pre-training on large datasets, but fail to address the data scarcity problem affecting the downstream decoder. On the other hand, methods such as data augmentation and the incorporation of individual judges' assessments could improve data utilization, but hard to ensure a satisfactory balance between the scale and quality of the additional dataset. To address these challenges of decoder inefficiency and suboptimal data utilization, we introduce a retrieval-augmented MOS prediction method with prior knowledge integration. This method employs a new approach to using SSL features by retrieving similar instances in the feature space to obtain scores, thereby boosting decoder performance. We also optimize dataset utilization by exploiting quality-related information from the dataset as prior knowledge. Furthermore, our framework includes a dynamic retrieval scope adaptation and a fusion network, enhancing both robustness and accuracy while minimizing the need for manual parameter tuning. Experimental results demonstrate that our retrieval approach, enhanced by prior knowledge, significantly outperforms existing state-of-the-art methods in the synthetic speech MOS prediction task across various scenarios.},
  archive  = {J},
  author   = {Hui Wang and Shiwan Zhao and Xiguang Zheng and Jiaming Zhou and Xuechen Wang and Yong Qin},
  doi      = {10.1109/TASLPRO.2025.3552957},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1520-1534},
  title    = {RAMP+: Retrieval-augmented MOS prediction with prior knowledge integration},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph reasoning with self-supervised reinforcement learning. <em>TASLPRO</em>, <em>33</em>, 1508-1519. (<a href='https://doi.org/10.1109/TASLPRO.2025.3540648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Reinforcement learning (RL) is an effective method of finding reasoning pathways in incomplete knowledge graphs (KGs). To overcome the challenges of a large action space, a self-supervised pre-training method is proposed to warm up the policy network before the RL training stage. To alleviate the distributional mismatch issue in general self-supervised RL (SSRL), in our supervised learning (SL) stage, the agent selects actions based on the policy network and learns from generated labels; this self-generation of labels is the intuition behind the name self-supervised. With this training framework, the information density of our SL objective is increased and the agent is prevented from getting stuck with the early rewarded paths. Our self-supervised RL (SSRL) method improves the performance of RL by pairing it with the wide coverage achieved by SL during pretraining, since the breadth of the SL objective makes it infeasible to train an agent with that alone. We show that our SSRL model meets or exceeds current state-of-the-art results on all Hits@k and mean reciprocal rank (MRR) metrics on four large benchmark KG datasets. This SSRL method can be used as a plug-in for any RL architecture for a KGR task. We adopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL models and experimentally show that our SSRL model consistently outperforms both baselines on all of these four KG reasoning tasks.},
  archive  = {J},
  author   = {Ying Ma and Owen Burns and Mingqiu Wang and Gang Li and Nan Du and Laurent El Shafey and Liqiang Wang and Izhak Shafran and Hagen Soltau},
  doi      = {10.1109/TASLPRO.2025.3540648},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1508-1519},
  title    = {Knowledge graph reasoning with self-supervised reinforcement learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADTMOS – Synthesized speech quality assessment based on audio distortion tokens. <em>TASLPRO</em>, <em>33</em>, 1493-1507. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the fields of voice conversion (VC) and text-to-speech (TTS), recent years have witnessed a growing interest in developing synthesized speech quality assessment (SQA) systems. For such systems, it is essential to reliably and accurately evaluate the quality of synthesized speech produced by VC and TTS systems, as this remains a crucial issue requiring further exploration. Among various evaluation standards of speech quality, the mean opinion score (MOS) is the most commonly used SQA metric. The rapid advancement of deep learning (DL) techniques has propelled the emergence of DL-based MOS-based SQA algorithms. Unfortunately, none of these methods incorporate listeners' perceptions of audio distortions, which are considered one of the key factors affecting listeners' MOS ratings. To fill such a research gap to some extent, we propose a novel speech quality assessment framework, namely ADTMOS (Audio Distortion Token-Guided Deep MOS Predictor). ADTMOS consists of three parts: a public encoding layer which encodes the audio embeddings, an audio distortion token extractor which extracts ADT scores related to the subjective perceptions of audio distortions, and a frame-wise MOS score generator which is responsible for computing frame-level MOS scores. Experimental results demonstrate that compared to the LDNet baseline, ADTMOS achieves a 0.83% improvement on the VCC2018-CSMSC dataset and a 4.58% increase on the BVCC dataset in the system-level Spearman's rank correlation coefficient (SRCC). Furthermore, two innovative data augmentation techniques have been developed for the SQA task, aiming to mitigate the challenges of data scarcity and uneven sample distribution commonly encountered in SQA datasets.},
  archive  = {J},
  author   = {Qiao Liang and Ying Shen and Tiantian Chen and Lin Zhang and Shengjie Zhao},
  doi      = {10.1109/TASLPRO.2025.3552925},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1493-1507},
  title    = {ADTMOS – Synthesized speech quality assessment based on audio distortion tokens},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot cross-lingual text-to-speech with style-enhanced normalization and auditory feedback training mechanism. <em>TASLPRO</em>, <em>33</em>, 1479-1492. (<a href='https://doi.org/10.1109/TASLPRO.2025.3548429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In an increasingly globalized and interconnected world, the ability to communicate in more than one language is a vital skill that can reduce language barriers and promote cultural interaction. However, mastering multiple languages requires a significant investment of time and effort. Here, zero-shot cross-lingual text-to-speech synthesis (TTS) offers benefits to augment human communication by producing high-quality speech in multiple languages while preserving the original speaker's vocal characteristics. However, building such a system presents several challenges, including ensuring high-quality synthesis and achieving similarity between the synthesized speaker and the reference speaker, especially when training a model for low-resource languages. In this study, we propose a novel technique known as Style-Enhanced Normalization TTS (STEN-TTS) to achieve two objectives: preserving synthesis quality while simultaneously enhancing the ability of zero-shot adaptation with just a few seconds of reference for the purpose of cross-lingual synthesis. The model itself can also be trained with low-resource data, but using data of only 10 or 20 minutes is a major challenge. To improve the quality of synthesized audio in low-resource languages, we propose a combination of STEN-TTS with different training methods, including unsupervised text encoding, knowledge distillation, and an auditory feedback mechanism. An experimental evaluation was conducted in five languages (English, Chinese, Indonesian, Japanese, and Vietnamese), considering high- and low-resource training data as well as seen and unseen speakers. The proposed approach has shown its effectiveness in a high-resource setting, achieving a remarkable similarity (SMOS) of 3.44$\pm$0.17 for cross-lingual conversion as well as verification scores of 93.4% and 80.5% for seen and unseen speakers, respectively. The results in a low-resource setting, measured by phoneme error rates, also indicate a substantial improvement, with enhancements of approximately 3-4% . In this case, the quality of speaker verification remains consistently high, achieving scores of 90.0% and 78.0% for seen and unseen speakers.},
  archive  = {J},
  author   = {Chung Tran and Chi Mai Luong and Sakriani Sakti},
  doi      = {10.1109/TASLPRO.2025.3548429},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1479-1492},
  title    = {Zero-shot cross-lingual text-to-speech with style-enhanced normalization and auditory feedback training mechanism},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PEVD-based speech enhancement in wireless acoustic sensor networks. <em>TASLPRO</em>, <em>33</em>, 1467-1478. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ubiquity of smart devices equipped with microphones in modern environments has opened the door to performing audio processing tasks such as speech enhancement over networks of microphones instead of traditional compact arrays. However, wireless acoustic sensor networks (WASNs) also introduce geometry estimation, clock synchronisation, and latency compensation as often necessary steps prior to traditional enhancement techniques. Instead, this paper investigates the use of blind polynomial eigenvalue decomposition (PEVD)-based enhancement in such networks with an emphasis on its robustness to communication latency between nodes. Simulations and experiments on real data show that the network-based PEVD enhancement consistently gives equivalent or better signal-to-noise ratio (SNR) than the best node in the network, even when experiencing communication latency. In-depth analysis of the network geometry and the PEVD filterbanks provides insights into the strengths and shortcomings of PEVD-based enhancement in networks, and shows it is robust to the issue of reference microphone selection. Finally, evaluation of PEVD as a pre-processing step to automatic speech recognition (ASR) shows an improvement of 7 points in word error rate (WER) over multichannel Wiener filtering for very noisy real conversational data.},
  archive  = {J},
  author   = {Emilie d'Olne and Vincent W. Neo and Patrick A. Naylor},
  doi      = {10.1109/TASLPRO.2025.3552893},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1467-1478},
  title    = {PEVD-based speech enhancement in wireless acoustic sensor networks},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic assessment of chinese dysarthria using audio-visual vowel graph attention network. <em>TASLPRO</em>, <em>33</em>, 1454-1466. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic assessment of dysarthria remains a highly challenging task due to the high heterogeneity in acoustic signals and the limited data. Currently, research on the automatic assessment of dysarthria primarily focuses on two approaches: one that utilizes expert features combined with machine learning, and the other that employs data-driven deep learning methods to extract representations. Studies have shown that expert features can effectively account for the heterogeneity of dysarthria but may lack comprehensiveness. In contrast, deep learning methods excel at uncovering latent features. Therefore, integrating the advantages of expert knowledge and deep learning to construct a neural network architecture based on expert knowledge may be beneficial for interpretability and assessment performance. In this context, the present paper proposes a vowel graph attention network based on audio-visual information, which effectively integrates the strengths of expert knowledge and deep learning. Firstly, the VGAN (Vowel Graph Attention Network) structure based on vowel space theory was designed, which has two branches to mine the information in features and the spatial correlation between vowels respectively. Secondly, a feature set based on expert knowledge and deep representation is designed. Finally, visual information was incorporated into the model to further enhance its robustness and generalizability. Tested on the Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database, this method exhibited superior performance in regression experiments targeting Frenchay scores compared to existing approaches.},
  archive  = {J},
  author   = {Xiaokang Liu and Xiaoxia Du and Juan Liu and Rongfeng Su and Manwa Lawrence Ng and Yumei Zhang and Yudong Yang and Shaofeng Zhao and Lan Wang and Nan Yan},
  doi      = {10.1109/TASLPRO.2025.3546562},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1454-1466},
  title    = {Automatic assessment of chinese dysarthria using audio-visual vowel graph attention network},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Whistle: Data-efficient multilingual and crosslingual speech recognition via weakly phonetic supervision. <em>TASLPRO</em>, <em>33</em>, 1440-1453. (<a href='https://doi.org/10.1109/TASLPRO.2025.3550683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pretraining with phonetic or graphemic transcription, and self-supervised pretraining. We find that pretraining with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pretraining with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency. It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency.},
  archive  = {J},
  author   = {Saierdaer Yusuyin and Te Ma and Hao Huang and Wenbo Zhao and Zhijian Ou},
  doi      = {10.1109/TASLPRO.2025.3550683},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1440-1453},
  title    = {Whistle: Data-efficient multilingual and crosslingual speech recognition via weakly phonetic supervision},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust weakly supervised bird species detection via peak aggregation and PIE. <em>TASLPRO</em>, <em>33</em>, 1427-1439. (<a href='https://doi.org/10.1109/TASLPRO.2025.3552983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A contemporary challenge in sound event detection is developing robust models with weak supervision. However, the problems arising from weak annotations are severe, especially in fields like bioacoustics, where the datasets contain non-negligible missing and incorrect labels, among other issues. Current methods developed on short and curated audio benchmarks misrepresent the difficulties of dealing with such scenarios. They leverage the Multiple-Instance-Learning framework, which relies on pooling functions for the many-to-one mapping of instances to their shared label. Unfortunately, non-standard assumptions under this framework have led to unreliable recipes that mix many sources of influence, resembling a murky semi-supervised approach with ill-posed objectives that do not address the fundamental problems of weak supervision in challenging and long recordings. This work introduces a novel weakly supervised training framework using peak-finding and attention aggregation. The significance of this straightforward and robust approach is demonstrated on a challenging bioacoustics dataset for bird species classification. Also, an effective data augmentation technique naturally stems from this framework to further enhance the models. The core idea of this method is to increase the fidelity of the input signals regarding their weakly assigned labels while keeping the sources of influence separate for incremental improvement. Finally, a reliable strategy via posterior analysis is suggested for truthful model comparison, which shows some metrics might mislead us on noisy and weakly labeled datasets. The result shows a clear improvement for such tasks and establishes a common practice for future weakly supervised audio models.},
  archive  = {J},
  author   = {Houtan Ghaffari and Paul Devos},
  doi      = {10.1109/TASLPRO.2025.3552983},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1427-1439},
  title    = {Robust weakly supervised bird species detection via peak aggregation and PIE},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EX-vector: Emotional X-vector transfer learning for speaker recognition with emotion domain adaption. <em>TASLPRO</em>, <em>33</em>, 1415-1426. (<a href='https://doi.org/10.1109/TASLPRO.2025.3540666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In emotional speaker recognition, the emotion-mismatch problem arises due to the inconsistency of the speaker's emotional state between the registration utterance and the test utterance. In this paper, we propose a new embedding-based method, Emotional X-Vector (EX-Vector), to address the above problems through a transfer learning strategy. EX-Vector introduces an emotion domain classifier with a gradient reverse layer to transfer emotion embeddings to emotion-independent speaker embeddings for emotion domain adaptation. Furthermore, the method incorporates an embedding cross-mapping module, which explores how individual units of speaker features influence each other to enrich the speaker information in the embedding, supporting both transfer learning and speaker recognition tasks. Attentive speaker identity compensation adopts attention mechanism and residual structure to retain the part of emotion information related to speaker identity and compensate it into speaker embedding during speaker recognition. The method extends and compensates the embedding content in speaker intrinsic feature extraction and speaker recognition respectively, preventing information loss during embedding transformation. The proposed EX-Vector is evaluated on emotional speaker recognition corpuses Mandarin Affective Speech Corpus (MASC), Interactive Emotional Dyadic Motion Capture (IEMOCAP), Crowd-Sourced Emotional Multi-modal Actors Dataset (CREMA-D) and podcasts recordings database (MSP-PODCAST). EX-Vector shows significant improvement in the evaluation metrics of EER and DCF compared with state-of-the-art methods in the task of emotional speaker recognition.},
  archive  = {J},
  author   = {Zhuo Yang and Dongdong Li and Yu Cai and Zhe Wang and Hai Yang},
  doi      = {10.1109/TASLPRO.2025.3540666},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1415-1426},
  title    = {EX-vector: Emotional X-vector transfer learning for speaker recognition with emotion domain adaption},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniPET-SPK: A unified framework for parameter-efficient tuning of pre-trained speech models for robust speaker verification. <em>TASLPRO</em>, <em>33</em>, 1402-1414. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the size of pre-trained models grows, fine-tuning becomes practically unfeasible due to expanding computation and storage requirements, as well as the risk of overfitting. In this study, we concentrate on exploring parameter-efficient tuning (PET) methods for adapting large-scale pre-trained self-supervised speech models to the speaker verification task. Correspondingly, we propose three parameter-efficient tuning methods: i) an adapter-tuning method, ii) a prompt-tuning method, and iii) a unified framework that effectively incorporates adapter-tuning and prompt-tuning with a dynamically learnable gating mechanism. First, we propose the Inner+Inter Adapter framework, which inserts two types of adapters into pre-trained models, allowing for adaptation of latent features within the intermediate Transformer layers and output embeddings from all Transformer layers, through a parallel adapter design. Second, we propose the Deep Speaker Prompting method that concatenates trainable prompt tokens into the input space of pre-trained models to guide adaptation. Lastly, we propose the UniPET-SPK, a unified framework that effectively incorporates these two alternate PET methods into a single framework with a dynamic trainable gating mechanism. The proposed UniPET-SPK learns to find the optimal mixture of PET methods to match different datasets and scenarios. We conduct a comprehensive set of experiments on several datasets to validate the effectiveness of the proposed parameter-efficient tuning methods. Experimental results on the VoxCeleb, CN-Celeb, and the 1$^{\mathrm{{st}}}$48-UTD forensic datasets demonstrate that the proposed UniPET-SPK can consistently outperform the two PET methods, fine-tuning, and other parameter-efficient tuning methods, achieving superior performance while updating only 5.4% of the parameters. We further conduct experiments on the CN-Celeb and 1$^{\mathrm{{st}}}$48-UTD datasets to demonstrate the robustness and generalization ability of the proposed methods for speaker verification in different languages and more challenging scenarios.},
  archive  = {J},
  author   = {Mufan Sang and John H. L. Hansen},
  doi      = {10.1109/TASLPRO.2025.3539005},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1402-1414},
  title    = {UniPET-SPK: A unified framework for parameter-efficient tuning of pre-trained speech models for robust speaker verification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASR error correction using large language models. <em>TASLPRO</em>, <em>33</em>, 1389-1401. (<a href='https://doi.org/10.1109/TASLPRO.2025.3551083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling.},
  archive  = {J},
  author   = {Rao Ma and Mengjie Qian and Mark Gales and Kate Knill},
  doi      = {10.1109/TASLPRO.2025.3551083},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1389-1401},
  title    = {ASR error correction using large language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An automated method to correct artifacts in neural text-to-speech models. <em>TASLPRO</em>, <em>33</em>, 1375-1388. (<a href='https://doi.org/10.1109/TASLPRO.2025.3550718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advances in deep learning technology have enabled high-quality speech synthesis, and text-to-speech models are widely used in a variety of applications. However, even state-of-the-art models still produce artificial speech, highlighting the need to correct errors in synthesized speech. Traditional speech correction methodologies face a number of challenges, such as the inefficiency of manually specifying errors, the need to retrain models, and the need for additional data to correct synthesized speech errors. In this paper, we present a novel approach that detects and corrects contextual errors within a model to improve synthetic speech without requiring additional resources or model retraining. Specifically, we analyze the inherent limitations of neural network encoders responsible for contextualizing input sentences and propose a method that automatically identifies abnormal encoder context vectors. We also introduce a correction algorithm that enhances the quality of speech by correcting the incorrect relationships between phonemes that cause abnormal encoder context vectors. Our algorithm demonstrated a 25.86% and 4.69% reduction in alignment errors and a 2.25% and 0.63% improvement in objective metrics such as Fréchet Wav2Vec distance for the Tacotron2 and VITS models. In addition, our algorithm improved the comparative mean opinion scores, a subjective evaluation, by 1.34 and 0.52. The results support the feasibility of a novel approach to identify, correct, and improve defects automatically in neural speech synthesis models. The results support the feasibility of a novel approach to identify, correct, and improve defects automatically in neural speech synthesis models.},
  archive  = {J},
  author   = {Seongyeop Jeong and June Sig Sung and Inchul Hwang and Jaesik Choi},
  doi      = {10.1109/TASLPRO.2025.3550718},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1375-1388},
  title    = {An automated method to correct artifacts in neural text-to-speech models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossSpeech++: Cross-lingual speech synthesis with decoupled language and speaker generation. <em>TASLPRO</em>, <em>33</em>, 1364-1374. (<a href='https://doi.org/10.1109/TASLPRO.2025.3547231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin.},
  archive  = {J},
  author   = {Ji-Hoon Kim and Hong-Sun Yang and Yoon-Cheol Ju and Il-Hwan Kim and Byeong-Yeol Kim and Joon Son Chung},
  doi      = {10.1109/TASLPRO.2025.3547231},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1364-1374},
  title    = {CrossSpeech++: Cross-lingual speech synthesis with decoupled language and speaker generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PeerRTF: Robust MVDR beamforming using graph convolutional network. <em>TASLPRO</em>, <em>33</em>, 1349-1363. (<a href='https://doi.org/10.1109/TASLPRO.2025.3548434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate and reliable identification of the relative transfer functions (RTFs) between microphones with respect to a desired source is an essential component in the design of microphone array beamformers, specifically when applying the minimum variance distortionless response (MVDR) criterion. Since an accurate estimation of the RTF in a noisy and reverberant environment is a cumbersome task, we aim at leveraging prior knowledge of the acoustic enclosure to robustify the RTFs estimation by learning the RTF manifold. In this paper, we present a novel robust RTF identification method, tested and trained using both real recordings and simulated scenarios, which relies on learning the RTF manifold using a graph convolutional network (GCN) to infer a robust representation of the RTFs in a confined area, and consequently enhance the beamformer's performance.},
  archive  = {J},
  author   = {Daniel Levi and Amit Sofer and Sharon Gannot},
  doi      = {10.1109/TASLPRO.2025.3548434},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1349-1363},
  title    = {PeerRTF: Robust MVDR beamforming using graph convolutional network},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DQ-data2vec: Decoupling quantization for multilingual speech recognition. <em>TASLPRO</em>, <em>33</em>, 1337-1348. (<a href='https://doi.org/10.1109/TASLPRO.2025.3551074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked prediction, demonstrating remarkable performance in monolingual ASR. Previous studies have revealed that data2vec's shallow layers capture speaker and language information, middle layers encode phoneme and word features, while deep layers are responsible for reconstruction. Language and phoneme features are crucial for multilingual ASR. However, data2vec's masked representation generation relies on multi-layer averaging, inevitably coupling these features. To address this limitation, we propose a decoupling quantization based data2vec (DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two improved online K-means quantizers. Our core idea is using the K-means quantizer with specified cluster numbers to decouple language and phoneme information for masked prediction. Specifically, in the language quantization, considering that the number of languages is significantly different from other irrelevant features (e.g., speakers), we assign the cluster number to match the number of languages, explicitly decoupling shallow layers' language-related information from irrelevant features. This strategy is also applied to decoupling the middle layers' phoneme and word features. In a self-supervised scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec achieves a relative reduction of ${9.51\%}$ in phoneme error rate (PER) and ${11.58\%}$ in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a weakly-supervised scenario incorporating language labels and high-resource language text labels, the relative reduction is ${18.09\%}$ and ${1.55\%}$, respectively.},
  archive  = {J},
  author   = {Qijie Shao and Linhao Dong and Kun Wei and Sining Sun and Lei Xie},
  doi      = {10.1109/TASLPRO.2025.3551074},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1337-1348},
  title    = {DQ-data2vec: Decoupling quantization for multilingual speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-aspect noise-based regularization for multi-modal relation extraction in media posts. <em>TASLPRO</em>, <em>33</em>, 1324-1336. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-Modal Relation Extraction (MMRE) plays a key role in various multimedia applications including, recommendation and information retrieval systems. MMRE aims to extract the semantic relation between entities by leveraging context from a text-image pair. By utilizing context from images, the challenge of learning from noisy images in MMRE emerges as a research problem by itself. For instance, subtle variations in similar images can act as noise and potentially impact the predictions made by MMRE models. To tackle this problem, current work utilizes attention mechanisms to fuse relevant text and image features or devise data augmentation techniques (e.g., via generative models) to improve generalization. However, the current performance still remains unsatisfactory. In an effort to improve upon the performance, we propose a Dual-Aspect Noise-based Regularization framework that encompasses two techniques: 1) noise removal through an adaptive gating mechanism, 2) fighting noise with noise to improve feature stability in the learning process. We find that combining these techniques encourages the model to focus on more relevant image features for MMRE. We carry out extensive experiments and demonstrate that our proposed model is further enhanced by exploring data augmentation techniques. This additional improvement leads the model to achieve state-of-the-art performance on the widely-used Multi-modal Neural Relation Extraction (MNRE) dataset, and show its effectiveness and generalizability on the Multi-Modal Named Entity Recognition task.},
  archive  = {J},
  author   = {Kai Sun and Bin Shi and Samuel Mensah and Wenjian Liu and Bo Dong},
  doi      = {10.1109/TASLPRO.2025.3546851},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1324-1336},
  title    = {Dual-aspect noise-based regularization for multi-modal relation extraction in media posts},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing overlapped speech detection and speaker counting with spatially-infused spectro-temporal conformer. <em>TASLPRO</em>, <em>33</em>, 1307-1323. (<a href='https://doi.org/10.1109/TASLPRO.2025.3545255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Overlapped speech is widely present in conversations and can cause significant performance degradation in speech processing such as diarization, enhancement, and recognition. Overlapped speech detection (OSD) and speaker counting, in particular when the speakers are in the far field, pose challenges as the overlapped part is usually short, and heavy reverberation and noise may present in the conversation scenario. Existing solutions predominantly rely on spectral features extracted from a single microphone signal to perform the detection. In this paper, we propose an innovative approach that is able to efficiently extract spatial features via spatial dictionary learning (SDL) and fuse spatial and spectral features derived from multi-channel array signals. In essence, a spectro-temporal module is employed to extract local information from the spectrogram which is able to better detect overlapping segments under real scenarios. Subsequently, we study two kinds of spatial features: directional statistics, which are projected onto spherical location grids using SDL, and generalized cross-correlation with phase transform (GCC-PHAT) to model the spatial characteristics of speakers. These spatial features are then integrated with the spectral features for the OSD and speaker counting. In addition, a non-parameter magnitude-weighted fusion method is employed to combine the magnitude spectrogram with SDL results, enabling the model to prioritize critical frequency bins. Finally, the performance of the proposed approach is studied under AMI, Alimeeting, and CHiME-6 corpora. Experimental results show that our proposed approach achieves better performance than methods using spectral features only. Furthermore, we also conduct experiments of speaker diarization with OSD to evaluate the detection performance.},
  archive  = {J},
  author   = {Weiguang Chen and Jielong Yang and Xionghu Zhong and Eng Siong Chng and Minjie Cai},
  doi      = {10.1109/TASLPRO.2025.3545255},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1307-1323},
  title    = {Enhancing overlapped speech detection and speaker counting with spatially-infused spectro-temporal conformer},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A closer look at class-incremental learning for multi-label audio classification. <em>TASLPRO</em>, <em>33</em>, 1293-1306. (<a href='https://doi.org/10.1109/TASLPRO.2025.3547233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The main challenge in class-incremental learning (CIL) using deep learning models is catastrophic forgetting, which refers to the significant drop in performance on the previous tasks when the model is trained sequentially on the new task. In our previous work on CIL for multi-label audio classification, we used an independent learning (IndL) mechanism to mitigate forgetting in the classifier layer. In this work, we take a closer look at the forgetting that occurs in the intermediate layers of the CIL model. We find that earlier layers of the model are less prone to forgetting, but later layers, especially Batch Normalization (BN) layers, are biased toward new tasks. We replace the standard BN layers with Group Normalization and Continual Normalization layers to reduce forgetting. Based on these observations, we only update the layers where forgetting occurs. These simple modifications improve the overall performance of the model. Further, we analyze the effect of exemplars–a small number of samples of the previous tasks–to reduce forgetting while the CIL model learns the new task. We propose a simple but effective exemplar selection strategy from a multi-label dataset using simulated annealing. Experiments are performed on a dataset with 50 sound classes, with an initial classification task containing 30 base classes and 4 incremental phases of 5 classes each. After each phase, the system is tested for multi-label classification with the entire set of classes learned so far. The proposed method outperforms other methods, with an average F1-score of 41.8% and 36.3% over the five phases on random classes of AudioSet and FSD50K respectively, with minimal forgetting of classes in phase 0.},
  archive  = {J},
  author   = {Manjunath Mulimani and Annamaria Mesaros},
  doi      = {10.1109/TASLPRO.2025.3547233},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1293-1306},
  title    = {A closer look at class-incremental learning for multi-label audio classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imposing correlation structures for deep binaural spatio-temporal wiener filtering. <em>TASLPRO</em>, <em>33</em>, 1278-1292. (<a href='https://doi.org/10.1109/TASLPRO.2025.3548454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To improve speech quality and intelligibility in environments with noise and interfering sounds, binaural speech enhancement algorithms use the microphone signals from both the left and the right hearing device to generate an enhanced output signal for each ear. As a multi-frame extension of the binaural multi-channel Wiener filter, in this paper we consider the binaural spatio-temporal Wiener filter (STWF) in the short-time Fourier transform domain, which requires estimates of the highly time-varying spatio-temporal correlations of the speech and interference components. To this end, the binaural STWF is embedded into an end-to-end supervised learning framework, where temporal convolutional networks estimate the required quantities, i.e., the inverse spatio-temporal correlation matrices of the interference component and the spatio-temporal correlation vectors and power spectral densities of the speech components. In this paper, we impose spatio-temporal correlation structure on these quantities and relate them between the left and the right hearing device, aiming to reduce computational complexity while maintaining speech enhancement and interaural cue preservation performance. Assuming that the spatial correlation of the speech component is stationary over a small number of frames, we propose to decompose the spatio-temporal correlation vectors as the Kronecker product of a relative transfer function vector and a temporal correlation vector, either considering a global reference microphone or a reference microphone for each hearing device. In addition, we consider a deep bilateral STWF by neglecting the spatio-temporal correlations of the speech and interference components between both devices. The imposed spatio-temporal correlation structures greatly differ in the number of parameters that need to be estimated. The performance of causal versions of the deep binaural and bilateral STWF algorithms is evaluated based on both simulated and measured binaural room impulse responses (BRIRs) as well as diverse speech and noise sources. The simulation results demonstrate that the proposed spatio-temporal correlation structures significantly reduce the computational complexity of the binaural STWF while yielding a similar speech enhancement and interaural cue preservation performance compared to not imposing any spatio-temporal correlation structure. Furthermore, the results confirm that the deep binaural STWF outperforms the binaural Conv-TasNet algorithm as well as an algorithm that directly estimates the binaural multi-frame filter coefficients, while approaching the performance of the non-causal binaural complex convolutional transformer network (BCCTN) algorithm.},
  archive  = {J},
  author   = {Marvin Tammen and Simon Doclo},
  doi      = {10.1109/TASLPRO.2025.3548454},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1278-1292},
  title    = {Imposing correlation structures for deep binaural spatio-temporal wiener filtering},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint music segmentation and clustering based on self-attentive contrastive learning of multifaceted self-similarity representation. <em>TASLPRO</em>, <em>33</em>, 1267-1277. (<a href='https://doi.org/10.1109/TASLPRO.2025.3548449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper describes a method of music structure analysis that aims to partition a music recording into musically meaningful segments and group similar segments with the same label. A basic approach to this task is to extract latent acoustic features with a deep neural network (DNN) and then perform segmentation and clustering based on self-similarity matrices over those features. The performance of this approach, however, is essentially limited because the latent features are not necessarily optimal for the following step, and the self-similarity matrices have often been hand-crafted based on prior knowledge of musical sections. To overcome this limitation, we propose a jointly-trainable network that has a feature extraction subnetwork followed by segmentation and clustering branches. The extraction subnetwork is implemented with a Transformer encoder, whose multi-head self-attention mechanism is expected to learn multifaceted self-similarity matrices in a data-driven manner. The clustering branch is implemented by deep-unfolding the expectation-maximization (EM) algorithm of a Gaussian mixture model and thus has no trainable parameters. The segmentation branch is introduced for supervised boundary detection, encouraging the temporal continuity of labels estimated by the clustering branch. The evaluation results show the effectiveness of the joint optimization and the superiority of the proposed method over state-of-the-art methods.},
  archive  = {J},
  author   = {Tsung-Ping Chen and Kazuyoshi Yoshii},
  doi      = {10.1109/TASLPRO.2025.3548449},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1267-1277},
  title    = {Joint music segmentation and clustering based on self-attentive contrastive learning of multifaceted self-similarity representation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompting, decoding, embedding: Leveraging pretrained language models for high-quality and diverse open rule induction. <em>TASLPRO</em>, <em>33</em>, 1255-1266. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Open rule induction (OpenRI) is devoted to obtaining reasoning rules expressed in natural languages. Compared with traditional rules with predefined logical symbols and domain predicates, open rules are more expressive and easier to use in real-world applications. Owing to the rich knowledge content and powerful text generation ability of pretrained language models (PLMs), researchers have begun adapting PLMs to induce open rules in recent years. However, the previous OpenRI method suffered from the bias of PLMs, thus usually generating tedious and redundant open rules. To alleviate the above issues, this paper proposes a novel framework, named Quadori (high-Quality and diverse open rule induction), which leverages the power of PLMs by prompting, decoding, and embedding to increase the quality and diversity of the induced open rules. Specifically, entity-type-based prompts and sampling-supported beam search are proposed for obtaining support instances according to the given rule head. A determinantal point process is then employed to generate rule bodies from the assembly support instances. Adequate experiments on a public open rule induction dataset and six relation extraction datasets show that Quadori is superior to the existing methods in both quality and diversity},
  archive  = {J},
  author   = {Wangtao Sun and Shizhu He and Jun Zhao and Kang Liu},
  doi      = {10.1109/TASLPRO.2025.3546556},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1255-1266},
  title    = {Prompting, decoding, embedding: Leveraging pretrained language models for high-quality and diverse open rule induction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vec-tok speech: Speech vectorization and tokenization for neural speech generation. <em>TASLPRO</em>, <em>33</em>, 1243-1254. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-quality texts and images in various tasks. While current speech LMs have made significant progress, there are still challenges to overcome in terms of achieving optimal speech quality and broad task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50,000 hours of speech, performs better than other SOTA models.},
  archive  = {J},
  author   = {Xinfa Zhu and Yuanjun Lv and Yi Lei and Tao Li and Wendi He and Hongbin Zhou and Heng Lu and Lei Xie},
  doi      = {10.1109/TASLPRO.2025.3546559},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1243-1254},
  title    = {Vec-tok speech: Speech vectorization and tokenization for neural speech generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving end-to-end sign language translation via multi-level contrastive learning. <em>TASLPRO</em>, <em>33</em>, 1230-1242. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sign Language Translation (SLT) aims to translate content from a sign language video into a spoken language sentence, which is a promising technology to bridge the communication gap between the deaf and hearing people. The end-to-end SLT model is increasingly becoming the dominant paradigm due to its inherent advantages in reducing error propagation and latency. Nonetheless, a significant limitation of this end-to-end paradigm is its heavy dependency on large-scale parallel data. This dependence poses a challenge to improving SLT performance due to the prohibitive costs associated with data collection and annotation. Our preliminary observation shows that this data scarcity leads to the collapse of the token (sub-word unit in this paper) representations and the inaccuracy of the generated tokens. To alleviate this issue, we propose MCL-SLT, a novel Multi-level Contrastive Learning method for SLT, which incorporates token- and sentence-level contrastive learning into SLT training to learn effective token representations. Specifically, token-level contrastive learning generates positive examples by data augmentation and selects diverse and hard negative examples from vocabulary to learn more discriminative token representations. Additionally, sentence-level contrastive learning utilizes sign video representations as anchors to refine the quality of token representations. Extensive experiments on three widely used datasets, PHOENIX-2014T, CSL-Daily, and How2Sign, demonstrate the effectiveness of MCL-SLT, with significant improvements over baselines, and also show superior robustness and generalization of our method in signer-independent settings.},
  archive  = {J},
  author   = {Biao Fu and Liang Zhang and Peigen Ye and Pei Yu and Cong Hu and Xiaodong Shi and Yidong Chen},
  doi      = {10.1109/TASLPRO.2025.3546817},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1230-1242},
  title    = {Improving end-to-end sign language translation via multi-level contrastive learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multimodal object-entity relation extraction via multi-aspect contrastive learning in large multimodal models. <em>TASLPRO</em>, <em>33</em>, 1220-1229. (<a href='https://doi.org/10.1109/TASLPRO.2025.3548473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal Object-Entity Relation Extraction (MORE) is an emerging task in information extraction, which aims to extract object-entity relational facts from text and image data. Despite obtaining promising results, previous works are based on small language models or small vision-language models, and the potential of using the power of generative large multimodal models (LMMs) for the MORE task still remains unclear. Moreover, most existing studies focus on learning the multimodal representation merely based on the supervision signals of each sample, failing to consider the semantic similarities and differences between samples. To tackle these two issues, in this paper, we propose a multi-AsPect cOntrastive Learning-enhanced Large multimOdal model named APOLLO, which formulates MORE as a generation problem and employs a parameter-efficient fune-tuning method LoRA to inject task-specific knowledge into a representative LMM named InstructBLIP, followed by enhancing the multimodal representation to capture the inter-sample relationship with a multi-aspect contrastive learning algorithm. Experimental results on a benchmark dataset demonstrate the superiority of APOLLO over existing multimodal approaches.},
  archive  = {J},
  author   = {Yaming Zhang and Jianfei Yu and Wenya Wang and Li Yang and Jia Yang and Rui Xia},
  doi      = {10.1109/TASLPRO.2025.3548473},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1220-1229},
  title    = {Enhancing multimodal object-entity relation extraction via multi-aspect contrastive learning in large multimodal models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MN-net: Speech enhancement network via modeling the noise. <em>TASLPRO</em>, <em>33</em>, 1208-1219. (<a href='https://doi.org/10.1109/TASLPRO.2025.3546819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently, deep learning-based speech enhancement methods generally focus on target speech extraction while neglecting modeling the other sound sources in the mixture. These methods still can't distinguish the target speech from the interference well. In this paper, we present a monaural speech enhancement network via Modeling the Noise (MN-Net), which includes a shared Encoder and three separate Decoders for parallel modeling the magnitude and phase spectrogram of target speech, and the complex spectrogram of noise. Specifically, we propose a Multi-Branch Feature Extractor (MBFE) module to capture the richer contextual information in mixture, and a Spatial Reconstruction Unit (SRU) to remove the redundancy from extracted features. We compared our proposed MN-Net with 18 classical speech enhancement methods on the VoiceBank+DEMAND dataset, and with 9 ones on DNS-Challenge dataset for denoising task, and with 7 ones on the WHAMR! dataset for simultaneous denoising & de-reverberation task. Our proposed MBFE module was applied to two classical speech enhancement methods, DB-AIAT and CMGAN, replacing their DenseBlocks module. The results demonstrate that applying the MBFE module can boost their performances while keeping smaller model size. A series of visualization analysis intuitively verify that modeling the noise can enable the network to distinguish the target speech from noise and other interference more accurately.},
  archive  = {J},
  author   = {Ying Hu and Qin Yang and Wenbing Wei and Li Lin and Liang He and Zhijian Ou and Wenzhong Yang},
  doi      = {10.1109/TASLPRO.2025.3546819},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1208-1219},
  title    = {MN-net: Speech enhancement network via modeling the noise},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging timbre semantic descriptors for cross-dataset instrument recognition. <em>TASLPRO</em>, <em>33</em>, 1196-1207. (<a href='https://doi.org/10.1109/TASLPRO.2025.3543974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Timbre, a perceptual attribute of musical sound, plays a critical role in both instrument recognition and timbre perception research. However, these fields are rarely integrated, and as a result, their findings rarely benefit one another. This study investigates how integrating timbre semantic descriptors as an auxiliary task can enhance musical instrument recognition under challenging cross-dataset conditions via multi-task learning (MTL). A standard flat MTL framework is introduced, alongside an asymmetric MTL proposed to amplify the influence of timbre semantic descriptors on instrument recognition. Experiments are conducted on Western and Chinese instrument datasets to evaluate cross-cultural performance. The results demonstrate significant improvements on both datasets using the flat MTL, with further analysis revealing the varying contributions of individual timbre descriptors and demonstrating its robustness across different hyperparameter settings. The asymmetric MTL achieves additional performance gains on Western datasets but not on Chinese ones. A dedicated analysis is carried out to investigate this discrepancy, offering insights into potential dataset-specific or cultural challenges.},
  archive  = {J},
  author   = {Shenyang Xu and Yuan Wang and Zijin Li and Feng Yu and Wei Li},
  doi      = {10.1109/TASLPRO.2025.3543974},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1196-1207},
  title    = {Leveraging timbre semantic descriptors for cross-dataset instrument recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSA-VC: Contrastive learning with selective attention in non-parallel voice conversion. <em>TASLPRO</em>, <em>33</em>, 1183-1195. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work introduces a novel approach to non-parallel voice conversion (VC) through contrastive learning with selective attention (CSA). Unlike traditional methods that suffer from inconsistent spectral feature mapping, our CSA model enhances spectral feature analysis and comparison by employing an attention matrix to measure spectral distances and select anchor points for contrastive loss. We assess our model's performance across various training schemes, including one-to-one and many-to-one voice conversions, gender differences, data scarcity, and accent variations, without adding extra parameters to maintain efficiency. Our evaluations, both subjective and objective, demonstrate superior sound quality in voice synthesis over baseline methods. Additionally, we explore the impact of selective attention in an ablation study, further validating our model's effectiveness in real-world voice conversion applications.},
  archive  = {J},
  author   = {Bima Prihasto and Yi-Xing Lin and Phuong Thi Le and Jia-Ching Wang and Chung-Hsien Wu},
  doi      = {10.1109/TASLPRO.2025.3542288},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1183-1195},
  title    = {CSA-VC: Contrastive learning with selective attention in non-parallel voice conversion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal audio-based disease prediction with transformer-based hierarchical fusion network. <em>TASLPRO</em>, <em>33</em>, 1170-1182. (<a href='https://doi.org/10.1109/TASLPRO.2025.3543975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio-based disease prediction is emerging as a promising supplement to traditional medical diagnosis methods, facilitating early, convenient, and non-invasive disease detection and prevention. Multimodal fusion, which integrates features from various domains within or across bio-acoustic modalities, has proven effective in enhancing diagnostic performance. However, most existing methods in the field employ unilateral fusion strategies that focus solely on either intra-modal or inter-modal fusion. This approach limits the full exploitation of the complementary nature of diverse acoustic feature domains and bio-acoustic modalities. Additionally, the inadequate and isolated exploration of latent dependencies within modality-specific and modality-shared spaces curtails their capacity to manage the inherent heterogeneity in multimodal data. To fill these gaps, we propose a transformer-based hierarchical fusion network designed for general multimodal audio-based disease prediction. Specifically, we seamlessly integrate intra-modal and inter-modal fusion in a hierarchical manner and proficiently encode the necessary intra-modal and inter-modal complementary correlations, respectively. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance in predicting three diseases: COVID-19, Parkinson's disease, and pathological dysarthria, showcasing its promising potential in a broad context of audio-based disease prediction tasks. Additionally, extensive ablation studies and qualitative analyses highlight the significant benefits of each main component within our model.},
  archive  = {J},
  author   = {Jinjin Cai and Ruiqi Wang and Dezhong Zhao and Ziqin Yuan and Victoria McKenna and Aaron Friedman and Rachel Foot and Susan Storey and Ryan Boente and Sudip Vhaduri and Byung-Cheol Min},
  doi      = {10.1109/TASLPRO.2025.3543975},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1170-1182},
  title    = {Multimodal audio-based disease prediction with transformer-based hierarchical fusion network},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anchor-aware position enhancement for weakly supervised knowledge graph alignment. <em>TASLPRO</em>, <em>33</em>, 1156-1169. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Entity alignment (EA) models currently rely heavily on large-scale labeled anchor links to succeed. However, these models often face challenges due to the limited availability of anchor links. To enhance the alignment performance of existing models under weakly supervised settings, some researchers have turned to active learning to search for more informative anchor links. Despite these efforts, the fundamental issue remains that existing neural EA models tend to overfit the limited anchor links, resulting in poor alignment performance. To tackle this problem, this paper addresses the challenge of low-resource EA by focusing on anchor entities and proposes a novel approach called Anchor-aware Position-Enhanced Low-resource Entity Alignment (APLEA). In the encoding process, we introduce a simple yet effective position encoding method that captures the long-term relationships between anchor entities and other distant entities using random walk with restart (RWR). This method allows us to extract more advanced positional features through a position attention layer. By incorporating structural information, relational information, and positional information from these three aspects, we obtain the final entity embeddings for alignment. In the decoding process, we address the issue of existing EA models disregarding the matched neighborhood constraints when inferring alignment results. Specifically, neighboring entities of a specific entity $e$ may not be strictly mapped to those of $e$’s counterpart. To overcome this limitation, we propose a simple refinement approach to the final similarity matrix without the need for additional training. Extensive experiments conducted on publicly available datasets demonstrate that APLEA significantly outperforms the state-of-the-art approaches by a large margin in weakly supervised EA tasks.},
  archive  = {J},
  author   = {Wei Tang and Haifeng Sun and Jingyu Wang and Qi Qi and Shimin Tao and Hao Yang and Bo Wu and LianYuan Li},
  doi      = {10.1109/TASLPRO.2025.3539467},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1156-1169},
  title    = {Anchor-aware position enhancement for weakly supervised knowledge graph alignment},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emozionalmente: A crowdsourced corpus of simulated emotional speech in italian. <em>TASLPRO</em>, <em>33</em>, 1142-1155. (<a href='https://doi.org/10.1109/TASLPRO.2025.3540662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER) relies on speech corpora collecting emotional voices for analysis. Emotions may vary by culture, language, and context—whether simulated, induced, or naturalistic—and resources featuring simulated emotional speech in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, an acted corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences simulating the Big Six emotions and neutrality. A subjective validation involving 829 individuals achieved a recognition accuracy of 66%, demonstrating the corpus' utility and representativeness. We also explored SER using the pretrained deep learning model wav2vec 2.0, fine-tuned to our corpus, which attained an accuracy of 82.45%. This paper details the crowdsourcing methodology, corpus analysis, and both human and computational validation studies, highlighting the corpus' broad applications in SER. Emozionalmente is publicly available, supporting further research in linguistics, speech processing, and affective computing within the Italian context.},
  archive  = {J},
  author   = {Fabio Catania and Jordan W. Wilke and Franca Garzotto},
  doi      = {10.1109/TASLPRO.2025.3540662},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1142-1155},
  title    = {Emozionalmente: A crowdsourced corpus of simulated emotional speech in italian},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can audio reveal music performance difficulty? insights from the piano syllabus dataset. <em>TASLPRO</em>, <em>33</em>, 1129-1141. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music Information Retrieval (MIR) field comprises some proof-of-concept works addressing this task that mainly focus on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has generally been neglected. This work addresses this gap in the field with two contributions: (i) PSyllabus, the first audio-based difficulty estimation dataset—collected from Piano Syllabus community—featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers as well as two additional benchmark datasets particularly compiled for evaluation purposes; and (ii) a recognition framework capable of managing different input representations—both in unimodal and multimodal manners—derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios proves the validity of the hypothesis and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset, developed code, and trained models are publicly shared to promote further research in the field.},
  archive  = {J},
  author   = {Pedro Ramoneda and Minhee Lee and Dasaem Jeong and Jose J. Valero-Mas and Xavier Serra},
  doi      = {10.1109/TASLPRO.2025.3539018},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1129-1141},
  title    = {Can audio reveal music performance difficulty? insights from the piano syllabus dataset},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEAR: A cross-modal pre-trained text encoder enhanced by acoustic representations for speech synthesis. <em>TASLPRO</em>, <em>33</em>, 1117-1128. (<a href='https://doi.org/10.1109/TASLPRO.2025.3545274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text encoders play an important role in text-to-speech (TTS) by analyzing text input and converting it into linguistic representations. In order to generate expressive speech from text, pre-training text encoders on large amounts of data has recently become a solution to generate richer and more effective linguistic representations. However, existing pre-trained text encoders only use the self-supervised target on the text data, without considering the relationship between text and speech modalities during the pre-training stage. In this paper, we propose TEAR, a cross-modal pre-trained Text Encoder enhanced by Acoustic Representations for TTS. In addition to conventional text pre-training, TEAR incorporates speech pre-training to extract semantic and prosody-related acoustic representations from speech. Then, TEAR introduces a novel cross-modal pre-training task for the text encoder, termed acoustics-aware joint prediction. This task leverages the acoustic representations generated by the preceding speech pre-training, enabling the linguistic representation to perceive and comprehend prosody during the encoding process. In our implementation, TEAR was pre-trained on 130 million unlabeled Chinese and English sentences, as well as 740,000 Chinese text-speech pairs. The results of the downstream TTS experiments on three expressive TTS datasets indicate that the proposed TEAR can encode more effective and comprehensive linguistic representations compared to the text-only pre-trained encoders, leading to the generation of more natural speech.},
  archive  = {J},
  author   = {Shiming Wang and Yang Ai and Liping Chen and Yajun Hu and Zhenhua Ling},
  doi      = {10.1109/TASLPRO.2025.3545274},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1117-1128},
  title    = {TEAR: A cross-modal pre-trained text encoder enhanced by acoustic representations for speech synthesis},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual echo suppression using dual-stream interactive transformers with selective multi-scale encoding. <em>TASLPRO</em>, <em>33</em>, 1103-1116. (<a href='https://doi.org/10.1109/TASLPRO.2025.3543966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional acoustic echo cancellation (AEC) employs linear adaptive filters to identify the echo path between the speaker and the microphone. Although AEC significantly enhances audio quality in voice communications, residual echo persists due to factors such as inaccurate estimation of echo path induced by nonlinear distortion of the far-end signal. Consequently, a post-suppression module is essential for achieving sufficient echo attenuation. This paper proposes a novel time domain end-to-end method with selective multi-scale encoder and attentional interactive module for nonlinear residual echo suppression (RES) in double-talk scenarios. Specifically, the selective multi-scale encoder can adaptively assign the channel-wise weights to features with multiple time resolutions based on the changing acoustic environment, thereby regulating the feature stream to effectively recover near-end speech. Moreover, the attentional interactive module provides a novel context-aware fusion strategy. Differing from conventional linear fusion operations, such as addition and concatenation employed in existing RES methods, this module dynamically calculates fusion weights for dual signal streams, enabling the neural network to benefit from their correlations. Experimental results demonstrate the superiority of the proposed method.},
  archive  = {J},
  author   = {Kai Xie and Ziye Yang and Jie Chen and Mengyao Zhu},
  doi      = {10.1109/TASLPRO.2025.3543966},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1103-1116},
  title    = {Residual echo suppression using dual-stream interactive transformers with selective multi-scale encoding},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Note-level singing melody transcription for time-aligned musical score generation. <em>TASLPRO</em>, <em>33</em>, 1088-1102. (<a href='https://doi.org/10.1109/TASLPRO.2025.3544064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic music transcription converts audio recordings into symbolic representations, facilitating music analysis, retrieval, and generation. A musical note is characterized by pitch, onset, and offset in an audio domain, whereas it is defined in terms of pitch and note value in a musical score domain. A time-aligned score, derived from timing information along with pitch and note value, allows matching a part of the score with the corresponding part of the music audio, enabling various applications. In this paper, we consider an extended version of the traditional note-level transcription task that recognizes onset, offset, and pitch, through including extraction of additional note value to generate a time-aligned score from an audio input. To address this new challenge, we propose an end-to-end framework that integrates recognition of the note value, pitch, and temporal information. This approach avoids error accumulation inherent in multi-stage methods and enhances accuracy through mutual reinforcement. Our framework employs tokenized representations specifically targeted for this task, through incorporating note value information. Furthermore, we introduce a pseudo-labeling technique to address a scarcity problem of annotated note value data. This technique produces approximate note value labels from existing datasets for the traditional note-level transcription. Experimental results demonstrate the superior performance of the proposed model in note-level transcription tasks when compared to existing state-of-the-art approaches. We also introduce new evaluation metrics that assess both temporal and note value aspects to demonstrate the robustness of the model. Moreover, qualitative assessments via visualized musical scores confirmed the effectiveness of our model in capturing the note values.},
  archive  = {J},
  author   = {Leekyung Kim and Sungwook Jeon and Wan Heo and Jonghun Park},
  doi      = {10.1109/TASLPRO.2025.3544064},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1088-1102},
  title    = {Note-level singing melody transcription for time-aligned musical score generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wireless networked adaptive active noise control with online secondary path modeling. <em>TASLPRO</em>, <em>33</em>, 1078-1087. (<a href='https://doi.org/10.1109/TASLPRO.2025.3544089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Active noise control (ANC) has been developed over the years to mitigate unwanted noise. However, the fixed placement of electroacoustic components often limits ANC performance. To address this constraint, wireless networked ANC (WNANC) has been proposed, incorporating wireless communication among the electroacoustic components, allowing for freedom of movement during ANC operation. WNANC operates with a digital twin architecture, where adaptation of the control filter is managed by a cloud server, resolving the disadvantages of adaptive ANC in algorithm complexity and hardware cost. Nevertheless, the secondary path model remains crucial for WNANC. This paper introduces digital twin online secondary path modeling (DT-OSPM) algorithms for continuous secondary path estimation in WNANC. A key advantage of the DT-OSPM algorithms is their data-reuse capability, which decouples nested adaptive filters into sequential ones. Control filter adaptation occurs after the secondary path model is updated, resulting in improved convergence. Simulation and experiment results demonstrate the effectiveness and efficiency of the DT-OSPM algorithms in a feedforward WNANC system.},
  archive  = {J},
  author   = {Chuang Shi and Hao Yu and Feiyu Du and Mingzhe Li and Le Zhang},
  doi      = {10.1109/TASLPRO.2025.3544089},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1078-1087},
  title    = {Wireless networked adaptive active noise control with online secondary path modeling},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-based distributed multi-frame kalman filtering with speech distortionless constraint for speech enhancement. <em>TASLPRO</em>, <em>33</em>, 1063-1077. (<a href='https://doi.org/10.1109/TASLPRO.2025.3543970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The widespread adoption and interconnection of intelligent devices equipped with microphones has further propelled the development of speech enhancement techniques in wireless acoustic sensor networks (WASNs). To further adapt flexibly to different environments, a diffusion-based distributed multi-frame Kalman filtering method with speech distortionless constraint for speech enhancement is proposed in this paper. Firstly, a multi-frame Kalman filtering method with speech distortionless constraint is proposed to suppress non-stationary noise and reduce speech distortion in each node. Then, to improve the transmission efficiency and alleviate the computational burden, the diffusion strategy is adopted to implement distributed multi-frame Kalman filtering through local communication among nodes to enhance the speech per node collaboratively. Finally, an adaptive diffusion weight estimation method based on the maximum likelihood criterion is proposed to enhance the robustness of the algorithm against abnormal situations. The proposed method can effectively suppress noise and improve speech quality in different noisy scenarios. Furthermore, it requires communication only among the microphones of the local node and neighbor nodes, thereby reducing the communication load. Experimental results validate the feasibility of the proposed method.},
  archive  = {J},
  author   = {Qingying Zhao and Ruijiang Chang and Zhe Chen and Fuliang Yin},
  doi      = {10.1109/TASLPRO.2025.3543970},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1063-1077},
  title    = {Diffusion-based distributed multi-frame kalman filtering with speech distortionless constraint for speech enhancement},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask frequency-time feature-wise linear modulation for piano transcription with pedal. <em>TASLPRO</em>, <em>33</em>, 1049-1062. (<a href='https://doi.org/10.1109/TASLPRO.2025.3544090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The multitask learning method has been successfully applied to piano transcription. For the multitask piano transcription methods, extracting the spectral and temporal information is still essential. In addition, the information interaction across different subtasks is also necessary. In this work, we propose a multitask piano transcription system using the frequency-time feature-wise linear modulation. With conventional spectral and temporal feature extraction operations, a self-conditioning modulation is designed to improve the efficiency of frequency-time information learning in each transcription subtask. The task-cross auxiliary contents are fully utilized through multilevel inter-task conditioning affine transformations. Similar to piano note transcription, the sustain pedal transcription is also conducted based on the multitask frequency-time feature-wise linear modulation model. The experiments demonstrate that the proposed model achieves state-of-the-art performance in piano note transcription on many metrics. Moreover, the model size and complexity are significantly smaller than those of previous state-of-the-art deep learning models.},
  archive  = {J},
  author   = {Qi Wang and Mingkuan Liu and Maoshen Jia},
  doi      = {10.1109/TASLPRO.2025.3544090},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1049-1062},
  title    = {Multitask frequency-time feature-wise linear modulation for piano transcription with pedal},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Head-related transfer function upsampling with spatial extrapolation features. <em>TASLPRO</em>, <em>33</em>, 1034-1048. (<a href='https://doi.org/10.1109/TASLPRO.2025.3544080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Head-related transfer functions (HRTFs) with high spatial resolution play a crucial role in spatial audio rendering. As the direct way to obtain HRTFs, acoustic measurement is usually time-consuming and costly. Therefore, the alternative way is to upsample the low-resolution HRTFs, aiming to increase the spatial sampling density. However, the magnitudes and phases of HRTFs vary rapidly with changes in source positions, leading to increased upsampling errors in the existing methods when there are sparser measurements. To achieve upsampling of HRTFs with lower error, this study proposes a neural network-based model that incorporates a spatial extrapolation feature. This model consists of two components: an encoder that extracts the spatial extrapolation feature from sparse measurements to extrapolate the distribution of magnitudes and interaural time differences (ITDs) at high spatial resolution grids, and a separate network that predicts high-resolution spectra or ITDs from the spatial extrapolation feature. The spatial extrapolation feature in the model represents the relationship between HRTF features at low spatial resolution grids and those at high spatial resolution grids. To validate the preservation of the original input information and assess the limitation of the proposed method, a decoder that reconstructs the input measurements is additionally included. Compared with existing methods, the proposed model concentrates on increasing the sampling density without being distracted by the compression and reconstruction of HRTF features. The results of objective and subjective evaluations confirmed the superior performance of the proposed method with the measured and simulated HRTF datasets over the existing methods.},
  archive  = {J},
  author   = {Jiale Zhao and Dingding Yao and Junfeng Li},
  doi      = {10.1109/TASLPRO.2025.3544080},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1034-1048},
  title    = {Head-related transfer function upsampling with spatial extrapolation features},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully reversing the shoebox image source method: From impulse responses to room parameters. <em>TASLPRO</em>, <em>33</em>, 1023-1033. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an algorithm that fully reverses the shoebox image source method (ISM), a popular and widely used room impulse response (RIR) simulator for cuboid rooms introduced by Allen and Berkley in 1979. More precisely, given a discrete multichannel RIR generated by the shoebox ISM for a microphone array of known geometry, the algorithm reliably recovers the 18 input parameters. These are the 3D source position, the 3 dimensions of the room, the 6-degrees-of-freedom room translation and orientation, and an absorption coefficient for each of the 6 room boundaries. The approach builds on a recently proposed gridless image source localization technique combined with new procedures for room axes recovery and first-order-reflection identification. Extensive simulated experiments reveal that near-exact recovery of all parameters is achieved for a 32-element, 8.4-cm-wide spherical microphone array and a sampling rate of 16 kHz using fully randomized input parameters within rooms of size 2 × 2 × 2 to 10 × 10 × 5 meters. Estimation errors decay towards zero when increasing the array size and sampling rate. The method is also shown to strongly outperform a known baseline, and its ability to extrapolate RIRs at new positions is demonstrated. Crucially, the approach is strictly limited to low-passed discrete RIRs simulated using the vanilla shoebox ISM. Nonetheless, it represents to our knowledge the first algorithmic demonstration that this difficult inverse problem is in-principle fully solvable over a wide range of configurations.},
  archive  = {J},
  author   = {Tom Sprunck and Antoine Deleforge and Yannick Privat and Cédric Foy},
  doi      = {10.1109/TASLPRO.2025.3536841},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1023-1033},
  title    = {Fully reversing the shoebox image source method: From impulse responses to room parameters},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EveMRC: Two-stage bidirectional evidence modeling for multi-choice machine reading comprehension. <em>TASLPRO</em>, <em>33</em>, 1011-1022. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine Reading Comprehension (MRC) requires a machine to answer questions after reading and comprehending the given documents. Multi-choice MRC is one of the most studied MRC tasks due to the convenience of evaluation and the diversity of question types. However, the interpretability of multi-choice MRC, especially in evidence extraction, remains underexplored because a correct answer may be derived by eliminating incorrect options rather than being supported by positive evidence. In this work, we propose a bidirectional evidence modeling framework, EveMRC, to enhance the explainability of Multi-choice MRC systems. Compared to previous works, our framework exclusively addresses the problem of bidirectional evidence selection, which not only selects positive evidence for the right answer but also selects negative evidence for wrong answers. The bidirectional evidence can also facilitate model decisions by incorporating it into a competition process. To avoid the high annotation cost of bidirectional evidence, our framework utilizes a novel weakly-supervised pipeline to train the evidence selector. Experimental results on four multi-choice MRC datasets demonstrate the effectiveness of our framework, which not only enhances the explainability of MRC systems but also improves their overall performance.},
  archive  = {J},
  author   = {Hongshen Xu and Lu Chen and Liangtai Sun and Ruisheng Cao and Da Ma and Kai Yu},
  doi      = {10.1109/TASLPRO.2025.3542294},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1011-1022},
  title    = {EveMRC: Two-stage bidirectional evidence modeling for multi-choice machine reading comprehension},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Should audio front-ends be adaptive? comparing learnable and adaptive front-ends. <em>TASLPRO</em>, <em>33</em>, 998-1010. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hand-crafted features, such as Mel-filterbanks, have traditionally been the choice for many audio processing applications. Recently, there has been a growing interest in learnable front-ends that extract representations directly from the raw audio waveform. However, both hand-crafted filterbanks and current learnable front-ends lead to fixed computation graphs at inference time, failing to dynamically adapt to varying acoustic environments, a key feature of human auditory systems. To this end, we explore the question of whether audio front-ends should be adaptive by comparing the Ada-FE front-end (a recently developed adaptive front-end that employs a neural adaptive feedback controller to dynamically adjust the Q-factors of its spectral decomposition filters) to established learnable front-ends. Specifically, we systematically investigate learnable front-ends and Ada-FE across two commonly used back-end backbones and a wide range of audio benchmarks including speech, sound event, and music. The comprehensive results show that our Ada-FE outperforms advanced learnable front-ends, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.},
  archive  = {J},
  author   = {Qiquan Zhang and Buddhi Wickramasinghe and Eliathamby Ambikairajah and Vidhyasaharan Sethu and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3542281},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {998-1010},
  title    = {Should audio front-ends be adaptive? comparing learnable and adaptive front-ends},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-trained and self-purified data augmentation for RST discourse parsing. <em>TASLPRO</em>, <em>33</em>, 987-997. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Rhetorical structure parsing has faced significant challenges in the past decade because of data scarcity. In order to alleviate this problem, previous research has explored human-engineered features, cross-lingual information, pre-trained language models, etc., and achieved sure success. In this work, we aim to relieve the problem of corpus size limitation through data augmentation (DA). Specifically, we introduce a novel method that combines a top-down parser trained on the small-scale RST-DT corpus and the large-scale Reuters data in a self-training fashion. In particular, we harness an adversarially trained data filter to purify the generated silver data greedily to achieve high-quality data augmentation. It is worth mentioning that the prior rhetorical structure knowledge memorized by the teacher system and the data purifier is not occlusive; it can be continuously updated with the self-training process. We implement the overall learning process end-to-end, which does not depend on any external discourse parser and needs minimal human intervention. Experimental results on RST-DT demonstrate that our RST parser, when enhanced with the proposed DA method, can significantly outperform the baseline systems on both gold standard and automatic EDUs.},
  archive  = {J},
  author   = {Longyin Zhang and Xin Tan and Fang Kong and Guodong Zhou},
  doi      = {10.1109/TASLPRO.2025.3542300},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {987-997},
  title    = {Self-trained and self-purified data augmentation for RST discourse parsing},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaR: Few-shot named entity recognition with meta-learning and relation network. <em>TASLPRO</em>, <em>33</em>, 974-986. (<a href='https://doi.org/10.1109/TASLPRO.2025.3537280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While conventional named entity recognition (NER) has achieved performance close to human ability, few-shot NER research has seen little use. Few-shot NER aims to recognize novel classes in a sentence using only a few labeled samples. Previous related work has mostly implemented entity classification based on methods such as Matching Networks, k-Nearest Neighbors (KNN), and Prototypical Networks; however, these methods cannot discover and represent nonlinear relationships between samples and classes. In response to the above problems, this paper proposes a new few-shot NER meta-learning framework, MetaR, which contains two sub-models, an entity location recognizer, and an entity type classifier. Specifically, we first train the entity location recognizer and then use it to recognize the entity's location, thus accomplishing both entity localization and non-entity filtration. Then, we propose a MAML-Relation network as the entity type classifier to categorize the entity location pieces extracted in the previous stage. By designing a new Relation Network for entity classification and applying MAML for parameter updating, the model can find a good set of initialization parameters, quickly adapt to novel classes, and fully exploit the nonlinear relationship between samples and classes to achieve better classification performance. Extended experiments on several benchmark datasets demonstrate that our few-shot NER framework achieves state-of-the-art performance compared to other methods.},
  archive  = {J},
  author   = {Bingli Sun and Kaiqi Gong and Wenxin Li and Xiao Song},
  doi      = {10.1109/TASLPRO.2025.3537280},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {974-986},
  title    = {MetaR: Few-shot named entity recognition with meta-learning and relation network},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Get the punchline: Headline-centric multi-view learning for personalized headline generation. <em>TASLPRO</em>, <em>33</em>, 962-973. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personalized news headline generation aims to generate a user-specific headline based on their reading histories. Existing approaches incorporate information from various aspects of news articles, which leads to a lack of focus on news headlines and fails to capture accurate information for guiding headline generation. As a result, the generated headlines lack differences across users and fail to reflect user-specific information. To address this problem, we propose a Headline-centric Multi-view Headline Generation Network, denoted as HMvNet. HMvNet is an extraction-integration method that centers around the punchline, i.e. the user-specific information contained in history headlines. HMvNet extracts user-preferred entity tokens and syntactic structures from history headlines through the token-level view and the sequence-level view, respectively. Besides, we propose a reinforcement learning method specifically designed for personalized headline generation to train HMvNet. This method allows the model to fully leverage extracted information and ensures the fluency of the generated headlines. Experimental results demonstrate the superiority of the proposed personalization modeling method as well as the reinforcement learning framework.},
  archive  = {J},
  author   = {Wenya Guo and Shenglong Yu and Ying Zhang and Zhengkun Zhang and Xiaojie Yuan},
  doi      = {10.1109/TASLPRO.2025.3539016},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {962-973},
  title    = {Get the punchline: Headline-centric multi-view learning for personalized headline generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acoustic prompt tuning: Empowering large language models with audition capabilities. <em>TASLPRO</em>, <em>33</em>, 949-961. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as the inputs to the language model. To mitigate data scarcity in the audio domain, a curriculum learning strategy is proposed by formulating diverse audio tasks in a sequential manner. Moreover, we improve the audio language model by using interleaved audio-text embeddings as the input sequence. In this improved model, zero constraints are imposed on the input format, thus it is capable of tackling diverse modelling tasks, such as few-shot audio classification and audio comparison. To further evaluate the advanced ability of the audio networks, we introduce natural language audio reasoning (NLAR), a new task that analyses two audio clips by comparison and summarisation. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the target datasets) across various tasks. We finally demonstrate APT's ability in extending frozen VLMs to the audio domain without fine-tuning, achieving promising results in audio-visual question and answering.},
  archive  = {J},
  author   = {Jinhua Liang and Xubo Liu and Wenwu Wang and Mark D. Plumbley and Huy Phan and Emmanouil Benetos},
  doi      = {10.1109/TASLPRO.2025.3533375},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {949-961},
  title    = {Acoustic prompt tuning: Empowering large language models with audition capabilities},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal knowledge distillation with multi-stage adaptive feature fusion for speech separation. <em>TASLPRO</em>, <em>33</em>, 935-948. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although audio-visual speech separation has achieved significant advancements, it is relatively difficult to obtain audio and visual modalities simultaneously in real scenarios, often leading to the issue of missing visual modality. Actually, during the training phase of the speech separation network, the developed audio and video datasets can be fully utilized to obtain an effectual audio-visual speech separation. However, enhancing the performance of unimodal models during testing using multimodal approaches poses a challenge. To address the problem, this paper proposes a cross-modal knowledge distillation method for speech separation, which leverages a multimodal model to enhance the unimodal model via knowledge distillation. Specifically, during the training phase, a pre-trained audio-visual network is used as the teacher and the audio-only network is used as the student. Then the teacher with the additional visual input transfers knowledge to the student to enhance the performance of the student model. During the test phase, the audio-only network only conducts speech separation. In addition, to further improve the performance of the teacher model, a multi-stage adaptive feature fusion method is proposed. The global and local perspectives are used to effectively capture deep audio-visual correlations. We have conducted extensive experiments with audio-visual datasets LRS2, LRS3, and VoxCeleb2. Experimental results demonstrate that our proposed method effectively improves the student model by 15%$\sim$25% relative improvements over the baseline in terms of SI-SNRi and SDRi.},
  archive  = {J},
  author   = {Cunhang Fan and Wang Xiang and Jianhua Tao and Jiangyan Yi and Zhao Lv},
  doi      = {10.1109/TASLPRO.2025.3533359},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {935-948},
  title    = {Cross-modal knowledge distillation with multi-stage adaptive feature fusion for speech separation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized and controllable voice style transfer with speech diffusion transformer. <em>TASLPRO</em>, <em>33</em>, 922-934. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although speech synthesis systems have remarkably advanced with their expansion into various applications, achieving robust voice style transfer while maintaining high-quality in zero-shot scenarios still remains challenging. In this paper, we propose a neural speech synthesis system with speech diffusion transformer (SDT) to effectively perform style transfer even in low-resource and zero-shot scenarios. We introduce a diffusion-based voice conversion network with strong style adaptation performance. We explore a transformer-based diffusion backbone network and style conditioning method that can simultaneously capture spatial and temporal information of acoustic characteristics for more efficient and robust speaker adaptation. This significantly improves the style adaptation performance compared to existing diffusion-based speech synthesis systems. Additionally, we propose a flexible neural pitch control method for personalized voice style transfer. Particularly, the styles between the source and target speakers in a voice conversion scenario can be flexibly adjusted to synthesize a personalized speech style appropriate for the application. Our experimental results demonstrate that the proposed method significantly improves the style transfer performance and pronunciation intelligibility while exhibiting, superior performance on low-resource and real-world data. Moreover, the proposed SDT can be easily extended to personalized speech synthesis tasks, such as voice conversion and text-to-speech, without re-training the model for each task.},
  archive  = {J},
  author   = {Ha-Yeong Choi and Sang-Hoon Lee and Seong-Whan Lee},
  doi      = {10.1109/TASLPRO.2025.3533362},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {922-934},
  title    = {Personalized and controllable voice style transfer with speech diffusion transformer},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EHealth: A chinese biomedical language model built via multi-level text discrimination. <em>TASLPRO</em>, <em>33</em>, 908-921. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on domain adaptation and focused mainly on English. This work introduces eHealth, a compact, encoder-based Chinese biomedical PLM that can be fine-tuned in a customized manner to effectively handle various Chinese biomedical language understanding tasks. Rather than relying on domain adaptation, eHealth is built from scratch using a novel pre-training framework. This framework trains eHealth as a discriminator through token- and sequence-level discrimination. Token-level discrimination detects corrupted input tokens and recovers their original forms from plausible candidates, while sequence-level discrimination further distinguishes corruptions of the same original sequence from those of others. As a result, eHealth can learn language semantics at both token and sequence levels. Moreover, eHealth is trained entirely from scratch with a newly constructed in-domain vocabulary, which may help improve tokenization and understanding of biomedical text. Extensive experiments on 11 Chinese biomedical language understanding tasks of various forms verify the effectiveness and superiority of eHealth. The pre-trained model as well as the code have been released to the public.},
  archive  = {J},
  author   = {Quan Wang and Songtai Dai and Benfeng Xu and Yajuan Lyu and Hua Wu and Haifeng Wang},
  doi      = {10.1109/TASLPRO.2025.3536177},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {908-921},
  title    = {EHealth: A chinese biomedical language model built via multi-level text discrimination},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-step attention-based feature combination cross-attention system for speech-based dementia detection. <em>TASLPRO</em>, <em>33</em>, 896-907. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dementia poses a significant global challenge, with profound personal, societal, and economic impacts. Although it is incurable, early detection is crucial for ensuring appropriate care and support. Dementia can impair a person's speech and language abilities, and studies have demonstrated promising results in using spoken language for automatic dementia detection. Recently, deep learning-based self-supervised learning models, such as wav2vec2.0 (w2v) and BERT, have shown success in extracting acoustic and linguistic information. However, most studies have relied on single datasets and relatively straightforward methods for extracting and combining acoustic and linguistic modalities. This paper presents an in-depth exploration of the application of SSL models in this context by proposing the Two-Step Attention-based Feature Combination Cross-attention system (TSAC-ATT) for speech-based dementia detection. The contributions of this paper are as follows: i) we explore and analyse acoustic and linguistic feature extraction pipelines using SSL models, including the proposed TSAC framework to create high-performing acoustic features from w2v's contextual layers; ii) we demonstrate that these features, when fused using cross-attention, outperform various feature combination approaches; iii) all experimental work is conducted on two publicly available datasets (DementiaBank and ADReSS), as well as the IVA dataset collected by the Royal Hallamshire Hospital, which includes recordings of the standard Cookie Theft task. We present state-of-the-art results, highlighting that acoustic-only features based on the w2v model can achieve very high performance across multiple datasets. Furthermore, we show that the upstream performance of the automatic speech recognition module does not always predict downstream classification performance.},
  archive  = {J},
  author   = {Yilin Pan and Bahman Mirheidari and Daniel Blackburn and Heidi Christensen},
  doi      = {10.1109/TASLPRO.2025.3533363},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {896-907},
  title    = {A two-step attention-based feature combination cross-attention system for speech-based dementia detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating subgroup disparities in speech models: A divergence-aware dual strategy. <em>TASLPRO</em>, <em>33</em>, 883-895. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech models may exhibit disparities in performance across different population subgroups. Prior mitigation efforts often rely on the manual user-driven selection of predefined data subgroups of interest. However, they fail to correctly identify all relevant subgroups associated with performance issues. We propose to mitigate performance disparities of subgroups that underperform, i.e., exhibit a divergence, relative to overall model performance. We tackle the performance disparities from two alternative perspectives - an in-processing one, implementing mitigation measures during model development, and a post-processing one, refining already trained models. For the in-processing scenario, we propose two approaches: a divergence-based regularization and a data augmentation technique to boost subgroup performance during model fine-tuning. The post-processing strategy introduces a divergence-aware data acquisition method to prioritize acquiring samples from underperforming subgroups. Experiments on a dataset for Automatic Speech Recognition, one for Emotion Recognition, and two datasets for Intent Classification in English and Italian highlight the improvement achieved by the divergence-aware strategies, which significantly reduce performance disparities and outperform traditional clustering-, KNN-, error-driven-, and random-based methods.},
  archive  = {J},
  author   = {Alkis Koudounas and Eliana Pastor and Luca de Alfaro and Elena Baralis},
  doi      = {10.1109/TASLPRO.2025.3539429},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {883-895},
  title    = {Mitigating subgroup disparities in speech models: A divergence-aware dual strategy},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to microphone array geometry transform. <em>TASLPRO</em>, <em>33</em>, 869-882. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Microphone array signal processing is significantly influenced by array parameters such as the number of sensors, array aperture, and array topology. These factors affect both the development of algorithms and their performance limitations. Consequently, applying algorithms designed for one array geometry to signals captured by different geometries can be challenging. To address this issue, we introduce a novel method referred to as the microphone array geometry transform, or simply the microphone array transform. This approach involves two key steps: 1) encoding the array observations in the frequency and angle domains, and 2) generating observations for the target array using array manifold vectors and the frequency-angle domain source signals. We formulate this problem as one of convex optimization and solve it using the well-established alternating direction method of multipliers (ADMM). Simulation results justify the effectiveness of the proposed method.},
  archive  = {J},
  author   = {Chao Pan and Jingdong Chen and Jacob Benesty},
  doi      = {10.1109/TASLPRO.2025.3539025},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {869-882},
  title    = {An approach to microphone array geometry transform},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for trigonometric beamformers: Theoretical analysis, efficient design and implementation. <em>TASLPRO</em>, <em>33</em>, 855-868. (<a href='https://doi.org/10.1109/TASLPRO.2025.3539029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Polynomial beamformers for microphone arrays have gained interest thanks to their efficient and flexible beam-steering capability. Recent findings indicate that polynomial beamformers for circular arrays can experience significant performance degradation due to the Runge's phenomenon. To address this issue, trigonometric beamformers that use trigonometric interpolation instead of polynomial interpolation have been proposed. However, the trigonometric beamformers require a large beamforming filter length to ensure their performance, which leads to high design and implementation complexity. In this paper, we propose a general framework for the design and implementation of trigonometric beamformers using the Laguerre filters, which include the existing trigonometric beamformers as a special case. Under the general framework, the performance of the trigonometric beamformers can be greatly improved for short beamforming filter length by exploiting the degrees of freedom of the Laguerre filter poles (LFPs). We theoretically derive the properties of the beamformer coefficients and LFPs of the generalized trigonometric beamformers. Based on the derived properties, we then develop efficient design approach and implementation structure for the generalized trigonometric beamformers, which significantly reduces the design and implementation complexity. Simulation results have demonstrated the superior performance of the generalized trigonometric beamformers and the high efficiency of the proposed design and implementation schemes.},
  archive  = {J},
  author   = {Zhan Zhang and Huawei Chen and Congwei Feng and Susanto Rahardja},
  doi      = {10.1109/TASLPRO.2025.3539029},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {855-868},
  title    = {A general framework for trigonometric beamformers: Theoretical analysis, efficient design and implementation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffuSyn: A diffusion-driven framework with syntactic dependency for aspect sentiment triplet extraction. <em>TASLPRO</em>, <em>33</em>, 842-854. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect Sentiment Triplet Extraction (ASTE) is a fine-grained sentiment analysis task that involves identifying aspect and opinion terms and conducting sentiment analysis for each aspect-opinion pair. We categorize existing methods into tagging-based, span-based, and generation methods. However, despite notable achievements, these methods still face certain challenges: span-based methods struggle to distinguish similar spans, and generation methods are prone to slower decoding speeds. To address these challenges, we propose a novel ASTE framework, called DiffuSyn, which leverages diffusion models and syntactic dependency parsing to improve the performance of sentiment analysis. Specifically, we first add Gaussian noise to the indices at the beginning and end of aspect words and opinion words in sentences based on a non-autoregressive diffusion model. We obtain accurate boundaries of aspect and opinion terms through boundary denoising, and thus identify precise spans. Secondly, we introduce a syntactic dependency parser to capture the syntactic dependencies within review sentences, thereby providing effective syntactic information for sentiment analysis. Finally, we complete the matching of aspects and opinions by leveraging the extracted boundaries and syntactic semantic information, facilitating the prediction of sentiment relationships. We conduct experiments on four public datasets (ASTE-Data-V2), and the results indicate the effectiveness of our approach in the ASTE task. Furthermore, our method achieves state-of-the-art performance in both Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks.},
  archive  = {J},
  author   = {Qiuhua Yi and Xiangjie Kong and Linan Zhu and Chenwei Zhang and Guojiang Shen},
  doi      = {10.1109/TASLPRO.2025.3536179},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {842-854},
  title    = {DiffuSyn: A diffusion-driven framework with syntactic dependency for aspect sentiment triplet extraction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hearing-loss compensation using deep neural networks: A framework and results from a listening test. <em>TASLPRO</em>, <em>33</em>, 828-841. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article investigates the use of deep neural networks (DNNs) for hearing-loss compensation. Hearing loss is a prevalent issue affecting millions of people worldwide, and conventional hearing aids have limitations in providing satisfactory compensation. DNNs have shown remarkable performance in various auditory tasks, including speech recognition, speaker identification, and music classification. In this study, we propose a DNN-based approach for hearing-loss compensation, which is trained on the outputs of hearing-impaired and normal-hearing DNN-based auditory models in response to speech signals. First, we introduce a framework for emulating auditory models using DNNs, focusing on an auditory-nerve model in the auditory pathway. We propose a linearization of the DNN-based approach, which we use to analyze the DNN-based hearing-loss compensation. Additionally we develop a simple approach to choose the acoustic center frequencies of the auditory model used for the compensation strategy. Finally, we evaluate, to our knowledge for the first time, the DNN-based hearing-loss compensation strategies using listening tests with hearing impaired listeners. The results demonstrate that the proposed approach results in feasible hearing-loss compensation strategies. Our proposed approach was shown to provide an increase in speech intelligibility versus an unprocessed baseline and was found to outperform a conventional approach in terms of both intelligibility and preference.},
  archive  = {J},
  author   = {Peter Leer and Jesper Jensen and Laurel H. Carney and Zheng-Hua Tan and Jan Østergaard and Lars Bramsløw},
  doi      = {10.1109/TASLPRO.2025.3536183},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {828-841},
  title    = {Hearing-loss compensation using deep neural networks: A framework and results from a listening test},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise robust cross-speaker emotion transfer in TTS through knowledge distillation and orthogonal constraint. <em>TASLPRO</em>, <em>33</em>, 812-827. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {By cross-speaker emotion transfer (CSEF) in text-to-speech (TTS) synthesis, we synthesize speech for a target speaker with the emotion transferred from reference speech by another (source) speaker. Traditionally CSEF is achieved by decoupling speaker and emotion components in speech, that highly depends on the availability of clean reference speech. To address the above issue, we propose a novel Noise-robust Cross-Speaker Emotion Transfer TTS model, i.e. NCE-TTS. NCE-TTS integrates the noise-robust emotion information extraction and noise-robust speaker-emotion disentanglement into a unified framework with two new modules, namely knowledge distillation and orthogonal constraint. The knowledge distillation aims to directly learn the emotion features of clean speech, from noisy speech, with a conditional diffusion model. The orthogonal constraint seeks to disentangle the deep emotion embedding and speaker embedding and further enhance the emotion-discriminative ability. Unlike the traditional cascaded approach of first denoising and then extracting features, we have built a new training framework that achieves better emotion transfer results in noisy scenarios. We conducted extensive experiments on a multi-speaker English emotional speech dataset ESD. The objective and subjective results demonstrate that the proposed NCE-TTS can synthesize emotionally rich speech while preserving the target speaker's voice in various noisy scenarios, with a significant improvement compared to all advanced baselines.},
  archive  = {J},
  author   = {Rui Liu and Kailin Liang and De Hu and Tao Li and Dongchao Yang and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3533361},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {812-827},
  title    = {Noise robust cross-speaker emotion transfer in TTS through knowledge distillation and orthogonal constraint},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual target speaker extraction with selective auditory attention. <em>TASLPRO</em>, <em>33</em>, 797-811. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio-visual target speaker extraction (AV-TSE) aims to extract the specific person's speech from the audio mixture given auxiliary visual cues. Previous methods usually search for the target voice through speech-lip synchronization. However, this strategy mainly focuseson the existence of target speech, while ignoring the variations of the noise characteristics, i.e., interference speaker and the background noise. That may result in extracting noisy signals from the incorrect sound source in challenging acoustic situations. To this end, we propose a novel selective auditory attention mechanism, which can suppress interference speakers and non-speech signals to avoid incorrect speaker extraction. By estimating and utilizing the undesired noisy signal through this mechanism, we design an AV-TSE framework named Subtraction-and-ExtrAction network (SEANet) to suppress the noisy signals. We conduct abundant experiments by re-implementing three popular AV-TSE methods as the baselines and involving nine metrics for evaluation. The experimental results show that our proposed SEANet achieves state-of-the-art results and performs well for all five datasets.},
  archive  = {J},
  author   = {Ruijie Tao and Xinyuan Qian and Yidi Jiang and Junjie Li and Jiadong Wang and Haizhou Li},
  doi      = {10.1109/TASLPRO.2025.3527766},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {797-811},
  title    = {Audio-visual target speaker extraction with selective auditory attention},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reshaping word embedding space with monolingual synonyms for bilingual lexicon induction. <em>TASLPRO</em>, <em>33</em>, 785-796. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent research on Bilingual Lexicon Induction (BLI) involves mapping monolingual word embeddings (WEs) into a shared space and obtaining word translations by retrieving the nearest cross-lingual neighbors. However, we observe that word vectors of some synonyms in the monolingual embedding space are located far apart, resulting in a reduced isomorphism between two monolingual WE spaces, which brings greater challenges in the WE spaces' mapping. To address this issue, we propose a novel method for enhancing BLI with synonyms. Specifically, we first build a monolingual synonym lexicon, which contains synonym pairs obtained from Wordnet. We then generate pseudo translation pairs that include synonyms of the original seed lexicon. Finally, the BLI model is jointly trained on the original and pseudo translation pairs, and the WE space is reshaped by bringing monolingual synonyms closer and aligning them with their cross-lingual translations simultaneously. The extensive experiments show that our method significantly outperforms the strong baseline models in word translation.},
  archive  = {J},
  author   = {Qiuyu Ding and Hailong Cao and Conghui Zhu and Tiejun Zhao},
  doi      = {10.1109/TASLPRO.2025.3536174},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {785-796},
  title    = {Reshaping word embedding space with monolingual synonyms for bilingual lexicon induction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multitask causality-inspired feature enhancement method for stance detection. <em>TASLPRO</em>, <em>33</em>, 773-784. (<a href='https://doi.org/10.1109/TASLPRO.2025.3536168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The existing stance detection methods have several limitations. (1) They utilize additional information such as sentiment or linguistic features to construct multitask frameworks for achieving performance bottleneck breakthroughs and only use the last encoder layers of their language models for semantic encoding. (2) They may establish spurious correlations between input and labels based on statistical dependence, causing text-target representations to contain more biased stance-unrelated noncausal features, which act as shortcuts for stance prediction without considering causal mechanisms, leading to performance bottlenecks. In this paper, we propose a novel multitask causality-inspired feature enhancement (MTCIFE) method for stance detection, achieving a performance breakthrough without additional information. MTCIFE introduces two stance detection tasks: an auxiliary stance detection task (ASDT) and a main stance detection task (MSDT), constructing the relation between text and a target in different ways by using different transformer encoder layers of bidirectional encoder representations from transformers (BERT); this augments the diversity of the obtained representations and makes them learn from each other. Then, we decouple stance-related causal features from stance-unrelated noncausal features and encourage their independence in both tasks. Considering the underlying causal mechanisms, we propose a causality-inspired feature enhancement (CIFE) module for implementing causal learning and intervention at the feature level. By integrating the CIFE module into both tasks via multitask learning, we aim to decouple the learning of causal and noncausal features, improving causal feature acquisition and mitigating the confounding effect of noncausal features. Extensive experiments demonstrate the outstanding performance of our model over other state-of-the-art approaches.},
  archive  = {J},
  author   = {Jian Xu and Bo Liu and Yanshan Xiao},
  doi      = {10.1109/TASLPRO.2025.3536168},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {773-784},
  title    = {A multitask causality-inspired feature enhancement method for stance detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural optimisation of fixed beamformers with flexible geometric constraints. <em>TASLPRO</em>, <em>33</em>, 759-772. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a novel approach to optimising fixed broadband beamformers using neural networks. The proposed neural network model for fixed beamformers allows for the optimisation of spatial filters while incorporating flexible geometric constraints. We propose a framework for the unified signal model applicable to all geometric settings and employ two heterogeneous neural networks to simultaneously optimise both the geometry and spatial filter of fixed beamforming. Furthermore, we introduce a technique called constrained naked neurons for the optimisation of spatial filters. Experimental results show that our approaches outperform conventional approaches in terms of Directivity Factor (DF) and White Noise Gain (WNG). Our study reveals the competitive performance of a circular microphone array that matches the capabilities of a concentric circular microphone array with the same number of microphones. We also validate the effectiveness of our model in a circular discal setting, where microphones can be placed arbitrarily. Given the same parameter settings, a circular discal array can be significantly better than a linear array.},
  archive  = {J},
  author   = {Longfei Felix Yan and Weilong Huang and Thushara D. Abhayapala and Jinwei Feng and W. Bastiaan Kleijn},
  doi      = {10.1109/TASLPRO.2025.3533372},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {759-772},
  title    = {Neural optimisation of fixed beamformers with flexible geometric constraints},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised depression detection in speech through self-learning based label correction. <em>TASLPRO</em>, <em>33</em>, 748-758. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automated Depression Detection (ADD) in speech aims to automatically estimate one's depressive attributes through artificial intelligence tools towards spoken signals. Nevertheless, existing speech-based ADD works fail to sufficiently consider weakly-supervised cases with inaccurate labels, which may typically appear in intelligent mental health. In this regard, we propose the Self-Learning-based Label Correction (SLLC) approach for weakly-supervised depression detection in speech. The proposed approach employs a self-learning manner connecting a label correction module and a depression detection module. Within the approach, the label correction module fuses likelihood-ratio-based and prototype-based label correction strategies in order to effectively correct the inaccurate labels, while the depression detection module aims at detecting depressed samples through a 1D convolutional recurrent neural network with multiple types of losses. The experimental results on two depression detection corpora show that our proposed SLLC approach performs better compared with existing state-of-the-art speech-based depression detection approaches, in the case of weak supervision with inaccurate labels for depression detection in speech.},
  archive  = {J},
  author   = {Yanfei Sun and Yuanyuan Zhou and Xinzhou Xu and Jin Qi and Feiyi Xu and Zhao Ren and Björn W. Schuller},
  doi      = {10.1109/TASLPRO.2025.3533370},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {748-758},
  title    = {Weakly-supervised depression detection in speech through self-learning based label correction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wideband relative transfer function (RTF) estimation exploiting frequency correlations. <em>TASLPRO</em>, <em>33</em>, 731-747. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article focuses on estimating relative transfer functions (RTFs) for beamforming applications. Traditional methods often assume that spectra are uncorrelated, an assumption that is often violated in practical scenarios due to factors such as time-domain windowing or the non-stationary nature of signals, as observed in speech. To overcome these limitations, we propose an RTF estimation technique that leverages spectral and spatial correlations through subspace analysis. Additionally, we derive Cramér–Rao bounds (CRBs) for the RTF estimation task, providing theoretical insights into the achievable estimation accuracy. These bounds reveal that channel estimation can be performed more accurately if the noise or the target signal exhibits spectral correlations. Experiments with both real and synthetic data show that our technique outperforms the narrowband maximum-likelihood estimator, known as covariance whitening (CW), when the target exhibits spectral correlations. Although the proposed algorithm generally achieves accuracy close to the theoretical bound, there is potential for further improvement, especially in scenarios with highly spectrally correlated noise. While channel estimation has various applications, we demonstrate the method using a minimum variance distortionless (MVDR) beamformer for multichannel speech enhancement. A free Python implementation is also provided.},
  archive  = {J},
  author   = {Giovanni Bologni and Richard C. Hendriks and Richard Heusdens},
  doi      = {10.1109/TASLPRO.2025.3533371},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {731-747},
  title    = {Wideband relative transfer function (RTF) estimation exploiting frequency correlations},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayesian adaptive learning of deep latent variables for acoustic knowledge transfer. <em>TASLPRO</em>, <em>33</em>, 719-730. (<a href='https://doi.org/10.1109/TASLPRO.2025.3530321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we propose a novel variational Bayesian adaptive learning approach for cross-domain knowledge transfer to address acoustic mismatches between training and testing conditions, such as recording devices and environmental noise. Different from the traditional Bayesian approaches that impose uncertainties on model parameters risking the curse of dimensionality due to the huge number of parameters, we focus on estimating a manageable number of latent variables in deep neural models. Knowledge learned from a source domain is thus encoded in prior distributions of deep latent variables and optimally combined, in a Bayesian sense, with a small set of adaptation data from a target domain to approximate the corresponding posterior distributions. Two different strategies are proposed and investigated to estimate the posterior distributions: Gaussian mean-field variational inference, and empirical Bayes. These strategies address the presence or absence of parallel data in the source and target domains. Furthermore, structural relationship modeling is investigated to enhance the approximation. We evaluated our proposed approaches on two acoustic adaptation tasks: 1) device adaptation for acoustic scene classification, and 2) noise adaptation for spoken command recognition. Experimental results show that the proposed variational Bayesian adaptive learning approach can obtain good improvements on target domain data, and consistently outperforms state-of-the-art knowledge transfer methods.},
  archive  = {J},
  author   = {Hu Hu and Sabato Marco Siniscalchi and Chao-Han Huck Yang and Chin-Hui Lee},
  doi      = {10.1109/TASLPRO.2025.3530321},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {719-730},
  title    = {Variational bayesian adaptive learning of deep latent variables for acoustic knowledge transfer},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural codec language models are zero-shot text to speech synthesizers. <em>TASLPRO</em>, <em>33</em>, 705-718. (<a href='https://doi.org/10.1109/TASLPRO.2025.3530270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 50 k hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capability and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment from the prompt in synthesis.},
  archive  = {J},
  author   = {Sanyuan Chen and Chengyi Wang and Yu Wu and Ziqiang Zhang and Long Zhou and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},
  doi      = {10.1109/TASLPRO.2025.3530270},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {705-718},
  title    = {Neural codec language models are zero-shot text to speech synthesizers},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TTSlow: Slow down text-to-speech with efficiency robustness evaluations. <em>TASLPRO</em>, <em>33</em>, 693-704. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text-to-speech (TTS) has been extensively studied for generating high-quality speech with textual inputs, playing a crucial role in various real-time applications. For real-world deployment, ensuring stable and timely generation in TTS models against minor input perturbations is of paramount importance. Therefore, evaluating the robustness of TTS models against such perturbations, commonly known as adversarial attacks, is highly desirable. In this paper, we propose TTSlow, a novel adversarial approach specifically tailored to slow down the speech generation process in TTS systems. To induce long TTS waiting time, we design novel efficiency-oriented adversarial loss to encourage endless generation process. TTSlow encompasses two attack strategies targeting both text inputs and speaker embedding. Specifically, we propose TTSlow-text, which utilizes a combination of homoglyphs-based and swap-based perturbations, along with TTSlow-spk, which employs a gradient optimization attack approach for speaker embedding. TTSlow serves as the first attack approach targeting a wide range of TTS models, including autoregressive and non-autoregressive TTS ones, thereby advancing exploration in audio security. Extensive experiments are conducted to evaluate the inference efficiency of TTS models, and in-depth analysis of generated speech intelligibility is performed using Gemini. The results demonstrate that TTSlow can effectively slow down two TTS models across three publicly available datasets.},
  archive  = {J},
  author   = {Xiaoxue Gao and Yiming Chen and Xianghu Yue and Yu Tsao and Nancy F. Chen},
  doi      = {10.1109/TASLPRO.2025.3533357},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {693-704},
  title    = {TTSlow: Slow down text-to-speech with efficiency robustness evaluations},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimicrophone signal parameter estimation in a multi-source noisy reverberant scenario. <em>TASLPRO</em>, <em>33</em>, 678-692. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Estimation of acoustic parameters is of great interest but very challenging in the multichannel microphone signal processing area. Existing methods either assume simple, but less realistic scenarios, or suffer from very high computational costs. In this work, we consider the more general scenario where multiple sources, late reverberation and noise exist concurrently. The parameters of interest include the relative transfer functions (RTFs) of the point sources (both target and interferers) and individual power spectral densities (PSDs) of the sources and the late reverberation. We first propose a robust late reverberation PSD estimator using an iterative compensation scheme. Then, based on an analysis of the variance of the sample covariance matrices, we propose a robust and joint estimator for the sources RTFs and PSDs using multiple time frames that share the same RTFs. We compare the proposed method with the state-of-the-art simultaneously confirmatory factor analysis (SCFA) method and the second order blind identification (SOBI) method. Experiments show that our proposed method reaches the estimation performance of SCFA, which significantly outperforms SOBI, but uses much less computational costs compared to SCFA.},
  archive  = {J},
  author   = {Changheng Li and Richard C. Hendriks},
  doi      = {10.1109/TASLPRO.2025.3533374},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {678-692},
  title    = {Multimicrophone signal parameter estimation in a multi-source noisy reverberant scenario},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VM-ASR: A lightweight dual-stream U-net model for efficient audio super-resolution. <em>TASLPRO</em>, <em>33</em>, 666-677. (<a href='https://doi.org/10.1109/TASLPRO.2025.3533365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio super-resolution (ASR), also known as bandwidth extension (BWE), aims to enhance the quality of low-resolution audio by recovering high-frequency components. However, existing methods often struggle to model harmonic relationships accurately and balance the inference speed and computational complexity. In this paper, we propose VM-ASR, a novel lightweight ASR model that leverages the Visual State Space (VSS) block to effectively capture global and local contextual information within audio spectrograms. This enables VM-ASR to model harmonic relationships more accurately, improving audio quality. Our experiments on the VCTK dataset demonstrate that VM-ASR consistently outperforms state-of-the-art methods in spectral reconstruction across various input-output sample rate pairs, achieving significantly lower Log-Spectral Distance (LSD) while maintaining a smaller model size (3.01 M parameters) and lower computational complexity (2.98 GFLOPS). This makes VM-ASR not only a promising solution for real-time applications and resource-constrained environments but also opens up exciting possibilities in telecommunications, speech synthesis, and audio restoration.},
  archive  = {J},
  author   = {Ting-Wei Zhang and Shanq-Jang Ruan},
  doi      = {10.1109/TASLPRO.2025.3533365},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {666-677},
  title    = {VM-ASR: A lightweight dual-stream U-net model for efficient audio super-resolution},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validity and robustness of denoisers: A proof of concept in speech denoising. <em>TASLPRO</em>, <em>33</em>, 650-665. (<a href='https://doi.org/10.1109/TASLP.2024.3507561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We can use randomized smoothing to assess and certify the robustness of Machine Learning models. It uses a norm ball around inputs to check if the model's output remains consistent. Adding a smoothing process, like a signal denoiser, can enhance the model's robustness against adversarial attacks. However, the robustness of the smoothing process itself is an open question, especially as denoisers are getting more complex. This study proposes a method to validate and certify the robustness of signal denoisers. Using a baseline and a user-defined noise model, we synthesize a significant noise pattern unseen by the denoiser. This pattern forms two balls around the baseline and the denoised signal, whose volume ratio quantifies the model's robustness. We applied this framework to discriminative and generative models for speech denoising, demonstrating its practicality. Our evaluations of the Demucs architecture, RNNoise, Resemble+Enhance, and VoiceFixer show that we can identify robust denoisers under different conditions and gain insights into the workings of black-box denoisers. This methodology offers a comprehensive metric for model comparison.},
  archive  = {J},
  author   = {Masoud Ebrahimi and Qamar Alfalouji and Mina Basirat},
  doi      = {10.1109/TASLP.2024.3507561},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {650-665},
  title    = {Validity and robustness of denoisers: A proof of concept in speech denoising},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-channel end-to-end network based on dynamic corpus of knowledge graph for intelligent recognition of traditional chinese medicine terminology. <em>TASLPRO</em>, <em>33</em>, 640-649. (<a href='https://doi.org/10.1109/TASLP.2024.3507574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate analysis of Traditional Chinese Medicine (TCM) terminology is pivotal in facilitating effective communication between TCM practitioners and patients, thereby enabling precise diagnosis and treatment. TCM terminology includes two forms: speech and text. However, current deep learning methods for TCM terminology recognition are hindered by insufficient corpus and defects of the end-to-end learning framework, which leads to the low accuracy of TCM terminology recognition. To solve the above problems, this paper first combines the information of text and picture of TCM terminology and proposes an extended model of TCM terminology corpus. Joint optimization of text and picture-based knowledge graph TCM terminology corpus expansion model is incorporated, and the traditional corpus is supplemented by incrementally constructing a dynamic TCM terminology corpus. Secondly, the text-speech end-to-end conversion mechanism is used to realize the synchronous incremental expansion of the TCM dynamic speech corpus. After that, the TCM dynamic speech corpus is deeply trained through a unified streaming and non-streaming two-pass end-to-end model, to realize the accurate recognition of the speech of TCM terminology. Additionally, to mitigate the readability challenges posed by redundant words in TCM pronunciation, a directed acyclic graph (DAG) and dynamic programming (DP) based framework is proposed for redundant word detection. The results show that the accuracy of the speech recognition algorithm proposed in this paper for the speech of TCM terminology is increased by 12.92%, 11.18%, and 19.95% compared with three public speech recognition engines of Iflytek, Aliyun, and Baidu, respectively. In addition, the detection accuracy of redundant words reaches 90.67% on average. This method is able to overcome the limitations of TCM terminology recognition. It provides a comprehensive solution with significantly higher recognition accuracy compared to existing methods.},
  archive  = {J},
  author   = {Wu Yulu and Wang Kun and Liu Xiufeng},
  doi      = {10.1109/TASLP.2024.3507574},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {640-649},
  title    = {A two-channel end-to-end network based on dynamic corpus of knowledge graph for intelligent recognition of traditional chinese medicine terminology},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch guidance encoder for robust acoustic echo suppression. <em>TASLPRO</em>, <em>33</em>, 627-639. (<a href='https://doi.org/10.1109/TASLP.2024.3519876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For efficient real-time speech communication, it is crucial to design a robust acoustic echo cancellation (AEC) system that can effectively mitigate echo signals, which degrade speech quality during conversations. However, many existing AEC algorithms struggle to balance the trade-off between effective echo suppression and preserving the quality of near-end speech across diverse scenarios. To address these challenges, we propose a dual-branch guidance (DBG) encoder within a neural AEC network, specifically designed to enhance the capture of echo components. Inspired by the relationship between input signals, our approach employs a guidance map to generate a latent mask that highlights echo-related regions. By partially computing a separative latent mask in the latent domain of the microphone feature, this method effectively discriminates echo components while accounting for the presence of near-end speech components, ultimately guiding the network in estimating the final mask, which suppresses the echo. Additionally, we introduce the far-end speech processing and state learning modules to generate reliable guidance maps, thereby enhancing adaptability across various scenarios and distortions, including time-variant delays. Experimental results under various environmental distortions demonstrate that the AEC module of our proposed encoder effectively manages trade-offs, achieving state-of-the-art AEC performance while operating in real-time. Samples of our method are available for listening on the demo page.},
  archive  = {J},
  author   = {Hyewon Han and Xiulian Peng and Doyeon Kim and Yan Lu and Hong-Goo Kang},
  doi      = {10.1109/TASLP.2024.3519876},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {627-639},
  title    = {Dual-branch guidance encoder for robust acoustic echo suppression},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiVoD: Pitch, volume and duration invariant representation for music tone quality evaluation. <em>TASLPRO</em>, <em>33</em>, 613-626. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Tone quality is of pivotal importance in the auditory perception of musical performance. Depending on the performer and the instrument, tone quality evaluation is subjective and time-consuming, with inherent difficulties stemming from the absence of precise measurement methods. In this study, we develop a novel method for tone quality evaluation utilizing an adversarial domain-invariant learning strategy to construct a representation invariant to changes in pitch, volume, and duration. The wide-band Mel frequency cepstral coefficients are employed for pitch-invariant feature extraction and instance normalization for volume invariance. An adversarial-trained time-delay neural network encoder is developed for enhancing pitch and duration invariance via random pitch shift and temporal segmentation. Experiments conducted on our curated dataset and the Good-sound dataset show that significant improvements from the new method are achieved in evaluating tone quality ascribed to performers and instruments, yielding a 15.3% and 9.5% increase in classification accuracy, respectively, compared to classical feature-based techniques. Remarkably, the class-wise outcomes exhibit enhancements in F-scores of 33.6% and 9.8% for each respective dataset. Ablation studies on pitch, volume, and duration invariance further underscore the efficacy of our approach. This substantial enhancement over existing methods presents a novel perspective on tone quality representation and offers a practical resource for music performance analysis.},
  archive  = {J},
  author   = {Yixin Wang and Xiaohong Guan and Youtian Du and Chenxu Wang and Xiaobing Li and Yu Pan},
  doi      = {10.1109/TASLPRO.2025.3525969},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {613-626},
  title    = {PiVoD: Pitch, volume and duration invariant representation for music tone quality evaluation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint beam search integrating CTC, attention, and transducer decoders. <em>TASLPRO</em>, <em>33</em>, 598-612. (<a href='https://doi.org/10.1109/TASLPRO.2025.3530324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {End-to-end automatic speech recognition (E2E-ASR) can be classified by its decoder architectures, such as connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention-based encoder-decoder, and Mask-CTC models. Each decoder architecture has advantages and disadvantages, leading practitioners to switch between these different models depending on application requirements. Instead of building separate models, we propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and Mask-CTC) share the same encoder – we refer to this as 4D modeling. The 4D model is trained jointly, which will bring model regularization and maximize the model robustness thanks to their complementary properties. To efficiently train the 4D model, we introduce a two-stage training strategy that stabilizes the joint training. In addition, we propose three novel joint beam search algorithms by combining three decoders (CTC, RNN-T, and attention) to further improve performance. These three beam search algorithms differ in which decoder is used as the primary decoder. We carefully evaluate the performance and computational tradeoffs associated with each algorithm. Experimental results demonstrate that the jointly trained 4D model outperforms the E2E-ASR models trained with only one individual decoder. Furthermore, we demonstrate that the proposed joint beam search algorithm outperforms the previously proposed CTC/attention decoding.},
  archive  = {J},
  author   = {Yui Sudo and Muhammad Shakeel and Yosuke Fukumoto and Brian Yan and Jiatong Shi and Yifan Peng and Shinji Watanabe},
  doi      = {10.1109/TASLPRO.2025.3530324},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {598-612},
  title    = {Joint beam search integrating CTC, attention, and transducer decoders},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive prompt learning with distilled connective knowledge for implicit discourse relation recognition. <em>TASLPRO</em>, <em>33</em>, 586-597. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Implicit discourse relation recognition (IDRR) aims at recognizing the discourse relation between two text segments without an explicit connective. Recently, prompt learning has been applied to the IDRR task with great performance improvements over various neural network-based approaches. However, the discrete nature of the state-art-of-art prompting approach requires manual design of templates and answers, a big hurdle for its practical applications. In this paper, we propose a continuous version of prompt learning together with connective knowledge distillation, called AdaptPrompt, to reduce manual design efforts via continuous prompting while further improving performance via knowledge transfer. In particular, we design and train a few virtual tokens to form continuous templates and automatically select the most suitable one by gradient search in the embedding space. We also design an answer-relation mapping rule to generate a few virtual answers as the answer space. Furthermore, we notice the importance of annotated connectives in the training dataset and design a teacher-student architecture for knowledge transfer. Experiments on the up-to-date PDTB Corpus V3.0 validate our design objectives in terms of the better relation recognition performance over the state-of-the-art competitors.},
  archive  = {J},
  author   = {Bang Wang and Zhenglin Wang and Wei Xiang and Yijun Mo},
  doi      = {10.1109/TASLPRO.2025.3527135},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {586-597},
  title    = {Adaptive prompt learning with distilled connective knowledge for implicit discourse relation recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learnable counterfactual attention for music classification. <em>TASLPRO</em>, <em>33</em>, 570-585. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Counterfactual attention learning (Rao et al. 2021) utilizes counterfactual causality to guide attention learning and has demonstrated great potential in vision-based fine-grained recognition tasks. Despite its excellent performance, existing counterfactual attention is not learned directly from the network itself; instead, it relies on employing random attentions. To address the limitation and considering the inherent differences between visual and acoustic characteristics, we target music classification tasks and present a learnable counterfactual attention (LCA) mechanism, to enhance the ability of counterfactual attention to help identify fine-grained sounds. Specifically, our LCA mechanism is implemented by introducing a counterfactual attention branch into the original attention-based deep-net model. Guided by multiple well-designed loss functions, the model pushes the counterfactual attention branch to uncover biased attention regions that are meaningful yet not overly discriminative (seemingly accurate but ultimately misleading), while guiding the main branch to deviate from those regions, thereby focusing attention on discriminative regions to learn task-specific features in fine-grained sounds. Evaluations on the benchmark datasets artist20 (Ellis et al. 2007), GTZAN (Tzanetakis et al. 2002), and FMA (Defferrard et al. 2017) demonstrate that our LCA mechanism brings a comprehensive performance improvement for deep-net models on singer identification and musical genre classification. Moreover, since the LCA mechanism is only used during training, it doesn't impact testing efficiency.},
  archive  = {J},
  author   = {Yi-Xing Lin and Jen-Chun Lin and Wen-Li Wei and Jia-Ching Wang},
  doi      = {10.1109/TASLPRO.2025.3527143},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {570-585},
  title    = {Learnable counterfactual attention for music classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase retrieval using deep dual alternating direction method of multipliers network with deep sparse prior knowledge. <em>TASLPRO</em>, <em>33</em>, 557-569. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, a deep phase retrieval algorithm for speech signals based on the dual Alternating Direction Method of Multipliers (ADMM) incorporating a deep prior network that exploits the sparsity of speech signals is presented. The proposed network, named DADMM-net, unfolds the dual ADMM for the $\ell _{1}$-regularized non-convex optimization problem of phase retrieval with several two-dimensional convolutional neural networks (2D-CNNs). In order to efficiently optimize the deep unfolding network for high-dimensional parameter vectors, a novel updating scheme referred to as soft coordinate descent (soft-CD) is proposed, where dual parameter updates are determined through interpolation between the current values and the updated values coordinate-wise with respect to the weights computed by deep networks in each layer. Numerical simulations on a publicly available dataset confirm the state-of-the-art performance of the proposed method in terms of perceptual evaluation of speech quality and short-time objective intelligibility with a significantly faster convergence speed compared to existing methods.},
  archive  = {J},
  author   = {Moogyeong Kim and Wonzoo Chung},
  doi      = {10.1109/TASLPRO.2025.3527152},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {557-569},
  title    = {Phase retrieval using deep dual alternating direction method of multipliers network with deep sparse prior knowledge},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSD: An AI-synthesized speech detection scheme using whisper feature and types classification. <em>TASLPRO</em>, <em>33</em>, 542-556. (<a href='https://doi.org/10.1109/TASLPRO.2024.3520385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current efforts in AI-synthesized speech detection often suffer from decreased performance due to detectors overfitting to irrelevant features or fitting features specific to certain synthesis methods. This issue becomes severe when facing unseen synthesis methods or real-world scenarios. To address these challenges, we first construct a multi-scenario dataset named PolyFake. Subsequently, we propose an AI-Synthesized Speech Detection Scheme using Whisper Feature and Types Classification (ASSD). In the feature processing stage, this method adopts a dual-stream network. Firstly, it utilizes the proposed SC_Encoder to separate and reconstruct redundant features, thereby reducing attention on irrelevant features. Meanwhile, it extracts features through a pre-trained Whisper_Encoder to augment feature generalization ability. Then, features extracted by the two encoders are decomposed, concatenated, and fused in both time and frequency domains, concurrently constructing time and frequency domain graphs. Through heterogeneous stacking graph attention, temporal and spectral information is fused to obtain generalized features. In the detection stage, employing a multi-task learning strategy, we introduce a multi-classification task for predicting synthesis method types to further segregate generalized features, obtaining common features of different synthesis methods for synthesized speech detection. The proposed method achieves lower equal error rate compared to state-of-the-art methods, with a relative improvement of 27% in generalization performance under a 2% increase in parameter count, on the In_the_wild and ASVspoof 2021 DF dataset and the testing scenarios of PolyFake.},
  archive  = {J},
  author   = {Chang Liu and Xiaolong Xu and Fu Xiao},
  doi      = {10.1109/TASLPRO.2024.3520385},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {542-556},
  title    = {ASSD: An AI-synthesized speech detection scheme using whisper feature and types classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain emotion recognition enhancement: A novel domain adaptation technique for speech-emotion recognition. <em>TASLPRO</em>, <em>33</em>, 528-541. (<a href='https://doi.org/10.1109/TASLP.2024.3498694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As artificial intelligence advances, speech-emotion recognition (SER) has become a critical research area. Traditional SER methods typically rely on homogeneous domain data for training and testing. This practice requires adaptation when confronted with real-world data's heterogeneous linguistic, methodological, and speaker-related attributes. These variances can degrade the accuracy and generalization of SER models. To address this gap, we introduce a novel domain adaptation technique, multi domain emotion recognition enhancement (MDERE), which utilizes a non-negative matrix to reduce the inflexibility of the conventional binary label matrix for source domain data. This process yields a label matrix that better adapts to the nuances of the source labels while preserving their original structure. This framework refines SER methods by fine-tuning a transformation matrix for enhanced emotion discrimination. Elastic net regularization, which combines L1 and L2 penalties, enriches the transformation matrix, selectively emphasizing relevant features to enhance the robustness of emotion detection. The framework constructs customized similarity and dissimilarity graphs to reconcile the differences between source and target domains, enabling nuanced cross-domain data analysis. Extensive testing on multiple cross-domain SER tasks has shown that MDERE substantially improves recognition accuracy, surpassing several state-of-the-art algorithms. These results demonstrate MDERE's ability to effectively align domain variations enhances the generalizability of SER systems.},
  archive  = {J},
  author   = {Ammar Amjad and Sucharita Khuntia and Hsien-Tsung Chang and Li-Chia Tai},
  doi      = {10.1109/TASLP.2024.3498694},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {528-541},
  title    = {Multi-domain emotion recognition enhancement: A novel domain adaptation technique for speech-emotion recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-document distillation via graph-based summarization of extracted essential knowledge. <em>TASLPRO</em>, <em>33</em>, 518-527. (<a href='https://doi.org/10.1109/TASLP.2024.3490375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains—including news articles, lawsuits, government reports, and scientific texts—under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.},
  archive  = {J},
  author   = {Luca Ragazzi and Gianluca Moro and Lorenzo Valgimigli and Riccardo Fiorani},
  doi      = {10.1109/TASLP.2024.3490375},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {518-527},
  title    = {Cross-document distillation via graph-based summarization of extracted essential knowledge},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge-enhanced contrastive learning network for weakly-supervised aspect detection. <em>TASLPRO</em>, <em>33</em>, 506-517. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect detection, aiming at identifying the aspects of review segments, is a fundamental task in opinion mining and aspect-based sentiment analysis. Due to the high cost and time consuming of human-annotation for massive reviews, several unsupervised and weakly-supervised methods are proposed recently. However, existing weakly-supervised models are mostly seed-driven methods based on co-occurrence of words, which suffer from lacking the ability of detecting the aspects with infrequent aspect terms and identifying Misc aspect. To tackle these problems, we leverage external knowledge to enhance the representation of aspects and segments by a weakly-supervised method. In this paper, we propose an aspect knowledge-enhanced contrastive learning (AKECL) network with two powerful knowledge-enhanced encoders for aspects and reivew segments to enhance weakly-supervised aspect detection task. Experiments in seven different domains show that AKECL outperforms the competitive baselines, and demonstrate the effectiveness of our proposed method, as well as the improvement by introducing external knowledge.},
  archive  = {J},
  author   = {Zhuoming Zheng and Yi Cai and Liuwu Li},
  doi      = {10.1109/TASLPRO.2025.3525964},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {506-517},
  title    = {A knowledge-enhanced contrastive learning network for weakly-supervised aspect detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical local-global transformer with dynamic positional encoding for document-level machine translation. <em>TASLPRO</em>, <em>33</em>, 494-505. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing document-level neural machine translation models aim to extend the translation unit from the sentence level to the entire document. However, current long sequence models face limitations due to the large hypothesis space problem associated with the standard attention mechanism. Moreover, as the length of the document increases, the translation performance decreases significantly. In this paper, we propose HLG, a hierarchical local-global Transformer in a single-stream manner to address the issue of hypothetical space and we introduce a local bias into the positional encoding layer, mitigating the sparsity problem caused by absolute positional encoding. The hierarchical architecture with dynamic positional encoding can facilitate the interaction between the current sentence and the broader context to capture the feature in entire documents, which is beneficial for modeling long sequences. Experimental results demonstrate that the HLG achieves state-of-the-art translation performance in non-pre-training settings for document-level neural machine translation, outperforming various strong baselines in both sentence-to-sentence and document-to-document baseline manners. Furthermore, our model exhibits stable performance when extending the document modeling length up to longer tokens on three benchmark datasets.},
  archive  = {J},
  author   = {Hao Yu and Degen Huang and Kaiyu Huang},
  doi      = {10.1109/TASLPRO.2025.3525965},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {494-505},
  title    = {Hierarchical local-global transformer with dynamic positional encoding for document-level machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utterance alignment of language models for effective user simulation in task-oriented dialogues. <em>TASLPRO</em>, <em>33</em>, 483-493. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional user simulators often rely on manually designed agendas, resulting in generated responses lacking diversity and spontaneity. However, building user simulators with large language models (LLMs) heavily depends on selecting examples for in-context learning. Additionally, complex tasks and lengthy contexts can further burden LLMs, making generating responses that meet the desired user goals challenging. To tackle the abovementioned issue, this study introduces AlignUS, which combines a small language model (SLM) and another LLM to construct a user simulator for task-oriented dialogue systems. The SLM can produce basic dialogue actions and natural language utterances through training. Meanwhile, the LLM verifies the dialogue actions generated by the SLM and polishes the generated natural language utterances using its reasoning capabilities. Additionally, the natural language utterances polished by the LLM are aligned with the SLM, enhancing the SLM's ability to generate diverse utterances. Based on this, the burden on the LLM can be alleviated, as it only needs to handle a small portion of the task without case sampling. Extensive experiments conducted on the MultiWOZ dataset validated the effectiveness of the proposed approach, highlighting improvements in response quality and diversity. Compared to the current state-of-the-art (SOTA), our model demonstrates an improvement of 0.26 in success rate and completion rate for the MultiWOZ dataset over 100 dialogue turns.},
  archive  = {J},
  author   = {Xiang Luo and Jin Wang and Xuejie Zhang},
  doi      = {10.1109/TASLPRO.2025.3525981},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {483-493},
  title    = {Utterance alignment of language models for effective user simulation in task-oriented dialogues},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing domain mismatch in unsupervised neural machine translation. <em>TASLPRO</em>, <em>33</em>, 472-482. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pretrained models have taken full advantage of monolingual corpora and achieved impressive results in training Unsupervised Neural Machine Translation (UNMT) models. However, when adapting UNMT models with in-domain monolingual corpora for domain-specific translation tasks, one of the languages may lack in-domain corpora, resulting in the unequal amount and proportion of in-domain monolingual corpora in each language. This problem situation is known as Domain Mismatch (DM). This study investigates the impact of DM in UNMT. We find that DM causes a translation quality disparity. That is, while in-domain monolingual corpora of a language can enhance the in-domain translation quality into that particular language, this enhancement cannot be generalized to the other language, and the translation quality into the other language remains deficient. To address this problem, we propose Domain-Aware Adaptation (DAA), which can be embedded in the vanilla UNMT model training process. By passing sentence-level domain information to the model during training and inference, DAA gives higher weight to in-domain data from open-domain corpora related to specific domains to alleviate domain mismatch. The experimental results on German-English and Romanian-English translation tasks specified in the IT, Koran, medical, and TED2020 domains demonstrate that DAA can efficiently exploit open-domain corpora to mitigate the quality disparity of translation caused by DM.},
  archive  = {J},
  author   = {Youyuan Lin and Rui Wang and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3527896},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {472-482},
  title    = {Addressing domain mismatch in unsupervised neural machine translation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separate anything you describe. <em>TASLPRO</em>, <em>33</em>, 458-471. (<a href='https://doi.org/10.1109/TASLP.2024.3520017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperforming previous audio-queried and language-queried sound separation models. Specifically, AudioSep achieved strong results including a signal-to-distortion ratio improvement (SDRi) of 7.74 dB across 527 sound classes of the AudioSet; 9.14 dB on the VGGSound dataset; 8.22 dB on the AudioCaps dataset; 6.85 dB on the Clotho dataset; 10.51 dB on the MUSIC dataset; 10.04 dB on the ESC-50 dataset; 8.16 dB on the DCASE 2024 Task 9 dataset; and a segmental signal to noise ratio (SSNR) of 9.21 dB on the Voicebank-Demand dataset.},
  archive  = {J},
  author   = {Xubo Liu and Qiuqiang Kong and Yan Zhao and Haohe Liu and Yi Yuan and Yuzhuo Liu and Rui Xia and Yuxuan Wang and Mark D. Plumbley and Wenwu Wang},
  doi      = {10.1109/TASLP.2024.3520017},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {458-471},
  title    = {Separate anything you describe},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end supervised hierarchical graph clustering for speaker diarization. <em>TASLPRO</em>, <em>33</em>, 448-457. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications. The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion. While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The embedding extractor is initialized using a pre-trained x-vector model while the GNN model is trained initially using the x-vector embeddings from the pre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbank features as input and jointly optimizes the embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization. Further, with additional inputs from an external overlap detector, the E-SHARC approach is capable of predicting the speakers in the overlapping speech regions. The experimental evaluation on benchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that the proposed E-SHARC framework provides competitive diarization results using graph based clustering methods.},
  archive  = {J},
  author   = {Prachi Singh and Sriram Ganapathy},
  doi      = {10.1109/TASLPRO.2025.3525967},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {448-457},
  title    = {End-to-end supervised hierarchical graph clustering for speaker diarization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multi-meta information embedding for recognizing complex named entities in a mechanics intelligent tutoring system. <em>TASLPRO</em>, <em>33</em>, 434-447. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Named entity recognition (NER) is a bottleneck in the development of intelligent tutoring system (ITS). NER is difficult in ITS due to the complexity and the lack of entity boundary of concepts that must be named. Compared with the open domain, ITS domains have a large number of specialized vocabulary items and long entities, which leads to a low recognition performance. In this study, we propose a collaborative model for ITS NER in the mechanics domain using Chinese texts. First, a multi-meta information embedding (MMIE) method including character embedding, lexicon embedding and radical embedding is proposed to capture the semantic information of characters for Chinese sentences. Second, a collaborative graph neural network (GNN) framework is presented to fully integrate the multi-meta information and learn the contextual information of each character. Then a collaborative transformer structure is employed to recognize the named entity. Finally, extensive experiments are carried out, and the experimental results indicated the proposed method outperforms previous methods in our collected dataset and three mainstream datasets. In practice, a series of ablation experiments and case studies implied that our method is effective and efficient for Chinese mechanics NER.},
  archive  = {J},
  author   = {Jiarong Zhang and Jinsha Yuan and Yang Liu and Shuangshuang Ban and Jin Zhang and Xinyu Zan},
  doi      = {10.1109/TASLPRO.2025.3527117},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {434-447},
  title    = {Exploring multi-meta information embedding for recognizing complex named entities in a mechanics intelligent tutoring system},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully few-shot class-incremental audio classification with adaptive improvement of stability and plasticity. <em>TASLPRO</em>, <em>33</em>, 418-433. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the problem of Few-shot Class-incremental Audio Classification (FCAC), training samples per class in the base session are required to be abundant. However, in many scenarios, it is difficult to collect abundant training samples in the base session because of data scarcity and high collection cost. In this paper, we explore a new FCAC problem, namely Fully FCAC (FFCAC), in which training samples for all classes in both the base and incremental sessions are few. Moreover, we propose a FFCAC method by adaptively improving the model's stability for seen classes and plasticity for unseen classes. The model consists of an Embedding Extractor (EE) and an Evolvable Classifier (EC). The EE consists of an encoder of pretrained Audio Spectrogram Transformer (AST), an encoder of finetuned AST and three fusion modules. In each incremental session, the two encoders are frozen for memorizing the knowledge learned by the model and thus can improve the model's stability. The three fusion modules are used to fuse the embeddings output by the two encoders. The EC is composed of a fully-connected layer and a Softmax layer. The fusion modules and EC are updated in incremental sessions to improve the model's plasticity for unseen classes. Besides, two losses are defined to train the model in the base and incremental sessions. Results on three public audio datasets (LS-100, NSynth-100, and FSC-89) show that our FFCAC method exceeds previous methods in accuracy under many conditions.},
  archive  = {J},
  author   = {Yongjie Si and Yanxiong Li and Jiaxin Tan and Guoqing Chen and Qianqian Li and Mladen Russo},
  doi      = {10.1109/TASLPRO.2025.3527147},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {418-433},
  title    = {Fully few-shot class-incremental audio classification with adaptive improvement of stability and plasticity},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modular dual learning for improving question answering and generation over knowledge graphs. <em>TASLPRO</em>, <em>33</em>, 401-417. (<a href='https://doi.org/10.1109/TASLPRO.2025.3527218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Question answering (QA) and question generation (QG) over knowledge graphs (KGs) are complementary because both involve mapping natural language to KG relations and forming reasoning paths. Prior work shows that KGQG can improve KGQA via data augmentation, but it remains unclear if KGQA can also support KGQG. Since the two tasks share a symmetrical structure, we hypothesize that jointly learning KGQA and KGQG could yield mutual benefits. Yet, the effectiveness of such joint learning has not been thoroughly examined. To address this, we introduce a modular dual learning framework that promotes dual-task collaboration through a self-organized modular sharing mechanism. The framework contains three main components: a KGQA module, a KGQG module, and a modular sharing network. Given an input, the model first generates a routing selector using a discrete latent variable, which then chooses suitable shared layers and assembles a task-specific network. These shared layers have identical structures and parameter sizes, and can appear at different positions along the processing path. A selective assessment procedure allows each module to incorporate the shared network, thus leveraging dual-task signals. Experiments on two standard benchmarks confirm that our framework significantly boosts the performance of both KGQA and KGQG. The dual signals also enhance generalization, and even when the trained module is replaced with a random network, the modular sharing network still markedly improves the new model with minimal fine-tuning, indicating knowledge transfer.},
  archive  = {J},
  author   = {Sheng Bi and Zeyi Miao and Qizhi Min},
  doi      = {10.1109/TASLPRO.2025.3527218},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {401-417},
  title    = {A modular dual learning for improving question answering and generation over knowledge graphs},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The codecfake dataset and countermeasures for the universally detection of deepfake audio. <em>TASLPRO</em>, <em>33</em>, 386-400. (<a href='https://doi.org/10.1109/TASLPRO.2025.3525966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for generalized detection methods. ALM-based deepfake audio currently exhibits widespread, high deception, and type versatility, posing a significant challenge to current audio deepfake detection (ADD) models trained solely on vocoded data. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially constructed the Codecfake dataset, an open-source, large-scale collection comprising over 1 million audio samples in both English and Chinese, focus on ALM-based audio detection. As countermeasure, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original sharpness aware minimization (SAM), we propose the CSAM strategy to learn a domain balanced and generalized minima. In our experiments, we first demonstrate that ADD model training with the Codecfake dataset can effectively detects ALM-based audio. Furthermore, our proposed generalization countermeasure yields the lowest average equal error rate (EER) of 0.616% across all test conditions compared to baseline models. The dataset and associated code are available online.},
  archive  = {J},
  author   = {Yuankun Xie and Yi Lu and Ruibo Fu and Zhengqi Wen and Zhiyong Wang and Jianhua Tao and Xin Qi and Xiaopeng Wang and Yukun Liu and Haonan Cheng and Long Ye and Yi Sun},
  doi      = {10.1109/TASLPRO.2025.3525966},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {386-400},
  title    = {The codecfake dataset and countermeasures for the universally detection of deepfake audio},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed identification and interpolation of the directivity of sound sources. <em>TASLPRO</em>, <em>33</em>, 372-385. (<a href='https://doi.org/10.1109/TASLP.2024.3519872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The radiation characteristics of sound sources are important, for example, for the simulation and auralization of sound fields or for the design of sound reinforcement systems. They are usually determined by measurements in an anechoic chamber with a discrete grid of microphones on a spherical surface and subsequent interpolation. This manuscript introduces a physically informed method for source directivity identification and interpolation based on source modeling using the adjoint Euler equations in a finite-difference time-domain approach. Synthetic analyses are used to investigate the properties of the method under different parameters: the number of microphones used, the effect of measurement noise and microphone misalignment, off-center source positioning, and the presence of reflecting walls. In addition, the method is applied to a real-world scenario involving the analysis of a musical instrument. The method is found to be a versatile and robust approach to sound source directivity analysis. Its particular strengths are its flexibility in microphone placement, its adaptability to non-centered sound sources, and its ability to account for non-anechoic environments.},
  archive  = {J},
  author   = {Mathias Lemke and Arne Hölter and Stefan Weinzierl},
  doi      = {10.1109/TASLP.2024.3519872},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {372-385},
  title    = {Physics-informed identification and interpolation of the directivity of sound sources},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting knowledge for few-shot table-to-text generation. <em>TASLPRO</em>, <em>33</em>, 359-371. (<a href='https://doi.org/10.1109/TASLP.2024.3430480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt unlabeled domain-specific knowledge into the model, which brings at least three benefits: 1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; 2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; 3) it allows us to design various tasks to employ the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-domain, few-shot natural language generation (NLG) data sets: Humans, Songs, and Books. Compared to previous state-of-the-art approaches, our model achieves superior performance in terms of both fluency and accuracy as judged by human and automatic evaluations.},
  archive  = {J},
  author   = {Zhixin Guo and Mingxuan Yan and Jiexing Qi and Jianping Zhou and Ziwei He and Guanjie Zheng and Xinbing Wang and Chenghu Zhou},
  doi      = {10.1109/TASLP.2024.3430480},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {359-371},
  title    = {Adapting knowledge for few-shot table-to-text generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple and interactive transformer for fine-grained emotion detection. <em>TASLPRO</em>, <em>33</em>, 347-358. (<a href='https://doi.org/10.1109/TASLP.2024.3487418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fine-grained emotion detection is a long-standing task in sentiment analysis. One of the crucial challenges in this task is to capture emotion correlations. Previous studies either ignore this challenge or resort to a chain-link structure (e.g. seq2seq), both of which encounter some thorny issues in reality. Unlike previous studies, we approach this challenge by proposing a masked language modeling like mechanism, named masked emotion modeling (MEM), which can capture emotion correlations in a simple but effective manner. Specifically, we augment the transformer model with emotion states and require the transformer model to recover masked emotion states. Furthermore, to fully enhance the effect of the proposed MEM, we also introduce a taxonomy-guided sampling strategy at the training phase and an iterative decoding strategy at the test phase. On two widely used datasets, we demonstrate that our method excels at capturing emotion correlations and achieves state-of-the-art results.},
  archive  = {J},
  author   = {Dianbo Sui and Bo Li and Hang Yang and Yubo Chen and Kang Liu and Dianhui Chu and Jun Zhao},
  doi      = {10.1109/TASLP.2024.3487418},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {347-358},
  title    = {A simple and interactive transformer for fine-grained emotion detection},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparallel spoken-text-style transfer for linguistic expression control in speech generation. <em>TASLPRO</em>, <em>33</em>, 333-346. (<a href='https://doi.org/10.1109/TASLPRO.2024.3522757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text style transfer is the task of converting the style of a text while preserving its content. However, under the nonparallel training condition, the task is still challenging, even for general-purpose large language models (LLMs). This study aims to improve the performance of text style transfer using task-specific methods in a labeled nonparallel condition. We target the transfer of spoken styles in the text domain to realize more flexible style control of synthetic speech when combined with text-to-speech synthesis. We propose a method for preserving “content words”, particularly for improving content preservation, and incorporate it in the conditional variational autoencoder for capturing the style information from the labeled nonparallel corpus. Further improvements were achieved by introducing positional embedding and cycle learning in both content preservation and style control performance. We have conducted a bi-directional transfer experiment on Japanese texts, focusing on “disfluency removal/insertion” and “standard/Kansai dialect conversion” as target styles. Our experimental evaluations reveal that: 1) the proposed method enhanced content preservation and style control performance; 2) our proposed method shows higher content preservation and computational efficiency while achieving comparable style control performance to large language models; 3) the proposed method has a positive perceptual impact on generated speech in terms of style reproducibility when applied to a preprocessing for TTS.},
  archive  = {J},
  author   = {Daiki Yoshioka and Yusuke Yasuda and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2024.3522757},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {333-346},
  title    = {Nonparallel spoken-text-style transfer for linguistic expression control in speech generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequence-wise speech waveform modeling via backpropagation optimization of quasi-harmonic parameters. <em>TASLPRO</em>, <em>33</em>, 319-332. (<a href='https://doi.org/10.1109/TASLPRO.2024.3522784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sinusoidal modeling (SM) has been studied for speech modeling, which aims at representing and flexibly resynthesizing the speech waveform by the frequency and complex amplitude of several components. However, SM methods model the speech frame by frame, which inevitably leads to a loss of information between individual frames. In addition, SM method cannot adaptively optimize the prior frequency during the modeling. Even quasi-harmonic models (QHMs), which can adjust the frequency adaptively, require several iterations to update the frequency and finally obtain a dubious value. These limitations cause the quality degradation of a resynthesized speech waveform, especially in unvoiced speech parts. In this paper, we propose a sequence-wise speech waveform modeling method based on gradient descent to obtain the required parameters by directly minimizing the reconstruction waveform error and novel spectrogram loss, which can accelerate the convergence, of the entire speech between the result and the target. The proposed method can specifically compensate for errors caused by the missing information between frames by making the parameter extraction and resynthesis processes consistent. To investigate the effectiveness of the proposed method, we conduct experimental evaluations using real speech utterances, demonstrating that the proposed method achieves an improvement in speech resynthesis quality, i.e., from 10.6 dB to 14.9 dB of signal-to-reconstruction error and from 2.30 dB to 2.04 dB of mel-cepstral distortion.},
  archive  = {J},
  author   = {Shaowen Chen and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2024.3522784},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {319-332},
  title    = {Sequence-wise speech waveform modeling via backpropagation optimization of quasi-harmonic parameters},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling a phased array of parametric array loudspeakers using the spherical wave expansion. <em>TASLPRO</em>, <em>33</em>, 309-318. (<a href='https://doi.org/10.1109/TASLPRO.2024.3522764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Parametric array loudspeakers (PALs) that utilize the phased array technique are receiving considerable attention because of their ability to electronically control and manipulate highly directional audio beams with great flexibility. However, accurately modeling the phased array PAL poses significant challenges due to the complex nature of the nonlinear processes involved, which results in the demodulated audio sound generated by the array not simply being a superposition of that generated by each individual array element. In addition, the nonlinear interaction between the ultrasound generated by any two distinct elements creates a coupled audio sound component that also contributes to the total audio sound field. Based on the quasilinear solution of the Westervelt equation as well as the spherical wave expansion method, this paper proposed a computationally efficient and accurate method to include the coupled audio sound component. Both simulations and experiments for a uniform linear PAL array are conducted to validate the method. The proposed method paved the way for simulating and designing phased array PAL systems.},
  archive  = {J},
  author   = {Tao Zhuang and Jia-Xin Zhong and Guilin Ma and Feng Niu and Jing Lu},
  doi      = {10.1109/TASLPRO.2024.3522764},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {309-318},
  title    = {Modeling a phased array of parametric array loudspeakers using the spherical wave expansion},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRIMAL: Prompting multiple language models for low-resource diverse response generation. <em>TASLPRO</em>, <em>33</em>, 297-308. (<a href='https://doi.org/10.1109/TASLP.2024.3507571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Low-resource conversation models are becoming increasingly important. Existing conversation models tend to generate uninformative responses that lack diversity, especially when the training data are limited. Researchers address this issue by refining training objectives or incorporating additional data sources. Learned from masses of diverse texts, pre-trained language models show their potential in text generation. However, language models (LMs) learn by approximating the distribution of a single ground truth, which limits their ability to capture the ‘one-to-many’ characteristic essential for generating diverse utterances in conversation tasks. In this paper, we propose to leverage multiple pre-trained LMs in a unified framework to diversify low-resource dialogue generation. We apply a prompt-based method to exploit pre-trained LMs for more information. To generate responses referring to multiple LMs, we design a multi-LMs decoding algorithm where LMs interact with each other at each step. We also propose a multi-LMs-based entropy for response generation to further enhance diversity. Experiments on two datasets demonstrate that our method outperforms competitive baselines in low-resource settings.},
  archive  = {J},
  author   = {Zhihua Wen and Zhiliang Tian and Shilong Pan and Kangchen Zhu and Xiangyun Meng and Yiping Song and Dongsheng Li},
  doi      = {10.1109/TASLP.2024.3507571},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {297-308},
  title    = {PRIMAL: Prompting multiple language models for low-resource diverse response generation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilevel joint unsupervised and supervised training for automatic speech recognition. <em>TASLPRO</em>, <em>33</em>, 286-296. (<a href='https://doi.org/10.1109/TASLP.2024.3519869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a bilevel joint unsupervised and supervised training (BL-JUST) framework for automatic speech recognition. Compared to the conventional pre-training and fine-tuning strategy which is a disconnected two-stage process, BL-JUST tries to optimize an acoustic model such that it simultaneously minimizes both the unsupervised and supervised loss functions. Because BL-JUST seeks matched local optima of both loss functions, acoustic representations learned by the acoustic model strike a good balance between being generic and task-specific. We solve the BL-JUST problem using penalty-based bilevel gradient descent and evaluate the trained deep neural network acoustic models on various datasets with a variety of architectures and loss functions. We show that BL-JUST can outperform the widely-used pre-training and fine-tuning strategy and some other popular semi-supervised techniques.},
  archive  = {J},
  author   = {Xiaodong Cui and A F M Saif and Songtao Lu and Lisha Chen and Tianyi Chen and Brian Kingsbury and George Saon},
  doi      = {10.1109/TASLP.2024.3519869},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {286-296},
  title    = {Bilevel joint unsupervised and supervised training for automatic speech recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-speaker distribution learning in voice anonymization. <em>TASLPRO</em>, <em>33</em>, 272-285. (<a href='https://doi.org/10.1109/TASLP.2024.3519879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {State-of-the-art framework for voice anonymization involves disentangling the speaker attributes from the original speech, substituting its representation with that of a pseudo-speaker, and generating the anonymized utterance. The generation of the pseudo-speaker representation typically involves two steps: selecting a cohort of speakers from a pre-defined speaker pool and then constructing the pseudo-speaker representation based on the selected cohort speakers. This paper focuses on pseudo-speaker modeling given the cohort speakers. In this regard, we propose to model the speaker attributes with a distribution instead of the conventional point estimate. Neural networks are utilized to learn pseudo-speaker distributions, leveraging the nonlinear speaker variations among the cohort speakers. Specifically, both utterances and frames are investigated as modeling units. Experiments were conducted on the LibriSpeech and VCTK datasets. The results demonstrated the efficacy of the proposed pseudo-speaker modeling method in improving voice protection performance, especially in increasing the voice distinctiveness among the anonymized utterances. Moreover, our results also show that the pseudo-speaker model performs better when learnt from frame-level representation compared to the utterance-level counterpart. Audio samples can be found in https://voiceprivacy.github.io/pseudo-speaker-distribution/.},
  archive  = {J},
  author   = {Liping Chen and Wenju Gu and Kong Aik Lee and Wu Guo and Zhen-Hua Ling},
  doi      = {10.1109/TASLP.2024.3519879},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {272-285},
  title    = {Pseudo-speaker distribution learning in voice anonymization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving personality consistency in conversation with commonsense knowledge. <em>TASLPRO</em>, <em>33</em>, 262-271. (<a href='https://doi.org/10.1109/TASLP.2024.3519799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Maintaining persona consistency is crucial for an open-domain dialogue system. Existing methods focus on extracting user persona from historical context to guide persona-consistent response generation. Actually, the historical conversations are usually noisy and uninformative, and thus the target response generation is challenging, as well as easily falling into sub-optimization, especially for the well-known semantic gap between the potential persona information in the history and the target response, which can be bridged by dynamically injecting commonsense knowledge for inference. Hence, in this paper, we propose a persona-consistent dialogue model, consisting of a unified word-concept representing framework and a commonsense reasoning component, to reduce the semantic distance between persona and response. Extensive experiments demonstrate that our proposed model achieves significant enhancements in persona coherence.},
  archive  = {J},
  author   = {Yifan Li and Shixuan Fan and Wei Wei and Xianling Mao and Kaihe Xu and Dangyang Chen},
  doi      = {10.1109/TASLP.2024.3519799},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {262-271},
  title    = {Improving personality consistency in conversation with commonsense knowledge},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot prompting for LLM-based machine translation using in-domain target sentences. <em>TASLPRO</em>, <em>33</em>, 251-261. (<a href='https://doi.org/10.1109/TASLP.2024.3519814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One promising aspect of using large language models (LLMs) for translation is their ability to adapt effectively to unseen domains without fine-tuning, through strategic prompting. However, the success of this approach largely depends on the selection of relevant examples from domain-specific databases to construct the few-shot prompt for the given input. This work propose a novel zero-shot technique that circumvents data limitations by generating synthetic translation examples from monolingual domain-specific target texts. Our preliminary experiments suggest that LLMs demonstrate increased sensitivity to noise on the target side of examples, while maintaining a tolerance for variations in example quality. Based on these findings, we generate pseudo-examples by using a cross-lingual sentence encoder to identify sentences in the target language that correspond to the input, and then pairing them with coarse translations derived from a dictionary. To improve the retrieval of better pseudo-examples for translation tasks, we enhance the sentence encoder by incorporating domain information through conditional layer normalization and introducing two innovative training objectives: LLM Preference Prediction and Windowed Sentence Prediction. Experiments conducted across various domains demonstrate that our approach not only significantly outperforms previous zero-shot methods but is also comparable to the robust NLLB-3.3B and ALMA-13B(-R) model.},
  archive  = {J},
  author   = {Baijun Ji and Xiangyu Duan and Yue Zhang and Kaixin Wu and Min Zhang},
  doi      = {10.1109/TASLP.2024.3519814},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {251-261},
  title    = {Zero-shot prompting for LLM-based machine translation using in-domain target sentences},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards high-quality and efficient speech bandwidth extension with parallel amplitude and phase prediction. <em>TASLPRO</em>, <em>33</em>, 236-250. (<a href='https://doi.org/10.1109/TASLP.2024.3519881'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech bandwidth extension (BWE) refers to widening the frequency bandwidth range of speech signals, enhancing the speech quality towards brighter and fuller. This paper proposes a generative adversarial network (GAN) based BWE model with parallel prediction of Amplitude and Phase spectra, named AP-BWE, which achieves both high-quality and efficient wideband speech waveform generation. The proposed AP-BWE generator is entirely based on convolutional neural networks (CNNs). It features a dual-stream architecture with mutual interaction, where the amplitude stream and the phase stream communicate with each other and respectively extend the high-frequency components from the source narrowband amplitude and phase spectra. To improve the naturalness of the extended speech signals, we employ a multi-period discriminator at the waveform level and design a pair of multi-resolution amplitude and phase discriminators at the spectral level, respectively. Experimental results demonstrate that our proposed AP-BWE achieves state-of-the-art performance in terms of speech quality for BWE tasks targeting sampling rates of both 16 kHz and 48 kHz. In terms of generation efficiency, due to the all-convolutional architecture and all-frame-level operations, the proposed AP-BWE can generate 48 kHz waveform samples 292.3 times faster than real-time on a single RTX 4090 GPU and 18.1 times faster than real-time on a single CPU. Notably, to our knowledge, AP-BWE is the first to achieve the direct extension of the high-frequency phase spectrum, which is beneficial for improving the effectiveness of existing BWE methods.},
  archive  = {J},
  author   = {Ye-Xin Lu and Yang Ai and Hui-Peng Du and Zhen-Hua Ling},
  doi      = {10.1109/TASLP.2024.3519881},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {236-250},
  title    = {Towards high-quality and efficient speech bandwidth extension with parallel amplitude and phase prediction},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attend to listen: A single-Input/Binaural-output heterophasic MVDR filter for noise reduction and perceptual rendering. <em>TASLPRO</em>, <em>33</em>, 224-235. (<a href='https://doi.org/10.1109/TASLP.2024.3519895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we present a novel single-input/ binaural-output (SIBO) minimum variance distortionless response (MVDR) noise reduction method, which involves formulating two MVDR sub-filters, one for the left ear and the other for the right ear, by minimizing the interaural coherence of the noise signal while ensuring the distortionless constraint, so that the desired speech signal can pass through the filter without distortion. Subsequently, a unique heterophasic binaural presentation is generated. The method effectively reduces noise while directing the desired signal and residual noise to different directions/zones in the perceptual space. This utilization of human binaural perception properties enhances speech intelligibility. A deep neural network (DNN) based noise covariance matrix estimation method facilitates the implementation of the binaural heterophasic filters in simulations and listening tests. The results demonstrate the superiority of the proposed SIBO MVDR method in enhancing both speech quality and intelligibility as compared to the conventional single-input/single-output (SISO) MVDR filter.},
  archive  = {J},
  author   = {Ningning Pan and Jilu Jin and Xianrui Wang and Jacob Benesty and Jingdong Chen},
  doi      = {10.1109/TASLP.2024.3519895},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {224-235},
  title    = {Attend to listen: A single-Input/Binaural-output heterophasic MVDR filter for noise reduction and perceptual rendering},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node-specific distributed generalized sidelobe canceler. <em>TASLPRO</em>, <em>33</em>, 211-223. (<a href='https://doi.org/10.1109/TASLP.2024.3513833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With booming growth of Internet of Things technique, acoustic sensor networks comprised of microphone array nodes have been increasingly applied to multi-source speech enhancement (SE). Compared with conventional centralized SE algorithms, distributed approaches achieve comparable performance with many advantages such as convenience, low cost and scalability. However, existing distributed methods either need to run once for each desired source independently, or require the number of channels transmitted by each node to increase proportionally with the number of desired sources. Furthermore, it is also challenging for each node to acquire all the steering vectors as demanded by existing methods. To address these issues, a node-specific distributed generalized sidelobe canceler (NS-DGSC) algorithm is proposed in this article. Here, “node-specific” refers to scenarios where each node estimates individual desired source. First, microphone signals at each node are pre-filtered by a local generalized sidelobe canceler (GSC), yielding preliminary enhancement for individual desired source. Then, for each node, preliminary results exchanged from other nodes are utilized to expand the lower branch of local GSC. Finally, the expanded lower branch is employed to calculate a new noise canceler, leading to the final output at each node. Compared with state-of-the-art distributed algorithms, our approach only requires single-channel transmission at each node as well as available local steering vectors for simultaneously enhancing multiple desired sources. Simulations across various signal-to-interference-plus-noise ratio (SINR) input, reverberation time, source-to-node distance, and steering vector estimation error conditions reveal that the proposed approach considerably outperforms existing methods in terms of signal-to-distortion ratio (SDR) and robustness in practice, while provides comparable SINR improvement. Real-world experiments also verify its effectiveness.},
  archive  = {J},
  author   = {Shiqin Li and Zhao Zhao and Zhiyong Xu and Yingjiao Rong},
  doi      = {10.1109/TASLP.2024.3513833},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {211-223},
  title    = {Node-specific distributed generalized sidelobe canceler},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex-bin2bin: A latency-flexible generative neural model for audio packet loss concealment. <em>TASLPRO</em>, <em>33</em>, 199-210. (<a href='https://doi.org/10.1109/TASLP.2024.3515794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite the significant advancements in networking technologies, transmission of data packets in real-time, particularly in speech communications, continues to face challenges due to the possibility of data loss. This loss not only compromises sound quality but also diminishes overall intelligibility. In such cases, Packet Loss Concealment (PLC) techniques could help by reconstructing the missing content and restoring the audio quality. This work proposes a novel method, that improves previous time-frequency generative inpainting approaches. Compared to other state-of-the-art methods, our proposed approach has the flexibility to restore lost packets either in real-time at low latency or in offline mode, without the need to retrain the network. Evaluations conducted against a recent state-of-the-art method, ranked at the top of the 2022 Microsoft PLC competition, and against four DNN-based PLC solutions from the literature, show superior scores in terms of task-specific metrics. The method has also been tested in more challenging scenarios than aforementioned ones, with packet loss rates of up to 50%, showing the ability to help automatic speech recognition (ASR) systems reduce word error rate (WER) by up to almost 50% relative improvement. Additionally, a comparative subjective evaluation has been conducted, confirming the effectiveness of the proposed method in relation to the state of the art.},
  archive  = {J},
  author   = {Carlo Aironi and Leonardo Gabrielli and Samuele Cornell and Stefano Squartini},
  doi      = {10.1109/TASLP.2024.3515794},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {199-210},
  title    = {Complex-bin2bin: A latency-flexible generative neural model for audio packet loss concealment},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a scalable and privacy-preserving audio surveillance system. <em>TASLPRO</em>, <em>33</em>, 186-198. (<a href='https://doi.org/10.1109/TASLP.2024.3516521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The human voice is one of the passive biometrics that can be used in a surveillance system to uniquely identify individuals. It allows law enforcement agencies to detect and track suspects by deploying capturing devices (such as microphones) within a certain region. To address the clear privacy concerns of such an approach, we propose an efficient way of detecting suspects in public areas—through their voices—while preserving the privacy of innocent individuals. More precisely, our approach is quite suitable for large-scale surveillance systems, where millions of recordings are analyzed every day. Our privacy-preserving model is built on top of the most accurate speaker recognition systems, and we show that the accuracy loss due to the added privacy-preserving layer is negligible. The latter employs a highly efficient cryptosystem to securely compute the similarity scores between the captured utterances and the ones stored in the suspects' database. Specifically, the system computes, for each suspect, the encrypted Probabilistic Linear Discriminant Analysis (PLDA) score and obliviously matches it against a set threshold. More importantly, we show that our computation and communication overhead is significantly lower compared to the state-of-the-art techniques, which facilitates a real-time surveillance operation. Our protocol necessitates a single round of communication between the server and the capturing device and, for a database of 100 suspects, the online computation time is only 135 ms on the capturing device and 35 ms on the server, whereas the required communication is 12 KB.},
  archive  = {J},
  author   = {Muhammad Mazhar Ullah Rathore and Elmahdi Bentafat and Spiridon Bakiras},
  doi      = {10.1109/TASLP.2024.3516521},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {186-198},
  title    = {Towards a scalable and privacy-preserving audio surveillance system},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SQ-whisper: Speaker-querying based whisper model for target-speaker ASR. <em>TASLPRO</em>, <em>33</em>, 175-185. (<a href='https://doi.org/10.1109/TASLP.2024.3513835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Benefiting from massive and diverse data sources, speech foundation models exhibit strong generalization and knowledge transfer capabilities to a wide range of downstream tasks. However, a limitation arises from their exclusive handling of single-speaker speech input, making them ineffective in recognizing multi-speaker overlapped speech, a common occurrence in real-world scenarios. In this study, we delve into the adaptation of speech foundation models to eliminate interfering speakers from overlapping speech and perform target-speaker automatic speech recognition (TS-ASR). Initially, we utilize the Whisper model as the foundation for adaptation and conduct a thorough comparison of its integration with existing target-speaker adaptation techniques. We then propose an innovative model termed Speaker-Querying Whisper (SQ-Whisper), which employs a set number of trainable queries to capture speaker prompts from overlapping speech based on target-speaker enrollment. These prompts serve to steer the model in extracting speaker-specific features and accurately recognizing target-speaker transcriptions. Experimental results demonstrate that our approach effectively adapts the pre-trained speech foundation model to TS-ASR. Compared with the robust TS-HuBERT model, the proposed SQ-Whisper significantly improves performance, yielding up to 15% and 10% relative reductions in word error rates (WERs) on the Libri2Mix and WSJ0-2Mix datasets, respectively. With data augmentation, we establish new state-of-the-art WERs of 14.6% on the Libri2Mix Test set and 4.4% on the WSJ0-2Mix Test set. Furthermore, we evaluate our model on the real-world AMI meeting dataset, which shows consistent improvement over other adaptation methods.},
  archive  = {J},
  author   = {Pengcheng Guo and Xuankai Chang and Hang Lv and Shinji Watanabe and Lei Xie},
  doi      = {10.1109/TASLP.2024.3513835},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {175-185},
  title    = {SQ-whisper: Speaker-querying based whisper model for target-speaker ASR},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bone conduction-aided speech enhancement with two-tower network and contrastive learning. <em>TASLPRO</em>, <em>33</em>, 163-174. (<a href='https://doi.org/10.1109/TASLP.2024.3512207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a time-domain two-tower network, namely BiNet, that jointly utilizes bone-conducted speech and noisy air-conducted speech for multi-modal speech enhancement. The presented BiNet adopts two independent encoders to map bone-conducted speech and noisy air-conducted speech into a shared embedding space. Subsequently, the decoder in BiNet reconstructs the target clean speech using the embedding features from both modalities. Compared to the widely adopted single-encoder networks, the presented two-tower structure can fully exploit the information intrinsic to each modality. To effectively fuse local and global speech features, we incorporate skip connections between the two encoders and the decoder in BiNet. We utilize a multi-scale mel-spectrogram loss function originally proposed for speech synthesis as the training objective for BiNet. Moreover, the two-tower structure of BiNet prompts us to leverage contrastive learning-based regularization. By controlling the similarity between the embedding features of bone-conducted speech and noisy air-conducted speech, we consider two types of regularization constraints on the two encoders of BiNet, and find that BiNet performs better as the embedding features of both modalities exhibit a higher similarity. Extensive experiments demonstrate that the proposed method outperforms single-modal and multi-modal speech enhancement systems significantly in terms of perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).},
  archive  = {J},
  author   = {Changtao Li and Feiran Yang and Jun Yang},
  doi      = {10.1109/TASLP.2024.3512207},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {163-174},
  title    = {Bone conduction-aided speech enhancement with two-tower network and contrastive learning},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum correntropy linear prediction for voice inverse filtering: Theoretical framework and practical implementation. <em>TASLPRO</em>, <em>33</em>, 152-162. (<a href='https://doi.org/10.1109/TASLP.2024.3512187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Voice inverse filtering methods aim at noninvasively estimating the glottal source information from the voice signal. These inverse filtering strategies typically rely on parametric models and variants of linear prediction for tuning the vocal tract filter. Weighted linear prediction schemes have proved to be the best performing for inverse filtering applications. However, the linear prediction and its variants are sensitive to the impulse-like acoustic excitations triggered by the abrupt glottal closure during voiced phonation. The present study examines the maximum correntropy criterion-based linear prediction (MCLP) for voice inverse filtering. Correntropy is a nonlinear, localized similarity measure inherently insensitive to peak-like outliers. Here, a theoretical framework is established for studying the properties of correntropy relevant for voice inverse filtering and for developing an algorithm to estimate vocal tract filter coefficients. The proposed algorithm results in a robust weighted linear prediction, where a correntropy weighting function is adjusted iteratively by a data-driven optimization scheme. The effects of correntropy kernel parameters on the performance of the MCLP method are analyzed. Characterization of the MCLP method for voice inverse filtering is addressed based on synthetic and natural sustained vowel signals. Simulations show that MCLP naturally overweights samples in the glottal closed phase, where the phonation model is more accurate. MCLP does not require prior information about the glottal instants, nor applying a predefined weighting function. Results show that MCLP performs similarly or better than other well-established inverse filtering methods based on weighted linear prediction.},
  archive  = {J},
  author   = {Iván A. Zalazar and Gabriel A. Alzamendi and Matías Zañartu and Gastón Schlotthauer},
  doi      = {10.1109/TASLP.2024.3512187},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {152-162},
  title    = {Maximum correntropy linear prediction for voice inverse filtering: Theoretical framework and practical implementation},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust beamforming for multispeaker audio conferencing under DOA uncertainty. <em>TASLPRO</em>, <em>33</em>, 139-151. (<a href='https://doi.org/10.1109/TASLP.2024.3512358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a robust microphone array beamforming approach specifically designed for multispeaker audio conferencing, where the directions of arrival (DOAs) of the speakers can vary. First, we address the configuration of the array geometry. To achieve a consistent spatial response across all potential directions on the $\mathsf {x-y}$ plane, we propose a hybrid array geometry that combines a concentric circular array (CCA) on the $\mathsf {x-y}$ plane with a uniform linear array (ULA) along the $\mathsf {z}$-axis. We discuss this geometry in detail and provide guidelines for selecting its parameters. Next, we focus on the beamforming strategy and introduce a method that directly controls the distortion level for signals originating from a specified near-end region (NER). In this context, we develop three high-directivity beamformers and evaluate their effectiveness based on white noise gain (WNG), directivity factor (DF), and beampattern characteristics. The proposed beamforming approach is experimentally assessed under various reverberant conditions, including both static and moving speech sources. Our results show that it outperforms existing methods in terms of perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) metrics. The improved performance is particularly pronounced in scenarios where the desired source direction differs from the nominal steering direction of the array, as well as when the reverberation level is mild.},
  archive  = {J},
  author   = {Gal Itzhak and Israel Cohen},
  doi      = {10.1109/TASLP.2024.3512358},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {139-151},
  title    = {Robust beamforming for multispeaker audio conferencing under DOA uncertainty},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bone-conducted speech codec based on AMR-WB framework and MHSA-CycleGAN network. <em>TASLPRO</em>, <em>33</em>, 124-138. (<a href='https://doi.org/10.1109/TASLP.2024.3511252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The integration of Bone-conducted Microphone (BCM) speech codec and speech bandwidth expansion in a tandem manner often suffers from the complex structure and high computational complexity. To address this problem, a bone-conducted speech codec based on the AMR-WB (Adaptive Multi-Rate Wideband) framework and MHSA-CycleGAN (Multi-Head Self-Attention CycleGAN) network is proposed in this paper. Specifically, a new AMR-WB encoder structure with bandwidth extension capabilities for BCM speech is proposed. Then, the ISF (Immittance Spectral Frequencies) parameters from BCM speech are converted into Air-conducted Microphone (ACM) speech by deep neural network for extending the high-frequency components of BCM speech. Finally, a novel CycleGAN network with multiple attention mechanisms and loss functions incorporating high-order statistical characteristics is presented to capture the nonlinear relationships among ISF parameters. The proposed method can improve speech quality during BCM speech encoding and has lower computational complexity. Simulation experiments confirm that the proposed method consistently delivers effective speech enhancement performance across various bitrates for BCM speech and reduces at least 68% run time compared with state-of-the-art methods.},
  archive  = {J},
  author   = {Xiaolong Hu and Zhe Chen and Fuliang Yin},
  doi      = {10.1109/TASLP.2024.3511252},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {124-138},
  title    = {Bone-conducted speech codec based on AMR-WB framework and MHSA-CycleGAN network},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic optimization for worst-case augmentation and classification. <em>TASLPRO</em>, <em>33</em>, 111-123. (<a href='https://doi.org/10.1109/TASLP.2024.3511264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Adversarial data augmentation techniques have recently demonstrated potential in enhancing the robustness of machine learning models by identifying potential worst-case data augmentation. However, most of the existing methods implemented a single augmentation strategy for different instances through a process of greedy search, resulting in suboptimal quality of the generated data. Furthermore, previous studies that incorporated reinforcement learning (RL) to apply unique augmentation strategies required high computational cost, as it necessitated a child network to compute the reward during the optimization process. Given these limitations, this study introduces a strategic adversarial data augmentation approach that leverages RL to search for and emulate the worst-case variations through a sequence of augmentation actions. By defining a reward function with an information-theoretic perspective along with the proper definition of state space, a proficient strategy for stacking multiple augmentation strategies can be carried out in an inexpensive way and can be smoothly integrated into classifier training, thereby enhancing model robustness against unseen noises. The proposed adversarial training method was evaluated on ten different types of unseen human-readable noises across six distinct text classification tasks. Experimental results indicate that the proposed method significantly improves model robustness in compensating for unseen noises.},
  archive  = {J},
  author   = {Jen-Tzung Chien and Mahdin Rohmatillah and Chang-Ting Chu},
  doi      = {10.1109/TASLP.2024.3511264},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {111-123},
  title    = {Strategic optimization for worst-case augmentation and classification},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USED: Universal speaker extraction and diarization. <em>TASLPRO</em>, <em>33</em>, 96-110. (<a href='https://doi.org/10.1109/TASLP.2024.3511268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speaker extraction and diarization are two enabling techniques for real-world speech applications. Speaker extraction aims to extract a target speaker's voice from a speech mixture, while speaker diarization demarcates speech segments by speaker, annotating ‘who spoke when’. Previous studies have typically treated the two tasks independently. In practical applications, it is more meaningful to have knowledge about ‘who spoke what and when’, which is captured by the two tasks. The two tasks share a similar objective of disentangling speakers. Speaker extraction operates in the frequency domain, whereas diarization is in the temporal domain. It is logical to believe that speaker activities obtained from speaker diarization can benefit speaker extraction, while the extracted speech offers more accurate speaker activity detection than the speech mixture. In this paper, we propose a unified model called Universal Speaker Extraction and Diarization (USED) to address output inconsistency and scenario mismatch issues. It is designed to manage speech mixtures with varying overlap ratios and variable number of speakers. We show that the USED model significantly outperforms the competitive baselines for speaker extraction and diarization tasks on LibriMix and SparseLibriMix datasets. We further validate the diarization performance on CALLHOME, a dataset based on real recordings, and experimental results indicate that our model surpasses recently proposed approaches.},
  archive  = {J},
  author   = {Junyi Ao and Mehmet Sinan Yıldırım and Ruijie Tao and Meng Ge and Shuai Wang and Yanmin Qian and Haizhou Li},
  doi      = {10.1109/TASLP.2024.3511268},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {96-110},
  title    = {USED: Universal speaker extraction and diarization},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified contextualized knowledge embedding method for static and temporal knowledge graph. <em>TASLPRO</em>, <em>33</em>, 82-95. (<a href='https://doi.org/10.1109/TASLP.2024.3507557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent years, there is a growing interest in knowledge graph embedding (KGE), which maps symbolic entities and relations into low-dimensional vector space to effectively represent structured data from the knowledge graph. In addition, the concept of temporal knowledge graph is proposed to document dynamically changing facts in the real world. Existing works attempt to incorporate temporal information into static KGE methods to accomplish temporal knowledge representations. However, existing static or temporal KGE approaches focus on the single query fact and ignore the query-relevant contextual information in the graph structure. This paper moves beyond the traditional way of scoring facts in distinct vector space and proposes a unified framework with pre-trained language models (PLM) to learn dynamic contextualized static/temporal knowledge graph embeddings, called CoS/TKGE. Given the query-specific subgraph, our model transforms it into an input sequence and uses the PLM to obtain the contextualized knowledge representations, which is flexible adaptive to the input graph contexts. We reformulate the link prediction task as a mask prediction problem to fine-tune the pre-trained language model. And the contrastive learning technique is employed to align dynamic contextual embeddings with static global embeddings. Experimental results on three widely used static and temporal KG datasets show the superiority of our model.},
  archive  = {J},
  author   = {Yifu Gao and Linbo Qiao and Zhen Huang and Zhigang Kan and Yongquan He and Dongsheng Li},
  doi      = {10.1109/TASLP.2024.3507557},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {82-95},
  title    = {Unified contextualized knowledge embedding method for static and temporal knowledge graph},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-aspect conditioning for diffusion-based music synthesis: Enhancing realism and acoustic control. <em>TASLPRO</em>, <em>33</em>, 68-81. (<a href='https://doi.org/10.1109/TASLP.2024.3507553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Music synthesis aims to generate audio from symbolic music representations, traditionally using techniques like concatenative synthesis and physical modeling. These methods offer good control but often lack expressiveness and realism in timbre. Recent advancements in diffusion-based models have enhanced the realism of synthesized audio, yet these models struggle with precise control over aspects like acoustics and timbre and are limited by the availability of high-quality annotated training data. In this paper, we introduce an advanced diffusion-based framework for music synthesis that further improves realism and introduces control through multi-aspect conditioning. This allows the synthesis from symbolic representations to accurately replicate specific performance and acoustic conditions. To address the need for precise multi-instrument target annotations, we propose using MIDI-aligned scores and automatic multi-instrument transcription based on neural networks. These methods effectively train our diffusion model with authentic audio, enhancing realism and capturing subtle nuances in performance and acoustics. As a second major contribution, we adopt conditioning techniques to gain control over multiple aspects, including score-related aspects like notes and instrumentation, as well as version-related aspects like performance and acoustics. This multi-aspect conditioning restores control over the music generation process, leading to greater fidelity in achieving the desired acoustic and stylistic outcomes. Finally, we validate our model's efficacy through systematic experiments, including qualitative listening tests and quantitative evaluation using Fréchet Audio Distance to assess version similarity, confirming the model's ability to generate realistic and expressive music, with acoustic control. Supporting evaluations and comparisons are detailed on our website (benadar293.github.io/multi-aspect-conditioning).},
  archive  = {J},
  author   = {Ben Maman and Johannes Zeitler and Meinard Müller and Amit H. Bermano},
  doi      = {10.1109/TASLP.2024.3507553},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {68-81},
  title    = {Multi-aspect conditioning for diffusion-based music synthesis: Enhancing realism and acoustic control},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating language model vulnerability to poisoning attacks in low-resource settings. <em>TASLPRO</em>, <em>33</em>, 54-67. (<a href='https://doi.org/10.1109/TASLP.2024.3507565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pre-trained language models are a highly effective source of knowledge transfer for natural language processing tasks, as their development represents an investment of resources beyond the reach of most researchers and end users. The widespread availability of such easily adaptable resources has enabled high levels of performance, which is especially valuable for low-resource language users who have typically been overlooked when it comes to NLP applications. However, these models introduce vulnerabilities in NLP toolchains, since they may prove vulnerable to attacks from malicious actors with access to the data used for downstream training. By perturbing instances from the training set, such attacks seek to undermine model capabilities and produce radically different outcomes during inference. We show that adversarial data manipulation has a severe effect on model performance, with BERT's performance dropping by more than 30% on average across all tasks at a poisoning ratio greater than 50%. Additionally, we conduct the first evaluation of this kind in the Basque language domain, establishing the vulnerability of low-resource models to the same form of attack.},
  archive  = {J},
  author   = {Richard Plant and Mario Valerio Giuffrida and Nikolaos Pitropakis and Dimitra Gkatzia},
  doi      = {10.1109/TASLP.2024.3507565},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {54-67},
  title    = {Evaluating language model vulnerability to poisoning attacks in low-resource settings},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport with class structure exploration for cross-domain speech emotion recognition. <em>TASLPRO</em>, <em>33</em>, 37-53. (<a href='https://doi.org/10.1109/TASLP.2024.3507578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER) has widespread applications in human-computer interaction. However, the performance of SER models often drops in domain mismatch scenarios. Although existing unsupervised domain adaptation (UDA) algorithms could mitigate the domain mismatch problem by aligning feature distributions between domains, they do not consider the distribution relationship between emotion categories, thereby decreasing the capability to discriminate emotions. Moreover, directly aligning the probability distributions of different domains overlooks the emotional structure information contained in the target domain samples. To overcome this limitation, this paper proposes a novel UDA for cross-domain SER, termed Optimal Transport (OT) with Class Structure Exploration (OTCSE). OTCSE aims to measure the global probability distribution distance (GPDD) differences while considering the distribution relationship between emotional categories. Additionally, it explores the intrinsic structure information in the target samples. Specifically, we first embed joint OT into a deep SER framework to measure the GPDD between domains. Second, we propose to use a self-supervised learning (SSL)-based domain exploration module in the OT adaptation process to assist in exploring class structure information. Finally, we propose a two-step optimization strategy that allows OTCSE to update the model parameters of the SER and SSL modules end-to-end while solving the optimal transport coupling based on Sinkhorn's iteration algorithm. Experimental results in cross-domain SER demonstrate that the proposed OTCSE outperformed state-of-the-art UDAs. Further, we explore the complementarity of GPDD alignment and class structure exploration. This synergy alleviated the negative transport problem in OT and enhanced the efficiency of representation learning in SSL.},
  archive  = {J},
  author   = {Ruiteng Zhang and Jianguo Wei and Xugang Lu and Junhai Xu and Yongwei Li and Di Jin and Lin Zhang and Wenhuan Lu},
  doi      = {10.1109/TASLP.2024.3507578},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {37-53},
  title    = {Optimal transport with class structure exploration for cross-domain speech emotion recognition},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multimodal graph integration network for multimodal sentiment analysis. <em>TASLPRO</em>, <em>33</em>, 23-36. (<a href='https://doi.org/10.1109/TASLP.2024.3507576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most current models for analyzing multimodal sequences often disregard the imbalanced contributions of individual modal representations caused by varying information densities, as well as the inherent multi-relational interactions across distinct modalities. Consequently, a biased understanding of the intricate interplay among modalities may be fostered, limiting prediction accuracy and effectiveness. To address these critical issues, we propose the Adaptive Multimodal Graph Integration Network (AMGIN) for multimodal sentiment analysis. Concretely, AMGIN transforms multimodal sequences into graph structures and discriminatively fuses complex intra- and inter-modal correlations by incorporating multiple edge types. To accurately modulate each modality's contribution, we present the Adaptive Modality Adjustment Mechanism (AMAM), consisting of two primary components: a modality refinement loss (MR Loss) that selectively optimizes the parameters of unimodal branches through backpropagation according to the relative confidence of modalities, and a modality-confidence gated module (MCGM) that adaptively filters out noise according to the deviation of modality-specific representations from the shared semantic distribution. Furthermore, we introduce feature reconstruction loss as an additional constraint to prevent excessive modification. To verify the effectiveness of our proposed approach, extensive experiments are conducted on three benchmark datasets commonly used in sentiment analysis, namely IEMOCAP, CMU-MOSEI, and CMU-MOSI. Additionally, we consider the task of multimodal humor detection on the UR-FUNNY dataset. The experimental results demonstrate the superiority of AMGIN over the state-of-the-art methods.},
  archive  = {J},
  author   = {Peizhu Gong and Jin Liu and Xiliang Zhang and Xingye Li and Lai Wei and Huihua He},
  doi      = {10.1109/TASLP.2024.3507576},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {23-36},
  title    = {Adaptive multimodal graph integration network for multimodal sentiment analysis},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predict-and-update network: Audio-visual speech recognition inspired by human speech perception. <em>TASLPRO</em>, <em>33</em>, 11-22. (<a href='https://doi.org/10.1109/TASLP.2024.3507575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Audio and visual signals complement each other in human speech perception, and the same applies to automatic speech recognition. The visual signal is less evident than the acoustic signal, but more robust in a complex acoustic environment, as far as speech perception is concerned. It remains a challenge how we effectively exploit the interaction between audio and visual signals for automatic speech recognition. There have been studies using visual signals as redundant or complementary information to audio input in a synchronous manner. However, human studies suggest another mechanism that visual signal primes the listener in advance, indicating when and which frequency to attend to. To simulate such a visual cueing mechanism, we propose a Predict-and-Update Network (P&U net) for Audio-Visual Speech Recognition (AVSR). In particular, we first predict the character posteriors of the spoken words, i.e. the visual embedding, based on the visual signal. The audio signal is then conditioned on the visual embedding via a novel cross-modal Conformer, that updates the character posteriors. We validate the effectiveness of the visual cueing mechanism through extensive experiments. The proposed P&U net outperforms the state-of-the-art AVSR methods on both LRS2-BBC and LRS3-BBC datasets, with the relative Word Error Rate (WER) reductions exceeding 10% and 40% under clean and noisy conditions, respectively.},
  archive  = {J},
  author   = {Jiadong Wang and Xinyuan Qian and Haizhou Li},
  doi      = {10.1109/TASLP.2024.3507575},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {11-22},
  title    = {Predict-and-update network: Audio-visual speech recognition inspired by human speech perception},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory-tuning: A unified parameter-efficient tuning method for pre-trained language models. <em>TASLPRO</em>, <em>33</em>, 1-10. (<a href='https://doi.org/10.1109/TASLP.2024.3507558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conventional fine-tuning encounters increasing difficulties given the size of current Pre-trained Language Models, which makes parameter-efficient tuning become the focal point of frontier research. Recent advances in this field is the unified tuning methods that aim to tune the representations of both multi-head attention (MHA) and fully connected feed-forward network (FFN) simultaneously, but they rely on existing tuning methods and do not explicitly model domain knowledge for downstream tasks. In this work, we propose memory-tuning, a novel unified parameter-efficient tuning method with task-specific knowledge learning, for both MHA and FFN components in Transformer blocks. We also prove that the well-known prefix tuning is also a kind of memory tuning, which further ensures memory tuning is a genuine unified tuning method. Experiments on eight benchmark data sets including both sentence- and token-level tasks demonstrate that our method outperforms the state-of-the-art baselines even full-tuning in most cases.},
  archive  = {J},
  author   = {Wang Qi and Rui Liu and Yuan Zuo and Fengzhi Li and Yong Chen and Junjie Wu},
  doi      = {10.1109/TASLP.2024.3507558},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  pages    = {1-10},
  title    = {Memory-tuning: A unified parameter-efficient tuning method for pre-trained language models},
  volume   = {33},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
