<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg">TVCG - 199</h2>
<ul>
<li><details>
<summary>
(2025). Visual features involved in determining apparent elasticity elicit touch desire. <em>TVCG</em>, <em>31</em>(10), 9530-9536. (<a href='https://doi.org/10.1109/TVCG.2025.3590469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elastic materials often invite the direct touch of users. It is an open question how seeing elastic materials invokes touch desire. The present study proposes a novel visual feature that modulates apparent elasticity and touch desire. The stimulus for our experiment was a clip in which a computer-rendered elastic surface was indented by a needle-like bar. The features of this stimulus that we focused on were spatial deformation range and indentation depth. Observers rated the following three impressions: apparent elasticity, touch desire, and anticipated touch pleasantness. The results showed that both apparent elasticity and touch desire peaked in the middle of the spatial deformation range. The two impressions also depended on indentation depth and were highly correlated with each other. Anticipated touch pleasantness showed a different peak tendency than the other two. An additional block showed that the deformation realism was not related to the above three impressions. The results suggest that apparent elasticity eliciting touch desire can be determined in the parameter space defined by the spatial deformation range and the indentation depth.},
  archive      = {J_TVCG},
  author       = {Takahiro Kawabe and Yusuke Ujitoko},
  doi          = {10.1109/TVCG.2025.3590469},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9530-9536},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual features involved in determining apparent elasticity elicit touch desire},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural 3D face shape stylization based on single style template via weakly supervised learning. <em>TVCG</em>, <em>31</em>(10), 9522-9529. (<a href='https://doi.org/10.1109/TVCG.2025.3573690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Face shape stylization refers to transforming a realistic 3D face shape into a different style, such as a cartoon face style. To solve this problem, this paper proposes modeling this task as a deformation transfer problem. This approach significantly reduces labor costs, as the artists would only need to create a single template for each face style. Realistic facial features of the original 3D face e.g. the nose or chin shape, would thus be automatically transferred to those in the style template. Deformation transfer methods, however, have two drawbacks. They are slow and they require re-optimization for every new input face. To address these weaknesses, we propose a neural network-based 3D face shape stylization method. This method is trained through weakly supervised learning, and its template's structure is preserved using our novel template-guided mesh smoothing regularization. Our method is the first learning-based deformation transfer method for 3D face shape stylization. Its employment offers the useful and practical benefit of not requiring paired training data. The experiments show that the quality of the stylized faces obtained by our method is comparable to that of the traditional deformation transfer method, achieving an average Chamfer Distance of approximately 0.01 mm. However, our approach significantly boosts the processing speed, achieving a rate approximately 3,000 times faster than the traditional deformation transfer.},
  archive      = {J_TVCG},
  author       = {Peizhi Yan and Rabab K. Ward and Qiang Tang and Shan Du},
  doi          = {10.1109/TVCG.2025.3573690},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9522-9529},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural 3D face shape stylization based on single style template via weakly supervised learning},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on 3D single-view object reconstruction. <em>TVCG</em>, <em>31</em>(10), 9502-9521. (<a href='https://doi.org/10.1109/TVCG.2025.3591770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-view 3D object reconstruction (SVOR) aims to recover the 3D shape of an object from a single 2D image. Despite advances in deep learning (DL), challenges such as incomplete image information, scarce 3D data annotation, and highly variable object shapes still limit the performance of SVOR. Meanwhile, with the rapid development of novel view synthesis (NVS) techniques, the SVOR field has received significant advancements. However, existing reviews have not comprehensively covered the rapid developments in NVS-based approaches. This article aims to fill this gap by highlighting the latest progress in SVOR, particularly advancements related to NVS-based methods. Additionally, we observed discrepancies between existing quality evaluation metrics in SVOR and human visual perception. This is because some critical object parts are essential to consider during the evaluation. For example, when reconstructing airplanes, critical parts like the empennage and wings are often overlooked in evaluation metrics due to their smaller size compared to the fuselage. Consequently, poor reconstruction of these parts may not significantly affect overall evaluation scores. To address this issue, we propose a more comprehensive evaluation method that reflects human visual perception accurately. To achieve this, we introduce a weighted evaluation method that considers part saliency and proposes a novel technique for automatically perceiving reconstruction discrepancies. This study effectively enhances the accuracy and consistency of evaluations through these approaches, offering new insights and methodologies, filling a void in the existing literature, and providing valuable contributions to both research and practical applications in SVOR.},
  archive      = {J_TVCG},
  author       = {Chenglizhao Chen and Ziyue Xue and Longyan Yang and Zhenyu Wu and Shanchen Pang and Hong Qin},
  doi          = {10.1109/TVCG.2025.3591770},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9502-9521},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comprehensive survey on 3D single-view object reconstruction},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented vision systems: Paradigms and applications. <em>TVCG</em>, <em>31</em>(10), 9484-9501. (<a href='https://doi.org/10.1109/TVCG.2025.3587527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) has grown from specialised uses to applications for the common public. One of these developments led to Augmented Vision (AV), which enhances vision beyond traditional methods like glasses or contact lenses. This review aims to compare and categorise AV systems according to the paradigms they implement to enhance the users’ vision. Additionally, the review examines whether researchers conduct measurements and analysis on the human visual system (HVS) when evaluating their system. Such an overall view will help future researchers position their work on AV. By understanding AV systems’ paradigms and approaches, researchers will be well-equipped to identify gaps, explore novel directions, and leverage existing advancements. We searched Scopus, Web of Science, and PubMed databases for publications until February 26, 2025, exploring citations and references for the selected articles to avoid missing out on relevant articles. We then conducted a two-step screening process that involved LLM-assisted screening of the article’s abstracts and an in-depth assessment of the article. This review follows the PRISMA statement, reducing bias risk. We selected 113 of 469 articles, as they improved users’ visual performance. We defined three main categories: (1) adding light to the incoming light field, (2) modifying the incoming light field, and (3) intersecting approaches. We found three main application areas: (1) task-specific, (2) vision correction, and (3) visual perception enhancement. The most typical application is task-specific. We identified a gap in the literature since just four of the papers we reviewed measured and analysed the accommodation while utilising the device.},
  archive      = {J_TVCG},
  author       = {Cristian Rendon-Cardona and Marie-Anne Burcklen and Richard Legras and Christian Sandor},
  doi          = {10.1109/TVCG.2025.3587527},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9484-9501},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented vision systems: Paradigms and applications},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on quality metrics for text-to-image generation. <em>TVCG</em>, <em>31</em>(10), 9464-9483. (<a href='https://doi.org/10.1109/TVCG.2025.3585077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-based text-to-image models do not only excel at generating realistic images, they also give designers more and more fine-grained control over the image content. Consequently, these approaches have gathered increased attention within the computer graphics research community, which has been historically devoted towards traditional rendering techniques, that offer precise control over scene parameters (e.g., objects, materials, and lighting). While the quality of conventionally rendered images is assessed through well established image quality metrics, such as SSIM or PSNR, the unique challenges of text-to-image generation require other, dedicated quality metrics. These metrics must be able to not only measure overall image quality, but also how well images reflect given text prompts, whereby the control of scene and rendering parameters is interweaved. Within this survey, we provide a comprehensive overview of such text-to-image quality metrics, and propose a taxonomy to categorize these metrics. Our taxonomy is grounded in the assumption, that there are two main quality criteria, namely compositional quality and general quality, that contribute to the overall image quality. Besides the metrics, this survey covers dedicated text-to-image benchmark datasets, over which the metrics are frequently computed. Finally, we identify limitations and open challenges in the field of text-to-image generation, and derive guidelines for practitioners conducting text-to-image evaluation.},
  archive      = {J_TVCG},
  author       = {Sebastian Hartwig and Dominik Engel and Leon Sick and Hannah Kniesel and Tristan Payer and Poonam Poonam and Michael Glöckler and Alex Bäuerle and Timo Ropinski},
  doi          = {10.1109/TVCG.2025.3585077},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9464-9483},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on quality metrics for text-to-image generation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural implicit representations for multi-view surface reconstruction: A survey. <em>TVCG</em>, <em>31</em>(10), 9444-9463. (<a href='https://doi.org/10.1109/TVCG.2025.3582627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverging from conventional explicit geometric representations, neural implicit representations utilize continuous function approximators to encode 3D surfaces through parametric formulations including signed distance fields (SDF), unsigned distance fields (UDF), occupancy fields (OF), and neural radiance fields (NeRF). These approaches demonstrate superior multi-view reconstruction fidelity by inherently supporting non-manifold geometries and complex topological variations, establishing themselves as foundational tools in 3D reconstruction. Neural implicit representations can be applied to a diverse array of reconstruction tasks, including object-level reconstruction, scene-level reconstruction, open-surface reconstruction and dynamic reconstruction. The exponential advancement of neural implicit representations in 3D reconstruction necessitates systematic analysis of their evolving methodologies and applications. This survey presents a structured synthesis of cutting-edge research from 2020-2025, establishing a dual-axis taxonomy that categorizes techniques by geometric representation types and application scenarios. Through this survey, we aim to familiarize emerging researchers with the current landscape of neural implicit representation in surface reconstruction, assess innovative contributions and limitations in existing research, and encourage prospective research directions.},
  archive      = {J_TVCG},
  author       = {Xinyun Zhang and Ruiqi Yu and Shuang Ren},
  doi          = {10.1109/TVCG.2025.3582627},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9444-9463},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural implicit representations for multi-view surface reconstruction: A survey},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handcrafted local feature descriptor-based point cloud registration and its applications: A review. <em>TVCG</em>, <em>31</em>(10), 9424-9443. (<a href='https://doi.org/10.1109/TVCG.2025.3569894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration serves as a fundamental problem across multiple fields including computer vision, computer graphics, and remote sensing. While local feature descriptors (LFDs) have long been established as a cornerstone for point cloud registration and the LFD-based approach has been extensively studied, the field has witnessed significant advancements in recent years. Despite these developments, the research community lacks a systematic review to consolidate these contributions, leaving many researchers unaware of recent progress in LFD-based registration. To address this gap, we present a comprehensive review that critically examines both state-of-the-art and widely referenced methods across all subtasks of LFD-based registration. Our work provides: (1) an extensive survey of existing methodologies, (2) in-depth analysis of their respective strengths and limitations, (3) insightful observations and practical recommendations, and (4) a thorough summary of relevant applications and publicly available datasets. This systematic overview offers valuable guidance for researchers pursuing future investigations in this domain.},
  archive      = {J_TVCG},
  author       = {Wuyong Tao and Ruisheng Wang and Xianghong Hua and Jingbin Liu and Xijiang Chen and Yufu Zang and Dong Chen and Dong Xu and Bufan Zhao},
  doi          = {10.1109/TVCG.2025.3569894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9424-9443},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handcrafted local feature descriptor-based point cloud registration and its applications: A review},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical analysis of the usage of dimensionality reduction in four domains. <em>TVCG</em>, <em>31</em>(10), 9405-9423. (<a href='https://doi.org/10.1109/TVCG.2025.3567989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is used as an important tool for unraveling the complexities of high-dimensional datasets in many fields of science, such as cell biology, chemical informatics, and physics. Visualizations of the dimensionally-reduced data enable scientists to delve into the intrinsic structures of their datasets and align them with established hypotheses. Visualization researchers have thus proposed many dimensionality reduction methods and interactive systems designed to uncover latent structures. At the same time, different scientific domains have formulated guidelines or common workflows for using dimensionality reduction techniques and visualizations for their respective fields. In this work, we present a critical analysis of the usage of dimensionality reduction in scientific domains outside of computer science. First, we conduct a bibliometric analysis of 21,249 academic publications that use dimensionality reduction to observe differences in the frequency of techniques across fields. Next, we conduct a survey of a 71-paper sample from four fields: biology, chemistry, physics, and business. Through this survey, we uncover common workflows, processes, and usage patterns, including the mixed use of confirmatory data analysis to validate a dataset and projection method and exploratory data analysis to then generate more hypotheses. We also find that misinterpretations and inappropriate usage is common, particularly in the visual interpretation of the resulting dimensionally reduced view. Lastly, we compare our observations with recent works in the visualization community in order to match work within our community to potential areas of impact outside our community. By comparing the usage found within scientific fields to the recent research output of the visualization community, we offer both validation of the progress of visualization research into dimensionality reduction and a call for action to produce techniques that meet the needs of scientific users.},
  archive      = {J_TVCG},
  author       = {Dylan Cashman and Mark Keller and Hyeon Jeon and Bum Chul Kwon and Qianwen Wang},
  doi          = {10.1109/TVCG.2025.3567989},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9405-9423},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A critical analysis of the usage of dimensionality reduction in four domains},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human performance and perception of uncertainty visualizations in geospatial applications: A scoping review. <em>TVCG</em>, <em>31</em>(10), 9387-9404. (<a href='https://doi.org/10.1109/TVCG.2025.3554969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geospatial data are often uncertain due to measurement, spatial, or temporal limitations. A knowledge gap exists about how geospatial uncertainty visualization techniques influence human factors measures. This comprehensive review synthesized the current literature on visual representations of uncertainty in geospatial data applications, identifying the breadth of techniques and the relationships between strategies and human performance and perception outcomes. Eligible articles described and evaluated at least one method for representing uncertainty in geographical data with participants, including land, ocean, weather, climate, and positioning data. Forty articles were included. Uncertainty was visualized using multivariate and univariate maps through colours, shapes, boundary regions, textures, symbols, grid noise, and text. There were varying effects, and no definitive superior method was identified. The predominant user focus was on novices. Trends were observed in supporting users understand uncertainty, user preferences, confidence, decision-making performance, and response times for different techniques and application contexts. The findings highlight the impacts of different categorizations within colour and shape techniques, heterogeneity in perception and performance evaluation, performance and perception mismatch, and differences and similarities between novices and experts. Contextual factors and user characteristics, including understanding the decision-maker's tasks, user type, and desired outcomes for decision-support appear to be important factors influencing the design of effective uncertainty visualizations. Future research on geospatial applications of uncertainty visualizations can expand on the observed trends with consistent and standardized measurement and reporting, further explore human performance and perception impacts with 3-dimensional and interactive uncertainty visualizations, and perform real-world evaluations within various contexts.},
  archive      = {J_TVCG},
  author       = {Ryan Tennant and Tania Randall},
  doi          = {10.1109/TVCG.2025.3554969},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9387-9404},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Human performance and perception of uncertainty visualizations in geospatial applications: A scoping review},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of deep learning in sports applications: Perception, comprehension, and decision. <em>TVCG</em>, <em>31</em>(10), 9368-9386. (<a href='https://doi.org/10.1109/TVCG.2025.3554801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has the potential to revolutionize sports performance, with applications ranging from perception and comprehension to decision. This article presents a comprehensive survey of deep learning in sports performance, focusing on three main aspects: algorithms, datasets and virtual environments, and challenges. First, we discuss the hierarchical structure of deep learning algorithms in sports performance which includes perception, comprehension and decision while comparing their strengths and weaknesses. Second, we list widely used existing datasets in sports and highlight their characteristics and limitations. Finally, we summarize current challenges and point out future trends of deep learning in sports. Our survey provides valuable reference material for researchers interested in deep learning in sports applications.},
  archive      = {J_TVCG},
  author       = {Zhonghan Zhao and Wenhao Chai and Shengyu Hao and Wenhao Hu and Guanhong Wang and Shidong Cao and Mingli Song and Jenq-Neng Hwang and Gaoang Wang},
  doi          = {10.1109/TVCG.2025.3554801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9368-9386},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of deep learning in sports applications: Perception, comprehension, and decision},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey on 3D reconstruction techniques: Large-scale urban city reconstruction and requirements. <em>TVCG</em>, <em>31</em>(10), 9343-9367. (<a href='https://doi.org/10.1109/TVCG.2025.3540669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D representations of large-scale and urban scenes are crucial across various industries, including autonomous driving, urban planning, natural resource supervision and many more. Large-scale industrial reconstructions are inherently complex and multifaceted. Many existing surveys primarily focus on academic progressions and often neglect the intricate and diverse needs of industry. This survey aims to bridge this gap by providing a comprehensive analysis of 3D reconstruction methods, with a focus on industrial requirements such as scalability and integration of human interaction. Our approach involves utilizing Affinity Diagramming to systematically analyze qualitative data gathered from industrial partners. This methodology enables us to gain deep insights into how recent literature addresses these specific industrial needs. The survey encompasses various aspects, including input and reconstruction modalities, architectural models, datasets, evaluation metrics, and the incorporation of prior knowledge. We further discuss practical implications derived from our analysis, highlighting key considerations for future advancements in 3D reconstruction methods tailored for large-scale applications.},
  archive      = {J_TVCG},
  author       = {Andreas Christodoulides and Gary K. L. Tam and James Clarke and Richard Smith and Jon Horgan and Nicholas Micallef and Jeremy Morley and Nelly Villamizar and Sean Walton},
  doi          = {10.1109/TVCG.2025.3540669},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9343-9367},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey on 3D reconstruction techniques: Large-scale urban city reconstruction and requirements},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided colorization state-of-the-science: A survey. <em>TVCG</em>, <em>31</em>(10), 9324-9342. (<a href='https://doi.org/10.1109/TVCG.2025.3543527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews published research in the field of computer-aided colorization technology. We argue that within this context, the colorization task can be considered to originate from computer graphics, advance by introducing computer vision, and progress towards the fusion of vision and graphics. Hence, we propose a specific taxonomy and organize the research work chronologically. We extend the existing reconstruction-based colorization evaluation techniques on the basis that aesthetic assessment should be introduced to ensure the computer-coloredimages closely satisfy human visual-related requirements. We then perform an aesthetic assessment using the proposed metric and existing evaluations, comparing the colorization performance of seven representative unconditional colorization models. Finally, we identify unresolved issues and propose fruitful areas for future research and development.},
  archive      = {J_TVCG},
  author       = {Yu Cao and Xin Duan and Xiangqiao Meng and P. Y. Mok and Ping Li and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2025.3543527},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9324-9342},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computer-aided colorization state-of-the-science: A survey},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging foundation models for crafting narrative visualization: A survey. <em>TVCG</em>, <em>31</em>(10), 9303-9323. (<a href='https://doi.org/10.1109/TVCG.2025.3542504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Narrative visualization transforms data into engaging stories, making complex information accessible to a broad audience. Foundation models, with their advanced capabilities such as natural language processing, content generation, and multimodal integration, hold substantial potential for enriching narrative visualization. Recently, a collection of techniques have been introduced for crafting narrative visualizations based on foundation models from different aspects. We build our survey upon 66 articles to study how foundation models can progressively engage in this process and then propose a reference model categorizing the reviewed literature into four essential phases: Analysis, Narration, Visualization, and Interaction. Furthermore, we identify eight specific tasks (e.g., Insight Extraction and Authoring) where foundation models are applied across these stages to facilitate the creation of visual narratives. Detailed descriptions, related literature, and reflections are presented for each task. To make it a more impactful and informative experience for diverse readers, we discuss key research problems and provide the strengths and weaknesses in each task to guide people in identifying and seizing opportunities while navigating challenges in this field.},
  archive      = {J_TVCG},
  author       = {Yi He and Ke Xu and Shixiong Cao and Yang Shi and Qing Chen and Nan Cao},
  doi          = {10.1109/TVCG.2025.3542504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9303-9323},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leveraging foundation models for crafting narrative visualization: A survey},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex surface fabrication via developable surface approximation: A survey. <em>TVCG</em>, <em>31</em>(10), 9284-9302. (<a href='https://doi.org/10.1109/TVCG.2025.3538782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex surfaces are commonly observed in various applications and have significant value in enhancing comfort, aesthetics, and functionality. However, their fabrication often involves complex and costly processes. To simplify the fabrication difficulty, significant research has focused on using 3D developable surfaces to approximate target 3D surfaces. This process involves converting target 3D surfaces into developable surfaces and then flattening them into 2D patterns. Since the geometric and topological diversity of target surfaces, this task is both comprehensive and intricate, encompassing multiple aspects from design to fabrication. In this paper, we review relevant technologies and methods in fabrication processes, classify them, and summarize a pipeline from design to fabrication. This provides a comprehensive introduction to the field for researchers and practitioners. Through the analysis of relevant literature, we also discuss some of the research challenges and future research opportunities.},
  archive      = {J_TVCG},
  author       = {Chao Yuan and Nan Cao and Yang Shi},
  doi          = {10.1109/TVCG.2025.3538782},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9284-9302},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Complex surface fabrication via developable surface approximation: A survey},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LatticeAnalytics: Strut-level visualization and inspection of additively manufactured lattice structures. <em>TVCG</em>, <em>31</em>(10), 9266-9283. (<a href='https://doi.org/10.1109/TVCG.2025.3593230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive manufacturing (AM) is revolutionizing the production of custom components with complex internal geometries, essential for high-performance applications in diverse fields such as medicine and defense. These AM parts optimize strength while minimizing weight by utilizing internal lattice structures consisting of large quantities of small interconnected struts. However, the complexity of these structures, combined with the challenges of using X-ray Computed Tomography (XCT) data, makes validation of part reliability difficult. This ultimately inhibits the development of novel parts for our collaborating material scientists. We introduce LatticeAnalytics, a novel framework specifically designed for visual inspection of defects in these lattice structures. Our framework offers an end-to-end solution that includes the data management of XCT scans, enables remote access for geographically dispersed teams through a web-based dashboard, and incorporates novel visualizations. Our analysis is facilitated by a coarse alignment between the lattice’s nominal model, a spatial graph, and the XCT data. We employ a simple VR-based approach for fast and rough alignment, followed by an offline registration and identification of the struts. With the nodes and struts aligned and identified in the volume, our framework allows querying of subvolumes containing a single strut at multiple resolutions. This avoids computation over the entire lattice and also allow for easy parallelization of down-stream computations, such as strut-specific metrics. To depict a fast overview of the strut quality, we introduce two innovative visual encodings, crucial for our collaborators’ research in creating novel AM parts: the Contour View and the Roughness Map, which depict critical geometrical and surface features of individual struts in standardized two 2D views. We evaluated the integrated system through expert interviews. The feedback confirms the framework’s practicality and its effectiveness in enhancing current inspection workflows. It solves major bottlenecks for our collaborators, ultimately helping them create novel parts with advanced properties.},
  archive      = {J_TVCG},
  author       = {Haichao Miao and Saurabh Narain and Vuthea Chheang and Garrett Hooten and Raiyan Seede and Pavol Klacansky and Kaila Morgen Bertsch and Gabe Guss and Brian Giera and Peer-Timo Bremer},
  doi          = {10.1109/TVCG.2025.3593230},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9266-9283},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LatticeAnalytics: Strut-level visualization and inspection of additively manufactured lattice structures},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards walkable and safe areas: DRL-based redirected walking leveraging spatial walkability entropy. <em>TVCG</em>, <em>31</em>(10), 9249-9265. (<a href='https://doi.org/10.1109/TVCG.2025.3595181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) expands the virtually reachable areas within confined physical spaces by real-walking locomotion. However, existing RDW controllers struggle with extracting spatial features, hindering the improvement for physical obstacle avoidance. To overcome this, we propose a novel spatial walkability-aware redirection controller utilizing deep reinforcement learning (DRL), which learns to enhance obstacle avoidance capability by leveraging comprehensive spatial features. Based on information entropy, we innovatively introduce the spatial walkability entropy (SWE) metric to characterize the walkability and safety of each physical position by assessing the difficulty of reaching its surroundings. Guided by this, we design a novel joint reward that considers both the SWE distribution and the user’s virtual-physical alignment, providing ample guidance for learning. Moreover, unlike existing controllers employing traditional reset strategies, we propose a novel reset method that maximizes regional entropy to guide users towards more open areas, reducing the re-collision risk. Extensive simulation experiments compare our controller with state-of-the-art (SOTA) redirection controllers. The results demonstrate that our controller significantly reduces physical collisions across various virtual-physical scenarios. Moreover, live user experiments confirm that our controller offers a superior roaming experience in practical settings.},
  archive      = {J_TVCG},
  author       = {Huiyu Li and Ying Ding and Yuang He and Linwei Fan and Xiang Xu},
  doi          = {10.1109/TVCG.2025.3595181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9249-9265},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards walkable and safe areas: DRL-based redirected walking leveraging spatial walkability entropy},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qffusion: Controllable portrait video editing via quadrant-grid attention learning. <em>TVCG</em>, <em>31</em>(10), 9237-9248. (<a href='https://doi.org/10.1109/TVCG.2025.3594004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Qffusion, a dual-frame-guided framework for portrait video editing. Specifically, we consider a design principle of “animation for editing”, and train Qffusion as a general animation framework from two still reference images while we can use it for portrait video editing easily by applying modified start and end frames as references during inference. Leveraging the powerful generative power of Stable Diffusion, we propose a Quadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which arranges the latent codes of two reference images and that of four facial conditions into a four-grid fashion, separately. Then, we fuse features of these two modalities and use self-attention for both appearance and temporal learning, where representations at different times are jointly modeled under QGA. Our Qffusion can achieve stable video editing without additional networks or complex training stages, where only the input format of Stable Diffusion is modified. Further, we propose a Quadrant-grid Propagation (QGP) inference strategy, which enjoys a unique advantage on stable arbitrary-length video generation by processing reference and condition frames recursively. Through extensive experiments, Qffusion consistently outperforms state-of-the-art techniques on portrait video editing.},
  archive      = {J_TVCG},
  author       = {Maomao Li and Lijian Lin and Yunfei Liu and Ye Zhu and Yu Li},
  doi          = {10.1109/TVCG.2025.3594004},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9237-9248},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Qffusion: Controllable portrait video editing via quadrant-grid attention learning},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dominant-eye-aware asymmetric foveated rendering for virtual reality. <em>TVCG</em>, <em>31</em>(10), 9225-9236. (<a href='https://doi.org/10.1109/TVCG.2025.3593899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the increasing computational demands of high-resolution virtual reality headsets, foveated rendering reduces pixel sampling in the peripheral regions of the visual field. However, existing methods have not fully leveraged binocular vision, particularly the dominant eye theory. In our prior work, we proposed Dominant-Eye-Aware foveated rendering optimized with Multi-Parameter foveation (DEAMP), which divided each eye’s visual field into fixed eccentricity layers ([0, 10]$^{\circ }$, [10, 22.5]$^{\circ }$, [22.5, 45]$^{\circ }$). The non-dominant eye received greater foveation within the same layers compared to the dominant eye. We further argue that the eccentricity ranges should vary between the eyes due to inter-eye, individual, and scene-specific differences. In this article, we introduce an enhanced method, Dominant-Eye-aware Asymmetric Foveated Rendering (DEA-FoR). This treats eccentricity as a new variable, allowing users to select eccentricity sets tailored to their eyes and supporting asymmetric configurations between the two eyes. Experimental results demonstrate significant improvements in rendering speed over our previous method while maintaining perceptual quality. Additionally, we found that individual differences and scene texture complexity significantly influence the eccentricity settings. This work offers new insights into perceptual differences in binocular vision and contributes to optimizing virtual reality experiences.},
  archive      = {J_TVCG},
  author       = {Zhimin Wang and Xiangyuan Gu and Feng Lu},
  doi          = {10.1109/TVCG.2025.3593899},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9225-9236},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dominant-eye-aware asymmetric foveated rendering for virtual reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PGT-NeuS: Progressive-growing tri-plane representation for neural surface reconstruction. <em>TVCG</em>, <em>31</em>(10), 9213-9224. (<a href='https://doi.org/10.1109/TVCG.2025.3590394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction from multi-view images is a long-standing problem in computer graphic. Neural 3D reconstruction, especially NeuS and its variants, has improved reconstruction quality compared to traditional methods. However, it is still a challenge for these methods to reconstruct fine-grained geometric details since the spherical harmonic positional encoding lacks the ability to express high-frequency signals. In this paper, we propose a multi-resolution tri-plane feature encoding that leverages the detail reconstruction capabilities of high-resolution tri-plane while using the smoothness of low-resolution tri-plane to suppress high-frequency artifacts. Additionally, a progressive training strategy is introduced, gradually merging scene details from coarse to fine granularity, enhancing reconstruction quality while maintaining training stability and reducing difficulty. Furthermore, to address reconstruction challenges arising from sparse viewpoints and inconsistent lighting in image datasets, we introduce normal priors as supervision and propose consistency verification for multi-view normal priors, which assesses the accuracy of normal priors and effectively supervise the reconstructed surfaces. Moreover, we propose a perturbing and fine-tuning strategy on regions of unreliable normal priors to further improve the quality of geometric surface reconstruction.},
  archive      = {J_TVCG},
  author       = {Xue-Kun Xiang and Yu-Jie Yuan and Wen-Bo Hu and Yu-Tao Liu and Yue-Wen Ma and Lin Gao},
  doi          = {10.1109/TVCG.2025.3590394},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9213-9224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PGT-NeuS: Progressive-growing tri-plane representation for neural surface reconstruction},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented reality productivity in-the-wild: A diary study of usage patterns and experiences of working with AR laptops in real-world settings. <em>TVCG</em>, <em>31</em>(10), 9195-9212. (<a href='https://doi.org/10.1109/TVCG.2025.3592962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) is increasingly positioned as a tool for knowledge work, providing beneficial affordances such as a virtually limitless display space that integrates digital information with the user’s physical surroundings. However, for AR to supplant traditional screen-based devices in knowledge work, it must support prolonged usage across diverse contexts. Until now, few studies have explored the effects, opportunities, and challenges of working in AR outside a controlled laboratory setting and for an extended duration. This gap in research limits our understanding of how users may adapt its affordances to their daily workflows and what barriers hinder its adoption. In this article, we present findings from a longitudinal diary study examining how participants incorporated an AR laptop — Sightful’s Spacetop EA — into their daily work routines. 14 participants used the device for 40-minute daily sessions over two weeks, collectively completing 103 hours of AR-based work. Through survey responses, workspace photographs, and post-study interviews, we analyzed usage patterns, workspace configurations, and evolving user perceptions. Our findings reveal key factors influencing participants’ usage of AR, including task demands, environmental constraints, social dynamics, and ergonomic considerations. We highlight how participants leveraged and configured AR’s virtual display space, along with emergent hybrid workflows that involved physical screens and tasks. Based on our results, we discuss both overlaps with current literature and new considerations and challenges for the future design of AR systems for pervasive and productive use.},
  archive      = {J_TVCG},
  author       = {Yi Fei Cheng and Ari Carden and Hyunsung Cho and Catarina G. Fidalgo and Jonathan Wieland and David Lindlbauer},
  doi          = {10.1109/TVCG.2025.3592962},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9195-9212},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented reality productivity in-the-wild: A diary study of usage patterns and experiences of working with AR laptops in real-world settings},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of avatar visibility and perspective on social presence and performance in dynamic VR collaboration tasks. <em>TVCG</em>, <em>31</em>(10), 9179-9194. (<a href='https://doi.org/10.1109/TVCG.2025.3591830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic collaboration is a common form of human interaction in everyday life. With the increasing support of virtual reality (VR) applications for such collaborative joint actions, understanding how visualization design choices affect collaboration becomes crucial. However, the impact of two key elements — avatar visibility and perspective — remains underexplored in dynamic collaborative tasks. To address this, we conducted two user studies representing distinct collaborative contexts: a proximal task and a spatially distributed task. Within these contexts, we investigated how varying levels of avatar visibility (full-body, upper-body, and head-and-hands) and perspectives (first-person perspective, 1PP, and third-person perspective, 3PP) influenced social presence and collaborative performance, including both task completion and team coordination aspects. Our findings revealed that 3PP significantly enhanced both social presence and team coordination compared to 1PP. The impact of avatar visibility was context-dependent: in environments with low dynamic complexity, even low-visibility avatars maintained effective collaboration, while complex spatial tasks benefited from increased avatar visibility. Furthermore, our research demonstrated that 3PP mitigated the negative effects of low-visibility avatars in 1PP, lessening coordination conflicts in spatially complex scenarios. Based on these findings, we propose design recommendations for avatar visibility and perspective choices in dynamic collaborative environments. Our research advances the understanding of how these fundamental visual design elements shape VR collaboration, informing the design of future interactive environments.},
  archive      = {J_TVCG},
  author       = {Zixun Wang and Xiangdong Li and Jinghua Huang and Yinghan Jin and Yao Chen and Dongliang Zhang},
  doi          = {10.1109/TVCG.2025.3591830},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9179-9194},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of avatar visibility and perspective on social presence and performance in dynamic VR collaboration tasks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-divided learning of fine-grained clothing behavior via flexible dynamic graphs. <em>TVCG</em>, <em>31</em>(10), 9166-9178. (<a href='https://doi.org/10.1109/TVCG.2025.3591816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in neural simulation techniques for clothing animation, these methods struggle to capture the dynamic details of garments during movement. This limitation restricts their applicability in scenarios where high-quality garment deformation is essential. To address this challenge, we introduce a novel graph learning-based approach to enhance deformation realism through designed mechanisms for mesh information propagation and external optimization strategies during model training. First, we address the issue of over-smoothing common in conventional graph processing techniques by introducing a flexible message-passing method. This approach effectively manages node interactions within the mesh, thereby improving the expressiveness of the model. Furthermore, acknowledging that uniform model supervision typically neglects high-frequency details during optimization, we analyze the spectral properties of clothing meshes. Based on this analysis, we introduce a frequency-division constraint aligned with the characteristics of different frequency bands, which aids in precisely controlling the generation of details. Our model further integrates self-collision and other physics-aware losses, enabling the learning of generalized and fine-grained dynamic deformations. Extensive evaluations and comparisons demonstrate the effectiveness of our approach, showing notable improvements over existing state-of-the-art solutions.},
  archive      = {J_TVCG},
  author       = {Tianxing Li and Rui Shi and Takashi Kanai and Qing Zhu},
  doi          = {10.1109/TVCG.2025.3591816},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9166-9178},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Frequency-divided learning of fine-grained clothing behavior via flexible dynamic graphs},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CS-net: Contribution-based sampling network for point cloud simplification. <em>TVCG</em>, <em>31</em>(10), 9154-9165. (<a href='https://doi.org/10.1109/TVCG.2025.3591189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud sampling plays a crucial role in reducing computation costs and storage requirements for various vision tasks. Traditional sampling methods, such as farthest point sampling, lack task-specific information and, as a result, cannot guarantee optimal performance in specific applications. Learning-based methods train a network to sample the point cloud for the targeted downstream task. However, they do not guarantee that the sampled points are the most relevant ones. Moreover, they may result in duplicate sampled points, which requires completion of the sampled point cloud through post-processing techniques. To address these limitations, we propose a contribution-based sampling network (CS-Net), where the sampling operation is formulated as a Top-$k$ operation. To ensure that the network can be trained in an end-to-end way using gradient descent algorithms, we use a differentiable approximation to the Top-$k$ operation via entropy regularization of an optimal transport problem. Our network consists of a feature embedding module, a cascade attention module, and a contribution scoring module. The feature embedding module includes a specifically designed spatial pooling layer to reduce parameters while preserving important features. The cascade attention module combines the outputs of three skip connected offset attention layers to emphasize the attractive features and suppress less important ones. The contribution scoring module generates a contribution score for each point and guides the sampling process to prioritize the most important ones. Experiments on the ModelNet40 and PU147 showed that CS-Net achieved state-of-the-art performance in two semantic-based downstream tasks (classification and registration) and two reconstruction-based tasks (compression and surface reconstruction). CS-Net also achieved high average precision for objection detection on the KITTI LiDAR point cloud dataset, demonstrating its effectiveness in three-dimensional object detection.},
  archive      = {J_TVCG},
  author       = {Tian Guo and Chen Chen and Hui Yuan and Xiaolong Mao and Raouf Hamzaoui and Junhui Hou},
  doi          = {10.1109/TVCG.2025.3591189},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9154-9165},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CS-net: Contribution-based sampling network for point cloud simplification},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A differentiable material point method framework for shape morphing. <em>TVCG</em>, <em>31</em>(10), 9140-9153. (<a href='https://doi.org/10.1109/TVCG.2025.3591729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel, physically-based morphing technique for elastic shapes, leveraging the differentiable material point method (MPM) with space-time control through per-particle deformation gradients to accommodate complex topology changes. This approach, grounded in MPM’s natural handling of dynamic topologies, is enhanced by a chained iterative optimization technique, allowing for the creation of both succinct and extended morphing sequences that maintain coherence over time. Demonstrated across various challenging scenarios, our method is able to produce detailed elastic deformation and topology transitions, all grounded within our physics-based simulation framework.},
  archive      = {J_TVCG},
  author       = {Michael Xu and Chang-Yong Song and David Levin and David Hyde},
  doi          = {10.1109/TVCG.2025.3591729},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9140-9153},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A differentiable material point method framework for shape morphing},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A rule-based optimization method for tooth alignment. <em>TVCG</em>, <em>31</em>(10), 9124-9139. (<a href='https://doi.org/10.1109/TVCG.2025.3591425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While tooth alignment is crucial for digital dentistry, especially in orthodontic treatment, existing computer-aided methods mainly focus on the 3D dental crown but overlook the entire teeth, which is essential for applications in orthodontics. Besides, clinical orthodontic rules are not fully considered in these methods, i.e., there should be no collisions and gaps between teeth, the upper jaw and lower jaw should have correct occlusion relationships, the teeth should comply with a reasonable dental arch curve, etc. To generate optimal tooth alignment results, we propose a rule-based optimization method for solving the tooth alignment problem that takes into consideration the clinical rules functionally and aesthetically. We optimize rule-driven objective functions by adjusting the 6-DoF transformations of each tooth. Besides, our optimization formulation supports customization for different clinical scenarios by specifying the various energy terms. Extensive experiments, ablation studies, and user studies have been conducted to validate the effectiveness of our method. Quantitative and qualitative comparisons demonstrate that our method generates better tooth alignments than previous methods.},
  archive      = {J_TVCG},
  author       = {Yuhan Ping and Guodong Wei and Guangshun Wei and Congyi Zhang and Noha A. SAID and Jia Pan and Shiqing Xin and Yuanfeng Zhou and Changhe Tu and Min Gu and Wenping Wang},
  doi          = {10.1109/TVCG.2025.3591425},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9124-9139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A rule-based optimization method for tooth alignment},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Working in extended reality in the wild: Worker and bystander experiences of XR virtual displays in public real-world settings. <em>TVCG</em>, <em>31</em>(10), 9104-9123. (<a href='https://doi.org/10.1109/TVCG.2025.3589283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although access to sufficient screen space is crucial to knowledge work, workers often find themselves with limited access to display infrastructure in remote or public settings. While virtual displays can be used to extend the available screen space through extended reality (XR) head-worn displays (HWD), we must better understand the implications of working with them in public settings from both users’ and bystanders’ viewpoints. To this end, we conducted two user studies. We first explored the usage of a hybrid AR display across real-world settings and tasks. We focused on how users take advantage of virtual displays and what social and environmental factors impact their usage of the system. A second study investigated the differences between working with a laptop, an AR system, or a VR system in public. We focused on a single location and participants performed a predefined task to enable direct comparisons between the conditions while also gathering data from bystanders. The combined results suggest a positive acceptance of XR technology in public settings and show that virtual displays can be used to accompany existing devices. We highlighted some environmental and social factors. We saw that previous XR experience and personality can influence how people perceive the use of XR in public. In addition, we confirmed that using XR in public still makes users stand out and that bystanders are curious about the devices, yet have no clear understanding of how they can be used.},
  archive      = {J_TVCG},
  author       = {Leonardo Pavanatto and Verena Biener and Jennifer Chandran and Snehanjali Kalamkar and Feiyu Lu and John J. Dudley and Jinghui Hu and G. Nikki Ramirez and Per Ola Kristensson and Alexander Giovannelli and Luke Schlueter and Jörg Müller and Jens Grubert and Doug A. Bowman},
  doi          = {10.1109/TVCG.2025.3589283},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9104-9123},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Working in extended reality in the wild: Worker and bystander experiences of XR virtual displays in public real-world settings},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the effectiveness of mixed reality as a simulation tool for augmented reality office applications. <em>TVCG</em>, <em>31</em>(10), 9092-9103. (<a href='https://doi.org/10.1109/TVCG.2025.3590002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual content in augmented reality (AR) applications can be tailored to a designer’s specifications. However, real-world environments are challenging to control precisely or replicate fully. Consequently, prototyping AR applications for specific environments is often difficult. One potential solution is employing mixed reality (MR) to simulate an AR system, enabling controlled experiments. Nevertheless, the effectiveness of using MR to simulate AR office work remains underexplored. In this paper, we report the results of a user study (N = 40) that investigated the impact of an MR simulation of an AR office on participants’ task performance and cognitive workload (CWL). Participants completed several office tasks in both an AR scene featuring a virtual monitor and an MR-simulated AR scene. During these tasks, CWL was measured using electroencephalography (EEG) and a subjective questionnaire. The results show that the performance of the pass-through window is a major constraint on the effectiveness of the MR simulation office. Finally, we discuss the study’s limitations and directions for future research.},
  archive      = {J_TVCG},
  author       = {Tianyu Liu and Weiping He and Mark Billinghurst},
  doi          = {10.1109/TVCG.2025.3590002},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9092-9103},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Assessing the effectiveness of mixed reality as a simulation tool for augmented reality office applications},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Helveg: Diagrams for software documentation. <em>TVCG</em>, <em>31</em>(10), 9079-9091. (<a href='https://doi.org/10.1109/TVCG.2025.3589748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software developers often have to gain an understanding of a codebase. Be it programmers getting onboarded onto a team project or, for example, developers striving to grasp an external open-source library. In either case, they frequently turn to the project’s documentation. However, documentation in its traditional textual form is ill-suited for this kind of high-level exploratory analysis, since it is immutable from the readers’ perspective and thus forces them to follow a predefined path. We have designed an approach bringing aspects of software architecture visualization to API reference documentation. It utilizes a highly interactive node-link diagram with expressive node glyphs and flexible filtering capabilities, providing a high-level overview of the codebase as well as details on demand. To test our design, we have implemented a prototype named Helveg, capable of automatically generating diagrams of C# codebases. User testing of Helveg confirmed its potential, but it also revealed problems with the readability, intuitiveness, and user experience of our tool. Therefore, in this paper, which is an extended version of our VISSOFT paper with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems through major changes to the glyph design, means of interaction, and user interface of the tool. To assess the improvements, this new version of Helveg was evaluated again with the same group of participants as the previous version.},
  archive      = {J_TVCG},
  author       = {Adam Štěpánek and David Kuťák and Barbora Kozlíková and Jan Byška},
  doi          = {10.1109/TVCG.2025.3589748},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9079-9091},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Helveg: Diagrams for software documentation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentiable collision-supervised tooth arrangement network with a decoupling perspective. <em>TVCG</em>, <em>31</em>(10), 9066-9078. (<a href='https://doi.org/10.1109/TVCG.2025.3589215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tooth arrangement is an essential step in the digital orthodontic planning process. Existing learning-based methods use hidden teeth features to directly regress teeth motions, which couples target pose perception and motion regression. It could lead to poor perceptions of three-dimensional transformation. They also ignore the possible overlaps or gaps between teeth of predicted dentition, which is generally unacceptable. Therefore, we propose DTAN, a differentiable collision-supervised tooth arrangement network, decoupling predicting tasks and feature modeling. DTAN decouples the tooth arrangement task by first predicting the hidden features of the final teeth poses and then using them to assist in regressing the motions between the beginning and target teeth. To learn the hidden features better, DTAN also decouples the teeth-hidden features into geometric and positional features, which are further supervised by feature consistency constraints. Furthermore, we propose a novel differentiable collision loss function for point cloud data to constrain the related gestures between teeth, which can be easily extended to other 3D point cloud tasks. We propose an arch-width guided tooth arrangement network, named C-DTAN, to make the results controllable. We construct three different tooth arrangement datasets and achieve drastically improved performance on accuracy and speed compared with existing methods.},
  archive      = {J_TVCG},
  author       = {Zhihui He and Chengyuan Wang and Shidong Yang and Li Chen and Yanheng Zhou and Shuo Wang},
  doi          = {10.1109/TVCG.2025.3589215},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9066-9078},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Differentiable collision-supervised tooth arrangement network with a decoupling perspective},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCUDF2: Improving efficiency and accuracy in extracting zero level sets from unsigned distance fields. <em>TVCG</em>, <em>31</em>(10), 9052-9065. (<a href='https://doi.org/10.1109/TVCG.2025.3588659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsigned distance fields (UDFs) provide a flexible representation for models with complex topologies, but accurately extracting their zero level sets remains challenging, particularly in preserving topological correctness and fine geometric details. We present DCUDF2, an enhanced method that builds upon DCUDF to address these limitations. Our approach introduces an accuracy-aware loss function with self-adaptive weights, enabling precise geometric fitting while avoiding over-smoothing. To improve robustness, we propose a topology correction strategy that reduces the sensitivity to hyper-parameter settings. Furthermore, we develop new operations leveraging self-adaptive weights to accelerate convergence and improve runtime efficiency. Extensive experiments on diverse datasets demonstrate that DCUDF2 consistently outperforms DCUDF and existing methods in both geometric fidelity and topological accuracy.},
  archive      = {J_TVCG},
  author       = {Xuhui Chen and Fugang Yu and Fei Hou and Wencheng Wang and Zhebin Zhang and Ying He},
  doi          = {10.1109/TVCG.2025.3588659},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9052-9065},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DCUDF2: Improving efficiency and accuracy in extracting zero level sets from unsigned distance fields},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Head-EyeK: Head-eye coordination and control learned in virtual reality. <em>TVCG</em>, <em>31</em>(10), 9039-9051. (<a href='https://doi.org/10.1109/TVCG.2025.3589333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human head-eye coordination is a complex behavior, shaped by physiological constraints, psychological context, and gaze intent. Current context-specific gaze models in both psychology and graphics fail to produce plausible head-eye coordination for general patterns of human gaze behavior. In this paper, we: 1) propose and validate an experimental protocol to collect head-eye motion data during sequential look-at tasks in Virtual Reality; 2) identify factors influencing head-eye coordination using this data; and 3) introduce a head-eye coordinated Inverse Kinematic gaze model Head-EyeK that integrates these insights. Our evaluation of Head-EyeK is three-fold: we show the impact of algorithmic parameters on gaze behavior; we show a favorable comparison to prior art both quantitatively against ground-truth data, and qualitatively using a perceptual study; and we show multiple scenarios of complex gaze behavior credibly animated using Head-EyeK.},
  archive      = {J_TVCG},
  author       = {Yifang Pan and Ludwig Sidenmark and Karan Singh},
  doi          = {10.1109/TVCG.2025.3589333},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9039-9051},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Head-EyeK: Head-eye coordination and control learned in virtual reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeneticPrism: Multifaceted visualization of citation-based scholarly research evolution. <em>TVCG</em>, <em>31</em>(10), 9024-9038. (<a href='https://doi.org/10.1109/TVCG.2025.3589485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the evolution of scholarly research is essential for many real-life decision-making processes in academia, such as research planning, frontier exploration, and award selection. Popular platforms like Google Scholar and Web of Science rely on numerical indicators that are too abstract to convey the context and content of scientific research, while most existing visualization approaches on mapping science do not consider the presentation of individual scholars’ research evolution using curated self-citation data. This paper builds on our previous work and proposes an integrated pipeline to visualize a scholar’s research evolution from multiple topic facets. A novel 3D prism-shaped visual metaphor is introduced as the overview of a scholar’s research profile, whilst their scientific evolution on each topic is displayed in a more structured manner. Additional designs by topic chord diagram, streamgraph visualization, and inter-topic flow map, optimized by an elaborate layout algorithm, assist in perceiving the scholar’s scientific evolution across topics. A new six-degree-impact glyph metaphor highlights key interdisciplinary works driving the evolution. The proposed visualization methods are evaluated through case studies analyzing the careers of prestigious Turing award laureates, one major visualization venue, and a focused user study.},
  archive      = {J_TVCG},
  author       = {Ye Sun and Zipeng Liu and Yuankai Luo and Lei Xia and Lei Shi},
  doi          = {10.1109/TVCG.2025.3589485},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9024-9038},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeneticPrism: Multifaceted visualization of citation-based scholarly research evolution},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CounterCrime - Using counterfactual explanations to explore crime reduction scenarios. <em>TVCG</em>, <em>31</em>(10), 9008-9023. (<a href='https://doi.org/10.1109/TVCG.2025.3586202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the impact of socioeconomic and urban variables on crime is a complex data analysis problem. Exploring synthetic, correlation-based scenarios using changes in a set of variables could alter a region’s definition from unsafe to safe (known counterfactual explanation), which can aid decision-makers in interpreting crime in that region and define public policies to mitigate criminal activity. We propose CounterCrime, a visual analytics tool for crime analysis that uses counterfactual explanations to add insights for this problem. This tool employs various interactive visual metaphors to explore the counterfactual explorations generated in each region. To facilitate exploration, we organize our analysis at three levels: the whole city, the region group, and the regional level. This work proposes a new perspective in crime analysis by creating “what-if” scenarios and allowing decision-makers to anticipate changes that would make a region safer. The tool guides the user in selecting variables with the most significant effect in all city regions. Using a greedy strategy, the system recommends the best variables that may influence crime in unsafe regions as the user explores. Our tool allows for identifying the most appropriate counterfactual explorations at the regional level by grouping them by similarity and determining their feasibility by comparing them with existing examples in other regions. Using crime data from São Paulo, Brazil, we validated our results with case studies. These case studies reveal interesting findings; for example, scenarios that influence crime in a particular unsafe region (or set of regions) might not influence crime in other unsafe regions.},
  archive      = {J_TVCG},
  author       = {Marcos M. Raimundo and Germain Garcia-Zanabria and Luis Gustavo Nonato and Jorge Poco},
  doi          = {10.1109/TVCG.2025.3586202},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {9008-9023},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CounterCrime - Using counterfactual explanations to explore crime reduction scenarios},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOST: Motion diffusion model for rare text via temporal clip banzhaf interaction. <em>TVCG</em>, <em>31</em>(10), 8994-9007. (<a href='https://doi.org/10.1109/TVCG.2025.3588509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MOST, a novel MOtion diffuSion model via Temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST’s retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.},
  archive      = {J_TVCG},
  author       = {Yin Wang and Mu Li and Zhiying Leng and Frederick W. B. Li and Xiaohui Liang},
  doi          = {10.1109/TVCG.2025.3588509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8994-9007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MOST: Motion diffusion model for rare text via temporal clip banzhaf interaction},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of data augmentation for learning-driven scientific visualization. <em>TVCG</em>, <em>31</em>(10), 8981-8993. (<a href='https://doi.org/10.1109/TVCG.2025.3587685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning heavily relies on the large amount of training samples. However, in scientific visualization, due to the high computational cost, only few data are available during training, which limits the performance of deep learning. A common technique to address the data sparsity issue is data augmentation. In this paper, we present a comprehensive study on nine data augmentation techniques (i.e., noise injection, interpolation, scale, flip, rotation, variational auto-encoder, generative adversarial network, diffusion model, and implicit neural representation) for understanding their effectiveness on two scientific visualization tasks, i.e., spatial super-resolution and ambient occlusion prediction. We compare the data quality, rendering fidelity, optimization time, and memory consumption of these data augmentation techniques using several scientific datasets with various characteristics. We investigate the effects of data augmentation on the method, quantity, and diversity for these tasks with various deep learning models. Our study shows that increasing the quantity and single-domain diversity of augmented data can boost model performance, while the method and cross-domain diversity of the augmented data do not have the same impact. Based on our findings, we discuss the opportunities and future directions for scientific data augmentation.},
  archive      = {J_TVCG},
  author       = {Jun Han and Hao Zheng and Jun Tao},
  doi          = {10.1109/TVCG.2025.3587685},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8981-8993},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A study of data augmentation for learning-driven scientific visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted squared volume minimization (WSVM) for generating uniform tetrahedral meshes. <em>TVCG</em>, <em>31</em>(10), 8969-8980. (<a href='https://doi.org/10.1109/TVCG.2025.3587642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new algorithm, Weighted Squared Volume Minimization (WSVM), for generating high-quality tetrahedral meshes from closed triangle meshes. Drawing inspiration from the principle of minimal surfaces that minimize squared surface area, WSVM employs a new energy function integrating weighted squared volumes for tetrahedral elements. When minimized with constant weights, this energy promotes uniform volumes among the tetrahedra. Adjusting the weights to account for local geometry further achieves uniform dihedral angles within the mesh. The algorithm begins with an initial tetrahedral mesh generated via Delaunay tetrahedralization and proceeds by sequentially minimizing volume-oriented and then dihedral angle-oriented energies. At each stage, it alternates between optimizing vertex positions and refining mesh connectivity through the iterative process. The algorithm operates fully automatically and requires no parameter tuning. Evaluations on a variety of 3D models demonstrate that WSVM consistently produces tetrahedral meshes of higher quality, with fewer slivers and enhanced uniformity compared to existing methods.},
  archive      = {J_TVCG},
  author       = {Kaixin Yu and Yifu Wang and Peng Song and Xiangqiao Meng and Ying He and Jianjun Chen},
  doi          = {10.1109/TVCG.2025.3587642},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8969-8980},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Weighted squared volume minimization (WSVM) for generating uniform tetrahedral meshes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of ankle tendon electrical stimulation on detection threshold and applicability of redirected walking. <em>TVCG</em>, <em>31</em>(10), 8956-8968. (<a href='https://doi.org/10.1109/TVCG.2025.3588032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) is a method for exploring virtual spaces larger than physical spaces while preserving a natural walking sensation. Expanding the range of visual manipulation gains that can be applied without causing discomfort is necessary to apply RDW in practice. Ankle tendon electrical stimulation (TES) can expand the range by inducing body tilt sensation and sway. Therefore, in this study, we proposed a locomotion method that applies ankle TES to RDW. In Experiment 1, we evaluated the effect of TES on the detection threshold (DT), which is the maximal gain at which visual manipulation remains unnoticed. The results indicated that the DT was expanded when TES was applied to induce the body tilt sensation in the same direction as the RDW’s visual manipulation. Specifically, the pooled mean of the DT was expanded by more than 18%. In Experiment 2, we evaluated the applicability, a supplementary index for assessing locomotion techniques. The results demonstrated that ankle TES mitigates the reduction of the applicability, especially under a curvature gain of $\pm 0.3 [m^{-1}]$.},
  archive      = {J_TVCG},
  author       = {Takashi Ota and Keigo Matsumoto and Kazuma Aoyama and Tomohiro Amemiya and Takuji Narumi and Hideaki Kuzuoka},
  doi          = {10.1109/TVCG.2025.3588032},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8956-8968},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of ankle tendon electrical stimulation on detection threshold and applicability of redirected walking},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-spherical optimal transport for semantic alignment in text-to-3D end-to-end generation. <em>TVCG</em>, <em>31</em>(10), 8944-8955. (<a href='https://doi.org/10.1109/TVCG.2025.3586646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent CLIP-guided 3D generation methods have achieved promising results but struggle with generating faithful 3D shapes that conform with input text due to the gap between text and image embeddings. To this end, this paper proposes HOTS3D which makes the first attempt to effectively bridge this gap by aligning text features to the image features with spherical optimal transport (SOT). However, in high-dimensional situations, solving the SOT remains a challenge. To obtain the SOT map for high-dimensional features obtained from CLIP encoding of two modalities, we mathematically formulate and derive the solution based on Villani’s theorem, which can directly align two hyper-sphere distributions without manifold exponential maps. Furthermore, we implement it by leveraging input convex neural networks (ICNNs) for the optimal Kantorovich potential. With the optimally mapped features, a diffusion-based generator is utilized to decode them into 3D shapes. Extensive quantitative and qualitative comparisons with state-of-the-art methods demonstrate the superiority of HOTS3D for text-to-3D generation, especially in the consistency with text semantics.},
  archive      = {J_TVCG},
  author       = {Zezeng Li and Weimin Wang and Yuming Zhao and Wenhai Li and Na Lei and Xianfeng Gu},
  doi          = {10.1109/TVCG.2025.3586646},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8944-8955},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hyper-spherical optimal transport for semantic alignment in text-to-3D end-to-end generation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Save it for the “Hot” day: An LLM-empowered visual analytics system for heat risk management. <em>TVCG</em>, <em>31</em>(10), 8928-8943. (<a href='https://doi.org/10.1109/TVCG.2025.3586689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as “thermoglyph” and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts’ analytics needs. We conducted an experiment on information extraction, a case study on the 2022 China Heatwave, and an expert survey & interview collaborated with six domain experts, demonstrating the usefulness of our system in providing in-depth and actionable insights for heat risk management.},
  archive      = {J_TVCG},
  author       = {Haobo Li and Wong Kam-Kwai and Yan Luo and Juntong Chen and Chengzhong Liu and Yaxuan Zhang and Alexis Kai Hon Lau and Huamin Qu and Dongyu Liu},
  doi          = {10.1109/TVCG.2025.3586689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8928-8943},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Save it for the “Hot” day: An LLM-empowered visual analytics system for heat risk management},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameterize structure with differentiable template for 3D shape generation. <em>TVCG</em>, <em>31</em>(10), 8915-8927. (<a href='https://doi.org/10.1109/TVCG.2025.3583987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural representation is crucial for reconstructing and generating editable 3D shapes with part semantics. Recent 3D shape generation works employ complicated networks and structure definitions relying on hierarchical annotations and pay less attention to the details inside parts. In this paper, we propose the method that parameterizes the shared structure in the same category using a differentiable template and corresponding fixed-length parameters. Specific parameters are fed into the template to calculate cuboids that indicate a concrete shape. We utilize the boundaries of three-view renderings of each cuboid to further describe the inside details. Shapes are represented with the parameters and three-view details inside cuboids, from which the SDF can be calculated to recover the object. Benefiting from our fixed-length parameters and three-view details, our networks for reconstruction and generation are simple and effective to learn the latent space. Our method can reconstruct or generate diverse shapes with complicated details, and interpolate them smoothly. Extensive evaluations demonstrate the superiority of our method on reconstruction from point cloud, generation, and interpolation.},
  archive      = {J_TVCG},
  author       = {Changfeng Ma and Pengxiao Guo and Shuangyu Yang and Yinuo Chen and Jie Guo and Chongjun Wang and Yanwen Guo and Wenping Wang},
  doi          = {10.1109/TVCG.2025.3583987},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8915-8927},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parameterize structure with differentiable template for 3D shape generation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating user input in automated object placement for augmented reality. <em>TVCG</em>, <em>31</em>(10), 8900-8914. (<a href='https://doi.org/10.1109/TVCG.2025.3583745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object placement in Augmented Reality (AR) is crucial for creating immersive and functional experiences. However, a critical research gap exists in combining user input with efficient automated placement, particularly in understanding spatial relationships and optimal placement. This study addresses this gap by presenting a novel object placement pipeline for AR applications that balances automation with user-directed placement. The pipeline employs entity recognition, object detection, depth estimation along with spawn area allocation to create a placement system. We compared our proposed method against manual placement in a comprehensive evaluation involving 50 participants. The evaluation included user experience questionnaires, a comparative study of task performance, and post-task interviews. Results indicate that our pipeline significantly reduces task completion time while maintaining comparable accuracy to manual placement. The UEQ-S and TENS scores revealed high user satisfaction. While manual placement offered more direct control, our method provided a more streamlined, efficient experience. This study contributes to the field of object placement in AR by demonstrating the potential of automated systems to enhance user experience and task efficiency.},
  archive      = {J_TVCG},
  author       = {Jalal Safari Bazargani and Abolghasem Sadeghi-Niaraki and Soo-Mi Choi},
  doi          = {10.1109/TVCG.2025.3583745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8900-8914},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integrating user input in automated object placement for augmented reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisit point cloud quality assessment: Current advances and a multiscale-inspired approach. <em>TVCG</em>, <em>31</em>(10), 8886-8899. (<a href='https://doi.org/10.1109/TVCG.2025.3582309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for full-reference point cloud quality assessment (PCQA) has extended across various point cloud services. Unlike image quality assessment, where the reference and the distorted images are naturally aligned in coordinates and thus allow point-to-point (P2P) color assessment, the coordinates and attributes of a 3D point cloud may both suffer from distortion, making the P2P evaluation unsuitable. To address this, PCQA methods usually define a set of key points and construct a neighborhood around each key point for neighbor-to-neighbor (N2N) computation on geometry and attribute. However, state-of-the-art PCQA methods often exhibit limitations in certain scenarios due to insufficient consideration of key points and neighborhoods. To overcome these challenges, this paper proposes PQI, a simple yet efficient metric to index point cloud quality. PQI suggests using scale-wise key points to uniformly perceive distortions within a point cloud, along with a mild neighborhood size associated with each key point for compromised N2N computation. To achieve this, PQI employs a multiscale framework to obtain key points, ensuring comprehensive feature representation and distortion detection throughout the entire point cloud. Such a multiscale method merges every eight points into one in the downsampling processing, implicitly embedding neighborhood information into a single point and thereby eliminating the need for an explicitly large neighborhood. Further, within each neighborhood, simple features, such as geometry Euclidean distance difference and attribute value difference, are extracted. Feature similarity is then calculated between the reference and the distorted samples at each scale and linearly weighted to generate the final PQI score. Extensive experiments demonstrate the superiority of PQI, consistently achieving high performance across several widely recognized PCQA datasets. Moreover, PQI is highly appealing for practical applications due to its low complexity and flexible scale options.},
  archive      = {J_TVCG},
  author       = {Junzhe Zhang and Tong Chen and Dandan Ding and Zhan Ma},
  doi          = {10.1109/TVCG.2025.3582309},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8886-8899},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisit point cloud quality assessment: Current advances and a multiscale-inspired approach},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardboard controller: A cost-effective method to support complex interactions in mobile VR. <em>TVCG</em>, <em>31</em>(10), 8874-8885. (<a href='https://doi.org/10.1109/TVCG.2025.3581158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the need for high-complexity low-cost interaction methods for mobile VR, we present a Cardboard Controller, supporting 6-degree-of-freedom target selection while being made of low-cost, highly accessible materials. We present two studies, one evaluating selection activation methods, and the other comparing performance and user experience of the Cardboard Controller using ray-casting and the virtual hand. Our Cardboard Controller has comparable throughput and task completion time to similar 3D input devices and can effectively support pointing and grabbing interactions, particularly when objects are within reach. We propose guidelines for designing low-cost interaction methods and input devices for mobile VR to encourage future research towards the democratization of VR.},
  archive      = {J_TVCG},
  author       = {Kristen Grinyer and Robert J. Teather},
  doi          = {10.1109/TVCG.2025.3581158},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8874-8885},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cardboard controller: A cost-effective method to support complex interactions in mobile VR},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graphon-based visual abstraction for large multi-layer networks. <em>TVCG</em>, <em>31</em>(10), 8859-8873. (<a href='https://doi.org/10.1109/TVCG.2025.3581034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph visualization techniques provide a foundational framework for offering comprehensive overviews and insights into cloud computing systems, facilitating efficient management and ensuring their availability and reliability. Despite the enhanced computational and storage capabilities of larger-scale cloud computing architectures, they introduce significant challenges to traditional graph-based visualization due to issues of hierarchical heterogeneity, scalability, and data incompleteness. This paper proposes a novel abstraction approach to visualize large multi-layer networks. Our method leverages graphons, a probabilistic representation of network layers, to encompass three core steps: an inner-layer summary to identify stable and volatile substructures, an inter-layer mixup for aligning heterogeneous network layers, and a context-aware multi-layer joint sampling technique aimed at reducing network scale while retaining essential topological characteristics. By abstracting complex network data into manageable weighted graphs, with each graph depicting a distinct network layer, our approach renders these intricate systems accessible on standard computing hardware. We validate our methodology through case studies, quantitative experiments and expert evaluations, demonstrating its effectiveness in managing large multi-layer networks, as well as its applicability to broader network types such as transportation and social networks.},
  archive      = {J_TVCG},
  author       = {Ziliang Wu and Minfeng Zhu and Zhaosong Huang and Junxu Chen and Tiansheng Zhang and Shengbing Shi and Hao Li and Qiang Bai and Hongchao Qu and Xiuqi Huang and Wei Chen},
  doi          = {10.1109/TVCG.2025.3581034},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8859-8873},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Graphon-based visual abstraction for large multi-layer networks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring remote collaborative tasks: The impact of avatar representation on dyadic haptic interactions in shared virtual environments. <em>TVCG</em>, <em>31</em>(10), 8846-8858. (<a href='https://doi.org/10.1109/TVCG.2025.3580546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is the first to explore the interplay between haptic interaction and avatar representation in Shared Virtual Environments (SVEs). Specifically, how these factors shape users’ sense of social presence during dyadic collaborations, while assessing potential effects on task performance. In a series of experiments, participants performed the collaborative task with haptic interaction under four avatar representation conditions: avatars of both participant and partner were displayed, only the participant’s avatar was displayed, only the partner’s avatar was displayed, and no avatars were displayed. The study finds that avatar representation, especially of the partner, significantly enhances the perception of social presence, which haptic interaction alone does not fully achieve. However, neither the presence nor the type of avatar representation impacts the task performance or participants’ force effort of the task, suggesting that haptic interaction provides sufficient interaction cues for the execution of the task. These results underscore the significance of integrating both visual and haptic modalities to optimize remote collaboration experiences in virtual environments, ensuring effective communication and a strong sense of social presence.},
  archive      = {J_TVCG},
  author       = {Genki Sasaki and Hiroshi Igarashi},
  doi          = {10.1109/TVCG.2025.3580546},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8846-8858},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring remote collaborative tasks: The impact of avatar representation on dyadic haptic interactions in shared virtual environments},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VOICE: Visual oracle for interaction, conversation, and explanation. <em>TVCG</em>, <em>31</em>(10), 8828-8845. (<a href='https://doi.org/10.1109/TVCG.2025.3579956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present VOICE, a novel approach to science communication that connects large language models’ conversational capabilities with interactive exploratory visualization. VOICE introduces several innovative technical contributions that drive our conversational visualization framework. Based on the collected design requirements, we introduce a two-layer agent architecture that can perform task assignment, instruction extraction, and coherent content generation. We employ fine-tuning and prompt engineering techniques to tailor agents’ performance to their specific roles and accurately respond to user queries. Our interactive text-to-visualization method generates a flythrough sequence matching the content explanation. In addition, natural language interaction provides capabilities to navigate and manipulate 3D models in real-time. The VOICE framework can receive arbitrary voice commands from the user and respond verbally, tightly coupled with a corresponding visual representation, with low latency and high accuracy. We demonstrate the effectiveness of our approach by implementing a proof-of-concept prototype and applying it to the molecular visualization domain: analyzing three 3D molecular models with multiscale and multi-instance attributes. Finally, we conduct a comprehensive evaluation of the system, including quantitative and qualitative analyses on our collected dataset, along with a detailed public user study and expert interviews. The results confirm that our framework and prototype effectively meet the design requirements and cater to the needs of diverse target users.},
  archive      = {J_TVCG},
  author       = {Donggang Jia and Alexandra Irger and Lonni Besançon and Ondřej Strnad and Deng Luo and Johanna Björklund and Alexandre Kouyoumdjian and Anders Ynnerman and Ivan Viola},
  doi          = {10.1109/TVCG.2025.3579956},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8828-8845},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VOICE: Visual oracle for interaction, conversation, and explanation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $C^{2}D$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>D</mml:mi></mml:mrow></mml:math>: Context-aware concept decomposition for personalized text-to-image synthesis. <em>TVCG</em>, <em>31</em>(10), 8814-8827. (<a href='https://doi.org/10.1109/TVCG.2025.3579776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept decomposition is a technique for personalized text-to-image synthesis which learns textual embeddings of subconcepts from images that depicting an original concept. The learned subconcepts can then be composed to create new images. However, existing methods fail to address the issue of contextual conflicts when subconcepts from different sources are combined because contextual information remains encapsulated within the subconcept embeddings. To tackle this problem, we propose a Context-aware Concept Decomposition ($C^{2}D$) framework. Specifically, we introduce a Similarity-Guided Divergent Embedding (SGDE) method to obtain subconcept embeddings. Then, we eliminate the latent contextual dependence between the subconcept embeddings and reconstruct the contextual information using an independent contextual embedding. This independent context can be combined with various subconcepts, enabling more controllable text-to-image synthesis based on subconcept recombination. Extensive experimental results demonstrate that our method outperforms existing approaches in both image quality and contextual consistency.},
  archive      = {J_TVCG},
  author       = {Jiang Xin and Xiaonan Fang and Xueling Zhu and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TVCG.2025.3579776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8814-8827},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {$C^{2}D$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>D</mml:mi></mml:mrow></mml:math>: Context-aware concept decomposition for personalized text-to-image synthesis},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualizationary: Automating design feedback for visualization designers using large language models. <em>TVCG</em>, <em>31</em>(10), 8796-8813. (<a href='https://doi.org/10.1109/TVCG.2025.3579700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive visualization editors empower users to author visualizations without writing code, but do not provide guidance on the art and craft of effective visual communication. In this article, we explore the potential of using an off-the-shelf large language models (LLMs) to provide actionable and customized feedback to visualization designers. Our implementation, Visualizationary, demonstrates how ChatGPT can be used for this purpose through two key components: a preamble of visualization design guidelines and a suite of perceptual filters that extract salient metrics from a visualization image. We present findings from a longitudinal user study involving 13 visualization designers—6 novices, 4 intermediates, and 3 experts—who authored a new visualization from scratch over several days. Our results indicate that providing guidance in natural language via an LLM can aid even seasoned designers in refining their visualizations.},
  archive      = {J_TVCG},
  author       = {Sungbok Shin and Sanghyun Hong and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2025.3579700},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8796-8813},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizationary: Automating design feedback for visualization designers using large language models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeRF-CA: Dynamic reconstruction of X-ray coronary angiography with extremely sparse-views. <em>TVCG</em>, <em>31</em>(10), 8782-8795. (<a href='https://doi.org/10.1109/TVCG.2025.3579162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray coronary angiography (CA) remains a significant clinical problem. Existing CA reconstruction methods often require extensive user interaction or large training datasets. Recently, Neural Radiance Field (NeRF) has successfully reconstructed high-fidelity scenes in natural and medical contexts without these requirements. However, challenges such as sparse-views, intra-scan motion, and complex vessel morphology hinder its direct application to CA data. We introduce NeRF-CA, a first step toward a fully automatic 4D CA reconstruction that achieves reconstructions from sparse coronary angiograms. To the best of our knowledge, we are the first to address the challenges of sparse-views and cardiac motion by decoupling the scene into the moving coronary artery and the static background, effectively translating the problem of motion into a strength. NeRF-CA serves as a first stepping stone for solving the 4D CA reconstruction problem, achieving adequate 4D reconstructions from as few as four angiograms, as required by clinical practice, while significantly outperforming state-of-the-art sparse-view X-ray NeRF. We validate our approach quantitatively and qualitatively using representative 4D phantom datasets and ablation studies.},
  archive      = {J_TVCG},
  author       = {Kirsten W.H. Maas and Danny Ruijters and Anna Vilanova and Nicola Pezzotti},
  doi          = {10.1109/TVCG.2025.3579162},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8782-8795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRF-CA: Dynamic reconstruction of X-ray coronary angiography with extremely sparse-views},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphTrials: Visual proofs of graph properties. <em>TVCG</em>, <em>31</em>(10), 8767-8781. (<a href='https://doi.org/10.1109/TVCG.2025.3577533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph and network visualization supports exploration, analysis and communication of relational data arising in many domains: from biological and social networks, to transportation and powergrid systems. With the arrival of AI-based question-answering tools, issues of trustworthiness and explainability of generated answers motivate a significant new role for visualization. In the context of graphs, we see the need for visualizations that can convince a critical audience that an assertion (e. g., from an AI) about the graph under analysis is valid. The requirements for such representations that convey precisely one specific graph property are quite different from standard network visualization criteria which optimize general aesthetics and readability. In this paper, we aim to provide a comprehensive introduction to visual proofs of graph properties and a foundation for further research in the area. We present a framework that defines what it means to visually prove a graph property. In the process, we introduce the notion of a visual certificate, that is, a specialized faithful graph visualization that leverages the viewer’s perception, in particular, pre-attentive processing (e. g., via pop-out effects), to verify a given assertion about the represented graph. We also discuss the relationships between visual complexity, cognitive load and complexity theory, and propose a classification based on visual proof complexity. Then, we provide further examples of visual certificates for problems in different visual proof complexity classes. Finally, we conclude the paper with a discussion of the limitations of our model and some open problems.},
  archive      = {J_TVCG},
  author       = {Henry Förster and Felix Klesen and Tim Dwyer and Peter Eades and Seok-Hee Hong and Stephen Kobourov and Giuseppe Liotta and Kazuo Misue and Fabrizio Montecchiani and Alexander Pastukhov and Falk Schreiber},
  doi          = {10.1109/TVCG.2025.3577533},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8767-8781},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GraphTrials: Visual proofs of graph properties},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse code query learning for speech-driven facial animation. <em>TVCG</em>, <em>31</em>(10), 8755-8766. (<a href='https://doi.org/10.1109/TVCG.2025.3577807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven facial animation aims to synthesize lip-synchronized 3D talking faces following the given speech signal. Prior methods to this task mostly focus on pursuing realism with deterministic systems, yet characterizing the potentially stochastic nature of facial motions has been to date rarely studied. While generative modeling approaches can easily handle the one-to-many mapping by repeatedly drawing samples, ensuring a diverse mode coverage of plausible facial motions on small-scale datasets remains challenging and less explored. In this article, we propose predicting multiple samples conditioned on the same audio signal and then explicitly encouraging sample diversity to address diverse facial animation synthesis. Our core insight is to guide our model to explore the expressive facial latent space with a diversity-promoting loss such that the desired latent codes for diversification can be ideally identified. To this end, building upon the rich facial prior learned with vector-quantized variational auto-encoding mechanism, our model temporally queries multiple stochastic codes which can be flexibly decoded into a diverse yet plausible set of speech-faithful facial motions. To further allow for control over different facial parts during generation, the proposed model is designed to predict different facial portions of interest in a sequential manner, and compose them to eventually form full-face motions. Our paradigm realizes both diverse and controllable facial animation synthesis in a unified formulation. We experimentally demonstrate that our method yields state-of-the-art performance both quantitatively and qualitatively, especially regarding sample diversity.},
  archive      = {J_TVCG},
  author       = {Chunzhi Gu and Shigeru Kuriyama and Katsuya Hotta},
  doi          = {10.1109/TVCG.2025.3577807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8755-8766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Diverse code query learning for speech-driven facial animation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient non-local point cloud denoising using curvature entropy and $\gamma$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>γ</mml:mi></mml:math>-norm minimization. <em>TVCG</em>, <em>31</em>(10), 8738-8754. (<a href='https://doi.org/10.1109/TVCG.2025.3577915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-local similarity (NLS) has been successfully applied to point cloud denoising. However, existing non-local methods either involve high algorithmic complexity in capturing NLS or suffer from diminished accuracy in estimating low-rank matrices. To address these problems, we propose a Point Cloud Denoising framework using $\gamma$-norm minimization based on Curvature Entropy (PCD-$\gamma$CE) for efficiently removing noise. First, we develop a structure descriptor, which exploits Curvature Entropy (CE) to accurately capture shape variation details of Non-Local Similar Structure (NLSS), and employs Angle Subdivision (AS) of NLSS to control the complexity of initial normal matrix construction. Second, we introduce $\gamma$-norm to construct a low-rank denoising model for initial normal matrix, thereby providing a nearly unbiased estimation of rank function with better robustness to noise. Extensive experiments on synthetic and raw scanned point clouds show that our approach outperforms the popular denoising methods, with a 99.90% time reduction and gains in Mean Square Error (MSE) and Chamfer Distance (CD) compared with the Weighted Nuclear Norm Minimization (WNNM) method.},
  archive      = {J_TVCG},
  author       = {Jian Chen and Feng Gao and Pingping Chen and Weisi Lin},
  doi          = {10.1109/TVCG.2025.3577915},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8738-8754},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient non-local point cloud denoising using curvature entropy and $\gamma$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>γ</mml:mi></mml:math>-norm minimization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HaHeAE: Learning generalisable joint representations of human hand and head movements in extended reality. <em>TVCG</em>, <em>31</em>(10), 8726-8737. (<a href='https://doi.org/10.1109/TVCG.2025.3576999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human hand and head movements are the most pervasive input modalities in extended reality (XR) and are significant for a wide range of applications. However, prior works on hand and head modelling in XR only explored a single modality or focused on specific applications. We present HaHeAE – a novel self-supervised method for learning generalisable joint representations of hand and head movements in XR. At the core of our method is an autoencoder (AE) that uses a graph convolutional network-based semantic encoder and a diffusion-based stochastic encoder to learn the joint semantic and stochastic representations of hand-head movements. It also features a diffusion-based decoder to reconstruct the original signals. Through extensive evaluations on three public XR datasets, we show that our method 1) significantly outperforms commonly used self-supervised methods by up to 74.1% in terms of reconstruction quality and is generalisable across users, activities, and XR environments, 2) enables new applications, including interpretable hand-head cluster identification and variable hand-head movement generation, and 3) can serve as an effective feature extractor for downstream tasks. Together, these results demonstrate the effectiveness of our method and underline the potential of self-supervised methods for jointly modelling hand-head behaviours in extended reality.},
  archive      = {J_TVCG},
  author       = {Zhiming Hu and Guanhua Zhang and Zheming Yin and Daniel Häufle and Syn Schmitt and Andreas Bulling},
  doi          = {10.1109/TVCG.2025.3576999},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8726-8737},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HaHeAE: Learning generalisable joint representations of human hand and head movements in extended reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time translation of upper-body gestures to virtual avatars in dissimilar telepresence environments. <em>TVCG</em>, <em>31</em>(10), 8711-8725. (<a href='https://doi.org/10.1109/TVCG.2025.3577156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mixed reality (MR) avatar-mediated telepresence, avatar movement must be adjusted to convey the user’s intent in a dissimilar space. This paper presents a novel neural network-based framework designed for translating upper-body gestures, which adjusts virtual avatar movements in dissimilar environments to accurately reflect the user’s intended gestures in real-time. Our framework translates a wide range of upper-body gestures, including eye gaze, deictic gestures, free-form gestures, and the transitions between them. A key feature of our framework is its ability to generate natural upper-body gestures for users of different sizes, irrespective of handedness and eye dominance, even though the training is based on data from a single person. Unlike previous methods that require paired motion between users and avatars for training, our framework uses an unpaired approach, significantly reducing training time and allowing for generating a wider variety of motion types. These advantages were made possible by designing two separate networks: the Motion Progression Network, which interprets sparse tracking signals from the user to determine motion progression, and the Upper-body Gesture Network, which autoregressively generates the avatar’s pose based on these progressions. We demonstrate the effectiveness of our framework through quantitative comparisons with state-of-the-art methods, qualitative animation results, and a user evaluation in MR telepresence scenarios.},
  archive      = {J_TVCG},
  author       = {Jiho Kang and Taehei Kim and Hyeshim Kim and Sung-Hee Lee},
  doi          = {10.1109/TVCG.2025.3577156},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8711-8725},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time translation of upper-body gestures to virtual avatars in dissimilar telepresence environments},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). At the peak: Empirical patterns for creating climaxes in data videos. <em>TVCG</em>, <em>31</em>(10), 8696-8710. (<a href='https://doi.org/10.1109/TVCG.2025.3576597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing popularity of data videos, guidance on designing narrative climaxes that maximise viewers’ emotional engagement remains scarce. To address this gap, our work leverages emotional theory to derive patterns for crafting emotionally resonant climaxes of data videos. We first analyzed the climaxes of 96 data videos, categorizing them into eight emotional dimensions based on Plutchik’s basic emotion model. Based on data analysis, we then formulated 40 patterns for creating narrative climaxes. To evaluate the patterns when applied as design hints, we conducted a user study with 48 participants, where Group A created data video climaxes using our patterns, Group B created them without our patterns, and Group C used other patterns as the baseline. Evaluations by two experts and 20 general audiences revealed that the climaxes created with the patterns were more emotionally engaging. The participants also praised the clarity and practicality of the patterns.},
  archive      = {J_TVCG},
  author       = {Zheng Wei and Yuelu Li and Wenchuan Lu and Qiming Gu and Huamin Qu and Xian Xu},
  doi          = {10.1109/TVCG.2025.3576597},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8696-8710},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {At the peak: Empirical patterns for creating climaxes in data videos},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning of event-guided video frame interpolation for rolling shutter frames. <em>TVCG</em>, <em>31</em>(10), 8683-8695. (<a href='https://doi.org/10.1109/TVCG.2025.3576305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most consumer cameras use rolling shutter (RS) exposure, the captured videos often suffer from distortions (e.g., skew and jelly effect). Also, these videos are impeded by the limited bandwidth and frame rate, which inevitably affect the video streaming experience. In this paper, we excavate the potential of event cameras as they enjoy high temporal resolution. Accordingly, we propose a framework to recover the global shutter (GS) high frame rate (i.e., slow motion) video without RS distortion from an RS camera and event camera. One challenge is the lack of real-world datasets for supervised training. Therefore, we explore self-supervised learning with the key idea of estimating the displacement field—a non-linear and dense 3D spatiotemporal representation of all pixels during the exposure time. This allows for a mutual reconstruction between RS and GS frames and facilitates slow-motion video recovery. We then combine the input RS frames with the DF to map them to the GS frames (RS-to-GS). Given the under-constrained nature of this mapping, we integrate it with the inverse mapping (GS-to-RS) and RS frame warping (RS-to-RS) for self-supervision. We evaluate our framework via objective analysis (i.e., quantitative and qualitative comparisons on four datasets) and subjective studies (i.e., user study). The results show that our framework can recover slow-motion videos without distortion, with much lower bandwidth (94% drop) and higher inference speed ($ 16\; {\rm ms}/{\rm frame}$) under $32 \times$ frame interpolation.},
  archive      = {J_TVCG},
  author       = {Yunfan Lu and Guoqiang Liang and Yiran Shen and Lin Wang},
  doi          = {10.1109/TVCG.2025.3576305},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8683-8695},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-supervised learning of event-guided video frame interpolation for rolling shutter frames},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JailbreakLens: Visual analysis of jailbreak attacks against large language models. <em>TVCG</em>, <em>31</em>(10), 8668-8682. (<a href='https://doi.org/10.1109/TVCG.2025.3575694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of large language models (LLMs) has underscored concerns regarding their security vulnerabilities, notably against jailbreak attacks, where adversaries design jailbreak prompts to circumvent safety mechanisms for potential misuse. Addressing these concerns necessitates a comprehensive analysis of jailbreak prompts to evaluate LLMs’ defensive capabilities and identify potential weaknesses. However, the complexity of evaluating jailbreak performance and understanding prompt characteristics makes this analysis laborious. We collaborate with domain experts to characterize problems and propose an LLM-assisted framework to streamline the analysis process. It provides automatic jailbreak assessment to facilitate performance evaluation and support analysis of components and keywords in prompts. Based on the framework, we design JailbreakLens, a visual analysis system that enables users to explore the jailbreak performance against the target model, conduct multi-level analysis of prompt characteristics, and refine prompt instances to verify findings. Through a case study, technical evaluations, and expert interviews, we demonstrate our system’s effectiveness in helping users evaluate model security and identify model weaknesses.},
  archive      = {J_TVCG},
  author       = {Yingchaojie Feng and Zhizhang Chen and Zhining Kang and Sijia Wang and Haoyu Tian and Wei Zhang and Minfeng Zhu and Wei Chen},
  doi          = {10.1109/TVCG.2025.3575694},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8668-8682},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {JailbreakLens: Visual analysis of jailbreak attacks against large language models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EGAvatar: Efficient GAN inversion for generalizable head avatar from few-shot images. <em>TVCG</em>, <em>31</em>(10), 8654-8667. (<a href='https://doi.org/10.1109/TVCG.2025.3575782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable head avatar reconstruction via the inversion of few-shot images using 3D generative models has demonstrated significant potential for efficient avatar creation. However, under limited input conditions, existing one-shot inversion methods often fail to produce high-fidelity results, frequently leading to shape distortions, expression deviations, and identity inconsistencies. To address these limitations, we propose EGAvatar, a novel and efficient 3DGAN inversion framework designed to generate high-fidelity, generalizable head avatars from few-shot images. The core principle of EGAvatar is a decoupling-by-inverting strategy, built upon an animatable 3DGAN prior. Specifically, we introduce an effective animatable 3DGAN model that synthesizes high-quality 3D avatars by integrating a coarse 3D triplane representation (derived from a latent 3DGAN) with an offset 3D triplane (learned via a triplane 3DGAN). Leveraging this architecture, we design a 3DGAN-based inversion approach to reconstruct 3D avatars efficiently. Additionally, we incorporate an expression-view disentanglement mechanism to maintain consistent appearance across varying expressions and viewpoints, thereby enhancing the generalizability of avatar reconstruction from limited input images. Extensive experiments conducted on two publicly available benchmarks and a private dataset demonstrate that EGAvatar outperforms existing state-of-the-art methods in both qualitative and quantitative evaluations. Notably, EGAvatar achieves superior performance while requiring significantly fewer input images and offering more efficient training and inference.},
  archive      = {J_TVCG},
  author       = {Hao Pan Ren and Wei Duan and Wan Yu Li and Yi Liu and Yu Dong Guo and Shi-Sheng Huang and Ju Yong Zhang and Hua Huang},
  doi          = {10.1109/TVCG.2025.3575782},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8654-8667},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EGAvatar: Efficient GAN inversion for generalizable head avatar from few-shot images},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASight: Fine-tuning auto-scheduling optimizations for model deployment via visual analytics. <em>TVCG</em>, <em>31</em>(10), 8637-8653. (<a href='https://doi.org/10.1109/TVCG.2025.3574194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upon completing the design and training phases, deploying a deep learning model to specific hardware becomes necessary prior to its implementation in practical applications. To enhance the performance of the model, the developers must optimize it to decrease inference latency. Auto-scheduling, an automated approach that generates optimization schemes, offers a feasible option for large-scale auto-deployment. Nevertheless, the low-level code generated by auto-scheduling closely resembles hardware coding and may present challenges for human comprehension, thereby hindering future manual optimization efforts. In this study, we introduce ASight, a visual analytics system to assist engineers in identifying performance bottlenecks, comprehending the auto-generated low-level code, and obtaining insights from auto-scheduling optimizations. We develop a subgraph matching algorithm capable of identifying graph isomorphism among Intermediate Representations to track performance bottlenecks from low-level metrics to high-level computational graphs. To address the substantial profiling metrics involved in auto-scheduling and derive optimization design principles by summarizing commonalities among auto-scheduling optimizations, we propose an enhanced visualization for the large search space of auto-scheduling. We validate the effectiveness of ASight through two case studies, one focused on a local machine and the other on a data center, along with a quantitative experiment exploring optimization design principles.},
  archive      = {J_TVCG},
  author       = {Laixin Xie and Chenyang Zhang and Ruofei Ma and Xingxing Xing and Wei Wan and Quan Li},
  doi          = {10.1109/TVCG.2025.3574194},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8637-8653},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ASight: Fine-tuning auto-scheduling optimizations for model deployment via visual analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StructLayoutFormer: Conditional structured layout generation via structure serialization and disentanglement. <em>TVCG</em>, <em>31</em>(10), 8623-8636. (<a href='https://doi.org/10.1109/TVCG.2025.3574311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured layouts are preferable in many 2D visual contents (e.g., GUIs, webpages) since the structural information allows convenient layout editing. Computational frameworks can help create structured layouts but require heavy labor input. Existing data-driven approaches are effective in automatically generating fixed layouts but fail to produce layout structures. We present StructLayoutFormer, a novel Transformer-based approach for conditional structured layout generation. We use a structure serialization scheme to represent structured layouts as sequences. To better control the structures of generated layouts, we disentangle the structural information from the element placements. Our approach is the first data-driven approach that achieves conditional structured layout generation and produces realistic layout structures explicitly. We compare our approach with existing data-driven layout generation approaches by including post-processing for structure extraction. Extensive experiments have shown that our approach exceeds these baselines in conditional structured layout generation. We also demonstrate that our approach is effective in extracting and transferring layout structures.},
  archive      = {J_TVCG},
  author       = {Xin Hu and Pengfei Xu and Jin Zhou and Hongbo Fu and Hui Huang},
  doi          = {10.1109/TVCG.2025.3574311},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8623-8636},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StructLayoutFormer: Conditional structured layout generation via structure serialization and disentanglement},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGLDBench: A benchmark suite for stress-guided lightweight 3D designs. <em>TVCG</em>, <em>31</em>(10), 8609-8622. (<a href='https://doi.org/10.1109/TVCG.2025.3573774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite for applying and evaluating material layout strategies to generate stiff, lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, including six reference strategies and a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates the systematic analysis and comparison of design strategies based on the mechanical properties they achieve. SGLDBench enables the evaluation of diverse load conditions and, through the tight integration of the solver, supports high-resolution designs and stiffness analysis. Additionally, SGLDBench emphasizes visual analysis to explore the relationship between the geometric structure of a design and the distribution of stresses, offering insights into the specific properties and behaviors of different design strategies. SGLDBench’s specific features are highlighted through several experiments, comparing the results of reference strategies with respect to geometric and mechanical properties.},
  archive      = {J_TVCG},
  author       = {Junpeng Wang and Dennis R. Bukenberger and Simon Niedermayr and Christoph Neuhauser and Jun Wu and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2025.3573774},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8609-8622},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SGLDBench: A benchmark suite for stress-guided lightweight 3D designs},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GroupTrackVis: A visual analytics approach for online group discussion-based teaching. <em>TVCG</em>, <em>31</em>(10), 8592-8608. (<a href='https://doi.org/10.1109/TVCG.2025.3573653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online group discussions play an important role in education reform by facilitating collaborative learning and knowledge sharing among participants. However, instructors face significant challenges in monitoring discussion progress, tracking student performance and understanding interaction dynamics due to overlapping conversations, time-varying participant behaviors, and hidden interaction patterns. To address these challenges, we propose GroupTrackVis, an interactive visual analytics system that incorporates both advanced algorithms and novel visualization designs, to help instructors analyze group discussions mainly from three perspectives: topic evolution, student performance, and interaction. GroupTrackVis proposes an enhanced topic segmentation algorithm by incorporating word vector weighting and reply relationship analysis, effectively disentangling overlapping discussions. It also extracts six key behavioral attributes from multimodal educational data, offering a comprehensive view of student performance and providing insights into the key factors driving learning outcomes. Additionally, a multi-layer tree network with edge bundling techniques is implemented to clearly visualize the dynamic evolution of student interactions. The integration of algorithms with interactive visualizations enables instructors to explore discussions quickly and dynamically adjust their analysis as the discussion evolves. The effectiveness of GroupTrackVis is demonstrated through two case studies, a user study, and expert interviews, highlighting its ability to support instructors in identifying engaged and disengaged students, and tracking discussion dynamics.},
  archive      = {J_TVCG},
  author       = {Xiaoyan Kui and Min Zhang and Mingkun Zhang and Ningkai Huang and Yuqi Guo and Jingwei Liu and Chao Zhang and Jiazhi Xia},
  doi          = {10.1109/TVCG.2025.3573653},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8592-8608},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GroupTrackVis: A visual analytics approach for online group discussion-based teaching},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Into the void: Mapping the unseen gaps in high dimensional data. <em>TVCG</em>, <em>31</em>(10), 8578-8591. (<a href='https://doi.org/10.1109/TVCG.2025.3572850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive pipeline, integrated with a visual analytics system called GapMiner, capable of exploring and exploiting untapped opportunities within the empty regions of high-dimensional datasets. Our approach utilizes a novel Empty-Space Search Algorithm (ESA) to identify the center points of these uncharted voids, which represent reservoirs for potentially valuable new configurations. Initially, this process is guided by user interactions through GapMiner, which visualizes Empty-Space Configurations (ESCs) within the context of the dataset and allows domain experts to explore and refine ESCs for subsequent validation in domain experiments or simulations. These activities iteratively enhance the dataset and contribute to training a connected deep neural network (DNN). As training progresses, the DNN gradually assumes the role of identifying and validating high-potential ESCs, reducing the need for direct user involvement. Once the DNN achieves sufficient accuracy, it autonomously guides the exploration of optimal configurations by predicting performance and refining configurations through a combination of gradient ascent and improved empty-space searches. Domain experts were actively involved throughout the system’s development. Our findings demonstrate that this methodology consistently generates superior novel configurations compared to conventional randomization-based approaches. We illustrate its effectiveness in multiple case studies with diverse objectives.},
  archive      = {J_TVCG},
  author       = {Xinyu Zhang and Tyler Estro and Geoff Kuenning and Erez Zadok and Klaus Mueller},
  doi          = {10.1109/TVCG.2025.3572850},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8578-8591},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Into the void: Mapping the unseen gaps in high dimensional data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view large reconstruction model via geometry-aware positional encoding and attention. <em>TVCG</em>, <em>31</em>(10), 8564-8577. (<a href='https://doi.org/10.1109/TVCG.2025.3572341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advancements in the Large Reconstruction Model (LRM) demonstrating impressive results, when extending its input from single image to multiple images, it exhibits inefficiencies, subpar geometric and texture quality, as well as slower convergence speed than expected. It is attributed to that, LRM formulates 3D reconstruction as a naive images-to-3D translation problem, ignoring the strong 3D coherence among the input images. In this article, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to reconstruct high-quality 3D shapes from multi-views in a 3D-aware manner. Specifically, we introduce a multi-view consistent cross-attention scheme to enable M-LRM to accurately query information from the input images. Moreover, we employ the 3D priors of the input multi-view images to initialize the triplane tokens. Compared to previous methods, the proposed M-LRM can generate 3D shapes of high fidelity. Experimental studies demonstrate that our model achieves a significant performance gain and faster training convergence.},
  archive      = {J_TVCG},
  author       = {Mengfei Li and Xiaoxiao Long and Yixun Liang and Weiyu Li and Yuan Liu and Peng Li and Wenhan Luo and Wenping Wang and Yike Guo},
  doi          = {10.1109/TVCG.2025.3572341},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8564-8577},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-view large reconstruction model via geometry-aware positional encoding and attention},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What draws your attention first? an attention prediction model based on spatial features in virtual reality. <em>TVCG</em>, <em>31</em>(10), 8552-8563. (<a href='https://doi.org/10.1109/TVCG.2025.3572408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding visual attention is key to designing efficient human-computer interaction, especially for virtual reality (VR) and augmented reality (AR) applications. However, the relationship between 3D spatial attributes of visual stimuli and visual attention is still underexplored. Thus, we design an experiment to collect a gaze dataset in VR, and use it to quantitatively model the probability of first attention between two stimuli. First, we construct the dataset by presenting subjects with a synthetic VR scene containing varying spatial configurations of two spheres. Second, we formulate their selective attention based on a probability model that takes as input two view-specific stimuli attributes: their eccentricities in the field of view and their sizes as visual angles. Third, we train two models using our gaze dataset to predict the probability distribution of a user’s preferences of visual stimuli within the scene. We evaluate our method by comparing model performance across two challenging synthetic scenes in VR. Our application case study demonstrates that VR designers can utilize our models for attention prediction in two-foreground-object scenarios, which are common when designing 3D content for storytelling or scene guidance. We make the dataset and the source code to visualize it available alongside this work.},
  archive      = {J_TVCG},
  author       = {Matthew S. Castellana and Ping Hu and Doris Gutierrez and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2025.3572408},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8552-8563},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What draws your attention first? an attention prediction model based on spatial features in virtual reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A typology of decision-making tasks for visualization. <em>TVCG</em>, <em>31</em>(10), 8536-8551. (<a href='https://doi.org/10.1109/TVCG.2025.3572842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite decision-making being a vital goal of data visualization, little work has been done to differentiate decision-making tasks within the field. While visualization task taxonomies and typologies exist, they often focus on more granular analytical tasks that are too low-level to describe large complex decisions, which can make it difficult to reason about and design decision-support tools. In this paper, we contribute a typology of decision-making tasks that were iteratively refined from a list of design goals distilled from a literature review. Our typology is concise and consists of only three tasks: CHOOSE, ACTIVATE, and CREATE. Although decision types originating in other disciplines exist, we provide definitions for these tasks that are suitable for the visualization community. Our proposed typology offers two benefits. First, the ability to compose and hierarchically organize the tasks enables flexible and clear descriptions of decisions with varying levels of complexities. Second, the typology encourages productive discourse between visualization designers and domain experts by abstracting the intricacies of data, thereby promoting clarity and rigorous analysis of decision-making processes. We demonstrate the benefits of our typology through four case studies, and present an evaluation of the typology from semi-structured interviews with experienced members of the visualization community who have contributed to developing or publishing decision support systems for domain experts. Our interviewees used our typology to delineate the decision-making processes supported by their systems, demonstrating its descriptive capacity and effectiveness. Finally, we present preliminary findings on the usefulness of our typology for visualization design.},
  archive      = {J_TVCG},
  author       = {Camelia D. Brumar and Sam Molnar and Gabriel Appleby and Kristi Potter and Remco Chang},
  doi          = {10.1109/TVCG.2025.3572842},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8536-8551},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A typology of decision-making tasks for visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast intersection-free remeshing of triangular meshes. <em>TVCG</em>, <em>31</em>(10), 8519-8535. (<a href='https://doi.org/10.1109/TVCG.2025.3569926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast intersection-free remeshing of triangular meshes that robustly and efficiently generates high-quality non-intersecting meshes. Conducting intersection checks on all local operations during remeshing to prevent intersections represents the principal efficiency bottleneck. Our method is based on a key observation: intersections primarily occur in structurally complex regions. Accordingly, we develop an adaptive method to identify these key regions and perform intersection checks only for local operations within these regions during remeshing, significantly improving the algorithmic efficiency. Our method is an order of magnitude faster than traditional approaches that perform intersection checks on all local operations. Furthermore, we introduce a flip-aware extension mechanism that effectively avoids triangle flipping by constraining the optimization space of local operations, thereby avoiding the formation of irregular sharp edges. We also employ an adaptive iterative size field to eliminate banding phenomenon and propose a quasi-geometric size field adjustment method to quickly achieve smooth size transitions, thereby improving mesh quality. Compared to state-of-the-art methods, our method consistently and quickly generates higher quality non-intersecting meshes. In addition, we have validated the robustness and efficiency of our method, using all 5,469 non-intersecting valid models from the Thingi10K dataset.},
  archive      = {J_TVCG},
  author       = {Taoran Liu and Hongfei Ye and Jianjun Chen},
  doi          = {10.1109/TVCG.2025.3569926},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8519-8535},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast intersection-free remeshing of triangular meshes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MixRF: Universal mixed radiance fields with points and rays aggregation. <em>TVCG</em>, <em>31</em>(10), 8503-8518. (<a href='https://doi.org/10.1109/TVCG.2025.3572015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in neural rendering methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D-GS), have significantly revolutionized photo-realistic novel view synthesis of scenes with multiple photos or videos as input. However, existing approaches within the NeRF and 3D-GS frameworks often assume the independence of point sampling and ray casting, which are intrinsic to volume rendering and alpha-blending techniques. These underlying assumptions limit the ability to aggregate context within subspaces, such as densities and colors in the radiance fields and pixels on the image plane, leading to synthesized images that lack fine details and smoothness. To overcome this, we propose a universal framework, MixRF, comprising a Radiance Field Mixer (RF-mixer) and a Color Domain Mixer (CD-mixer), to sufficiently aggregate and fully explore information in neighboring sampled points and casting rays, separately. The RF-mixer treats sampled points as an explicit point cloud, enabling the aggregation of density and color attributes from neighboring points to better capture local geometry and appearance. Meanwhile, the CD-mixer rearranges rendered pixels on the sub-image plane, improving smoothness and recovering fine details and textures. Both mixers employ a kernel-based mixing strategy to facilitate effective and controllable attribute aggregation, ensuring a more comprehensive exploration of radiance values and pixel information. Extensive experiments demonstrate that our MixRF framework is compatible with radiance field-based methods, including NeRF and 3D-GS designs. The proposed framework dramatically enhances performance in both qualitative and quantitative evaluations, with less than a $ 25\%$ increase in computational overhead during inference.},
  archive      = {J_TVCG},
  author       = {Haiyang Bai and Tao Lu and Jiaqi Zhu and Wei Huang and Chang Gou and Jie Guo and Lijun Chen and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3572015},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8503-8518},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MixRF: Universal mixed radiance fields with points and rays aggregation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive rendering of relightable and animatable gaussian avatars. <em>TVCG</em>, <em>31</em>(10), 8491-8502. (<a href='https://doi.org/10.1109/TVCG.2025.3569923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating relightable and animatable avatars from multi-view or monocular videos is a challenging task for digital human creation and virtual reality applications. Previous methods rely on neural radiance fields or ray tracing, resulting in slow training and rendering processes. By utilizing Gaussian Splatting, we propose a simple and efficient method to decouple body materials and lighting from sparse-view or monocular avatar videos, so that the avatar can be rendered simultaneously under novel viewpoints, poses, and lightings at interactive frame rates (6.9 fps). Specifically, we first obtain the canonical body mesh using a signed distance function and assign attributes to each mesh vertex. The Gaussians in the canonical space then interpolate from nearby body mesh vertices to obtain the attributes. We subsequently deform the Gaussians to the posed space using forward skinning, and combine the learnable environment light with the Gaussian attributes for shading computation. To achieve fast shadow modeling, we rasterize the posed body mesh from dense viewpoints to obtain the visibility. Our approach is not only simple but also fast enough to allow interactive rendering of avatar animation under environmental light changes. Experiments demonstrate that, compared to previous works, our method can render higher quality results at a faster speed on both synthetic and real datasets.},
  archive      = {J_TVCG},
  author       = {Youyi Zhan and Tianjia Shao and He Wang and Yin Yang and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3569923},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8491-8502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive rendering of relightable and animatable gaussian avatars},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A potential field method for tooth motion planning in orthodontic treatment. <em>TVCG</em>, <em>31</em>(10), 8480-8490. (<a href='https://doi.org/10.1109/TVCG.2025.3567299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invisible orthodontics, commonly known as clear alignment treatment, offers a more comfortable and aesthetically pleasing alternative in orthodontic care, attracting considerable attention in the dental community in recent years. It replaces conventional metal braces with a series of removable, and transparent aligners. Each aligner is crafted to facilitate a gradual adjustment of the teeth, ensuring progressive stages of dental correction. This necessitates the design for teeth motion. Here we present an automatic method and a system for generating collision-free teeth motion planning while avoiding gaps between adjacent teeth, which is unacceptable in clinical practice. To tackle this task, we formulate it as a constrained optimization problem and utilize the interior point method for its solution. We also developed an interactive system that enables dentists to easily visualize and edit the paths. Our method significantly speeds up the clear aligner planning process, creating the desired motion paths for a full set of teeth in under five minutes—a task that typically requires several hours of manual work. Our experiments and user studies confirm the effectiveness of this method in planning teeth movement, showcasing its potential to streamline orthodontic procedures.},
  archive      = {J_TVCG},
  author       = {Yumeng Liu and Yuexin Ma and Lei Yang and Congyi Zhang and Guangshun Wei and Runnan Chen and Min Gu and Jia Pan and Zhengbao Yang and Taku Komura and Shiqing Xin and Yuanfeng Zhou and Changhe Tu and Wenping Wang},
  doi          = {10.1109/TVCG.2025.3567299},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8480-8490},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A potential field method for tooth motion planning in orthodontic treatment},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDA-net: A global feature point cloud completion network based on serialization and dual attention. <em>TVCG</em>, <em>31</em>(10), 8466-8479. (<a href='https://doi.org/10.1109/TVCG.2025.3571467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is essential for restoring 3D geometric data lost due to occlusions or sensor limitations. Existing methods often rely on k-nearest neighbor(KNN)-based local feature extraction, which focuses on neighborhoods around central points while neglecting critical global structural information. Additionally, Transformer-based approaches, while effective at modeling global context, typically use central point feature sequences to reduce computational complexity. This windowed attention strategy compromises the preservation of global context, leading to incomplete modeling of the point cloud’s overall structure. To address these challenges, we propose SDA-Net, a dual-attention point cloud completion network utilizing multiple serialization strategies. These strategies transform unordered point clouds into structured sequences, enabling comprehensive modeling of inter-point relationships. Additionally, the dual-attention mechanism enhances global feature extraction through complementary spatial and channel-wise self-attention, effectively compensating for the loss of global context. Extensive experiments demonstrate that SDA-Net achieves state-of-the-art performance, including an average Chamfer Distance (CD) of 6.48 on the PCN dataset. Furthermore, it excels in real-world applications, accurately reconstructing fine-grained details in LiDAR-scanned point clouds.},
  archive      = {J_TVCG},
  author       = {Weichao Wu and Yongyang Xu and Zhong Xie},
  doi          = {10.1109/TVCG.2025.3571467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8466-8479},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SDA-net: A global feature point cloud completion network based on serialization and dual attention},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing provenance as an attribute for visual data analysis: A design probe with ProvenanceLens. <em>TVCG</em>, <em>31</em>(10), 8452-8465. (<a href='https://doi.org/10.1109/TVCG.2025.3571708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytic provenance can be visually encoded to help users track their ongoing analysis trajectories, recall past interactions, and inform new analytic directions. Despite its significance, provenance is often hardwired into analytics systems, affording limited user control and opportunities for self-reflection. We thus propose modeling provenance as an attribute that is available to users during analysis. We demonstrate this concept by modeling two provenance attributes that track the recency and frequency of user interactions with data. We integrate these attributes into a visual data analysis system prototype, ProvenanceLens, wherein users can visualize their interaction recency and frequency by mapping them to encoding channels (e.g., color, size) or applying data transformations (e.g., filter, sort). Using ProvenanceLens as a design probe, we conduct an exploratory study with sixteen users to investigate how these provenance-tracking affordances are utilized for both decision-making and self-reflection. We find that users can accurately and confidently answer questions about their analysis, and we show that mismatches between the user's mental model and the provenance encodings can be surprising, thereby prompting useful self-reflection. We also report on the user strategies surrounding these affordances, and reflect on their intuitiveness and effectiveness in representing provenance.},
  archive      = {J_TVCG},
  author       = {Arpit Narechania and Shunan Guo and Eunyee Koh and Alex Endert and Jane Hoffswell},
  doi          = {10.1109/TVCG.2025.3571708},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8452-8465},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Utilizing provenance as an attribute for visual data analysis: A design probe with ProvenanceLens},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and evaluation of a 6-DoF wearable fingertip device for haptic shape rendering. <em>TVCG</em>, <em>31</em>(10), 8439-8451. (<a href='https://doi.org/10.1109/TVCG.2025.3571705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual objects contain increasingly rich attribute information, small wearable fingertip devices need to have higher degrees of freedom (DoFs) to convey the haptic sensation of virtual objects. In order to effectively display the shape features of virtual objects to users through curvature, we designed a 6-DoF wearable fingertip device (WFD). This WFD combines a 6-DoF Stewart parallel mechanism, consisting of a static platform and a mobile platform connected by six revolute-spherical-spherical kinematic chains. The translation and rotation of the mobile platform are driven by six miniature servo motors, which can simulate haptic sensations such as making and breaking contact, sliding, and skin stretch when the fingertip interacts with a virtual surface. The WFD is fixed at the user’s dominant index finger using hook-and-loop fasteners, with a size of 68 × 59 × 56 mm$^{3}$ and a mass of 45.5 g. We analyzed and validated the kinematic model of the WFD and tested its force output capability. Finally, we invited 15 adults to conduct three subjective perception experiments to evaluate the performance of the WFD in curvature perception and shape display. The experimental results show that: (1) The just noticeable difference (JND) for curvature identification using the WFD is 3.02$\pm$0.23 m$^{-1}$; (2) The 6-DoF haptic feedback provided by the WFD improves the accuracy of curved surface recognition from 53.4$\pm$7.1% in 3-DoF to 72.0$\pm$5.9%; (3) Even without visual feedback, the shape recognition accuracy of the WFD when combined with the Touch device reaches 82.3$\pm$8.2% . Experimental results show that the WFD has good performance and potential in curvature perception and shape display.},
  archive      = {J_TVCG},
  author       = {Dapeng Chen and Da Yu and Yi Ding and Haojun Ni and Lifeng Zhu and Hong Zeng and Zhong Wei and Jia Liu and Aiguo Song},
  doi          = {10.1109/TVCG.2025.3571705},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8439-8451},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design and evaluation of a 6-DoF wearable fingertip device for haptic shape rendering},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CreativeSynth: Cross-art-attention for artistic image synthesis with multimodal diffusion. <em>TVCG</em>, <em>31</em>(10), 8425-8438. (<a href='https://doi.org/10.1109/TVCG.2025.3570771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although remarkable progress has been made in image style transfer, style is just one of the components of artistic paintings. Directly transferring extracted style features to natural images often results in outputs with obvious synthetic traces. This is because key painting attributes including layout, perspective, shape, and semantics often cannot be conveyed and expressed through style transfer. Large-scale pretrained text-to-image generation models have demonstrated their capability to synthesize a vast amount of high-quality images. However, even with extensive textual descriptions, it is challenging to fully express the unique visual properties and details of paintings. Moreover, generic models often disrupt the overall artistic effect when modifying specific areas, making it more complicated to achieve a unified aesthetic in artworks. Our main novel idea is to integrate multimodal semantic information as a synthesis guide into artworks, rather than transferring style to the real world. We also aim to reduce the disruption to the harmony of artworks while simplifying the guidance conditions. Specifically, we propose an innovative multi-task unified framework called CreativeSynth, based on the diffusion model with the ability to coordinate multimodal inputs. CreativeSynth combines multimodal features with customized attention mechanisms to seamlessly integrate real-world semantic content into the art domain through Cross-Art-Attention for aesthetic maintenance and semantic fusion. We demonstrate the results of our method across a wide range of different art categories, proving that CreativeSynth bridges the gap between generative models and artistic expression.},
  archive      = {J_TVCG},
  author       = {Nisha Huang and Weiming Dong and Yuxin Zhang and Fan Tang and Ronghui Li and Chongyang Ma and Xiu Li and Tong-Yee Lee and Changsheng Xu},
  doi          = {10.1109/TVCG.2025.3570771},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8425-8438},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CreativeSynth: Cross-art-attention for artistic image synthesis with multimodal diffusion},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-error reconstruction of directional functions with spherical harmonics. <em>TVCG</em>, <em>31</em>(10), 8413-8424. (<a href='https://doi.org/10.1109/TVCG.2025.3570092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel approach for the low-error reconstruction of directional functions with spherical harmonics. We introduce a modified version of Spherical Gaussians with adaptive narrowness and amplitude to represent the input data in an intermediate form. This representation is then projected into spherical harmonics using a closed-form analytical solution. Because of the spectral properties of the proposed representation, the amount of ringing artifacts is reduced, and the overall precision of the reconstructed function is improved. The proposed method is more precise comparing to existing methods. The presented solution can be used in several graphical applications, as discussed in this paper. For example, the method is suitable for sparse models such as indirect illumination or reflectance functions.},
  archive      = {J_TVCG},
  author       = {Michal Vlnas and Tomáš Milet and Pavel Zemčík},
  doi          = {10.1109/TVCG.2025.3570092},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8413-8424},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Low-error reconstruction of directional functions with spherical harmonics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulating two-phase fluid-rigid interactions with an overset-grid kinetic solver. <em>TVCG</em>, <em>31</em>(10), 8397-8412. (<a href='https://doi.org/10.1109/TVCG.2025.3570570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating the coupled dynamics between rigid bodies and two-phase fluids, especially those with a large density ratio and a high Reynolds number, is computationally demanding but visually compelling with a broad range of applications. Traditional approaches that directly solve the Navier-Stokes equations often struggle to reproduce these flow phenomena due to stronger numerical diffusion, resulting in lower accuracy. While recent advancements in kinetic lattice Boltzmann methods for two-phase flows have notably enhanced efficiency and accuracy, challenges remain in correctly managing fluid-rigid boundaries, resulting in physically inconsistent results. In this article, we propose a novel kinetic framework for fluid-rigid interaction involving two fluid phases. Our approach leverages the idea of an overset grid, and proposes a novel formulation in the two-phase flow context with multiple improvements to handle complex scenarios and support moving multi-resolution domains with boundary layer control. These new contributions successfully resolve many issues inherent in previous methods and enable physically more consistent simulations of two-phase flow phenomena. We have conducted both quantitative and qualitative evaluations, compared our method to previous techniques, and validated its physical consistency through real-world experiments. Additionally, we demonstrate the versatility of our method across various scenarios.},
  archive      = {J_TVCG},
  author       = {Xiaoyu Xiao and Ding Lin and Yiheng Wu and Kai Bai and Xiaopei Liu},
  doi          = {10.1109/TVCG.2025.3570570},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8397-8412},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simulating two-phase fluid-rigid interactions with an overset-grid kinetic solver},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised learning of global object-centric representations for compositional scene understanding. <em>TVCG</em>, <em>31</em>(10), 8385-8396. (<a href='https://doi.org/10.1109/TVCG.2025.3570426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to extract invariant visual features of objects from complex scenes and identify the same objects in different scenes is inborn for humans. To endow AI systems with such capability, we introduce a novel compositional scene understanding method known as Compositional Scene understanding via Global Object-centric representations (CSGOs). CSGO achieves comprehensive scene understanding, including the discovery and identification of objects, by leveraging a set of learnable global object-centric representations in an unsupervised manner. CSGO comprises three components: 1) Local Object-Centric Learning, which is responsible for extracting localized and scene-specific object-centric representations to discover objects; 2) Image Decoding, facilitating the reconstruction of object and scene images using the obtained object-centric representation as input; and 3) Global Object-Centric Learning, identifying the object across diverse scenes according to a set of learnable global object-centric representations, which indicates the scene-free intrinsic attributes (i.e., appearance and shape) of objects. Experimental results on three synthetic datasets and one real-world scene dataset demonstrate that CSGO has excellent object identification and attribute disentanglement abilities. Furthermore, the scene decomposition performance (indicating object discovery performance) of CSGO is superior to comparison methods.},
  archive      = {J_TVCG},
  author       = {Tonglin Chen and Yinxuan Huang and Jinghao Huang and Bin Li and Xiangyang Xue},
  doi          = {10.1109/TVCG.2025.3570426},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8385-8396},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unsupervised learning of global object-centric representations for compositional scene understanding},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MotionCrafter: Plug-and-play motion guidance for diffusion models. <em>TVCG</em>, <em>31</em>(10), 8372-8384. (<a href='https://doi.org/10.1109/TVCG.2025.3568880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The essence of a video lies in the dynamic motions. While text-to-video generative diffusion models have made significant strides in creating diverse content, effectively controlling specific motions through text prompts remains a challenge. By utilizing user-specified reference videos, the more precise guidance for character actions, object movements, and camera movements can be achieved. This gives rise to the task of motion customization, where the primary challenge lies in effectively decoupling the appearance and motion within a video clip. To address this challenge, we introduce MotionCrafter, a novel one-shot instance-guided motion customization method that is suitable for both pre-trained text-to-video and text-to-image diffusion models. MotionCrafter employs a parallel spatial-temporal architecture that integrates the reference motion into the temporal component of the base model, while independently adjusting the spatial module for character or style control. To enhance the disentanglement of motion and appearance, we propose an innovative dual-branch motion disentanglement approach, which includes a motion disentanglement loss and an appearance prior enhancement strategy. To facilitate more efficient learning of motions, we further propose a novel timestep-layered tuning strategy that directs the diffusion model to focus on motion-level information. Through comprehensive quantitative and qualitative experiments, along with user preference tests, we demonstrate that MotionCrafter can successfully integrate dynamic motions while maintaining the coherence and quality of the base model, providing a wide range of appearance generation capabilities. MotionCrafter can be applied to various personalized backbones in the community to generate videos with a variety of artistic styles.},
  archive      = {J_TVCG},
  author       = {Yuxin Zhang and Weiming Dong and Fan Tang and Nisha Huang and Haibin Huang and Chongyang Ma and Pengfei Wan and Tong-Yee Lee and Changsheng Xu},
  doi          = {10.1109/TVCG.2025.3568880},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8372-8384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MotionCrafter: Plug-and-play motion guidance for diffusion models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtual, augmented, and extended reality applied to science communication: A systematic literature review. <em>TVCG</em>, <em>31</em>(10), 8359-8371. (<a href='https://doi.org/10.1109/TVCG.2025.3569398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended reality (XR)—which includes virtual reality (VR) and augmented reality (AR)—is becoming increasingly popular for sharing scientific knowledge. This research evaluates the state-of-the-art in XR for scientific communication. Our two-phase methodology began with a Systematic Literature Review, identifying 94 relevant articles and conference papers from the last decade (2013-2023) sourced from the Web of Science and SCOPUS databases. These publications show scholars and practitioners using XR to convey scientific findings, foster awareness, ignite interest, shape opinions, and enhance understanding. In the second phase, we applied data clustering and analysis. Our findings highlight a significant increase in XR studies over the last decade, with the XR technologies used for communication (N = 24), dissemination (N = 23), educational/training (N = 21), and decision-making (N = 10). Our results indicate the need to establish clearer guidelines for aligning science communication and to create more possibilities to publish peer-reviewed research in.},
  archive      = {J_TVCG},
  author       = {Juan Romero-Luis and José Luis Rubio-Tamayo and Alberto Sanchez-Acedo and Daniel Wuebben and Valeri Codesido-Linares},
  doi          = {10.1109/TVCG.2025.3569398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8359-8371},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual, augmented, and extended reality applied to science communication: A systematic literature review},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TexHOI: Reconstructing textures of 3D unknown objects in monocular hand-object interaction scenes. <em>TVCG</em>, <em>31</em>(10), 8347-8358. (<a href='https://doi.org/10.1109/TVCG.2025.3567276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing 3D models of dynamic, real-world objects with high-fidelity textures from monocular frame sequences has been a challenging problem in recent years. This difficulty stems from factors such as shadows, indirect illumination, and inaccurate object-pose estimations due to occluding hand-object interactions. To address these challenges, we propose a novel approach that predicts the hand’s impact on environmental visibility and indirect illumination on the object’s surface albedo. Our method first learns the geometry and low-fidelity texture of the object, hand, and background through composite rendering of radiance fields. Simultaneously, we optimize the hand and object poses to achieve accurate object-pose estimations. We then refine physics-based rendering parameters—including roughness, specularity, albedo, hand visibility, skin color reflections, and environmental illumination—to produce precise albedo, and accurate hand illumination and shadow regions. Our approach surpasses state-of-the-art methods in texture reconstruction and, to the best of our knowledge, is the first to account for hand-object interactions in object texture reconstruction.},
  archive      = {J_TVCG},
  author       = {Alakh Aggarwal and Ningna Wang and Xiaohu Guo},
  doi          = {10.1109/TVCG.2025.3567276},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8347-8358},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TexHOI: Reconstructing textures of 3D unknown objects in monocular hand-object interaction scenes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ActiveAR: Augmented reality task support system with proactive context and virtual content management. <em>TVCG</em>, <em>31</em>(10), 8334-8346. (<a href='https://doi.org/10.1109/TVCG.2025.3567346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) has long been expected to help users improve their working efficiency. However, due to the absence of intelligent systems, existing AR applications are greatly affected by the virtual content interference with real-world activities. Unlike existing work, which focuses more on hiding virtual content to reduce interference, in this work, we propose an innovative AR Task Support System where virtual contents actively guide users with task completion. During task execution, our system proactively searches for and tracks key objects in the scene, and uses this context information to automatically select appropriate virtual content and display positions. Through introducing open-world prompt-based visual models, our system can effectively retrieve few-shot or even zero-shot objects that are uncommon in the dataset. This approach extends the use of AR Task Support System beyond controlled industrial settings to more uncontrolled daily scenarios, overcoming the limitations of existing systems. It also significantly reduces development costs for developers. We demonstrate the advantages of our system over traditional virtual content management systems through a series of experiments that are closer to users’ real usage situations.},
  archive      = {J_TVCG},
  author       = {Renjie Zhang and Jia Liu and Taishi Sawabe and Yuichiro Fujimoto and Masayuki Kanbara and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3567346},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8334-8346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ActiveAR: Augmented reality task support system with proactive context and virtual content management},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prismatic: Interactive multi-view cluster analysis of concept stocks. <em>TVCG</em>, <em>31</em>(10), 8320-8333. (<a href='https://doi.org/10.1109/TVCG.2025.3567084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.},
  archive      = {J_TVCG},
  author       = {Wong Kam-Kwai and Yan Luo and Xuanwu Yue and Wei Chen and Huamin Qu},
  doi          = {10.1109/TVCG.2025.3567084},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8320-8333},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Prismatic: Interactive multi-view cluster analysis of concept stocks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developable approximation via isomap on gauss image. <em>TVCG</em>, <em>31</em>(10), 8310-8319. (<a href='https://doi.org/10.1109/TVCG.2025.3566887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to generate developable approximations for triangular meshes. Instead of fitting the Gauss image using a geodesic circle in the local neighborhood, we apply a nonlinear dimensionality reduction method, called Isomap, to use a general curve on the sphere for fitting. This brings us a larger space to represent the Gauss image in the local neighborhood as a 1D structure. Specifically, each triangle is assigned a target normal after local fitting; then, we deform the mesh to approach the target normal globally. By iteratively performing fitting and deformation, we obtain the developable approximation. We demonstrate the feasibility and effectiveness of our method over various examples. Compared to the state-of-the-art methods, our results exhibit a higher fidelity to the input mesh while possessing more prominent and visually distinct undevelopable seam curves.},
  archive      = {J_TVCG},
  author       = {Yuan-Yuan Cheng and Qing Fang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1109/TVCG.2025.3566887},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8310-8319},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Developable approximation via isomap on gauss image},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape cloud collage on irregular canvas. <em>TVCG</em>, <em>31</em>(10), 8297-8309. (<a href='https://doi.org/10.1109/TVCG.2025.3566942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a challenging and novel problem in 2D shape cloud visualization: arranging irregular 2D shapes on an irregular canvas to minimize gaps and overlaps while emphasizing critical shapes by displaying them in larger sizes. The concept of a shape cloud is inspired by word clouds, which are widely used in visualization research to aesthetically summarize textual datasets by highlighting significant words with larger font sizes. We extend this concept to images, introducing shape clouds as a powerful and expressive visualization tool, guided by the principle that “a picture is worth a thousand words. Despite the potential of this approach, solutions in this domain remain largely unexplored.” To bridge this gap, we develop a 2D shape cloud collage framework that compactly arranges 2D shapes, emphasizing important objects with larger sizes, analogous to the principles of word clouds. This task presents unique challenges, as existing 2D shape layout methods are not designed for scalable irregular packing. Applying these methods often results in suboptimal layouts, such as excessive empty spaces or inaccurate representations of the underlying data. To overcome these limitations, we propose a novel layout framework that leverages recent advances in differentiable optimization. Specifically, we formulate the irregular packing problem as an optimization task, modeling the object arrangement process as a differentiable pipeline. This approach enables fast and accurate end-to-end optimization, producing high-quality layouts. Experimental results show that our system efficiently creates visually appealing and high-quality shape clouds on arbitrary canvas shapes, outperforming existing methods.},
  archive      = {J_TVCG},
  author       = {Sheng-Yi Yao and Dong-Yi Wu and Thi-Ngoc-Hanh Le and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2025.3566942},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8297-8309},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shape cloud collage on irregular canvas},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting performance models of distal pointing tasks in virtual reality. <em>TVCG</em>, <em>31</em>(10), 8283-8296. (<a href='https://doi.org/10.1109/TVCG.2025.3567078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance models of interaction, such as Fitts’ law, are important tools for predicting and explaining human motor performance and for designing high-performance user interfaces. Extensive prior work has proposed such models for the 3D interaction task of distal pointing, in which the user points their hand or a device at a distant target in order to select it. However, there is no consensus on how to compute the index of difficulty for distal pointing tasks. We present a preliminary study suggesting that existing models may not be sufficient to model distal pointing performance with current virtual reality technologies. Based on these results, we hypothesized that both the form of the model and the standard method for collecting empirical data for pointing tasks might need to change in order to achieve a more accurate and valid distal pointing model. In our main study, we used a new methodology to collect distal pointing data and evaluated traditional models, purely ballistic models, and two-part models. Ultimately, we found that the best model used a simple Fitts’-law-style index of difficulty with angular measures of amplitude and width.},
  archive      = {J_TVCG},
  author       = {Logan Lane and Feiyu Lu and Shakiba Davari and Robert J. Teather and Doug A. Bowman},
  doi          = {10.1109/TVCG.2025.3567078},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8283-8296},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting performance models of distal pointing tasks in virtual reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A perceptually optimized and self-calibrated tone mapping operator. <em>TVCG</em>, <em>31</em>(10), 8268-8282. (<a href='https://doi.org/10.1109/TVCG.2025.3566377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks, taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance, a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, we input the same HDR image—self-calibrated to different maximum luminance levels—into the learned tone mapping network, and generate a pseudo-multi-exposure image stack with varying detail visibility and color saturation. We then train another fusion network to merge the LDR image stack into a desired LDR image by maximizing a variant of the structural similarity index for multi-exposure image fusion, proven perceptually relevant to fused image quality. Extensive experiments show that our method produces images with consistently better visual quality while ranking among the fastest local TMOs.},
  archive      = {J_TVCG},
  author       = {Peibei Cao and Chenyang Le and Yuming Fang and Kede Ma},
  doi          = {10.1109/TVCG.2025.3566377},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8268-8282},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A perceptually optimized and self-calibrated tone mapping operator},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Make PBR materials tileable with latent diffusion inpainting. <em>TVCG</em>, <em>31</em>(10), 8256-8267. (<a href='https://doi.org/10.1109/TVCG.2025.3566315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically-based-rendering (PBR) materials are crucial in modern rendering pipelines, and many studies have focused on acquiring these materials from reality or images. However, existing methods may result in non-tileable results, since the realistic inputs usually have seams. Compared to non-tileable materials, tileable PBR materials have more universal application scenarios. To address this issue, we introduce MaTi, a novel pipeline that converts non-tileable PBR materials into tileable ones with minimal distortion. MaTi rearranges material patches to align boundaries at the center of the image, and then uses a diffusion model to inpaint the seams. We use scaled gamma correction to reduce the occurrence of collapse when processing special material maps. The color correction and triangular blending are adopt to preserve the original material information. Additionally, we design a division and blending strategy to efficiently handle high resolution materials. Our experiments demonstrate that MaTi can seamlessly modify PBR materials while preserving the original information, outperforming existing synthesis methods.},
  archive      = {J_TVCG},
  author       = {Xiaoyu Zhan and Jianxin Yang and Jun Wang and Yuanqi Li and Jie Guo and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3566315},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8256-8267},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Make PBR materials tileable with latent diffusion inpainting},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept lens: Visual comparison and evaluation of generative model manipulations. <em>TVCG</em>, <em>31</em>(10), 8243-8255. (<a href='https://doi.org/10.1109/TVCG.2025.3564537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models are becoming a transformative technology for the creation and editing of images. However, it remains challenging to harness these models for precise image manipulation. These challenges often manifest as inconsistency in the editing process, where both the type and amount of semantic change, depend on the image being manipulated. Moreover, there exist many methods for computing image manipulations, whose development is hindered by the matter of inconsistency. This paper aims to address these challenges by improving how we evaluate, compare, and explore the space of manipulations offered by a generative model. We present Concept Lens, a visual interface that is designed to aid users in understanding semantic concepts carried in image manipulations, and how these manipulations vary over generated images. Given the large space of possible images produced by a generative model, Concept Lens is designed to support the exploration of both generated images, and their manipulations, at multiple levels of detail. To this end, the layout of Concept Lens is informed by two hierarchies: a hierarchical organization of (1) original images, grouped by their similarities, and (2) image manipulations, where manipulations that induce similar changes are grouped together. This layout allows one to discover the types of images that consistently respond to a group of manipulations, and vice versa, manipulations that consistently respond to a group of codes. We show the benefits of this design across multiple use cases, specifically, studying the quality of manipulations for a single method, and offering a means of comparing different methods.},
  archive      = {J_TVCG},
  author       = {Sangwon Jeong and Mingwei Li and Matthew Berger and Shusen Liu},
  doi          = {10.1109/TVCG.2025.3564537},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8243-8255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Concept lens: Visual comparison and evaluation of generative model manipulations},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSmoothFace: Generalized smooth talking face generation via fine grained 3D face guidance. <em>TVCG</em>, <em>31</em>(10), 8231-8242. (<a href='https://doi.org/10.1109/TVCG.2025.3566382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3D face model, which can synthesize smooth lip dynamics while preserving the speaker’s identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip-synchronization, and visual quality.},
  archive      = {J_TVCG},
  author       = {Haiming Zhang and Zhihao Yuan and Chaoda Zheng and Xu Yan and Baoyuan Wang and Guanbin Li and Song Wu and Shuguang Cui and Zhen Li},
  doi          = {10.1109/TVCG.2025.3566382},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8231-8242},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GSmoothFace: Generalized smooth talking face generation via fine grained 3D face guidance},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sketch2Seq: Reconstruct CAD models from feature-based sketch segmentation. <em>TVCG</em>, <em>31</em>(10), 8214-8230. (<a href='https://doi.org/10.1109/TVCG.2025.3566544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based modeling studies reconstructing models from sketches automatically, allowing users visualize design concepts rapidly. Generating CAD models based on user sketches helps reduce the learning curve for novice users, which promotes the everyday use of CAD software, and expands its reach to non-professional groups. While various algorithms study automatically generating models from single sketch or line drawing, they often produce non-editable models or editable models limited to simple extrusion operations. To improve this issue, we propose a novel sketch-based modeling system, Sketch2Seq, which generates complex, semantic, and editable CAD models. Our system eliminates the need for additional annotations from users and produces models that support subsequent application in commercial software. The core of our method lies in understanding users’ design intent from CAD sketches. We design a novel sketch segmentation network for identifying diverse operation features in CAD sketches, which utilizes geometric features of strokes and different levels of topological connections. Additionally, to tackle the segmentation task, a dataset for CAD sketch segmentation is introduced. Comparative experiments and ablation evaluations prove the effectiveness of the proposed method. Based on segmentation result, coarse CAD sequences are generated and progressively executed. Meanwhile, the orders and parameters of the CAD sequences are optimized with context models and input sketches. All algorithms are integrated into a user interface. Experiments and evaluations validate the feasibility and superiority of our entire system which is able to reconstruct more complex features and achieve better results for longer sequence.},
  archive      = {J_TVCG},
  author       = {Yue Sun and Jituo Li and Ziqin Xu and Jialu Zhang and Xinqi Liu and Dongliang Zhang and Guodong Lu},
  doi          = {10.1109/TVCG.2025.3566544},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8214-8230},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch2Seq: Reconstruct CAD models from feature-based sketch segmentation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical fuzzy-cluster-aware grid layout for large-scale data. <em>TVCG</em>, <em>31</em>(10), 8200-8213. (<a href='https://doi.org/10.1109/TVCG.2025.3566558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clusters, where ambiguous samples belong to multiple clusters, are common in real-world applications. Analyzing such ambiguous samples in large-scale datasets is crucial for practical applications, such as diagnosing machine learning models. A promising method to support such analysis is through hierarchical cluster-aware grid visualizations, which offer high space efficiency and clear cluster perception. However, existing cluster-aware grid layout methods cannot clarify ambiguity among fuzzy clusters, which limits their effectiveness in fuzzy cluster analysis. To tackle this issue, we introduce a hierarchical fuzzy-cluster-aware grid layout method that supports hierarchical exploration of large-scale datasets. Throughout the hierarchical exploration, it is crucial to facilitate fuzzy cluster analysis while maintaining visual continuity for users. To achieve this, we propose a two-step optimization strategy for enhancing cluster perception, clarifying ambiguity, and preserving stability during the exploration. The first step is to create cluster-aware partitions, where each partition corresponds to a cluster. This step focuses on enhancing cluster perception and maintaining the previous shapes and positions of clusters to preserve stability at the cluster level. The second step is to generate a grid layout for each partition. In addition to placing similar samples together, this step also places ambiguous samples near the boundaries to clarify ambiguity and reveal the root causes of their occurrences and maintains the relative positions of the samples in the same cluster to preserve stability at the sample level. Several quantitative experiments and a use case are conducted to demonstrate the effectiveness and usefulness of our method in analyzing large-scale datasets, especially in fuzzy cluster analysis.},
  archive      = {J_TVCG},
  author       = {Yuxing Zhou and Changjian Chen and Zhiyang Shen and Jiangning Zhu and Jiashu Chen and Weikai Yang and Shixia Liu},
  doi          = {10.1109/TVCG.2025.3566558},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8200-8213},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hierarchical fuzzy-cluster-aware grid layout for large-scale data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoARF++: Content-aware radiance field aligning model complexity with scene intricacy. <em>TVCG</em>, <em>31</em>(10), 8187-8199. (<a href='https://doi.org/10.1109/TVCG.2025.3566071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the concept of Content-Aware Radiance Fields (CoARF), which adaptively aligns the model complexity with the scene intricacy. By examining the intricacies of radiance fields from three perspectives, model complexity is adapted through scalable feature grids, dynamic neural networks, and model quantization. Specifically, we propose a hash collision detection mechanism that removes redundant feature grid by restricting the valid hash collision to reasonable level, making the space complexity scalable. We introduce an uncertainty-aware decoded layer, where simple points are early-exited to prevent them from being processed by deeper network layers, ensuring computational complexity scalable. Furthermore, we propose Learned Bitwidth Quantization (LBQ) and Adversarial Content-Aware Quantization (A-CAQ) paradigms by making the bitwidth of parameters differentiable and trainable, allowing for adjustable quantization schemes. Building on these techniques, the proposed CoARF++ framework enables a scalable pipeline for radiance fields that is tailored to the unique characteristics of scene complexity and quality requirement. Extensive experiments demonstrate a significant and adjustable reduction in model complexity across various NeRF variants, while maintaining the necessary reconstruction and rendering quality, making it advantageous for the practical deployment of radiance field models.},
  archive      = {J_TVCG},
  author       = {Weihang Liu and Xue Xian Zheng and Yuke Li and Tareq Y. Al-Naffouri and Jingyi Yu and Xin Lou},
  doi          = {10.1109/TVCG.2025.3566071},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8187-8199},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CoARF++: Content-aware radiance field aligning model complexity with scene intricacy},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic view synthesis from small camera motion videos. <em>TVCG</em>, <em>31</em>(10), 8174-8186. (<a href='https://doi.org/10.1109/TVCG.2025.3565642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis for dynamic 3D scenes poses a significant challenge. Many notable efforts use NeRF-based approaches to address this task and yield impressive results. However, these methods rely heavily on sufficient motion parallax in the input images or videos. When the camera motion range becomes limited or even stationary (i.e., small camera motion), existing methods encounter two primary challenges: incorrect representation of scene geometry and inaccurate estimation of camera parameters. These challenges make prior methods struggle to produce satisfactory results or even become ineffective. To address the first challenge, we propose a novel Distribution-based Depth Regularization (DDR) that ensures the rendering weight distribution to align with the true distribution. Specifically, unlike previous methods that use depth loss to calculate the error of the expectation, we calculate the expectation of the error by using Gumbel-softmax to differentiably sample points from discrete rendering weight distribution. Additionally, we introduce constraints that enforce the volume density of spatial points before the object boundary along the ray to be near zero, ensuring that our model learns the correct geometry of the scene. To demystify the DDR, we further propose a visualization tool that enables observing the scene geometry representation at the rendering weight level. For the second challenge, we incorporate camera parameter learning during training to enhance the robustness of our model to camera parameters. We conduct extensive experiments to demonstrate the effectiveness of our approach in representing scenes with small camera motion input, and our results compare favorably to state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Huiqiang Sun and Xingyi Li and Juewen Peng and Liao Shen and Zhiguo Cao and Ke Xian and Guosheng Lin},
  doi          = {10.1109/TVCG.2025.3565642},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8174-8186},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic view synthesis from small camera motion videos},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplane-based cross-view interaction mechanism for robust light field angular super-resolution. <em>TVCG</em>, <em>31</em>(10), 8159-8173. (<a href='https://doi.org/10.1109/TVCG.2025.3564643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense sampling of the light field (LF) is essential for various applications, such as virtual reality. However, the collection process is prohibitively expensive due to technological limitations in imaging. Synthesizing novel views from sparse LF data, known as LF Angular Super-Resolution (LFASR), offers an effective solution to this problem. Accurate cross-view interaction is crucial for this task, given the complementary information between LF views. Previous methods, however, suffer from limited reconstruction quality due to inefficient view interaction. To address this, we propose a Multiplane-based Cross-view Interaction Mechanism (MCIM) for robust LFASR. Extensive comparisons with state-of-the-art methods demonstrate that our method achieves superior performance, both visually and quantitatively. Specifically, Drawing inspiration from MultiPlane Images (MPI) in scene modeling, our mechanism incorporates a novel Multiplane Feature Fusion (MPFF) strategy. This strategy facilitates fast and accurate cross-view interaction, enhancing the network’s robustness to scene geometry and suitability for different-baseline LF scenes. Furthermore, to address information redundancy in multiplanes, we leverage the transparency property of MPI and devise a plane selection strategy. Finally, we propose CSTNet, a Cross-Shaped Transformer-based network for LFASR, which employs a cross-shaped self-attention mechanism to enable low-cost training and inference. Experimental results on various angular super-resolution tasks validate that our network achieves state-of-the-art performance on both synthetic and real-world LF scenes.},
  archive      = {J_TVCG},
  author       = {Rongshan Chen and Hao Sheng and Da Yang and Ruixuan Cong and Zhenglong Cui and Sizhe Wang},
  doi          = {10.1109/TVCG.2025.3564643},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8159-8173},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiplane-based cross-view interaction mechanism for robust light field angular super-resolution},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VIGMA: An open-access framework for visual gait and motion analytics. <em>TVCG</em>, <em>31</em>(10), 8143-8158. (<a href='https://doi.org/10.1109/TVCG.2025.3564866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation.},
  archive      = {J_TVCG},
  author       = {Kazi Shahrukh Omar and Shuaijie Wang and Ridhuparan Kungumaraju and Tanvi Bhatt and Fabio Miranda},
  doi          = {10.1109/TVCG.2025.3564866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8143-8158},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIGMA: An open-access framework for visual gait and motion analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diff-3DCap: Shape captioning with diffusion models. <em>TVCG</em>, <em>31</em>(10), 8129-8142. (<a href='https://doi.org/10.1109/TVCG.2025.3564664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of 3D shape captioning occupies a significant place within the domain of computer graphics and has garnered considerable interest in recent years. Traditional approaches to this challenge frequently depend on the utilization of costly voxel representations or object detection techniques, yet often fail to deliver satisfactory outcomes. To address the above challenges, in this paper, we introduce Diff-3DCap, which employs a sequence of projected views to represent a 3D object and a continuous diffusion model to facilitate the captioning process. More precisely, our approach utilizes the continuous diffusion model to perturb the embedded captions during the forward phase by introducing Gaussian noise and then predicts the reconstructed annotation during the reverse phase. Embedded within the diffusion framework is a commitment to leveraging a visual embedding obtained from a pre-trained visual-language model, which naturally allows the embedding to serve as a guiding signal, eliminating the need for an additional classifier. Extensive results of our experiments indicate that Diff-3DCap can achieve performance comparable to that of the current state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Jiawei Wen and Shiyang Li and Shiqing Xin and Ligang Liu},
  doi          = {10.1109/TVCG.2025.3564664},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8129-8142},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Diff-3DCap: Shape captioning with diffusion models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCINR: A divide-and-conquer implicit neural representation for compressing time-varying volumetric data in hours. <em>TVCG</em>, <em>31</em>(10), 8116-8128. (<a href='https://doi.org/10.1109/TVCG.2025.3564255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit neural representation (INR) has been a powerful paradigm for effectively compressing time-varying volumetric data. However, the optimization process can span days or even weeks due to its reliance on coordinate-based inputs and outputs for modeling volumetric data. To address this issue, we introduce a divide-and-conquer INR (DCINR), significantly accelerating the compressing process of time-varying volumetric data in hours. Our approach starts by dividing the data set into a set of non-overlapping blocks. Then, we apply a block selection strategy to weed out redundant blocks to reduce the computation cost without sacrificing performance. In parallel, each selected block is modeled by a tiny INR, with the size of the INR being adapted to match the information richness in the block. The block size is determined by maximizing the average network capacity. After optimization, the optimized INRs are utilized to decompress the data set. By evaluating our approach across various time-varying volumetric data sets, DCINR surpasses learning-based and lossy compression approaches in compression ratio, visual fidelity, and various performance metrics. Additionally, this method operates within a comparable compression time to that of lossy compressors, achieves extreme compression ratios ranging from thousands to tens of thousands, and preserves features with high quality.},
  archive      = {J_TVCG},
  author       = {Jun Han and Fan Yang},
  doi          = {10.1109/TVCG.2025.3564255},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8116-8128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DCINR: A divide-and-conquer implicit neural representation for compressing time-varying volumetric data in hours},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrinsic decomposition with robustly separating and restoring colored illumination. <em>TVCG</em>, <em>31</em>(10), 8098-8115. (<a href='https://doi.org/10.1109/TVCG.2025.3564229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic decomposition separates an image into reflectance and shading, which contributes to image editing, augmented reality, etc. Despite recent efforts dedicated to this field, effectively separating colored illumination from reflectance and correctly restoring it into shading remains an challenge. We propose a deep intrinsic decomposition method to address this issue. Specifically, by transforming intrinsic decomposition process in RGB image domains into the combination of intensity and chromaticity domains, we propose a novel macro intrinsic decomposition network framework. This framework enables the generation of finer intrinsic components through more relevant features propagation and more detailed sub-constraints guidance. In order to expand the macro network, we integrate multiple attention mechanism modules in key positions of encoders, which enhances the extraction of distinct features. We also propose a skip connection module based on specific deep features guidance, which can filter out features that are physically irrelevant to each intrinsic component. Our method not only outperforms state-of-the-art methods across multiple datasets, but also robustly separates illumination from reflectance and restores it into shading in various types of images. By leveraging our intrinsic images, we achieve visually superior image editing effects compared to other methods, while also being able to manipulate the inherent lighting of the original scene.},
  archive      = {J_TVCG},
  author       = {Hao Sha and Shining Ma and Tongtai Cao and Yu Han and Yu Liu and Yue Liu},
  doi          = {10.1109/TVCG.2025.3564229},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8098-8115},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Intrinsic decomposition with robustly separating and restoring colored illumination},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of user perspective, visual context, and feedback on interactions with AR targets on magic-lens displays. <em>TVCG</em>, <em>31</em>(10), 8085-8097. (<a href='https://doi.org/10.1109/TVCG.2025.3563609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing tasks in a close range using augmented content or instructions visualized on a 2D display can be difficult because of missing visual information in the third dimension. This is because the world on the screen is rendered from the perspective of a single camera, typically on the device itself. However, when performing tasks using hands, haptic feedback supports vision, and prior knowledge and visual context affect task performance. This study rendered the world on a display from the user’s perspective to re-enable depth cues from motion parallax and compared it with the conventional device perspective during haptic interactions. We conducted a user study involving 20 subjects and two experiments. First, the accuracy of touchpoint and depth estimation was measured under the conditions of a visual context and perspective rendering on a magic-lens display. We found that user-perspective rendering slightly improved the touch accuracy of targets on a physical surface; however, it significantly improved interactions without tactile feedback. This effect is relatively large when contextual information from the environment is absent, and it diminishes with increased haptic interactions. In the second experiment, we used a user-perspective magic lens to validate the proposed method in a practical needle injection scenario and confirm that the initial injections to virtual targets were more accurate. The results indicate that user-perspective rendering on magic lenses improves immediate performance in haptic tasks, suggesting they are particularly advantageous for frequently changing environments or short-duration tasks.},
  archive      = {J_TVCG},
  author       = {Geert Lugtenberg and Isidro Butaslac and Taishi Sawabe and Yuichiro Fujimoto and Masayuki Kanbara and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3563609},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8085-8097},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of user perspective, visual context, and feedback on interactions with AR targets on magic-lens displays},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delving into invisible semantics for generalized one-shot neural human rendering. <em>TVCG</em>, <em>31</em>(10), 8070-8084. (<a href='https://doi.org/10.1109/TVCG.2025.3563229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional human neural radiance fields often overlook crucial body semantics, resulting in ambiguous reconstructions, particularly in occluded regions. To address this problem, we propose the Super-Semantic Disentangled Neural Renderer (SSD-NeRF), which employs rich regional semantic priors to enhance human rendering accuracy. This approach initiates with a Visible-Invisible Semantic Propagation module, ensuring coherent semantic assignment to occluded parts based on visible body segments. Furthermore, a Region-Wise Texture Propagation module independently extends textures from visible to occluded areas within semantic regions, thereby avoiding irrelevant texture mixtures and preserving semantic consistency. Additionally, a view-aware curricular learning approach is integrated to bolster the model's robustness and output quality across different viewpoints. Extensive evaluations confirm that SSD-NeRF surpasses leading methods, particularly in generating quality and structurally semantic reconstructions of unseen or occluded views and poses.},
  archive      = {J_TVCG},
  author       = {Yihong Lin and Xuemiao Xu and Huaidong Zhang and Cheng Xu and Weijie Li and Yi Xie and Jing Qin and Shengfeng He},
  doi          = {10.1109/TVCG.2025.3563229},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8070-8084},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Delving into invisible semantics for generalized one-shot neural human rendering},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Part-aware shape generation with latent 3D diffusion of neural voxel fields. <em>TVCG</em>, <em>31</em>(10), 8057-8069. (<a href='https://doi.org/10.1109/TVCG.2025.3562871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel latent 3D diffusion model for generating neural voxel fields with precise part-aware structures and high-quality textures. In comparison to existing methods, this approach incorporates two key designs to guarantee high-quality and accurate part-aware generation. On one hand, we introduce a latent 3D diffusion process for neural voxel fields, incorporating part-aware information into the diffusion process and allowing generation at significantly higher resolutions to capture rich textural and geometric details accurately. On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding accurate part decomposition and producing high-quality rendering results. Importantly, part-aware learning establishes structural relationships to generate texture information for similar regions, thereby facilitating high-quality rendering results. We evaluate our approach across eight different data classes through extensive experimentation and comparisons with state-of-the-art methods. The results demonstrate that our proposed method has superior generative capabilities in part-aware shape generation, outperforming existing state-of-the-art methods. Moreover, we have conducted image- and text-guided shape generation via the conditioned diffusion process, showcasing the advanced potential in multi-modal guided shape generation.},
  archive      = {J_TVCG},
  author       = {Yuhang Huang and Shilong Zou and Xinwang Liu and Kai Xu},
  doi          = {10.1109/TVCG.2025.3562871},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8057-8069},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Part-aware shape generation with latent 3D diffusion of neural voxel fields},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrustME: A context-aware explainability model to promote user trust in guidance. <em>TVCG</em>, <em>31</em>(10), 8040-8056. (<a href='https://doi.org/10.1109/TVCG.2025.3562929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guidance-enhanced approaches are used to support users in making sense of their data and overcoming challenging analytical scenarios. While recent literature underscores the value of guidance, a lack of clear explanations to motivate system interventions may still negatively impact guidance effectiveness. Hence, guidance-enhanced VA approaches require meticulous design, demanding contextual adjustments for developing appropriate explanations. Our article discusses the concept of explainable guidance and how it impacts the user–system relationship—specifically, a user's trust in guidance within the VA process. We subsequently propose a model that supports the design of explainability strategies for guidance in VA. The model builds upon flourishing literature in explainable AI, available guidelines for developing effective guidance in VA systems, and accrued knowledge on user–system trust dynamics. Our model responds to challenges concerning guidance adoption and context-effectiveness by fostering trust through appropriately designed explanations. To demonstrate the model's value, we employ it in designing explanations within two existing VA scenarios. We also describe a design walk-through with a guidance expert to showcase how our model supports designers in clarifying the rationale behind system interventions and designing explainable guidance.},
  archive      = {J_TVCG},
  author       = {Maath Musleh and Renata G. Raidou and Davide Ceneda},
  doi          = {10.1109/TVCG.2025.3562929},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8040-8056},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TrustME: A context-aware explainability model to promote user trust in guidance},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtualized point cloud rendering. <em>TVCG</em>, <em>31</em>(10), 8026-8039. (<a href='https://doi.org/10.1109/TVCG.2025.3562696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing technologies, such as LiDAR, produce billions of points that commonly exceed the storage capacity of the GPU, restricting their processing and rendering. Level of detail (LoD) techniques have been widely investigated, but building the LoD structures is also time-consuming. This study proposes a GPU-driven culling system focused on determining the number of points visible in every frame. It can manipulate point clouds of any arbitrary size while maintaining a low memory footprint in both the CPU and GPU. Instead of organizing point clouds into hierarchical data structures, these are split into groups of points sorted using the Hilbert encoding. This alternative alleviates the occurrence of anomalous groups found in Morton curves. Instead of keeping the entire point cloud in the GPU, points are transferred on demand to ensure real-time capability. Accordingly, our solution can manipulate huge point clouds even in commodity hardware with low memory capacities. Moreover, hole filling is implemented to cover the gaps derived from insufficient density and our LoD system. Our proposal was evaluated with point clouds of up to 18 billion points, achieving an average of 80 frames per second (FPS) without perceptible quality loss. Relaxing memory constraints further enhances visual quality while maintaining an interactive frame rate. We assessed our method on real-world data, comparing it against three state-of-the-art methods, demonstrating its ability to handle significantly larger point clouds.},
  archive      = {J_TVCG},
  author       = {José Antonio Collado and Alfonso López and Juan Manuel Jurado and Juan Roberto Jiménez},
  doi          = {10.1109/TVCG.2025.3562696},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8026-8039},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtualized point cloud rendering},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive multi-plane images construction for light field occlusion removal. <em>TVCG</em>, <em>31</em>(10), 8012-8025. (<a href='https://doi.org/10.1109/TVCG.2025.3561374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Light Field (LF) shows great potential in removing occlusion since the objects occluded in some views may be visible in other views. However, existing LF-based methods implicitly model each scene and can only remove objects that have positive disparities in one central views. In this article, we propose a novel Progressive Multi-Plane Images (MPI) Construction method specifically designed for LF-based occlusion removal. Different from the previous MPI construction methods, we progressively construct MPIs layer by layer in order from near to far. In order to accurately model the current layer, the positions of foreground occlusions in the nearer layers are taken as occlusion prior. Specifically, we propose an Occlusion-Aware Attention Network to generate each layer of MPIs with reliable information in occluded regions. For each layer, occlusions in the current layer are filtered out so that the background is better recovered just using the visible views instead of the other occluded views. Then, by simply removing the layers containing occlusions and rendering MPIs in kinds of viewpoints, the occlusion removal results for different views are generated. Experiments on synthetic and real-world scenes show that our method outperforms state-of-the-art LF occlusion removal methods in quantitative and visual comparisons. Moreover, we also apply the proposed progressive MPI construction method to the view synthesis task. The occlusion edges in our synthesized views achieve significantly better quality, which also verifies that our method can better model the occluded regions.},
  archive      = {J_TVCG},
  author       = {Shuo Zhang and Song Chang and Zhuoyu Shi and Youfang Lin},
  doi          = {10.1109/TVCG.2025.3561374},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {8012-8025},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Progressive multi-plane images construction for light field occlusion removal},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling wireframe meshes with discrete equivalence classes. <em>TVCG</em>, <em>31</em>(10), 7998-8011. (<a href='https://doi.org/10.1109/TVCG.2025.3561370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a problem of modeling wireframe meshes where the vertices and edges fall into a set of discrete equivalence classes, respectively. This problem is motivated by the need of fabricating large wireframe structures at lower cost and faster speed since both nodes (thickened vertices) and rods (thickened edges) can be mass-produced. Given a 3D shape represented as a wireframe mesh, our goal is to compute a set of template vertices and a set of template edges, whose instances can be used to produce a fabricable wireframe mesh that approximates the input shape. To achieve this goal, we propose a computational approach that generates the template vertices and template edges by iteratively clustering and optimizing the mesh vertices and edges. At the clustering stage, we cluster mesh vertices and edges according to their shape and length, respectively. At the optimization stage, we first locally optimize the mesh to reduce the number of clusters of vertices and/or edges, and then globally optimize the mesh to reduce the intra-cluster variance for vertices and edges, while facilitating fabricability of the wireframe mesh. We demonstrate that our approach is able to model wireframe meshes with various shapes and topologies, compare it with three state-of-the-art approaches to show its superiority, and validate fabricability of our results by making three physical prototypes.},
  archive      = {J_TVCG},
  author       = {Pengyun Qiu and Rulin Chen and Peng Song and Ying He},
  doi          = {10.1109/TVCG.2025.3561370},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7998-8011},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling wireframe meshes with discrete equivalence classes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MonoRelief: Recovering 2.5D relief from a single image. <em>TVCG</em>, <em>31</em>(10), 7986-7997. (<a href='https://doi.org/10.1109/TVCG.2025.3561361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce MonoRelief, a novel method that combines the strengths of a depth map and a normal map to achieve high-quality relief recovery from a single image. By constructing a large-scale relief dataset that encompasses a diverse range of relief shapes, materials, and lighting conditions, we enable the training of a robust normal estimation network capable of handling various types of relief images. Furthermore, we leverage the state-of-the-art method, DepthAnything v2 (Yang et al. 2024), to generate depth maps from the input images. By integrating the strengths of both maps, MonoRelief recovers 2.5D reliefs with reasonable depth structures and intricate geometrical details. We validate the effectiveness and robustness of MonoRelief through comprehensive experiments, and showcase its potential in a variety of downstream applications, including Image-to-Relief, Text-to-Relief, Lines-to-Relief and relief reproduction.},
  archive      = {J_TVCG},
  author       = {Lipeng Gao and Yu-Wei Zhang and Mingqiang Wei and Hui Liu and Yanzhao Chen and Huadong Qiu and Caiming Zhang},
  doi          = {10.1109/TVCG.2025.3561361},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7986-7997},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MonoRelief: Recovering 2.5D relief from a single image},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLINT: Learning-based flow estimation and temporal interpolation for scientific ensemble visualization. <em>TVCG</em>, <em>31</em>(10), 7970-7985. (<a href='https://doi.org/10.1109/TVCG.2025.3561091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present FLINT (learning-based FLow estimation and temporal INTerpolation), a novel deep learning-based approach to estimate flow fields for 2D+time and 3D+time scientific ensemble data. FLINT can flexibly handle different types of scenarios with (1) a flow field being partially available for some members (e.g., omitted due to space constraints) or (2) no flow field being available at all (e.g., because it could not be acquired during an experiment). The design of our architecture allows to flexibly cater to both cases simply by adapting our modular loss functions, effectively treating the different scenarios as flow-supervised and flow-unsupervised problems, respectively (with respect to the presence or absence of ground-truth flow). To the best of our knowledge, FLINT is the first approach to perform flow estimation from scientific ensembles, generating a corresponding flow field for each discrete timestep, even in the absence of original flow information. Additionally, FLINT produces high-quality temporal interpolants between scalar fields. FLINT employs several neural blocks, each featuring several convolutional and deconvolutional layers. We demonstrate performance and accuracy for different usage scenarios with scientific ensembles from both simulations and experiments.},
  archive      = {J_TVCG},
  author       = {Hamid Gadirov and Jos B.T.M. Roerdink and Steffen Frey},
  doi          = {10.1109/TVCG.2025.3561091},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7970-7985},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FLINT: Learning-based flow estimation and temporal interpolation for scientific ensemble visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible and probabilistic topology tracking with partial optimal transport. <em>TVCG</em>, <em>31</em>(10), 7951-7969. (<a href='https://doi.org/10.1109/TVCG.2025.3561300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a flexible and probabilistic framework for tracking topological features in time-varying scalar fields using merge trees and partial optimal transport. Merge trees are topological descriptors that record the evolution of connected components in the sublevel sets of scalar fields. We present a new technique for modeling and comparing merge trees using tools from partial optimal transport. In particular, we model a merge tree as a measure network, that is, a network equipped with a probability distribution, and define a notion of distance on the space of merge trees inspired by partial optimal transport. Such a distance offers a new and flexible perspective for encoding intrinsic and extrinsic information in the comparative measures of merge trees. More importantly, it gives rise to a partial matching between topological features in time-varying data, thus enabling flexible topology tracking for scientific simulations. Furthermore, such partial matching may be interpreted as probabilistic coupling between features at adjacent time steps, which gives rise to probabilistic tracking graphs. We derive a stability result for our distance and provide numerous experiments indicating the efficacy of our framework in extracting meaningful feature tracks.},
  archive      = {J_TVCG},
  author       = {Mingzhe Li and Xinyuan Yan and Lin Yan and Tom Needham and Bei Wang},
  doi          = {10.1109/TVCG.2025.3561300},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7951-7969},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Flexible and probabilistic topology tracking with partial optimal transport},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeRFFaceShop: Learning a photo-realistic 3D-aware generative model of animatable and relightable heads from large-scale in-the-wild videos. <em>TVCG</em>, <em>31</em>(10), 7938-7950. (<a href='https://doi.org/10.1109/TVCG.2025.3560869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animatable and relightable 3D facial generation has fundamental applications in computer vision and graphics. Although animation and relighting are highly correlated, previous methods usually address them separately. Effectively combining animation methods and relighting methods is nontrivial. In terms of explicit shading models, animatable methods cannot be easily extended to achieve realistic relighting results, such as shadow effects, due to prohibitive computational training costs. Regarding implicit lighting representations, current animatable methods cannot be incorporated due to their inharmonious animation representations, i.e., deforming spatial points. This paper, armed with a lightweight but effective lighting representation, presents a compatible animation representation to achieve a disentangled generative model of 3D animatable and relightable heads. Our represented animation allows for updating and control of realistic lighting effects. Due to the disentangled nature of our representations, we learn the animation and relighting from large-scale, in-the-wild videos instead of relying on a morphable model. We show that our method can synthesize geometrically consistent and detailed motion along with the disentangled control of lighting conditions. We further show that our method is still compatible with morphable models for driving generated avatars. Our method can also be extended to domains without video data by domain transfer to achieve a broader range of animatable and relightable head synthesis. We will release the code for reproducibility and facilitating future research.},
  archive      = {J_TVCG},
  author       = {Kaiwen Jiang and Feng-Lin Liu and Shu-Yu Chen and Pengfei Wan and Yuan Zhang and Yu-Kun Lai and Hongbo Fu and Lin Gao},
  doi          = {10.1109/TVCG.2025.3560869},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7938-7950},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRFFaceShop: Learning a photo-realistic 3D-aware generative model of animatable and relightable heads from large-scale in-the-wild videos},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JailbreakHunter: A visual analytics approach for jailbreak prompts discovery from large-scale human-LLM conversational datasets. <em>TVCG</em>, <em>31</em>(10), 7923-7937. (<a href='https://doi.org/10.1109/TVCG.2025.3557568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have gained significant attention but also raised concerns due to the risk of misuse. Jailbreak prompts, a popular type of adversarial attack towards LLMs, have appeared and constantly evolved to breach the safety protocols of LLMs. To address this issue, LLMs are regularly updated with safety patches based on reported jailbreak prompts. However, malicious users often keep their successful jailbreak prompts private to exploit LLMs. To uncover these private jailbreak prompts, extensive analysis of large-scale conversational datasets is necessary to identify prompts that still manage to bypass the system's defenses. This task is highly challenging due to the immense volume of conversation data, diverse characteristics of jailbreak prompts, and their presence in complex multi-turn conversations. To tackle these challenges, we introduce JailbreakHunter, a visual analytics approach for identifying jailbreak prompts in large-scale human-LLM conversational datasets. We have designed a workflow with three analysis levels: group-level, conversation-level, and turn-level. Group-level analysis enables users to grasp the distribution of conversations and identify suspicious conversations using multiple criteria, such as similarity with reported jailbreak prompts in previous research and attack success rates. Conversation-level analysis facilitates the understanding of the progress of conversations and helps discover jailbreak prompts within their conversation contexts. Turn-level analysis allows users to explore the semantic similarity and token overlap between a single-turn prompt and the reported jailbreak prompts, aiding in the identification of new jailbreak strategies. The effectiveness and usability of the system were verified through multiple case studies and expert interviews.},
  archive      = {J_TVCG},
  author       = {Zhihua Jin and Shiyi Liu and Haotian Li and Xun Zhao and Huamin Qu},
  doi          = {10.1109/TVCG.2025.3557568},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7923-7937},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {JailbreakHunter: A visual analytics approach for jailbreak prompts discovery from large-scale human-LLM conversational datasets},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminating rasterization: Direct vector floor plan generation with DiffPlanner. <em>TVCG</em>, <em>31</em>(10), 7906-7922. (<a href='https://doi.org/10.1109/TVCG.2025.3559682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The boundary-constrained floor plan generation problem aims to generate the topological and geometric properties of a set of rooms within a given boundary. Recently, learning-based methods have made significant progress in generating realistic floor plans. However, these methods involve a workflow of converting vector data into raster images, using image-based generative models, and then converting the results back into vector data. This process is complex and redundant, often resulting in information loss. Raster images, unlike vector data, cannot scale without losing detail and precision. To address these issues, we propose a novel deep learning framework called DiffPlanner for boundary-constrained floor plan generation, which operates entirely in vector space. Our framework is a Transformer-based conditional diffusion model that integrates an alignment mechanism in training, aligning the optimization trajectory of the model with the iterative design processes of designers. This enables our model to handle complex vector data, better fit the distribution of the predicted targets, accomplish the challenging task of floor plan layout design, and achieve user-controllable generation. We conduct quantitative comparisons, qualitative evaluations, ablation experiments, and perceptual studies to evaluate our method. Extensive experiments demonstrate that DiffPlanner surpasses existing state-of-the-art methods in generating floor plans and bubble diagrams in the creative stages, offering more controllability to users and producing higher-quality results that closely match the ground truths.},
  archive      = {J_TVCG},
  author       = {Shidong Wang and Renato Pajarola},
  doi          = {10.1109/TVCG.2025.3559682},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7906-7922},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eliminating rasterization: Direct vector floor plan generation with DiffPlanner},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised 3D point cloud completion via multi-view adversarial learning. <em>TVCG</em>, <em>31</em>(10), 7890-7905. (<a href='https://doi.org/10.1109/TVCG.2025.3559340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, scanned point clouds are often incomplete due to occlusion issues. The tasks of self-supervised and weakly-supervised point cloud completion involve reconstructing missing regions of these incomplete objects without the supervision of complete ground truth. Current methods either rely on multiple views of partial observations for supervision or overlook the intrinsic geometric similarity that can be identified and utilized from the given partial point clouds. In this paper, we propose MAL-UPC, a framework that effectively leverages both region-level and category-specific geometric similarities to complete missing structures. Our MAL-UPC does not require any 3D complete supervision and only necessitates single-view partial observations in the training set. Specifically, we first introduce a Pattern Retrieval Network to retrieve similar position and curvature patterns between the partial input and the predicted shape, then leverage these similarities to densify and refine the reconstructed results. Additionally, we render the reconstructed complete shape into multi-view depth maps and design an adversarial learning module to learn the geometry of the target shape from category-specific single-view depth images of the partial point clouds in the training set. To achieve anisotropic rendering, we design a density-aware radius estimation algorithm to improve the quality of the rendered images. Our MAL-UPC outperforms current state-of-the-art self-supervised methods and even some unpaired approaches.},
  archive      = {J_TVCG},
  author       = {Lintai Wu and Xianjing Cheng and Yong Xu and Huanqiang Zeng and Junhui Hou},
  doi          = {10.1109/TVCG.2025.3559340},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7890-7905},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unsupervised 3D point cloud completion via multi-view adversarial learning},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stylizing sparse-view 3D scenes with hierarchical neural representation. <em>TVCG</em>, <em>31</em>(10), 7876-7889. (<a href='https://doi.org/10.1109/TVCG.2025.3558468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene stylization refers to generating stylized images of the scene at arbitrary novel view angles following a given set of style images while ensuring consistency when rendered from different views. Recently, several 3D style transfer methods leveraging the scene reconstruction capabilities of pre-trained neural radiance fields (NeRF) have been proposed. To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency.},
  archive      = {J_TVCG},
  author       = {Yifan Wang and Ang Gao and Yi Gong and Yuan Zeng},
  doi          = {10.1109/TVCG.2025.3558468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7876-7889},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stylizing sparse-view 3D scenes with hierarchical neural representation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visagreement: Visualizing and exploring explanations (Dis)Agreement. <em>TVCG</em>, <em>31</em>(10), 7862-7875. (<a href='https://doi.org/10.1109/TVCG.2025.3558074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of distinct machine learning explanation methods has leveraged a number of new issues to be investigated. The disagreement problem is one such issue, as there may be scenarios where the output of different explanation methods disagree with each other. Although understanding how often, when, and where explanation methods agree or disagree is important to increase confidence in the explanations, few works have been dedicated to investigating such a problem. In this work, we proposed Visagreement, a visualization tool designed to assist practitioners in investigating the disagreement problem. Visagreement builds upon metrics to quantitatively compare and evaluate explanations, enabling visual resources to uncover where and why methods mostly agree or disagree. The tool is tailored for tabular data with binary classification and focuses on local feature importance methods. In the provided use cases, Visagreement turned out to be effective in revealing, among other phenomena, how disagreements relate to the quality of the explanations and machine learning model accuracy, thus assisting users in deciding where and when to trust explanations. To assess the effectiveness and practical utility of Visagreement, we conducted an evaluation involving four experts. These experts assessed the tool's Effectiveness, Usability, and Impact on Decision-Making. The experts confirm the Visagreement tool's effectiveness and user-friendliness, making it a valuable asset for analyzing and exploring (dis)agreements.},
  archive      = {J_TVCG},
  author       = {Priscylla Silva and Vitoria Guardieiro and Brian Barr and Claudio Silva and Luis Gustavo Nonato},
  doi          = {10.1109/TVCG.2025.3558074},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7862-7875},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visagreement: Visualizing and exploring explanations (Dis)Agreement},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified smooth vector graphics: Modeling gradient meshes and curve-based approaches jointly as poisson problem. <em>TVCG</em>, <em>31</em>(10), 7848-7861. (<a href='https://doi.org/10.1109/TVCG.2025.3558263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on smooth vector graphics is separated into two independent research threads: one on interpolation-based gradient meshes and the other on diffusion-based curve formulations. With this paper, we propose a mathematical formulation that unifies gradient meshes and curve-based approaches as solution to a Poisson problem. To combine these two well-known representations, we first generate a non-overlapping intermediate patch representation that specifies for each patch a target Laplacian and boundary conditions. Unifying the treatment of boundary conditions adds further artistic degrees of freedoms to the existing formulations, such as Neumann conditions on diffusion curves. To synthesize a raster image for a given output resolution, we then rasterize boundary conditions and Laplacians for the respective patches and compute the final image as solution to a Poisson problem. We evaluate the method on various test scenes containing gradient meshes and curve-based primitives. Since our mathematical formulation works with established smooth vector graphics primitives on the front-end, it is compatible with existing content creation pipelines and with established editing tools. Rather than continuing two separate research paths, we hope that a unification of the formulations will lead to new rasterization and vectorization tools in the future that utilize the strengths of both approaches.},
  archive      = {J_TVCG},
  author       = {Xingze Tian and Tobias Günther},
  doi          = {10.1109/TVCG.2025.3558263},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7848-7861},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unified smooth vector graphics: Modeling gradient meshes and curve-based approaches jointly as poisson problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous presence continuum: Portal overlays for overlapping worlds in virtual reality. <em>TVCG</em>, <em>31</em>(10), 7834-7847. (<a href='https://doi.org/10.1109/TVCG.2025.3558178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Simultaneous Presence (SP) Continuum, a novel concept designed to enhance the use of portals in virtual reality (VR) by understanding users’ experiences across multiple environments. Portals traditionally provide access to secondary worlds, but their limited field of view (FoV) can hinder full engagement with these environments. To overcome this, we introduce portal overlays, which expand the portal's FoV by superimposing the secondary world over the user's view of the primary world. We explore several overlay techniques—Contours, Blended (Opacity and Stencil), and Absolute—to adjust user experience across the SP Continuum. The Contours overlay, renders only the edges of objects, offering a minimalistic view and immersion of the secondary world. The Blended overlay allows for adjustable immersion between worlds through opacity or the distribution of pixels. The Absolute overlay virtually immerses users completely in the secondary world. Importantly, these overlays are context-activated instead of always on, to suit situational needs. Our study revealed high SP was possible in both worlds, and overlays significantly impacted users’ experiences on the SP Continuum. The Blended overlay provided balanced SP, while Contours had the highest primary presence but lower secondary presence. In contrast, the Absolute overlay, alternating full immersion between worlds, had the highest secondary presence but resulted in reduced primary presence, slower navigation and selection times, higher effort and frustration, and lower usability.},
  archive      = {J_TVCG},
  author       = {Daniel Ablett and Andrew Cunningham and Gun A. Lee and Bruce H. Thomas},
  doi          = {10.1109/TVCG.2025.3558178},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7834-7847},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simultaneous presence continuum: Portal overlays for overlapping worlds in virtual reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StruGauAvatar: Learning structured 3D gaussians for animatable avatars from monocular videos. <em>TVCG</em>, <em>31</em>(10), 7820-7833. (<a href='https://doi.org/10.1109/TVCG.2025.3557457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been witnessed in the field of neural 3D avatar reconstruction. Among all related tasks, building an animatable avatar from monocular videos is one of the most challenging ones, yet it also has a wide range of applications. The “animatable” means that we need to transfer any arbitrary and unseen poses onto the avatar and generate new 3D videos. Thanks to the rise of the powerful representation of NeRF, generating a high-fidelity animatable avatar from videos has become easier and more accessible. Despite their impressive visual results, the substantial training and rendering overhead dramatically hamper their applications. 3D Gaussian Splatting, as a timely new representation, has demonstrated its high-quality and high-efficiency rendering. This has led to many concurrent works to introduce 3D-GS to animatable avatar building. Although they demonstrate very high-fidelity renderings for poses similar to the training video frames, poor results are produced when the poses are far from training. We argue that this is primarily because the Gaussian points lack structures. Thus, we suggest involving DMTet to represent the coarse geometry of the avatar. In our representation, the majority of Gaussian points are bound to the mesh vertices, while some free Gaussian is allowed to expand to better fit the given video. Furthermore, we develop a dual-space optimization framework to jointly optimize the DMTet, Gaussian points, and skinning weights under two spaces. In this sense, Gaussian points are transformed in a constrained way, which dramatically improves the generalization ability for unseen poses. This is well demonstrated via extensive experiments.},
  archive      = {J_TVCG},
  author       = {Yihao Zhi and Wanhu Sun and Jiahao Chang and Chongjie Ye and Wensen Feng and Xiaoguang Han},
  doi          = {10.1109/TVCG.2025.3557457},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7820-7833},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StruGauAvatar: Learning structured 3D gaussians for animatable avatars from monocular videos},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palette-based color harmonization. <em>TVCG</em>, <em>31</em>(10), 7809-7819. (<a href='https://doi.org/10.1109/TVCG.2025.3546210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a palette-based framework for color composition for visual applications and three large-scale, wide-ranging perceptual studies on the perception of color harmonization. We abstract relationships between palette colors as a compact set of axes describing harmonic templates over perceptually uniform color wheels. Our framework provides a basis for interactive color-aware operations such as color harmonization of images and videos. Because our approach to harmonization is palette-based, we are able to conduct the first controlled perceptual experiments evaluating preferences for harmonized images and color palettes. In a third study, we compare preference for archetypical harmonic palettes. In total, our studies involved over 1000 participants. We found that participants do not prefer harmonized images and that some archetypal palettes are reliably viewed as less harmonious than random palettes. These studies raise important questions for research and artistic practice.},
  archive      = {J_TVCG},
  author       = {Jianchao Tan and Jose Echevarria and Yotam Gingold},
  doi          = {10.1109/TVCG.2025.3546210},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7809-7819},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Palette-based color harmonization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelize over data particle advection: Participation, ping pong particles, and overhead. <em>TVCG</em>, <em>31</em>(10), 7795-7808. (<a href='https://doi.org/10.1109/TVCG.2025.3557453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle advection is one of the foundational algorithms for visualization and analysis and is central to understanding vector fields common to scientific simulations. Achieving efficient performance with large data in a distributed memory setting is notoriously difficult. Because of its simplicity and minimized movement of large vector field data, the Parallelize over Data (POD) algorithm has become a de facto standard. Despite its simplicity and ubiquitous usage, the scaling issues with the POD algorithm are known and have been described throughout the literature. In this paper, we describe a set of in-depth analyses of the POD algorithm that shed new light on the underlying causes for the poor performance of this algorithm. We designed a series of representative workloads to study the performance of the POD algorithm and executed them on a supercomputer while collecting timing and statistical data for analysis. we then performed two different types of analysis. In the first analysis, we introduce two novel metrics for measuring algorithmic efficiency over the course of a workload run. The second analysis was from the perspective of the particles being advected. Using particle-centric analysis, we identify that the overheads associated with particle movement between processes (not the communication itself) have a dramatic impact on the overall execution time. These overheads become particularly costly when flow features span multiple blocks, resulting in repeated particle circulation (which we term “ping pong particles”) between blocks. Our findings shed important light on the underlying causes of poor performance and offer directions for future research to address these limitations.},
  archive      = {J_TVCG},
  author       = {Zhe Wang and Kenneth Moreland and Matthew Larsen and James Kress and Hank Childs and David Pugmire},
  doi          = {10.1109/TVCG.2025.3557453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7795-7808},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parallelize over data particle advection: Participation, ping pong particles, and overhead},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep frequency awareness functional maps for robust shape matching. <em>TVCG</em>, <em>31</em>(10), 7781-7794. (<a href='https://doi.org/10.1109/TVCG.2025.3556209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional deep functional map frameworks are widely used for 3D shape matching; however, many methods fail to adaptively capture the relevant frequency information required for functional map estimation in complex scenarios, leading to poor performance, especially under significant deformations. To address these challenges, we propose a novel unsupervised learning-based framework, Deep Frequency Awareness Functional Maps (DFAFM), specifically designed to tackle diverse shape-matching problems. Our approach introduces the Spectral Filter Operator Preservation constraint, which ensures the preservation of critical frequency information. These constraints promote frequency awareness by learning a set of spectral filters and incorporating them as a loss function to jointly supervise the functional maps, pointwise maps, and spectral filters. The spectral filters are constructed using orthonormal Jacobi polynomials with learnable coefficients, enabling adaptive and efficient frequency representation. Furthermore, we propose a refinement strategy that leverages the learned spectral filters and constraints to enhance the accuracy of the final pointwise map. Extensive experiments conducted on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art approaches, particularly in challenging scenarios involving non-isometric deformations and inconsistent topology.},
  archive      = {J_TVCG},
  author       = {Feifan Luo and Qinsong Li and Ling Hu and Haibo Wang and Haojun Xu and Xinru Liu and Shengjun Liu and Hongyang Chen},
  doi          = {10.1109/TVCG.2025.3556209},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7781-7794},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep frequency awareness functional maps for robust shape matching},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GaussEdit: Adaptive 3D scene editing with text and image prompts. <em>TVCG</em>, <em>31</em>(10), 7769-7780. (<a href='https://doi.org/10.1109/TVCG.2025.3556745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Junlong Yu and Kai Chao and Shiqing Xin and Ligang Liu},
  doi          = {10.1109/TVCG.2025.3556745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7769-7780},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GaussEdit: Adaptive 3D scene editing with text and image prompts},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IllumiDiff: Indoor illumination estimation from a single image with diffusion model. <em>TVCG</em>, <em>31</em>(10), 7752-7768. (<a href='https://doi.org/10.1109/TVCG.2025.3553853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illumination estimation from a single indoor image is a promising yet challenging task. Existing indoor illumination estimation methods mainly regress lighting parameters or infer a panorama from a limited field-of-view image. Nevertheless, these methods fail to recover a panorama with both well-distributed illumination and detailed environment textures, leading to a lack of realism in rendering the embedded 3D objects with complex materials. This paper presents a novel multi-stage illumination estimation framework named IllumiDiff. Specifically, in Stage I, we first estimate illumination conditions from the input image, including the illumination distribution as well as the environmental texture of the scene. In Stage II, guided by the estimated illumination conditions, we design a conditional panoramic texture diffusion model to generate a high-quality LDR panorama. In Stage III, we leverage the illumination conditions to further reconstruct the LDR panorama to an HDR panorama. Extensive experiments demonstrate that our IllumiDiff can generate an HDR panorama with realistic illumination distribution and rich texture details from a single limited field-of-view indoor image. The generated panorama can produce impressive rendering results for the embedded 3D objects with various materials.},
  archive      = {J_TVCG},
  author       = {Shiyuan Shen and Zhongyun Bao and Wenju Xu and Chunxia Xiao},
  doi          = {10.1109/TVCG.2025.3553853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7752-7768},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IllumiDiff: Indoor illumination estimation from a single image with diffusion model},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViTon-GUN: Person-to-person virtual try-on via garment unwrapping. <em>TVCG</em>, <em>31</em>(10), 7740-7751. (<a href='https://doi.org/10.1109/TVCG.2025.3550776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-based Person-to-Person (P2P) virtual try-on, involving the direct transfer of garments from one person to another, is one of the most promising applications of human-centric image generation. However, existing approaches struggle to accurately learn the clothing deformation when directly warping the garment from the source pose onto the target pose. To address this, we propose Person-to-Person virtual try-on via Garment UNwrapping, a novel framework dubbed as ViTon-GUN. Specifically, we divide the P2P task into two subtasks: Person-to-Garment (P2G) and Garment-to-Person (G2P). The P2G aims to unwrap the target garment from a source pose to a canonical representation based on A-Pose. In the P2G stage, we enable the implementation of a flow-based P2G scheme by introducing an A-Pose estimator and establishing comprehensive training conditions. Building upon this step-wise strategy, we introduce a novel pipeline for P2P try-on. Once trained, the P2G strategy can serve as a “plug-and-play” module, which efficiently adapts existing diffusion-based pre-trained G2P models to P2P try-on without further training. Quantitative and qualitative experiments demonstrate that our ViTon-GUN performs remarkably well on P2P try-on, even for dresses with intricate design details.},
  archive      = {J_TVCG},
  author       = {Nannan Zhang and Zhenyu Xie and Zhengwentai Sun and Hairui Zhu and Zirong Jin and Nan Xiang and Xiaoguang Han and Song Wu},
  doi          = {10.1109/TVCG.2025.3550776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7740-7751},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViTon-GUN: Person-to-person virtual try-on via garment unwrapping},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). As-rigid-as-possible deformation of gaussian radiance fields. <em>TVCG</em>, <em>31</em>(10), 7727-7739. (<a href='https://doi.org/10.1109/TVCG.2025.3555404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) models radiance fields as sparsely distributed 3D Gaussians, providing a compelling solution to novel view synthesis at high resolutions and real-time frame rates. However, deforming objects represented by 3D Gaussians remains a challenging task. Existing methods deform a 3DGS object by editing Gaussians geometrically. These approaches ignore the fact that it is the radiance field that rasterizes and renders the final image. The inconsistency between the deformed 3D Gaussians and the desired radiance field inevitably leads to artifacts in the final results. In this paper, we propose an interactive method for as-rigid-as-possible (ARAP) deformation of the Gaussian radiance fields. Specifically, after performing geometric edits on the Gaussians, we further optimize Gaussians to ensure its rasterization yields a similar result as the deformed radiance field. To facilitate this objective, we design radial features to mathematically describe the radial difference before and after the deformation, which are densely sampled across the radiance field. Additionally, we propose an adaptive anisotropic spatial low-pass filter to prevent aliasing issues during sampling and to preserve the field with the varying non-uniform sampling intervals. Users can interactively employ this tool to achieve large-scale ARAP deformations of the radiance field. Since our method maintains the consistency of the Gaussian radiance field before and after deformation, it avoids artifacts that are common in existing 3DGS deformation frameworks. Meanwhile, our method keeps the high quality and efficiency of 3DGS in rendering.},
  archive      = {J_TVCG},
  author       = {Xinhao Tong and Tianjia Shao and Yanlin Weng and Yin Yang and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3555404},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7727-7739},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {As-rigid-as-possible deformation of gaussian radiance fields},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual aware foveated rendering. <em>TVCG</em>, <em>31</em>(10), 7711-7726. (<a href='https://doi.org/10.1109/TVCG.2025.3554737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of geometry and rendering effects in virtual reality (VR) scenes, existing foveated rendering methods for VR head-mounted displays (HMDs) struggle to meet users’ demands for VR scene rendering with high frame rates ($\geq 60fps$ for rendering binocular foveated images in VR scenes containing over 50 m triangles). Current research validates that auditory content affects the perception of the human visual system (HVS). However, existing foveated rendering methods primarily model the HVS's eccentricity-dependent visual perception ability on the visual content in VR while ignoring the impact of auditory content on the HVS's visual perception. In this article, we introduce an auditory-content-based perceived rendering quality analysis to quantify the impact of visual perception under different auditory conditions in foveated rendering. Based on the analysis results, we propose an audio-visual aware foveated rendering method (AvFR). AvFR first constructs an audio-visual feature-driven perception model that predicts the eccentricity-based visual perception in real time by combining the scene's audio-visual content, and then proposes a foveated rendering cost optimization algorithm to adaptively control the shading rate of different regions with the guidance of the perception model. In complex scenes with visual and auditory content containing over 1.17 m triangles, AvFR renders high-quality binocular foveated images at an average frame rate of 116$fps$. The results of the main user study and performance evaluation validate that AvFR achieves significant performance improvement (up to 1.4× speedup) without lowering the perceived visual quality compared with the state-of-the-art VR-HMD foveated rendering method.},
  archive      = {J_TVCG},
  author       = {Xuehuai Shi and Yucheng Li and Jiaheng Li and Jian Wu and Jieming Yin and Xiaobai Chen and Lili Wang},
  doi          = {10.1109/TVCG.2025.3554737},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7711-7726},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Audio-visual aware foveated rendering},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ${\rm{H}}_{2}{\rm{O}}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi mathvariant="normal">H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:math>-NeRF: Radiance fields reconstruction for two-hand-held objects. <em>TVCG</em>, <em>31</em>(10), 7696-7710. (<a href='https://doi.org/10.1109/TVCG.2025.3553975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work aims to reconstruct the appearance and geometry of the two-hand-held object from a sequence of color images. In contrast to traditional single-hand-held manipulation, two-hand-holding allows more flexible interaction, thereby providing back views of the object, which is particularly convenient for reconstruction but generates complex view-dependent occlusions. The recent development of neural rendering provides new potential for hand-held object reconstruction. In this paper, we propose a novel neural representation-based framework to recover radiance fields of the two-hand-held object, named ${\rm{H}}_{2}{\rm{O}}$-NeRF. We first design an object-centric semantic module based on the geometric signed distance function cues to predict 3D object-centric regions and develop the view-dependent visible module based on the image-related cues to label 2D occluded regions. We then combine them to obtain a 2D visible mask that adaptively guides ray sampling on the object for optimization. We also provide a newly collected ${\rm{H}}_{2}{\rm{O}}$ dataset to validate the proposed method. Experiments show that our method achieves superior performance on reconstruction completeness and view-consistency synthesis compared to the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Xinxin Liu and Qi Zhang and Xin Huang and Ying Feng and Guoqing Zhou and Qing Wang},
  doi          = {10.1109/TVCG.2025.3553975},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7696-7710},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {${\rm{H}}_{2}{\rm{O}}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi mathvariant="normal">H</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:math>-NeRF: Radiance fields reconstruction for two-hand-held objects},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving neural volume rendering via learning view-dependent integral approximation. <em>TVCG</em>, <em>31</em>(10), 7684-7695. (<a href='https://doi.org/10.1109/TVCG.2025.3554692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields (NeRFs) have achieved impressive view synthesis results by learning an implicit volumetric representation from multi-view images. To project the implicit representation into an image, NeRF employs volume rendering that approximates the continuous integrals of rays as an accumulation of the colors and densities of the sampled points. Although this approximation enables efficient rendering, it ignores the direction information in point intervals, resulting in ambiguous features and limited reconstruction quality. In this paper, we propose a learning method that utilizes learnable view-dependent features to improve scene representation and reconstruction. We model the volume rendering integral with a piecewise constant volume density and spherical harmonic-guided view-dependent features, facilitating ambiguity elimination while preserving the rendering efficiency. In addition, we introduce a regularization term that restricts the anisotropic representation effect to be local, with negligible effect on geometry representations, and that encourages recovering the correct geometry. Our method is flexible and can be plugged into NeRF-based frameworks. Extensive experiments show that the proposed representation can boost the rendering quality of various NeRFs and achieve state-of-the-art rendering performance on both synthetic and real-world scenes.},
  archive      = {J_TVCG},
  author       = {Yifan Wang and Jun Xu and Yuan Zeng and Yi Gong},
  doi          = {10.1109/TVCG.2025.3554692},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7684-7695},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving neural volume rendering via learning view-dependent integral approximation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mixed reality car A-pillar design support system utilizing projection mapping. <em>TVCG</em>, <em>31</em>(10), 7674-7683. (<a href='https://doi.org/10.1109/TVCG.2025.3554037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping (PM) is useful in the product design process, since it seamlessly bridges a physical mockup and its digital twin by allowing designers to interactively explore new textures, colors, and shapes without the need to create new physical mockups. While PM has proven effective for car interior design, previous research focused solely on supporting the design of dashboards and instrument panels, neglecting evaluation in realistic driving scenarios. This paper introduces a self-contained car interior design support system that extends beyond the dashboard to include the A-pillars. Additionally, to enable designers to evaluate their designs in authentic driving conditions, we integrate a driving simulator, complete with a motion platform, into the PM system. Through the construction of a prototype, we demonstrate the feasibility of our proposed system. Finally, through user studies, we derive guidelines for PM-based car interior design to optimize the user experience.},
  archive      = {J_TVCG},
  author       = {Ryotaro Yoshida and Toshihiro Hara and Yusaku Takeda and Kenji Murase and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2025.3554037},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7674-7683},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A mixed reality car A-pillar design support system utilizing projection mapping},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-view 3D hair modeling with clumping optimization. <em>TVCG</em>, <em>31</em>(10), 7661-7673. (<a href='https://doi.org/10.1109/TVCG.2025.3552919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning advancements have enabled the generation of visually plausible hair geometry from a single image, but the results still do not meet the realism required for further applications (e.g., high quality hair rendering and simulation). One of the essential element that is missing in previous single-view hair reconstruction methods is the clumping effect of hair, which is influenced by scalp secretions and oils, and is a key ingredient for high-quality hair rendering and simulation. Inspired by common practices in industrial production which simulates realistic hair clumping by allowing artists to adjust clumping parameters, we aim to integrate these clumping effects into single-view hair reconstruction. We introduce a hierarchical hair representation that incorporates a clumping modifier into the guide hair and skinning-based hair expressions. This representation utilizes guide strands and skinning weights to express the basic geometric structure of the hair. The clumping modifier allows for the expression of more detailed and realistic clumping effects. Based on this representation, We design a fully differentiable framework integrating a neural measurement of clumping and a line-based rasterization renderer to iteratively solve guide strands positions and clumping parameters. Our method demonstrates superior performance both qualitatively and quantitatively compared to state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Zhongsi Tang and Jiahao Geng and Yanlin Weng and Youyi Zheng and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3552919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7661-7673},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Single-view 3D hair modeling with clumping optimization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SynthLens: Visual analytics for facilitating multi-step synthetic route design. <em>TVCG</em>, <em>31</em>(10), 7647-7660. (<a href='https://doi.org/10.1109/TVCG.2025.3552134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing synthetic routes for novel molecules is pivotal in various fields like medicine and chemistry. In this process, researchers need to explore a set of synthetic reactions to transform starting molecules into intermediates step by step until the target novel molecule is obtained. However, designing synthetic routes presents challenges for researchers. First, researchers need to make decisions among numerous possible synthetic reactions at each step, considering various criteria (e.g., yield, experimental duration, and the count of experimental steps) to construct the synthetic route. Second, they must consider the potential impact of one choice at each step on the overall synthetic route. To address these challenges, we proposed SynthLens, a visual analytics system to facilitate the iterative construction of synthetic routes by exploring multiple possibilities for synthetic reactions at each step of construction. Specifically, we have introduced a tree-form visualization in SynthLensto compare and evaluate all the explored routes at various exploration steps, considering both the exploration step and multiple criteria. Our system empowers researchers to consider their construction process comprehensively, guiding them toward promising exploration directions to complete the synthetic route. We validated the usability and effectiveness of SynthLensthrough a quantitative evaluation and expert interviews, highlighting its role in facilitating the design process of synthetic routes. Finally, we discussed the insights of SynthLensto inspire other multi-criteria decision-making scenarios with visual analytics.},
  archive      = {J_TVCG},
  author       = {Qipeng Wang and Rui Sheng and Shaolun Ruan and Xiaofu Jin and Chuhan Shi and Min Zhu},
  doi          = {10.1109/TVCG.2025.3552134},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7647-7660},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SynthLens: Visual analytics for facilitating multi-step synthetic route design},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HYPNOS: Interactive data lineage tracing for data transformation scripts. <em>TVCG</em>, <em>31</em>(10), 7632-7646. (<a href='https://doi.org/10.1109/TVCG.2025.3552091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a formal data analysis workflow, data validation is a necessary step that helps data analysts verify the quality of the data and ensure the reliability of the results. Data analysts usually need to validate the result when encountering an unexpected result, such as an abnormal record in a table. In order to understand how a specific record is derived, they would backtrace it in the pipeline step by step via checking the code lines, exposing the intermediate tables, and finding the data records from which it is derived. However, manually reviewing code and backtracing data requires certain expertise, while inspecting the traced records in multiple tables and interpreting their relationships is tedious. In this work, we propose HYPNOS, a visualization system that supports interactive data lineage tracing for data transformation scripts. HYPNOS uses a lineage module for parsing and adapting code to capture both schema-level and instance-level data lineage from data transformation scripts. Then, it provides users with a lineage view for obtaining an overview of the data transformation process and a detail view for tracing instance-level data lineage and inspecting details. HYPNOS reveals different levels of data relationships and helps users with data lineage tracing. We demonstrate the usability and effectiveness of HYPNOS through a use case, interviews of four expert users, and a user study.},
  archive      = {J_TVCG},
  author       = {Xiwen Cai and Xiaodong Ge and Kai Xiong and Shuainan Ye and Di Weng and Ke Xu and Datong Wei and Jiang Long and Yingcai Wu},
  doi          = {10.1109/TVCG.2025.3552091},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7632-7646},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HYPNOS: Interactive data lineage tracing for data transformation scripts},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraVIS: A user trace analyzer to support user-centered design of visual analytics solutions. <em>TVCG</em>, <em>31</em>(10), 7614-7631. (<a href='https://doi.org/10.1109/TVCG.2025.3546863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Analytics (VA) has become a paramount discipline in supporting data analysis in many scientific domains, empowering the human user with automatic capabilities while keeping the lead in the analysis. At the same time, designing an effective VA solution is not a simple task, requiring its adaptation to the problem at hand and the intended user of the system. In this scenario, the User-Centered Design (UCD) methodology provides the framework to incorporate user needs into the design of a VA solution. On the other hand, its implementation mainly relies on qualitative feedback, with the designer missing tools supporting her in quantitatively reporting the user feedback and using it to hypothesize and test the successive changes to the VA solution. To overcome this limitation, we propose TraVIS, a Visual Analytics solution allowing the loading of a web-based VA system, collecting user traces, and analyzing them with respect to the system at hand. In this process, the designer can leverage the collected traces and relate them to the tasks the VA solution supports and how those can be achieved. Using TraVIS, the designer can identify ineffective interaction paths, analyze the user traces support to task completion, hypothesize corrections to the design, and evaluate the effect of changes. We evaluated TraVIS through experimentation with 11 VA systems from literature, a use case, and user evaluation with five experts. Results show the benefits that TraVIS provides in terms of identifying design problems and efficient support for UCD.},
  archive      = {J_TVCG},
  author       = {Matteo Filosa and Alexandra Plexousaki and Matteo Di Stadio and Francesco Bovi and Dario Benvenuti and Tiziana Catarci and Marco Angelini},
  doi          = {10.1109/TVCG.2025.3546863},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7614-7631},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TraVIS: A user trace analyzer to support user-centered design of visual analytics solutions},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why is AI not a panacea for data workers? an interview study on human-AI collaboration in data storytelling. <em>TVCG</em>, <em>31</em>(10), 7598-7613. (<a href='https://doi.org/10.1109/TVCG.2025.3552017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the potential for human-AI collaboration in the context of data storytelling for data workers. Data storytelling communicates insights and knowledge from data analysis. It plays a vital role in data workers’ daily jobs since it boosts team collaboration and public communication. However, to make an appealing data story, data workers need to spend tremendous effort on various tasks, including outlining and styling the story. Recently, a growing research trend has been exploring how to assist data storytelling with advanced artificial intelligence (AI). However, existing studies focus more on individual tasks in the workflow of data storytelling and do not reveal a complete picture of humans’ preference for collaborating with AI. To address this gap, we conducted an interview study with 18 data workers to explore their preferences for AI collaboration in the planning, implementation, and communication stages of their workflow. We propose a framework for expected AI collaborators’ roles, categorize people's expectations for the level of automation for different tasks, and delve into the reasons behind them. Our research provides insights and suggestions for the design of future AI-powered data storytelling tools.},
  archive      = {J_TVCG},
  author       = {Haotian Li and Yun Wang and Q. Vera Liao and Huamin Qu},
  doi          = {10.1109/TVCG.2025.3552017},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7598-7613},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Why is AI not a panacea for data workers? an interview study on human-AI collaboration in data storytelling},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented dynamic data physicalization: Blending shape-changing data sculptures with virtual content for interactive visualization. <em>TVCG</em>, <em>31</em>(10), 7580-7597. (<a href='https://doi.org/10.1109/TVCG.2025.3547432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the concept of Augmented Dynamic Data Physicalization, the combination of shape-changing physical data representations with high-resolution virtual content. Tangible data sculptures, for example using mid-air shape-changing interfaces, are aesthetically appealing and persistent, but also technically and spatially limited. Blending them with Augmented Reality overlays such as scales, labels, or other contextual information opens up new possibilities. We explore the potential of this promising combination and propose a set of essential visualization components and interaction principles. They facilitate sophisticated hybrid data visualizations, for example Overview & Detail techniques or 3D view aggregations. We discuss three implemented applications that demonstrate how our approach can be used for personal information hubs, interactive exhibitions, and immersive data analytics. Based on these use cases, we conducted hands-on sessions with external experts, resulting in valuable feedback and insights. They highlight the potential of combining dynamic physicalizations with dynamic AR overlays to create rich and engaging data experiences.},
  archive      = {J_TVCG},
  author       = {Severin Engert and Andreas Peetz and Konstantin Klamka and Pierre Surer and Tobias Isenberg and Raimund Dachselt},
  doi          = {10.1109/TVCG.2025.3547432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7580-7597},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented dynamic data physicalization: Blending shape-changing data sculptures with virtual content for interactive visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowHON: Representing flow fields using higher-order networks. <em>TVCG</em>, <em>31</em>(10), 7565-7579. (<a href='https://doi.org/10.1109/TVCG.2025.3550130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow fields are often partitioned into data blocks for massively parallel computation and analysis based on blockwise relationships. However, most of the previous techniques only consider the first-order dependencies among blocks, which is insufficient in describing complex flow patterns. In this work, we present FlowHON, an approach to construct higher-order networks (HONs) from flow fields. FlowHON captures the inherent higher-order dependencies in flow fields as nodes and estimates the transitions among them as edges. We formulate the HON construction as an optimization problem with three linear transformations. The first two layers correspond to the node generation and the third one corresponds to edge estimation. Our formulation allows the node generation and edge estimation to be solved in a unified framework. With FlowHON, the rich set of traditional graph algorithms can be applied without any modification to analyze flow fields, while leveraging the higher-order information to understand the inherent structure and manage flow data for efficiency. We demonstrate the effectiveness of FlowHON using a series of downstream tasks, including estimating the density of particles during tracing, partitioning flow fields for data management, and understanding flow fields using the node-link diagram representation of networks.},
  archive      = {J_TVCG},
  author       = {Nan Chen and Zhihong Li and Jun Tao},
  doi          = {10.1109/TVCG.2025.3550130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7565-7579},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlowHON: Representing flow fields using higher-order networks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TextIR: A simple framework for text-based editable image restoration. <em>TVCG</em>, <em>31</em>(10), 7549-7564. (<a href='https://doi.org/10.1109/TVCG.2025.3550844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many current image restoration approaches utilize neural networks to acquire robust image-level priors from extensive datasets, aiming to reconstruct missing details. Nevertheless, these methods often falter with images that exhibit significant information gaps. While incorporating external priors or leveraging reference images can provide supplemental information, these strategies are limited in their practical scope. Alternatively, textual inputs offer greater accessibility and adaptability. In this study, we develop a sophisticated framework enabling users to guide the restoration of deteriorated images via textual descriptions. Utilizing the text-image compatibility feature of CLIP enhances the integration of textual and visual data. Our versatile framework supports multiple restoration activities such as image inpainting, super-resolution, and colorization. Comprehensive testing validates our technique's efficacy.},
  archive      = {J_TVCG},
  author       = {Yunpeng Bai and Cairong Wang and Shuzhao Xie and Chao Dong and Chun Yuan and Zhi Wang},
  doi          = {10.1109/TVCG.2025.3550844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7549-7564},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TextIR: A simple framework for text-based editable image restoration},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Magic-tap: A kinematics-driven virtual hand selection technique in AR/VR. <em>TVCG</em>, <em>31</em>(10), 7535-7548. (<a href='https://doi.org/10.1109/TVCG.2025.3549044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the design of a selection technique in virtual environments leveraging kinematic data derived from hand movements. We first identified the intrinsic challenges of virtual hand selection techniques, particularly in complex settings, including Accidental Selection, Slow Selection, Failed Selection, and Fragmented Selection. To mitigate these issues, we introduce Magic-Tap, a selection technique that ascertains the trigger of an object based on real-time variations in virtual hand acceleration and speed, seamlessly integrating the pointing and triggering processes without requiring explicit triggering signals. The parameter settings of Magic-Tap were fine-tuned through Study One, ameliorating its trigger rate, error rate, and trigger time. Furthermore, we compared Magic-Tap with three conventional virtual hand selection techniques (Touch, Dwell-Time, and Pinch) in Study Two. The results indicate that the task completion time of Magic-Tap is comparable to Touch in all situations while exhibiting an error rate as low as Dwell-Time and Pinch.},
  archive      = {J_TVCG},
  author       = {Ruyang Yu and Yixuan Liu and Zijian Wu and Tao Luo},
  doi          = {10.1109/TVCG.2025.3549044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7535-7548},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Magic-tap: A kinematics-driven virtual hand selection technique in AR/VR},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time neural homogeneous translucent material rendering using diffusion blocks. <em>TVCG</em>, <em>31</em>(10), 7519-7534. (<a href='https://doi.org/10.1109/TVCG.2025.3548442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering realistic appearances of homogeneous translucent materials, such as milk and marble, poses challenges due to the complexity of subsurface scattering. In this paper, we present a neural method for real-time rendering of homogeneous translucent objects. Based on the observation that light propagation inside a highly scattered media is like a diffusion process (Stam 1995), we propose a neural data structure named diffusion block to mimic the behavior of the diffusion process. The diffusion block is built upon a recent network structure named DiffusionNet (Sharp et al. 2022) with a few modifications to adapt to our problem of translucent rendering. Our network is lightweight and efficient, leading to a real-time rendering method. Furthermore, our method supports dynamic material properties and diverse lighting conditions. Comparisons with state-of-the-art real-time translucent rendering methods demonstrate the superiority of our method in rendering quality.},
  archive      = {J_TVCG},
  author       = {Di An and Liangfu Kang and Kun Xu},
  doi          = {10.1109/TVCG.2025.3548442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7519-7534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time neural homogeneous translucent material rendering using diffusion blocks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised non-rigid human point cloud registration based on deformation field fusion. <em>TVCG</em>, <em>31</em>(10), 7506-7518. (<a href='https://doi.org/10.1109/TVCG.2025.3547778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human point cloud registration is a critical problem in the fields of computer vision and computer graphics applications. Currently, due to the presence of joint hinges and limb occlusions in human point clouds, point cloud alignment is challenging. To address these two limits, this paper proposes an unsupervised non-rigid human point cloud registration method based on deformation field fusion. The method mainly consists of the deep dynamic link deformation field estimation module and the probabilistic alignment deformation field estimation module. The deep dynamic link deformation field estimation module uses a time series network to convert non-rigid deformation into multiple rigid deformations. Then, feature extraction is performed to estimate the deformation field based on the rigid deformations. The probabilistic alignment deformation field estimation module builds on a Gaussian mixture model and adds local and global constraint conditions for deformation field estimation. Finally, the two deformation fields are fused into the total deformed field by aligning them, which enhances the sensitivity to both global and local feature information. The experimental results on public datasets and real private datasets demonstrate that the proposed method has higher accuracy and better robustness under joint hinges and limb adhesion conditions.},
  archive      = {J_TVCG},
  author       = {Yinghao Li and Yue Liu and Zhiyuan Dong and Linjun Jiang and Yusong Lin},
  doi          = {10.1109/TVCG.2025.3547778},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7506-7518},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unsupervised non-rigid human point cloud registration based on deformation field fusion},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of sensorimotor contingencies and eye scanpath entropy in presence in virtual reality: A reinforcement learning paradigm. <em>TVCG</em>, <em>31</em>(10), 7492-7505. (<a href='https://doi.org/10.1109/TVCG.2025.3547241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensorimotor contingencies (SC) refer to the rules by which we use our body to perceive. It has been argued that to the extent that a virtual reality (VR) application affords natural SC so the greater likelihood that participants will experience Place Illusion (PI), the illusion of ‘being there’ (a component of presence) in the virtual environment. However, notwithstanding numerous studies this only has anecdotal support. Here we used a reinforcement learning (RL) paradigm where 26 participants experienced a VR scenario where the RL agent could sequentially propose changes to 5 binary factors: mono or stereo vision, 3 or 6 degrees of freedom head tracking, mono or spatialised sound, low or high display resolution, or one of two color schemes. The first 4 are SC, whereas the last is not. Participants could reject or accept each change proposed by the RL, until convergence. Participants were more likely to accept changes from low to high SC than changes to the color. Additionally, theory suggests that increased PI should be associated with lower eye scanpath entropy. Our results show that mean entropy did decrease over time and the final level of entropy was negatively correlated with a post exposure questionnaire-based assessment of PI.},
  archive      = {J_TVCG},
  author       = {Esen Küçüktütüncü and Francisco Macia-Varela and Joan Llobera and Mel Slater},
  doi          = {10.1109/TVCG.2025.3547241},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7492-7505},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of sensorimotor contingencies and eye scanpath entropy in presence in virtual reality: A reinforcement learning paradigm},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GlossyGS: Inverse rendering of glossy objects with 3D gaussian splatting. <em>TVCG</em>, <em>31</em>(10), 7478-7491. (<a href='https://doi.org/10.1109/TVCG.2025.3547063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing objects from posed images is a crucial and complex task in computer graphics and computer vision. While NeRF-based neural reconstruction methods have exhibited impressive reconstruction ability, they tend to be time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS) for inverse rendering, which have led to quick and effective outcomes. However, these techniques generally have difficulty in producing believable geometries and materials for glossy objects, a challenge that stems from the inherent ambiguities of inverse rendering. To address this, we introduce GlossyGS, an innovative 3D-GS-based inverse rendering framework that aims to precisely reconstruct the geometry and materials of glossy objects by integrating material priors. The key idea is the use of micro-facet geometry segmentation prior, which helps to reduce the intrinsic ambiguities and improve the decomposition of geometries and materials. Additionally, we introduce a normal map prefiltering strategy to more accurately simulate the normal distribution of reflective surfaces. These strategies are integrated into a hybrid geometry and material representation that employs both explicit and implicit methods to depict glossy objects. We demonstrate through quantitative analysis and qualitative visualization that the proposed method is effective to reconstruct high-fidelity geometries and materials of glossy objects, and performs favorably against State-of-the-Arts.},
  archive      = {J_TVCG},
  author       = {Shuichang Lai and Letian Huang and Jie Guo and Kai Cheng and Bowen Pan and Xiaoxiao Long and Jiangjing Lyu and Chengfei Lv and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3547063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7478-7491},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GlossyGS: Inverse rendering of glossy objects with 3D gaussian splatting},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep point cloud edge reconstruction via surface patch segmentation. <em>TVCG</em>, <em>31</em>(10), 7463-7477. (<a href='https://doi.org/10.1109/TVCG.2025.3547411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric edge reconstruction for point cloud data is a fundamental problem in computer graphics. Existing methods first classify points as either edge points (including corners) or non-edge points, and then fit parametric edges to the edge points. However, few points are exactly sampled on edges in practical scenarios, leading to significant fitting errors in the reconstructed edges. Prominent deep learning-based methods also primarily emphasize edge points, overlooking the potential of non-edge areas. Given that sparse and non-uniform edge points cannot provide adequate information, we address this challenge by leveraging neighboring segmented patches to supply additional cues. We introduce a novel two-stage framework that reconstructs edges precisely and completely via surface patch segmentation. First, we propose PCER-Net, a Point Cloud Edge Reconstruction Network that segments surface patches, detects edge points, and predicts normals simultaneously. Second, a joint optimization module is designed to reconstruct a complete and precise 3D wireframe by fully utilizing the predicted results of the network. Concretely, the segmented patches enable accurate fitting of parametric edges, even when sparse points are not precisely distributed along the model's edges. Corners can also be naturally detected from the segmented patches. Benefiting from fitted edges and detected corners, a complete and precise 3D wireframe model with topology connections can be reconstructed by geometric optimization. Finally, we present a versatile patch-edge dataset, including CAD and everyday models (furniture), to generalize our method. Extensive experiments and comparisons against previous methods demonstrate our effectiveness and superiority. We will release the code and dataset to facilitate future research.},
  archive      = {J_TVCG},
  author       = {Yuanqi Li and Hongshen Wang and Yansong Liu and Jingcheng Huang and Shun Liu and Chenyu Huang and Jianwei Guo and Jie Guo and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3547411},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7463-7477},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep point cloud edge reconstruction via surface patch segmentation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A serial perspective on photometric stereo of filtering and serializing spatial information. <em>TVCG</em>, <em>31</em>(10), 7448-7462. (<a href='https://doi.org/10.1109/TVCG.2025.3546657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel method of Filtering and Serializing Spatial Information to tackle uncalibrated photometric stereo tasks, termed FSSI-PS. Photometric stereo aims to recover surface normals from images with varying lighting and is crucial for tasks like 3D reconstruction and defect detection. Current methods in complex surface reconstruction are costly and inaccurate due to redundant feature representations from GCN or Transformer modules, caused by the weak global information extraction capability of GCNs or the large computational cost of Transformers. Furthermore, the trainset’s lack of richness in texture complexity makes reconstruction more difficult. We address these issues by optimizing feature maps and dataset richness through serializing and filtering. First, we use Mamba-RNN to optimize feature representation by directly fusing feature maps, which reduces redundancy and uses minimal computational resources. Specifically, we treat input spatial information as a sequence and serialize it by sorting. Furthermore, we introduce the Mean Angular Variation metric to assess reconstruction difficulty by measuring texture complexity. It classifies PS-Sculpture and PS-Blobby into three categories: Difficult, Normal, and Simple. We use this to construct DNS-S+B, a photometric stereo training set with rich complexity levels. Our method is compared with state-of-the-art methods on the DiLiGenT and LUCES benchmarks to highlight effectiveness.},
  archive      = {J_TVCG},
  author       = {Minzhe Xu and Xin Ding and You Yang and Yinqiang Zheng and Qiong Liu},
  doi          = {10.1109/TVCG.2025.3546657},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7448-7462},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A serial perspective on photometric stereo of filtering and serializing spatial information},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttributionScanner: A visual analytics system for model validation with metadata-free slice finding. <em>TVCG</em>, <em>31</em>(10), 7436-7447. (<a href='https://doi.org/10.1109/TVCG.2025.3546644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data slice finding is an emerging technique for validating machine learning (ML) models by identifying and analyzing subgroups in a dataset that exhibit poor performance, often characterized by distinct feature sets or descriptive metadata. However, in the context of validating vision models involving unstructured image data, this approach faces significant challenges, including the laborious and costly requirement for additional metadata and the complex task of interpreting the root causes of underperformance. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for metadata-free data slice finding. Our system identifies interpretable data slices that involve common model behaviors and visualizes these patterns through an Attribution Mosaic design. Our interactive interface provides straightforward guidance for users to detect, interpret, and annotate predominant model issues, such as spurious correlations (model biases) and mislabeled data, with minimal effort. Additionally, it employs a cutting-edge model regularization technique to mitigate the detected issues and enhance the model’s performance. The efficacy of AttributionScanner is demonstrated through use cases involving two benchmark datasets, with qualitative and quantitative evaluations showcasing its substantial effectiveness in vision model validation, ultimately leading to more reliable and accurate models.},
  archive      = {J_TVCG},
  author       = {Xiwei Xuan and Jorge Piazentin Ono and Liang Gou and Kwan-Liu Ma and Liu Ren},
  doi          = {10.1109/TVCG.2025.3546644},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7436-7447},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AttributionScanner: A visual analytics system for model validation with metadata-free slice finding},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGAvatar: Relightable 4D gaussian avatar from monocular videos. <em>TVCG</em>, <em>31</em>(10), 7421-7435. (<a href='https://doi.org/10.1109/TVCG.2025.3543603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relightable 4D avatar reconstruction which enables high fidelity and real-time rendering continues to be a crucial but challenging problem, especially from monocular videos. Previous NeRF-based 4D avatars enable photo-realistic relighting but are too slow for rendering, while point-based or mesh-based 4D avatars are efficient but have limited rendering quality. The recent success of 3D Gaussian Splatting, i.e., 3DGS, has inspired a series of impressive 4D Gaussian avatars, however, most of which only focus on faithful appearance reconstruction but are not relightable. To address such issues, this article proposes a new Relightable 4D Gaussian Avatar, i.e., RGAvatar, tailored for high fidelity relightable rendering from monocular videos. Our key idea is to introduce a new relightable 4D Gaussian representation, based on which we can directly perform high fidelity Physically Based Rendering, and an effective joint learning mechanism for compact 4D Gaussian reconstruction with SDF regulation and accurate materials and lighting decomposition. By comparing with previous state-of-the-art approaches, RGAvatar can significantly outperform previous approaches in relightable rendering quality and speed. To our best knowledge, RGAvatar contributes a new state-of-the-art 4D Gaussian avatar from monocular videos, which enables high fidelity relightable rendering in a quite efficient manner.},
  archive      = {J_TVCG},
  author       = {Zhe Fan and Shi-Sheng Huang and Yichi Zhang and Dachao Shang and Juyong Zhang and Yudong Guo and Hua Huang},
  doi          = {10.1109/TVCG.2025.3543603},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7421-7435},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RGAvatar: Relightable 4D gaussian avatar from monocular videos},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XROps: A visual workflow management system for dynamic immersive analytics. <em>TVCG</em>, <em>31</em>(10), 7407-7420. (<a href='https://doi.org/10.1109/TVCG.2025.3546467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive analytics is gaining attention across multiple domains due to its capability to facilitate intuitive data analysis in expansive environments through user interaction with data. However, creating immersive analytics systems for specific tasks is challenging due to the need for programming expertise and significant development effort. Despite the introduction of various immersive visualization authoring toolkits, domain experts still face hurdles in adopting immersive analytics into their workflow, particularly when faced with dynamically changing tasks and data in real time. To lower such technical barriers, we introduce XROps, a web-based authoring system that allows users to create immersive analytics applications through interactive visual programming, without the need for low-level scripting or coding. XROps enables dynamic immersive analytics authoring by allowing users to modify each step of the data visualization process with immediate feedback, enabling them to build visualizations on-the-fly and adapt to changing environments. It also supports the integration and visualization of real-time sensor data from XR devices—a key feature of immersive analytics—facilitating the creation of various analysis scenarios. We evaluated the usability of XROps through a user study and demonstrate its efficacy and usefulness in several example scenarios.},
  archive      = {J_TVCG},
  author       = {Suemin Jeon and JunYoung Choi and Haejin Jeong and Won-Ki Jeong},
  doi          = {10.1109/TVCG.2025.3546467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7407-7420},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {XROps: A visual workflow management system for dynamic immersive analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MC-NeRF: Multi-camera neural radiance fields for multi-camera image acquisition systems. <em>TVCG</em>, <em>31</em>(10), 7391-7406. (<a href='https://doi.org/10.1109/TVCG.2025.3546290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) use multi-view images for 3D scene representation, demonstrating remarkable performance. As one of the primary sources of multi-view images, multi-camera systems encounter challenges such as varying intrinsic parameters and frequent pose changes. Most previous NeRF-based methods assume a unique camera and rarely consider multi-camera scenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic parameters still remain susceptible to suboptimal solutions when these parameters are poor initialized. In this paper, we propose MC-NeRF, a method for joint optimization of both intrinsic and extrinsic parameters alongside NeRF, allowing individual camera parameters for each image. First, we analyze the coupling issue that arises from the joint optimization between intrinsics and extrinsics, and propose a decoupling constraint utilizing auxiliary images. To further address the degenerate cases in the decoupling process, we introduce an efficient auxiliary image acquisition scheme to mitigate these effects. Furthermore, recognizing that most existing datasets are designed for a unique camera, we provided a new dataset that includes both simulated data and real-world data. Experiments demonstrate the effectiveness of our method in scenarios where each image corresponds to different camera parameters. Specifically, our approach outperforms the baselines favorably in terms of intrinsics estimation, extrinsics estimation, scale estimation, and rendering quality.},
  archive      = {J_TVCG},
  author       = {Yu Gao and Lutong Su and Hao Liang and Yufeng Yue and Yi Yang and Mengyin Fu},
  doi          = {10.1109/TVCG.2025.3546290},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7391-7406},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MC-NeRF: Multi-camera neural radiance fields for multi-camera image acquisition systems},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDPilot: Exploring partial dependence plots through ranking, filtering, and clustering. <em>TVCG</em>, <em>31</em>(10), 7377-7390. (<a href='https://doi.org/10.1109/TVCG.2025.3545025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial dependence plots (PDPs) and individual conditional expectation (ICE) plots are visualizations used for explaining the behavior of machine learning (ML) models trained on tabular datasets. They show how the values of a feature or pair of features impact a model’s predictions. However, in models with a large number of features, it is impractical for an ML practitioner to analyze all possible plots. To address this, we present new techniques for ranking and filtering PDP and ICE plots and build upon existing strategies for clustering the lines in ICE plots. Together, these techniques aim to help ML practitioners efficiently explore PDP and ICE plots and identify interesting model behavior. We integrate these techniques into PDPilot, a visual analytics tool that runs in Jupyter notebooks. We use PDPilot to study how 7 ML practitioners utilize the ranking, filtering, and clustering techniques to analyze an ML model.},
  archive      = {J_TVCG},
  author       = {Daniel Kerrigan and Brian Barr and Enrico Bertini},
  doi          = {10.1109/TVCG.2025.3545025},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7377-7390},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PDPilot: Exploring partial dependence plots through ranking, filtering, and clustering},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interweaving mathematics and art: Drawing graphs as celtic knots and links with CelticGraph. <em>TVCG</em>, <em>31</em>(10), 7363-7376. (<a href='https://doi.org/10.1109/TVCG.2025.3545481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Celtic knots, an ancient art form often linked to Celtic heritage, have been used historically in the decoration of monuments and manuscripts, often symbolizing the notions of eternity and interconnectedness. This paper introduces the framework CelticGraph designed for illustrating graphs in the style of Celtic knots and links. The process of creating these drawings raises interesting combinatorial concepts in the theory of circuits in planar graphs. Further, CelticGraph uses a novel algorithm to represent edges as Bézier curves, aiming to show each link as a smooth curve with limited curvature. We also show that with our production mechanisms we can compute any 4-regular plane graph and thereby any celtic knot or link. The CelticGraph framework for drawing graphs as celtic knots and links is implemented as an add-on of Vanted, a network visualization and analysis tool.},
  archive      = {J_TVCG},
  author       = {Niklas Gröne and Peter Eades and Karsten Klein and Patrick Eades and Leo Schreiber and Ulf Hailer and Hugo A. D. do Nascimento and Falk Schreiber},
  doi          = {10.1109/TVCG.2025.3545481},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7363-7376},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interweaving mathematics and art: Drawing graphs as celtic knots and links with CelticGraph},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PolyGraph: A graph-based method for floorplan reconstruction from 3D scans. <em>TVCG</em>, <em>31</em>(10), 7350-7362. (<a href='https://doi.org/10.1109/TVCG.2025.3544769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of reconstructing indoor floorplans has become an increasingly popular subject, offering substantial benefits across various applications such as interior design, virtual reality, and robotics. Despite the growing interest, existing approaches frequently encounter challenges due to high computational costs and sensitivity to errors in primitive detection. In this article, we introduce PolyGraph, a new computational framework that combines a deep-learning based primitive detection network with an optimization-based reconstruction algorithm to facilitate high-quality reconstruction results. Specifically, we develop a novel guided wall point primitive estimation network capable of generating dense samples along wall boundaries. This network not only retains structural detail but also shows improved robustness in the detection phase. Then, PolyGraph utilizes wall points to establish a graph-based representation, formulating indoor floorplan reconstruction as a subgraph optimization problem. This approach significantly reduces the search space comparing to existing pixel-level optimization approaches. By utilizing “structural weight”, we seamlessly integrate the structural information of walls and rooms into graph representations, ensuring high-quality reconstruction results. Experimental results demonstrate PolyGraph's effectiveness and its advantages compared to other optimization-based approaches, showcasing its computational efficiency, and its ability to preserve structural integrity and capture fine details, as quantified by the structure metrics.},
  archive      = {J_TVCG},
  author       = {Qian Sun and Chenrong Fang and Shuang Liu and Yidan Sun and Yu Shang and Ying He},
  doi          = {10.1109/TVCG.2025.3544769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7350-7362},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PolyGraph: A graph-based method for floorplan reconstruction from 3D scans},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Every angle is worth a second glance: Mining kinematic skeletal structures from multi-view joint cloud. <em>TVCG</em>, <em>31</em>(10), 7337-7349. (<a href='https://doi.org/10.1109/TVCG.2025.3542442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios.},
  archive      = {J_TVCG},
  author       = {Junkun Jiang and Jie Chen and Ho Yin Au and Mingyuan Chen and Wei Xue and Yike Guo},
  doi          = {10.1109/TVCG.2025.3542442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7337-7349},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Every angle is worth a second glance: Mining kinematic skeletal structures from multi-view joint cloud},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment analyzer: A tool for analyzing comment sets and thread structures of news articles. <em>TVCG</em>, <em>31</em>(10), 7324-7336. (<a href='https://doi.org/10.1109/TVCG.2025.3544733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of visually guided data exploration tools limits the scope of research questions communication scientists are able to study. The Comment Analyzer steps in where traditional statistical tools fail when it comes to researching the commenting behavior of news article readers. The basis of such an analysis are comment-thread corpora in which comments are tagged with various deliberative quality indicators as well as political stance. Our analysis tool provides a visual querying system for the exploration and analysis of such corpora and allows social scientists to gain insights into the distributions and relations between comment attributes, the homogeneity of thread sets, frequent thread structures and changes in comment qualities over the course of a single but in particular of multiple threads at once. We developed the tool in close collaboration with communication scientists in a user-centered approach. The system has proven its utility in thorough reviews with the communication scientists, by corroborating existing findings in the literature but particularly by provoking and answering new research questions. Final reviews with five independent experts confirmed these observations and revealed the potential of the Comment Analyzer for other datasets currently being created and analyzed in the communication sciences.},
  archive      = {J_TVCG},
  author       = {Dora Kiesel and Patrick Riehmann and Ines Engelmann and Hanna Ramezani and Bernd Froehlich},
  doi          = {10.1109/TVCG.2025.3544733},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7324-7336},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comment analyzer: A tool for analyzing comment sets and thread structures of news articles},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time, free-viewpoint holographic patient rendering for telerehabilitation via a single camera: A data-driven approach with 3D gaussian splatting for real-world adaptation. <em>TVCG</em>, <em>31</em>(10), 7311-7323. (<a href='https://doi.org/10.1109/TVCG.2025.3544297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telerehabilitation is a cost-effective alternative to in-clinic rehabilitation. Although convenient, it lacks immersive and free-viewpoint patient visualization. Current research explores two solutions to this issue. Mesh-based methods use 3D models and motion capture for AR visualization. However, they are labor-intensive and less photorealistic than 2D images. Microsoft's Holoportation generates photorealistic 3D models with eight RGBD cameras in real time. However, it requires complex setups, high GPU power, and high-speed communication infrastructure, making deployment challenging. This article presents a Real-Time Free-Viewpoint Holographic Patient Rendering (RT-FVHP) system for telerehabilitation. Unlike traditional methods that require manually crafted assets such as 3D meshes, texture maps, and skeletal rigging, our data-driven approach eliminates the need for explicit asset definitions. Inspired by the HumanNeRF framework, we retarget dynamic human poses to a canonical pose and leverage 3D Gaussian Splatting to train a neural network in canonical space for patient representation. The trained model generates 2D RGB$\sigma$ outputs via Gaussian Splatting rasterization, guided by camera parameters and human pose inputs. Compatible with HoloLens 2 and web-based platforms, RT-FVHP operates effectively under real-world conditions, including handling occlusions caused by treadmills. Occlusion handling is accomplished using our Shape-Enforced Gaussian Density Control (SGDC), which initializes and densifies 3D Gaussians in occluded regions using estimated SMPL human body priors. This approach minimizes manual intervention while ensuring complete body reconstruction. With efficient Gaussian rasterization, the model delivers real-time performance of up to 400 FPS at 1080p resolution on a dedicated RTX6000 GPU.},
  archive      = {J_TVCG},
  author       = {Shengting Cao and Jiamiao Zhao and Fei Hu and Yu Gan},
  doi          = {10.1109/TVCG.2025.3544297},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7311-7323},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time, free-viewpoint holographic patient rendering for telerehabilitation via a single camera: A data-driven approach with 3D gaussian splatting for real-world adaptation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EverywhereAR: A visual authoring system for creating adaptive AR game scenes. <em>TVCG</em>, <em>31</em>(10), 7297-7310. (<a href='https://doi.org/10.1109/TVCG.2025.3544021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal application of Augmented Reality (AR) technology, AR games empower players to bridge reality with virtuality, offering a distinct and immersive experience set apart from traditional games. However, when creating AR games, one of the most formidable challenges faced by designers pertains to the unpredictability of intricate real-world environments, which hinders crafting naturally integrated scenes where virtual objects harmoniously blend with the players’ surroundings. In this paper, we introduce EverywhereAR, a system that is capable of flexibly realizing the designer’s idea in various real-world scenes. It provides a designer-friendly Game Scene Template development interface, for designers to quickly graphify their inspirations. To achieve the best AR game scene, this work proposes a highly customizable integration method. According to the integrated AR scene graph, the system will arrange each virtual object in a reasonable position to make the generated game scene look natural. We conducted an experiment to evaluate our system’s performance across various game scene templates and real-world environments. Results from the experiment indicated that our system was able to generate AR game scenes matching the quality of scenes manually created by professional designers. In addition, we conducted another experiment to assess the effectiveness and usability of the proposed interface. The experiment results showed that the interface was intuitive and efficient, allowing users to create a simple game scene within one minute.},
  archive      = {J_TVCG},
  author       = {Jia Liu and Renjie Zhang and Isidro Butaslac and Taishi Sawabe and Yuichiro Fujimoto and Masayuki Kanbara and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3544021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7297-7310},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EverywhereAR: A visual authoring system for creating adaptive AR game scenes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informal skill-sharing in collaborative immersive analytics. <em>TVCG</em>, <em>31</em>(10), 7284-7296. (<a href='https://doi.org/10.1109/TVCG.2025.3542675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Newcomers to immersive analytics systems would benefit from informally learning data analysis skills (e.g., reading a vis, using a system) when collaborating with more expert participants. We want to design tools that facilitate this informal learning by encouraging the informal sharing of data literacy skills during collaborative immersive analysis. A first step toward this goal is to understand how informal skill sharing takes place in order to identify how it could be improved. We experimentally studied informal skill-sharing in pairs of participants analyzing scatterplots in a shared virtual reality environment. We used an original mixed-method to analyze video and log recordings as well as subjective experiential data, based on common ground theory and the grounding process. We uncovered 101 episodes of skill-sharing, organized in 14 recurring types, and identified associated problems from which we could propose six implications for designing systems that favor informal skill-sharing, hence skill learning. The method can be used to study informal skill sharing in other systems enabling embodied face-to-face collaboration, but would need to be simplified for large-scale use.},
  archive      = {J_TVCG},
  author       = {Pierre Vaslin and Yannick Prié},
  doi          = {10.1109/TVCG.2025.3542675},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7284-7296},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Informal skill-sharing in collaborative immersive analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing agent personality in crowd simulation improves social presence and experienced realism in immersive VR. <em>TVCG</em>, <em>31</em>(10), 7269-7283. (<a href='https://doi.org/10.1109/TVCG.2025.3543740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convincing crowd behavior simulation is becoming essential in many application domains, including video games, cinematography, urban planning, safety simulations, and training. In this article, we propose a novel and lightweight mesoscopic system for personality-based crowd simulation in immersive virtual reality (iVR). We use the Big Five personality framework, also known as OCEAN, to model a synthetic personality for each autonomous agent. Agents can autonomously aggregate in formations using machine learning-based clustering techniques operating on OCEAN. Moreover, agents can also externalize their personality traits by performing peculiar behavioral animations. To choose which animations to perform, we adopt a probabilistic approach that considers each OCEAN dimension as a continuous spectrum with two extremes linked to pairs of animations. Our system is designed to be flexible and suitable for different applications. Flexibility is achieved by using graphs to store agent and map topology data that control how the agents move and behave at runtime. In a within-subjects study with 40 users, we compare our personality-based system against a basic system that does not use personality. Results show that introducing personality into iVR crowd simulation enhances users’ social presence and experienced realism. Introducing personality also increases the perceived match between the agents and the virtual environment where the simulation takes place.},
  archive      = {J_TVCG},
  author       = {Massimiliano Pascoli and Fabio Buttussi and Konstantin Schekotihin and Luca Chittaro},
  doi          = {10.1109/TVCG.2025.3543740},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7269-7283},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing agent personality in crowd simulation improves social presence and experienced realism in immersive VR},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware spectral visualization. <em>TVCG</em>, <em>31</em>(10), 7254-7268. (<a href='https://doi.org/10.1109/TVCG.2025.3542898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One common task in time series analysis is the visual investigation of spectra such as Fourier spectra or wavelet spectra to identify dominating frequencies. In this article, we present the propagation of data uncertainty to the spectra and its visualization. We consider the Fourier and continuous wavelet transformations, which are two common spectral analysis methods. Deriving the propagation for time series that can be modeled as a Gaussian process leads to a combination of weighted non-central chi-squared distributions in the spectrum. Percentile-based visualizations explicitly encode the non-normal uncertainty in the 1D Fourier and 2D wavelet spectrum. We enrich the visualization by including correlations, sensitivity, and signal-to-noise analysis. For visual exploration, we combine the different visualizations into an interactive approach that allows for investigating the uncertain time series in the temporal and spectral domains. Finally, we show the usefulness of our approach by applying it to several real-world data sets and by a qualitative interview study with visualization experts.},
  archive      = {J_TVCG},
  author       = {Marina Evers and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2025.3542898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7254-7268},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty-aware spectral visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable human video generation from sparse sketches. <em>TVCG</em>, <em>31</em>(10), 7243-7253. (<a href='https://doi.org/10.1109/TVCG.2025.3543687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in human fashion video generation have transformed the field, producing various promising effects. Existing methods mainly focus on pose control but lack the ability to achieve sketch-based control, largely due to the absence of appearance-consistent and shape-varying knowledge in existing datasets. Moreover, the necessity of sequential structure inputs to control video generation hinders real-world applications. To address these limitations, we introduce Sketch2HumanVideo, an approach that, for the first time, achieves sketch-controllable human video generation with three conditions: temporally sparse sketches, a spatially sparse pose sequence, and a reference appearance image. Our key contribution is a sparse sketch encoder, which takes the first two conditions as input, enabling precise and multi-view control of shape motion. To provide the above knowledge, we leverage the expertise of two pretrained models to synthesize a dataset comprising shape-varying yet appearance-consistent examples for model training. Furthermore, we introduce an enlarging-and-resampling scheme to enhance high-frequency details of local regions in resource-constrained scenarios, thereby promoting the generation of realistic videos. Through qualitative and quantitative experiments, our method showcases superior performance to state-of-the-art approaches and flexible control.},
  archive      = {J_TVCG},
  author       = {Linzi Qu and Jiaxiang Shang and Miu-Ling Lam and Hongbo Fu},
  doi          = {10.1109/TVCG.2025.3543687},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7243-7253},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Controllable human video generation from sparse sketches},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous scatterplot and image moments for time-varying bivariate field analysis of electronic structure evolution. <em>TVCG</em>, <em>31</em>(10), 7229-7242. (<a href='https://doi.org/10.1109/TVCG.2025.3543619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photoinduced electronic transitions are complex quantum-mechanical processes where electrons move between energy levels due to the absorption of light. This induces dynamics i.e., coupled changes in the electronic structure and nuclear geometry, that drive physical and chemical processes of importance in diverse fields ranging from photobiology and materials design to medicine. The evolving electronic structure can be characterized by two electron density fields: hole and particle natural transition orbitals (NTOs). A study of the two density fields helps understand the movement of electronic charge from one part of the molecule to another, specifically the donor and acceptor regions. Previous works in this area rely on side-by-side visual comparisons of isosurfaces, statistical approaches, or visual analysis of bivariate fields restricted to limited time instances. We propose a new method to analyze time-varying bivariate fields with a large number of instances, as pertinent to understand electronic structure changes during light-induced dynamics. Since the NTO fields depend on the nuclear geometry, the nuclear motion leads to a large number of bivariate field instances. Structures like tracking graphs have been used to analyze time-varying univariate fields. This article presents a structured and practical approach to feature-directed visual exploration of time-varying bivariate fields using continuous scatterplots (CSPs) and image moment-based descriptors, tailored for studying the evolving electronic structure following photoexcitation. The CSP of the bivariate field at every time step is represented using an image moment vector of length 4. The collection of all image moment vector descriptors is considered as a point cloud in $\mathbb {R}^{4}$ and visualized using principal component analysis. Choosing an appropriate pair of principal components results in a representation of the point cloud as a curve on the plane. This representation supports tasks such as identifying interesting time steps, identifying patterns within the bivariate field, and tracking their evolution over time. We present two case studies on excited-state dynamics in molecular systems that demonstrate how the time-varying bivariate field analysis helps provide application-specific insights.},
  archive      = {J_TVCG},
  author       = {Mohit Sharma and Talha Bin Masood and Nanna Holmgaard List and Ingrid Hotz and Vijay Natarajan},
  doi          = {10.1109/TVCG.2025.3543619},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7229-7242},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Continuous scatterplot and image moments for time-varying bivariate field analysis of electronic structure evolution},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical point saliency for 3D keypoint detection. <em>TVCG</em>, <em>31</em>(10), 7211-7228. (<a href='https://doi.org/10.1109/TVCG.2025.3542465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint detection plays a fundamental role in many applications, such as 3D reconstruction, object registration, and shape retrieval, and has attracted significant interest from researchers in computer vision and graphics. However, due to the ambiguity of the keypoint and the complexity of 3D objects, it is still tricky for existing 3D keypoint detection methods to generate stable keypoints with good coverage, especially for unsupervised detection methods. This paper proposes a 3D keypoint detection method based on hierarchical point saliency. This method can effectively and accurately locate the keypoints of a 3D point cloud, and it does not require complex training processes. First, we propose a simple and effective point descriptor called the local geometric structure feature, which can effectively characterize the geometric structure changes of 3D point clouds and has a strong feature identification ability. Second, we define two saliency measures used to characterize the saliency of points in the point cloud, which are low-level and high-level saliency. Third, we hierarchically characterize the saliency of points by combining the low-level and high-level saliency, thus measuring the probability that a point belongs to a keypoint. Finally, we extensively test our method on three benchmark 3D point cloud datasets, and the experimental results demonstrate that our method achieves state-of-the-art performance in keypoint detection tasks, significantly superior to the prior hand-crafted and deep-learning-based 3D keypoint detection methods.},
  archive      = {J_TVCG},
  author       = {Chengzhuan Yang and Yinhuang Chen and Qian Yu and Hui Wei and Fei Wu and Zhonglong Zheng},
  doi          = {10.1109/TVCG.2025.3542465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7211-7228},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hierarchical point saliency for 3D keypoint detection},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drillboards: Adaptive visualization dashboards for dynamic personalization of visualization experiences. <em>TVCG</em>, <em>31</em>(10), 7196-7210. (<a href='https://doi.org/10.1109/TVCG.2025.3542606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present drillboards, a technique for adaptive visualization dashboards consisting of a hierarchy of coordinated charts that the user can drill down to reach a desired level of detail depending on their expertise, interest, and desired effort. This functionality allows different users to personalize the same dashboard to their specific needs and expertise. The technique is based on a formal vocabulary of chart representations and rules for merging multiple charts of different types and data into single composite representations. The drillboard hierarchy is created by iteratively applying these rules starting from a baseline dashboard, with each consecutive operation yielding a new dashboard with fewer charts and progressively more abstract and simplified views. We also present an authoring tool for building drillboards and show how it can be applied to an agricultural dataset with hundreds of expert users. Our evaluation asked three domain experts to author drillboards for their own datasets, which we then showed to casual end-users with favorable outcomes.},
  archive      = {J_TVCG},
  author       = {Sungbok Shin and Inyoup Na and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2025.3542606},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7196-7210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Drillboards: Adaptive visualization dashboards for dynamic personalization of visualization experiences},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One shot learning for edge detection on point clouds. <em>TVCG</em>, <em>31</em>(10), 7184-7195. (<a href='https://doi.org/10.1109/TVCG.2025.3542475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each scanner possesses its unique characteristics and exhibits its distinct sampling error distribution. Training a network on a dataset that includes data collected from different scanners is less effective than training it on data specific to a single scanner. Therefore, we present a novel one-shot learning method allowing for edge extraction on point clouds, by learning the specific data distribution of the target point cloud, and thus achieve superior results compared to networks that were trained on general data distributions. More specifically, we present how to train a lightweight network named OSFENet (One-Shot edge Feature Extraction Network), by designing a filtered-KNN-based surface patch representation that supports a one-shot learning framework. Additionally, we introduce an RBF_DoS module, which integrates Radial Basis Function-based Descriptor of the Surface patch, highly beneficial for the edge extraction on point clouds. The advantage of the proposed OSFENet is demonstrated through comparative analyses against 7 baselines on the ABC dataset, and its practical utility is validated by results across diverse real-scanned datasets, including indoor scenes like S3DIS dataset, and outdoor scenes such as the Semantic3D dataset and UrbanBIS dataset.},
  archive      = {J_TVCG},
  author       = {Zhikun Tu and Yuhe Zhang and Yiou Jia and Kang Li and Daniel Cohen-Or},
  doi          = {10.1109/TVCG.2025.3542475},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7184-7195},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {One shot learning for edge detection on point clouds},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPORT: From zero-shot prompts to real-time motion generation. <em>TVCG</em>, <em>31</em>(10), 7171-7183. (<a href='https://doi.org/10.1109/TVCG.2025.3542631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time motion generation has garnered significant attention within the fields of computer animation and gaming. Existing methods typically realize motion control via isolated style or content labels, resulting in short, simply motion clips. In this paper, we propose a motion generation framework, called SPORT (“from zero-Shot Prompt tO Real-Time motion generation”), for generating real-time and ever-changing motions using zero-shot prompts. SPORT consists of three primary components: (1) a body-part phase autoencoder that ensures smooth transitions between diverse motions; (2) a body-part content encoder that mitigates semantic gap between texts and motions; (3) a diffusion-based decoder that accelerates the denoising process while enhancing the diversity and realism of motions. Moreover, we develop a prototype for real-time application in Unity, demonstrating that our approach effectively considering the semantic gap caused by abstract style texts and rapidly changing terrains. Through qualitative and quantitative comparisons, we show that SPORT outperforms other approaches in terms of motion quality, style diversity and inference speed.},
  archive      = {J_TVCG},
  author       = {Bin Ji and Ye Pan and Zhimeng Liu and Shuai Tan and Xiaokang Yang},
  doi          = {10.1109/TVCG.2025.3542631},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7171-7183},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SPORT: From zero-shot prompts to real-time motion generation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNNFairViz: Visual analysis for graph neural network fairness. <em>TVCG</em>, <em>31</em>(10), 7153-7170. (<a href='https://doi.org/10.1109/TVCG.2025.3542419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Graph Neural Networks (GNNs) show promise for various applications like social networks and financial networks. However, they exhibit fairness issues, particularly in human-related decision contexts, risking unfair treatment of groups historically subject to discrimination. While several visual analytics studies have explored fairness in machine learning (ML), few have tackled the particular challenges posed by GNNs. We propose a visual analytics framework for GNN fairness analysis, offering insights into how attribute and structural biases may introduce model bias. Our framework is model-agnostic and tailored for real-world scenarios with multiple and multinary sensitive attributes, utilizing an extended suite of fairness metrics. To operationalize the framework, we develop GNNFairViz, a visual analysis tool that integrates seamlessly into the GNN development workflow, offering interactive visualizations. Our tool enables GNN model developers, the target users, to analyze model bias comprehensively, facilitating node selection, fairness inspection, and diagnostics. We evaluate our approach through two usage scenarios and expert interviews, confirming its effectiveness and usability in GNN fairness analysis. Furthermore, we summarize two general insights into GNN fairness based on our observations on the usage of GNNFairViz, highlighting the prevalence of the “Overwhelming Effect” in highly unbalanced datasets and the importance of suitable GNN architecture selection for bias mitigation.},
  archive      = {J_TVCG},
  author       = {Xinwu Ye and Jielin Feng and Erasmo Purificato and Ludovico Boratto and Michael Kamp and Zengfeng Huang and Siming Chen},
  doi          = {10.1109/TVCG.2025.3542419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7153-7170},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GNNFairViz: Visual analysis for graph neural network fairness},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging network science and vision science: Mapping perceptual mechanisms to network visualization tasks. <em>TVCG</em>, <em>31</em>(10), 7137-7152. (<a href='https://doi.org/10.1109/TVCG.2025.3541571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network visualizations are understudied in graphical perception. As a result, most network visualization designs still largely rely on designer intuition and algorithm optimizations rather than being guided by knowledge of human perception. The lack of perceptual understanding of network visualizations also limits the generalizability of past empirical evaluations, given their focus on performance over causal interpretation. To bridge this gap between perception and network visualization, we introduce a framework highlighting five key perceptual mechanisms used in node-link diagrams and adjacency matrices: attention, visual search, perceptual organization, ensemble coding, and object recognition. Our framework describes the role these perceptual mechanisms play in common network analytical tasks. We use the framework to revisit four past empirical investigations and outline future design experiments that can help produce more perceptually effective network visualizations. We anticipate this connection will afford translational understanding to guide more effective network visualization design and offer hypotheses for perception-aware network visualizations.},
  archive      = {J_TVCG},
  author       = {S. Sandra Bae and Kyle Cave and Carsten Görg and Paul Rosen and Danielle Albers Szafir and Cindy Xiong Bearfield},
  doi          = {10.1109/TVCG.2025.3541571},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7137-7152},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bridging network science and vision science: Mapping perceptual mechanisms to network visualization tasks},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual acuity consistent foveated rendering towards retinal resolution. <em>TVCG</em>, <em>31</em>(10), 7121-7136. (<a href='https://doi.org/10.1109/TVCG.2025.3539851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior foveated rendering methods often suffer from a limitation where the shading load escalates with increasing display resolution, leading to decreased efficiency, particularly when dealing with retinal-level resolutions. To tackle this challenge, we begin with the essence of the human visual system (HVS) perception and present visual acuity-consistent foveated rendering (VaFR), aiming to achieve exceptional rendering performance at retinal-level resolutions. Specifically, we propose a method with a novel log-polar mapping function derived from the human visual acuity model, which accommodates the natural bandwidth of the visual system. This mapping function and its associated shading rate guarantee a consistent output of rendering information, regardless of variations in the display resolution of the VR HMD. Consequently, our VaFR outperforms alternative methods, improving rendering speed while preserving perceptual visual quality, particularly when operating at retinal resolutions. We validate our approach using both the rasterization and ray-casting rendering pipelines. We also validate our approach using different binocular rendering strategies for HMD devices. In diverse testing scenarios, our approach delivers better perceptual visual quality than prior foveated rendering while achieving an impressive speedup of 6.5×−9.29× for deferred rendering of 3D scenarios and an even more powerful speedup of 10.4×−16.4× for ray-casting at retinal resolution. Additionally, our approach significantly enhances the rendering performance of binocular 8 K path tracing, achieving smooth frame rates.},
  archive      = {J_TVCG},
  author       = {Zhi Zhang and Meng Gai and Sheng Li},
  doi          = {10.1109/TVCG.2025.3539851},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7121-7136},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual acuity consistent foveated rendering towards retinal resolution},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Floor plan restoration: A multimodal method under one second. <em>TVCG</em>, <em>31</em>(10), 7107-7120. (<a href='https://doi.org/10.1109/TVCG.2025.3539497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floor plan restoration aims to recover vector and semantic information from raster floor plan images, which is significant for advanced applications including interior design, interative walkthroughs, and layout planning. Existing methods generally adopt a two-stage paradigm: a parsing stage to extract semantic elements such as rooms, walls, doors, and windows from raster images; and then a vectorization stage to convert them into vector graphics. However, these methods are deficient in both accuracy and efficiency due to the neglect of the unique cues of floor plans compared to natural images. To address the above issues, we propose MMParseNet that yields accurate parsing results by incorporating multimodal cues unique to floor plans, such as room names, furniture icons, and room boundaries. Moreover, we implement an efficiency-optimized vectorization method based on PCA that avoids unnecessary iterative solutions. We conduct both quantitative and qualitative experiments on three public and one self-built dataset. The results exhibit consistent improvements in accuracy and sub-second overall restoration time across various datasets.},
  archive      = {J_TVCG},
  author       = {Tao Wen and You-Ming Fu and Chun-Xia Xiao and Hai-Ming Xiang and Chao Liang},
  doi          = {10.1109/TVCG.2025.3539497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7107-7120},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Floor plan restoration: A multimodal method under one second},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From requirement to solution: Unveiling problem-driven design patterns in visual analytics. <em>TVCG</em>, <em>31</em>(10), 7089-7106. (<a href='https://doi.org/10.1109/TVCG.2025.3538768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Analytics (VA) researchers frequently collaborate closely with domain experts to derive requirements and select appropriate solutions to fulfill these requirements. Despite strides made in exploring requirement and solution spaces, challenges persist due to the absence of guidance in the initial consideration space and the lack of shared problem-solving knowledge, often resulting in suboptimal solutions. To address these issues, we conducted an empirical study of VA research, with a focus on mapping the relations between requirement and solution spaces. Analyzing 220 VA papers, we formulate refined topologies for data, requirements, and solutions. We propose conceptualizing the connections between requirements, data, and solutions through knowledge graphs and utilizing solution paths to encapsulate fundamental problem-solving knowledge in visual analytics research. Through the integration of solution paths into a graph and analyzing their interconnections, we identified a subset of problem-driven design patterns that demonstrated the efficacy of our approach. By externalizing problem-solving knowledge and formulating problem-driven design patterns, our aim is to streamline the exploration of consideration space, facilitating the inclusion of “good” solutions, and establish a benchmark for shared design decisions among researchers and readers.},
  archive      = {J_TVCG},
  author       = {Yuchen Wu and Shenghan Gao and Shizhen Zhang and Xiaofeng Dou and Xingbo Wang and Quan Li},
  doi          = {10.1109/TVCG.2025.3538768},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7089-7106},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From requirement to solution: Unveiling problem-driven design patterns in visual analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can people's brains synchronize during remote AR collaboration?. <em>TVCG</em>, <em>31</em>(10), 7078-7088. (<a href='https://doi.org/10.1109/TVCG.2025.3538509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that brain synchrony can indicate the quality of social interaction in real-world communication. However, there is a lack of research on measurement of brain synchrony during social interactions in remote AR. In this study, we investigated the brain synchrony of remote augmented reality (AR; Study 1) and face-to-face (FTF; Study 2) interactions. Functional near-infrared spectroscopy was used to measure the brain synchrony during the tangram puzzle task. In a collaboration condition, participants worked together to solve the puzzle. In an individual condition, participants solved the puzzle independently. We recruited 46 participants in Study 1 and 48 participants in Study 2. Study 1 showed there was a significant difference in brain synchrony between the individual and collaboration conditions, and a positive correlation was observed between brain synchrony and the task performance in the collaboration condition. A comparison between Study 1 and 2 suggested that the difference between the collaboration and individual conditions was maintained, and some differences were observed in the brain synchrony between the AR and FTF interactions. These results suggest that measurement of brain synchrony is beneficial for social interaction in remote AR collaborations. The implications of these results on future remote interactions are discussed.},
  archive      = {J_TVCG},
  author       = {Jaehwan You and Myeongul Jung and Kwanguk Kim},
  doi          = {10.1109/TVCG.2025.3538509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7078-7088},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Can people's brains synchronize during remote AR collaboration?},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring spatial hybrid user interface for visual sensemaking. <em>TVCG</em>, <em>31</em>(10), 7062-7077. (<a href='https://doi.org/10.1109/TVCG.2025.3538771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We built a spatial hybrid system that combines a personal computer (PC) and virtual reality (VR) for visual sensemaking, addressing limitations in both environments. Although VR offers immense potential for interactive data visualization (e.g., large display space and spatial navigation), it can also present challenges such as imprecise interactions and user fatigue. At the same time, a PC offers precise and familiar interactions but has limited display space and interaction modality. Therefore, we iteratively designed a spatial hybrid system (PC+VR) to complement these two environments by enabling seamless switching between PC and VR environments. To evaluate the system's effectiveness and user experience, we compared it to using a single computing environment (i.e., PC-only and VR-only). Our study results (N=18) showed that spatial PC+VR could combine the benefits of both devices to outperform user preference for VR-only without a negative impact on performance from device switching overhead. Finally, we discussed future design implications.},
  archive      = {J_TVCG},
  author       = {Wai Tong and Haobo Li and Meng Xia and Wong Kam-Kwai and Ting-Chuen Pong and Huamin Qu and Yalong Yang},
  doi          = {10.1109/TVCG.2025.3538771},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7062-7077},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring spatial hybrid user interface for visual sensemaking},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and readable layered network visualizations using large neighborhood search. <em>TVCG</em>, <em>31</em>(10), 7048-7061. (<a href='https://doi.org/10.1109/TVCG.2025.3537898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layered network visualizations assign each node to one of several parallel axes. They can convey sequence or flow data, hierarchies, or multiple data classes, but edge crossings and long edges often impair readability. Layout algorithms can reduce edge crossings and shorten edges using quick heuristics or optimal methods that prioritize human readability over computation speed. This work uses an optimization metaheuristic to provide the best of both worlds: high-quality layouts within a predetermined execution time. Our adaptation of the large neighborhood search (LNS) metaheuristic repeatedly selects fixed-sized subgraphs to lay out optimally. We conducted a computational evaluation using 450 synthetic networks to compare five ways of selecting candidate nodes, four ways of selecting their neighboring subgraph, and three criteria for determining subgraph size. LNS generally halved the number of crossings versus the barycentric heuristic while maintaining a reasonable runtime. Our best approach randomly selected candidate nodes, used degree centrality to pick cluster-like neighborhoods, and chose smaller neighborhoods that could be optimally laid out in 0.6 or 1.2 seconds (versus 6 seconds). In a case study visualizing 13 control flow graphs, most with over 1000 nodes, we show that our method can be employed to create visualizations with fewer crossings than Tabu Search, another metaheuristic, and vastly outperforms an ILP solver when runtime is bounded.},
  archive      = {J_TVCG},
  author       = {Connor Wilson and Tarik Crnovrsanin and Eduardo Puerta and Cody Dunne},
  doi          = {10.1109/TVCG.2025.3537898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7048-7061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast and readable layered network visualizations using large neighborhood search},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DashSpace: A live collaborative platform for immersive and ubiquitous analytics. <em>TVCG</em>, <em>31</em>(10), 7034-7047. (<a href='https://doi.org/10.1109/TVCG.2025.3537679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.},
  archive      = {J_TVCG},
  author       = {Marcel Borowski and Peter W. S. Butcher and Janus Bager Kristensen and Jonas Oxenbøll Petersen and Panagiotis D. Ritsos and Clemens N. Klokmose and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2025.3537679},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7034-7047},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DashSpace: A live collaborative platform for immersive and ubiquitous analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Volume-based space-time cube for large-scale continuous spatial time series. <em>TVCG</em>, <em>31</em>(10), 7019-7033. (<a href='https://doi.org/10.1109/TVCG.2025.3537115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial time series visualization offers scientific research pathways and analytical decision-making tools across various spatiotemporal domains. Despite many advanced methodologies, the seamless integration of temporal and spatial information remains a challenge. The space-time cube (STC) stands out as a promising approach for the synergistic presentation of spatial and temporal information, with successful applications across various spatiotemporal datasets. However, the STC is plagued by well-known issues such as visual occlusion and depth ambiguity, which are further exacerbated when dealing with large-scale spatial time series data. In this study, we introduce a novel technical framework termed VolumeSTCube, designed for continuous spatiotemporal phenomena. It first leverages the concept of the STC to transform discretely distributed spatial time series data into continuously volumetric data. Subsequently, volume rendering and surface rendering techniques are employed to visualize the transformed volumetric data. Volume rendering is utilized to mitigate visual occlusion, while surface rendering provides pattern details by enhanced lighting information. Lastly, we design interactions to facilitate the exploration and analysis from temporal, spatial, and spatiotemporal perspectives. VolumeSTCube is evaluated through a computational experiment, a real-world case study with one expert, and a controlled user study with twelve non-experts, compared against a baseline from prior work, showing its superiority and effectiveness in large-scale spatial time series analysis.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Jiabao Huang and Chenxi Ruan and Jialing Li and Shaowu Gao and Yi Cai},
  doi          = {10.1109/TVCG.2025.3537115},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7019-7033},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Volume-based space-time cube for large-scale continuous spatial time series},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do LLMs have visualization literacy? an evaluation on modified visualizations to test generalization in data interpretation. <em>TVCG</em>, <em>31</em>(10), 7004-7018. (<a href='https://doi.org/10.1109/TVCG.2025.3536358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we assess the visualization literacy of two prominent Large Language Models (LLMs): OpenAI’s Generative Pretrained Transformers (GPT), the backend of ChatGPT, and Google’s Gemini, previously known as Bard, to establish benchmarks for assessing their visualization capabilities. While LLMs have shown promise in generating chart descriptions, captions, and design suggestions, their potential for evaluating visualizations remains under-explored. Collecting data from humans for evaluations has been a bottleneck for visualization research in terms of both time and money, and if LLMs were able to serve, even in some limited role, as evaluators, they could be a significant resource. To investigate the feasibility of using LLMs in the visualization evaluation process, we explore the extent to which LLMs possess visualization literacy—a crucial factor for their effective utility in the field. We conducted a series of experiments using a modified 53-item Visualization Literacy Assessment Test (VLAT) for and . Our findings indicate that the LLMs we explored currently fail to achieve the same levels of visualization literacy when compared to data from the general public reported in VLAT, and LLMs heavily relied on their pre-existing knowledge to answer questions instead of utilizing the information provided by the visualization when answering questions.},
  archive      = {J_TVCG},
  author       = {Jiayi Hong and Christian Seto and Arlen Fan and Ross Maciejewski},
  doi          = {10.1109/TVCG.2025.3536358},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {7004-7018},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Do LLMs have visualization literacy? an evaluation on modified visualizations to test generalization in data interpretation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VISTA: A visual analytics framework to enhance foundation model-generated data labels. <em>TVCG</em>, <em>31</em>(10), 6991-7003. (<a href='https://doi.org/10.1109/TVCG.2025.3535896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA’s effectiveness from both quantitative and qualitative perspectives.},
  archive      = {J_TVCG},
  author       = {Xiwei Xuan and Xiaoqi Wang and Wenbin He and Jorge Piazentin Ono and Liang Gou and Kwan-Liu Ma and Liu Ren},
  doi          = {10.1109/TVCG.2025.3535896},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6991-7003},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VISTA: A visual analytics framework to enhance foundation model-generated data labels},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neck goes VRrr: Reducing rotation-induced virtual reality sickness through neck muscle vibrations. <em>TVCG</em>, <em>31</em>(10), 6977-6990. (<a href='https://doi.org/10.1109/TVCG.2025.3535942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of virtual reality (VR), VR sickness is becoming a key barrier for users to have prolonged VR experience. To alleviate VR sickness, researchers focused on reducing sensory mismatch by aligning the visual information with other sensory cues during the VR experience. We present a wearable haptic interface enabling neck muscle vibration (NMV) with a multi-stimulus configuration. NMV’s vibration on muscle spindles causes a haptic proprioceptive illusion of muscle stretch. Through NMV, we provide a simulated sensation of neck rotation to users without physically rotating the neck. For a left and right rotation on the yaw axis, we vibrated the sternocleidomastoid (SCM) muscles and splenius capitis (SC) muscles. Our lightweight interface vibrates different combinations of actuators on the left and right SCM and SC muscles to deliver multi-stimulus NMV in a desired illusory direction. We found that NMV sensation differs among individuals and is less effective during neck rotation. Based on these results, we developed a calibration and rendering process for NMV using real-time VR rotation information with varying viewpoint control. Our evaluation, which used a VR scene mimicking a common VR experience, showed that NMV effectively reduces rotation-induced VR sickness and improves the overall VR experience, such as presence.},
  archive      = {J_TVCG},
  author       = {Kun-Woo Song and Sang Ho Yoon},
  doi          = {10.1109/TVCG.2025.3535942},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6977-6990},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neck goes VRrr: Reducing rotation-induced virtual reality sickness through neck muscle vibrations},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unpaired 3D shape-to-shape translation via gradient-guided triplane diffusion. <em>TVCG</em>, <em>31</em>(10), 6963-6976. (<a href='https://doi.org/10.1109/TVCG.2025.3535531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired shape-to-shape translation refers to the task of transforming the geometry and semantics of an input shape into a new shape domain without paired training data. Previous methods utilize GAN-based architectures to perform shape translation, employing adversarial training to transform the source shape encoding into the target domain in the low-dimensional latent feature space. However, these methods encounter difficulties in generating diverse and high-quality results, as they often suffer from issues such as “mode collapse”. This leads to limited generation diversity and makes it challenging to find an accurate latent code that adequately represents the input shape. In this article, we achieve unpaired shape-to-shape translation via a triplane diffusion model, in which we factorize 3D objects into triplane representations and conduct a diffusion process on these representations to accomplish shape domain transformation. We observe that by adding an appropriate amount of noise to an input object during the forward diffusion process, domain-specific shape structures are smoothed out while the overall structure is still preserved. Subsequently, we progressively remove the noise via an unconditional diffusion model trained on the target shape domain in the reverse diffusion process. This allows us to obtain a denoised output that retains the structural similarities of the source input while aligning with the distribution of the target shape domain. During this process, we propose two gradient-based guidance mechanisms to guide the translation process to guarantee more faithful results during the denoising process. We conduct extensive experiments on different shape domains, and the experimental results demonstrate that our method achieves superior shape fidelity with high quality compared to current state-of-the-art baselines.},
  archive      = {J_TVCG},
  author       = {Wenxiao Zhang and Hossein Rahmani and Jun Liu},
  doi          = {10.1109/TVCG.2025.3535531},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6963-6976},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unpaired 3D shape-to-shape translation via gradient-guided triplane diffusion},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PromptAid: Visual prompt exploration, perturbation, testing and iteration for large language models. <em>TVCG</em>, <em>31</em>(10), 6946-6962. (<a href='https://doi.org/10.1109/TVCG.2025.3535332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc natural language processing (NLP) tasks with simple natural language prompts. Part of the appeal for LLMs is their approachability to the general public, including individuals with little technical expertise in NLP. However, prompts can vary significantly in terms of their linguistic structure, context, and other semantics, and modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses coordinated visualizations which allow users to improve prompts via three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through a pre-study involving NLP experts, and evaluated via a robust mixed-methods user study. Our findings indicate that PromptAid helps users to iterate over prompts with less cognitive overhead, generate diverse prompts with the help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.},
  archive      = {J_TVCG},
  author       = {Aditi Mishra and Bretho Danzy and Utkarsh Soni and Anjana Arunkumar and Jinbin Huang and Bum Chul Kwon and Chris Bryan},
  doi          = {10.1109/TVCG.2025.3535332},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6946-6962},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PromptAid: Visual prompt exploration, perturbation, testing and iteration for large language models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dashboard vision: Using eye-tracking to understand and predict dashboard viewing behaviors. <em>TVCG</em>, <em>31</em>(10), 6930-6945. (<a href='https://doi.org/10.1109/TVCG.2025.3532497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dashboards serve as effective visualization tools for conveying complex information. However, there exists a knowledge gap regarding how dashboard designs impact user engagement, necessitating designers to rely on their design expertise. Saliency has been used to comprehend viewing behaviors and assess visualizations, yet existing saliency models are primarily designed for single-view visualizations. To address this, we conduct an eye-tracking study to quantify participants’ viewing patterns on dashboards. We collect eye-movement data from 60 participants, each viewing 36 dashboards (16 representative dashboards shared across all and 20 unique to each participant), totaling 1,216 dashboards and 2,160 eye-movement data instances. Analysis of the data from 16 dashboards viewed by all participants provides insights into how dashboard objects and layout designs influence viewing behaviors. Our analysis confirms known viewing patterns and reveals new patterns related to dashboard layout designs. Using the eye-movement data and identified patterns, we develop a saliency model to predict viewing behaviors with dashboards. Compared to state-of-the-art models for single-view visualizations, our model demonstrates overall improvement in prediction performance for dashboards. Finally, we propose potential dashboard design guidelines, illustrate an application case, and discuss general scanning strategies along with limitations and future work.},
  archive      = {J_TVCG},
  author       = {Manling Yang and Yihan Hou and Ling Li and Remco Chang and Wei Zeng},
  doi          = {10.1109/TVCG.2025.3532497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6930-6945},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dashboard vision: Using eye-tracking to understand and predict dashboard viewing behaviors},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive knowledge transfer network based on human visual perception mechanism for no-reference point cloud quality assessment. <em>TVCG</em>, <em>31</em>(10), 6915-6929. (<a href='https://doi.org/10.1109/TVCG.2025.3532651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud perceptual quality assessment plays a critical role in many applications, including compression and communication. We propose PKT-PCQA, a point-based no-reference point cloud quality assessment deep learning network that emulates the human visual system by using progressive knowledge transfer to convert coarse-grained quality classification knowledge into a fine-grained quality prediction task. PKT-PCQA exploits local and global features, as well as an attention mechanism based on spatial and channel attention modules. Experiments on three large and independent point cloud assessment datasets show that PKT-PCQA outperforms existing no-reference and reduced-reference point cloud quality assessment methods and achieves better or similar performance compared to several State-of-the-Art full-reference methods.},
  archive      = {J_TVCG},
  author       = {Honglei Su and Yiyun Liu and Qi Liu and Hui Yuan and Raouf Hamzaoui},
  doi          = {10.1109/TVCG.2025.3532651},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6915-6929},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Progressive knowledge transfer network based on human visual perception mechanism for no-reference point cloud quality assessment},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraSculptor: Visual analytics for enhanced decision-making in road traffic planning. <em>TVCG</em>, <em>31</em>(10), 6899-6914. (<a href='https://doi.org/10.1109/TVCG.2025.3532498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of urban road networks significantly influences traffic conditions, underscoring the importance of informed traffic planning. Traffic planning experts rely on specialized platforms to simulate traffic systems, assessing the efficacy of the road network across various states of modifications. Nevertheless, a prevailing issue persists: many existing traffic planning platforms exhibit inefficiencies in flexibly interacting with the road network’s structure and attributes and intuitively comparing multiple states during the iterative planning process. This paper introduces TraSculptor, an interactive planning decision-making system. To develop TraSculptor, we identify and address two challenges: interactive modification of road networks and intuitive comparison of multiple network states. For the first challenge, we establish flexible interactions to enable experts to easily and directly modify the road network on the map. For the second challenge, we design a comparison view with a history tree of multiple states and a road-state matrix to facilitate intuitive comparison of road network states. To evaluate TraSculptor, we provided a usage scenario where the Braess’s paradox was showcased, invited experts to perform a case study on the Sioux Falls network, and collected expert feedback through interviews.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Yuanbang Liu and Mingrui Zhu and Da Xiang and Haiyue Yu and Zicheng Su and Qinglong Lu and Tobias Schreck and Yi Cai},
  doi          = {10.1109/TVCG.2025.3532498},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6899-6914},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TraSculptor: Visual analytics for enhanced decision-making in road traffic planning},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning instance-semantic sparse representation towards unsupervised object segmentation and shape abstraction with repeatable primitives. <em>TVCG</em>, <em>31</em>(10), 6884-6898. (<a href='https://doi.org/10.1109/TVCG.2025.3532081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding 3D object shapes necessitates shape representation by object parts abstracted from results of instance and semantic segmentation. Promising shape representations enable computers to interpret a shape with meaningful parts and identify their repeatability. However, supervised shape representations depend on costly annotation efforts, while current unsupervised methods work under strong semantic priors and involve multi-stage training, thereby limiting their generalization and deployment in shape reasoning and understanding. Driven by the tendency of high-dimensional semantically similar features to lie in or near low-dimensional subspaces, we introduce a one-stage, fully unsupervised framework towards semantic-aware shape representation. This framework produces joint instance segmentation, semantic segmentation, and shape abstraction through sparse representation and feature alignment of object parts in a high-dimensional space. For sparse representation, we devise a sparse latent membership pursuit method that models each object part feature as a sparse convex combination of point features at either the semantic or instance level, promoting part features in the same subspace to exhibit similar semantics. For feature alignment, we customize an attention-based strategy in the feature space to align instance- and semantic-level object part features and reconstruct the input shape using both of them, ensuring geometric reusability and semantic consistency of object parts. To firm up semantic disambiguation, we construct cascade unfrozen learning on geometric parameters of object parts. Experiments conducted on benchmark datasets confirm that our approach results in instance- and semantic-level joint segmentation and shape abstraction with repeatable primitives, providing coherent semantic interpretations of 3D object shapes across categories in a one-stage, fully unsupervised manner, without relying on annotations or heuristic semantic priors.},
  archive      = {J_TVCG},
  author       = {Jiaxin Li and Hongxing Wang and Jiawei Tan and Zhilong Ou and Junsong Yuan},
  doi          = {10.1109/TVCG.2025.3532081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6884-6898},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Aligning instance-semantic sparse representation towards unsupervised object segmentation and shape abstraction with repeatable primitives},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AmplitudeArrow: On-the-go AR menu selection using consecutive simple head gestures and amplitude visualization. <em>TVCG</em>, <em>31</em>(10), 6870-6883. (<a href='https://doi.org/10.1109/TVCG.2025.3531378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heads-up computing aims to provide synergistic digital assistance that minimally interferes with users’ on-the-go daily activities. Currently, the input modalities of heads-up computing are mainly voice and finger gestures. In this work, we propose and evaluate the AmplitudeArrow (AA) technique designed for on-the-go AR menu selection to demonstrate that consecutive simple head gestures can also be an effective input modality for heads-up computing. Specifically, AA arranges menu icons into one/two row(s). To select a target icon, the user first makes their head yaw to pre-select the target icon or the column containing it and then makes their head pitch to make the arrow in the target icon expand until the arrow covers the target icon completely, i.e., the pitch amplitude surpasses the selection confirmation threshold. User studies indicated that AA demonstrated robust resistance to walking-caused head perturbation and external factors such as other people/obstacles, delivering high accuracy (error rate $&lt; $ 5$\%$) and fast speed ($&lt; $ 1.5s per selection) when there were no more than six icon columns (twelve icons) distributed horizontally and evenly in a menu area with a horizontal visual angle of $43^{\circ }$.},
  archive      = {J_TVCG},
  author       = {Yang Tian and Youpeng Zhang and Yukang Yan and Shengdong Zhao and Xiaojuan Ma and Yuanchun Shi},
  doi          = {10.1109/TVCG.2025.3531378},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6870-6883},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AmplitudeArrow: On-the-go AR menu selection using consecutive simple head gestures and amplitude visualization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZigzagNetVis: Suggesting temporal resolutions for graph visualization using zigzag persistence. <em>TVCG</em>, <em>31</em>(10), 6852-6869. (<a href='https://doi.org/10.1109/TVCG.2025.3528197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs are commonly used to represent complex systems and track the evolution of their constituents over time. Visualizing these graphs is crucial as it allows one to quickly identify anomalies, trends, patterns, and other properties that facilitate better decision-making. In this context, selecting an appropriate temporal resolution is essential for constructing and visually analyzing the layout. The choice of resolution is particularly important, especially when dealing with temporally sparse graphs. In such cases, changing the temporal resolution by grouping events (i.e., edges) from consecutive timestamps — a technique known as timeslicing — can aid in the analysis and reveal patterns that might not be discernible otherwise. However, selecting an appropriate temporal resolution is a challenging task. In this paper, we propose ZigzagNetVis, a methodology that suggests temporal resolutions potentially relevant for analyzing a given graph, i.e., resolutions that lead to substantial topological changes in the graph structure. ZigzagNetVis achieves this by leveraging zigzag persistent homology, a well-established technique from Topological Data Analysis (TDA). To improve visual graph analysis, ZigzagNetVis incorporates the colored barcode, a novel timeline-based visualization inspired by persistence barcodes commonly used in TDA. We also contribute with a web-based system prototype that implements suggestion methodology and visualization tools. Finally, we demonstrate the usefulness and effectiveness of ZigzagNetVis through a usage scenario, a user study with 27 participants, and a detailed quantitative evaluation.},
  archive      = {J_TVCG},
  author       = {Raphaël Tinarrage and Jean R. Ponciano and Claudio D. G. Linhares and Agma J. M. Traina and Jorge Poco},
  doi          = {10.1109/TVCG.2025.3528197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6852-6869},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ZigzagNetVis: Suggesting temporal resolutions for graph visualization using zigzag persistence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersive data-driven storytelling: Scoping an emerging field through the lenses of research, journalism, and games. <em>TVCG</em>, <em>31</em>(10), 6839-6851. (<a href='https://doi.org/10.1109/TVCG.2025.3531138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, data-driven stories have found a firm footing in journalism, business, and education. They leverage visualization and storytelling to convey information to broader audiences. Likewise, immersive technologies, like augmented and virtual reality devices, provide excellent potential for exploring and explaining data, thus inviting research on how data-driven storytelling transfers to immersive environments. To gain a better understanding of this exciting novel research area, we conducted a scoping review on the emerging notion of immersive data-driven storytelling, extended by surveying immersive data journalism and by analyzing immersive games, selected based on community reviews and tags. We present our methodology for the survey and discuss prominent themes that coalesce the knowledge we extracted from the literature, journalism, and games. These themes include, among others, the spatial embodiment of narration, the incorporation of the users and their context into narratives, and the balance between guiding the user versus promoting serendipity. Our discussion of these themes reveals research opportunities and challenges that will inform the design of immersive data-driven stories in the future.},
  archive      = {J_TVCG},
  author       = {Julián Méndez and Weizhou Luo and Rufat Rzayev and Wolfgang Büschel and Raimund Dachselt},
  doi          = {10.1109/TVCG.2025.3531138},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6839-6851},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive data-driven storytelling: Scoping an emerging field through the lenses of research, journalism, and games},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptually-aligned dynamic facial projection mapping by high-speed face-tracking method and lens-shift co-axial setup. <em>TVCG</em>, <em>31</em>(10), 6824-6838. (<a href='https://doi.org/10.1109/TVCG.2025.3527203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Facial Projection Mapping (DFPM) overlays computer-generated images onto human faces to create immersive experiences that have been used in the makeup and entertainment industries. In this study, we propose two concepts to reduce the misalignment artifacts between projected images and target faces, which is a persistent challenge for DFPM. Our first concept is a high-speed face-tracking method that exploits temporal information. We first introduce a cropped-area-limited inter/extrapolation-based face detection framework, which allows parallel execution with facial landmark detection. We then propose a novel hybrid facial landmark detection method that combines fast Ensemble of Regression Trees (ERT)-based detections and an auxiliary detection. ERT-based detections rapidly produce results in 0.107 ms using temporal information with the support of auxiliary detection to recover from detection errors. To train the facial landmark detection method, we propose an innovative method for simulating high-frame-rate video annotations to address the lack of publicly available high-frame-rate annotated datasets. Our second concept is a lens-shift co-axial projector-camera setup that maintains a high optical alignment with only a 1.274-pixel error between 1 m and 2 m depth. This setup reduces misalignment by applying the same optical designs to the projector and camera without causing large misalignment as in conventional methods. Based on these concepts, we developed a novel high-speed DFPM system that achieves nearly perfect alignment with human visual perception.},
  archive      = {J_TVCG},
  author       = {Hao-Lun Peng and Kengo Sato and Soran Nakagawa and Yoshihiro Watanabe},
  doi          = {10.1109/TVCG.2025.3527203},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6824-6838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptually-aligned dynamic facial projection mapping by high-speed face-tracking method and lens-shift co-axial setup},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards voronoi diagrams of surface patches. <em>TVCG</em>, <em>31</em>(10), 6810-6823. (<a href='https://doi.org/10.1109/TVCG.2025.3531445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extraction of a high-fidelity 3D medial axis is a crucial operation in CAD. When dealing with a polygonal model as input, ensuring accuracy and tidiness becomes challenging due to discretization errors inherent in the mesh surface. Commonly, existing approaches yield medial-axis surfaces with various artifacts, including zigzag boundaries, bumpy surfaces, unwanted spikes, and non-smooth stitching curves. Considering that the surface of a CAD model can be easily decomposed into a collection of surface patches, its 3D medial axis can be extracted by computing the Voronoi diagram of these surface patches, where each surface patch serves as a generator. However, no solver currently exists for accurately computing such an extended Voronoi diagram. Under the assumption that each generator defines a linear distance field over a sufficiently small range, our approach operates by tetrahedralizing the region of interest and computing the medial axis within each tetrahedral element. Just as SurfaceVoronoi computes surface-based Voronoi diagrams by cutting a 3D prism with 3D planes (each plane encodes a linear field in a triangle), the key operation in this paper is to conduct the hyperplane cutting process in 4D, where each hyperplane encodes a linear field in a tetrahedron. In comparison with the state-of-the-art, our algorithm produces better outcomes. Furthermore, it can also be used to compute the offset surface.},
  archive      = {J_TVCG},
  author       = {Pengfei Wang and Jiantao Song and Lei Wang and Shiqing Xin and Dong-Ming Yan and Shuangmin Chen and Changhe Tu and Wenping Wang},
  doi          = {10.1109/TVCG.2025.3531445},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6810-6823},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards voronoi diagrams of surface patches},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards photorealistic portrait style transfer in unconstrained conditions. <em>TVCG</em>, <em>31</em>(10), 6796-6809. (<a href='https://doi.org/10.1109/TVCG.2025.3529751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a photorealistic portrait style transfer approach that allows for producing high-quality results in previously challenging unconstrained conditions, e.g., large facial perspective difference between portraits, faces with complex illumination (e.g., shadow and highlight) and occlusion, and can test without portrait parsing masks. We achieve this by developing a framework to learn robust dense correspondence across portraits for semantically aligned style transfer, where a regional style contrastive learning strategy is devised to boost the effectiveness of semantic-aware style transfer while enhancing the robustness to complex illumination. Extensive experiments demonstrate the superiority of our method.},
  archive      = {J_TVCG},
  author       = {Xinbo Wang and Qing Zhang and Yongwei Nie and Wei-Shi Zheng},
  doi          = {10.1109/TVCG.2025.3529751},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6796-6809},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards photorealistic portrait style transfer in unconstrained conditions},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Narrative player: Reviving data narratives with visuals. <em>TVCG</em>, <em>31</em>(10), 6781-6795. (<a href='https://doi.org/10.1109/TVCG.2025.3530512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-rich documents are commonly found across various fields such as business, finance, and science. However, a general limitation of these documents for reading is their reliance on text to convey data and facts. Visual representation of text aids in providing a satisfactory reading experience in comprehension and engagement. However, existing work emphasizes presenting the insights within phrases or sentences, rather than fully conveying data stories within the whole paragraphs and engaging readers. To provide readers with satisfactory data stories, this paper presents Narrative Player, a novel method that automatically revives data narratives with consistent and contextualized visuals. Specifically, it accepts a paragraph and corresponding data table as input and leverages LLMs to characterize the clauses and extract contextualized data facts. Subsequently, the facts are transformed into a coherent visualization sequence with a carefully designed optimization-based approach. Animations are also assigned between adjacent visualizations to enable seamless transitions. Finally, the visualization sequence, transition animations, and audio narration generated by text-to-speech technologies are rendered into a data video. The evaluation results showed that the automatic-generated data videos were well-received by participants and experts for enhancing reading.},
  archive      = {J_TVCG},
  author       = {Zekai Shao and Leixian Shen and Haotian Li and Yi Shan and Huamin Qu and Yun Wang and Siming Chen},
  doi          = {10.1109/TVCG.2025.3530512},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6781-6795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Narrative player: Reviving data narratives with visuals},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDF-ISM: Internal structure modeling of human head using probabilistic directed distance field. <em>TVCG</em>, <em>31</em>(10), 6767-6780. (<a href='https://doi.org/10.1109/TVCG.2025.3530484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing interest surrounding 3D human heads for digital avatars and simulations has highlighted the need for accurate internal modeling rather than solely focusing on external approximations. Existing approaches rely on traditional optimization techniques applied to explicit 3D representations like point clouds and meshes, leading to computational inefficiencies and challenges in capturing local geometric features. To tackle these problems, we propose a novel modeling method called DDF-ISM. It leverages a probabilistic Directed Distance Field for Internal Structure Modeling, facilitating efficient and anatomically accurate deformation of different parts of the human head. DDF-ISM comprises two key components: 1) a probabilistic DDF network for implicit representation of the target model to provide crucial local geometric information, and 2) a conditioned deformation network guided by the local geometry. Additionally, we introduce a large-scale dataset of human heads with internal structures derived from high-quality Computed Tomography (CT) scans, along with well-designed template models encompassing skull, mandible, brain, and head surface. Evaluation on this dataset showcases the superiority of our approach over existing methods, exhibiting superior performance in both modeling quality and efficiency.},
  archive      = {J_TVCG},
  author       = {Zhuoman Liu and Yan Luximon and Wei Lin Ng and Eric Chung},
  doi          = {10.1109/TVCG.2025.3530484},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6767-6780},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DDF-ISM: Internal structure modeling of human head using probabilistic directed distance field},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable and high-quality neural implicit representation for 3D reconstruction. <em>TVCG</em>, <em>31</em>(10), 6751-6766. (<a href='https://doi.org/10.1109/TVCG.2025.3530452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various SDF-based neural implicit surface reconstruction methods have been proposed recently, and have demonstrated remarkable modeling capabilities. However, due to the global nature and limited representation ability of a single network, existing methods still suffer from many drawbacks, such as limited accuracy and scale of the reconstruction. In this paper, we propose a versatile, scalable and high-quality neural implicit representation to address these issues. We integrate a divide-and-conquer approach into the neural SDF-based reconstruction. Specifically, we model the object or scene as a fusion of multiple independent local neural SDFs with overlapping regions. The construction of our representation involves three key steps: (1) constructing the distribution and overlap relationship of the local radiance fields based on object structure or data distribution, (2) relative pose registration for adjacent local SDFs, and (3) SDF blending. Thanks to the independent representation of each local region, our approach can not only achieve high-fidelity surface reconstruction, but also enable scalable scene reconstruction. Extensive experimental results demonstrate the effectiveness and practicality of our proposed method.},
  archive      = {J_TVCG},
  author       = {Leyuan Yang and Bailin Deng and Juyong Zhang},
  doi          = {10.1109/TVCG.2025.3530452},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6751-6766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable and high-quality neural implicit representation for 3D reconstruction},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpeechAct: Towards generating whole-body motion from speech. <em>TVCG</em>, <em>31</em>(10), 6737-6750. (<a href='https://doi.org/10.1109/TVCG.2025.3529611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole-body motion generation from speech audio is crucial for computer graphics and immersive VR/AR. Prior methods struggle to produce natural and diverse whole-body motions from speech. In this paper, we introduce a novel method, named SpeechAct, based on a hybrid point representation and contrastive motion learning to boost realism and diversity in motion generation. Our hybrid point representation leverages the advantages of keypoint representation and surface points of 3D body model, which is easy to learn and helps to achieve smooth and natural motion generation from speech audio. We design a VQ-VAE to learn a motion codebook using our hybrid presentation, and then regress the motion from the input audio using a translation model. To boost diversity in motion generation, we propose a contrastive motion learning method according to the intuitive idea that the generated motion should be different from the motions of other audios and other speakers. We collect negative samples from other audio inputs and other speakers using our translation model. With these negative samples, we pull the current motion away from them using a contrastive loss to produce more distinctive representations. In addition, we compose a face generator to generate deterministic face motion due to the strong connection between the face movements and the speech audio. Experimental results validate the superior performance of our model. The code is available at http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct/index.html.},
  archive      = {J_TVCG},
  author       = {Jinsong Zhang and Minjie Zhu and Yuxiang Zhang and Zerong Zheng and Yebin Liu and Kun Li},
  doi          = {10.1109/TVCG.2025.3529611},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6737-6750},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SpeechAct: Towards generating whole-body motion from speech},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAT: Visibility aware transformer for fine-grained clothed human reconstruction. <em>TVCG</em>, <em>31</em>(10), 6719-6736. (<a href='https://doi.org/10.1109/TVCG.2025.3528021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reconstruct 3D clothed human with accurate fine-grained details from sparse views, we propose a deep cooperating two-level global to fine-grained reconstruction framework that constructs robust global geometry to guide fine-grained geometry learning. The core of the framework is a novel visibility aware Transformer VAT, which bridges the two-level reconstruction architecture by connecting its global encoder and fine-grained decoder with two pixel-aligned implicit functions, respectively. The global encoder fuses semantic features of multiple views to integrate global geometric features. In the fine-grained decoder, visibility aware attention mechanism is designed to efficiently fuse multi-view and multi-scale features for mining fine-grained geometric features. The global encoder and fine-grained decoder are connected by a global embeding module to form a deep cooperation in the two-level framework, which provides global geometric embedding as a query guidance for calculating visibility aware attention in the fine-grained decoder. In addition, to extract highly aligned multi-scale features for the two-level reconstruction architecture, we design an image feature extractor MSUNet, which establishes strong semantic connections between different scales at minimal cost. Our proposed framework is end-to-end trainable, with all modules jointly optimized. We validate the effectiveness of our framework on public benchmarks, and experimental results demonstrate that our method has significant advantages over state-of-the-art methods in terms of both fine-grained performance and generalization.},
  archive      = {J_TVCG},
  author       = {Xiaoyan Zhang and Zibin Zhu and Hong Xie and Sisi Ren and Jianmin Jiang},
  doi          = {10.1109/TVCG.2025.3528021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6719-6736},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VAT: Visibility aware transformer for fine-grained clothed human reconstruction},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-criteria decision analysis for aiding glyph design. <em>TVCG</em>, <em>31</em>(10), 6705-6718. (<a href='https://doi.org/10.1109/TVCG.2025.3526918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glyph-based visualization is one of the main techniques for visualizing complex multivariate data. With small glyphs, data variables are typically encoded with relatively low visual and perceptual precision. Glyph designers have to contemplate the trade-offs in allocating visual channels when there is a large number of data variables. While there are many successful glyph designs in the literature, there is not yet a systematic method for assisting visualization designers to evaluate different design options that feature different types of trade-offs. In this paper, we present an evaluation scheme based on the multi-criteria decision analysis (MCDA) methodology. The scheme provides designers with a structured way to consider their glyph designs from a range of perspectives, while rendering a semi-quantitative template for evaluating different design options. In addition, this work provides guideposts for future empirical research to obtain more quantitative measurements that can be used in MCDA-aided glyph design processes.},
  archive      = {J_TVCG},
  author       = {Hong-Po Hsieh and Amy Zavatsky and Min Chen},
  doi          = {10.1109/TVCG.2025.3526918},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6705-6718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-criteria decision analysis for aiding glyph design},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithms for consistent dynamic labeling of maps with a time-slider interface. <em>TVCG</em>, <em>31</em>(10), 6691-6704. (<a href='https://doi.org/10.1109/TVCG.2025.3527582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User interfaces for inspecting spatio-temporal events often allow their users to filter the events by specifying a time window with a time slider. We consider the case that filtered events are visualized on a map using textual or iconic labels. However, to ensure a clear visualization, not all filtered events are annotated with a label. We present algorithms for setting up a data structure that encodes for every possible time window the set of displayed labels. Our algorithms ensure that the displayed labels never overlap and guarantee the stability of the labeling during certain basic interactions with the time slider. Assuming that the labels have different priorities (weights), we aim to maximize the weight of the displayed labels integrated over all possible time windows. As basic interactions, we consider moving the entire time window, symmetrically scaling it, and dragging one of its endpoints. We consider two stability requirements: (1) during a basic interaction, a label should appear and disappear at most once; (2) if a label is displayed for a time window $Q$, then it is also displayed for all the time windows contained in $Q$ and that contain its timestamp. We prove that finding an optimal solution is NP-hard and propose efficient constant-factor approximation algorithms for unit-square and unit-disk labels, as well as a fast greedy heuristic for arbitrarily shaped labels. In experiments on real-world data, we compare the non-exact algorithms with an exact approach through integer linear programming.},
  archive      = {J_TVCG},
  author       = {Annika Bonerath and Anne Driemel and Jan-Henrik Haunert and Herman Haverkort and Elmar Langetepe and Benjamin Niedermann},
  doi          = {10.1109/TVCG.2025.3527582},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6691-6704},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Algorithms for consistent dynamic labeling of maps with a time-slider interface},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-field visualization: Trait design and trait-induced merge trees. <em>TVCG</em>, <em>31</em>(10), 6677-6690. (<a href='https://doi.org/10.1109/TVCG.2025.3525974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature level sets (FLS) have shown significant potential in the analysis of multi-field data by using traits defined in attribute space to specify features in the domain. In this work, we address key challenges in the practical use of FLS: trait design and feature selection for rendering. To simplify trait design, we propose a Cartesian decomposition of traits into simpler components, making the process more intuitive and computationally efficient. Additionally, we utilize dictionary learning results to automatically suggest point traits. To enhance feature selection, we introduce trait-induced merge trees (TIMTs), a generalization of merge trees for feature level sets, aimed at topologically analyzing tensor fields or general multi-variate data. The leaves in the TIMT represent areas in the input data that are closest to the defined trait, thereby most closely resembling the defined feature. This merge tree provides a hierarchy of features, enabling the querying of the most relevant and persistent features. Our method includes various query techniques for the tree, allowing the highlighting of different aspects. We demonstrate the cross-application capabilities of this approach through five case studies from different domains.},
  archive      = {J_TVCG},
  author       = {Danhua Lei and Jochen Jankowai and Petar Hristov and Hamish Carr and Leif Denby and Talha Bin Masood and Ingrid Hotz},
  doi          = {10.1109/TVCG.2025.3525974},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6677-6690},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-field visualization: Trait design and trait-induced merge trees},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mesh2Brep: B-rep reconstruction via robust primitive fitting and intersection-aware constraints. <em>TVCG</em>, <em>31</em>(10), 6661-6676. (<a href='https://doi.org/10.1109/TVCG.2025.3525844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In boundary representation (B-rep) reconstruction for computer aided design (CAD) applications, it is still challenging with existing methods to distinguish primitives in the smoothly blended regions reasonably. Thus, intensive manual post-processing is always required for correcting the primitives and their neighboring relationships to obtain a valid B-rep solid, seriously preventing the efficiency. In this paper, we address these challenges by presenting two novel techniques. The first is to robustly extract primitives by iteratively estimating the probability distribution of the noise to eliminate outliers. The second is to present intersection-aware constraints, like tangency and collinearity constraints, to correctly obtain intersections between primitives, which have not been explored in existing methods to our knowledge. Therefore, we can effectively extract primitives, especially those blended smoothly, and obtain high-quality relationships between them. As a result, a valid B-rep model can be constructed without a lot of manual post-processing on topology correction, while not with existing methods. As a benefit, with our constructed B-rep models, their corresponding meshes can be intuitively and conveniently edited, which is quite useful in CAD applications. Experimental results show that our proposed B-rep construction method outperforms both classical and recent learning-based methods in terms of reconstruction efficiency and accuracy.},
  archive      = {J_TVCG},
  author       = {Zeyu Shen and Mingyang Zhao and Dong-Ming Yan and Wencheng Wang},
  doi          = {10.1109/TVCG.2025.3525844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6661-6676},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh2Brep: B-rep reconstruction via robust primitive fitting and intersection-aware constraints},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointCG: Self-supervised point cloud learning via joint completion and generation. <em>TVCG</em>, <em>31</em>(10), 6648-6660. (<a href='https://doi.org/10.1109/TVCG.2025.3526257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of self-supervised point cloud learning lies in setting up appropriate pretext tasks, to construct a pre-training framework that enables the encoder to perceive 3D objects effectively. In this article, we integrate two prevalent methods, masked point modeling (MPM) and 3D-to-2D generation, as pretext tasks within a pre-training framework. We leverage the spatial awareness and precise supervision offered by these two methods to address their respective limitations: ambiguous supervision signals and insensitivity to geometric information. Specifically, the proposed framework, abbreviated as PointCG, consists of a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. We first capture visible points from arbitrary views as inputs by removing hidden points. Then, HPC extracts representations of the inputs with an encoder and completes the entire shape with a decoder, while AIG is used to generate rendered images based on the visible points’ representations. Extensive experiments demonstrate the superiority of the proposed method over the baselines in various downstream tasks. Our code will be made available upon acceptance.},
  archive      = {J_TVCG},
  author       = {Yun Liu and Peng Li and Xuefeng Yan and Liangliang Nan and Bing Wang and Honghua Chen and Lina Gong and Wei Zhao and Mingqiang Wei},
  doi          = {10.1109/TVCG.2025.3526257},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6648-6660},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PointCG: Self-supervised point cloud learning via joint completion and generation},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantics-aware avatar locomotion adaption for indoor cross-scene AR telepresence. <em>TVCG</em>, <em>31</em>(10), 6633-6647. (<a href='https://doi.org/10.1109/TVCG.2025.3525697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically dispersed users often rely on virtual avatars as intermediaries to facilitate interactive communication and collaboration. However, existing methods for augmented reality (AR) telepresence applications exhibit limitations, including restricted movement within confined sub-areas, lack of smooth transitions, and the necessity for manually establishing object mapping between dissimilar environments. We present a novel interactive AR framework for virtual avatar locomotion adaption while preserving semantic coherence across dissimilar indoor scenes. Initially, we conduct a preliminary user study to identify key attributes influencing preferred avatar movement. These attributes are quantified as features, and a dataset of user annotations on avatar movements is created. Based on the user interaction and scene configurations, we employ a deep reinforcement learning neural network to guide the avatar to the ideal position while maximizing semantic coherence. We validate our proposed framework through simulations and user studies by implementing an AR-based 3D telepresence prototype, demonstrating the efficacy of our framework in conveying user intentions across dissimilar environments, enabling natural and immersive 3D telepresence interactions.},
  archive      = {J_TVCG},
  author       = {Yi-Jun Li and Hao-Zhong Yang and Wen-Tong Shu and Miao Wang},
  doi          = {10.1109/TVCG.2025.3525697},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6633-6647},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantics-aware avatar locomotion adaption for indoor cross-scene AR telepresence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
