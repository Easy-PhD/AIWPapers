<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="eai">EAI - 11</h2>
<ul>
<li><details>
<summary>
(2025). Federated learning strategies for integrating composite meta-consistency loss with multi-head attention. <em>EAI</em>, <em>38</em>(4), 725-742. (<a href='https://doi.org/10.1177/30504554251340238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of accuracy decline caused by feature redundancy, this paper designs a federated learning strategy that combines composite meta-consistency loss and multi-head attention. Firstly, this paper decorrelates the features based on the dual theory of constraints to eliminate redundant information, and improves the stability of the model through gradient-based regularization. Composite meta-consistency loss is constructed based on these two optimization methods. Experiments show that compared with the latest algorithms, the maximum accuracy of CIFAR-10 and Oxford-Pets in this paper is improved by 0.82% and 2.19%, respectively. After that, this paper introduces multi-head attention into the framework of federated learning. After capturing richer context information in the process of feature extraction, the combination of inner-layer update and outer-layer update of the meta-learning method enables the federated learning framework to effectively cope with the data distribution of different clients and finally accelerate the convergence speed. Compared with other algorithms, the average accuracy of the first 40 rounds in the MINIST, CIFAR-10 and CIFAR-100 data sets is higher. In CIFAR-10, SVHN, Oxford-Pets, taking Robust-HDP as the benchmark, the speedup ratio reaches 1.5, 1.42, and 1.34, respectively, which is faster than other algorithms.},
  archive      = {J_EAI},
  author       = {Afei Li and Xiaolei Yang and Li Ma and Lu Yu and Liyu Hao and Yongshan Liu},
  doi          = {10.1177/30504554251340238},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {725-742},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Federated learning strategies for integrating composite meta-consistency loss with multi-head attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection. <em>EAI</em>, <em>38</em>(4), 707-724. (<a href='https://doi.org/10.1177/30504554251340241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that pillar-based detectors perform better in terms of both accuracy and speed, but these detectors perform poorly for detecting small objects such as pedestrians and cyclists. To solve this problem, we propose a highly efficient pillar-based model named MASNet, which mainly consists of a pillar mix attention (PMA) module, an attention-pooling operation, and a focal sparse network (FSN) module. The PMA module encodes the pillar features by fusion of the point-wise attention module and the channel-wise attention module. The attention-pooling operation aggregates the attention-encoded pillar features in a more comprehensive way to obtain the most expressive pillar features. In addition, the FSN module exploits the intrinsic sparsity of the data by introducing focal sparse convolution, which enriches the learned pillar features in the foreground without adding redundant pillars in other regions. On the KITTI three-dimensional (3D) Object Detection Benchmark, it achieves a 3D average precision of (77.81%, 60.30%, and 53.92%) in easy, moderate, and hard levels, which outperforms other pillar-based methods for the detection of cyclists. Additionally, our method is only 0.52% lower than the top-ranked method (pillar feature network (PIFENet)) on the KITTI Bird's Eye View pedestrian leaderboard, but our inference speed reaches 41 frames per second ahead of PIFENet by 57.69%.},
  archive      = {J_EAI},
  author       = {Fuqiang You and H Ziheng Zhang and Hao Chen},
  doi          = {10.1177/30504554251340241},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {707-724},
  shortjournal = {Eur. Artif. Intell.},
  title        = {MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal coupling prompt learning for image classification tasks. <em>EAI</em>, <em>38</em>(4), 684-706. (<a href='https://doi.org/10.1177/30504554251335569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, vision-language pretraining (VLP) models have become a crucial driving force in the advancement of artificial intelligence. Besides, studies such as contrastive language-image pretraining (CLIP) have demonstrated that incorporating prompt learning within VLP models can significantly enhance the performance of downstream tasks. However, we believe that CLIP’s visual encoder suffers from feature extraction bias in image classification tasks, which is because of the uneven quantity and distribution of image features CLIP learned between the pretraining and fine-tuning stages. This can be further summarized as an inherent bias in feature extraction for differently distributed samples during the pretraining phase. To address the above problem this paper proposes (i) text-semantic hierarchical injection prompt learning method, which constructs self-attention layers and prompt mapping structures and injects text semantic features into the visual encoder layer by layer to generate visual prompt features and (ii) visual-semantic attention interactive prompt learning method, which further integrates text embeddings with the output features of the visual encoder through cross-attention and constructs instance-level text prompt features for each image. Based on the two above methods, this paper further proposes the multimodal coupling prompt learning CLIP (MCPL-CLIP) to enhance CLIP’s performance in image classification tasks. Experiments conducted on 15 image classification datasets demonstrate that MCPL-CLIP outperforms baseline models such as MaPLe, CoCoOp, and CoOp in cross-dataset transfer, domain generalization, and base-to-novel class generalization tasks, showcasing its superior text semantic representation and visual feature extraction capabilities.},
  archive      = {J_EAI},
  author       = {Yufei Liu and Hua Cheng and Yiquan Fang and Yiming Pan and Zehong Qian and Xiaoning Chen},
  doi          = {10.1177/30504554251335569},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {684-706},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multimodal coupling prompt learning for image classification tasks},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KANDiff: Low-light image enhancement based on diffusion models. <em>EAI</em>, <em>38</em>(4), 666-683. (<a href='https://doi.org/10.1177/30504554251342571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement technique aims to improve the contrast and brightness of low-light images. Diffusion models, attributed to adeptness at capturing intricate details, have achieved good results in image enhancement, but there are problems such as inadequate estimation of noise characteristics and the emergence of color bias in the enhanced images. To address the aforementioned problems, this paper proposes a diffusion models-based method for low-light image enhancement, termed KANDiff. In the diffusion model architecture, this paper adds the nonlinear learnable activation function Kolmogorov–Arnold network to the noise estimation network U-Net to generate higher quality enhanced images. Additionally, KANDiff mitigates color bias in the enhanced images through a joint loss function and employs a patch-based image restoration strategy to significantly enhance the model generalization capability. The experimental results show that the KANDiff algorithm proposed in this paper can achieve high-quality image enhancement and achieve better enhancement effects compared to other algorithms.},
  archive      = {J_EAI},
  author       = {Yuanxin Ren and Minghui Yue and Yuxuan He and Liye Zhang},
  doi          = {10.1177/30504554251342571},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {666-683},
  shortjournal = {Eur. Artif. Intell.},
  title        = {KANDiff: Low-light image enhancement based on diffusion models},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-aspect graph representation feature integration for recommender dialogue system. <em>EAI</em>, <em>38</em>(4), 630-648. (<a href='https://doi.org/10.1177/30504554251347451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation-based dialogue systems aim to capture user preferences via interactive conversations for personalized recommendations. While existing studies focus on modeling user preferences, real-time dialog scenarios face challenges in balancing historical conversation contexts and immediate interests. This study proposes MGIRD, a multi-aspect graph representation approach integrating ordinary graphs and hypergraphs. We use graph structures to model users’ current interests and hypergraphs for historical conversation features, while incorporating historical behaviors in the recommendation module to balance context relevance. A novel item selection mechanism is introduced during dialog generation to naturally integrate recommended items. Experiments on Chinese TG-Redial and English Redial datasets show MGIRD outperforms most state-of-the-art methods in recommendation accuracy and dialog diversity, validating its effectiveness in enhancing recommendation quality and conversational fluency.},
  archive      = {J_EAI},
  author       = {Shi Li and Qing Yang Bai},
  doi          = {10.1177/30504554251347451},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {630-648},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multi-aspect graph representation feature integration for recommender dialogue system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph anomaly detection model combining dual behavior contrast. <em>EAI</em>, <em>38</em>(4), 617-629. (<a href='https://doi.org/10.1177/30504554251347752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current dynamic graph anomaly detection models learn multibehavior patterns for abnormal edges poorly and rely too much on the differences in long-term snapshots. Aiming at the above problems, combine dual behavior contrast dynamic graph anomaly detection model is proposed. Firstly, a dual behavior learning module is designed, where the role-based behavior learning submodule constructs graphlet degree vector by identifying four self-isomorphic orbits to capture deep structural features, while the attribute-based behavior learning submodule obtains attribute vectors through graph convolutional network. Then, the results are combined in the dynamic edge representation module to form the dynamic representations of edges to capture dual behavior patterns. Lastly, the anomaly detection module is designed to detect newly generated edges by combining contrastive learning with gated recurrent unit. We conduct experiments from four perspectives: anomaly detection accuracy, parameter sensitivity, robustness of module variants, and model runtime efficiency. The results demonstrate that the model achieves a peak accuracy of 92.05% in the task of dynamic edge anomaly detection.},
  archive      = {J_EAI},
  author       = {Jian Feng and Xiaotian Zhao},
  doi          = {10.1177/30504554251347752},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {617-629},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dynamic graph anomaly detection model combining dual behavior contrast},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action recognition based virtual panorama live broadcasting system. <em>EAI</em>, <em>38</em>(4), 601-616. (<a href='https://doi.org/10.1177/30504554251347439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant success of transformers in natural language processing, an increasing number of researchers are introducing them into the field of computer vision, particularly for action recognition. As a crucial task in video understanding, action recognition has significant applications in live broadcasting, autonomous driving, and medical diagnostics. The attention mechanisms in transformers mimicking human visual attention allocation, thereby enhancing the processing capabilities and comprehension of long video sequences. However, they often overlook the aggregation of multiscale detail features and the hierarchical representations of early visual information. Additionally, attention networks are computationally intensive and parameter-heavy, complicating model training and extending inference times, rendering them unsuitable for real-time applications. To crack these nuts, we propose a lightweight multiscale action recognition model based on convolutional enhancement block (ConvEB) and multiscale average pooling encoder. The ConvEB aims to establish long-range dependencies among multiscale local features in the early stages of the network, providing effective inductive biases for the attention network to compensate for the loss of detailed information. Moreover, we introduce a parallel pooling mixer to replace the original attention mixer, ensuring model lightweight while maintaining recognition accuracy. Finally, we deploy this model in the construction of a virtual panorama live broadcasting system. Experimental results demonstrate that our action recognition algorithm achieves competitive performance, and the constructed panoramic system basically meets the needs of daily live broadcasting.},
  archive      = {J_EAI},
  author       = {Lichuan Geng and Zihao Zhao and Qiaohong Hou},
  doi          = {10.1177/30504554251347439},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {601-616},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Action recognition based virtual panorama live broadcasting system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 581-600. (<a href='https://doi.org/10.1177/30504554251328322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify objects seamlessly embedded in the surrounding environment. Due to the high inherent similarity between the texture of the camouflaged object and its complex background, making COD far more challenging than traditional target detection. To solve these problems, we propose a method that uses holistic boundary information to optimize COD through a two-stage strategy. Specifically, the feature enhancement module is initially implemented to refine features at different scales and emphasize boundary details of camouflaged entities. Then, our network employs a boundary localization module to guide low-level local edge features through high-level global semantic. Furthermore, the boundary-embedded feature aggregation module is introduced to achieve cross-level fusion of multi-scale features, by embedding and effectively activating boundary information, which reduces the interference from cluttered backgrounds. Extensive experiments on four benchmark datasets demonstrate that our proposed model outperforms the other 17 state-of-the-art COD methods. The source code and results of our method are available at https://github.com/WObaibai/BSNet .},
  archive      = {J_EAI},
  author       = {Yuhong Chen and Meng Dai and Qing Zhang and Jiayun Wu},
  doi          = {10.1177/30504554251328322},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {581-600},
  shortjournal = {Eur. Artif. Intell.},
  title        = {BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 530-542. (<a href='https://doi.org/10.1177/30504554251351219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is an emerging research direction in computer vision in recent years, aiming to segment objects that are visually integrated with the background, which is a valuable task and has attracted increasing interest from researchers. Since camouflaged objects are integrated with their surroundings, their boundaries are also very blurred, and it becomes an important issue in COD to segment the edges of the objects accurately and completely. To address the above issues, in this article, we propose a novel multi-level edge-enhanced fusion for camouflaged object detection network (ME 2 FNet). Specifically, we design a residual texture enhanced module to obtain more refined features from the noise-filled backbone features. Then, we design an edge extraction module (EEM), which aims to extract effective edge semantic information from low-level features and high-level features by a simple local channel attention mechanism. Finally, we design a boundary-guided fusion module, which aims to fuse the previously obtained prior information. It can fuse the edge information extracted by EEM with the features at different levels of the backbone network, and guide the learning under the supervision of ground truth. At the same time, it fuses the high-level global information with the features at different levels, so that the final predicted edge is clearer and the overall structure is more complete. Extensive experiments on three challenging benchmark datasets have shown that ME 2 FNet outperforms multiple leading-edge models in recent years and achieves advanced results under four widely used evaluation metrics.},
  archive      = {J_EAI},
  author       = {Xuwei Tong and Guangjian Zhang and Yuhao Yang},
  doi          = {10.1177/30504554251351219},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {530-542},
  shortjournal = {Eur. Artif. Intell.},
  title        = {ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging context-sensitivity for user-centered explainability. <em>EAI</em>, <em>38</em>(4), 496-529. (<a href='https://doi.org/10.1177/30504554251331568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread integration of artificial intelligence (AI) into our daily lives has spurred an escalating demand for explainable AI (XAI). This demand is particularly pronounced in critical domains such as healthcare and finance, where understanding the decision-making processes of AI models is paramount. Despite noteworthy strides in XAI, prevailing approaches often neglect the crucial dimension of context, resulting in explanations that are challenging to comprehend and act upon for different stakeholders. This paper advocates for a paradigm shift towards context-sensitive explainability, tailoring explanations to users’ specific needs and understanding promoting inclusivity and accessibility. We propose a novel context taxonomy and a versatile framework, “ConEX” for developing context-sensitive explanations using any state-of-the-art post hoc explainer. Our empirical user study highlights diverse preferences for contextualization levels, emphasizing the importance of catering to these preferences to build trust and satisfaction in AI systems. Our contributions extend beyond the theoretical realm, offering practical guidance for developing context-sensitive explanations that are tailored to the specific needs of diverse stakeholders. By embracing context-sensitive explainability, we can unlock the true potential of AI, fostering trust, transparency, and informed decision-making across various domains.},
  archive      = {J_EAI},
  author       = {Yasmeen Khaled and Nourhan Ehab},
  doi          = {10.1177/30504554251331568},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {496-529},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Leveraging context-sensitivity for user-centered explainability},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conversational agent that learns to be aligned with the moral value of respect. <em>EAI</em>, <em>38</em>(4), 457-473. (<a href='https://doi.org/10.1177/30504554241311168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videogame developers typically conduct user experience surveys to gather feedback from users once they have played. Nevertheless, as users may not recall all the details once finished, we propose an ethical conversational agent that respectfully conducts the survey during gameplay. To achieve this without hindering user’s engagement, we resort to reinforcement learning and an ethical embedding algorithm. Specifically, we transform the learning environment so that it guarantees that the agent learns to be respectful (i.e. aligned with the moral value of respect) while pursuing its individual objective of eliciting as much feedback information as possible. When applying this approach to a simple videogame, our comparative tests between the two agents (ethical and unethical) empirically demonstrate that endowing a survey-oriented conversational agent with this moral value of respect avoids disturbing user’s engagement while still pursuing its individual objective, which is to gather as much information as possible.},
  archive      = {J_EAI},
  author       = {Eric Roselló-Marín and Inmaculada Rodríguez and Maite Lopez-Sanchez and Manel Rodríguez-Soto and Juan Antonio Rodríguez-Aguilar},
  doi          = {10.1177/30504554241311168},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {457-473},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A conversational agent that learns to be aligned with the moral value of respect},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
