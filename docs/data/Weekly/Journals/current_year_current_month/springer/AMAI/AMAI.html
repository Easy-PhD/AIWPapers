<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AMAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="amai">AMAI - 6</h2>
<ul>
<li><details>
<summary>
(2025). Automatic error function learning with interpretable compositional networks. <em>AMAI</em>, <em>93</em>(3), 441-469. (<a href='https://doi.org/10.1007/s10472-022-09829-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Constraint Programming, constraints are usually represented as predicates allowing or forbidding combinations of values. However, some algorithms can exploit a finer representation: error functions. By associating a function to each constraint type to evaluate the quality of an assignment, it extends the expressiveness of regular Constraint Satisfaction Problem/Constrained Optimization Problem formalisms. Their usage comes with a price though: it makes problem modeling significantly harder, since users must provide a set of error functions that are not always easy to define. Here, we propose a method to automatically learn an error function corresponding to a constraint, given its predicate version only. This is, to the best of our knowledge, the first attempt to automatically learn error functions for hard constraints. In this paper, we also give for the first time a formal definition of combinatorial problems with hard constraints represented by error functions. Our method aims to learn error functions in a supervised fashion, trying to reproduce either the Hamming or the Manhattan distance, by using a graph model we named Interpretable Compositional Networks. This model allows us to get interpretable results. We run experiments on 7 different constraints to show its versatility. Experiments show that our system can learn functions that scale to high dimensions, and can learn fairly good functions over incomplete spaces. We also show that learned error functions can be used efficiently to represent constraints in different classic problems.},
  archive      = {J_AMAI},
  author       = {Richoux, Florian and Baffier, Jean-François},
  doi          = {10.1007/s10472-022-09829-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {441-469},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Automatic error function learning with interpretable compositional networks},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven approach to neural architecture search initialization. <em>AMAI</em>, <em>93</em>(3), 413-440. (<a href='https://doi.org/10.1007/s10472-022-09823-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic design in neural architecture search (NAS) has received a lot of attention, aiming to improve performance and reduce computational cost. Despite the great advances made, few authors have proposed to tailor initialization techniques for NAS. However, the literature shows that a good initial set of solutions facilitates finding the optima. Therefore, in this study, we propose a data-driven technique to initialize a population-based NAS algorithm. First, we perform a calibrated clustering analysis of the search space, and second, we extract the centroids and use them to initialize a NAS algorithm. We benchmark our proposed approach against random and Latin hypercube sampling initialization using three population-based algorithms, namely a genetic algorithm, an evolutionary algorithm, and aging evolution, on CIFAR-10. More specifically, we use NAS-Bench-101 to leverage the availability of NAS benchmarks. The results show that compared to random and Latin hypercube sampling, the proposed initialization technique enables achieving significant long-term improvements for two of the search baselines, and sometimes in various search scenarios (various training budget). Besides, we also investigate how an initial population gathered on the tabular benchmark can be used for improving search on another dataset, the So2Sat LCZ-42. Our results show similar improvements on the target dataset, despite a limited training budget. Moreover, we analyse the distributions of solutions obtained and find that that the population provided by the data-driven initialization technique enables retrieving local optima (maxima) of high fitness and similar configurations.},
  archive      = {J_AMAI},
  author       = {Traoré, Kalifou René and Camero, Andrés and Zhu, Xiao Xiang},
  doi          = {10.1007/s10472-022-09823-0},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {413-440},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A data-driven approach to neural architecture search initialization},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chance constrained conic-segmentation support vector machine with uncertain data. <em>AMAI</em>, <em>93</em>(3), 389-411. (<a href='https://doi.org/10.1007/s10472-022-09822-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machines (SVM) is one of the well known supervised machine learning model. The standard SVM models are dealing with the situation where the exact values of the data points are known. This paper studies the SVM model when the data set contains uncertain or mislabelled data points. To ensure the small probability of misclassification for the uncertain data, a chance constrained conic-segmentation SVM model is proposed for multiclass classification. Based on the data set, a mixed integer programming formulation for the chance constrained conic-segmentation SVM is derived. Kernelization of chance constrained conic-segmentation SVM model is also exploited for nonlinear classification. The geometric interpretation is presented to show how the chance constrained conic-segmentation SVM works on uncertain data. Finally, experimental results are presented to demonstrate the effectiveness of the chance constrained conic-segmentation SVM for both artificial and real-world data.},
  archive      = {J_AMAI},
  author       = {Peng, Shen and Canessa, Gianpiero and Allen-Zhao, Zhihua},
  doi          = {10.1007/s10472-022-09822-1},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {389-411},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Chance constrained conic-segmentation support vector machine with uncertain data},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning of variable ordering heuristics for constraint optimisation problems. <em>AMAI</em>, <em>93</em>(3), 359-388. (<a href='https://doi.org/10.1007/s10472-022-09816-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solvers for constraint optimisation problems exploit variable and value ordering heuristics. Numerous expert-designed heuristics exist, while recent research learns novel, customised heuristics from past problem instances. This article addresses unseen problems for which no historical data is available. We propose one-shot learning of customised, problem instance-specific heuristics. To do so, we introduce the concept of deep heuristics, a data-driven approach to learn extended versions of a given variable ordering heuristic online. First, for a problem instance, an initial online probing phase collects data, from which a deep heuristic function is learned. The learned heuristics can look ahead arbitrarily-many levels in the search tree instead of a ‘shallow’ localised lookahead of classical heuristics. A restart-based search strategy allows for multiple learned models to be acquired and exploited in the solver’s optimisation. We demonstrate deep variable ordering heuristics based on the smallest, anti first-fail, and maximum regret heuristics. Results on instances from the MiniZinc benchmark suite show that deep heuristics solve 20% more problem instances while improving on overall runtime for the Open Stacks and Evilshop benchmark problems.},
  archive      = {J_AMAI},
  author       = {Doolaard, Floris and Yorke-Smith, Neil},
  doi          = {10.1007/s10472-022-09816-z},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {359-388},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Online learning of variable ordering heuristics for constraint optimisation problems},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from obstructions: An effective deep learning approach for minimum vertex cover. <em>AMAI</em>, <em>93</em>(3), 347-358. (<a href='https://doi.org/10.1007/s10472-022-09813-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intractability has for decades motivated the development of a plethora of methodologies that mainly aim at a quality-time trade-off. The use of Machine Learning has finally emerged as one of the possible tools to obtain approximate solutions to $$\mathcal {N}\mathcal {P}$$ -hard optimization problems. Recently, Dai et al. introduced a method for computing such approximate solutions for instances of the Vertex Cover problem. In this paper we consider the effectiveness of selecting a proper training strategy by considering special problem instances called obstructions that we believe carry some intrinsic properties of the problem. Capitalizing on the recent work of Dai et al. on Vertex Cover, and using the same case study as well as 19 other problem instances, we show the utility of using obstructions for training neural networks. Experiments show that training with obstructions results in a surprisingly huge reduction in number of iterations needed for convergence, thus gaining a substantial reduction in the time needed for training the model.},
  archive      = {J_AMAI},
  author       = {Abu-Khzam, Faisal N. and Abd El-Wahab, Mohamed M. and Haidous, Moussa and Yosri, Noureldin},
  doi          = {10.1007/s10472-022-09813-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {347-358},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Learning from obstructions: An effective deep learning approach for minimum vertex cover},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science meets optimization II. <em>AMAI</em>, <em>93</em>(3), 343-345. (<a href='https://doi.org/10.1007/s10472-025-09980-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Zhang, Yingqian and Guns, Tias and Lombardi, Michele and De Causmaecker, Patrick},
  doi          = {10.1007/s10472-025-09980-y},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {343-345},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Data science meets optimization II},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
