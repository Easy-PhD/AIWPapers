<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac">SAC - 29</h2>
<ul>
<li><details>
<summary>
(2025). Improving the prediction accuracy of statistical models: A new hierarchical clustering approach. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10683-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and machine learning practitioners frequently encounter datasets originated from multiple populations but containing the same type of measurements. In such cases, predictive analytics is typically carried out by either fitting a separate model to each dataset independently or by merging the datasets and fitting a single model to the combined data. These approaches overlook the potential existence of multiple groups of datasets associated with different underlying models, and, therefore, fail to exploit the inherent similarity between datasets to improve predictions. A third alternative is to perform pairwise comparisons between the populations before fitting the models. However, this is not always feasible, can become a very challenging task with complex models, and often does not rely on predictive accuracy. To address these issues, we propose a clustering approach designed to improve predictions in general databases. The method is based on a novel type of objective function that represents the total by-group prediction error. The clustering problem is solved using a hierarchical-type algorithm of agglomerative nature that automatically obtains the resulting clustering partition in a fully data-driven manner. An additional advantage of this procedure is that the number of clusters is treated as a variable in the minimization problem, allowing it to be determined naturally in a way that optimizes the predictive accuracy of the underlying models. Furthermore, the technique is versatile and can be used with any type of model for both regression, and classification tasks. Several simulation experiments and two real-world applications involving housing prices demonstrate that the procedure outperforms benchmark approaches in terms of predictive accuracy.},
  archive      = {J_SAC},
  author       = {López-Oriona, Ángel and Sun, Ying and Vilar, José A.},
  doi          = {10.1007/s11222-025-10683-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Improving the prediction accuracy of statistical models: A new hierarchical clustering approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10690-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are essential for advancing medical knowledge and improving health care, with Randomized Clinical Trials (RCTs) considered the gold standard for minimizing bias and generating reliable evidence on treatment efficacy and safety. Stepped-wedge individual RCTs, which randomize participants into sequences transitioning from control to intervention at staggered time points, are increasingly adopted. To improve their design, we propose an information-theoretic framework based on D– and A–optimality criteria for participant allocation to sequences. Our approach leverages semidefinite programming for automated computation and is applicable across a range of settings, varying in: (i) number of sequences, (ii) attrition rates, (iii) optimality criteria, (iv) error correlation structures, and (v) multi-objective designs using the $$\epsilon $$ -constraint method.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Moerbeek, Mirjam},
  doi          = {10.1007/s11222-025-10690-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast post-process bayesian inference with variational sparse bayesian quadrature. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10695-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (vsbq), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. vsbq reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that vsbq builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.},
  archive      = {J_SAC},
  author       = {Li, Chengkun and Clarté, Grégoire and Jørgensen, Martin and Acerbi, Luigi},
  doi          = {10.1007/s11222-025-10695-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Fast post-process bayesian inference with variational sparse bayesian quadrature},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10706-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many experimental designs, it is known a priori that the mean effects of the factors follow a monotone ordering. In this article, the problem of testing the homogeneity of the effects of both the factors against the alternative of their simultaneous monotone ordering is studied for a two-way crossed heteroscedastic ANOVA model. An intersection type test based on likelihood ratios of two sub-hypotheses and two multiple comparison tests are developed. Algorithms are proposed for the implementation of these tests using a parametric bootstrap approach and the asymptotic accuracy of this approach is also established. Extensive simulations are carried out to study the efficacy of these tests in controlling the type-I error rates and achieving a good power performance. It is shown that the proposed tests achieve the nominal size values regardless of the dimension of the design, number of replications in each cell and level of heterogeneity of error variances. Further, they are seen to have very good powers indicating consistency of tests. The proposed tests are further examined for their robustness under deviation from normality. An ‘R’ package is developed and shared on the open platform ‘GitHub’ for easy usage by practitioners. Finally, the applicability of our proposed test procedures is illustrated using two real data sets on mortality rates.},
  archive      = {J_SAC},
  author       = {Dey, Raju and Kumar, Somesh},
  doi          = {10.1007/s11222-025-10706-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-based fitting for graph signals. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10689-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of monitoring tools has led to an emerging demand for analyzing data residing on graphs, referred to as graph signals. In this study, we propose a quantile-based fitting method for graph signals, which can be applicable to graph signals with a wide range of distributions. Unlike traditional data fitting methods, such as smoothing splines or quantile smoothing splines in Euclidean space, the proposed method is designed for the graph domain, considering the inherent structure of graphs. In contrast to prevalent graph signal fitting methods that rely on optimization problems with $$L_2$$ -norm fidelity, the proposed method provides robust fits for graph signals in the presence of outliers. More importantly, it identifies various distributional structures of graph signals beyond the mean feature. We further investigate the theoretical properties of the proposed solution, including its existence and uniqueness. Through a comprehensive simulation study and real data analysis, we demonstrate the promising performance of the proposed method.},
  archive      = {J_SAC},
  author       = {Kim, Kyusoon and Oh, Hee-Seok},
  doi          = {10.1007/s11222-025-10689-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Quantile-based fitting for graph signals},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10696-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a finite-horizon optimal stopping problem the optimal stopping time is typically given by the first moment at which a sufficient statistic, namely a process containing all the relevant information on the problem, exceeds an unknown time-dependent boundary. This boundary often turns out to be the solution of a highly nonlinear integral equation involving the transition density of the sufficient statistic. When this density cannot be computed directly or easily, standard methods for solving the integral equation must be modified. This situation arises in sequential detection problems and in the pricing of certain derivative securities, where the corresponding sufficient statistics follow the so called Shiryaev process. In this context, we analyze and implement three distinct numerical methods for solving the integral equations characterizing the associated optimal stopping boundaries: two of them rely on solutions to partial differential equations, while the third is based on approximating the distribution of the sufficient statistic using a log-normal distribution. We demonstrate that these approaches return accurate results and are generally efficient.},
  archive      = {J_SAC},
  author       = {Buonaguidi, B.},
  doi          = {10.1007/s11222-025-10696-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10697-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing three-way data is challenging due to complex dependencies between observations, which must be accounted for to ensure reliable results. We focus on hierarchical, multivariate, binary data organized in a three-way data structure, where rows correspond to first-level units, columns to variables, and layers to second-level units within which the first-level units are nested. In this framework, model-based clustering methods can be effectively employed for dimensionality reduction purposes, facilitating a clear understanding of the phenomenon under investigation. In this work, we propose a novel modeling tool for a hierarchical clustering of first- and second-level units. We extend the Mixture of Latent Trait Analyzers (MLTA) with concomitant variables by letting prior component probabilities depend also on second-level-specific random effects. Parameter estimation is performed by means of a double EM algorithm based on a variational approximation of the model log-likelihood function, along with a nonparametric maximum likelihood estimation of the second-level-specific random effect distribution. This latter approach allows to estimate a discrete distribution which directly provides a clustering of second-level units. Within (conditional on) each of such clusters, first-level units are partitioned thanks to the MLTA specification. The proposal is applied to data from the European Social Survey to partition countries (second-level units) according to the baseline attitude of their residents (first-level units) toward digital technologies (variables). Within these clusters, residents are partitioned on the basis of their attitude toward specific digital skills. The influence of socio-economic factors on the identification of digitalization profiles is also taken into consideration via a concomitant variable approach.},
  archive      = {J_SAC},
  author       = {Failli, Dalila and Marino, Maria Francesca and Arpino, Bruno},
  doi          = {10.1007/s11222-025-10697-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact computation of angular halfspace depth. <em>SAC</em>, <em>35</em>(6), 1-29. (<a href='https://doi.org/10.1007/s11222-025-10700-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The angular halfspace depth ( $$ahD$$ ) was, already in 1987, the first depth function proposed for the nonparametric analysis of directional data. Mainly due to its presumed high computational cost and lack of efficient computational algorithms, it was never widely used in directional data analysis. We address the problem of the exact computation of $$ahD$$ in any dimension d. We proceed in two steps: (i) We express $$ahD$$ as a generalized (Euclidean) halfspace depth in dimension $$d-1$$ , using a projection approach. That allows us to develop fast exact computational algorithms for $$ahD$$ in dimensions $$d=1, 2, 3$$ . (ii) In spaces of dimension 3]]d 3 we design an inductive procedure that reduces the dimensionality d in the computation of $$ahD$$ , until the algorithms for $$d \le 3$$ can be used. Using our advances we develop a family of powerful algorithms for the computation of $$ahD$$ in any dimension d. Our procedures are implemented efficiently in C++ with an interface in R. A detailed analysis of the complexity of the novel algorithms is performed. Surprisingly, we show that computing $$ahD$$ of multiple points with respect to the same dataset is substantially faster than the same task for the classical (Euclidean) halfspace depth.},
  archive      = {J_SAC},
  author       = {Dyckerhoff, Rainer and Nagy, Stanislav},
  doi          = {10.1007/s11222-025-10700-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Exact computation of angular halfspace depth},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing mean independence with functional covariate. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10705-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new nonparametric conditional mean independence test for a scalar response and a functional covariate. The test statistics are built from continuous functionals over a residual marked empirical process indexed by a randomly projected functional covariate, which is less sensitive to tuning parameters and circumvents the curse of dimensionality. The asymptotic properties of the proposed test statistics under the null and the fixed alternative are established. We also show that our proposed test is able to detect a broad class of local alternatives converging to the null at the parametric rate. Due to the non-pivotal limiting null distribution, we use an easy-to-implement multiplier bootstrap procedure to estimate the critical values. Monte Carlo simulation studies demonstrate that our test outperforms other tests available in the literature due to its higher power and computational efficiency. The proposed test is further illustrated by analyzing the Tecator data set.},
  archive      = {J_SAC},
  author       = {Feng, Yongzhen and Li, Jie and Lu, Haokun and Song, Xiaojun},
  doi          = {10.1007/s11222-025-10705-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Testing mean independence with functional covariate},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random sampling of contingency tables and partitions: Two practical examples of the burnside process. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10708-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives new, efficient algorithms for approximate uniform sampling of contingency tables and integer partitions. The algorithms use the Burnside process, a general algorithm for sampling a uniform orbit of a finite group acting on a finite set. We show that a technique called ‘lumping’ can be used to derive efficient implementations of the Burnside process. For both contingency tables and partitions, the lumped processes have far lower per step complexity than the original Markov chains. We also define a second Markov chain for partitions called the reflected Burnside process. The reflected Burnside process maintains the computational advantages of the lumped process but empirically converges to the uniform distribution much more rapidly. By using the reflected Burnside process we can easily sample uniform partitions of size $$10^{10}$$ .},
  archive      = {J_SAC},
  author       = {Diaconis, Persi and Howes, Michael},
  doi          = {10.1007/s11222-025-10708-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Random sampling of contingency tables and partitions: Two practical examples of the burnside process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10709-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering has gained interest in gene expression data analysis due to its ability to identify groups of samples that exhibit similar behaviour in specific subsets of genes (or vice versa), in contrast to traditional clustering methods that classify samples based on all genes. Despite advances, biclustering remains a challenging problem, even with cutting-edge methodologies. This paper introduces an extension of the recently proposed Spike-and-Slab Lasso Biclustering (SSLB) algorithm, termed Outcome-Guided SSLB (OG-SSLB), aimed at enhancing the identification of biclusters in gene expression analysis. Our proposed approach integrates disease outcomes into the biclustering framework through Bayesian profile regression. By leveraging additional clinical information, OG-SSLB improves the interpretability and relevance of the resulting biclusters. Comprehensive simulations and numerical experiments demonstrate that OG-SSLB achieves superior performance, with improved accuracy in estimating the number of clusters and higher consensus scores compared to the original SSLB method. Furthermore, OG-SSLB effectively identifies meaningful patterns and associations between gene expression profiles and disease states. These promising results demonstrate the effectiveness of OG-SSLB in advancing biclustering techniques, providing a powerful tool for uncovering biologically relevant insights. The OGSSLB software can be found as an R/C++ package at https://github.com/luisvargasmieles/OGSSLB .},
  archive      = {J_SAC},
  author       = {Vargas-Mieles, Luis A. and Kirk, Paul D. W. and Wallace, Chris},
  doi          = {10.1007/s11222-025-10709-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed estimation and inference for high-dimensional confounded models. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10710-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden confounded model has been widely applied in many fields. Without adjusting for the hidden confounders, the estimators from the standard methods of high-dimensional models could be biased, potentially leading to spurious scientific discoveries. Meanwhile, distributed computation has attracted wide attention in modern statistical learning. Based on high-dimensional confounded models, this paper proposes a deconfounding and debiasing approach for distributed computing, aiming to obtain accurate estimation by reducing the confounding effect and bias. Two different distributed methods are applied: one is the straightforward divide-and-conquer (DC) method and the other is the communication-efficient surrogate likelihood (CSL) method. The former is easy to use in practice, while the latter uses surrogate loss to achieve better performance than the former through multiple iterations. The estimation accuracy and asymptotic theories for both DC and CSL estimators are established. Extensive simulation experiments verify the good performance of the two estimators and two real data applications are also presented to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Fei, Yuxin and Ma, Wei and Wang, Lei},
  doi          = {10.1007/s11222-025-10710-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributed estimation and inference for high-dimensional confounded models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive tree ensembles for composite quantile regressions. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10711-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach that integrates Bayesian additive regression trees (BART) with the composite quantile regression (CQR) framework, creating a robust method for modeling complex relationships between predictors and outcomes under various error distributions. Unlike traditional quantile regression, which focuses on specific quantile levels, our proposed method, composite quantile BART, offers greater flexibility in capturing the entire conditional distribution of the response variable. By leveraging the strengths of BART and CQR, the proposed method provides enhanced predictive performance, especially in the presence of heavy-tailed errors and non-linear covariate effects. Numerical studies confirm that the proposed composite quantile BART method generally outperforms classical BART, quantile BART, and composite quantile linear regression models in terms of RMSE, especially under heavy-tailed or contaminated error distributions. Notably, under contaminated normal errors, it reduces RMSE by approximately 17% compared to composite quantile regression, and by 27% compared to classical BART.},
  archive      = {J_SAC},
  author       = {Lim, Yaeji and Lu, Ruijin and Ville, Madeleine St. and Chen, Zhen},
  doi          = {10.1007/s11222-025-10711-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive tree ensembles for composite quantile regressions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation based bayesian optimization. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10715-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimization over categorical or mixed covariate spaces, GPs may not be ideal. This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires sampling-based access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO. We demonstrate empirically the effectiveness of SBBO using various choices of surrogate models in applications involving combinatorial optimization.},
  archive      = {J_SAC},
  author       = {Naveiro, Roi and Tang, Becky},
  doi          = {10.1007/s11222-025-10715-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based bayesian optimization},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10716-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering profiles of longitudinal data is a prevalent method employed for analyzing heterogeneity patterns among individuals, aiming to identify clusters based on diverse patterns of the mean progression trajectories. The correlation structure plays a crucial role in longitudinal data analysis, as accurate modeling of this structure enhances the estimation efficiency of mean trajectories. In this paper, we assume that subjects are sampled from a Gaussian mixture distribution, and incorporate regularized bandable precision matrix structure information for each subgroup. In order to identify the latent group structure, we employ concave penalty functions to estimate the pairwise differences of the model parameters derived from finite mixture models. The model parameters and cluster labels are estimated simultaneously using the Expectation-Maximization (EM) algorithm in conjunction with the Alternating Direction Method of Multipliers (ADMM) algorithm. We establish computational convergence and provide a statistical guarantee through demonstrating the asymptotic rate. Numerical studies and real data analysis show improved clustering results and excellent accuracy performance.},
  archive      = {J_SAC},
  author       = {Liang, Chunhui and Ma, Wenqing},
  doi          = {10.1007/s11222-025-10716-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor train approximation of multivariate gaussian density by scaling and squaring. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10707-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor train decomposition is a promising tool for dealing with high dimensional arrays. Point mass filters utilise such arrays for representing probability density functions of the state. Proofs of concept of the application of the low rank decomposition have been provided in the literature. However, the application requires to design parameters, such as tensor train ranks. Since the parameters dictating the computational requirements are derived from the data according to more abstract hyper-parameters such as precision, an analysis of benchmark examples is needed for allocating resources. This paper studies the ranks in the case of Gaussian densities. The influence of correlation and the effect of rounding are discussed first. Efficiency of the density representation used by standard point mass filters is considered next. Aspects of series expansion of the Gaussian density evaluated over array are considered for the tensor train format. The growth of the ranks is illustrated on a four-dimensional example. An observation of the growth for a multi-dimensional case is made last. The lessons learned are valuable for designing efficient point mass filters. Namely, they show that at least the naive implementations of tensor decomposition methods do not break the curse of dimensionality.},
  archive      = {J_SAC},
  author       = {Ajgl, Jiří and Straka, Ondřej},
  doi          = {10.1007/s11222-025-10707-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Tensor train approximation of multivariate gaussian density by scaling and squaring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new resampling method for meta gaussian distributions. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10717-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta Gaussian distributions, also known as multivariate Gaussian copula models, are a type of statistical distribution that is particularly useful in modeling dependencies among variables. The key advantage of meta Gaussian distributions is their flexibility - they can capture a wide range of dependency structures, making them a powerful tool for statistical modeling. Discrete approximations of continuous multivariate distributions, such as meta Gaussian distributions, which are of significant importance, are widely utilized across numerous disciplines. This paper introduces a new resampling method based on mean square error representative points (MSE-RPs) to construct accurate approximations for meta Gaussian distributions, thereby enhancing precision in statistical analysis. We carry out a systematic examination of the structural patterns and characteristics of MSE-RPs of these distributions. From a theoretical perspective, we analyze the invariance properties in copula-based association measures by leveraging group theory. This allows us to identify more stable invariants that are suitable for complex dependency structures. Through a simulation study, we demonstrate that MSE-RPs achieve significantly higher estimation accuracy for mean vectors and correlation matrices compared to Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods. Furthermore, MSE-RPs offer faster computation relative to the QMC method based on generalized good lattice points (GGLP) sets. Finally, we illustrate the practical advantages of our approach through empirical analysis on real-world datasets.},
  archive      = {J_SAC},
  author       = {He, Pingan and Fang, Kai-Tai and He, Ping and Ye, Huajun},
  doi          = {10.1007/s11222-025-10717-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A new resampling method for meta gaussian distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive subgroup logistic regression for classification with unobserved heterogeneity. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10712-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved heterogeneity refers to the variation among subjects that is not accounted for by the observed features used in a model. Its presence poses a substantial challenge to statistical modeling. This study introduces the Predictive Subgroup Logistic Regression (PSLR) model, which extends the conventional logistic regression and is specifically designed to address unobserved heterogeneity in classification problems. The PSLR model incorporates subject-specific intercepts in the log odds, fitted through a penalized likelihood approach with a concave pairwise fusion penalty. A novel two-step procedure is developed to facilitate the out-of-sample predictions for new subjects whose subgroup membership labels are unknown. This procedure allows the PSLR model to perform both inferential and predictive tasks. Through extensive simulation studies and an empirical application to a customer churn dataset in the telecommunications industry, the PSLR model not only demonstrates great performance in various aggregate accuracy metrics but also achieves a balanced effectiveness in sensitivity and specificity.},
  archive      = {J_SAC},
  author       = {Chen, Kun and Huang, Rui and Tong, Zhiwei},
  doi          = {10.1007/s11222-025-10712-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Predictive subgroup logistic regression for classification with unobserved heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient learning of symmetric positive-definite matrix regression. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10714-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data are nowadays frequently encountered due to the advance in data-collection techniques. Under the Tikhonov regularization framework, this paper focuses on the gradient learning in a regression setting where the response is a symmetric positive-definite (SPD) matrix and the predictor is a Euclidean vector. We endow the SPD manifold with the Log-Euclidean metric to transform our model on the manifold to the Euclidean space and calculate the gradients by solving a linear system under the assumption that the gradient function resides in a reproducing kernel Hilbert space. We further simplify our algorithm and reduce the dimension of the linear system by singular value decomposition. Theoretical properties about the approximation error of the reducing-matrix-size algorithm and the error bound of gradient estimation are investigated as well. In numerical experiments, we show the validity of our SPD gradient learning algorithm in variable selection and sufficient dimension reduction. A real-world dataset about New York taxi networks is studied to illustrate the applicability of our algorithm.},
  archive      = {J_SAC},
  author       = {Chen, Baiyu and Fu, Xiaoyi and Li, Yunchen and Wang, Xiaozhou and Yu, Zhou},
  doi          = {10.1007/s11222-025-10714-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Gradient learning of symmetric positive-definite matrix regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10721-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a breast cancer study, we consider the analysis of interval-censored failure time data in the presence of a cure fraction. Although a great deal of literature has been established for the analysis of interval-censored data with cure fractions, there is no established method that adequately handles the limited sample size issue. Corresponding to this, we propose a transfer learning approach under the proportional hazards mixture cure models for interval-censored data with the aim to transfer the information from the informative auxiliary samples in a larger cohort to improve the performance of the target regression analysis. To assess the proposed method, an extensive simulation study is performed and suggests that it works well in practical situations. Furthermore, we apply the method to the breast cancer study with the focus on Black women by leveraging the data of other racial women and obtain the improved results.},
  archive      = {J_SAC},
  author       = {Lou, Yichen and Sun, Jianguo and Wang, Peijie and Zhao, Shishun},
  doi          = {10.1007/s11222-025-10721-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust mean-shift clustering based on impartial trimming. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10718-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust version of the mean-shift algorithm is developed to cope with the presence of contaminated data when targeting a clustering problem by means of a modal approach. The goal is to protect nonparametric density-based clustering from the deleterious effect of outliers. Their occurrence affects the analysis mainly because outliers lead to the detection of spurious modes and groups. Therefore, the proposed methodology aims to recover the underlying clustered data configuration, while detecting and discarding outliers. A strategy to select the level of trimming is discussed. The finite sample behaviour of the proposed method is investigated by Monte Carlo numerical studies and empirical applications.},
  archive      = {J_SAC},
  author       = {Greco, Luca and Menardi, Giovanna},
  doi          = {10.1007/s11222-025-10718-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Robust mean-shift clustering based on impartial trimming},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-network assisted clustering using a grouped factor model. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10719-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the clustering task of large-scale panel data with assistance from multi-network information under the grouped factor model. In many real-world clustering tasks, multiple networks can be observed for the same set of cross-sectional units based on different types of interactions. Different networks are different portraits of latent group memberships, which inspired us to utilize multi-network information to improve the clustering accuracy and stability. Therefore, we propose a multi-network-assisted clustering method that encourages coherence between the clustering results and the weighted multi-network in a penalized manner. We also developed a flexible weight learning strategy to identify the clustering capacity differences of multiple networks. A computationally efficient algorithm with random initialization was developed to implement penalized estimation. Thorough simulation studies demonstrate that the proposed method is more promising than existing competitors, even with misleading network information. Finally, application to the daily returns of stocks traded on the Shanghai and Shenzhen stock exchanges demonstrates the effectiveness and efficiency of the new method. Supplementary materials for this article are available online.},
  archive      = {J_SAC},
  author       = {Liang, Wanwan and Wu, Ben and Fan, Xinyan and Zhang, Bo},
  doi          = {10.1007/s11222-025-10719-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Multi-network assisted clustering using a grouped factor model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating quantile regression for multi-source subgroup identification. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10713-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity and the multi-source nature of modern datasets present significant challenges for statistical methodologies. Motivated by the Stimulant Reduction Intervention using Dosed Exercise (STRIDE) study, we analyze primary outcomes from two complementary data sources to assess heterogeneity in treatment effects between experimental and control groups at medium and high quantiles. To address these challenges, we propose a novel approach that integrates quantile regression with weighted quantile loss and joint pairwise fusion penalties, enabling joint subgroup identification across data sources. Our method distinguishes between homogeneous and heterogeneous effects using a center regularization term and can detect sources lacking group structures. Theoretically, we establish weak Oracle properties, ensuring consistent estimation of group structures. Computationally, we employ the alternating direction method of multipliers (ADMM) and mitigate the burden of pairwise fusion through a k-nearest neighbors trimming method. The effectiveness of our approach is demonstrated through numerical simulations and an application to the STRIDE study.},
  archive      = {J_SAC},
  author       = {Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.1007/s11222-025-10713-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Integrating quantile regression for multi-source subgroup identification},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data. <em>SAC</em>, <em>35</em>(6), 1-31. (<a href='https://doi.org/10.1007/s11222-025-10720-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture of multivariate Poisson-log normal factor analyzers is introduced by imposing constraints on the covariance matrix, which results in flexible models for clustering purposes. In particular, a class of eight parsimonious mixture models based on the mixtures of factor analyzers is introduced. The variational Gaussian approximation is used for parameter estimation, and information criteria are used for model selection. The proposed models are explored in the context of clustering discrete data arising from RNA sequencing studies. Using real and simulated data, the models are shown to give favourable clustering performance. The GitHub R package for this work is available at https://github.com/anjalisilva/mixMPLNFA and is released under the open-source MIT license.},
  archive      = {J_SAC},
  author       = {Payne, Andrea and Silva, Anjali and Rothstein, Steven J and McNicholas, Paul D. and Subedi, Sanjeena},
  doi          = {10.1007/s11222-025-10720-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized spectral clustering for large-scale multi-layer networks. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10723-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-layer networks with large numbers of nodes, edges, and layers arise across various domains, which poses a great computational challenge for downstream analysis. In this paper, we develop an efficient randomized spectral clustering algorithm for community detection in multi-layer networks. We first utilize the random sampling strategy to sparsify the adjacency matrix of each layer. Then we use the random projection strategy to accelerate the eigen-decomposition of the sum of squared sparsified adjacency matrices of all layers. The communities are finally obtained via the k-means of the eigenvectors. The algorithm not only has low time complexity but also saves storage space. Theoretically, we study the misclassification error rate of the proposed algorithm under the multi-layer stochastic block model, which shows that the randomization does not deteriorate the error bound under certain conditions. Numerical studies on multi-layer networks with millions of nodes show the superior efficiency of the proposed algorithm, which achieves clustering results rapidly. We develop a new R package, MLRclust, which makes the proposed methods available for both simulated and real multi-layer networks.},
  archive      = {J_SAC},
  author       = {Su, Wenqing and Guo, Xiao and Chang, Xiangyu and Yang, Ying},
  doi          = {10.1007/s11222-025-10723-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Randomized spectral clustering for large-scale multi-layer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quantile function estimation with beta-kernel smoothing. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10725-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a class of nonparametric quantile function estimators based on Beta kernel smoothing. We conduct a rigorous investigation into their large-sample properties, including asymptotic normality and mean squared equivalence to existing methods. Through extensive simulation studies, we demonstrate that the proposed Beta kernel estimators perform comparably to or outperform traditional empirical and symmetric location-scale kernel-based quantile estimators. Additionally, we provide two real-world applications to illustrate the practical effectiveness of our approach. The results suggest that Beta kernel smoothing offers a flexible and efficient alternative for quantile function estimation, particularly in cases where classical methods exhibit inefficiencies.},
  archive      = {J_SAC},
  author       = {Li, Juan and Yu, Ping and Shi, Jianhong and Song, Weixing},
  doi          = {10.1007/s11222-025-10725-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing quantile function estimation with beta-kernel smoothing},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive weighted composite quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10726-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a nonparametric Bayesian estimation and model selection method for additive models based on weighted composite quantile regression. This method identifies the unknown smooth functions of the additive model as linear, nonlinear, or zero components by setting a multiplicative parameterized spike-slab prior distribution, which solves the problem of selecting predictive variable components in partially linear additive models when prior information is insufficient. In addition, it further generalizes the composite quantile regression to additive models by combining information from multiple quantiles. A Bayesian hierarchical model is established based on the mixed representation of the asymmetric Laplace distribution, and the posterior distributions of all unknown parameters are sampled by Markov chain Monte Carlo (MCMC). Finally, in the simulation and real data analysis, the variable selection results, root mean square error and other indicators are used to further prove that the method is more competitive than the existing methods.},
  archive      = {J_SAC},
  author       = {Ji, Yonggang and Wang, Mian and Zhou, Maoyuan},
  doi          = {10.1007/s11222-025-10726-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive weighted composite quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive stratified monte carlo using decision trees. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10731-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been known for a long time that stratification is one possible strategy to obtain higher convergence rates for the Monte Carlo estimation of integrals over the hypercube $$[0, 1]^s$$ of dimension s. However, stratified estimators such as Haber’s are not practical as s grows, as they require $$\mathcal {O}(k^s)$$ evaluations for some $$k\ge 2$$ . We propose an adaptive stratification strategy, where the strata are derived from a decision tree applied to a preliminary sample. We show that this strategy leads to higher convergence rates, that is the corresponding estimators converge at rate $$\mathcal {O}(N^{-1/2-r})$$ for some $$r>0$$ for certain classes of functions. Empirically, we show through numerical experiments that the method may improve on standard Monte Carlo even when s is large.},
  archive      = {J_SAC},
  author       = {Chopin, Nicolas and Wang, Hejin and Gerber, Mathieu},
  doi          = {10.1007/s11222-025-10731-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive stratified monte carlo using decision trees},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid variational bayesian approach for spatial random effects structural equation modeling. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10730-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, structural equation modeling (SEM) has been widely applied in fields such as education, psychology, and environmental science. However, most studies overlook the spatial dependencies within the data, and there is limited research on SEM for spatial data. Spatial random effects (SRE) models, which flexibly capture spatial variation through a series of spatial basis functions (e.g., multiresolution wavelet basis functions), have become a powerful tool for spatial data analysis. This study extends the traditional SEM by incorporating SRE, resulting in a spatial random effects structural equation model (SRE-SEM) for modeling complex environmental spatial data. The model is fitted using a hybrid variational Bayes algorithm, with some model parameters estimated via the standard mean-field variational Bayes approach. To address the intractable posterior and the dimensionality of latent variables, the Metropolis-Hastings algorithm is employed to sample from the exact conditional posterior distribution of the latent variables. Additionally, a fixed-form variational Bayes approach is used to estimate the matrix on which the spatial covariance matrix depends. Simulation studies and case analyses demonstrate that the proposed model effectively captures the structure of spatial data, while the introduced estimation algorithms significantly improve computational efficiency. This study provides a robust and efficient framework for spatial data analysis, offering a promising solution for modeling complex environmental and socio-economic phenomena.},
  archive      = {J_SAC},
  author       = {Wu, Ying and Zhu, Hongyu and Zhang, Jiwei and Lu, Jing},
  doi          = {10.1007/s11222-025-10730-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A hybrid variational bayesian approach for spatial random effects structural equation modeling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
