<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva">MVA - 6</h2>
<ul>
<li><details>
<summary>
(2025). ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation. <em>MVA</em>, <em>36</em>(6), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01737-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based Blood Volume Pulse (BVP) signal extraction technology has demonstrated significant potential in remote health monitoring. However, most current methods are susceptible to interference from lighting changes and have limited generalization ability in dynamic or complex environments. This paper proposes a dual-channel path data concatenation network called ConcatPhys to improve the accuracy and robustness of remote heart rate (HR) estimation. First, a region-focus (RF) block is introduced to concentrate on spatial attention mechanisms, focusing on physiologically relevant regions. This approach effectively uncovers subtle local feature changes and suppresses irrelevant features, reducing sensitivity to background noise and lighting variations. Second, a dual-path framework is constructed for remote photoplethysmography (rPPG) signal prediction. By incorporating dual-path frequency-domain consistency loss and adjacent-frame similarity loss, the network’s anti-interference capability against illumination variations such as lighting changes is strengthened. Finally, leveraging the temporal correlation between adjacent video frames over short intervals, three consecutive feature image segments are concatenated. By averaging the HR values of these three adjacent segments, the video-level HR is computed. This approach enables efficient reconstruction of rPPG signals and accurate HR estimation using only a 6-s facial video segment. Experimental results demonstrate that ConcatPhys achieves state-of-the-art performance across multiple public datasets (VIPL-HR, OBF, UBFC-rPPG), highlighting its significant potential for remote health monitoring applications.},
  archive      = {J_MVA},
  author       = {Ge, Xiaorui and Xing, Jiahe and Li, Bin},
  doi          = {10.1007/s00138-025-01737-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports. <em>MVA</em>, <em>36</em>(6), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01733-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research work, a Deep Learning (DL) approach utilizing Spatial Transformer, Temporal Transformer, and Collaborative Movement Centric (CMC) module is presented to classify sports event based on athlete movements in competitive sports. Primarily, the Spatial Transformer utilizing the Spatial Feature extraction module (SFEM) is introduced to extract detailed spatial information from video frames. Here, the SFEM employs Modulated Moving Average Graph Convolutional Network (MMA-GCN) to extract complex spatial relationships by learning the knowledge from offset and modulation parameters. Secondly, the Temporal Transformer is designed that employs Temporal Feature extraction module (TFEM) module to extract long-range temporal dependencies and model evolution across consecutive frames. Lastly, the CMC module combines spatial and temporal information into a unified representation which is used to perform sports events categorization based on athlete movements. The proposed model is evaluated on Olympic sports dataset and University of Central Florida’s (UCF) Sports dataset across distinct evaluation measures. The model achieved 98.36% accuracy, 99.42% precision, 98.42% recall, 98.91% F1-score on the Olympic Sports dataset and 98.64% accuracy, 98.45% precision, 98.91% recall, and 98.68% F1-score on the UCF Sports dataset. Moreover, this method showed supreme results compared with existing techniques in all metrics, demonstrating its effectiveness and potential for high- applications in sports analytics and athlete monitoring.},
  archive      = {J_MVA},
  author       = {Gao, Yilun and Zou, Jie and Zhang, Yuexin},
  doi          = {10.1007/s00138-025-01733-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained 3D vehicle shape manipulation via latent space editing. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01739-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant potential of 3D object editing to impact various industries, recent research in 3D generation and editing has primarily focused on converting text and images into 3D models, often paying limited attention to the need for fine-grained control over existing 3D objects. This paper introduces a framework that uses a pre-trained regressor to enable continuous and attribute-specific modifications of both the stylistic and geometric attributes of 3D vehicle models. Here, “fine-grained control” refers to the ability to adjust specific geometric or stylistic attributes (such as roof length or perceived luxury) in a continuous and independent manner. Our method aims to preserve the identity of vehicle 3D objects and support multi-attribute editing, allowing for extensive customization while maintaining the model’s structural integrity. The framework leverages DeepSDF to obtain latent representations suitable for continuous attribute editing. Experimental results demonstrate the effectiveness of our approach in achieving detailed, controlled edits on a variety of vehicle 3D models. The code is released at https://github.com/JiangDong-miao/Vehicle_LatentEdit .},
  archive      = {J_MVA},
  author       = {Miao, JiangDong and Ikeda, Tatsuya and Raytchev, Bisser and Mizoguchi, Ryota and Hiraoka, Takenori and Nakashima, Takuji and Shimizu, Keigo and Higaki, Toru and Kaneda, Kazufumi},
  doi          = {10.1007/s00138-025-01739-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fine-grained 3D vehicle shape manipulation via latent space editing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01741-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from magnetic resonance imaging (MRI) remains a challenging task due to the inherent heterogeneity of tumor morphology, class imbalance within datasets, and the limitations of individual deep learning models. To address these challenges, we propose ED-ViTTL (Ensembled Deep Vision Transformer and Transfer Learning), a hybrid framework that leverages both local and global feature representations to enhance diagnostic performance. The model integrates five advanced variants of the Vision Transformer (R50-ViT-L/16, ViT-L/16, ViT-L/32, ViT-B/16, and ViT-B/32) alongside a transfer-learned VGG19 convolutional neural network. Feature embeddings extracted from the ViT and CNN branches are fused through fully connected layers, enabling robust classification into four categories: glioma, meningioma, pituitary tumor, and healthy brain. Experiments were conducted on a publicly available dataset comprising 3264 MRI scans, partitioned into training (70%), validation (15%), and testing (15%) sets using stratified sampling. To mitigate class imbalance and improve model generalization, we employed stratified 5-fold cross-validation, class-weighted categorical cross-entropy, and extensive data augmentation. The best-performing ensemble configuration (ViT-B/32 + VGG19) achieved a classification accuracy of 98.67%, with class-specific AUC values exceeding 0.99 and ROC curves demonstrating clear inter-class separability. Performance metrics, including precision, recall, and F1-scores, remained consistently high across folds, with the confusion matrix indicating minimal misclassifications. These findings demonstrate that ED-ViTTL delivers stable and reproducible results, underscoring its potential as a reliable computer-aided diagnostic tool for assessing brain tumors in clinical practice.},
  archive      = {J_MVA},
  author       = {Thakur, Amit and Patnaik, Pawan Kumar and Kumar, Manoj and Choudhary, Chaitali},
  doi          = {10.1007/s00138-025-01741-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editing implicit and explicit representations of radiance fields: A survey. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01742-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent years by offering a new volumetric representation, which is compact and provides high-quality image rendering. However, the methods to edit those radiance fields developed slower than the many improvements to other aspects of NeRF. With the recent development of alternative radiance field-based representations inspired by NeRF as well as the worldwide rise in popularity of text-to-image models, many new opportunities and strategies have emerged to provide radiance field editing. In this paper, we deliver a comprehensive survey of the different editing methods present in the literature for NeRF and other similar radiance field representations. We propose a new taxonomy for classifying existing works based on their editing methodologies, review pioneering models, reflect on current and potential new applications of radiance field editing, and compare state-of-the-art approaches in terms of editing options and performance.},
  archive      = {J_MVA},
  author       = {Hubert, Arthur and Elghazaly, Gamal and Frank, Raphaël},
  doi          = {10.1007/s00138-025-01742-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editing implicit and explicit representations of radiance fields: A survey},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems. <em>MVA</em>, <em>36</em>(6), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01744-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lane segmentation is essential for effective path planning and lane following in autonomous driving, especially in scenarios with significant occlusion from vehicles and pedestrians. Existing models often struggle under such conditions, leading to unreliable navigation and safety risks. We propose two innovative approaches to enhance lane detection in these challenging environments, each showing notable improvements over conventional methods. The first approach aug-Segment improves conventional lane detection models by augmenting the training dataset (e.g. CULanes) with simulated occlusions and training a segmentation model. This method achieves a 12% improvement over a number of SOTA models on the CULanes dataset. Additionally, we present a second approach, LOID: Lane Occlusion Inpainting and Detection, designed to address the issue more comprehensively and with improved robustness. LOID introduces an advanced lane segmentation network that uses an image processing pipeline to identify and mask occluded regions. An inpainting model is then applied to reconstruct the road environment in the occluded areas. The enhanced image is then processed by a lane detection algorithm, resulting in a 20% and 24% improvement over several SOTA models on the BDDK100 and CULanes datasets respectively, highlighting the effectiveness of the proposed approach.},
  archive      = {J_MVA},
  author       = {Agrawal, Aayush and Sivakumar, Ashmitha Jaysi and Kaif, Ibrahim and Banerjee, Chayan},
  doi          = {10.1007/s00138-025-01744-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
