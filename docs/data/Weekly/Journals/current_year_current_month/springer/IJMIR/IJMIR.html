<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir">IJMIR - 9</h2>
<ul>
<li><details>
<summary>
(2025). MCDINO: Self-supervised learning of masks based on combination of multi-path channel attention and local feature weighting. <em>IJMIR</em>, <em>14</em>(3), 1-13. (<a href='https://doi.org/10.1007/s13735-025-00371-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViT) have made significant progress in computer vision tasks, especially after the introduction of masking operations, which have demonstrated stronger performance in areas such as image classification and detection. However, in self-supervised learning, masking tends to weaken the correlation of local features of an image, which affects the effectiveness of downstream tasks. To address this problem, this paper proposes a new model method, MCDINO, which extends the capability of image mask operation based on the DINO model (based on improved denoising anchor frame DETR). MCDINO introduces a multipath channel attention mechanism to dynamically compute the importance weight of the feature region to capture the key information among local features effectively; meanwhile, it designs a local feature weighted fusion operation in the Block to improve the fineness of local feature expression and correlation modeling capability. Experiments demonstrate that MCDINO significantly outperforms DINO in the ImageNet100 fine-tuning task, and also achieves better performance in downstream tasks such as copy detection. The results show that MCDINO can effectively enhance the feature learning and representation ability under masking conditions and drive the performance of visual tasks. The code has been open-sourced at: https://github.com/wangzy2024/MCDINO},
  archive      = {J_IJMIR},
  author       = {Shao, Yunxue and Wang, Zhiyang and Wang, Lingfeng},
  doi          = {10.1007/s13735-025-00371-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MCDINO: Self-supervised learning of masks based on combination of multi-path channel attention and local feature weighting},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic fusion and contrastive generation for generalized zero-shot learning. <em>IJMIR</em>, <em>14</em>(3), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00372-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to leverage a classifier trained on seen classes to categorize instances from both seen and unseen classes. Several approaches have been introduced to synthesize visual features that simulate those of unseen classes for training classifiers. However, existing methods only emphasize the distributional relationships between synthesized and real features, while neglecting the inter-class relationships among the synthesized features. Consequently, synthesized visual features exhibit significant loose intra-class distributions and numerous outliers. Furthermore, the generator trained solely on seen classes tend to overfit these classes. In this paper, a Semantic Fusion and Contrastive Generation (SFCG) framework is proposed for GZSL. Specifically, a visual-semantic contrastive generation method and a visual features similarity loss are explored to address the challenges of loose intra-class distribution and outliers in synthesized visual features. Moreover, semantic attributes are fused to create novel and diverse semantic instances for training a balanced generator. The SFCG model is evaluated on four widely-used ZSL benchmark datasets: CUB, FLO, AWA2, and SUN. It achieves harmonic mean accuracies of 68.4% on CUB, 71.3% on FLO, 73.4% on AWA2, and 45.2% on SUN, demonstrating the efficacy of the proposed method.},
  archive      = {J_IJMIR},
  author       = {Yang, Guan and Sun, Weihao and Liu, Xiaoming and Liu, Yang and Wang, Chen},
  doi          = {10.1007/s13735-025-00372-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Semantic fusion and contrastive generation for generalized zero-shot learning},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv10 for small object detection with context-aware and adaptive modules. <em>IJMIR</em>, <em>14</em>(3), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00373-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection poses substantial challenges due to limited pixel count and sparse features, yet it holds immense significance in applications like autonomous driving and unmanned aerial vehicles. This paper introduces an algorithm derived from YOLOv10, significantly enhancing the accuracy of detecting small objects. We have devised a novel module, the Context-aware and Enhanced Capture Module (C2A), which addresses the intricacies of small object detection by skillfully integrating multi-scale features and contextual information to bolster recognition and capture capabilities. Additionally, we incorporate the Receptive Field Attention Convolution (RFAConv) mechanism, leveraging attention weights to precisely evaluate the significance of each receptive field positionâ€™s information, facilitating the extraction of crucial small object features. Furthermore, we introduce Adaptive Convolution Kernel (AKConv) technology, which dynamically adjusts the convolution kernel to effectively handle small objects of varying sizes and shapes, thereby enhancing detection performance. Evaluations on the VisDrone2021 dataset reveal that our algorithm achieves AP50 and mAP scores of 29.3 and 16.4. This substantial performance uplift underscores the efficacy of our proposed algorithm in small object detection tasks.},
  archive      = {J_IJMIR},
  author       = {Wang, Jian and Su, Jia and Wen, Zonghui and Sun, Yongqing},
  doi          = {10.1007/s13735-025-00373-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Enhanced YOLOv10 for small object detection with context-aware and adaptive modules},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPE-YOLO: Improved low-light object detection using a two-way pyramid enhancement network. <em>IJMIR</em>, <em>14</em>(3), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00374-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-light conditions, image details will be severely lost due to the lack of illumination and noise interference, resulting in unsatisfactory performance based on existing object detection methods. To address this issue, we propose a Two-Way Pyramid Enhancement Network (TPE-NET) and cascade it with YOLOv3 to construct an end-to-end low-light object detection framework, termed TPE-YOLO. First, TPE-NET uses Laplacian pyramid to divide low-light images into Gaussian low-frequency components and Laplacian high-frequency components of different scales. Specifically, for Gaussian low-frequency components, we design Low-Frequency Detail Enhancement Module (LDEM) to filter out image noise and enhance the semantic and texture information of the image. For Laplacian high-frequency components, High-Frequency Edge Enhancement Module (HEEM) is proposed to capture the edge and global feature information of the image. Secondly, Context Aggregation Module (CAM) is designed to mine and aggregate multi-scale contextual semantics through parallel dilated convolutions to further enhance the feature representation of high- and low-frequency information components. In addition, Color Extraction Module (CEM) and color consistency loss are designed to supplement color information for restored images and reduce image color distortion, thereby making object features more discriminative. We evaluate our proposed method on the low-light object detection benchmark dataset ExDark. Experimental results show that TPE-YOLO performs well under various low-light conditions, achieving 78.4% mAP, which is better than the existing low-light object detection methods. Code and weights are available at https://github.com/zzxf123/TPE-YOLO .},
  archive      = {J_IJMIR},
  author       = {Zhang, Xiaofei and Di, Xiaoguang and Zhu, Runwen},
  doi          = {10.1007/s13735-025-00374-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {TPE-YOLO: Improved low-light object detection using a two-way pyramid enhancement network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remote sensing image change captioning: A comprehensive review. <em>IJMIR</em>, <em>14</em>(3), 1-20. (<a href='https://doi.org/10.1007/s13735-025-00375-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing change detection automatically identifies temporal changes in specific geographic areas by analyzing multi-temporal remote sensing images, which locates altered regions but struggles to interpret the underlying semantic implications of these changes. The emergence of Remote Sensing Image Change Captioning (RSICC) has opened new avenues for change interpretation, aiming to understand semantic variations between bi-temporal remote sensing images and express them through natural language. By integrating vision with language, RSICC provides higher-level scene understanding for applications such as environmental monitoring, disaster response, and urban planning. In recent years, despite the continuous emergence of various RSICC datasets and methodologies, this field still lacks systematic review studies. We first categorize and discuss datasets and evaluation metrics. Then, we propose a novel typology that presents the developmental process, similarities and differences, model architecture, and performance comparison of existing RSICC approaches. Furthermore, we discuss the main algorithmic contributions and future research directions based on technical challenges and application requirements. This study fills a critical gap in this emerging field and provides valuable reference for researchers in related disciplines.},
  archive      = {J_IJMIR},
  author       = {Zou, Shiwei and Wei, Yingmei and Xie, Yuxiang and Lao, Mingrui and Luan, Xidao},
  doi          = {10.1007/s13735-025-00375-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Remote sensing image change captioning: A comprehensive review},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted semantic feature based self-supervised deep cross-modal hashing. <em>IJMIR</em>, <em>14</em>(3), 1-22. (<a href='https://doi.org/10.1007/s13735-025-00378-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fast respond the large-scale cross-modal retrieval task, the deep cross-modal hashing algorithms map different modalities into the low-dimensional Hamming space and measure their similarity degree using Hamming distance. The self-supervised methods train deep hashing network based on the semantic multi-label and preserve the original semantic relationship in the Hamming space. However, most self-supervised methods neglect the semantic weight inconsistency problem among the image, text and multi-label. Moreover, they ignore preserving the relative ranking orders among the retrieval samples. To address the above issues, we propose a novel method termed weighted semantic feature based self-supervised deep cross-modal hashing (WFSCH). Firstly, we design the weighted semantic feature module to adaptively generate the weighted semantic features for the image, text and multi-label. The weighted semantic features improve the accuracy of describing different modal content and enhance the self-supervised semantic constraint. Secondly, to further improve the cross-modal retrieval performance, we preserve the original intra- and inter-modal triplet ranking relationship in the Hamming space. Finally, to minimize the discrepancy between the pairwise hashing similarity and the multi-label semantic relationship, we design the multi-label semantic similarity preserving loss based on the positive-constraint Kullback-Leibler (KL) divergence, which fully explores different modal multi-label semantic information. We conduct extensive experiments on four publicly available datasets including MIRFLICKR-25K, NUS-WIDE, MS COCO2014 and IAPR TC-12. The experimental results demonstrate that the proposed WFSCH algorithm outperforms the state-of-the-art cross-modal hashing algorithms.},
  archive      = {J_IJMIR},
  author       = {Gao, Limeng and Wang, Zhen and Wang, Xinzhong and Zheng, Zhen and Chen, Haixu and Lu, Shihong},
  doi          = {10.1007/s13735-025-00378-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Weighted semantic feature based self-supervised deep cross-modal hashing},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A similarity-optimized and semantic-aligned method for unsupervised cross-modal hashing retrieval. <em>IJMIR</em>, <em>14</em>(3), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00376-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of multimodal data types, unsupervised cross-modal hashing retrieval technology has become a vital solution for efficient storage and fast retrieval by learning universal binary hash codes. However, existing methods often suffer from inaccurate similarity measurements and imbalanced information between modalities, which limit the further improvement of retrieval performance. To address these challenges, this paper introduces a Similarity-Optimized and Semantic-Aligned Method for Unsupervised Cross-modal Hashing Retrieval (SOSAH), which contains a similarity graph optimization strategy and a hierarchical multimodal semantic-aware alignment module. (1) The similarity graph optimization strategy enhances the similarity between closely related samples while suppressing the similarity of unrelated samples. This refinement improves the expressiveness of the similarity matrix, effectively guiding the learning of highly discriminative hash codes. (2) To solve the problem of the imbalance of the modalities, we construct a hierarchical multimodal semantic-aware alignment module. This module employs a graph convolutional network to align intra-modal correlations and a hierarchical similarity aggregation network to align inter-modal relationships. Experimental results on two publicly available datasets demonstrate the effectiveness of the proposed approach and show significant improvements in retrieval performance.},
  archive      = {J_IJMIR},
  author       = {Li, Xin and Li, Xiuyuan and Li, Mingyong and Ge, Mingyuan},
  doi          = {10.1007/s13735-025-00376-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A similarity-optimized and semantic-aligned method for unsupervised cross-modal hashing retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETG: The graph convolutional network was enhanced with an EA-transformer for aspect sentiment triplet extraction. <em>IJMIR</em>, <em>14</em>(3), 1-17. (<a href='https://doi.org/10.1007/s13735-025-00377-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Aspect Sentiment Triplet Extraction task involves identifying aspect terms, corresponding opinion terms, and their sentiment polarity within sentences, to comprehensively capture the text's fine-grained information. Existing methods predominantly utilize end-to-end approaches based on graph convolutional networks (GCNs). However, traditional GCN models may encounter long-distance dependency problems when dealing with longer sentences. Although increasing the number of GCN layers can help cover longer-distance dependencies, it leads to over-smoothing, where node features become indistinguishable due to the excessive aggregation of neighboring features as the number of layers increases. Therefore, in this article, we propose a GCN model enhanced by an edge attention transformer (EA-Transformer). We first extract local subgraph structures through GCNs and then capture global graph structures using the EA-Transformer method. Specifically, we use the EA-Transformer to learn different dependency types of adjacent edges; even edges with the same dependency type can obtain different representations and weights. In addition, when constructing edge representations, we incorporate multiple linguistic features and introduce an Additive Context Attention module, which enables the model to better capture and utilize critical contextual information. Extensive experiments on four benchmark datasets demonstrate that our model outperforms existing methods, achieving higher F1 scores while performing excellently in extracting overlapping triplets and handling long-distance dependencies.},
  archive      = {J_IJMIR},
  author       = {Yang, Kun and Gao, Bin and Li, Linlin and Li, Yutong and Liu, Shutian and Liu, Zhengjun},
  doi          = {10.1007/s13735-025-00377-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {ETG: The graph convolutional network was enhanced with an EA-transformer for aspect sentiment triplet extraction},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-NeuS: Neural implicit surface reconstruction using feature multi-view consistency constraint. <em>IJMIR</em>, <em>14</em>(3), 1-15. (<a href='https://doi.org/10.1007/s13735-025-00379-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the emergence of Neural Radiance Fields, neural implicit surface reconstruction methods have achieved remarkable progress. However, the reconstruction process still struggles with precision, often leading to blurry regions in the fine details. To address this challenge, we propose Feature-NeuS, a neural implicit surface reconstruction method that integrates multi-view semantic features consistency. We extract multi-scale features that include high-level semantic representations and low-level visual information, which serve as prior knowledge for the network to refine the fine details in the 3D model. Additionally, we introduce a multi-view consistency loss for surface points, employing multi-view geometric constraints on image level and feature level to enhance the accuracy and sharpness of the reconstruction. Our method aims to better capture and refine the intricate structures of the 3D reconstructed model by utilizing both image-derived features and multi-view consistency. Extensive qualitative and quantitative experiments demonstrate that our method outperforms the state-of-the-art on DTU, BlendedMVS, and Tank & Temple datasets, particularly in recovering fine model details.},
  archive      = {J_IJMIR},
  author       = {Ding, WenYu and Wu, YaHong and Liu, Feng},
  doi          = {10.1007/s13735-025-00379-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Feature-NeuS: Neural implicit surface reconstruction using feature multi-view consistency constraint},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
