<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin">APIN - 43</h2>
<ul>
<li><details>
<summary>
(2025). PretopoMD: Pretopology-based mixed data hierarchical clustering. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06770-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm’s robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.},
  archive      = {J_APIN},
  author       = {Levy, Loup-Noé and Guerard, Guillaume and Djebali, Sonia and Amor, Soufian Ben},
  doi          = {10.1007/s10489-025-06770-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {PretopoMD: Pretopology-based mixed data hierarchical clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets. <em>APIN</em>, <em>55</em>(15), 1-30. (<a href='https://doi.org/10.1007/s10489-025-06836-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of credit risk data imbalance reduces the effectiveness of assessment models. Existing oversampling methods focus only on a partial sample of a few classes, resulting in a lack of diversity in the types of data generated. This paper proposes an innovative GAN variant called Magnify-GAN. The originality of Magnify-GAN lies in the fact that it is equipped with a primary discriminator and multiple secondary discriminators, each of which employs a different loss function. This multi-discriminator approach not only improves the learning results, but also enriches the feedback received during the training process. In addition, we integrate an innovative dynamic coefficient mechanism to enable the model to dynamically adapt to changes in data distribution. To further improve stability and address the common modal collapse problem in GAN, a gradient penalty method is embedded in the training protocol. This integrated strategy ensures that Magnify-GAN can effectively generate samples representing various minority classes within the real data. Compared to ten classical imbalanced sampling methods, Magnify-GAN demonstrates superior performance in precision, F1-score, and AUC values across six synthetic and four real-world imbalanced datasets. Ablation studies, visualized through heatmaps, reveal the complementary synergy between the core modules. Furthermore, a complexity analysis shows that Magnify-GAN offers significant performance gains with moderate increases in computational cost compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Dong, Xiaogang and Wang, Lifei and Qin, Xiwen and Shi, Hongyu},
  doi          = {10.1007/s10489-025-06836-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-30},
  shortjournal = {Appl. Intell.},
  title        = {Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph convolutional network for time series classification using recurrence plots. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06841-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a crucial task across various domains, and its performance heavily depends on the quality of input representations. Among various representations, the recurrence plot (RP) effectively captures topological recurrence, the unique property of time series data. However, conventional convolutional neural networks (CNNs) cannot fully exploit this property since they treat the RP as grid-like data. In this study, we propose RP-GCN, a novel approach that uses a graph convolutional network (GCN) to exploit topological recurrence inherent in the RP, thereby improving TSC performance. Our method transforms a multivariate time series into graphs where state matrices act as node feature matrices and RPs serve as adjacency matrices, enabling graph convolution to utilize recurrence relationships. We evaluated RP-GCN on 35 benchmark multivariate time series classification datasets and demonstrated superior accuracy and efficient inference time compared to existing methods.},
  archive      = {J_APIN},
  author       = {Kang, Hyewon and Lee, Taek-Ho and Lee, Junghye},
  doi          = {10.1007/s10489-025-06841-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A graph convolutional network for time series classification using recurrence plots},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06846-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shield operational parameters are inherently noisy and, relative to concurrent geological exploration data, contain considerable redundancy, they must be pre-processed before the datasets input to artificial intelligence models. This paper presents a denoising and compression method for preprocessing shield operational parameters, integrating it with the stratal slicing method for predicting disc cutter wear. The operational parameter signals affecting cutter wear are first denoised using wavelet transform, Fourier transform, rolling average, and autoencoder techniques. The proposed Ring-based Summation Averaging (RSA) and Piecewise Aggregate Averaging (PAA) methods are then used to compress the denoised signals, resulting in compressed sequences composed of key points equal to the number of tunnel rings, effectively matching the geological parameters expanded by the stratal slicing method. Furthermore, the prepared data were tested using the long short-term memory (LSTM) + attention mechanism (AM) model to evaluate its application effectiveness in the Guangzhou Metro Line 18 railway. The results show that data compressed using PAA not only better tracks signal variations but also allows for flexible control of the output length of the compressed sequence. The combination of wavelet transforms denoising (WTD) with PAA exhibited the best wear prediction results, achieving R2 / MSE = 0.95 / 2.21 mm. By integrating WTD, PAA, stratal slicing method, and sequence models, a comprehensive and universal methodology is established that can predict disc cutter wear based on initial geological data and shield operational parameters.},
  archive      = {J_APIN},
  author       = {Mo, Deyun and Bai, Liping and Liao, Wenjiang and Tian, Xinyuan and Huang, Weiran},
  doi          = {10.1007/s10489-025-06846-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic gradient accelerated by negative momentum for training deep neural networks. <em>APIN</em>, <em>55</em>(15), 1-13. (<a href='https://doi.org/10.1007/s10489-025-06900-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast and robust stochastic optimization algorithms for training deep neural networks (DNNs) are still a topic of heated discussion. As a simple but effective way, the momentum technique, which utilizes historical gradient information, shows significant promise in training DNNs both theoretically and empirically. Nonetheless, the accumulation of error gradients in stochastic settings leads to the failure of momentum techniques, e.g., Nesterov’s accelerated gradient (NAG), in accelerating stochastic optimization algorithms. To address this problem, a novel type of stochastic optimization algorithm based on negative momentum (NM) is developed and analyzed. In this work, we applied NM to vanilla stochastic gradient descent (SGD), leading to SGD-NM. Although a convex combination of previous and current historical information is adopted in SGD-NM, fewer hyperparameters are introduced than those of the existing NM techniques. Meanwhile, we establish a theoretical guarantee for the resulting SGD-NM and show that SGD-NM enjoys the same low computational cost as vanilla SGD. To further show the superiority of NM in stochastic optimization algorithms, we propose a variant of stochastically controlled stochastic gradient (SCSG) based on the negative momentum technique, termed SCSG-NM, which achieves faster convergence compared to SCSG. Finally, we conduct experiments on various DNN architectures and benchmark datasets. The comparison results with state-of-the-art stochastic optimization algorithms show the great potential of NM in accelerating stochastic optimization, including more robust to large learning rates and better generalization.},
  archive      = {J_APIN},
  author       = {Li, Xiaotian and Yang, Zhuang and Wang, Yang},
  doi          = {10.1007/s10489-025-06900-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Stochastic gradient accelerated by negative momentum for training deep neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). External information-augmented contrastive learning framework for fake news detection. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06807-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news and information overload on social media has led to increased public confusion and poses a serious threat to social stability. Traditional fake news detection methods typically focus solely on the content of the news itself, making them vulnerable to manipulation by disinformation campaigns. This limitation highlights the need for a more comprehensive approach that incorporates external information to improve detection accuracy. In response to this challenge, we propose a novel framework for fake news detection, named External Information-Augmented Contrastive Learning (EACL). The EACL framework consists of three key modules: (1) the External Information Construction Module, which utilizes entity linking, embedding, and retrieval techniques to analyze news from both factual and public opinion perspectives, thus creating an analysis-friendly environment; (2) the Consistency Feature Extraction Module, which employs a distance-aware signed attention mechanism to model the consistency between news content and external information, while filtering out irrelevant data; and (3) the Comparative Learning Enhancement Module, which constructs positive and negative sample pairs to enhance the learning of semantic differences between fake and real news. Extensive qualitative and quantitative experiments conducted on two real-world datasets demonstrate that EACL achieves impressive accuracy rates of 85.2% and 82.9%, significantly outperforming existing baseline methods. The results further illustrate the effectiveness of integrating external information and contrastive learning in combating misinformation.},
  archive      = {J_APIN},
  author       = {Fang, Xiaochang and Zhang, Huaxiang and Wu, Hongchen and Liu, Li and Yu, Hongzhu and Li, Hongxuan and Jing, Zhaorong},
  doi          = {10.1007/s10489-025-06807-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {External information-augmented contrastive learning framework for fake news detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06859-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial filter-based calibration-training algorithms play a crucial role in improving the information transfer rate (ITR) of steady-state visual evoked potential based brain-computer interfaces (SSVEP-BCIs). These algorithms optimize spatial filters by suppressing the non-SSVEP related components, thereby enhancing the signal-to-noise ratio (SNR) of electroencephalogram (EEG) signals. However, conventional methods neglect the temporally-varying and spatially-coupled characteristics of EEG signals, leading to inherent ITR bottlenecks in BCIs. To this end, we propose a novel SSVEP detection algorithm, termed as multi-stimulus Generalized and Corrected Canonical Correlation Analysis (msGC3A), which is extended and corrected from the generalized canonical correlation analysis algorithm. Specifically, we develop corrected sine-cosine reference templates that enhance the spatial filters’ generalization capability across multiple stimuli. Moreover, we formulate a weighted correlation coefficient that synergistically integrates both generalized and corrected multi-stimulus templates for further enhancement. Empirical experiments have been conducted on two publicly available benchmark SSVEP datasets, and we compared the ensemble version of our msGC3A algorithm with four state-of-the-art algorithms. The results have shown that our algorithm significantly improves SSVEP detection performance while requiring less calibration data. Furthermore, we also conducted ablation experiments to show the adaptive capacity of employing our algorithm for SSVEP-BCIs.},
  archive      = {J_APIN},
  author       = {Lv, Yanhao and Luo, Tian-jian},
  doi          = {10.1007/s10489-025-06859-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding the optimization mechanisms in deep learning. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06875-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we adopt a probability distribution estimation perspective to explore the optimization mechanisms of supervised classification using deep neural networks. We demonstrate that, when employing the Fenchel-Young loss, despite the non-convex nature of the fitting error with respect to the model’s parameters, global optimal solutions can be approximated by simultaneously minimizing both the gradient norm and the structural error. The former can be controlled through gradient descent algorithms. For the latter, we prove that it can be managed by increasing the number of parameters and ensuring parameter independence, thereby providing theoretical insights into mechanisms such as overparameterization and random initialization. Ultimately, the paper validates the key conclusions of the proposed method through empirical results, illustrating its practical effectiveness.},
  archive      = {J_APIN},
  author       = {Qi, Binchuan and Gong, Wei and Li, Li},
  doi          = {10.1007/s10489-025-06875-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Towards understanding the optimization mechanisms in deep learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust low-rank representation with structured similarity learning for multi-label classification. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06879-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling high-dimensional, noisy data in multi-label classification is challenging, as feature abundance and noise obscure actual data-label relationships. Traditional approaches often model labels and features independently, limiting dependency modeling and noise reduction. To address this, we propose a unified framework combining low-rank representation using nuclear norm regularization with structured similarity learning. This simultaneously projects features and labels into low-rank spaces while preserving key inter-sample and inter-label relationships through structural constraints, further capturing fine-grained correlations via a learned similarity Matrix. Extensive experiments on five benchmark datasets show our model outperforms state-of-the-art methods, achieving a 16% reduction in Hamming Lossl and a 14% improvement in Micro-F1 on high-dimensional, noisy datasets like CAL500 and Corel16k7, with consistent gains in Macro-F1 and Example-F1. These results demonstrate the model’s strong capability for noisy, high-dimensional multi-label classification.},
  archive      = {J_APIN},
  author       = {Ntaye, Emmanuel and Zhou, Conghua and Liu, Zhifeng and Song, Heping and Issahaku, Fadilul-lah Yassaanah and Shen, Xiang-Jun},
  doi          = {10.1007/s10489-025-06879-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Robust low-rank representation with structured similarity learning for multi-label classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06886-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in channel-dependent Transformer-based forecasters highlight the efficacy of variate tokenization for time series forecasting. Despite this progress, challenges remain in handling complex time series. The vanilla Transformer, while effective in certain scenarios, faces limitations in addressing intricate cross-variate interactions and diverse temporal patterns. This paper presents the Enhanced Inverted Transformer (EiT for short), enhancing standard Transformer blocks for advanced modeling and blending of variate tokens. EiT incorporates three key innovations: First, a hybrid multi-patch attention mechanism that adaptively fuses global and local attention maps, capturing both stable and volatile correlations to mitigate overfitting and enrich inter-channel communication. Second, a multi-head feed-forward network with specialized heads for various temporal patterns, enhancing parameter efficiency and contributing to robust multivariate predictions. Third, paired channel normalization applied to each layer, preserving crucial channel-specific statistics and boosting forecasting performance. By integrating these innovations, EiT effectively overcomes limitations and unlocks the potential of variate tokens for accurate and robust multivariate time series forecasting. Extensive evaluations demonstrate that EiT achieves state-of-the-art (SOTA) performance, surpassing the previous method, the inverted Transformer, by an average of 4.4% in Mean Squared Error (MSE) and 3.4% in Mean Absolute Error (MAE) across five challenging long-term forecasting datasets.},
  archive      = {J_APIN},
  author       = {Li, Xin-Yi and Yang, Yu-Bin},
  doi          = {10.1007/s10489-025-06886-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid federated unlearning with tuning parameters based on fisher information matrix. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06593-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning approach widely applied in privacy-sensitive scenarios. With the emergence of the “right to be forgotten” and the pursuit of data accuracy, there is an increasing demand to quickly and accurately delete targeted information from models while ensuring model performance. Therefore, federated unlearning has been introduced. Although current federated unlearning methods achieve effective unlearning, they often involve lengthy processes and require servers to store extensive historical update information. We propose a novel rapid federated unlearning method named FedTune. This method leverages the Fisher information matrix computed on the client side to assess the correlation between model parameters and the target data, identifying key parameters for adjustment. Based on the importance of these parameters and the frequency of client participation, FedTune determines appropriate adjustment ratios to increase the classification loss on the target data, thereby reducing the model’s accuracy and achieving effective data unlearning. Finally, the server collaborates with the remaining clients for a few rounds of retraining to restore the overall classification performance rapidly. We evaluated the FedTune method on the MNIST, CIFAR-10, and PURCHASE datasets, considering both fixed and dynamic client selection scenarios in privacy-sensitive and contamination settings. Experimental results show that FedTune reduces the time consumed by the unlearning process and server storage costs of the unlearning algorithm while ensuring model classification accuracy and effective unlearning compared to other unlearning algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Fengda and Xu, Qianyi and Wang, Hao and Guo, Dingding},
  doi          = {10.1007/s10489-025-06593-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Rapid federated unlearning with tuning parameters based on fisher information matrix},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm. <em>APIN</em>, <em>55</em>(15), 1-36. (<a href='https://doi.org/10.1007/s10489-025-06724-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality Function Deployment (QFD) is a classic customer requirements (CRs)-oriented quality management method. However, the increasing complexity and diversity of CRs in the modern society makes it impossible for the traditional QFD approach with a limited number of team members (TMs) to fully satisfy CRs. Therefore, in order to solve the QFD problem in complex environments, this paper proposes an improved QFD method based on Dempster–Shafer evidence theory (D-S theory) and hierarchical clustering algorithm in large-scale group environments. Firstly, utilizing the advantages of D-S theory in information processing and synthesis, the evaluation of quality characteristics (QCs) in the form of probabilistic linguistic term sets (PLTSs) is transformed into basic probability assignments (BPAs) to handle uncertainty more flexibly. Secondly, this paper designs a hierarchical clustering algorithm based on bounded confidence to divide TMs into subgroups, and fully considers the interaction willingness of TMs during the clustering process to ensure the efficiency and accuracy of decision-making. On this basis, the Stepwise Weight Assessment Ratio Analysis (SWARA) method based on distance degree is introduced to calculate the weight of CRs in a more objective way. Then, the Decision-making Trial and Evaluation Laboratory (DEMATEL) method based on D-S theory is used to deeply analyze the mutual influence relationship between QCs to reveal its internal logic. Besides, combined with the psychological expectations of TMs, the disappointment theory is used to prioritize QCs to ensure that products or services are more in line with customer expectations. Finally, this paper applies the proposed method to the development process of mobile health applications (mHealth apps) from the perspective of privacy security, verifying the practicability and superiority of the method. The effectiveness of the method in CRs transformation and product design optimization is further demonstrated through parametric and comparative analyses.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Feng, Xuan and Zhang, Jihao and Zhang, Bo and Wang, Wenxin and Liu, Peide},
  doi          = {10.1007/s10489-025-06724-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-36},
  shortjournal = {Appl. Intell.},
  title        = {A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way clustering ensemble based on shadowed sets with five approximation regions. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06726-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble is a powerful technique for aggregating multiple clustering results. In order to address the challenge in clustering analysis which was brought by the uncertainty information in the datasets, this work presents a novel three-way clustering ensemble method based on shadowed sets with five approximation regions (3WCE-S5). Firstly, a set of clustering members are generated by fuzzy c-means clustering (FCM). A new shadowed sets is approximated by five regions, named as shadowed sets with five approximation regions (S5). Then, all objects are initially partitioned into five regions according to their membership degrees, which are provided by FCM. Secondly, according to multi-granularity rough sets, objects are further assigned into six approximated regions, namely a core region and five fringe regions. There is a partial order relationship between these six different approximate regions. Finally, the above six regions are processed by the new shadowed sets again to generate the output of three-way clustering. Ten University of California Irvine (UCI) data sets are employed to test the performance of this approach and five comparative methods. Accuracy (ACC), adjusted rand index (ARI), normalized mutual information (NMI) and time cost are utilized to quantify the clustering results.},
  archive      = {J_APIN},
  author       = {Yi, Huangjian and Guo, Dongkai and Zhang, Qinran and He, Xiaowei and Ren, Ruisi},
  doi          = {10.1007/s10489-025-06726-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Three-way clustering ensemble based on shadowed sets with five approximation regions},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic interaction-enhanced encoding network for math word problem solving. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06850-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving math word problems (MWPs) requires machines to understand not only the literal meaning of text but also the abstract logic and mathematical reasoning embedded within it. However, existing models often lack explicit reasoning capabilities for semantic information, particularly when dealing with complex math word problem texts. Additionally, these models tend to embed all kinds of information without fine-grained selection, which may introduce unexpected noise for mathematical expression generation. To address these challenges, we propose a Semantic Interaction-Enhanced Encoding Network (SIEN) for math expression generation is proposed in this paper. Firstly, SIEN constructs a semantic role interaction graph for each problem and employs a graph attention neural network to learn interaction and semantic information, offering a more structured and enriched view of the math word problem text. Secondly, SIEN introduces a multi-channel adapter module that simultaneously learns comprehensive contextual information from numeric information channel, hierarchical semantic information channel, and interaction information channel. Furthermore, SIEN introduces a dynamic weighting mechanism that adjusts the information weight from each channel to prioritize relevant information and reduce noise. Experimental results on three public benchmark datasets demonstrate that SIEN achieves significant performance improvement over other state-of-the-art baseline models.},
  archive      = {J_APIN},
  author       = {Xiao, Lingsheng and Chen, Yuzhong and Liu, Zhanghui and Zhong, Jiayuan and Dong, Yu},
  doi          = {10.1007/s10489-025-06850-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Semantic interaction-enhanced encoding network for math word problem solving},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06899-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial discharge pattern recognition (PDPR) is a crucial cornerstone for condition monitoring and safe operation of electrical devices. It has become an important hotspot in the field of energy systems. However, it faces several challenges, including noise interference, signal complexity, and difficulty in data labeling. This study proposes an efficient multi-scale masked autoencoder (MAE) network (KRMNet) to effectively address these challenges. KRMNet learns common and important multi-scale features and long-range semantic dependencies of partial discharge signals. Furthermore, by using the MAE structure and the transformer as the backbone, the model extracts key distinguishing features from phase-resolved partial discharge (PRPD) signals with background noise, interference, and labeling issues in an efficient and self-supervised manner. In addition, the efficient multi-scale module uses an efficient multi-scale attention mechanism to aggregate key information from multiple feature dimensions. The integration of the efficient multi-scale attention mechanism and contrastive learning methods improves the model’s ability to distinguish key information and resist interference. Experiments on two challenging PRPD datasets show that our proposed KRMNet achieves detection accuracies of 88.5% and 90.2% on noisy (ECPD) and clean (PUPD) datasets, respectively. This finding suggests that the method faces challenges in effectively managing noise interferences and missing labels.},
  archive      = {J_APIN},
  author       = {Deng, Yi and Chen, Jiawen and Xie, Quan and Tan, Dapeng and Liu, Hai},
  doi          = {10.1007/s10489-025-06899-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLDM-palm: A controllable latent diffusion model for high-fidelity palmprint generation based on bézier curves. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06923-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using limited real data to synthesize realistic palmprints and expand training samples for recognition models has become a promising direction in palmprint recognition. However, the pseudo-palmprints generated by existing models still exhibit significant discrepancies from real ones, particularly in crease structures and fine-grained details. In this paper, we first introduce Latent Diffusion Models (LDM) as the backbone to improve the quality of palmprint generation. Secondly, to incorporate Bézier curves as control conditions into the model, we propose the Palm-to-Bézier Module (P2BM), which maps real palmprints to their corresponding Bézier-style pseudo-Bézier curves. These curves establish the connection between real palmprints and Bézier curves, which are used as conditional inputs during diffusion model training. At inference time, Bézier curves can be provided as conditions to generate high-resolution, fine-grained, and highly realistic synthetic palmprints. Thirdly, to enable Bézier curves to better model palmprint creases, we propose 12 Bézier curves templates based on real crease distribution priors. With only 10-step Denoising Diffusion Implicit Models (DDIM) sampling, our method achieves a significantly lower Fréchet Inception Distance (FID) compared to existing palmprint generation approaches. Moreover, the recognition models trained on the synthetic palmprints generated by our model achieve new state-of-the-art results in both Fisher Discriminant Ratio (FDR) and TAR@FAR metrics. Under a 1:1 train-test fine-tuning setting, our model improves average TAR@FAR= $$10^{-6}$$ performance by over $$10\%$$ compared to prior methods. We name our model CLDM-Palm (Controllable Latent Diffusion Model-Palm).},
  archive      = {J_APIN},
  author       = {Zhu, Yuanpan and Jia, Donghuai and Chu, Kevin and Zhi, Wenshuang and Li, Weide and Chen, Shukai},
  doi          = {10.1007/s10489-025-06923-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {CLDM-palm: A controllable latent diffusion model for high-fidelity palmprint generation based on bézier curves},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RXNet: Cross-modality person re-identification based on a dual-branch network. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06501-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of text-based person re-identification (TI-ReID) is to match individuals using various methods by integrating information from both images and text. TI-ReID encounters significant challenges because of the clear differences in features between images and textual descriptions. Contemporary techniques commonly utilize a method that merges general and specific characteristics to obtain more detailed feature representations. However, these techniques depend on additional models for estimating or segmenting human poses to determine local characteristics, making it challenging to apply them in practice. To solve this problem, we propose a dual-path network based on RegNet and XLNet for TI-ReID (RXNet). In the image segment, RegNet is employed to acquire multitiered semantic image attributes and dynamically assimilate distinct local features through visual focus. In the text segment, XLNet is utilized, to extract significant semantic attributes from the text via a two-way encoding system based on an autoregressive model. Furthermore, to increase the efficacy of our model, we develop both residual triplet attention and dual attention to align features across different modalities. Additionally, we replace cross-entropy ID loss with smoothing ID loss to prevent overfitting while improving the efficiency of the model. Experimental results on the CUHK-PEDES dataset show that the proposed method achieves a rank-1/mAP accuracy of 85.49%/73.40%, outperforming the current state-of-the-art methods by a large margin.},
  archive      = {J_APIN},
  author       = {Zhang, Weiyang and Guo, Jiong and Liu, Qiang and Zou, Maoyang and Chen, Honggang and Peng, Jing},
  doi          = {10.1007/s10489-025-06501-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {RXNet: Cross-modality person re-identification based on a dual-branch network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prototypes guided model transformations between personalization and generalization in federated learning. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06566-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has gained popularity due to its ability to train a collaborative model while preserving privacy. However, it still faces limitations when dealing with heterogeneous data, primarily manifesting as the performance degradation of the global model and the inadaptability of the single global model to the divergence of client data distributions. Although the above issues are summarized by researchers as goals for generalization and personalization, few studies have simultaneously addressed both goals, with most prioritizing one over the other. In this paper, it is demonstrated that the FL iteration already incorporates model transformations between personalization and generalization, with a focus on ensuring the smooth functionality of these transformations under high data heterogeneity. Specifically, a novel Federated Prototype Transformation Framework (FedPT) is proposed, which is capable of generating a well-performing generalized model as well as personalized models simultaneously. FedPT constructs local prototype classifiers that explicitly guide personalized model optimization during local training, and these can be aggregated into a global prototype classifier suitable for generic tasks. The momentum update design retains the global knowledge in local training and aligns features between clients, which results in a smoother iteration. Moreover, an improved sample-level contrastive loss is presented to dig into deeper representations, achieving high-quality prototype generation even for missing or imbalanced classes. Experimental results demonstrate the exceptional performance of FedPT in both generalization and personalization tasks, outperforming latest methods.},
  archive      = {J_APIN},
  author       = {Xi, Yuan and Li, Qiong and Mao, HaoKun},
  doi          = {10.1007/s10489-025-06566-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Prototypes guided model transformations between personalization and generalization in federated learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIMCAR: Dynamic intent modeling and context-aware recommendations in sparse data environment towards next basket prediction. <em>APIN</em>, <em>55</em>(15), 1-30. (<a href='https://doi.org/10.1007/s10489-025-06796-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fast-changing world of e-commerce, the success of recommender systems is crucial for boosting user engagement and increasing sales. Conventional models often struggle with evolving user preferences and data sparsity, hindering accurate predictions. Existing Graph-based regularization mechanisms and deep learning approaches address these challenges but remain sensitive to noise and computational complexity, limiting their effectiveness in large-scale, real-time settings. We propose a novel multi-layered Next Basket Recommender System called dynamic intent modelling and context-aware recommendation (DIMCAR) model to overcome these limitations. First, we resolve the data sparsity problem by constructing a novel optimized Graph Sparse Regularization framework for Non-negative Matrix Factorization (OGSR-NMF) framework integrating a time-varying graph structure, a novel hybrid sparsity norm, a modified Proximal Alternating Linearized Minimization (mPALM). Additionally, we dynamically model user intents and context using attention mechanisms and Gated Recurrent Units (GRUs). Finally, we integrate a novel Adaptive Reptile Basket Optimization Algorithm into a Deep Convolutional Neural Network, enhancing the model's adaptability to changing user behaviours in real time. Theoretical analysis and experiments on four benchmark datasets demonstrate that DIMCAR outperforms existing models in recommendation accuracy and user satisfaction.},
  archive      = {J_APIN},
  author       = {Arthur, John Kingsley and Zhou, Conghua and Shen, Xiang-Jun and Amber-Doh, Ronky Wrancis and Mantey, Eric Appiah and Osei-Kwakye, Jeremiah},
  doi          = {10.1007/s10489-025-06796-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-30},
  shortjournal = {Appl. Intell.},
  title        = {DIMCAR: Dynamic intent modeling and context-aware recommendations in sparse data environment towards next basket prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EGPT-SPE: Story point effort estimation using improved GPT-2 by removing inefficient attention heads. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06824-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating story points from user requirements is crucial in the Software Development Life Cycle (SDLC) as it impacts resource allocation and timelines; inaccuracies can lead to missed deadlines and increased costs, harming a company’s reputation. While various techniques have emerged to automate this process, conventional machine learning methods often fail to understand the context of user requirements, and deep learning approaches face high computational costs. To address these issues, the Efficient GPT for Story Point Estimation (EGPT-SPE) algorithm optimizes the Multi-Head Attention module by removing inefficient heads, enhancing accuracy and reducing costs. Experiments on the Choetkiertikul dataset (23,313 issues across 16 open-source projects) and the TAWOS dataset (458,232 issues across 39 open-source projects from 12 public JIRA repositories) demonstrated a 5 to 15 percent accuracy improvement in both within-project and cross-project estimations, validating the algorithm’s effectiveness in agile story point estimation.},
  archive      = {J_APIN},
  author       = {Cheemaa, Amna Shahid and Azhar, Muhammad and Arif, Fahim and ul haq, Qazi Mazhar and Sohail, Muhammad and Iqbal, Asma},
  doi          = {10.1007/s10489-025-06824-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {EGPT-SPE: Story point effort estimation using improved GPT-2 by removing inefficient attention heads},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing vitiligo stage diagnosis through a reliable multimodal model with uncertainty calibration. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06839-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vitiligo is a common dermatological disease featuring hypopigmentation. Accurate staging of vitiligo is crucial for enhancing treatment efficacy. However, traditional diagnostic methods, which rely on physicians' subjective judgments, are time-consuming, labor-intensive, and prone to misdiagnosis. Recently, AI-powered multimodal dermatological classification models have demonstrated significant potential in this area. But the credibility of these models at the decision-making stage is an area that requires further refinement. This study proposes a multimodal disease staging diagnostic model with uncertainty calibration to analyze multimodal samples from three stages of vitiligo. The model innovatively extracts feature information from various modalities and transforms it into a Dirichlet distribution to assess sample uncertainty. Then, the Dempster—Shafer theory is used to fuse evidence from different modalities, generating a final diagnostic result and an uncertainty score. Additionally, an uncertainty—based loss function is designed. And by using an uncertainty threshold method, the model can detect high—uncertainty samples that require additional judgment, effectively reducing the risk of misdiagnosis and missed diagnosis. Experimental results show that this model outperforms existing methods in terms of accuracy, precision, recall, and F1—score. Anomaly detection and noise—resistance experiments verify the model's robustness in handling unknown and noisy data. This model offers a new approach for AI-assisted vitiligo diagnosis, which can assist doctors in making more accurate diagnostic decisions, contribute to improving treatment efficiency.},
  archive      = {J_APIN},
  author       = {Li, Zhiming and Jiang, Shuying and Xiang, Fan and Li, Chunying and Li, Shuli and Gao, Tianwen and He, Kaiqiao and Chen, Jianru and Zhang, Junpeng and Zhang, Junran},
  doi          = {10.1007/s10489-025-06839-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing vitiligo stage diagnosis through a reliable multimodal model with uncertainty calibration},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning techniques for point cloud tasks: A review. <em>APIN</em>, <em>55</em>(15), 1-52. (<a href='https://doi.org/10.1007/s10489-025-06854-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a significant means of representing 3D scenes, point clouds are extensively utilized in various fields Such as computer vision, autonomous driving, robotic interaction, and urban modeling. While deep learning has achieved remarkable Success in the realm of two-dimensional images, and its application to three-dimensional point clouds is also progressively gaining traction. However, the irregular and unstructured nature of point cloud data presents numerous challenges when applying deep learning algorithms to these 3D representations. To foster future research endeavors, this paper concentrates on three fundamental tasks associated with point clouds: classification, object detection, and semantic segmentation. It systematically reviews the current state of development regarding deep learning algorithms pertinent to these tasks. By organizing and analyzing existing literature alongside experimental results derived from publicly available datasets, this paper compares the strengths of different methodologies while also highlighting their limitations. Ultimately, it summarizes the technical challenges encountered in advancing deep learning algorithms for point clouds and outlines potential avenues for progress within this domain.},
  archive      = {J_APIN},
  author       = {Song, Xiaona and Zhang, Haozhe and Wang, Lijun and Niu, Jinxing and Zhu, Ying and Nian, Junjie and Cheng, Ruixue},
  doi          = {10.1007/s10489-025-06854-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-52},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning techniques for point cloud tasks: A review},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing adversarial robustness in power quality classification systems: An attention-based defense framework. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06865-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power quality monitoring is essential for ensuring the reliability, stability, and security of modern electrical networks. While deep learning models have demonstrated exceptional performance in classifying power quality disturbances, they remain critically vulnerable to adversarial perturbations—posing significant risks to smart grid cybersecurity. This paper introduces three novel contributions to the field of power quality cybersecurity: (1) Signal-Agnostic Adversarial (SAA) attacks—a perturbation method tailored specifically for power quality signals; (2) an attention-based convolutional neural network (CNN) architecture that consistently achieves 5–7% points higher robustness under attack compared to conventional models; and (3) comprehensive vulnerability fingerprinting, which exposes architecture-specific adversarial attack patterns and provides insights into structural weaknesses. We conduct a systematic analysis of CNN-based power quality classification models subjected to adversarial manipulations and propose effective defense strategies. Three attack methodologies are introduced and evaluated: the Fast Gradient Sign Method (FGSM), Signal-Specific Adversarial (SSA) attacks, and the proposed SAA attacks. Experimental results reveal catastrophic degradation in model performance, with accuracy reductions of up to 80–90% points under attack. To mitigate these vulnerabilities, our attention-based CNN model demonstrates significantly improved resilience, and adversarial training further enhances robustness—achieving up to 58.47% accuracy against SSA, the most potent attack vector. The findings underscore critical security implications of deep learning in power systems and offer practical mitigation strategies for enhancing robustness in real-world smart grid deployments.},
  archive      = {J_APIN},
  author       = {Alanazi, Mubarak and Alkhaldi, Nasser S.},
  doi          = {10.1007/s10489-025-06865-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing adversarial robustness in power quality classification systems: An attention-based defense framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual residual aggregation network for visual-language prompt tuning. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06866-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning leverages a series of learnable prompts to effectively guide pre-trained visual language models (VLMs) to adapt to various downstream tasks. VLMs encode deep features from both visual and textual branches and learn the joint embedding space of the two modalities by optimizing the contrast loss. However, existing prompt tuning methods face two critical challenges: (1) One challenge is the forgetting of generalized knowledge. As features propagate through the visual encoder, generalizable knowledge captured in shallow layers is gradually lost, ultimately impairing the generalization ability of the joint embedding space for new classes. (2) The other challenge is that models trained on the base class suffer from semantic bias. To address these issues, we propose Visual Residual Aggregation Network for Visual-Language Prompt Tuning (VraPT). VraPT comprises two sequentially connected components: a residual aggregation module and a semantic consistency module. Firstly, in order to solve the problem of generalized knowledge forgetting, the residual aggregation module enables adaptive fusion of generalized features, which effectively preserves generalized knowledge. It also reveals the importance of shallow features in enhancing the generalization capability of text prompts. The fused representation is then fed into the semantic consistency module which is used to address the problem of semantic bias. By minimizing the divergence from the true semantic distribution, this module enhances the semantic representations in the visual space as well as the semantic coherence of the learnable prompts. Our method enables the learned prompts to retain both discriminative semantic information and generalized knowledge. Extensive experiments show that our proposed VraPT is an effective prompt tuning method, especially in recognizing new classes with great improvement. On average, VraPT improves the accuracy on base classes by 1.06% and on new classes by 2.63% across 11 datasets, along with a 1.91% gain in the harmonic mean (H) metric.},
  archive      = {J_APIN},
  author       = {Yu, Yunqian and Guo, Feng and Tian, Xianlong and Chen, Biao and Jing, Mengmeng and Zuo, Lin},
  doi          = {10.1007/s10489-025-06866-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Visual residual aggregation network for visual-language prompt tuning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection method for improving shape perception of small object defects on metal surfaces. <em>APIN</em>, <em>55</em>(15), 1-15. (<a href='https://doi.org/10.1007/s10489-025-06873-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defects on metal surfaces often exhibit complexity with diverse shapes, small sizes, and irregular patterns, leading to frequent missed and false detections during inspection and posing significant challenges to automated detection systems. Existing advanced object detectors, when applied directly to small defect detection on metal surfaces, fail to achieve satisfactory results. To mitigate these issues, we proposed a detection method to enhance the shape perception of small object defects on metal surfaces, namely MetalYOLO. Firstly, a novel location-aware attention mechanism is designed to integrate deformable convolutions to form a new feature selection module to enhance the focus on key defect features, optimizes the generation of offsets, and improve the model’s ability to adapt to complex shape objects. Secondly, the standard up-sampling module is replaced with a dynamic sampling module to dynamically adjust the sampling pattern of the input feature distribution to improve computational efficiency and retain complex or small-scale object features, thereby improving detection accuracy. Finally, a new detail-enhanced detection head is designed to further improve the network’s ability to capture fine-grained details by introducing a detail-enhanced attention-sharing module so as to utilize contextual information to selectively suppress irrelevant features, thereby reducing information redundancy. The proposed model is compared with baseline models on the ILS-MB and NEU-DET datasets. and the experimental results show significant improvements in false detection and missed detection rates with only a slight loss in inference speed. Meanwhile, the mAP reached 80.4% and 79.0%, respectively, which is 1.7% and 3.2% higher than the baseline algorithm.},
  archive      = {J_APIN},
  author       = {Zhu, Xingfei and Montagne, Christophe and Wang, Qimeng and Hu, Lingxiang and Yu, Jinghu and Tabia, Hedi and Hu, Qianqian},
  doi          = {10.1007/s10489-025-06873-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Detection method for improving shape perception of small object defects on metal surfaces},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing act: Engagement detection in online learning through master-assistant models with an enhanced hierarchical attention mechanism. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06893-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of online learning calls for the establishment of effective approaches to monitor and boost student engagement, which constitutes a key element influencing learning outcomes. The class imbalances within engagement datasets pose substantial challenges to precise detection and classification. Existing methods for detecting student engagement in online learning adopt weighted loss to address the issue of class imbalance in public datasets. However, due to the challenge of selecting appropriate weights and the risk of overfitting, the effectiveness of this approach often relies on extensive experiments for manual adjustments. To tackle this problem, we propose a Master-Assistant model to address the performance degradation caused by class imbalance to ensure effective detection of student engagement. The Assistant model is designed for coarse-grained classification according to different assistant strategies to assist the Master model for fine-grained classification. Furthermore, we extract multiple engagement-related handcrafted features and assigned different weights via an enhanced hierarchical attention mechanism. Finally, an accuracy of 70.69% and an F1-score of 68% are achieved on the Dataset for Affective States in E-Environments (DAiSEE), setting new state-of-the-art (SOTA) scores. Additionally, experiments on three other imbalanced datasets also validate the robustness of the Master-Assistant model in solving the class imbalance problem.},
  archive      = {J_APIN},
  author       = {Han, Tingting and Liu, Ruqian and Dou, Shuwei and Wang, Wei and Ding, Xiaoming and Zhang, Wenxia and Lang, Jihao and Li, Wenxuan and Han, Jixing},
  doi          = {10.1007/s10489-025-06893-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Balancing act: Engagement detection in online learning through master-assistant models with an enhanced hierarchical attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransMambaCC: Integrating transformer and pyramid mamba network for RGB-T crowd counting. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06912-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T crowd counting is a challenging task that integrates RGB and thermal images to address the limitations of RGB-only approaches in scenes with poor illumination or occlusion. While transformer-based models have shown remarkable success in terms of capturing long-range dependencies, their high computational demands limit their practical applicability. To address this issue, a novel hybrid model named TransMambaCC, which fuses the analytical strength of transformer with the computational efficiency of Mamba, is proposed. This integration not only improves crowd analysis performance, but also significantly reduces computational overhead of the model. Additionally, a Pyramid Mamba module is innovatively designed to address the head-scale variations observed in congested scenes. Extensive experiments conducted on the RGBT-CC dataset demonstrate the superiority of TransMambaCC over the existing approaches in terms of both accuracy and efficiency. Furthermore, the model exhibits strong generalization capabilities, as evidenced by its performance on the ShanghaiTechRGBD dataset. The code is available at https://github.com/yjchen3250/TransMambaCC .},
  archive      = {J_APIN},
  author       = {Chen, Yangjian and Zhao, Huailin and Huang, Liangjun and Yang, Yubo and Kang, Wencan and Zhang, Jianwei},
  doi          = {10.1007/s10489-025-06912-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {TransMambaCC: Integrating transformer and pyramid mamba network for RGB-T crowd counting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning promotion policies with attention-based deep Q-networks. <em>APIN</em>, <em>55</em>(15), 1-15. (<a href='https://doi.org/10.1007/s10489-025-06914-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial services, personalized promotion strategies are critical for sustaining customer engagement and driving asset growth. We present FAT-DQN, a deep reinforcement learning framework for off-line environments that models sequential decision-making as a Markov Decision Process (MDP), where promotional actions influence future changes in customer assets under management (AUM). FAT-DQN extends the standard Deep Q-Network (DQN) architecture with a multi-head self-attention mechanism over promotion–reward histories augmented by learnable temporal encodings, and applies Feature-wise Linear Modulation (FiLM) to incorporate customer-segment embeddings. To improve robustness, we employ per-customer reward normalization and evaluate policies with both ranking-based metrics and counterfactual off-policy estimators. Empirical results on real promotion logs show that FAT-DQN consistently outperforms baseline methods, yielding a higher mean NDCG@3 (0.7744) compared to Batch-Constrained deep Q-learning (BCQ, 0.7325) and DQN (0.6852). It further improves alignment between predicted and realized outcomes, achieving a Spearman correlation of 0.2584, compared to 0.1619 for BCQ and 0.1522 for DQN. Counterfactual evaluations further show that FAT-DQN delivers consistently strong off-policy estimates, confirming its robustness across evaluation settings. These findings demonstrate that attention-based architectures with modulation offer a more effective and interpretable alternative to standard reinforcement learning approaches for personalized promotion planning in financial services.},
  archive      = {J_APIN},
  author       = {Xu, Yingnan and Wu, Xuchun and Li, Zhenjun and Liu, Congli and Zhang, Yansheng},
  doi          = {10.1007/s10489-025-06914-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Learning promotion policies with attention-based deep Q-networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal prompt learning with selective feature fusion: Towards robust cross-modal alignment. <em>APIN</em>, <em>55</em>(15), 1-28. (<a href='https://doi.org/10.1007/s10489-025-06919-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision–language models (VLMs) have shown impressive transferability but still struggle with robustness and generalization when applied to downstream tasks with limited supervision. To address these challenges, we propose a Selective Feature Fusion (SFF) framework that adaptively suppresses noisy visual regions and reinforces task-relevant cross-modal cues through lightweight, learnable gating. Our approach integrates text-guided visual masking and image-aware textual calibration into a unified pipeline, enabling more discriminative and semantically aligned multimodal representations. Comprehensive evaluations across nine widely used benchmarks demonstrate that our method consistently surpasses strong prompt-learning baselines under both few-shot and base-to-novel generalization settings. In particular, under the 8-shot scenario, our approach achieves the best overall accuracy, maintaining a clear margin over representative methods such as CoCoOp and MaPLe. These results highlight not only the robustness of our design but also its effectiveness in capturing cross-modal semantics under data-limited conditions. Further analyses, including ablation studies and qualitative visualizations, confirm that the proposed gating and calibration modules are complementary and play indispensable roles in improving performance. Taken together, this work provides a simple yet powerful strategy for enhancing the adaptability and generalization of VLMs in real-world scenarios.},
  archive      = {J_APIN},
  author       = {Han, Jiabao and Wang, Yahui and Zhong, Wei and Zhang, Ying and Yuan, Xichao},
  doi          = {10.1007/s10489-025-06919-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal prompt learning with selective feature fusion: Towards robust cross-modal alignment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifier enhancement based on credible sample selection for partial multi-label learning. <em>APIN</em>, <em>55</em>(15), 1-23. (<a href='https://doi.org/10.1007/s10489-025-06769-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) is a weakly supervised framework where each training sample is associated with several candidate labels, which include noisy labels. The main goal is to overcome the noise interference and achieve a well-trained classifier. Given that the sample features contain redundancy and the sample labels include noise, these factors can introduce interference during classifier training. Therefore, we aim to construct the sample set that prioritizes those with less noise, higher representativeness and confidence to improve the effectiveness of the model. To achieve this, we propose a new PML approach with classifier enhancement based on credible sample selection, called PML-CECS. Specifically, this paper first projects the feature space and label space into the subset space, enhancing the consistency of representation within the subset space by sharing projection information during this process. Then orthogonalization is applied to the subset space to reduce noise and redundant correlations, thereby improving the representativeness and reliability of the data. Next, the manifold structure reinforces the instance-level consistency between features and labels within the subset space. And leveraging the subset samples as new learning information further enhances the classifier’s performance. Finally, to mitigate erroneous correlations arising from noise interference, pseudo-labels are introduced and integrated into the model training. Extensive experiments have validated the feasibility of this approach.},
  archive      = {J_APIN},
  author       = {Mu, Jiaguo and Chen, Yu and Sun, Weijun and Wan, Zhenyu and Wang, Shengwei and Tao, Tao},
  doi          = {10.1007/s10489-025-06769-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Classifier enhancement based on credible sample selection for partial multi-label learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Question answering system based on the combination of large language model and knowledge graph. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06828-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large language models (LLMs) have achieved remarkable progress in natural language processing. However, their application in question-answering systems continues to face challenges such as insufficient credibility and interpretability of responses, as well as high computational resource demands. To address these issues, this paper proposes a question-answering system that integrates knowledge graphs with lightweight LLMs. Specifically, a lightweight front-end model based on BERT and T5 is employed to extract and transform logical forms from natural language queries, which are then executed on a knowledge graph. Subsequently, a smaller-scale LLM generates credible and interpretable answers based on these query results. Experimental results show that the proposed method achieves F1 scores of 75.6% and 76.8% on the WebQSP and GRAILQA datasets, respectively, surpassing other representative approaches. Furthermore, integrating the extracted knowledge with the ChatGLM-6B model significantly improves answer quality, increasing ratings by 112.8% for simple questions and 77.4% for complex questions, thus validating the effectiveness of our approach.},
  archive      = {J_APIN},
  author       = {Wang, Jihong and Zhang, Yichen and Liu, Wei},
  doi          = {10.1007/s10489-025-06828-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Question answering system based on the combination of large language model and knowledge graph},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-crafted narratives: An empirical study on generating interactive stories using generative pre-training transformers. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06833-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive storytelling, which has long captivated audiences, is embracing evolution. Dungeons & Dragons, a timeless example of open-ended interactive storytelling, has entered the digital realm, with platforms such as AI Dungeon employing large-language models to enhance the experience. Amid the growing utilization of Chat-GPT and concerns regarding AI’s potential to replace jobs, this empirical study examines the capability of GPT-3.5 and GPT-4 to autonomously generate an engaging interactive narrative, alongside the necessary Twine code for compiling the story. To assess the quality of the generated story, user perceptions of its authorship, and its future, an accompanying survey was conducted. The result shows that even though some concerns arise regarding authorship, AI is promising in the field of generating interactive stories.},
  archive      = {J_APIN},
  author       = {de Souza Mendes, Ana Carolina and Adsero, Mason and Palicka, Joshua and Zholdoshov, Nurulla and Zhao, Xin},
  doi          = {10.1007/s10489-025-06833-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {AI-crafted narratives: An empirical study on generating interactive stories using generative pre-training transformers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPSP-CLIP: Learning generic pseudo-state prompts for flexible zero-shot anomaly detection. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06843-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale foundation models such as Contrastive Language-Image Pre-training (CLIP) have shown great potential in zero-shot anomaly detection (ZSAD) task, allowing a single model to generalize to unseen categories without fine-tuning on specific classes. However, existing ZSAD methods often rely on rigid prompt designs, which makes them difficult to adapt to the diverse characteristics of industrial products. Additionally, the need to manually define category-specific and state-specific prompts limits their scalability and generalization. This paper proposes a generic pseudo-state prompting model based on CLIP (GPSP-CLIP) to address these challenges. The motivation behind GPSP-CLIP is to develop a flexible prompting method capable of representing normal and anomalous conditions across various applications without relying on predefined text prompts. Technically, GPSP-CLIP employs fully learnable parameters to generate broad, pseudo-state text features, enabling generalization across different industrial contexts. By employing distinct prompt learning strategies for anomaly classification and segmentation, GPSP-CLIP optimizes each task independently. This enables the model to effectively capture high-level semantics through global prompts while identifying fine-grained defect patterns via local prompts. Experimental results on the well-known MVTec and VisA datasets demonstrate improved performance, with a 1.8% improvement in AP for anomaly classification and a 1.3% gain in AUPRO for anomaly segmentation compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Hu, Weiyu and Zhou, Shubo and Gao, Yongbin and Jiang, Xue-Qin},
  doi          = {10.1007/s10489-025-06843-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {GPSP-CLIP: Learning generic pseudo-state prompts for flexible zero-shot anomaly detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards identification and explainable localization of slopes in autonomous excavation: A feature fused CAM approach. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06881-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous earthmoving requires excavators to identify and localize slopes within complex environments while operating with limited computational resources. To address this challenge, we propose an explainable localization method that leverages the explainability of machine learning (ML) models for slope identification and localization, which also guides the excavator in optimal digging point determination. Our approach integrates a modified residual neural network with joint features derived from Class Activation Mapping (CAM), enhanced through transfer learning to fine-tune a pre-trained model for the target task. Evaluations on public SODA dataset demonstrate significant improvements in localization performance, with a 45.6% increase in the Intersection over Union (IoU) metric compared to the original CAM. Further performance gains are observed when preprocessing based on identification precedes localization, with IoU improving by over 70%. Furthermore, we constructed a few-shot slope dataset to validate the method’s efficacy under low-cost and resource-constrained conditions. The results indicate that our approach enables continuous explainable localization, effectively guiding an unmanned excavator in autonomous earthmoving. The proposed approach proves highly practical for engineering applications, addressing the challenges of large-scale datasets and high computational resource demands, thereby providing an effective technical pathway for applying ML methods to the automation of construction machinery.},
  archive      = {J_APIN},
  author       = {Zou, Xinrui and Wang, Ziwei and Song, Yancheng and Jia, Liangjiu and Wang, Gangju and Liu, Guangjun},
  doi          = {10.1007/s10489-025-06881-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Towards identification and explainable localization of slopes in autonomous excavation: A feature fused CAM approach},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D efficient and essentialized swin transformer network for alzheimer’s disease diagnosis. <em>APIN</em>, <em>55</em>(15), 1-15. (<a href='https://doi.org/10.1007/s10489-025-06884-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods (e.g., convolutional neural networks, CNNs) have been widely applied to Alzheimer’s disease diagnosis based on structural magnetic resonance imaging (sMRI) data. However, CNN-based methods face significant Limitations in capturing the global feature distribution of the whole brain. Transformer-based models have shown promise in addressing this issue, but they often sacrifice local feature sensitivity. Moreover, the large number of parameters in Transformer-based models results in a strong dependence on large-scale datasets, which is difficult to satisfy in real-world 3D medical imaging scenarios. Through comprehensive consideration, we propose a 3D Efficient and Essentialized Swin Transformer Network (E2STN) to strike a balance between being lightweight and comprehensive feature extraction, thereby boosting Alzheimer’s disease diagnosis performance in 3D dataset scenarios. Specifically, E2STN includes four modules: an Efficient Swin Transformer (EST) module for identifying global structural information and being lightweight to reduce reliance on large-scale datasets, which is a novel task-oriented Transformer architecture; a Focused Feature Enhancement Convolution Unit (FFE-CU) for enhancing lesion details, thereby compensating for the limited perception of fine-grained pathological information by the Transformer; a Disease Risk Map generator (DRMg) for visualizing pathological regions; and an ROI-based classifier for precise categorization. Our proposed method has been validated by two diagnosis tasks (i.e., Alzheimer’s disease diagnosis and mild cognitive impairment conversion prediction) on the ADNI dataset. Compared to several state-of-the-art methods, our model demonstrates superior performance.},
  archive      = {J_APIN},
  author       = {Huang, Shengchao and Dai, Qun},
  doi          = {10.1007/s10489-025-06884-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {A 3D efficient and essentialized swin transformer network for alzheimer’s disease diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection based on information entropy with variable precision fuzzy mixed granularity. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06890-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rough set theory allows defining different fuzzy relationships for different attribute types to quantify the similarity between objects. Meanwhile, information entropy, a powerful tool for quantifying uncertainty, is further extended within this framework to fuzzy rough set-based information entropy. The granularity division of traditional fuzzy entropy usually relies on fuzzy similarity relationships. In this paper, we first define variable precision mixed fuzzy granularity, combine it with fuzzy entropy to construct the information entropy based on variable precision mixed fuzzy granularity, and define fuzzy granularity entropy (FGe), fuzzy granularity joint entropy (FGJe), fuzzy granularity conditional entropy (FGCe), and fuzzy granularity mutual information (FGMI), and study the relationship and related properties among them. Then the importance function for evaluating the importance of features is constructed using FGMI, which lays the foundation for the feature selection (FS) algorithm. To evaluate the performance of the algorithm, numerical experiments are conducted on 15 public datasets and compared with other algorithms. The experimental results show that the method shows good adaptability and FS ability for handling different types of data.},
  archive      = {J_APIN},
  author       = {Wang, Jiaxin and Wang, Jingqian and Zhang, Xiaohong and Liu, Jun},
  doi          = {10.1007/s10489-025-06890-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection based on information entropy with variable precision fuzzy mixed granularity},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient relational context perception for knowledge graph completion. <em>APIN</em>, <em>55</em>(15), 1-11. (<a href='https://doi.org/10.1007/s10489-025-06902-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) provide a structured representation of knowledge but often suffer from challenges of incompleteness. To address this, link prediction or knowledge graph completion (KGC) aims to infer missing new facts based on existing facts in KGs. Previous knowledge graph embedding models are limited in their ability to capture expressive features, especially when compared to deeper, multi-layer models. These approaches also assign a single static embedding to each entity and relation, disregarding the fact that entities and relations can exhibit different behaviors in varying graph contexts. Due to complex context over a fact triple of a KG, existing methods have to leverage complex non-linear context encoder, like transformer, to project entity and relation into low dimensional representations, resulting in high computation cost. To overcome these limitations, we propose the Triple Receptance Perception (TRP) architecture–an attention-free and lightweight encoder inspired by RWKV–that models sequential dependencies to capture the dynamic contextual semantics of entities and relations. Then we use Tucker tensor decomposition to calculate triple scores, providing robust relational decoding capabilities. This integration allows for more expressive representations. Experiments on benchmark datasets such as YAGO3-10, UMLS, FB15k, and FB13 in link prediction and triple classification tasks demonstrate that our method performs better than several state-of-the-art models, proving the effectiveness of the integration.},
  archive      = {J_APIN},
  author       = {Tu, Wenkai and Wan, Guojia and Shang, Zhengchun and Du, Bo},
  doi          = {10.1007/s10489-025-06902-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-11},
  shortjournal = {Appl. Intell.},
  title        = {Efficient relational context perception for knowledge graph completion},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time task scheduling strategy for 3D printing cloud platforms in health scenes. <em>APIN</em>, <em>55</em>(15), 1-17. (<a href='https://doi.org/10.1007/s10489-025-06907-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In health scenes, 3D Printing Cloud Platform (3DPCP) needs to cope with unpredictable fluctuations in tasks and resources, but traditional scheduling methods have problems such as incomplete consideration of factors, poor optimization, and weak dynamic adaptability, which make it difficult to meet real-time scheduling requirements. To this end, the real-time task scheduling problem of 3DPCP for health scenes is defined, a real-time task scheduling model is established, the design time of user personalized services is considered, a rescheduling scheme is designed in combination with task variations and device variations, and a scheduling strategy that incorporates dynamic mechanisms and improved multi-objective greywolf optimization algorithms is proposed in order to minimize the integrated scheduling cost and the average delivery time of the product. The findings of simulation experiments show that when equipment changes are not considered, compared with the optimal heuristic algorithm in this field, the average cost of the proposed algorithm is reduced by 2014.1 yuan, and the average delivery time is shortened by 1.52 h. When equipment changes are considered, compared with the multi-objective Genetic Algorithm Dynamic Strategies (GADS), the average cost of the proposed algorithm is reduced by 2984.57 yuan, and the average delivery time is shortened by 0.39 h, which validates the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {He, Jianjia and Wu, Jian and Ni, Jingran and Zhang, Yuning and Siau, Keng Leng},
  doi          = {10.1007/s10489-025-06907-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Real-time task scheduling strategy for 3D printing cloud platforms in health scenes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A manufacturing knowledge graph completion method based on a lightweight dual encoding model. <em>APIN</em>, <em>55</em>(15), 1-17. (<a href='https://doi.org/10.1007/s10489-025-06909-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring stable equipment operation is crucial for manufacturing. Intelligent maintenance decisions powered by manufacturing knowledge graphs can reduce reliance on manual maintenance and enhance efficiency. However, existing knowledge graphs face challenges such as sparse information and complex relationship modeling. Knowledge graph completion can predict missing relationships and entities to enrich the graph. Current completion methods neglect semantic information in entity descriptions, leading to incomplete data, while encoding triples and descriptions increases computational costs. Therefore, this paper proposes a Lightweight Dual Encoding Model (LDEM) for manufacturing knowledge graph completion. LDEM uses ALBERT to encode entity descriptions and captures rich semantics through precomputed embeddings. The graph attention module aggregates neighborhood information, and ConvKB decodes embeddings into predictions. The dataset used in this study comes from a vehicle welding workshop in Chongqing, China. Experiments show that LDEM outperforms state-of-the-art models in all metrics, achieving 80.1 points in Hits@10 and demonstrating superior ability to capture entity relationships and semantic information, thereby enhancing the completion of the manufacturing knowledge graph.},
  archive      = {J_APIN},
  author       = {Qi, Xing and Shen, Xiaoyu and Zhang, Yucheng and Yang, Bo and Xie, Keqiang and Dong, Nan},
  doi          = {10.1007/s10489-025-06909-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A manufacturing knowledge graph completion method based on a lightweight dual encoding model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on full lifecycle health management of permanent magnet synchronous electric drum driven by digital twin with dynamic update. <em>APIN</em>, <em>55</em>(15), 1-28. (<a href='https://doi.org/10.1007/s10489-025-06910-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, research on fault Prognostics and Health Management (PHM) based on Digital Twin mainly focuses on integrating real-time data from various sources to facilitate comprehensive product inspection and health management. However, existing DT research faces three main theoretical bottlenecks: the lack of dynamic evolution mechanisms in multi-physics coupled modeling, static models’ difficulty in adapting to the drift of equipment degradation characteristics, and health status assessment’s reliance on prior fault samples. To address these issues, this paper proposes a comprehensive lifecycle dynamic management method for the Tubular Permanent Magnet Synchronous Electric Drum (TPMSED), by constructing a Dual-service Lifecycle Management Digital Twin Model (DSL-DT) that achieves deep integration of physical entities and virtual spaces. Firstly, a multi-physics coupled dynamic model is established, integrating the nonlinear interactions of electromagnetic fields, temperature fields, and dynamic fields. This is achieved through a combination of finite element simulation and data-driven approaches, addressing the challenge of characterizing equipment performance degradation under complex operating conditions. Secondly, an innovative dual dynamic adjustment mechanism for compensator updates and parameter updates is designed, utilizing ridge regression algorithms and adaptive gradient algorithms to achieve online optimization of model parameters, effectively suppressing model mismatch during the degradation process. Lastly, a health index (HI)-based state assessment method is proposed, which triggers model updates by comparing characteristic deviations with thresholds, significantly enhancing the accuracy of Remaining Useful Life (RUL) predictions. Experimental validation on an intelligent conveying system development platform demonstrates that this method can accurately track the performance evolution of equipment throughout its lifecycle, providing a new theoretical paradigm and technical pathway for health management of complex electromechanical systems.},
  archive      = {J_APIN},
  author       = {Chen, Wei and Wu, Weimin and Zhang, Mei and Li, Huashun and Shi, Qing},
  doi          = {10.1007/s10489-025-06910-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {Research on full lifecycle health management of permanent magnet synchronous electric drum driven by digital twin with dynamic update},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way density peak clustering in incomplete information systems. <em>APIN</em>, <em>55</em>(15), 1-14. (<a href='https://doi.org/10.1007/s10489-025-06911-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of data in the information systems will result in uncertainty in the classification of objects, and the fringe region of the cluster in the three-way clustering reflects the uncertainty of the clustering results, which can fit the incomplete information system well. Consequently, this paper proposes a three-way clustering framework in incomplete information systems, drawing on the density peak clustering algorithm and the model of three-way decision. Firstly, this paper defines the reflexive binary relation in incomplete information systems and determines the center of the corresponding object class. Then, the density peak clustering algorithm is utilized to identify the optimal clustering centers among all object class centers. Subsequently, the membership degree and relative loss function matrix of the object under each cluster center are defined according to the distance relationship between each object and all cluster centers. Finally, the clustering rules are obtained by the minimum risk decision theory, and the initial clustering results are processed to meet the three-way clustering criteria. In the experimental section of this paper, two sets of experiments are designed to show the clustering accuracy of the proposed algorithm and the influence of parameters on the clustering results.},
  archive      = {J_APIN},
  author       = {Li, Zhao and Mi, Ju-sheng and Li, Lei-jun},
  doi          = {10.1007/s10489-025-06911-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Three-way density peak clustering in incomplete information systems},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive approach to enhance fault prediction through code-comment analysis with CodeBERT. <em>APIN</em>, <em>55</em>(15), 1-35. (<a href='https://doi.org/10.1007/s10489-025-06820-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Fault Prediction (SFP) is a key step in enhancing the quality and accuracy of source code. This study examines various combinations of software code metrics and CodeBERT approaches, aiming to extract valuable information from both code and comments for comprehensive code embedding. This study employs advanced code-embedding methodologies, incorporating code and comments and utilizing the state-of-the-art CodeBERT model. The study uses the stacking classifier ensemble approach, with logistic regression and XGBoost as the ultimate estimators, alongside 22 classifiers across 41 diverse datasets. The results indicate that the proposed approach successfully predicts faulty source codes based on accuracy and Area under Curve (AUC). Through the experiment, the proposed approach achieved an accuracy of 80.39% and an AUC of 0.7458.},
  archive      = {J_APIN},
  author       = {Yadav, Monika and Kumar, Lov and Passricha, Vishal},
  doi          = {10.1007/s10489-025-06820-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-35},
  shortjournal = {Appl. Intell.},
  title        = {A comprehensive approach to enhance fault prediction through code-comment analysis with CodeBERT},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSV-pose: Pose estimation based on geometric similarity voting. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06853-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation is a fundamental problem in 3D computer vision and has gained significant attention with the rapid advancements in autonomous driving, robotics, and augmented reality. Traditional voting-based approaches often suffer from reduced accuracy when dealing with partially observed objects. To overcome this limitation, our method incorporates a superpoint matching network to compute local geometric similarities, which effectively guides the voting process and enhances pose estimation robustness. Experimental results demonstrate that our approach achieves comparable performance to the current state-of-the-art (SOTA) method, GPV-Pose, under standard conditions. More importantly, in robustness tests with incomplete objects, our method significantly surpasses GPV-Pose. For instance, under a 20% incompleteness ratio, the accuracy of GPV-Pose drops by 61.6% under the $$5^{\circ }2\,\text {cm}$$ criterion, whereas our method experiences only a 21.8% reduction.},
  archive      = {J_APIN},
  author       = {Zhao, Xi and Zhang, Yuekun and Wu, Jinji},
  doi          = {10.1007/s10489-025-06853-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {GSV-pose: Pose estimation based on geometric similarity voting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
