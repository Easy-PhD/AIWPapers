<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NCA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nca">NCA - 67</h2>
<ul>
<li><details>
<summary>
(2025). Optimized ensemble machine learning cancer classification system for clinical decision-making. <em>NCA</em>, <em>37</em>(29), 24483-24498. (<a href='https://doi.org/10.1007/s00521-025-11599-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical and healthcare informatics research is accelerating at an unprecedented rate due to the growth and accumulation of large volumes of biological and clinical data. There are new opportunities to use big data to uncover new insights and develop creative approaches to improve the quality of cancer treatment. The microarray gene expression profile is used to efficiently and accurately classify cancer cells for clinical decision-making. In this study, a cancer classification system using an optimized ensemble machine learning approach which is based on artificial bee colony (ABC) optimization and an ensemble of machine learning classifiers is proposed as a result of this effort to distinguish between acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML) utilizing microarray gene expression patterns. The relevant cancer features are optimally selected using the ABC technique to improve the performance of the proposed ensemble learning-based classification system. Moreover, the gene expression dataset is balanced using an upsampling technique and an equal number of ALL and AML records have been used for experimentation. The proposed methodology’s accuracy and other performance metrics are looked at, and the suggested model is contrasted to several base machine learning algorithms based on performance criteria to show how useful it is. Furthermore, to demonstrate the importance of the suggested strategy, receiver operating characteristics (ROC) analysis has been performed, and it is seen that the area under the ROC curve of the proposed approach is higher compared to the existing approaches.},
  archive      = {J_NCA},
  author       = {Amma, N. G. Bhuvaneswari and Amma, N. G. Nageswari},
  doi          = {10.1007/s00521-025-11599-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24483-24498},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized ensemble machine learning cancer classification system for clinical decision-making},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breast cancer classification by converging tumour region probability density and texture feature-based clustering. <em>NCA</em>, <em>37</em>(29), 24461-24481. (<a href='https://doi.org/10.1007/s00521-025-11596-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel technique for the segmentation and classification of regions in breast tumour images by integrating a convergence-based density model, coupled with texture feature-based clustering. The segmentation process starts with an active contour that estimates probability densities for foreground, background, and tumour regions. A key advantage of the proposed method is its independence from an annotated training set. Thus, it reduces sensitivity to dataset variability by using intensity-driven convergence. To address overlapping structures in mammographic images, an edge-path contour-splitting methodology is employed for accurate boundary separation. Finally, a probabilistic neural network (PNN) is used to classify the tumour regions based on texture features. Both qualitative and quantitative results are presented to demonstrate the effectiveness of the proposed method. The proposed method achieves an accuracy of 92%, with a sensitivity of 95.4% and specificity of 94.2%. Performance evaluation with ROC analysis confirms the robustness and diagnostic reliability of the proposed approach in identifying breast tumours.},
  archive      = {J_NCA},
  author       = {Kumari, Bersha and Nandal, Amita and Dhaka, Arvind and Alhudhaif, Adi and Polat, Kemal},
  doi          = {10.1007/s00521-025-11596-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24461-24481},
  shortjournal = {Neural Comput. Appl.},
  title        = {Breast cancer classification by converging tumour region probability density and texture feature-based clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HR-YOLOv8: An innovative model to detect mitosis and identify cancer regions in histopathological images. <em>NCA</em>, <em>37</em>(29), 24441-24460. (<a href='https://doi.org/10.1007/s00521-025-11594-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pathologists use histopathology images to identify breast cancer. Under these circumstances, the identification of mitosis in tissues becomes a potent prognostic marker for breast cancer. Mitosis serves as a vital marker for pinpointing areas of tumor aggressiveness and assessing the probability of disease recurrence. The HR-YOLOv8 model is presented in this paper as a highly accurate way to identify regions of breast cancer and detect mitosis. There are two phases to the study. Through the integration of the HRNet blocks into the YOLOv8 backbone, mitosis is identified in the first stage. The MST algorithm locates the breast cancer region in the second stage. The MST algorithm is employed to merge separate nodes, enabling the identification of cancer regions. HR-YOLOv8 is evaluated on MIDOG21, TUPAC16, and MiDeSeC that datasets are specifically focused on breast cancer, using metrics such as accuracy and F1-score for mitosis detection and AUC, sensitivity, and specificity for breast cancer regions. The results obtained from the study show that the proposed model can identify mitosis and recognize breast cancer regions with high precision.},
  archive      = {J_NCA},
  author       = {Nemati, Nooshin and Samet, Refik},
  doi          = {10.1007/s00521-025-11594-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24441-24460},
  shortjournal = {Neural Comput. Appl.},
  title        = {HR-YOLOv8: An innovative model to detect mitosis and identify cancer regions in histopathological images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for efficient topic modeling. <em>NCA</em>, <em>37</em>(29), 24421-24439. (<a href='https://doi.org/10.1007/s00521-025-11593-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of large language models (LLMs) in research is becoming increasingly prevalent, as they offer advanced capabilities in processing and generating human-like text. However, this advancement comes with a significant trade-off in terms of time and computational costs. In this paper, we demonstrate that analyzing large text datasets with the use of LLMs can be performed efficiently in terms of both time and energy. For this purpose, we utilize the Llama pre-trained model. In more detail, we study the topic modeling task where the goal is to discover and identify topics in large text corpora. The basis of our approach is a hierarchical divisive clustering technique that clusters the data based on their semantic similarity, after employing a Sentence-BERT encoder, pre-trained on a variety of data across different tasks. Then, using an LLM, we identify topics for representative samples from each cluster. Additionally, we introduce a new evaluation method that leverages the capabilities of LLMs to assess the alignment between discovered topics and ground truth labels, providing a robust validation metric. Our findings indicate that it is possible to effectively reduce the computational cost of the topic modeling process compared to the direct application of LLMs and BERTopic, while simultaneously enhancing inference time and overall efficiency, thereby surpassing the current state-of-the-art capabilities of BERTopic.},
  archive      = {J_NCA},
  author       = {Theocharopoulos, Panagiotis C. and Anagnostou, Panagiotis and Georgakopoulos, Spiros V. and Tasoulis, Sotiris K. and Plagianakos, Vassilis P.},
  doi          = {10.1007/s00521-025-11593-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24421-24439},
  shortjournal = {Neural Comput. Appl.},
  title        = {Large language models for efficient topic modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplex network-based representation of vision transformers for visual explainability. <em>NCA</em>, <em>37</em>(29), 24385-24420. (<a href='https://doi.org/10.1007/s00521-025-11591-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enormous growth of artificial intelligence (AI), and deep learning (DL) in particular, has led to the widespread use of these systems in a variety of contexts. One DL model capable of addressing complex computer vision tasks is the vision transformer (ViT). Despite its huge success, the reasoning behind the inferences it makes is often unclear, which poses significant challenges in critical scenarios. In this paper, we propose a new approach called MUltiplex Transformer EXplainer (MUTEX), which aims to explain the inferences made by ViTs. MUTEX combines multiplex network-based representations of attention matrices and mask perturbation approaches to provide insight into the inference process of ViTs. By mapping the attention layers of a ViT into a multiplex network, MUTEX is able to analyze the relationships between different parts of the input image and identify the image patches that most influence the inference process. We tested MUTEX on a subset of ImageNet and on BloodMNIST and compared its performance with that of existing visual explainability approaches. In addition, to assess the robustness and adaptability of MUTEX, we conducted a qualitative analysis, along with a hyperparameter and ablation study, which allowed us to further appreciate its potential in visual explainability of ViT.},
  archive      = {J_NCA},
  author       = {Marchetti, Michele and Traini, Davide and Ursino, Domenico and Virgili, Luca},
  doi          = {10.1007/s00521-025-11591-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24385-24420},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multiplex network-based representation of vision transformers for visual explainability},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced arrhythmia detection using spiking neural networks: An in-depth analysis of ECG data from the MIMIC-IV clinical database. <em>NCA</em>, <em>37</em>(29), 24365-24384. (<a href='https://doi.org/10.1007/s00521-025-11585-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of electrocardiogram (ECG) signals is essential for detecting arrhythmias such as bradycardia, ventricular tachycardia, and atrial fibrillation. This study utilizes MIMIC-IV-ECG dataset, containing over 800,000 recordings, to assess the effectiveness of spiking neural networks (SNNs) in arrhythmia finding. Various deep learning architectures including hybrid, leaky integrate-and-fire networks, spiking convolutional neural networks (SCNNs), convolutional spiking neural networks, S-RNN, and LSTM-SNN, are trained using key ECG features, with performance enhanced through data augmentation and feature engineering. Our findings identified the innovative SCNN demonstrate outstanding arrhythmia classification ability at a highly to notable 97% accuracy; while hybrid models like norse-hybrid and dense-spike show their own capabilities by incorporating traditional deep learning architectures with the concept of spiking neurons and offer additional performance improvements. These findings highlight the potential of neuromorphic computing for ECG analysis, with future work focusing on real-time processing and clinical scalability.},
  archive      = {J_NCA},
  author       = {Verma, Gunjan and Gocher, Honey and Verma, Sweety},
  doi          = {10.1007/s00521-025-11585-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24365-24384},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced arrhythmia detection using spiking neural networks: An in-depth analysis of ECG data from the MIMIC-IV clinical database},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid algorithm: Long short-term memory-genetic algorithm for optimizing uncertain revenue of wind farms in electricity markets. <em>NCA</em>, <em>37</em>(29), 24345-24364. (<a href='https://doi.org/10.1007/s00521-025-11582-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a robust hybrid optimization algorithm that combines artificial intelligence with genetic algorithms (GA) to maximize revenue for electricity generation plants, addressing the challenges posed by wind generation uncertainty in liberalized power markets. The novel method leverages the prediction capabilities of the long short-term memory algorithm, a deep learning methodology, to forecast superior genetic traits. These enhanced individuals are then incorporated into the population, accelerating the evolutionary process of the GA and improving its ability to achieve local optimality. The effectiveness of the proposed algorithm is demonstrated through experimental validation of the revenue optimization problem, aiming to increase wind energy utilization by reducing compensation risks related to electricity production fluctuations in the market. The performance of the algorithm in recommending wind power bidding capacity is evaluated using the IEEE 30-bus and 118-bus power system model. Comparative analysis shows that auctioned wind power consumption increased by 12% compared to the traditional GA and by more than 20% compared to the mixed integer linear programming (MILP) method, with a corresponding revenue increase of 7% compared to the MILP scenario. Furthermore, comparison with previous advanced GA research on the optimal power flow problem indicates not only a reduction in the number of generations but also significant savings in computation time; the effectiveness of the approach is confirmed by a more than 22% reduction in the NFFE index (the number of fitness function evaluations).},
  archive      = {J_NCA},
  author       = {Dinh, Ngoc Sang and Dinh, Le Song Binh and Truong, Viet Anh},
  doi          = {10.1007/s00521-025-11582-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24345-24364},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel hybrid algorithm: Long short-term memory-genetic algorithm for optimizing uncertain revenue of wind farms in electricity markets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting artificial rabbit optimization for solving classification problem: Benchmark dataset analysis. <em>NCA</em>, <em>37</em>(29), 24325-24344. (<a href='https://doi.org/10.1007/s00521-025-11572-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, bio-inspired metaheuristic algorithms have been widespread in various application areas due to their straightforward implementation and ability to handle and solve complex problems. Despite the widespread adoption of metaheuristic algorithms, there is a lack of adaptation of the artificial rabbit optimization (ARO) algorithm to solve classification tasks. Specifically, the ARO algorithm has primarily been used for hyperparameter tuning machine learning algorithms or to select the best feature subset within a wrapper feature selection without cooperating in the classification process. This paper has introduced a new direct adaptation of the ARO algorithm for classification tasks. This adaptation was constructed by directly using the ARO algorithm to find the optimal centroids that could solve the classification task. Therefore, the ARO algorithm was shifted directly to the classification task by identifying the optimal centroid for each class label and minimizing the number of misclassified instances in the training data. The proposed direct adaptation of the ARO algorithm for classification tasks was tested and evaluated using eleven benchmark datasets from various domains. We conducted in-depth investigations of the other seven bio-inspired optimization algorithms alongside the ARO algorithm. The accomplished results demonstrated the robustness and effectiveness of the ARO algorithm for solving the classification problem compared to other bio-inspired optimization algorithms. Consequently, by extending ARO to focus on optimal centroid identification, our approach has enhanced the performance of classification tasks, thereby improving the effectiveness of decision-making models.},
  archive      = {J_NCA},
  author       = {Almseidin, Mohammad},
  doi          = {10.1007/s00521-025-11572-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24325-24344},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adapting artificial rabbit optimization for solving classification problem: Benchmark dataset analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient task offloading based on modified elk herd optimizer for minimizing response times in fog-enabled IoT. <em>NCA</em>, <em>37</em>(29), 24303-24323. (<a href='https://doi.org/10.1007/s00521-025-11569-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fog computing has emerged as a prominent research because of the widespread adoption and continuous advancements in Internet of Things (IoT) technologies. Fog nodes (FNs) offer storage and computational capabilities to resource-constrained IoT devices, enabling them to support IoT applications with high computing needs. Moreover, closeness FNs to IoT devices ensure they meet the latency demands of IoT applications. However, increasing the need to offload the tasks, combined with limited IoT resources, requires to development an efficient task offloading. To address this challenge, a task offloading approach utilizing the modified elk herd optimizer (MEHO) is proposed to assign tasks to FNs. MEHO is designed as an optimization approach aimed at minimizing response time. Comprehensive simulations show that MEHO outperforms other methods under various numbers of FNs, service rate, and rate of arrival data. MEHO achieves a reduction in average response time for maximum tasks by 12%, 16%, 18%, 19%, 26%, and 41% compared to modified sparrow search algorithm, sparrow search algorithm, artificial bee colony optimization, ant colony optimization, particle swarm optimization, and round robin, respectively.},
  archive      = {J_NCA},
  author       = {Alfawaz, Oruba and Khedr, Ahmed M. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-025-11569-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24303-24323},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient task offloading based on modified elk herd optimizer for minimizing response times in fog-enabled IoT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft computing techniques for predicting thermal conductivity of bentonite–fly ash/-sand composite materials. <em>NCA</em>, <em>37</em>(29), 24281-24301. (<a href='https://doi.org/10.1007/s00521-025-11568-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bentonite–sand/fly ash-based thermal backfill materials are used as a heat transfer medium between the heat sources (e.g., underground power cable) and near-field. The characterization of materials in the laboratory, as a thermal backfill material, often requires extensive field as well as laboratory work, which is quite expensive and time-consuming. Therefore, this paper aims to develop a soft computing (SC) technique to predict the thermal conductivity due to its ability to handle complex, nonlinear and uncertain relationships in soil behavior. To achieve this goal, four SC algorithms, namely artificial neural network (ANN), ANN–PSO (particle swarm optimization), adaptive neural network-based fuzzy inference system (ANFIS) and extreme gradient boosting (XGB), were utilized based on the experimental database considering compaction state and physical properties of backfill as input variables. The performance of different SC techniques was evaluated based on scatter plots, statistical indices, Taylor’s diagrams and rank analysis. The results exhibited that XGB predicts more accurately than ANFIS, ANN and ANN–PSO. Finally, the influence and significance of the input parameters on XGB model performance are highlighted using Shapley additive explanation (SHAP) analysis. The findings demonstrated that the water content, dry density and particle size content had the most significant impact on the thermal conductivity of bentonite–sand/fly ash-based backfill material.},
  archive      = {J_NCA},
  author       = {Bharti, Vishakha and Sah, Pawan Kishor and Kumar, Shiv Shankar and Das, Bhabani Shankar},
  doi          = {10.1007/s00521-025-11568-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24281-24301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Soft computing techniques for predicting thermal conductivity of bentonite–fly ash/-sand composite materials},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing monocular depth estimation with an advanced encoder-decoder architecture. <em>NCA</em>, <em>37</em>(29), 24265-24280. (<a href='https://doi.org/10.1007/s00521-025-11566-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant technological advancements have been made in autonomous navigation, impacting various fields such as robotics, autonomous vehicles, and unmanned aerial vehicles. These systems typically use the distances of surrounding objects as input. Monocular depth estimation, which involves estimating depths from a single RGB image, plays a crucial role in this context. In this study, we proposed an encoder-decoder model for monocular depth estimation. Additionally, we introduced a weighted loss function designed to minimize depth image reconstruction errors and penalize distortions in the scene domain of the depth map. The proposed model was evaluated using the NYU Depth V2 dataset, and the results surpassed those of state-of-the-art models on the same dataset. Specifically, our model achieved a validation accuracy of 0.9823, an average relative error (rel) of 0.04713 and a root mean square error (RMSE) of 0.2372, representing significant reductions of 60% and 49%, respectively, compared to contemporary techniques, even with a small training dataset.},
  archive      = {J_NCA},
  author       = {El-Alfy, Yasser and Baroudi, Uthman and Luqman, Hamzah},
  doi          = {10.1007/s00521-025-11566-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24265-24280},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing monocular depth estimation with an advanced encoder-decoder architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViT-DtC: Vision transformer-based design-to-code framework for code generation from generated UI designs and hand-drawn sketches. <em>NCA</em>, <em>37</em>(29), 24243-24264. (<a href='https://doi.org/10.1007/s00521-025-11565-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code generation in software development from various types of user interface (UI) design images can significantly reduce the manual effort required, accelerate development timelines, and facilitate collaboration between designers and developers. Previous studies have restricted the utilized types of UI designs to either generated UI designs or hand-drawn sketches separately, lacking the automatic detection capability for the design type. Moreover, studies frequently necessitated complex datasets with bounding box annotations and sophisticated computer vision preprocessing steps, especially within the context of hand-drawn sketches. In this paper, we introduce the novel comprehensive Vision Transformer-based Design-to-Code (ViT-DtC) framework, which tackles image classification and code generation from both generated UI designs and hand-drawn sketches by harnessing the capabilities of Vision Transformers. The proposed ViT-DtC undergoes fine-tuning to classify UI designs into specific categories, including web, iOS, Android, and hand-drawn sketches. Subsequently, the design is directed to the appropriate code generator to generate domain-specific language (DSL) tokens for UI elements based on the identified design type. The experimental results exhibited an exceptional proficiency in accurately classifying all designs. Employing a greedy search strategy, the proposed ViT-DtC framework achieved an average accuracy of 97.28% in generating UI elements on the modified web dataset (without capturing their state or color information). In iOS and Android designs, an average accuracy of 82.4% and 81.1% was achieved, respectively. Remarkably, when extended to handle hand-drawn sketches, an average accuracy of 84.8% was maintained.},
  archive      = {J_NCA},
  author       = {Ahmed, Areeg and Azab, Shahira and Moussa, Sherin M. and Abdelhamid, Yasser},
  doi          = {10.1007/s00521-025-11565-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24243-24264},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViT-DtC: Vision transformer-based design-to-code framework for code generation from generated UI designs and hand-drawn sketches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EWOSCA: An enhanced walrus optimizer-based secure clustering approach for IoT-based WSNs under adversarial contexts. <em>NCA</em>, <em>37</em>(29), 24209-24242. (<a href='https://doi.org/10.1007/s00521-025-11564-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an integral constituent of the Internet of Things (IoT), wireless sensor networks (WSNs) are revolutionizing different aspects of everyday life through intelligent and cost-effective applications. The dual challenges of security and efficiency are the major concerns facing any WSN deployment. Clustering is widely recognized as one of the most effective strategies for enhancing the WSN lifespan. While cluster heads (CHs) serve a pivotal role by managing data aggregation and communication, if CHs are compromised, the integrity of the collected data is lost, posing a significant risk to the network’s reliability and effectiveness. This work proposes an enhanced walrus optimizer-based secure clustering approach (EWOSCA) for IoT-based WSNs under adversarial contexts. An enhanced walrus optimizer (EWO) is developed to address the key shortcomings of the original WO. It incorporates an adaptive inertia weight strategy to better balance exploration and exploitation, a transverse crossover mechanism to tackle premature stagnation by enhancing diversity and generating high-quality solutions, and a colony predation strategy (CPS) to tackle the limited adaptability by dynamically refining the search process using the best-performing solutions. The EWOSCA approach adapts EWO and prioritizes the selection of secure, reliable, and energy-efficient CHs. Furthermore, an Adaptive weighted average function, AwE(), is devised and utilized while designing the fitness function for adapting the algorithm in response to varying network conditions over time. Simulation results reveal that EWOSCA can effectively handle varying rates of malicious or compromised nodes and surpasses recent schemes in terms of effective clustering, energy efficiency, reliability, and overall WSN lifetime.},
  archive      = {J_NCA},
  author       = {Khedr, Ahmed M. and V., Pravija Raj P. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-025-11564-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24209-24242},
  shortjournal = {Neural Comput. Appl.},
  title        = {EWOSCA: An enhanced walrus optimizer-based secure clustering approach for IoT-based WSNs under adversarial contexts},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal MRI data augmentation and attention-based modeling for interpretable brain tumor classification. <em>NCA</em>, <em>37</em>(29), 24191-24207. (<a href='https://doi.org/10.1007/s00521-025-11561-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from MRI images is crucial for guiding treatment planning and improving clinical outcomes. However, current methods face limitations in generalization due to small datasets and lack of interpretability, both critical in medical applications. To address these challenges, we propose a multi-class brain tumor classification system, exploring three distinct models based on VGG19 architecture. These include an attention-module-based model, a deeper convolutional network, and a linear-boosted model. Each model is trained on a large, aggregated dataset with six types of image augmentations to improve generalization. Among the three models, the attention-based model achieves the highest classification performance by focusing on relevant tumor regions. Local interpretable model-agnostic explanations are used to visualize the decision-making process, enhancing model transparency. Our results demonstrate that the attention-based model outperforms baseline and state-of-the-art methods, making it a robust and interpretable solution for brain tumor diagnosis.},
  archive      = {J_NCA},
  author       = {Alanazi, Mubarak A.},
  doi          = {10.1007/s00521-025-11561-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24191-24207},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-modal MRI data augmentation and attention-based modeling for interpretable brain tumor classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing YOLOv9 for automated detection of stroke lesions in brain CT images. <em>NCA</em>, <em>37</em>(29), 24169-24189. (<a href='https://doi.org/10.1007/s00521-025-11560-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prompt detection and accurate localization of stroke-induced cerebral damage in Computed Tomography (CT) scans are essential for optimal treatment and patient prognosis. Traditional techniques significantly depend on the proficiency of radiologists, which can be labor-intensive and susceptible to inaccuracies. This study presents a novel methodology utilizing the YOLOv9 deep learning architecture, termed StrokeYOLO, for the automated detection of stroke lesions in brain CT images. Although YOLOv9 is primarily employed for general object detection, we have modified it to specifically focus on the nuanced characteristics of stroke lesions. Through the modification of anchor boxes, the refinement of feature extraction, and the fine-tuning of the model, we have improved its capacity to accurately identify smaller stroke regions. The proposed model was trained and assessed on a dataset of annotated brain CT scans, exhibiting outstanding efficacy in identifying ischemic and hemorrhagic stroke regions. The detection accuracy was further corroborated against expert annotations, attaining results that were comparable to or superior to traditional methods. Furthermore, we utilized explainable AI methodologies to enhance transparency in the model's decision-making process, thereby promoting trust and clinical implementation. This study emphasizes the capabilities of YOLOv9 as a real-time, automated instrument for facilitating stroke diagnosis while tackling the challenges and prospects of utilizing object detection models in medical imaging. StrokeYOLO attained an accuracy of 98.7%, precision of 99.92%, recall of 99.1%, and F1-measure of 99.51% on the evaluation dataset, surpassing current methodologies. The findings underscore the capability of YOLOv9 as a real-time, automated instrument for facilitating stroke diagnosis while tackling the challenges and prospects of utilizing object detection models in medical imaging.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Shaban, Warda M.},
  doi          = {10.1007/s00521-025-11560-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24169-24189},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing YOLOv9 for automated detection of stroke lesions in brain CT images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing heart health: An AI-driven analysis of dietary habits, unveiling impacts on human health and attitudes. <em>NCA</em>, <em>37</em>(29), 24149-24167. (<a href='https://doi.org/10.1007/s00521-025-11559-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel deep learning-based approach for analyzing the relationship between eating habits and heart health. The method leverages wearable technology, smartphone applications, and food diaries to gather comprehensive dietary data. This data is then processed by recurrent neural networks (RNNs) and convolutional neural networks (CNNs) to extract significant dietary patterns and features relevant to cardiovascular health. By integrating dietary information with other health-related data, a comprehensive model is constructed to analyze the intricate interactions between lifestyle, nutrition, and cardiovascular outcomes. This approach facilitates the generation of personalized dietary recommendations and enables a more precise and objective evaluation of eating habits. To validate the effectiveness of the proposed methodology, an LSTM model is implemented and achieves a precision of 0.982, indicating a high percentage of true positive predictions. Additionally, the model demonstrates an accuracy of 98.9%, highlighting its ability to classify nearly all instances accurately. These exceptional results suggest the suitability of the modified LSTM model for further investigation and potential real-world implementation.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and ZainEldin, Hanaa and Gamel, Samah A.},
  doi          = {10.1007/s00521-025-11559-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24149-24167},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing heart health: An AI-driven analysis of dietary habits, unveiling impacts on human health and attitudes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting design suitability of box-shaped sustainable timber structural members using machine learning and hyperparameter optimization. <em>NCA</em>, <em>37</em>(29), 24123-24148. (<a href='https://doi.org/10.1007/s00521-025-11556-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement of timber structures is essential for sustainable construction, focusing on enhancing ductility and energy dissipation. While machine learning offers a promising approach to accelerate and simplify the design process for such structures, its application in this field remains underexplored. This study develops digital models of box-shape timber members to assess their suitability for design (TS 647 Building Code for Timber Structures, Türk Standardlari Enstitüsü, Ankara, 1979), considering cross-section, span, loading, screws and material properties. A dataset of 2000 design cases was generated, incorporating variations in these factors based on design calculations in accordance with the relevant standards. After data preprocessing, machine learning (ML) classification algorithms, including Decision Tree (DT), Gaussian Naïve Bayes (GNB), K-Nearest Neighbour (KNN), Linear Discriminant Analysis Classification (LDA), Logistic Regression (LR), Support Vector Machine (SVM) and Voting Ensemble Classification (VEC), were employed to predict design suitability (Fx), where Fx indicates whether the generated design meets structural criteria. The study found that Fx positively correlated with cross-section, span and moment of inertia, while it had a strong negative correlation with moment, stress, shear and deflection. Accuracy scores ranged from 91.7% to 98.6%, with LR performing the best (98.6%), while GNB had the lowest score (91.7%). These results were supported by model performance metrics, namely precision, recall, F1, AUC and MCC scores. Additionally, hyperparameter optimization was applied to improve the performance metrics of the models, resulting in more accurate and reliable predictions. It improved DT, KNN and SVM, while LR, LDA and GNB showed no significant changes. These findings highlight the effectiveness of machine learning in predicting the suitability of timber structure designs, providing a more efficient approach to structural assessment.},
  archive      = {J_NCA},
  author       = {Cosut, Muhammed and Bekdas, Gebrail and Nigdeli, Sinan Melih and Isikdag, Umit},
  doi          = {10.1007/s00521-025-11556-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24123-24148},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predicting design suitability of box-shaped sustainable timber structural members using machine learning and hyperparameter optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PINNs for solving unsteady maxwell’s equations: Convergence issues and comparative assessment with compact schemes. <em>NCA</em>, <em>37</em>(29), 24103-24122. (<a href='https://doi.org/10.1007/s00521-025-11554-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-Informed Neural Networks (PINNs) have recently gained prominence as a mesh-free, physics-integrated framework for solving partial differential equations. In this work, we evaluate the capabilities of PINNs in solving unsteady Maxwell’s equations, benchmarking their performance against two established numerical schemes: the finite-difference time-domain method and a compact Padé scheme. The investigation spans three canonical test cases, involving one-dimensional free-space wave propagation and two-dimensional Gaussian pulse evolution in both periodic and dielectric media. The convergence enhancement strategies including random Fourier feature embeddings, spatio-temporal periodicity enforcement, and temporal causality constraints are assessed systematically through ablation study. The results suggest that architectural design choices must be closely aligned with the governing physics to ensure stable and accurate convergence. Using Neural Tangent Kernel analysis, the intrinsic “uneven learning” behavior of PINNs is uncovered. It was observed that PINNs can fail to prioritize regions with high error, instead converging more rapidly where the loss is already small, contrary to effective optimization principles. Overall, this study demonstrates that PINNs, with appropriate architecture, can match or surpass numerical solvers in accuracy and flexibility. However, challenges remain in addressing spatial inhomogeneity of convergence rate (uneven learning), adapting training to localized high-gradient features, and computational cost.},
  archive      = {J_NCA},
  author       = {Shaviner, Gal G. and Chandravamsi, Hemanth and Pisnoy, Shimon and Chen, Ziv and Frankel, Steven H.},
  doi          = {10.1007/s00521-025-11554-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24103-24122},
  shortjournal = {Neural Comput. Appl.},
  title        = {PINNs for solving unsteady maxwell’s equations: Convergence issues and comparative assessment with compact schemes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based diagnostic model for autism spectrum disorder using blood biomarkers. <em>NCA</em>, <em>37</em>(29), 24075-24102. (<a href='https://doi.org/10.1007/s00521-025-11553-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autistic spectrum disorder (ASD) is a neurological condition characterized by difficulties in social interaction, communication, and repetitive behaviors. Despite its largely hereditary nature, early detection is crucial, and a potential strategy for more efficient and quicker diagnosis is to employ artificial intelligence (AI). In this paper, a new system is introduced that is called automatic screening autism (ASA) system. ASA comprises three primary stages: data preparation (DP), feature selection (FS), and patient detection (PD). In the first stage, the used dataset is preprocessed through several steps: handling missing values and rejecting outliers. Then, these preprocessed features are fed to the FS stage to select the most important features using improved genetic algorithm (IGA). IGA composed of two stages; (i) pre-selection stage (PS2) using information gain (IG) and (ii) conclusive selection stage (CS2) using genetic algorithm (GA). Subsequently, these attributes are input into the proposed classification model using optimized deep neural network (ODNN). Actually, ODNN is based on optimized weights of traditional DNN using various optimization algorithms, and the final decision is derived from the best performance. ASA has been evaluated against contemporary methodologies. Results from experimental studies demonstrate that the proposed ASA outperforms its competitors regarding accuracy, precision, sensitivity, and F-measure, achieving values of approximately 99.10, 98.90, 98.70, and 98.80% in that order.},
  archive      = {J_NCA},
  author       = {Shaban, Warda M.},
  doi          = {10.1007/s00521-025-11553-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24075-24102},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial intelligence-based diagnostic model for autism spectrum disorder using blood biomarkers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of PEM fuel cells via puma optimizer with the aid of practical verifications. <em>NCA</em>, <em>37</em>(29), 24051-24074. (<a href='https://doi.org/10.1007/s00521-025-11552-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents a novel application of the recently developed Puma Optimizer (PO) for identifying the unknown parameters in Mann’s model, which is widely used for characterizing the behavior of Polymer Electrolyte Membrane Fuel Cells (PEMFCs). The proposed PO-based methodology is rigorously evaluated using three test cases. One test case involves experimental I–V measurements under various operating conditions from a commercial PEMFC stack, the Horizon H-100 (100 W), which was assembled and tested in the laboratory. The other two cases are established benchmark PEMFC systems, the Ballard Mark V 5 kW and BCS 500 W units. Comprehensive statistical analyses over multiple independent runs are performed to confirm the consistency and reliability of the PO-based approach. To evaluate the optimizer’s accuracy and robustness, comparisons are made with three other metaheuristic algorithms: two recently developed methods, the Propagation Search Algorithm (PSA) and the Walrus Optimization Algorithm (WOA), and a widely recognized one, the Slime Mold Optimizer (SMO). The comparisons show that the PO optimizer consistently achieved the lowest total square error (TSE) across all test cases, outperforming the other algorithms. Specifically, it develops the lowest values of 0.835811 for the Ballard Mark V 5 kW, 0.011281 for the BCS 0.5 kW, 0.711452 at 30 °C, 0.886144 at 35 °C, and 2.057015 at 40 °C for the Horizon H-100 fuel cell. These findings confirm that the PO is a reliable and effective tool for parameter estimation in PEMFC modeling under both simulated and real-world experimental conditions.},
  archive      = {J_NCA},
  author       = {Ashraf, Hossam and Abdellatif, Sameh O. and Elkholy, Mahmoud M. and El-Fergany, Attia A.},
  doi          = {10.1007/s00521-025-11552-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24051-24074},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance analysis of PEM fuel cells via puma optimizer with the aid of practical verifications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving DNNs for time-series classification using state and gradient abstraction-based preprocessing. <em>NCA</em>, <em>37</em>(29), 24025-24049. (<a href='https://doi.org/10.1007/s00521-025-11550-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series classification is important in various domains and tasks, and has attracted meaningful research over the past decades. Recent years have brought meaningful advancements in using deep neural networks (DNNs) architectures to classify time-series. However, challenges due to measurement errors, missing values, and irregular sampling are still a concern. To minimize their impact on the classification performance, various standardization methods are commonly applied to the raw continuous data as a preprocessing stage to the DNN. Instead, we suggest employing temporal abstraction, wherein the raw time-series is converted into a symbolic representation of time points. The transformed data can then be utilized as input for the DNNs. Specifically, we explore the impact of combining temporal abstraction with convolution-based sequence models and recurrent neural networks. To assess the effectiveness of these methods, we conducted evaluations on a total of 128 univariate and 13 multivariate time-series datasets. Through our framework, which incorporates the temporal abstraction process, we significantly enhanced the performance of various state-of-the-art DNNs used for time-series classification tasks. Our evaluation shows that our methods are significantly superior in classification prediction across all seven evaluation metrics for univariate time-series datasets, outperforming in terms of AUC-ROC for multivariate time-series datasets, compared to predictions using standardized raw data.},
  archive      = {J_NCA},
  author       = {Itzhak, Nevo and Tal, Shahar and Cohen, Hadas and Daniel, Osher and Kopylov, Roze and Moskovitch, Robert},
  doi          = {10.1007/s00521-025-11550-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24025-24049},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving DNNs for time-series classification using state and gradient abstraction-based preprocessing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing cervical cancer detection: A new optimized explainable artificial intelligence model. <em>NCA</em>, <em>37</em>(29), 23979-24023. (<a href='https://doi.org/10.1007/s00521-025-11548-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a classification model for analyzing cervical cancer images, addressing one of the most prevalent cancers among women worldwide. Early detection is crucial for improving recovery rates and reducing mortality. The integration of artificial intelligence (AI) in medical diagnostics has shown promise in cervical cancer screening by enabling faster results, reducing dependency on specialists, and minimizing biases. While AI-based solutions exist, ongoing research aims to enhance accuracy and efficiency. Advancements in deep learning (DL) have facilitated the development of automated frameworks for medical image analysis, including cervical cancer detection. This research proposes a five-phase model: (1) pre-processing the dataset, (2) extracting features using pre-trained models, (3) optimizing feature selection with the Copula Entropy-based Golden Jackal Optimization (CE-based GJO) algorithm, (4) optimizing hyperparameters using the Sea Horse Optimizer (SHO), and (5) employing explainable AI (XAI) to identify key cytomorphological features in classification. The proposed model is trained on the SipakMed dataset, the largest publicly available cervical cancer dataset on Kaggle. Experimental results demonstrate the proposed model’s superior performance, achieving high precision (0.9985), specificity (0.9996), F-measure (0.9985), and accuracy (0.9985). It outperforms leading benchmark studies, highlighting its potential for precise cervical cancer diagnosis. Additionally, the model offers a secure, cost-effective, and efficient AI-driven solution for early detection and screening.},
  archive      = {J_NCA},
  author       = {Abdel-Salam, Mahmoud and Askr, Heba and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-025-11548-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23979-24023},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing cervical cancer detection: A new optimized explainable artificial intelligence model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating and benchmarking nutritional supplement providers using a multi-criteria decision-making modeling approach. <em>NCA</em>, <em>37</em>(29), 23941-23978. (<a href='https://doi.org/10.1007/s00521-025-11547-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To determine the adequacy and balance of nutritional supplements for children, a high level of assessment and ranking of available Nutritional Supplement Providers (NSPs) is needed. Inherently, the evaluation of nutritional supplements is complex because it involves (1) different criteria, (2) trade-offs in preferences, and (3) the development of models that require data to be handled in a standardized manner. This study aims to develop a robust decision-making framework for assessing and benchmarking NSPs for children, thereby supporting informed choices in pediatric dietary supplementation. The methodology is developed in three phases. Phase 1: Identifying and collecting the dataset. Phase 2: Develop a decision matrix that includes 28 nutritional criteria, following eight NSPs as alternatives. Phase 3: Integration of mathematical process of 2-Tuple Linguistic q-Rung Picture Fuzzy Level-Based Weight Assessment (2TLq-RPF-LBWA), which offers a rigorous evaluation of the nutritional information for the children’s dietary supplements referred to in this study as Nutritional Information for Kids' Dietary Supplements (NIKDSs), and the Additively Ratio Assessment (ARAS) method helps in benchmarking the NSPs. The results from the 2TLq-RPF-LBWA indicated that the most highly weighted nutrients were 'vitamin D' (0.0545), 'iron' (0.0521), 'vitamin B12' (0.0500), and 'vitamin B9' (folic acid) (0.0480), which were the most highly weighted nutrients, whereas 'chromium' had the lowest weight (0.0245). ARAS benchmarking analysis indicated that "Centrum Kids Chewable Multivitamins" had a rating of 0.7626, ranking it in the top position. The "KINDER Multivitamin Syrup" was second, with a rating of 0.2893; "Maddovit Junior" was third, with a rating of 0.2599; and "OLIGOVIT Syrup" was rated lowest, with a rating of 0.0575. To verify the rigor of the results, three sensitivity analysis scenarios were developed, confirming that the methodology was robust and improved decision-making related to the development of safer, effective vitamin and mineral supplements for children. Overall, this study provides valuable benchmarking information to help parents, professionals, and manufacturers select suitable nutritional supplements for children.},
  archive      = {J_NCA},
  author       = {Habeeb, Mustafa Abdulfattah and Khaleel, Yahya Layth and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Sharaf, Iman Mohamad},
  doi          = {10.1007/s00521-025-11547-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23941-23978},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluating and benchmarking nutritional supplement providers using a multi-criteria decision-making modeling approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A benchmark study of optimizers for short-term solar PV power forecasting using neural networks under real-world constraints. <em>NCA</em>, <em>37</em>(29), 23909-23939. (<a href='https://doi.org/10.1007/s00521-025-11546-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term photovoltaic (PV) power forecasts are critical for efficient grid balancing, yet training optimizers, often overlooked compared to neural network architectures, significantly influence prediction accuracy and convergence speed. Prior research primarily focuses on adjusting network architectures, typically employing a single optimizer (commonly Adam), thus leaving optimizer selection underexplored, especially under noisy and incomplete real-world PV data. This study systematically benchmarks four optimizers—Adam, Adaptive Gradient (Adagrad), Rectified Adam (RAdam), and Lookahead—across three deep-learning architectures (Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN)-LSTM, and LSTM-Autoencoder) using data from two distinct PV sites. Unlike prior works, we assess optimizer effectiveness across a wide range of conditions, including varying training data lengths, sampling intervals, and missing data patterns (both random and block-wise). Using two real-world PV datasets representing semi-arid and desert climates, we analyze forecasting accuracy, convergence time, and robustness. Our empirical results demonstrate that RAdam consistently outperforms Adam by achieving up to 36% lower forecasting error under noisy and incomplete data conditions, while Lookahead offers up to 40% faster convergence in deep hybrid models. These gains translate into tighter reserve-margin planning and smoother inverter set-points, advancing state-of-the-art PV forecast pipelines. The paper concludes with optimizer-architecture recommendations for practitioners facing latency or compute constraints.},
  archive      = {J_NCA},
  author       = {Dhingra, Saloni and Gruosso, Giambattista and Storti Gajani, Giancarlo},
  doi          = {10.1007/s00521-025-11546-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23909-23939},
  shortjournal = {Neural Comput. Appl.},
  title        = {A benchmark study of optimizers for short-term solar PV power forecasting using neural networks under real-world constraints},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing aviation safety and efficiency: Applying artificial intelligence (AI) to address navigational challenges. <em>NCA</em>, <em>37</em>(29), 23883-23907. (<a href='https://doi.org/10.1007/s00521-025-11544-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision Navigation and Timing systems, integral to air traffic management, primarily rely on the Global Navigation Satellite System (GNSS) for accurate data transmission. However, GNSS signals, particularly those used in Automatic Dependent Surveillance-Broadcast (ADS-B) systems, are susceptible to interference, leading to potential safety risks in aviation. This study presents a novel AI-driven Flight Trajectory Prediction Framework, leveraging machine learning and deep learning techniques to detect and mitigate GNSS signal interference. By training models on ADS-B data, this framework identifies potential interference and evaluates its impact on air traffic. The implementation of this AI-based system enhances the reliability and security of air navigation, significantly reducing human error and elevating overall safety standards in aviation. Experimental results demonstrate the framework’s efficacy in improving navigational accuracy and operational efficiency within modern air traffic control systems.},
  archive      = {J_NCA},
  author       = {Hamza, Alyaa A. and Yosef, Rehan Ahmed and Rahouma, Kamel Hussien},
  doi          = {10.1007/s00521-025-11544-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23883-23907},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing aviation safety and efficiency: Applying artificial intelligence (AI) to address navigational challenges},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing false alarms by identifying depression-mimicking expressions. <em>NCA</em>, <em>37</em>(29), 23863-23882. (<a href='https://doi.org/10.1007/s00521-025-11543-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, people have been using social media platforms to express their feelings and share their mental health struggles openly and anonymously. This surge has motivated many researchers to take advantage of social media as a valuable resource of data to detect severe depression. However, existing approaches have significant limitations as they rely on datasets with a wide disparity between depressive and non-depressive instances, ignoring depression-mimicking expressions (such as stress, anxiety, sadness, sarcasm, and complaints) that are frequently misclassified as severe depression due to their linguistic overlap. This, in turn, leads to false alarms about inaccurate cases of depression, undermining our confidence in the detection system. We present the first study that aims to detect severe depression while simultaneously differentiating it from other depression-mimicking expressions. We curated and annotated a new dataset and refined existing ones to capture these depression-mimicking expressions. We implement and compare the performance of state-of-the-art large language models (LLMs), with RoBERTa emerging as a top-performing model, achieving an AUC of 98.37% on the validation set and 98.53% on the separate test set. Notably, fine-tuning of the models led to an impressive average AUC increase in around 40% over their original baseline versions, significantly enhancing the models’ ability to distinguish severe depression from depression-mimicking expressions. The fine-tuned RoBERTa model generalized well to an external dataset, increasing AUC from 0.65 to 0.97 and significantly reducing false alarms. The significant improvement highlights the effectiveness of fine-tuning LLMs on carefully curated data, reducing false alarms, and boosting the model’s reliability and applicability in practical settings.},
  archive      = {J_NCA},
  author       = {Ghouch, Baraa Abou and Khreich, Wael},
  doi          = {10.1007/s00521-025-11543-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23863-23882},
  shortjournal = {Neural Comput. Appl.},
  title        = {Reducing false alarms by identifying depression-mimicking expressions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nutriconv: Multitask learning framework for digital dietary tracking trained on EFSA’s pancake dataset. <em>NCA</em>, <em>37</em>(29), 23833-23862. (<a href='https://doi.org/10.1007/s00521-025-11542-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prevalence of nutrition-related health conditions calls for advanced tools to support reliable and efficient dietary monitoring. This paper presents NutriConv, a lightweight multitask convolutional neural network designed to simultaneously perform food classification and weight estimation from single-item food images. Trained on the institutionally validated PANCAKE dataset from the European Food Safety Authority, NutriConv combines classification and regression objectives within a unified architecture, optimized via a hybrid loss function. While its classification accuracy remains lower than that of specialized single-task models, NutriConv achieves competitive regression performance and offers a practical balance between both tasks. Its compact design enables deployment on resource-constrained platforms such as smartglasses and mobile health devices, expanding its usability in real-world dietary tracking scenarios. Extensive experiments confirm its robustness, including external validation on the Nutrition5K dataset, underscoring the model’s generalizability. This work highlights the potential of multitask learning for integrated, scalable, and accessible AI-based nutrition assessment.},
  archive      = {J_NCA},
  author       = {Junquera, Enol and Rico, Noelia and Díaz, Irene and González, Sonia and Remeseiro, Beatriz},
  doi          = {10.1007/s00521-025-11542-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23833-23862},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nutriconv: Multitask learning framework for digital dietary tracking trained on EFSA’s pancake dataset},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced fault detection and diagnosis in wind turbine systems using canonical variate analysis and reconstruction-based contribution method. <em>NCA</em>, <em>37</em>(29), 23811-23832. (<a href='https://doi.org/10.1007/s00521-025-11541-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection and diagnosis (FDD) is critical for ensuring the performance, safety, and reliability of industrial systems, especially in the expanding wind energy sector. As wind turbine installations continue to increase globally, maintaining operational reliability has become increasingly important due to their complex and nonlinear nature. Traditional FDD methods often underperform in such systems due to challenges in handling high-dimensional, nonlinear data. This paper proposes a robust methodology for fault detection and diagnosis in wind turbines. The proposed approach employs canonical variate analysis (CVA) for fault detection by analyzing multivariate data, and reconstruction-based contribution (RBC) for fault isolation by quantifying the contribution of individual variables. The methodology was validated using benchmark data from a real wind turbine system. The results demonstrate high effectiveness in detecting and diagnosing faults, highlighting the approach’s capability to manage complex, nonlinear systems. Simulation results show that the proposed methodology outperforms traditional techniques such as PCA, PLS, EMPRM, and TPCR achieving higher fault detection rates and improved sensitivity, with detection accuracy exceeding $$95\%$$ across multiple fault scenarios. These findings confirm the successful achievement of the research objectives and represent a significant advancement in enhancing the safety, reliability, and operational performance of wind turbine systems under dynamic conditions.},
  archive      = {J_NCA},
  author       = {Elshenawy, Lamiaa M. and Gafar, Ahmed A. and Awad, Hamdi A.},
  doi          = {10.1007/s00521-025-11541-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23811-23832},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced fault detection and diagnosis in wind turbine systems using canonical variate analysis and reconstruction-based contribution method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal object detection: An architecture using feature-level fusion and deep learning. <em>NCA</em>, <em>37</em>(29), 23799-23810. (<a href='https://doi.org/10.1007/s00521-025-11521-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is one of the most fundamental problems to tackle in the computer vision research area. Recent advances in multimodal data streams and deep learning architectures have prompted a fast growth in the field of multimodal learning, which brings several advantages over single-modality approaches for object detection, such as improved accuracy, robustness to noise and ambiguity, handling of complex scenarios and adaptability to diverse data. Some of the biggest challenges when implementing a multimodal learning approach are the selection of the fusion strategy, design of processing architecture, modality alignment/synchronization and interpretability of such high-dimensional representations. To address this challenge, we propose a feature-level fusion architecture for object detection based on extracting YOLO features from images, spectral and rhythm features from sound using Mel-frequency cepstral coefficients, and general descriptors from radar modalities that, after timestamp and homography transformation matrix alignment, are combined with an attention mechanism into a single classification network. Preliminary experiments indicate that the proposed architecture can constitute itself as a base pipeline for several different multimodal object detection tasks in real-world applications.},
  archive      = {J_NCA},
  author       = {Silva, Rui and Coelho, Eduardo and Pimenta, Nuno and Durães, Dalila and Alves, Victor and Bandeira, Lourenço and Machado, José and Novais, Paulo and Melo-Pinto, Pedro},
  doi          = {10.1007/s00521-025-11521-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23799-23810},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal object detection: An architecture using feature-level fusion and deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports. <em>NCA</em>, <em>37</em>(29), 23789-23797. (<a href='https://doi.org/10.1007/s00521-024-10640-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated radiology report generation is an emerging field for improving patient care and alleviating radiologist workload. However, existing methods face a range of challenges such as limited data availability, clinical metric performance, and interpretability. To address these issues, we propose a contrastive concept-phrase pre-training (C2P2) method, which utilizes a phrase-concept grounding task for contrastive learning. C2P2 learns the correspondence between phrases in a report and image concepts by using a phrase classification task to train a multi-label classifier for X-rays and extracting visual concepts of phrases using class activation maps. We then fine-tune a pre-trained BERT model to translate the extracted phrases into reports. Our proposed method outperforms or matches the previous state of the art in clinical efficacy metrics on both internal and external datasets. Moreover, C2P2 leverages more vision language data for pre-training and provides visual explanations of generated phrases.},
  archive      = {J_NCA},
  author       = {Tubaishat, Abdallah and Zia, Tehseen and Windridge, David and Nawaz, Muhammad and Razzaq, Saad},
  doi          = {10.1007/s00521-024-10640-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23789-23797},
  shortjournal = {Neural Comput. Appl.},
  title        = {Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing machine learning algorithms for imputation of missing time series in meteorological data. <em>NCA</em>, <em>37</em>(29), 23773-23787. (<a href='https://doi.org/10.1007/s00521-024-10601-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores advanced feedforward neural networks specifically multi-layer perceptron (MLP), long short-term memory (LSTM), and convolutional neural networks (CNNs) as time-series imputation techniques to address the challenge of missing data in analytical contexts. The study evaluates their performance by introducing artificial data gaps of varying durations 3 days, 1 week, and 1 month. The results reveal that all three algorithms (MLP, LSTM, and CNN) exhibit the ability to estimate incomplete data, yet with differing accuracies. LSTM and CNN outperform in filling short-term gaps (3 days and 1 week) with R2 values of 77% and 70% for LSTM, and 58.4% and 69.7% for CNN. MLP also demonstrates effectiveness, achieving accuracies of 74.9% for a 3-day gap and 67.7% for a 1-week gap. Notably, CNN proves the most accurate for monthly data gaps, attaining an R2 value of 70.1%. The findings suggest that the selection of imputation techniques should consider the specific time gap, with CNN highlighted as particularly effective for monthly gaps. In conclusion, this study provides valuable insights for researchers and practitioners engaged in imputing missing data in time-series analysis.},
  archive      = {J_NCA},
  author       = {Boujoudar, Mohamed and El Ydrissi, Massaab and Abraim, Mounir and Bouarfa, Ibtissam and El Alani, Omaima and Ghennioui, Hicham and Bennouna, El Ghali},
  doi          = {10.1007/s00521-024-10601-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23773-23787},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comparing machine learning algorithms for imputation of missing time series in meteorological data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a model for the study and measurement of consciousness in artificial cognitive systems based on the integrated information theory. <em>NCA</em>, <em>37</em>(29), 23739-23771. (<a href='https://doi.org/10.1007/s00521-024-10584-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study and measurement of consciousness in artificial cognitive systems have been subjects of great interest in the fields of artificial intelligence and neuroscience. This article introduces an innovative model based on the integrated information theory (IIT) aimed at addressing this fundamental issue. The IIT, proposed by Giulio Tononi, offers a robust theoretical approach to comprehend consciousness in terms of information and its integration. Our model focuses on identifying and quantifying key aspects of consciousness in artificial systems, including information complexity and the structure of artificial neural connections. We have developed a conceptual framework to assess the “integrity” of information within a system, enabling us to measure consciousness in terms of its capacity to integrate information coherently, setting it apart from other approaches. Through experiments and simulations conducted with a video game using the Go No Go task, we demonstrate how our model can be applied to artificial cognitive systems, allowing for the evaluation of their level of consciousness in different contexts. This approach carries significant implications for the advancement of more advanced and ethical artificial intelligence systems, as it provides a methodology for evaluating and comparing their level of consciousness. Ultimately, our work contributes to the progress in understanding and measuring consciousness in artificial systems, paving the way for future developments in artificial intelligence and computational neuroscience.},
  archive      = {J_NCA},
  author       = {Guerrero, Luz Enith and Arango-López, Jeferson and Castillo, Luis Fernando and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10584-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23739-23771},
  shortjournal = {Neural Comput. Appl.},
  title        = {Development of a model for the study and measurement of consciousness in artificial cognitive systems based on the integrated information theory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and machine learning approach for detecting spatial and temporal forgeries in videos. <em>NCA</em>, <em>37</em>(29), 23723-23737. (<a href='https://doi.org/10.1007/s00521-024-10558-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video forgery detection has become increasingly critical due to the rise of sophisticated video manipulation techniques. Traditional methods often struggle to keep up with the ever-evolving sophistication of forgery techniques, necessitating innovative and advanced solutions. In response to this challenge, we propose a hybrid approach that combines deep learning and machine learning techniques. This approach leverages the power of DenseNet121 for excellent spatial feature extraction, utilizes a spatiotemporal autoencoder to capture spatiotemporal dependencies, and employs a gradient boosting machine (GBM) to effectively classify the video clips based on the extracted features. This provides a robust and comprehensive solution for detecting spatial and temporal forgeries in videos. By combining these components, this approach offers a more robust and comprehensive solution for detecting spatial and temporal forgeries in videos. To evaluate our approach, we are using a diverse dataset of videos (VTD and VIFFD) with different forgery scenarios and conducting extensive experiments. The results demonstrate the effectiveness and accuracy of our hybrid approach in detecting spatial and temporal forgeries, surpassing the performance of individual classifiers and traditional methods.},
  archive      = {J_NCA},
  author       = {Singh, Upasana and Rathor, Sandeep and Kumar, Manoj},
  doi          = {10.1007/s00521-024-10558-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23723-23737},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid deep learning and machine learning approach for detecting spatial and temporal forgeries in videos},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of causal inference in the DQN sampling process for classical control problems. <em>NCA</em>, <em>37</em>(29), 23709-23721. (<a href='https://doi.org/10.1007/s00521-024-10540-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, causal inference is integrated into deep reinforcement learning to enhance sampling in classical control environments. The problem we’re working on is "classical control," where an agent makes decisions to keep systems balanced. With the help of artificial intelligence and causal inference, we have developed a method that adjusts a deep Q-network’s experience memory by adjusting the priority of transitions. According to the agent’s actions, these priorities are based on the magnitude of causal differences. We have applied our methodology to a reference environment in reinforcement learning. In comparison with a deep Q-network based on conventional random sampling, the results indicate significant improvements in performance and learning efficiency. Our study shows that causal inference can be integrated into the sampling process so that experience transitions can be selected more intelligently, resulting in more effective learning for classical control problems. The study contributes to the convergence between artificial intelligence and causal inference, offering new perspectives for the application of reinforcement learning techniques in real-world applications where precise control is essential.},
  archive      = {J_NCA},
  author       = {Velez Bedoya, Jairo Ivan and Gonzalez Bedia, Manuel and Castillo Ossa, Luis Fernando and Arango Lopez, Jeferson and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10540-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23709-23721},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integration of causal inference in the DQN sampling process for classical control problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model. <em>NCA</em>, <em>37</em>(29), 23707-23708. (<a href='https://doi.org/10.1007/s00521-025-11146-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Islam, Umar and Shah, Babar and Al-Atawi, Abdullah A. and Arnone, Gioia and Abonazel, Mohamed R. and Ali, Ijaz and Moreira, Fernando},
  doi          = {10.1007/s00521-025-11146-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23707-23708},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction to: Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model. <em>NCA</em>, <em>37</em>(29), 23683-23706. (<a href='https://doi.org/10.1007/s00521-024-10169-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethereum has emerged as a major platform for decentralized apps and smart contracts with the heightened interest in cryptocurrencies in recent years. Investors and market participants in the cryptocurrency space will find it increasingly important to use reliable price prediction models as Ethereum's popularity grows. To better estimate Ethereum prices around the world, we propose "EtherVoyant," a novel hybrid forecasting model that combines the advantages of ARIMA and SARIMA methods. To improve its forecasting abilities, EtherVoyant uses Ethereum price history to train ARIMA and SARIMA components independently before fusing their predictions. With the help of feature engineering and data preparation, we further improve the model so that it can deal with real-world difficulties like missing values and seasonality in the data. We also investigate hyperparameter optimization for the model's best possible performance. We compare EtherVoyant's forecasts against those of the more conventional ARIMA and SARIMA models to determine its efficacy. By providing more precise and trustworthy price forecasts, our trial results suggest that EtherVoyant is superior to the individual models. The importance of this study resides in the fact that it will lead to the creation of a sophisticated time series forecasting model that will be useful to cryptocurrency investors, traders, and decision-makers. We hope that by making EtherVoyant available on a worldwide scale, we will help advance the field of cryptocurrency analytics and encourage wider adoption of blockchain-based assets.},
  archive      = {J_NCA},
  author       = {Islam, Umar and Shah, Babar and Al-Atawi, Abdullah A. and Arnone, Gioia and Abonazel, Mohamed R. and Ali, Ijaz and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10169-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23683-23706},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering privacy and resilience: A decentralized federated learning approach to cyberbullying detection. <em>NCA</em>, <em>37</em>(29), 23667-23682. (<a href='https://doi.org/10.1007/s00521-024-10148-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a rapidly changing digital world, the rise of cyberbullying has become a pressing issue that calls for creative and flexible solutions to detect and prevent it. To address this urgent need, we introduce a novel method: decentralized federated learning for cyberbullying detection using a ring topology network. Traditional federated learning (FL) paradigms have traditionally relied on centralized servers to manage important operations. However, this centralized model faces complex challenges related to privacy vulnerabilities, fairness disparities, and scalability constraints. Our solution brings about a significant change, embracing decentralization as the foundation of a strong and privacy-focused cyberbullying detection framework. This research represents a significant shift away from centralization, as it relies on distributed clients in a ring topology network to collectively carry out crucial FL tasks. Through the redistribution of these responsibilities and the establishment of direct communication channels between neighboring nodes, our approach successfully avoids the challenges related to central server instability and bias. The ring topology architecture creates a decentralized ecosystem, carefully crafted to prioritize the confidentiality of user data while enhancing the resilience and efficiency of the FL process. Our model architecture for cyberbullying detection is carefully crafted, utilizing the powerful capabilities of GRU, LSTM, BERT, and Word2Vec embedding, along with emotional features. This architectural innovation perfectly aligns with the ring topology FL approach, enabling localized updates, efficient aggregation, and flexible adaptability. It is worth mentioning that the BERT model consistently outperforms its competitors, delivering exceptional results.},
  archive      = {J_NCA},
  author       = {Khan, Umair and Khan, Salabat and Mussiraliyeva, Shynar and Samee, Nagwan Abdel and Alabdulhafith, Maali and Shah, Khalid},
  doi          = {10.1007/s00521-024-10148-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23667-23682},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering privacy and resilience: A decentralized federated learning approach to cyberbullying detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-personalized employment in urban hubs: Multimodal fusion architectures for personality-based job matching. <em>NCA</em>, <em>37</em>(28), 23651-23666. (<a href='https://doi.org/10.1007/s00521-024-10587-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of smart cities, employment strategies have been steering towards a more personalized approach, aiming to enhance job satisfaction and boost economic efficiency. This paper explores an advanced solution by integrating multimodal deep learning to create a hyper-personalized job matching system based on individual personality traits. We employed the First Impressions V2 dataset, a comprehensive collection encompassing various data modalities suitable for extracting personality insights. Among various architectures tested, the fusion of XceptionResNet with BERT emerged as the most promising, delivering unparalleled results. The combined model achieved an accuracy of 92.12%, an R2 score of 54.49%, a mean squared error of 0.0098, and a root mean squared error of 0.0992. These empirical findings demonstrate the effectiveness of the XceptionResNet + BERT in mapping personality traits, paving the way for an innovative, and efficient approach to job matching in urban environments. This work has the potential to revolutionize recruitment strategies in smart cities, ensuring placements that are not only skill-aligned but also personality-congruent, optimizing both individual satisfaction and organizational productivity. A set of theoretical case studies in technology, banking, healthcare, and retail sectors within smart cities illustrate how the model could optimize both individual satisfaction and organizational productivity.},
  archive      = {J_NCA},
  author       = {Jain, Dipika and Sangwan, Saurabh Raj and Kumar, Akshi},
  doi          = {10.1007/s00521-024-10587-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23651-23666},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hyper-personalized employment in urban hubs: Multimodal fusion architectures for personality-based job matching},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based segmentation for medical data hiding with galois field. <em>NCA</em>, <em>37</em>(28), 23635-23650. (<a href='https://doi.org/10.1007/s00521-023-09151-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hiding is of the utmost importance for protecting the copyright of image content, given the widespread use of images in the healthcare domain. Presently, medical image security is important not only for protecting individual privacy but also for accurate diagnosis and treatment. In this paper, a deep learning-based segmentation for a medical data hiding technique with the Galois field is proposed. This technique uses a customised UNet3+ deep learning network to segment a medical image into a Region of Interest and a Non-Region of Interest. Through the proper spatial and transform-based embedding method, multiple marks are embedded into both parts of the medical image. In addition, encryption is utilised to provide additional security for protecting sensitive information when transmitted over an open channel so that the information cannot be retrieved. The extensive experimental results show that the proposed technique for medical images achieves a good balance between imperceptibility and robustness with high security. Further, the obtained results showed the superiority of our technique over state-of-the-art techniques, demonstrating that it can provide a reliable security solution for healthcare data.},
  archive      = {J_NCA},
  author       = {Amrit, P. and Singh, K. N. and Baranwal, N. and Singh, A. K. and Singh, J. P. and Zhou, H.},
  doi          = {10.1007/s00521-023-09151-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23635-23650},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based segmentation for medical data hiding with galois field},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A highly densed deep neural architecture for classification of the multi-organs in fetal ultrasound scans. <em>NCA</em>, <em>37</em>(28), 23619-23633. (<a href='https://doi.org/10.1007/s00521-023-09148-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) makes a substantial contribution to decision-making in many intricate application areas of the medical sciences. One such application is organ classification in maternal–fetal ultrasound scans using sophisticated AI methodology like deep neural networks for better analysis. In this paper, we present a novel and highly dense deep neural architecture specifically designed for the multi-organ classification of fetal ultrasound scans. Our proposed approach introduces a unique combination of densely connected layers, convolution layers, and skip connections, tailored to accurately identify and classify multiple fetal organs simultaneously. To the best of our knowledge, this is the first study to address the comprehensive classification of multiple organs in fetal ultrasound images using such a highly dense neural network. The architecture is designed to capture both local and global features, critical for distinguishing intricate organ structures in ultrasound images. The model also minimizes the gradient loss for faster convergence of the model parameters in the training phase. Through extensive evaluation of a curated dataset of fetal ultrasound scans, we demonstrate that our novel architecture achieves superior classification accuracy—96.85%, precision—97.12%, recall—96.66%, F1-score—96.88%, and AUC-ROC score—97.27%, outperforming state-of-the-art methods in the context of multi-organ classification. Thus, the proposed highly dense deep neural architecture presents a promising avenue for enhancing fetal ultrasound imaging, bringing potential benefits to prenatal care, and contributing to improved neonatal outcomes.},
  archive      = {J_NCA},
  author       = {Srivastava, Somya and Vidyarthi, Ankit and Jain, Shikha},
  doi          = {10.1007/s00521-023-09148-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23619-23633},
  shortjournal = {Neural Comput. Appl.},
  title        = {A highly densed deep neural architecture for classification of the multi-organs in fetal ultrasound scans},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Round trip time meets transformers: High-fidelity human counting in cluttered environments. <em>NCA</em>, <em>37</em>(28), 23591-23617. (<a href='https://doi.org/10.1007/s00521-025-11540-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate human counting in indoor environments is essential for optimizing people-centric applications, such as crowd management, disaster response, and monitoring in settings like shopping malls and healthcare facilities. Traditional vision approaches face challenges with poor lighting conditions and raise privacy concerns. WiFi-based solutions enable device-free human counting by detecting disruptions in wireless signals caused by human presence. However, methods using received signal strength indicator are unreliable due to physical obstructions, multipath fading, radio interference, and fluctuating access point power. While WiFi channel state information-based systems are more sensitive to environmental changes, they lack standardization, limiting their practicality. To overcome these limitations, this paper presents Time4Count, an innovative device-free indoor human counting system that leverages round trip time measurements to achieve high accuracy and scalability. Time4Count capitalizes on human-induced fluctuations in signal propagation time to accurately estimate the number of individuals in a space. By employing a multivariate transformer-based feature extraction method, the system effectively mitigates non-line-of-sight errors and signal distortions, ensuring robust performance even in cluttered indoor environments. Additionally, Time4Count integrates spatial discretization and multi-label classification techniques, enabling it to count an unlimited number of individuals in real-time. The system was rigorously evaluated in two realistic, cluttered environments using commodity hardware, involving up to 15 participants. Experimental results reveal that Time4Count achieves an high counting accuracy of 92.7%. To our knowledge, Time4Count is the first RTT-based indoor counting system, providing a precise solution for indoor monitoring. Implementation is available at: https://github.com/mclab-osaka/time4count .},
  archive      = {J_NCA},
  author       = {Yonekura, Haruki and Rizk, Hamada and Yamaguchi, Hirozumi},
  doi          = {10.1007/s00521-025-11540-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23591-23617},
  shortjournal = {Neural Comput. Appl.},
  title        = {Round trip time meets transformers: High-fidelity human counting in cluttered environments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable information fusion approach to impute meteorological missing values toward cross-domain intelligent forecasting. <em>NCA</em>, <em>37</em>(28), 23533-23589. (<a href='https://doi.org/10.1007/s00521-025-11538-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep robust domain adaptation (DRDA) model for heterogeneous weighted information fusion (HWIF), called DRDAHWIF-gram, is proposed to impute long-term missing meteorological data for cross-domain intelligent forecasting. This study’s core contribution lies in assessing forest climate change risks through explainable machine learning, which leverages a multi-criteria computational performance metric to establish a smart early warning index (SEWI). Two real-world case studies, utilizing the Hyrcanian forest dataset and the comparative climatic data, validate the model’s ability to capture predictable meteorological patterns, including temperature, humidity, solar radiation, and wind speed trends, across diverse climatological domains. Evaluated across multiple forecast horizons, the developed model outperforms conventional methods, demonstrating higher accuracy and reliability in weather variation monitoring. The high SEWI confirms the stability of meteorological domains, supporting the deployment of intelligent climatic hazard early warning systems. This approach advances expert forecasting for meteorological threats under forest climate change.},
  archive      = {J_NCA},
  author       = {Zarchi, Milad and Aslani, Zohreh Hashemi and Tee, Kong Fah},
  doi          = {10.1007/s00521-025-11538-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23533-23589},
  shortjournal = {Neural Comput. Appl.},
  title        = {An interpretable information fusion approach to impute meteorological missing values toward cross-domain intelligent forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking arabic event relation extraction: A comprehensive survey. <em>NCA</em>, <em>37</em>(28), 23513-23531. (<a href='https://doi.org/10.1007/s00521-025-11537-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event relation extraction tasks are paramount in natural language processing (NLP), facilitating a deeper understanding of textual narratives and enhancing language comprehension. However, a notable gap emerges when considering the Arabic language, with limited research dedicated to event-related tasks. This paper addresses this research gap by embarking on a comprehensive exploration of Arabic event relation extraction tasks. It is the first survey to explore Arabic event relation extraction tasks, illustrating the tasks, available corpora, existing works, and associated evaluation metrics. It also addresses the inherent challenges of Arabic event relations and provides insights into established English corpora and research for reference. Furthermore, the paper sets a future research agenda, emphasizing the need for dedicated corpora across tasks to advance the field and automate Arabic text and narrative comprehension.},
  archive      = {J_NCA},
  author       = {Aldawsari, Mohammed},
  doi          = {10.1007/s00521-025-11537-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23513-23531},
  shortjournal = {Neural Comput. Appl.},
  title        = {Unlocking arabic event relation extraction: A comprehensive survey},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of compressive strengths of portland cement with random forest, support vector machine and gradient boosting models. <em>NCA</em>, <em>37</em>(28), 23495-23511. (<a href='https://doi.org/10.1007/s00521-025-11536-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents machine learning models to predict compressive strengths of 924 CEM I 42.5 R type Portland cements. Particularly the utilized machine learning algorithms are adaptive network-based fuzzy inference systems, Random Forest, Support Vector Machine, Extreme Gradient Boosting, Light Gradient Boosting and Categorical Boosting. For machine learning, collected data contained 15 input features that show the physical and chemical properties of the cements. The compressive strengths at 1, 2, 7 and 28 days were defined as the output parameters. Models for each hydration day were trained with 748 data points and tested with 176 data points. Then, compressive strength test results and machine learning predictions were compared using statistical methods such as R-squared, mean absolute percentage error and root-mean-square error. The results indicate that Gradient Boosting models, in particular, accurately predict compressive strength, demonstrating that it is possible to estimate compressive strength without mechanical tests. In our developed Gradient Boosting model, the RMSE accuracy exceeds 95%, further supporting its reliability. The developed machine learning models offer substantial savings in both time and cost for compressive strength estimation.},
  archive      = {J_NCA},
  author       = {Ozcan, Giyasettin and Gulbandilar, Eyyup and Kocak, Yilmaz},
  doi          = {10.1007/s00521-025-11536-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23495-23511},
  shortjournal = {Neural Comput. Appl.},
  title        = {Prediction of compressive strengths of portland cement with random forest, support vector machine and gradient boosting models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable classification by local error amplification. <em>NCA</em>, <em>37</em>(28), 23473-23494. (<a href='https://doi.org/10.1007/s00521-025-11535-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel predictive approach rooted in the field of explainable AI, designed to provide sketches of the action of a non-interpretable machine learning model. The use case is psychological profiling and concerns the alignment between individuals’ personalities and specific job roles within the fields of psychology and career development. Our approach builds upon the foundation of BAPC (Before and After prediction Parameter Correction) (Sobieczky and Geiß in Explainable AI by BAPC—Before and After correction Parameter Comparison, 2023), an explainable AI technique designed to elucidate predictions from models with interpretable parameters of a surrogate ’base’ model. We extend this method by incorporating error amplification (EA), a new technique effectively integrating decision trees as our base model. In the three-step methodology, rooted in the principles of BAPC, enhanced by error amplification, we firstly train a base model using decision trees and identify the model’s errors. Second, we introduce an AI model to predict the errors made by the base model. Subsequently, we amplify these predicted errors in a localized manner around the individual point of interest. In the third step, we retrain the base model using the error-amplified data set. The disparity between the decision boundaries generated by the initial and the retrained decision trees provides explainable insights into enhancing specific personality features. While our method finds its application in personality-job alignment here, its versatility extends to a wide range of scenarios, offering valuable insights into data set feature interpretability by elucidating their effective changes.},
  archive      = {J_NCA},
  author       = {Dudkin, Erika and Sobieczky, Florian},
  doi          = {10.1007/s00521-025-11535-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23473-23494},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable classification by local error amplification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Histogram self-organizing map for cluster detection and visualization. <em>NCA</em>, <em>37</em>(28), 23447-23472. (<a href='https://doi.org/10.1007/s00521-025-11530-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering of histogram data, which preserves important distributional information, remains a significant challenge due to the limitations of existing approaches. This paper presents a novel two-level clustering algorithm explicitly designed for histogram data. It integrates a self-organizing map (SOM) framework with the $$L_2$$ Wasserstein distance to capture distributional features such as location, scale, and shape. By enriching prototypes with local density and connectivity measures, the approach automatically infers cluster numbers without prior knowledge. Experiments on synthetic and real-world datasets underscore the quality of the obtained results in comparison with other distance-based or histogram-based approaches, the computational efficiency of the proposed approach, and its interpretability through topology-preserving visualizations.},
  archive      = {J_NCA},
  author       = {Cabanes, Guénaël and Bennani, Younès and Rastin, Parisa},
  doi          = {10.1007/s00521-025-11530-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23447-23472},
  shortjournal = {Neural Comput. Appl.},
  title        = {Histogram self-organizing map for cluster detection and visualization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in NN: From fitting most to fitting a few. <em>NCA</em>, <em>37</em>(28), 23423-23446. (<a href='https://doi.org/10.1007/s00521-025-11528-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute to the understanding of learning behavior of neural networks by showing that in principle a few possibly noisy samples can significantly alter weights of a network, even after most samples have been fit. Formally, we approach learning dynamics by analyzing prediction accuracy, input reconstruction ability, and prediction performance. A neural network can exhibit a prototype-learning phase decreasing reconstruction loss initially, possibly increasing the loss of a small set of well-defined samples, i.e., defined based on the L2-norm and dot product with the mean of class samples. Toward the end of training, parameter updates might mostly reduce the classification loss of a few samples, which increases reconstruction loss. Aside from providing a mathematical analysis of a linear network, we also assess the behavior using common datasets and architectures from computer vision. On the practical side, our work supports training data selection with low computational effort, i.e., identifying samples that are more likely to be misclassified. This can help in identifying labeling errors, ensuring a diverse dataset, and data valuation. Our work also casts a different view on the notion of outliers in supervised learning.},
  archive      = {J_NCA},
  author       = {Schneider, Johannes},
  doi          = {10.1007/s00521-025-11528-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23423-23446},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning in NN: From fitting most to fitting a few},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IntruSafe: A FCNN-LSTM hybrid IoMT intrusion detection system for both string and 2D-spatial data using sandwich architecture. <em>NCA</em>, <em>37</em>(28), 23395-23422. (<a href='https://doi.org/10.1007/s00521-025-11527-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) is a resource-constrained device with limited computational capabilities. However, the market worth of this section is booming rapidly. The IoMT manufacturers need to offer their products at a competitive price, which forces them to use simplified architecture, leaving limited and, to some extent, no scope to employ sophisticated cybersecurity algorithms. As a result, IoMT has become a lucrative practice ground for cybercriminals. The IoMT sector deals with valuable, confidential healthcare-related data and offers convenient, personalized healthcare services. That is why the market demand and IoMT intrusion are experiencing massive growth. An innovative Intrusion Detection System (IDS), IntruSafe, has been studied, developed, and presented in this paper that combines Fully Connected Convolutional Neural Network (FCNN) and Long Short-Term Memory (LSTM) to protect the IoMT network from malicious signals. The IntruSafe combines FCNN and LSTM to ensure the detection of both malicious text and image data. It detects and simultaneously protects the IoMT network from further intrusion with only a 0.18% service interruption rate. This high-performing IDS detects intrusion with 97.66% accuracy, 98.50% precision, 97.33% recall, and 97.85% F1-score. With outstanding performance, IntruSafe is a promising IDS that will facilitate further growth of the IoMT sector while minimizing the risks of a successful intrusion.},
  archive      = {J_NCA},
  author       = {Alazab, Moutaz and Awajan, Albara and Obeidat, Areej and Faruqui, Nuruzzaman and Rehman, Hafeez Ur},
  doi          = {10.1007/s00521-025-11527-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23395-23422},
  shortjournal = {Neural Comput. Appl.},
  title        = {IntruSafe: A FCNN-LSTM hybrid IoMT intrusion detection system for both string and 2D-spatial data using sandwich architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaze-guided contrastive unsupervised representations learning. <em>NCA</em>, <em>37</em>(28), 23381-23393. (<a href='https://doi.org/10.1007/s00521-025-11526-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the integration of information-rich prior knowledge, specifically human gaze data, to enhance representation learning through contrastive methods. We propose gaze-guided contrastive unsupervised representation learning, a novel framework harnessing human gaze data to guide the selection of positive and negative samples for contrastive learning. By leveraging human gaze information, we capture meaningful patterns in visual task dynamics, enabling the agent to acquire effective strategies from demonstrations and achieve superior performance. Our findings demonstrate significant improvements over baseline algorithms, highlighting the value of gaze-guided representation learning in reducing data requirements and accelerating learning. This approach offers broad applicability to vision-based tasks, emphasizing the critical role of human gaze in improving task efficiency and generalization.},
  archive      = {J_NCA},
  author       = {Distefano, Joseph P. and Manjunatha, Hemanth and Thammineni, Chaithanya and Dalland, Kristian and Esfahani, Ehsan T.},
  doi          = {10.1007/s00521-025-11526-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23381-23393},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gaze-guided contrastive unsupervised representations learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAHGNet: Modeling hybrid-grained heterogenous dependency efficiently for traffic prediction. <em>NCA</em>, <em>37</em>(28), 23359-23379. (<a href='https://doi.org/10.1007/s00521-025-11525-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction plays a critical role in the intelligent transportation system, and it is also a challenging task because of the underlying complex spatiotemporal patterns and heterogeneities evolving across time. However, most present works mostly concentrate on solely capturing Spatial-temporal dependency or extracting implicit similarity graphs, but the hybrid-granularity evolution is ignored in their modeling process. In this paper, we proposed a novel data-driven end-to-end framework, named spatiotemporal aware hybrid graph network (STAHGNet), to couple the hybrid-grained heterogeneous correlations in series simultaneously through an elaborately hybrid graph attention module (HGAT) and coarse-granularity temporal graph (CTG) generator. Furthermore, an automotive feature engineering with domain knowledge and a random neighbor sampling strategy is utilized to improve efficiency and reduce computational complexity. The MAE, RMSE, and MAPE are used for evaluation metrics. Tested on four real-life datasets, our proposal outperforms eight classical baselines and four state-of-the-art (SOTA) methods (e.g., MAE 14.82 on PeMSD3; MAE 18.92 on PeMSD4). Besides, extensive experiments and visualizations verify the effectiveness of each component in STAHGNet. In terms of computational cost, STAHGNet saves at least four times the space compared to the previous SOTA models. The proposed model will be beneficial for more efficient TFP as well as intelligent transport system construction.},
  archive      = {J_NCA},
  author       = {Wang, Jiyao and Peng, Zehua and Zhang, Yijia and He, Dengbo and Lei, Chen},
  doi          = {10.1007/s00521-025-11525-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23359-23379},
  shortjournal = {Neural Comput. Appl.},
  title        = {STAHGNet: Modeling hybrid-grained heterogenous dependency efficiently for traffic prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TA-ASTGCN: A trend-aware adaptive spatio-temporal graph convolutional network for traffic flow prediction. <em>NCA</em>, <em>37</em>(28), 23335-23358. (<a href='https://doi.org/10.1007/s00521-025-11513-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial to improving the safety and efficiency of transportation systems. Although existing deep learning methods have made some progress in traffic flow prediction, most methods fail to effectively capture multi-source spatial relationships and long-term temporal dependencies. To address these problems, we present a traffic prediction model based on convolutional trend-aware attention and adaptive spatio-temporal graph convolution (TA-ASTGCN). The model captures complex spatio-temporal relationships by alternately stacking multiple convolutional trend-aware attention layers and adaptive spatio-temporal graph convolution layers in the encoder and decoder. The convolutional trend-aware attention layer captures the trends in traffic flow sequences and the correlations between nodes. The adaptive spatio-temporal graph convolution layer adaptively adjusts the strength of node relationships to effectively mine spatial correlations in traffic data. Compared with ASTGNN, with the best predictive performance in the baseline models, our TA-ASTGCN showed on average 3.69%, 3.14%, and 3.80% improvement in MAE, RMSE, and MAPE on four datasets. Experimental results showed that the prediction performance of the proposed model is better than the baseline models.},
  archive      = {J_NCA},
  author       = {Cai, Chuang and Guo, Huijie and Zhang, Zhenlin and Dou, Tianfeng and Wu, Dong and Qi, Kaiyuan and Bai, Yuqin and Ren, Chongguang},
  doi          = {10.1007/s00521-025-11513-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23335-23358},
  shortjournal = {Neural Comput. Appl.},
  title        = {TA-ASTGCN: A trend-aware adaptive spatio-temporal graph convolutional network for traffic flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-organizing soft sensor for process control systems: Integrating support vector regression with subtractive clustering. <em>NCA</em>, <em>37</em>(28), 23301-23333. (<a href='https://doi.org/10.1007/s00521-025-11504-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely prediction of critical process variables is crucial for optimizing performance in process control systems. Traditional hardware sensors often face challenges such as high costs, maintenance issues, and response delays. Data-driven soft sensors present a promising alternative by utilizing historical and real-time process data to estimate key variables. This paper introduces an integrated approach that combines support vector regression (SVR) with subtractive clustering and Grey wolf optimizer (GWO) to develop an advanced soft sensor. Subtractive clustering serves as a self-organizing technique, identifying and grouping the most relevant data points from historical data to establish a robust foundation for training the SVR model, thereby capturing complex nonlinear relationships within the process. To further improve the model’s performance, GWO is used to fine-tune SVR hyperparameters, optimizing the search space for the best results. The proposed soft sensor’s prediction accuracy is evaluated using the Tennessee Eastman process benchmark, the Tennessee Eastman process poses significant challenges due to its highly nonlinear nature and the strong interactions among variables, and its uncertainty is analyzed through a quantile regression scheme. Simulation results demonstrate that this approach improves process monitoring, reduces operational costs, and enhances overall system reliability and efficiency.},
  archive      = {J_NCA},
  author       = {Elshenawy, Lamiaa M. and Badawy, Ahmed and AbouOmar, Mahmoud S. and Mahmoud, Tarek A.},
  doi          = {10.1007/s00521-025-11504-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23301-23333},
  shortjournal = {Neural Comput. Appl.},
  title        = {A self-organizing soft sensor for process control systems: Integrating support vector regression with subtractive clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossGCL: Cross-view graph contrastive learning with dual tasks for drug recommendation. <em>NCA</em>, <em>37</em>(28), 23273-23299. (<a href='https://doi.org/10.1007/s00521-025-11497-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug recommendation aims to recommend proper drugs according to patients’ symptoms. A patient usually has multiple symptoms, and a doctor needs to prescribe a combination of drugs rather than an individual drug for a patient. However, existing works mainly model the semantic interactions between symptoms and drugs at the item level but ignore set-level semantic interactions, which are also crucial for drug recommendation tasks. To address the above issues, we propose Cross-view Graph Contrastive Learning with Dual Tasks for Drug Recommendation (CrossGCL) to recommend drugs. Our model learns the representation of symptoms and drugs from the item-level interactions (item view) and the set-level interactions (set view) and then maximizes the mutual information from the two views by cross-view contrastive learning so that the representation from different views can be enhanced by each other. Besides, we construct two structural-duality learning tasks, a symptom-driven drug prediction task and a drug-driven symptom prediction task, to better capture the set-level and item-level semantic information. We conduct experiments on the widely used public MIMIC-III dataset and show that our model outperforms state-of-the-art models.},
  archive      = {J_NCA},
  author       = {Wen, Wushao and Wang, Liang and Fang, Lihuang and Chen, Qiangpu and Qin, Jinghui},
  doi          = {10.1007/s00521-025-11497-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23273-23299},
  shortjournal = {Neural Comput. Appl.},
  title        = {CrossGCL: Cross-view graph contrastive learning with dual tasks for drug recommendation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fully convolutional neural network for fast detection, classification, and segmentation of fabric defects. <em>NCA</em>, <em>37</em>(28), 23249-23272. (<a href='https://doi.org/10.1007/s00521-025-11495-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of artificial intelligence and image processing for fabric defect detection is gaining prominence due to its practical significance in enhancing production quality. This study proposes a fast and accurate convolutional neural network (CNN) designed to detect defects in fabric with minimal computational complexity. The model processes input images of size 256 × 256 and generates defect masks of size 64 × 64. To improve detection accuracy, the model incorporates techniques such as ResNet, scheduled learning rate policies, data augmentation, and a weighted cross-entropy loss function. Trained on a diverse dataset of 2,681 defect samples from four fabric types and defect classes (holes, oil stains, color stains, and roller marks), the model achieved an accuracy of over 96%, a loss value below 0.1, and high recall, precision, and F1-Score. Compared to other state-of-the-art models, the proposed model delivers competitive performance with significantly faster prediction times, making it suitable for real-world fabric inspection applications.},
  archive      = {J_NCA},
  author       = {Mohammed, Swash Sami and Clarke, Hülya Gökalp and Mahmood, Sarmad Nozad},
  doi          = {10.1007/s00521-025-11495-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23249-23272},
  shortjournal = {Neural Comput. Appl.},
  title        = {A fully convolutional neural network for fast detection, classification, and segmentation of fabric defects},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance evaluation of AI and hybrid-AI models for estimation of evaporation in lesser himalayan valley. <em>NCA</em>, <em>37</em>(28), 23219-23248. (<a href='https://doi.org/10.1007/s00521-025-11492-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaporation holds a significant position in the global hydrological cycle and is one of the intricate phenomena widely affected by various hydrometeorological parameters. Evaporation accounts for 66% of global precipitation losses, profoundly influencing surface and rainfall losses, necessitating its meticulous quantification. Its estimation is resource-intensive, time-consuming, costly and sensitive to climatic and spatial variability. Over 22 numerical-physical methods are available, affected by time, data availability and climatic conditions. Data-driven artificial intelligence (AI) and machine learning (ML) models can be useful where it is very difficult to estimate the evaporation spatially and precisely. The current study uses 10 daily hydrometeorological in situ parameters: temperature (maximum, minimum and mean), vapour pressure (7.19 h, 14.19 h), relative humidity (7.19 h, 14.19 h), rainfall, bright sunshine hours and mean wind velocity for 23 years (2001:2023) for a Lesser Himalayan Valley (Doon Valley), India. The models developed for the estimation are ANN-SGD, ANN-LM, ANN-Adam, SVM and LSTM in addition to two hybrid models; $$R^{2}$$ , $${\text{NSE}}$$ and $${\text{MARE}}$$ measure ANN-PSO and ANN-GA and the performance of the models. The study enlightens on two major outcomes: the application of the various AI-based models for estimation of evaporation and the intercomparison of their outputs with the hybrid-AI models. All models show $$R^{2}$$ > 0.8, 0.15 ≤ $${\text{MARE}}$$ ≤ 0.25 and $${\text{NSE}}$$ ≥ 0.73, signifying robust performances. ANN-PSO and ANN-GA outperformed other models by integrating AI learning with optimization algorithms, addressing single-algorithm limitations. The study’s findings can assist researchers and act as a tool for the local stakeholders in managing water resources.},
  archive      = {J_NCA},
  author       = {Rajkumar, Gupta Abhishek and Nema, Manish Kumar and Khare, Deepak},
  doi          = {10.1007/s00521-025-11492-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23219-23248},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance evaluation of AI and hybrid-AI models for estimation of evaporation in lesser himalayan valley},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An autoencoder-based approach for feature engineering in cardiovascular disease prediction. <em>NCA</em>, <em>37</em>(28), 23185-23218. (<a href='https://doi.org/10.1007/s00521-025-11484-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease remains the leading cause of death worldwide, underscoring the urgent need for accurate, timely diagnosis to improve patient outcomes. Traditional diagnostic methods often struggle with accuracy, delayed intervention, and the complexity of analyzing diverse patient data. Moreover, existing machine learning approaches face challenges due to missing values, class imbalances, and limited features, all of which can negatively affect model performance. To overcome these limitations, this study introduces a novel autoencoder-based framework to enhance feature engineering for heart disease prediction. Autoencoders are employed for both dimensionality reduction and the generation of new, informative features that capture complex, non-linear relationships within the data. The methodology is evaluated using eight diverse heart disease datasets, including the widely recognized Cleveland and Hungarian datasets, with performance assessed through accuracy, balanced accuracy, ROC AUC, F1 score, and computational efficiency. Experimental results demonstrate notable improvements, with the method achieving 91.12% accuracy on the Cleveland dataset and 86.96% on the Hungarian dataset. Comparative analysis across four experimental scenarios—baseline performance with original features, autoencoder-based feature generation, feature reduction, and Bayesian hyperparameter optimization—reveals that the proposed approach substantially outperforms traditional techniques, achieving up to 96% accuracy on concatenated datasets and 95% on a comprehensive dataset. These compelling results highlight the potential of advanced autoencoder-driven feature engineering combined with sophisticated optimization strategies to significantly enhance diagnostic accuracy and generalization in heart disease prediction.},
  archive      = {J_NCA},
  author       = {Rashed, Amr E. Eldin and Badawy, Mahmoud and Elhosseini, Mostafa A. and Bahgat, Waleed M.},
  doi          = {10.1007/s00521-025-11484-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23185-23218},
  shortjournal = {Neural Comput. Appl.},
  title        = {An autoencoder-based approach for feature engineering in cardiovascular disease prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A census-based genetic algorithm for target set selection problem in social networks. <em>NCA</em>, <em>37</em>(28), 23155-23183. (<a href='https://doi.org/10.1007/s00521-025-11480-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the target set selection (TSS) problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph is given. A minimum size vertex subset needs to be found to “activate” such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called “a census-based genetic algorithm” for the TSS problem. In our algorithm, the concept of a census is utilized to gather and store information about individuals in a population and collect census data from the individuals constructed during the algorithm’s execution so that greater diversity and avoiding premature convergence can be achieved at locally optimal solutions. Specifically, two distinct census informations have been used: (a) For individuals, the algorithm stores how many times it has been identified during the execution; (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also be self-adjusted by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check individual’s feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, the proposed algorithm has been employed on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.},
  archive      = {J_NCA},
  author       = {Rahman, Md. Samiur and Ahsan, Mohammad Shamim and Chen, Cheng-Wu and Varadarajan, Vijayakumar},
  doi          = {10.1007/s00521-025-11480-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23155-23183},
  shortjournal = {Neural Comput. Appl.},
  title        = {A census-based genetic algorithm for target set selection problem in social networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwitchTree: In-network computing and traffic analyses with random forests. <em>NCA</em>, <em>37</em>(28), 23143-23154. (<a href='https://doi.org/10.1007/s00521-020-05440-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of machine learning in different domains is also finding applications in networking. However, this may need real-time analyses of network data which is challenging. The challenge is caused by the big data size and the need for bandwidth to transfer network data to a central location hosting the analyses server. In order to address that challenge, the in-network computing paradigm is gaining popularity with the advances in programmable data plane solutions. In this paper, we perform in-network analysis of the network data by exploiting the power of programmable data plane. We propose SwitchTree which embeds Random Forest algorithm inside a programmable switch such that the Random Forest is configurable and re-configurable at runtime. We show how some flow level stateful features can be estimated, such as the round-trip time and bitrate of each flow. We evaluate the performance of SwitchTree using system level experiments and network traces. Results show that SwitchTree is able to detect network attacks at line speed with high accuracy.},
  archive      = {J_NCA},
  author       = {Lee, Jong-Hyouk and Singh, Kamal},
  doi          = {10.1007/s00521-020-05440-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23143-23154},
  shortjournal = {Neural Comput. Appl.},
  title        = {SwitchTree: In-network computing and traffic analyses with random forests},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TermInformer: Unsupervised term mining and analysis in biomedical literature. <em>NCA</em>, <em>37</em>(28), 23129-23142. (<a href='https://doi.org/10.1007/s00521-020-05335-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terminology is the most basic information that researchers and literature analysis systems need to understand. Mining terms and revealing the semantic relationships between terms can help biomedical researchers find solutions to some major health problems and motivate researchers to explore innovative biomedical research issues. However, how to mine terms from biomedical literature remains a challenge. At present, the research on text segmentation in natural language processing (NLP) technology has not been well applied in the biomedical field. Named entity recognition models usually require a large amount of training corpus, and the types of entities that the model can recognize are limited. Besides, dictionary-based methods mainly use pre-established vocabularies to match the text. However, this method can only match terms in a specific field, and the process of collecting terms is time-consuming and labour-intensive. Many scenarios faced in the field of biomedical research are unsupervised, i.e. unlabelled corpora, and the system may not have much prior knowledge. This paper proposes the TermInformer project, which aims to mine the meaning of terms in an open fashion by calculating terms and find solutions to some of the significant problems in our society. We propose an unsupervised method that can automatically mine terms in the text without relying on external resources. Our method can generally be applied to any document data. Combined with the word vector training algorithm, we can obtain reusable term embeddings, which can be used in any NLP downstream application. This paper compares term embeddings with existing word embeddings. The results show that our method can better reflect the semantic relationship between terms. Finally, we use the proposed method to find potential factors and treatments for lung cancer, breast cancer, and coronavirus.},
  archive      = {J_NCA},
  author       = {Tiwari, Prayag and Uprety, Sagar and Dehdashti, Shahram and Hossain, M. Shamim},
  doi          = {10.1007/s00521-020-05335-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23129-23142},
  shortjournal = {Neural Comput. Appl.},
  title        = {TermInformer: Unsupervised term mining and analysis in biomedical literature},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bio-inspired computation for big data fusion, storage, processing, learning and visualization: State of the art and future directions. <em>NCA</em>, <em>37</em>(28), 23097-23127. (<a href='https://doi.org/10.1007/s00521-021-06332-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This overview gravitates on research achievements that have recently emerged from the confluence between Big Data technologies and bio-inspired computation. A manifold of reasons can be identified for the profitable synergy between these two paradigms, all rooted on the adaptability, intelligence and robustness that biologically inspired principles can provide to technologies aimed to manage, retrieve, fuse and process Big Data efficiently. We delve into this research field by first analyzing in depth the existing literature, with a focus on advances reported in the last few years. This prior literature analysis is complemented by an identification of the new trends and open challenges in Big Data that remain unsolved to date, and that can be effectively addressed by bio-inspired algorithms. As a second contribution, this work elaborates on how bio-inspired algorithms need to be adapted for their use in a Big Data context, in which data fusion becomes crucial as a previous step to allow processing and mining several and potentially heterogeneous data sources. This analysis allows exploring and comparing the scope and efficiency of existing approaches across different problems and domains, with the purpose of identifying new potential applications and research niches. Finally, this survey highlights open issues that remain unsolved to date in this research avenue, alongside a prescription of recommendations for future research.},
  archive      = {J_NCA},
  author       = {Torre-Bastida, Ana I. and Díaz-de-Arcaya, Josu and Osaba, Eneko and Muhammad, Khan and Camacho, David and Del Ser, Javier},
  doi          = {10.1007/s00521-021-06332-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23097-23127},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bio-inspired computation for big data fusion, storage, processing, learning and visualization: State of the art and future directions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-sync: A novel deep learning-based tool for semantic-aware subtitling synchronisation. <em>NCA</em>, <em>37</em>(28), 23081-23095. (<a href='https://doi.org/10.1007/s00521-021-05751-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subtitles are a key element to make any media content accessible for people who suffer from hearing impairment and for elderly people, but also useful when watching TV in a noisy environment or learning new languages. Most of the time, subtitles are generated manually in advance, building a verbatim and synchronised transcription of the audio. However, in TV live broadcasts, captions are created in real time by a re-speaker with the help of a voice recognition software, which inevitability leads to delays and lack of synchronisation. In this paper, we present Deep-Sync, a tool for the alignment of subtitles with the audio-visual content. The architecture integrates a deep language representation model and a real-time voice recognition software to build a semantic-aware alignment tool that successfully aligns most of the subtitles even when there is no direct correspondence between the re-speaker and the audio content. In order to avoid any kind of censorship, Deep-Sync can be deployed directly on users’ TVs causing a small delay to perform the alignment, but avoiding to delay the signal at the broadcaster station. Deep-Sync was compared with other subtitles alignment tool, showing that our proposal is able to improve the synchronisation in all tested cases.},
  archive      = {J_NCA},
  author       = {Martín, Alejandro and González-Carrasco, Israel and Rodriguez-Fernandez, Victor and Souto-Rico, Mónica and Camacho, David and Ruiz-Mezcua, Belén},
  doi          = {10.1007/s00521-021-05751-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23081-23095},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep-sync: A novel deep learning-based tool for semantic-aware subtitling synchronisation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking vital signs of a patient using channel state information and machine learning for a smart healthcare system. <em>NCA</em>, <em>37</em>(28), 23065-23079. (<a href='https://doi.org/10.1007/s00521-020-05631-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a smart healthcare system, the sensor-embedded wearable devices have the ability to track various vital signs of a patient. However, such devices need to be worn by the patients all the time. These devices have limitations such as their battery lifetime, charging mechanism, and hardware-related cost. Moreover, these devices transmit a huge amount of redundant and inconsistent data. The transmitted data need to be fused to remove any outlier so that only highly- refined data are available for decision-making. In this paper, we use channel state information (CSI) to track the vital signs of a patient and remove any outliers from the gathered data. We monitor the respiration rate of a patient during sleep with minimal hardware-related cost. Our CSI-based approach no longer requires the patients to wear any wearables and can monitor even the minute fluctuations in a WiFi signal. For extracting useful features from the respiratory data, three types of feature extraction techniques are used. In order to select important features from the extracted feature space, three feature selection algorithms, i.e., Relief, mRMR, and Lasso, have been investigated. In addition, for predicting the health conditions of a patient, four machine learning classification algorithms, i.e., KNN, SVM, DT, and RF, are utilized. The use of CSI ensures that highly refined and fused data are available for feature selection, and the selected features are presented to the ML classification algorithms for predicting the health condition of the patient.},
  archive      = {J_NCA},
  author       = {Khan, Muhammad Imran and Jan, Mian Ahmad and Muhammad, Yar and Do, Dinh-Thuan and Rehman, Ateeq ur and Mavromoustakis, Constandinos X. and Pallis, Evangelos},
  doi          = {10.1007/s00521-020-05631-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23065-23079},
  shortjournal = {Neural Comput. Appl.},
  title        = {Tracking vital signs of a patient using channel state information and machine learning for a smart healthcare system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convert index trading to option strategies via LSTM architecture. <em>NCA</em>, <em>37</em>(28), 23047-23064. (<a href='https://doi.org/10.1007/s00521-020-05377-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past, most strategies were mainly designed to focus on stocks or futures as the trading target. However, due to the enormous number of companies in the market, it is not easy to select a set of stocks or futures for investment. By investigating each company’s financial situation and the trend of the overall financial market, people can invest precisely in the market and choose to go long or short. Moreover, how to determine the position size of the transaction is also a problematic issue. In the past, many money management theories were based on the Kelly criterion. And they put a certain percentage of their total funds into the market for trading. Nonetheless, three massive problems cannot be overcome. First, futures are leveraged transactions, and extra funds must be deposited as margin. It causes that the position size is hard to be estimated by the Kelly criterion. The second point is that the trading strategy is difficult to determine the winning rate in the financial market and cannot be brought into the Kelly criterion to calculate the optimal fraction. Last, the financial data are always massive. A big data technique should be applied to resolve this issue and enhance the performance of the framework to reveal knowledge in the financial data. Therefore, in this paper, a concept of converting the original futures trading strategy into options trading is proposed. An LSTM (long short-term memory)-based framework is proposed to predict the profit probability of the original futures strategy and convert the corresponding daily take-profit and stop-loss points according to the delta value of the options. Finally, the proposed framework brings the results into the Kelly criterion to get the optimal fraction of options trading. The final research results show that options trading is closer to the optimal fraction calculated by the Kelly criterion than futures trading. If the original futures trading strategy can profit, the benefits after converting to options trading can be further superior.},
  archive      = {J_NCA},
  author       = {Wu, Jimmy Ming-Tai and Wu, Mu-En and Hung, Pang-Jen and Hassan, Mohammad Mehedi and Fortino, Giancarlo},
  doi          = {10.1007/s00521-020-05377-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23047-23064},
  shortjournal = {Neural Comput. Appl.},
  title        = {Convert index trading to option strategies via LSTM architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based driver distraction identification framework over edge cloud. <em>NCA</em>, <em>37</em>(28), 23031-23046. (<a href='https://doi.org/10.1007/s00521-020-05328-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the number of traffic accidents has been increased globally. One of the main reasons for this increase is the distraction of the driver on the road. Distracted driving can cause collisions and cause injury, death, or property damage. New techniques can help to mitigate this problem, and one of the recent approaches is to employ body wearable sensors or camera sensors in the vehicle for real-time monitoring and detection of drivers’ distraction and behaviors, such as cell phone use, talking, eating, drinking, radio tuning, navigation interaction, or even combing hair while driving. However, this type of approach requires not only a powerful training module but also a lightweight module for real-time detection and analyzing the captured data. Data need to be collected from specific wearable or camera sensors in order to detect drivers’ distraction and ensure immediate feedback by the administrator for safe driving. Therefore, in this paper, we propose an effective camera-based framework for real-time identification of drivers’ distraction by using a deep learning approach with edge and cloud computing technologies. More specifically, the framework consists of three modules, including the distraction detection module deployed on edge devices in the vehicle environment, the training module deployed in the cloud environment, and finally the analyzing module implemented in the monitoring environment (administrator side) connected with a telecommunication network. The proposed framework is developed using two deep learning models. The first is a custom deep convolutional neural network (CDCNN) model, and the second one is a visual geometry group-16 (VGG16)-based fine-tuned model. Several experiments are conducted on a public large-scale driver distraction dataset to evaluate the two models. The experimental results show that the accuracy rates were 99.64% for the first model and 99.73% for the second model using a holdout test set of 10%. In addition, the first and second models have achieved accuracy rates of 99.36% and 99.57% using a holdout test set of 30%. The results confirmed the applicability and appropriateness of the adopted deep learning models for designing the proposed driver distraction detection framework.},
  archive      = {J_NCA},
  author       = {Gumaei, Abdu and Al-Rakhami, Mabrook and Hassan, Mohammad Mehedi and Alamri, Atif and Alhussein, Musaed and Razzaque, Md. Abdur and Fortino, Giancarlo},
  doi          = {10.1007/s00521-020-05328-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23031-23046},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning-based driver distraction identification framework over edge cloud},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeTrAs: Deep learning-based healthcare framework for IoT-based assistance of alzheimer patients. <em>NCA</em>, <em>37</em>(28), 23017-23029. (<a href='https://doi.org/10.1007/s00521-020-05327-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare 4.0 paradigm aims at realization of data-driven and patient-centric health systems wherein advanced sensors can be deployed to provide personalized assistance. Hence, extreme mentally affected patients from diseases like Alzheimer can be assisted using sophisticated algorithms and enabling technologies. Motivated from this fact, in this paper, DeTrAs: Deep Learning-based Internet of Health Framework for the Assistance of Alzheimer Patients is proposed. DeTrAs works in three phases: (1) A recurrent neural network-based Alzheimer prediction scheme is proposed which uses sensory movement data, (2) an ensemble approach for abnormality tracking for Alzheimer patients is designed which comprises two parts: (a) convolutional neural network-based emotion detection scheme and (b) timestamp window-based natural language processing scheme, and (3) an IoT-based assistance mechanism for the Alzheimer patients is also presented. The evaluation of DeTrAs depicts almost 10–20% improvement in terms of accuracy in contrast to the different existing machine learning algorithms.},
  archive      = {J_NCA},
  author       = {Sharma, Sumit and Dudeja, Rajan Kumar and Aujla, Gagangeet Singh and Bali, Rasmeet Singh and Kumar, Neeraj},
  doi          = {10.1007/s00521-020-05327-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23017-23029},
  shortjournal = {Neural Comput. Appl.},
  title        = {DeTrAs: Deep learning-based healthcare framework for IoT-based assistance of alzheimer patients},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal medical image fusion algorithm in the era of big data. <em>NCA</em>, <em>37</em>(28), 22995-23015. (<a href='https://doi.org/10.1007/s00521-020-05173-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image-based medical decision-making, different modalities of medical images of a given organ of a patient are captured. Each of these images will represent a modality that will render the examined organ differently, leading to different observations of a given phenomenon (such as stroke). The accurate analysis of each of these modalities promotes the detection of more appropriate medical decisions. Multimodal medical imaging is a research field that consists in the development of robust algorithms that can enable the fusion of image information acquired by different sets of modalities. In this paper, a novel multimodal medical image fusion algorithm is proposed for a wide range of medical diagnostic problems. It is based on the application of a boundary measured pulse-coupled neural network fusion strategy and an energy attribute fusion strategy in a non-subsampled shearlet transform domain. Our algorithm was validated in dataset with modalities of several diseases, namely glioma, Alzheimer’s, and metastatic bronchogenic carcinoma, which contain more than 100 image pairs. Qualitative and quantitative evaluation verifies that the proposed algorithm outperforms most of the current algorithms, providing important ideas for medical diagnosis.},
  archive      = {J_NCA},
  author       = {Tan, Wei and Tiwari, Prayag and Pandey, Hari Mohan and Moreira, Catarina and Jaiswal, Amit Kumar},
  doi          = {10.1007/s00521-020-05173-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {22995-23015},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal medical image fusion algorithm in the era of big data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-series forecasting of bitcoin prices using high-dimensional features: A machine learning approach. <em>NCA</em>, <em>37</em>(28), 22979-22993. (<a href='https://doi.org/10.1007/s00521-020-05129-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh–ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.},
  archive      = {J_NCA},
  author       = {Mudassir, Mohammed and Bennbaia, Shada and Unal, Devrim and Hammoudeh, Mohammad},
  doi          = {10.1007/s00521-020-05129-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {22979-22993},
  shortjournal = {Neural Comput. Appl.},
  title        = {Time-series forecasting of bitcoin prices using high-dimensional features: A machine learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
