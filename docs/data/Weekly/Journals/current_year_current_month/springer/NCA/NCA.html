<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NCA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nca">NCA - 115</h2>
<ul>
<li><details>
<summary>
(2025). Discriminating psychological stress levels: Multi-level attentive LSTM approach. <em>NCA</em>, <em>37</em>(30), 25579-25599. (<a href='https://doi.org/10.1007/s00521-025-11608-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this challenging era, psychological stress has become a pervasive issue affecting human well-being. Timely identification of stress, especially through physiological signals such as Electrocardiograms (ECG), is essential for early intervention and effective stress management, as these signals offer objective, real-time insights into autonomic nervous system responses that are closely linked to stress levels. While existing research largely focuses on binary classification (e.g., stressed vs. non-stressed) or multi-level classification restricted to specific domains such as workplace environments or academic settings, there remains a gap in developing a generalized model applicable across diverse real-world conditions. This study introduces a novel Multi-Level Attentive LSTM Network (MALN) for classifying real-world ECG signals into three stress levels: normal, low, and mid. The approach leverages the robustness of deep learning in analyzing complex biomedical time-series data. ECG signals from drivers (a high-stress occupational domain) and graduate students (an academic stress domain) are transformed into time-dependent HRV and spectral sequences to preserve meaningful temporal and frequency-based characteristics. Two attention-guided Bi-directional LSTM modules are designed to extract relevant patterns from these sequences. Their outputs are concatenated and passed through a fully connected layer to distinguish the features into three stress levels. Experimental evaluations conducted on a public drivers’ ECG dataset and a custom-built student ECG dataset show that the proposed model effectively generalizes across distinct domains and outperforms state-of-the-art methods in multi-level stress classification. This work is available at https://github.com/Ramyashri14-0593/Stress-Level-Classification .},
  archive      = {J_NCA},
  author       = {Ramteke, Ramyashri B. and Gajbhiye, Gaurav O. and Thool, Vijaya R.},
  doi          = {10.1007/s00521-025-11608-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25579-25599},
  shortjournal = {Neural Comput. Appl.},
  title        = {Discriminating psychological stress levels: Multi-level attentive LSTM approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling epidemic dynamics: Harnessing the synergy of social media data and mobility patterns during COVID-19. <em>NCA</em>, <em>37</em>(30), 25555-25578. (<a href='https://doi.org/10.1007/s00521-025-11606-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of social media data and mobility patterns presents a unique opportunity to delve deeper into societal behavior and its implications on epidemic dynamics. Social media platforms have become veritable repositories of real-time, user-generated data, reflecting public sentiments, discussions, and perceptions regarding health concerns, including infectious diseases. Concurrently, mobility patterns, derived from transportation usage, geolocation services, and movement data, offer insights into population movements and travel behaviors critical for understanding disease spread. This paper proposes an approach exploiting social media data and mobility patterns to perform epidemic predictions, whose experimental evaluation has been carried out on COVID-19 pandemic data. Leveraging these diverse data sources, we aim to uncover synergies and correlations between online discussions and travel behaviors that can contribute to more accurate and proactive epidemic forecasts. Our central focus lies in discerning the alignment between peaks in social media discussions and corresponding fluctuations in mobility patterns. By identifying and analyzing these alignments, our aim is to clarify their potential as predictive indicators for upcoming epidemic trends. Results obtained from real-world datasets about the city of Chicago (USA) demonstrate the efficacy of the proposed method in predicting the spread of the epidemic accurately. The explainability analysis reveals a significant correlation between tweet content and actual COVID-19 data, affirming Twitter’s credibility as a dependable indicator of epidemic spread. This underscores the growing importance of social media user-generated data as a valuable resource for monitoring and comprehending epidemic outbreaks.},
  archive      = {J_NCA},
  author       = {Cesario, Eugenio and Comito, Carmela},
  doi          = {10.1007/s00521-025-11606-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25555-25578},
  shortjournal = {Neural Comput. Appl.},
  title        = {Unveiling epidemic dynamics: Harnessing the synergy of social media data and mobility patterns during COVID-19},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain tumor diagnosis using hybrid quantum-classical neural networks. <em>NCA</em>, <em>37</em>(30), 25535-25554. (<a href='https://doi.org/10.1007/s00521-025-11601-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor classification has become a critical entry point in enhancing the diagnosis and the subsequent management in medical imaging. In an attempt to design an efficient classifier for brain tumors, this study introduces the hybrid quantum-classical neural networks (H-QNNs). The model comprises standard convolutional layers for the extraction of relevant features from input images with a quantum neural network based on ZZFeatureMap with RealAmplitudes Ansatz for enhanced feature identification. MRI images of four tumor classes were assessed in the study, which include glioma, meningioma, pituitary tumor, and no tumor. The proposed framework led to a classification accuracy of 95.58% with precision, recall, and F1-score at 95.58%, significantly surpassing legacy methods. Data imbalance was resolved using data augmentation and weighted cross-entropy loss, while methods such as weight decay were used to increase model resilience. Findings indicate the future application of QML in improving the efficiency of computer-aided image analysis for medical diagnosis and accuracy of classification. Specific strategies mentioned for future work include extending the method to larger datasets and improving quantum components to support additional tasks.},
  archive      = {J_NCA},
  author       = {Ranga, Deepak and Prajapat, Sunil and Kumar, Kranti and Kumar, Pankaj},
  doi          = {10.1007/s00521-025-11601-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25535-25554},
  shortjournal = {Neural Comput. Appl.},
  title        = {Brain tumor diagnosis using hybrid quantum-classical neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized deep CNN with rotation-driven features for malaria parasite detection. <em>NCA</em>, <em>37</em>(30), 25515-25534. (<a href='https://doi.org/10.1007/s00521-025-11598-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual microscopic analysis of blood smears for malaria detection is labor-intensive, time-consuming, and prone to human error. To address these limitations, this study proposes a self-supervised learning approach using RotNet, a rotation-based model, to differentiate between healthy and malaria-infected cells. RotNet learns image representations by predicting the rotation angle of input images, eliminating the need for manually labeled data. This method uses synthetic rotation labels to train the model, significantly reducing dependency on human annotations. The core contribution of this work lies in using embeddings and weights from a rotation-based pretext task on unlabeled images to improve classification accuracy in a downstream convolutional neural network (CNN) task. Key highlights include: (1) an efficient and accurate deep learning solution for malaria detection, (2) a novel rotation-based self-supervised strategy, and (3) a practical approach to tackle the scarcity of labeled medical data. The proposed method was tested on the National Institutes of Health (NIH) malaria dataset, achieving a remarkable AUC of 99.8%, surpassing previously reported results. Moreover, similar accuracy was attained using only 10% of the labeled data compared to fully supervised training, highlighting the effectiveness of self-supervised pretraining. Overall, RotNet demonstrates high potential as a robust, scalable, and efficient tool for automated malaria diagnosis.},
  archive      = {J_NCA},
  author       = {Kumar, Sudhakar and Singh, Sunil K. and Mengi, Gopal and Singh, Animesh and Dubey, Arun Kumar and Gupta, Brij B. and Alhalabi, Wadee and Arya, Varsha and Nedjah, Nadia},
  doi          = {10.1007/s00521-025-11598-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25515-25534},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized deep CNN with rotation-driven features for malaria parasite detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EfficientOvaNet: Efficient deep learning model for multiclass classification of benign ovarian cyst using ultrasound images. <em>NCA</em>, <em>37</em>(30), 25495-25514. (<a href='https://doi.org/10.1007/s00521-025-11589-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ovarian cysts are common conditions in women, potentially leading to infertility, malignancy, or torsion if not identified and treated early. Artificial intelligence (AI) has proven effective in medical imaging; however, most research focuses on distinguishing between benign and malignant cysts. it is equally important to classify specific cyst types to aid in ovarian cancer prevention. This study aims to develop an efficient model to classify seven benign cyst types—Polycystic Ovary Syndrome (PCOS), Serous, Carcinoma, Dermoid, Hemorrhagic, Complex, and Simple cysts using ultrasound images. A novel deep transfer learning model, EfficientOvaNet, was proposed and trained on ultrasound images of ovarian cysts. Diverse data augmentation techniques were applied to address class imbalance issues. The model’s performance was benchmarked against established deep learning architectures, including VGG16, VGG19, ResNet50, DenseNet201, and InceptionV3, using metrics such as accuracy, precision, recall, and F1-score. EfficientOvaNet achieved superior performance across all evaluated metrics, outperforming state-of-the-art models in accurately identifying all seven types of ovarian cysts. The model demonstrated high reliability and robustness, addressing class imbalance effectively and ensuring accurate predictions. EfficientOvaNet offers a promising solution for accurate classification of ovarian cysts, contributing to early diagnosis, improved treatment strategies, and ovarian cancer prevention. This study highlights the model’s potential for real-world applications in medical imaging and its capability to overcome significant challenges such as data imbalance.},
  archive      = {J_NCA},
  author       = {Parekh, Aarti and Desai, Madhavi},
  doi          = {10.1007/s00521-025-11589-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25495-25514},
  shortjournal = {Neural Comput. Appl.},
  title        = {EfficientOvaNet: Efficient deep learning model for multiclass classification of benign ovarian cyst using ultrasound images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding analysis in inhibition response classification using sinc-EEGNet and partial directed coherence. <em>NCA</em>, <em>37</em>(30), 25473-25493. (<a href='https://doi.org/10.1007/s00521-025-11588-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous model networks have been developed and studied to classify the electroencephalogram (EEG) signals and achieve impressive classification accuracy. Despite the advancements in classification accuracy, there is a notable lack of research into understanding how these models interpret and represent the intricate features of EEG signals. Thus, understanding the neural dynamics underlying cognitive control is essential for advancing EEG-based classification models. This study investigates the interpretability of the Sinc-EEGNet model by integrating feature visualization of the spatial filters and Sinc-kernels, as well as partial directed coherence (PDC) analysis, with a specific focus on the destination channels activated during inhibitory processes. The Sinc-EEGNet model was applied to EEG data of twenty healthy subjects doing the Stroop task, a well-known test of cognitive control, to evaluate its ability to inhibit response. The methodology involves training the Sinc-EEGNet model on preprocessed EEG signals with Sinc-based filters and applying feature visualization techniques to understand the model’s output. PDC analysis is then employed to validate the spatial filter activation and identify the neural connections during inhibitory processes. Results demonstrate that the model achieves a classification accuracy of 97%, with clear distinctions in neural activations between subjects. Integrating feature visualization with PDC enhances model interpretability, providing insights into the neural dynamics of inhibitory control and confirming the effectiveness of Sinc-EEGNet in EEG-based classification tasks.},
  archive      = {J_NCA},
  author       = {Sahar, Noor Syazwana and Safri, Norlaili Mat and Izzuddin, Tarmizi and Zakaria, Nor Aini and Kadir, Nurul Ashikin Abdul},
  doi          = {10.1007/s00521-025-11588-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25473-25493},
  shortjournal = {Neural Comput. Appl.},
  title        = {Encoding analysis in inhibition response classification using sinc-EEGNet and partial directed coherence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Financial asset price prediction with graph neural network-based temporal deep learning models. <em>NCA</em>, <em>37</em>(30), 25445-25471. (<a href='https://doi.org/10.1007/s00521-025-11586-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in the prices of multiple financial assets over time can be characterized by their complex nature and interdependence. More traditional forecasting approaches may overlook the interdependencies among these assets, since they may not fully consider the spatial-temporal dependencies between them. Graph neural networks (GNNs) have emerged as powerful tools for modeling complex relational dependencies in areas such as social network analysis and traffic forecasting. However, their application in asset price prediction remains relatively unexplored. Here, we investigate GNNs’ effectiveness in forecasting multiple financial asset prices jointly, specifically in the foreign exchange (Forex) and cryptocurrency markets. We employ three spatio-temporal GNN frameworks-MTGNN, StemGNN, and FourierGNN-which are all recognized for their state-of-the-art performance in forecasting multivariate time series. These models transform time-series data into graphs and capture both spatial and temporal dependencies. They significantly outperform the baseline methods, including LSTM, ARIMA, and VAR, in predicting financial asset prices in the highly volatile cryptocurrency market. While the performance gap is less obvious in the relatively stable Forex market, GNN-based models still demonstrate a general advantage over LSTM, although they are outperformed by ARIMA. Through a series of experiments and backtesting strategies, we assess the predictive power and profitability of these models in portfolio construction. Our code and datasets are publicly available at https://github.com/seferlab/temporal gnn .},
  archive      = {J_NCA},
  author       = {Uygun, Yasin and Sefer, Emre},
  doi          = {10.1007/s00521-025-11586-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25445-25471},
  shortjournal = {Neural Comput. Appl.},
  title        = {Financial asset price prediction with graph neural network-based temporal deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel metaheuristic algorithm inspired by tree pruning for solving benchmark optimization and engineering design problems. <em>NCA</em>, <em>37</em>(30), 25415-25443. (<a href='https://doi.org/10.1007/s00521-025-11584-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the large number of metaheuristic algorithms developed for solving optimization problems, nature-inspired algorithms still face challenges such as premature convergence, getting trapped in local optima, and failing to maintain a proper balance between exploration and exploitation. This study aims to develop a robust optimization algorithm that addresses these limitations by maintaining an effective balance between global exploration and local exploitation, thereby improving convergence toward optimal solutions. In this paper, the tree pruning optimization (TPO) algorithm is proposed, inspired by arborists' tree pruning methods. TPO begins by removing dead or diseased branches and then selects the best leader by eliminating competing leaders. The removal of weaker and suckering branches represents the exploration phase, while selecting a dominant leader and removing other competing branches represents the exploitation phase. These two phases are controlled using a random variable. Elitism and a crossover operator are employed to select the best branches for the next iteration. To evaluate the proposed TPO algorithm, benchmark suites from CEC2019 (10 functions) and CEC2022 (12 functions) were used, comparing TPO against nine well-known and competitive optimization algorithms. TPO outperformed all compared algorithms individually in at least 7 out of 10 functions in CEC2019 and 8 out of 12 functions in CEC2022. It achieved strong average ranking scores of 1.8 and 1.4, respectively. Statistical analyses confirmed the superior performance of TPO in most cases, supported by p values (p < 0.05) and boxplot visualizations. Furthermore, TPO demonstrated its effectiveness on real-world engineering problems. It achieved the best results and ranked first in solving the speed reducer design, tension/compression spring design, and welded beam design problems, outperforming all competitors in terms of minimizing cost and satisfying constraints.},
  archive      = {J_NCA},
  author       = {Mohammed, Hardi M.},
  doi          = {10.1007/s00521-025-11584-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25415-25443},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel metaheuristic algorithm inspired by tree pruning for solving benchmark optimization and engineering design problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MindfulLIME: A stable solution for explanations of machine learning models with enhanced localization precision—a medical image case study. <em>NCA</em>, <em>37</em>(30), 25387-25413. (<a href='https://doi.org/10.1007/s00521-025-11583-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring transparency in machine learning decisions is critically important, especially in sensitive sectors such as healthcare, finance, and justice. Despite this, some popular explainable algorithms, such as Local Interpretable Model-agnostic Explanations (LIME), often produce unstable explanations due to the random generation of perturbed samples. Random perturbation introduces small changes or noise to modified instances of the original data, leading to inconsistent explanations. Even slight variations in the generated samples significantly affect the explanations provided by such models, undermining trust and hindering the adoption of interpretable models. To address this challenge, we propose MindfulLIME, a novel algorithm that intelligently generates purposive samples using a graph-based pruning algorithm and uncertainty sampling. MindfulLIME substantially improves the consistency of visual explanations compared to random sampling approaches. Our experimental evaluation, conducted on a widely recognized chest X-ray dataset, confirms MindfulLIME’s stability with a 100% success rate in delivering reliable explanations under identical conditions. Additionally, MindfulLIME improves the localization precision of visual explanations by reducing the distance between the generated explanations and the actual local annotations compared to LIME. We also performed comprehensive experiments considering various segmentation algorithms and sample numbers, focusing on stability, quality, and efficiency. The results demonstrate the outstanding performance of MindfulLIME across different segmentation settings, generating fewer high-quality samples within a reasonable processing time. By addressing the stability limitations of LIME in the context of image data, MindfulLIME notably contributes to enhancing the trustworthiness and interpretability of machine learning models applied to specific medical images, which is a critical application.},
  archive      = {J_NCA},
  author       = {Rahimiaghdam, Shakiba and Alemdar, Hande},
  doi          = {10.1007/s00521-025-11583-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25387-25413},
  shortjournal = {Neural Comput. Appl.},
  title        = {MindfulLIME: A stable solution for explanations of machine learning models with enhanced localization precision—a medical image case study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing algorithmic trading with wavelet-based deep reinforcement learning: A multi-indicator approach. <em>NCA</em>, <em>37</em>(30), 25339-25385. (<a href='https://doi.org/10.1007/s00521-025-11581-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates wavelet-enhanced deep reinforcement learning (DRL) for trading S&P 500 futures, assessing four wavelet families (Daubechies, Symlets, Coiflets, and Biorthogonal) in conjunction with three DRL algorithms (PPO, A2C, and DQN). We employ level-2 decomposition utilizing conservative soft thresholding on market microstructure indicators (DIX, GEX, VIX), enhancing signal-to-noise ratios by 25–41 dB. The coif4+DQN combination yields the most robust outcomes (Sharpe ratio: 0.96; total return: 112.5%), while A2C+db1 and PPO+db4 attain Sharpe ratios of 0.803 and 0.801, respectively. In comparison with XGBoost, random forest, and Logistic regression, wavelet-DRL attains a 35–70% superior Sharpe ratio and reduces maximum drawdowns (0.28–0.34 vs. 0.39–0.42). Feature-set analysis reveals that DIX, GEX, and VIX combined surpass single-indicator configurations by 18.7% in Sharpe ratio. Statistical analyses validate the robustness, revealing that 82.1% of maximum drawdown (MDD) and 67.9% of Sharpe ratio comparisons are significant at the 5% level. Our findings support a gradual implementation strategy—transitioning from A2C+db1 to DQN+coif4—to enhance institutional algorithmic trading efficacy.},
  archive      = {J_NCA},
  author       = {Casares, Antonio José Martínez},
  doi          = {10.1007/s00521-025-11581-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25339-25385},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing algorithmic trading with wavelet-based deep reinforcement learning: A multi-indicator approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of neural ordinary differential equations and neural operators for modeling temporal dynamics. <em>NCA</em>, <em>37</em>(30), 25319-25338. (<a href='https://doi.org/10.1007/s00521-025-11580-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the dynamics of relational systems is a key challenge in the natural sciences, with applications ranging from simulating molecular interactions to analyzing particle mechanics. Machine learning approaches have made significant progress in this area by using graph neural networks to learn and visualize spatial interactions effectively. Neural ordinary differential equations (Neural ODEs) and neural operators (NO) represent two distinct paradigms. However, a clear comparative understanding of when to prefer one over the other is still lacking. To address this gap, we present the first systematic comparison between two representative architectures: EGNO (Equivariant Graph Neural Operator) and SEGNO (Second-order Equivariant Graph Neural Ordinary Differential Equation). Through a series of experiments, we investigate their strengths and limitations in various simulation scenarios in the multi-step trajectory prediction tasks. Specifically, we employ rollout strategies and different input/output configurations, including multiple and irregularly sampled time steps. Our findings highlight a key trade-off between precision and stability that is central to model selection. SEGNO demonstrates superior robustness and stability over long prediction horizons, making it well-suited for tasks requiring reliable long-term forecasting. Conversely, EGNO offers higher precision during early stages of the trajectory and better leverages diverse training configurations, thanks to its discretization-invariant design. In summary, Neural Operators (EGNO) are preferable when short-term accuracy and data efficiency are critical, while Neural ODEs (SEGNO) are advantageous for scenarios demanding stable long-term predictions. This work not only clarifies the practical advantages of each approach but also lays the groundwork for informed model selection and future hybrid strategies in dynamical system modeling.},
  archive      = {J_NCA},
  author       = {Celia, Matteo and Monaco, Simone and Apiletti, Daniele},
  doi          = {10.1007/s00521-025-11580-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25319-25338},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comparative study of neural ordinary differential equations and neural operators for modeling temporal dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bioinspired optimization on controlled anthropomorphic manipulator robots. <em>NCA</em>, <em>37</em>(30), 25283-25317. (<a href='https://doi.org/10.1007/s00521-025-11579-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bioinspired optimization algorithms, derived from biological processes such as bacterial foraging and swarm behavior, have shown increasing potential in addressing high-dimensional, nonlinear, and time-varying problems in engineering. Their integration into robotic control architectures enables the development of adaptive, model-flexible schemes that are robust to uncertainty and real-time constraints. Anthropomorphic manipulator robots, widely used in manufacturing and medical applications, require high-performance motion control under structural uncertainty, dynamic perturbations, and limited sensing. This paper proposes a unified and robust control scheme that integrates three key components: (i) a bacterial foraging optimization algorithm for offline initialization of controller weights, (ii) B-spline artificial neural networks for online adjustment of adaptive control gains, and (iii) a robust motion control law based on integral reconstruction theory, which eliminates the need for velocity measurement or full dynamic models and avoids high-gain compensation. This architecture overcomes several limitations of classical model-based, PID, or adaptive-only approaches by combining learning, compensation, and optimization within a scalable framework. The proposed method is validated through multiple simulation studies involving anthropomorphic manipulators with documented physical parameters and subjected to varying disturbances. Comparative analysis demonstrates superior tracking precision, reduced control effort, and faster convergence dynamics. These results confirm the practical viability of the proposed framework for motion control in dynamically uncertain robotic platforms.},
  archive      = {J_NCA},
  author       = {Galvan-Perez, Daniel and Beltran-Carbajal, Francisco and Yañez-Badillo, Hugo and Rivas-Cambero, Ivan and Gabbar, Hossam A. and Tapia-Olvera, Ruben},
  doi          = {10.1007/s00521-025-11579-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25283-25317},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bioinspired optimization on controlled anthropomorphic manipulator robots},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EV charging duration prediction using advanced ensemble learning techniques and feature importance analysis. <em>NCA</em>, <em>37</em>(30), 25257-25281. (<a href='https://doi.org/10.1007/s00521-025-11576-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden growth in electric vehicle (EV) usage has highlighted the need for advanced and efficient charging infrastructure. This research applies sophisticated ensemble learning models to actual charging session data and proposes a data-based approach to estimating the EV charging duration. The dataset comprises features like battery specs, charging session parameters, arrival and departure times, energy consumed, and temporal variables derived from departure time (hour, day of the week, month). Further extensive preprocessing was undertaken to treat the outliers, especially those from heavy-duty EVs, and to derive more helpful features such as the State of Charge (SOC) differential and charging ratio. The goal of this paper was to predict the charging duration, measured in minutes, using ensembles of algorithm frameworks: XGBoost, LightGBM, CatBoost, and Random Forest. Feature importance analysis was undertaken to evaluate the contribution of each variable to the prediction. In its best run, with an R2 of 0.90, an RMSE of 5.49, and a MAPE of 11.05%, XGBoost achieved the best performance, beating all the other models. This demonstrates the capability of ensemble methods in learning and simulating the EV charging behavior and thereby benefiting the management of charging stations. The study also provides some valuable insights into consumption patterns and possible congestion timings that can aid in better infrastructure planning and user experience.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Salem, Mohamed and Hamza, Alyaa A.},
  doi          = {10.1007/s00521-025-11576-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25257-25281},
  shortjournal = {Neural Comput. Appl.},
  title        = {EV charging duration prediction using advanced ensemble learning techniques and feature importance analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for house pricing: Evaluating categorical encoders and price correction indexes on real transaction data. <em>NCA</em>, <em>37</em>(30), 25229-25256. (<a href='https://doi.org/10.1007/s00521-025-11575-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real estate market is highly influenced by heterogeneous and subjective factors, making property pricing a challenging problem for predictive modeling. This study aims to evaluate the performance of machine learning (ML) models in predicting residential property sales values, focusing on the effect of categorical variable encoding and real estate price correction indexes. A dataset of 306 properties sold in Juiz de Fora, Brazil, was collected from real estate companies and brokers, including physical and locational attributes. Two algorithms were compared—Extreme Gradient Boosting (XGBoost) and Artificial Neural Networks (ANN), using tenfold cross-validation —combined with four encoders (Label, One Hot, Target, and James–Stein) and four Brazilian price correction indexes (IPCA, IGP-M, INCC, and FipeZAP). The R2 varied from 0.44 to 0.72, with the best results being achieved by the XGBoost model, the Target encoder, and the configuration without price correction (using month and year of sale as input, not any correction index). This result indicates that the ML algorithm can better overcome the temporality issue than generic/national inflation indexes. Feature importance analysis revealed that the number of bathrooms, property area, and number of rooms were the most influential variables. The study highlights that real transaction data combined with appropriate categorical encoding can significantly improve predictive accuracy, while public price indexes may not necessarily enhance performance due to regional and methodological differences. These findings contribute to data-driven property valuation, offering methodological insights for both researchers and practitioners.},
  archive      = {J_NCA},
  author       = {Pita, Romário Parreira and Carvalho, Aldo Ribeiro de and Castro, Júlia Assumpção de and Paixão, Rafael Christian Fonseca de and Cury, Alexandre Abrahão and Mendes, Julia Castro},
  doi          = {10.1007/s00521-025-11575-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25229-25256},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning for house pricing: Evaluating categorical encoders and price correction indexes on real transaction data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved transformer-based approach for industrial micro-object defect detection. <em>NCA</em>, <em>37</em>(30), 25201-25228. (<a href='https://doi.org/10.1007/s00521-025-11573-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-precision manufacturing, detecting small and complex defects is crucial for ensuring product quality and production efficiency. Industrial micro-defect detection is especially important in the mass production of micro-parts. Traditional methods struggle with accuracy and speed due to the small size, complexity, and high density of the targets. To address these challenges, this paper proposes a transformer-based method using glass beads as a case study, named Glass Bead Detection with Transformers (GLB-DETR). First, a partially convolutional feature extraction network is designed to improve detection accuracy for small targets while reducing model complexity. Next, an optimized hybrid encoder module is introduced to enhance cross-scale feature fusion and real-time detection efficiency. Finally, an improved shape intersection-over-union loss function is applied to improve localization accuracy for small targets. Experimental results show that GLB-DETR outperforms the Real-Time Detection Transformer and You Only Look Once models in both accuracy and real-time performance. Specifically, the mean Average Precision has improved by at least 3.06%, the number of model parameters has been reduced by at least 18.1%, and the detection speed has increased by at least 1.06%. The method also demonstrates strong generalization ability on public datasets, further validating its effectiveness and superiority.},
  archive      = {J_NCA},
  author       = {Gao, Shan and Wang, YuMeng and Ma, ZongFang},
  doi          = {10.1007/s00521-025-11573-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25201-25228},
  shortjournal = {Neural Comput. Appl.},
  title        = {An improved transformer-based approach for industrial micro-object defect detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmentation of microscopic images of dermatophytes in clinical samples using ResNet and attention-based modifications of U-net. <em>NCA</em>, <em>37</em>(30), 25183-25199. (<a href='https://doi.org/10.1007/s00521-025-11571-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct microscopic images of dermatophytes face the challenge of distracting artifacts such as cell debris from skin preparations, air bubbles, cellulose fibers, and also blurred images arising from out-of-focus areas, all of which make the diagnosis at the clinics cumbersome. Automated detection will be the preferred choice under such situations. Work in the area of semantic segmentation of dermatophytes has not yet been attempted, while a few attempts have been reported on the object detection and classification aspects. Our work focusses on the modification of the popular and efficient U-Net architecture with the introduction of residual, squeeze and excitation (SE), and attention-gating modules. A hybrid of weighted binary cross-entropy and dice loss functions was also used to obviate the imbalanced pixel class distribution. The dataset included images captured with 10 $$\times$$ and 40 $$\times$$ objective lenses. Significant increase has been progressively noticed in dice score with an appreciable improvement of about 8% and 13% in 10 $$\times$$ and 40 $$\times$$ magnifications, respectively. Qualitative performance evaluation conducted using heatmaps and predicted images permitted visual verification of the progress made. All the metrics evaluated gave satisfactory values which indicated better performance of the proposed model over the compared architectures.},
  archive      = {J_NCA},
  author       = {Rajitha, K. V. and Krishnamoorthy, Anusha and Prakash, P. Y. and Govindan, Sreejith and Rao, Raghavendra and Prasad, Keerthana},
  doi          = {10.1007/s00521-025-11571-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25183-25199},
  shortjournal = {Neural Comput. Appl.},
  title        = {Segmentation of microscopic images of dermatophytes in clinical samples using ResNet and attention-based modifications of U-net},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TD-CLNet: A time-distributed CNN-LSTM network for fault detection in belt conveyor idlers. <em>NCA</em>, <em>37</em>(30), 25151-25181. (<a href='https://doi.org/10.1007/s00521-025-11570-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection in belt conveyor idlers is crucial for minimising downtime and reducing maintenance costs in industrial operations. Traditional methods, like vibration or temperature-based monitoring, face limitations, including challenging sensor installation and restricted data accessibility. Moreover, these approaches often emphasise spatial features, neglecting the temporal dynamics essential for understanding idler performance over time. This study introduces TD-CLNet, a hybrid fault detection framework that leverages acoustic signals captured via contactless microphones processed through a Time-Distributed CNN-LSTM architecture. The model combines the spatial feature extraction capabilities of Convolutional Neural Networks (CNNs) with the temporal sequence modelling strengths of Long Short-Term Memory (LSTM) networks. A key innovation is the use of the Time-Distributed layer, which enables consistent feature extraction across individual log-Mel spectrogram frames while preserving their temporal relationships. This ensures a robust and coordinated learning process, efficiently addressing the challenges of detecting complementary and relevant features. The performance of TD-CLNet is compared to a frame-based feature extraction approach, which treats each log-Mel spectrogram frame as an independent sample, as well as traditional machine learning methods. Results demonstrate that TD-CLNet achieves a test accuracy of 92% on real-world idler data using K-fold cross-validation, significantly outperforming competing methods. This research provides a scalable and effective solution for fault detection in belt conveyor idlers, advancing predictive maintenance strategies, improving operational efficiency, and minimising unplanned downtime in industrial environments.},
  archive      = {J_NCA},
  author       = {Alharbi, Fahad and Luo, Suhuai and Yang, Guang},
  doi          = {10.1007/s00521-025-11570-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25151-25181},
  shortjournal = {Neural Comput. Appl.},
  title        = {TD-CLNet: A time-distributed CNN-LSTM network for fault detection in belt conveyor idlers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale residual attention model for retinal image feature segmentation: Toward chemical exposure detection. <em>NCA</em>, <em>37</em>(30), 25123-25149. (<a href='https://doi.org/10.1007/s00521-025-11567-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel methodology for detecting chemical-induced retinal damage through fundus imaging by leveraging the correlation between retinal abnormalities and chemical exposures. Fundus imaging, a noninvasive and cost-efficient diagnostic modality, is extensively used for detecting ophthalmic and systemic conditions. Pathological manifestations commonly include vascular abnormalities in blood vessels, optic disk deformities, and lesions such as hemorrhages and exudates. Accurate identification and segmentation of these features are crucial for clinically evaluating ocular anomalies. Many pathological conditions share similar manifestations associated with chemical exposure, common to well-known retinal diseases. This paper introduces a multi-scale residual attention U-Net model for retinal feature segmentation and comprehensively maps retinal abnormalities to chemical exposure. To capture broader contextual information, dilated convolutions with layer-specific dilation rates are incorporated into parallel paths within each layer of the U-Net architecture. Furthermore, the convolutional block attention module (CBAM) is integrated into each path to dynamically emphasize spatial and channel-wise features. We also introduce a weighted auxiliary loss function that utilizes output from each decoder block, forcing the model to learn features at a lower-level feature space and enhancing overall system performance. The proposed method is evaluated on three publicly available datasets: DRIVE and STARE for blood vessel segmentation and IDRiD for segmenting exudates, optic disks, and hemorrhages. Performance metrics, including specificity, sensitivity, accuracy, and AUC score, are employed to assess the model. The proposed method can be adapted for automated retinal screening systems for early detection of chemical exposure for public health surveillance and occupational health monitoring.},
  archive      = {J_NCA},
  author       = {Aouti, Rajesh and Redkar, Sangram and Dwivedi, Prabha},
  doi          = {10.1007/s00521-025-11567-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25123-25149},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-scale residual attention model for retinal image feature segmentation: Toward chemical exposure detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving pediatric trauma care: An automated system for wrist trauma detection using GELAN. <em>NCA</em>, <em>37</em>(30), 25095-25121. (<a href='https://doi.org/10.1007/s00521-025-11539-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trauma is a major cause of disability among children, requiring swift and accurate diagnosis for effective treatment. This paper introduces an automated method that uses deep learning to detect and categorize fractures in children using X-ray images. The system makes use of the GRAZPEDWRI-DX dataset, which consists of 20,327 annotated X-ray images of pediatric wrist fractures. Our architecture, which is built upon the generalized efficient layer aggregation network (GELAN), effectively tackles the issues of class imbalance and image resolution. As a result, it achieves state-of-the-art performance in both trauma and severity detection. Our proposed framework surpassed the most advanced techniques, showcasing exceptional precision and effectiveness, achieving a mean average precision (mAP50) score of 74.1%, 95%, and 85.5% for Task A (trauma detection), Task B (fracture detection), and Task C (fracture severity detection), respectively. The results of our study highlight the capacity of deep learning to improve the diagnosis of pediatric trauma, decrease the burden on radiologists, and boost patient outcomes.},
  archive      = {J_NCA},
  author       = {Basak, Promit and Mushtak, Adam and Ouda, Mohamed and Nobi, Sadia Farhana and Hasan, Anwarul and Chowdhury, Muhammad E. H.},
  doi          = {10.1007/s00521-025-11539-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25095-25121},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving pediatric trauma care: An automated system for wrist trauma detection using GELAN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDNER: Integrating MRC with diffusion models for enhanced named entity recognition. <em>NCA</em>, <em>37</em>(30), 25077-25094. (<a href='https://doi.org/10.1007/s00521-025-11533-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have recently achieved impressive results in generative tasks, extending their capabilities from continuous data to discrete text. However, these models still face challenges when it comes to accurately capturing discrete entity information, especially in tasks like named entity recognition (NER). To address this, we propose MDNER, a novel NER model that integrates diffusion models with a Machine Reading Comprehension (MRC) framework. Unlike DIFFUSIONNER, which relies solely on diffusion processes without semantic priors, MDNER introduces MRC-guided denoising to explicitly align label semantics and contextual representations during boundary refinement. During training, MDNER concatenates the input text with predefined queries and processes them through BERT to obtain enhanced contextual representations. It then gradually adds noise to the golden entity boundaries and trains the model to recover these boundaries through a denoising process. In inference, the process starts by sampling noisy entity boundaries from a Gaussian distribution and inputs them with the text into MDNER. MDNER iteratively refines these noisy boundaries using the learned denoising process. By reformulating the NER task as a denoising diffusion process on entity boundaries, MDNER leverages the shared denoising objectives of diffusion models and pre-trained language model (PLM) to generate entity spans from noise. Experiments demonstrate that MDNER achieves state-of-the-art results across flat and nested NER datasets, with improvements of +0.18% and +0.78% in F1 scores on CoNLL2003 and OntoNotes5.0, respectively. For nested NER datasets such as ACE2005, it achieves gains of +1.66% in F1 scores. These results highlight MDNER’s superior performance in complex entity recognition scenarios.},
  archive      = {J_NCA},
  author       = {Wang, Mengying and Lv, Wenxiu and Zhang, YuKun and Yan, Danfeng},
  doi          = {10.1007/s00521-025-11533-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25077-25094},
  shortjournal = {Neural Comput. Appl.},
  title        = {MDNER: Integrating MRC with diffusion models for enhanced named entity recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph isomorphism attention network combined with pre-trained language models: A novel approach for crystal material property prediction. <em>NCA</em>, <em>37</em>(30), 25061-25076. (<a href='https://doi.org/10.1007/s00521-025-11532-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting crystal material properties is a central task in AI-for-science, yet existing graph neural network methods face three limitations: (1) conventional neighbor-based graph construction causes node features to be dominated by neighbors, losing atomic uniqueness, (2) such atomic graphs only capture short-range dependencies via immediate neighbors while neglecting long-range interactions through multi-hop connections, and (3) insufficient utilization of chemical formulae, often relying on simplistic one-hot encoding without material-specific knowledge. To address these, we propose the graph isomorphism attention network (GIAT), which balances self-node and neighbor information to preserve atomic characteristics while capturing short-range dependencies. Additionally, we integrate a GraphTransformer to model long-range atomic interactions, forming a complementary framework with GIAT. Furthermore, we employ matBERT, a material science-specific language model, to encode chemical formulae, leveraging domain knowledge from both individual materials and their analogs. Experiments show that our model achieves state-of-the-art performance by synergistically combining short-range (GIAT), long-range (GraphTransformer), and chemical formula embeddings (matBERT).},
  archive      = {J_NCA},
  author       = {Kang, Jiahao and Yang, Liang and Zeng, Jingjie and Sun, Zhi and Lin, Hongfei and Li, Junpeng},
  doi          = {10.1007/s00521-025-11532-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25061-25076},
  shortjournal = {Neural Comput. Appl.},
  title        = {Graph isomorphism attention network combined with pre-trained language models: A novel approach for crystal material property prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swarm-based metaheuristic and reptile search algorithm for downstream operation-dependent fragmentation size prediction. <em>NCA</em>, <em>37</em>(30), 25033-25059. (<a href='https://doi.org/10.1007/s00521-025-11321-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement in blast fragmentation material size aids downstream operation efficiency by supplying a high percentage of 80% passing size of rocks (D80) material. In this study, blast fragmentation percentage prediction models were developed using five soft computing approaches, i.e., multilayer perceptron (MLP), multivariate adaptive regression spline (MARS), support vector regression SVR), decision tree (DT), and random forest (RF), combined with reptile search algorithm. Blast characteristics were obtained from 407 production blasts at the Anguran lead and zinc mine in Mahenshan, Iran. The particle size distribution resulting from blasting was determined using a digital image processing approach in the current paper. The novelty of this research lies in the integration of the reptile search algorithm (RSA) with machine-learning models for enhanced rock fragmentation prediction. In this study, the RSA was used to optimize RF model and accurate prediction of rock fragmentation due to mine blasting. The proposed models’ prediction accuracy was compared using nine performance indices such as root mean square error (RMSE), correlation coefficient (R), coefficient of determination (R2), mean absolute percentage error (MAPE), weighted mean absolute percentage error (WMAPE), Variance Accounted For (VAF), Nash–Sutcliffe efficiency (NS), Willmott’s index of agreement (WI) and bias index (Bias). Based on the obtained results, the RF-RSA model outperformed other developed models with the lowest error and highest accuracy. Hence, the results indicated that the RF-RSA model outperforms other models in predicting blast fragmentation, achieving the lowest R2 of 0.9734 and RMSE of 0.8386 in the training phase, and 0.9715 and 0.7464 in the testing phase. Furthermore, the model attained the lowest MAPE values of 2.3067 (training) and 2.0959 (testing), demonstrating its robustness and predictive accuracy. These findings highlight the effectiveness of the proposed AI-based approach in optimizing blast fragmentation prediction, contributing to improved operational efficiency in mining.},
  archive      = {J_NCA},
  author       = {Chen, Lihua and Taiwo, Blessing Olamide and Hosseini, Shahab and Kahraman, Esma and Fissha, Yewuhalashet and Sazid, Mohammed and Famobuwa, Oluwaseun Victor and Faluyi, Joshua Oluwaseyi and Akinlabi, Adams Abiodun and Ikeda, Hajime and Kawamura, Youhei},
  doi          = {10.1007/s00521-025-11321-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {25033-25059},
  shortjournal = {Neural Comput. Appl.},
  title        = {Swarm-based metaheuristic and reptile search algorithm for downstream operation-dependent fragmentation size prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing forensic dentistry: A comprehensive review of machine learning and deep learning applications in dental image analysis. <em>NCA</em>, <em>37</em>(30), 24997-25032. (<a href='https://doi.org/10.1007/s00521-025-11612-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of forensic dentistry has witnessed a marked rise in the utilization of artificial intelligence (AI), particularly in the domains of age estimation, gender classification, and human identification through dental image analysis. Although there are more publications in this field now than there used to be, there has not been a full review of how good the methods are and how things have changed over time. The aim of this systematic review is to assess the state of the literature on AI-based forensic dental image analysis using two complementary methods: (1) Publication dynamics and keyword trends are examined using bibliometric analysis, and (2) study goals, AI techniques, and experimental protocols are evaluated using systematic content review. A well-planned search was done in the Web of Science for the years between 2020 and 2024. The review followed PRISMA 2020 guidelines. Sixty-four articles met the criteria and were analyzed both quantitatively and qualitatively. The highest publication count was observed in 2022, with a stabilization trend noted in 2023 and 2024. Most of the studies focused on age estimation, with gender classification and identification coming a close second. CNNs were the dominant model architecture. While the field is maturing, there are still key challenges, such as the heterogeneity of datasets, the lack of external validation, and the limited attention given to forensic admissibility. The addressing of these issues will be vital for the translation of AI innovations into robust forensic tools.},
  archive      = {J_NCA},
  author       = {Karcioglu, Abdullah Ammar and Yilmaz, Rabia Meryem and Yaganoglu, Mete and Almohammad, Mahmoud and Laloglu, Abubekir},
  doi          = {10.1007/s00521-025-11612-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24997-25032},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing forensic dentistry: A comprehensive review of machine learning and deep learning applications in dental image analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital footprints and personality prediction: Integrating methodological innovations and ethical considerations in social media analysis. <em>NCA</em>, <em>37</em>(30), 24953-24996. (<a href='https://doi.org/10.1007/s00521-025-11607-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the emerging area of personality prediction using social media, emphasizing the simultaneous emphasis on improving research methods and addressing ethical concerns. The widespread use of digital technology, especially social media, presents an interesting opportunity to automatically detect personality traits from digital footprints. The study conducts an extensive analysis of existing literature to extract important findings regarding contemporary methods for predicting personality through automation. It thoroughly reviews different personality models, computational datasets, and sources of social media data. The primary contribution of this research is its thorough examination of the complex correlation between online social behaviors and personality traits, highlighting the potential of using social media behaviors as dependable indicators of personality. This investigation demonstrates the power of digital footprints to provide profound insights into human psychology, leading to substantial impacts on the disciplines of psychology, marketing, and social media design. Furthermore, the study thoroughly examines the ethical issues associated with forecasting personality traits, highlighting the imperative of employing ethical research methodologies, transparency, and preserving privacy when utilizing social media data. The main objective of this study is to integrate empirical evidence, theoretical frameworks, and ethical considerations to establish the framework for future research that is both scientifically rigorous and ethically responsible. Our findings provide illumination on the challenges and opportunities in personality prediction research.},
  archive      = {J_NCA},
  author       = {Maharani, Warih and Gani, Prati Hutari},
  doi          = {10.1007/s00521-025-11607-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24953-24996},
  shortjournal = {Neural Comput. Appl.},
  title        = {Digital footprints and personality prediction: Integrating methodological innovations and ethical considerations in social media analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pre-communication mechanism for evolutionary multitasking optimization. <em>NCA</em>, <em>37</em>(30), 24919-24951. (<a href='https://doi.org/10.1007/s00521-025-11534-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multitasking optimization (EMTO) aims to make use of evolutionary algorithms to solve multiple optimization tasks simultaneously through exploiting the relevant information between different tasks. However, the existing EMTO algorithms (EMTOAs) usually begin the optimization process from scratch without considering the prior information between different tasks, which would restrict the solving performance. This paper proposes a pre-communication mechanism (PCM) for EMTO, which takes the distribution information of the initial population corresponding to each task as the prior information, and uses the correlation of the prior information to provide refined solutions for each task in the each generation of the early evolution. Firstly, after generating initial individual solutions, PCM constructs a Gaussian distribution model on the individual solutions of each task as the prior information. Next, in the each generation of the early evolution, PCM takes each task as the target task in turn and learns the similarity information between the target task and other tasks by constructing a Gaussian mixture model. Mixture coefficients represent the learned similarity information between the target task and other tasks. Finally, the individual solutions sampled from the obtained Gaussian mixture model of each task compete with original individual solutions of each generation of early evolution to obtain refined individual solutions. The experimental results show that PCM can help the existing EMTOAs to solve multitasking optimization problems more effectively and efficiently.},
  archive      = {J_NCA},
  author       = {Yang, Cuicui and Li, Xiang and Ji, Junzhong and Zhang, Xiaoyu},
  doi          = {10.1007/s00521-025-11534-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24919-24951},
  shortjournal = {Neural Comput. Appl.},
  title        = {A pre-communication mechanism for evolutionary multitasking optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can large language models beat wall street? evaluating GPT-4’s impact on financial decision-making with MarketSenseAI. <em>NCA</em>, <em>37</em>(30), 24893-24918. (<a href='https://doi.org/10.1007/s00521-024-10613-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4’s advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability, and acceptance. Through empirical testing on the competitive S&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10–30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies.},
  archive      = {J_NCA},
  author       = {Fatouros, George and Metaxas, Kostas and Soldatos, John and Kyriazis, Dimosthenis},
  doi          = {10.1007/s00521-024-10613-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24893-24918},
  shortjournal = {Neural Comput. Appl.},
  title        = {Can large language models beat wall street? evaluating GPT-4’s impact on financial decision-making with MarketSenseAI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chinese fine-grained financial sentiment analysis with large language models. <em>NCA</em>, <em>37</em>(30), 24883-24892. (<a href='https://doi.org/10.1007/s00521-024-10603-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the need for more high-quality and large-scale annotated corpora designed explicitly for financial text sentiment analysis, which, in turn, limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which should be the focus of future research.},
  archive      = {J_NCA},
  author       = {Lan, Yinyu and Wu, Yanru and Xu, Wang and Feng, Weiqiang and Zhang, Youhao},
  doi          = {10.1007/s00521-024-10603-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24883-24892},
  shortjournal = {Neural Comput. Appl.},
  title        = {Chinese fine-grained financial sentiment analysis with large language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model XAI approach for illicit activity investigation in bitcoin. <em>NCA</em>, <em>37</em>(30), 24869-24881. (<a href='https://doi.org/10.1007/s00521-024-10510-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) present an opportunity for interpreting and investigating financial cybercrime activity. Analysts are tasked with learning and understanding the ever-shifting domain of financial crime, coupled with the latest tools being produced to assist them in their investigative tasks. We present an LLM XAI approach for identifying and explaining illicit activity in Bitcoin. We show that LLMs can produce intuitive and highly useful contextual narratives when prompted with Bitcoin transaction data. Extracting embeddings of the narratives allow for the calculation of similarity metrics capable of identifying other illicit transactions. We develop a pipeline of collating similar transactions, creating contextual narratives explaining their similarity, and produce a summary report all for the goal of aiding the investigative analyst.},
  archive      = {J_NCA},
  author       = {Nicholls, Jack and Kuppa, Aditya and Le-Khac, Nhien-An},
  doi          = {10.1007/s00521-024-10510-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24869-24881},
  shortjournal = {Neural Comput. Appl.},
  title        = {Large language model XAI approach for illicit activity investigation in bitcoin},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models in finance (FinLLMs). <em>NCA</em>, <em>37</em>(30), 24853-24867. (<a href='https://doi.org/10.1007/s00521-024-10495-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have demonstrated remarkable capabilities and have attracted significant attention across diverse domains, including financial services. Despite the extensive research into general-domain LLMs and their immense potential in finance, financial LLMs (FinLLMs) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, downstream tasks associated with datasets, evaluations, and opportunities and challenges. Firstly, we present a chronological overview of general-domain language models (LMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across eight financial LMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets and provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency. To support AI research in finance, we compile a collection of accessible datasets and benchmarks on GitHub. ( https://github.com/adlnlp/FinLLMs )},
  archive      = {J_NCA},
  author       = {Lee, Jean and Stevens, Nicholas and Han, Soyeon Caren},
  doi          = {10.1007/s00521-024-10495-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24853-24867},
  shortjournal = {Neural Comput. Appl.},
  title        = {Large language models in finance (FinLLMs)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient all-pairs approach for multi-objective dynamic shortest path problems. <em>NCA</em>, <em>37</em>(30), 24823-24851. (<a href='https://doi.org/10.1007/s00521-025-11437-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Shortest Path problem is fundamental for determining optimal routes in various applications. The Dynamic Shortest Path problem extends this concept to evolving graph structures. However, existing algorithms often fail to address decision-making complexities involving multiple objectives. In our previous work, we introduced the Multi-Objective Dynamic Shortest Path problem to address this gap. This paper presents the All-pairs Multi-objective Dynamic Shortest Path algorithm, offering a novel approach that combines a labeling-correcting method with the Optimistic Linear Support algorithm. This hybrid methodology enhances efficiency by minimizing redundant calculations during graph updates. Extensive testing demonstrates that our algorithm is over 3.22 times faster than baseline algorithms in producing Pareto solutions. This work advances techniques for multi-objective dynamic shortest paths and tackles challenges in evolving graph structures, paving the way for future research in this dynamic field.},
  archive      = {J_NCA},
  author       = {da Silva, Juarez Machado and Ramos, Gabriel de Oliveira and Barbosa, Jorge Luis Victória},
  doi          = {10.1007/s00521-025-11437-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24823-24851},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient all-pairs approach for multi-objective dynamic shortest path problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling uncertainty with parametric surrogate-assisted optimization for dynamic multi-objective problems. <em>NCA</em>, <em>37</em>(30), 24793-24822. (<a href='https://doi.org/10.1007/s00521-025-11359-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of dynamic multi-objective optimization problems, a shift of Pareto fronts is a consequence of changing environmental parameters. Current state-of-the-art methods define this dynamic nature as unknown time-dependent variables influencing the objective space of the problem. However, when dealing with real-world processes, these time-dependent variables are not always unknown. Processes are characterized by an intricate relationship between a series of decision variables, observed parameters, but also unknown parameters. Observed parameters can be seen as causal responses to underlying unknown environmental variables. This implies that they are at least partially determined by the state of other variables and reveal information about the state of the process that is not known when solely considering the decision variables. This paper discusses the types of uncertainty present in dynamic processes and how these uncertainties can be quantified and handled in surrogate modeling. A novel approach, PSAMOO, is proposed to integrate environmental parameters into parametric surrogate models to achieve more efficient Pareto front tracking in dynamic optimization. An extensive evaluation on dynamic problems shows how epistemic and aleatoric uncertainty can effectively be quantified and accounted for resulting in an improved dynamic Pareto front tracking and approximation.},
  archive      = {J_NCA},
  author       = {Temmerman, Arne De and Ryck, Matthias De and Verbeke, Mathias},
  doi          = {10.1007/s00521-025-11359-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24793-24822},
  shortjournal = {Neural Comput. Appl.},
  title        = {Handling uncertainty with parametric surrogate-assisted optimization for dynamic multi-objective problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective reinforcement learning framework for beneficent artificial intelligence. <em>NCA</em>, <em>37</em>(30), 24773-24791. (<a href='https://doi.org/10.1007/s00521-025-11311-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) systems are more widely deployed and utilized, they have greater potential to inflict harm or provide benefits to individuals. Designers must consider these concepts when building AI systems, but it is difficult to capture these notions in a mathematical framework. An ethical framework put forth by London and Heidari (Minds Mach, 2024. https://doi.org/10.1007/s11023-024-09696-8 ) provides structure and formal definitions of benefits and harms in relation to an individual’s life plans. This leaves an open question of how these concepts impact the decision-making of an AI system. We leverage their work to provide a direct translation of these theoretical ethical concepts to a standard multi-objective reinforcement learning (MORL) decision model. Using this model, we show that multi-objective rewards are necessary to capture benefits and harms in accordance with their definitions. We demonstrate how to capture these concepts in a MORL system and provide scenarios to highlight the utility of this work.},
  archive      = {J_NCA},
  author       = {Nickelson, Anna and Perkins, Russell and London, Alex John and Robinette, Paul and Tumer, Kagan},
  doi          = {10.1007/s00521-025-11311-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24773-24791},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-objective reinforcement learning framework for beneficent artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utility-inspired generalizations of TOPSIS. <em>NCA</em>, <em>37</em>(30), 24743-24772. (<a href='https://doi.org/10.1007/s00521-025-11238-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TOPSIS, a popular method for ranking alternatives, is based on aggregated distances to ideal and anti-ideal points. As such, it was considered to be essentially different from widely popular and acknowledged ’utility-based methods’, which build rankings from weight-averaged utility values. Nonetheless, TOPSIS has recently been shown to be a natural generalization of these ’utility-based methods’ on the grounds that the distances it uses can be decomposed into so-called weight-scaled means (WM) and weight-scaled standard deviations (WSD) of utilities. However, the influence that these two components exert on the final ranking cannot be in any way implemented in the standard TOPSIS. This is why, building on our previous results, in this paper we put forward modifications that make TOPSIS aggregations responsive to WM and WSD, achieving some amount of well interpretable control over how the rankings are influenced by WM and WSD. The modifications constitute a natural generalization of the standard TOPSIS method because, thanks to them, the generalized TOPSIS may turn into the original TOPSIS or, otherwise, following the decision-maker’s preferences, may trade off WM for WSD or WSD for WM. In the latter case, TOPSIS gradually reduces to a regular ’utility-based method’. All in all, we believe that the proposed generalizations constitute an interesting practical tool for influencing the ranking by controlled application of a new form of decision-maker’s preferences.},
  archive      = {J_NCA},
  author       = {Szczęch, Izabela and Susmaga, Robert},
  doi          = {10.1007/s00521-025-11238-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24743-24772},
  shortjournal = {Neural Comput. Appl.},
  title        = {Utility-inspired generalizations of TOPSIS},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multi-objective reinforcement learning for utility-based infrastructural maintenance optimization. <em>NCA</em>, <em>37</em>(30), 24719-24742. (<a href='https://doi.org/10.1007/s00521-024-10954-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce multi-objective deep centralized multi-agent actor-critic (MO-DCMAC), a multi-objective reinforcement learning method for infrastructural maintenance optimization, an area traditionally dominated by single-objective reinforcement learning (RL) approaches. Previous single-objective RL methods combine multiple objectives, such as probability of collapse and cost, into a singular reward signal through reward-shaping. In contrast, MO-DCMAC can optimize a policy for multiple objectives directly, even when the utility function is nonlinear. We evaluated MO-DCMAC using two utility functions, which use probability of collapse and cost as input. The first utility function is the threshold utility, in which MO-DCMAC should minimize cost so that the probability of collapse is never above the threshold. The second is based on the failure mode, effects, and criticality analysis methodology used by asset managers to assess maintenance plans. We evaluated MO-DCMAC, with both utility functions, in multiple maintenance environments, including ones based on a case study of the historical quay walls of Amsterdam. The performance of MO-DCMAC was compared against multiple rule-based policies based on heuristics currently used for constructing maintenance plans. Our results demonstrate that MO-DCMAC outperforms traditional rule-based policies across various environments and utility functions.},
  archive      = {J_NCA},
  author       = {van Remmerden, Jesse and Kenter, Maurice and Roijers, Diederik M. and Andriotis, Charalampos and Zhang, Yingqian and Bukhsh, Zaharah},
  doi          = {10.1007/s00521-024-10954-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24719-24742},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep multi-objective reinforcement learning for utility-based infrastructural maintenance optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-weight ranking for multi-criteria decision making. <em>NCA</em>, <em>37</em>(30), 24705-24717. (<a href='https://doi.org/10.1007/s00521-024-10626-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basic problem in multi-criteria decision-making (MCDM) is to find a ranking for alternatives which are not directly comparable with each other. A number of different methods exists. In this paper, a new ranking method is proposed by turning a statistical function into an MCDM ranking function. In particular, cone distribution functions from multivariate statistics are used as ranking functions and their features are investigated. Our findings demonstrate that this procedure can be considered as an upgrade of the weighted sum ranking insofar as it absorbs a whole collection of weighted sums at once instead of fixing a particular one in advance. The new ranking–in contrast to a pure weighted sum ranking–is also able to detect “non-convex” parts of the Pareto frontier. The rank reversal phenomenon is studied, and it is explained why it might even be useful for analyzing the ranking procedure. The ranking is extended to sets providing unary indicators for set preferences which establishes the link between set optimization methods and set-based multi-objective optimization. This also provides a new tool for evaluating the outcomes of evolutionary algorithms for multi-criteria optimization. The proposed method has implications for preferences learning and categorization of multi-dimensional data points, both with respect to an underlying non-complete order relation.},
  archive      = {J_NCA},
  author       = {Hamel, Andreas H. and Kostner, Daniel},
  doi          = {10.1007/s00521-024-10626-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24705-24717},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-weight ranking for multi-criteria decision making},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising multi-value alignment: A multi-objective evolutionary strategy for normative multi-agent systems. <em>NCA</em>, <em>37</em>(30), 24685-24703. (<a href='https://doi.org/10.1007/s00521-024-10625-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Value alignment in normative multi-agent systems (NorMAS) is used to promote a certain value and to ensure consistent behaviour of agents in autonomous intelligent systems with human values. Although various techniques were proposed in the literature for addressing the value alignment challenges in NorMAS, aligning multiple values remains an area requiring further investigation, especially in heterogeneous systems where agents may have different, potentially conflicting, coherent or unrelated norms and values. In such systems, it is crucial to simultaneously compromise and optimise values while synthesising an optimally aligned set of norms. Moreover, to deal with the conflicting or unrelated values and norms, we need to consider the norms and values as independent distinct sets. This research proposes a novel framework, Norms Optimisation and Values Alignment (NOVA), which enables multi-value alignment in heterogeneous NorMAS using parametric norms, multi-objective evolutionary algorithms (MOEAs) and decentralised reasoning. NOVA models the values and norms as independent distinct sets, then formalises the problem as a multi-objective optimisation problem (MOP), and optimises these two sets simultaneously while aligning them. To understand various aspects of this complex problem, several evolutionary algorithms were used to find a set of optimised norm parameters considering two tax scenarios with two and five values. The results show the impact of the selected evolutionary algorithm on the solution, and the importance of understanding the relation between values and norms when prioritising them using different reasoning strategies.},
  archive      = {J_NCA},
  author       = {Riad, Maha and de Carvalho, Vinicius Renan and Golpayegani, Fatemeh},
  doi          = {10.1007/s00521-024-10625-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24685-24703},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimising multi-value alignment: A multi-objective evolutionary strategy for normative multi-agent systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization in multi-objective machine learning. <em>NCA</em>, <em>37</em>(30), 24669-24683. (<a href='https://doi.org/10.1007/s00521-024-10616-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern machine learning tasks often require considering not just one but multiple objectives. For example, besides the prediction quality, this could be the efficiency, robustness or fairness of the learned models, or any of their combinations. Multi-objective learning offers a natural framework for handling such problems without having to commit to early trade-offs. Surprisingly, statistical learning theory so far offers almost no insight into the generalization properties of multi-objective learning. In this work, we make first steps to fill this gap: We establish foundational generalization bounds for the multi-objective setting as well as generalization and excess bounds for learning with scalarizations. We also provide the first theoretical analysis of the relation between the Pareto-optimal sets of the true objectives and the Pareto-optimal sets of their empirical approximations from training data. In particular, we show a surprising asymmetry: All Pareto-optimal solutions can be approximated by empirically Pareto-optimal ones, but not vice versa.},
  archive      = {J_NCA},
  author       = {Súkeník, Peter and Lampert, Christoph},
  doi          = {10.1007/s00521-024-10616-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24669-24683},
  shortjournal = {Neural Comput. Appl.},
  title        = {Generalization in multi-objective machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fade: Fairness-aware deep ensemble for quantifying uncertainty. <em>NCA</em>, <em>37</em>(30), 24655-24667. (<a href='https://doi.org/10.1007/s00521-024-10523-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper brings together two important aspects of trustworthy ML that go beyond mere accuracy that is calibration and fairness. A model is well calibrated if predictions that have a confidence of 0.6 are true 60% of the time. For fairness, we require calibration-by-groups and no underestimation, i.e., that predictions are consistent with respect to the actual distribution, across sensitive feature values. The deep ensemble method provides uncertainty estimates, as a form of transparency, by retraining the same neural network architecture with different weight initializations to produce a distribution of predictions. This method provides a compelling approach to approximating Bayesian predictive distributions, resulting in well-calibrated predictions. Through a series of experiments, we demonstrate that deep ensemble models are often well calibrated in terms of posterior probabilities but less so when a sensitive attribute is involved, resulting in biased predictions. This should not come as a surprise because the distributions of weights in the deep ensemble are usually optimized to maximize generalization accuracy without explicit consideration of fairness. To address this issue, we propose Fairness-Aware Deep Ensemble (FADE), a multi-objective optimization strategy to optimize the deep ensemble model on accuracy and fairness. We empirically evaluate our framework on two synthetic and twelve real-world datasets. We find that FADE can obtain fairer models while still attaining adequate overall generalization accuracy and calibrated probability estimates.},
  archive      = {J_NCA},
  author       = {Blanzeisky, William and Cunningham, Pádraig},
  doi          = {10.1007/s00521-024-10523-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24655-24667},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fade: Fairness-aware deep ensemble for quantifying uncertainty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear scalarization in stochastic multi-objective MDPs. <em>NCA</em>, <em>37</em>(30), 24641-24653. (<a href='https://doi.org/10.1007/s00521-024-10504-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many sequential decision problems of interest are best described in relation to multiple, possibly conflicting objectives. In recent years, there has been a growing interest in the application of reinforcement learning techniques to solve this type of problem. However, most existing multi-objective reinforcement learning (MORL) algorithms only apply in cases where the utility over the different criteria is linear. Unfortunately, in many real-world use cases, users’ preferences cannot be represented accurately by linear scalarization. Nonlinear scalarizations provide the required expressivity, but pose a number of issues when tackled by reinforcement learning, with so far little study and no definite solution. In particular, current methods struggle to deal with the most general MORL setting, where the environment is stochastic and the scalarization of the expected returns associated with each objective is nonlinear. In this paper, we present a model-free, value-based algorithm aimed at this type of problem: we prove that it converges to the optimal deterministic policy, confirm this result with experimental evidence, and discuss its limitations.},
  archive      = {J_NCA},
  author       = {Vincent, Marc},
  doi          = {10.1007/s00521-024-10504-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24641-24653},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nonlinear scalarization in stochastic multi-objective MDPs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based adaptive control and analysis for nonlinear systems with state-dependent constraint. <em>NCA</em>, <em>37</em>(30), 24625-24640. (<a href='https://doi.org/10.1007/s00521-025-11052-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the adaptive control issue for nonlinear systems with state-dependent constraint is investigated. The state-dependent constraint is not only related to time but also related to the historical state information of the system. It is a new type of complex constraint. Moreover, the input quantization and unknown system dynamics are also considered here, which are solved by using radial basis function neural networks. Then, an adaptive controller is proposed by using the backstepping method and the barrier Lyapunov function approach such that all signals in the resulted system are bounded, all system states satisfy their corresponding state-dependent constraint condition and the system output tracks the desired tracking signal. Finally, a simulation example is employed to further show the effectiveness of the proposed approach.},
  archive      = {J_NCA},
  author       = {Li, Guangshi},
  doi          = {10.1007/s00521-025-11052-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24625-24640},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning-based adaptive control and analysis for nonlinear systems with state-dependent constraint},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based adaptive fuzzy control for electro-hydraulic servo system. <em>NCA</em>, <em>37</em>(30), 24607-24624. (<a href='https://doi.org/10.1007/s00521-024-10741-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel adaptive fuzzy controller based on deep reinforcement learning (DRL) is introduced for electro-hydraulic servo systems. The controller combines the strengths of fuzzy proportional–integral (PI) control and deep Q-learning network (DQLN) to achieve real-time adaptation and improve the control performance. The purpose of this paper is to address the challenges of controlling electro-hydraulic servo systems by developing an adaptive controller that can dynamically adjust its control parameters based on the system’s state. The traditional fuzzy PI controller is enhanced with DRL techniques to enable automatic adaptation and compensation for changing online conditions. The proposed adaptive controller utilizes a DQLN to dynamically adjust the scaling factors of the input/output membership functions. By using the DQLN algorithm, the controller learns from a variety of system data to determine the optimal control parameters. The update equation of the weights for the Q-network is derived using the Lyapunov stability (LS) theorem, which overcomes the limitations of gradient descent (GD) methods such as instability and local minima trapping. To evaluate the effectiveness of the proposed controller, it is practically implemented to regulate an electro-hydraulic servo system. The controller’s performance is compared against other existing controllers, and its enhancements are demonstrated through experimental evaluation.},
  archive      = {J_NCA},
  author       = {Khater, A. Aziz and Fekry, Mohamed and El-Bardini, Mohammad and El-Nagar, Ahmad M.},
  doi          = {10.1007/s00521-024-10741-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24607-24624},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep reinforcement learning-based adaptive fuzzy control for electro-hydraulic servo system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced intelligent water drops with genetic algorithm for multi-objective mixed time window vehicle routing. <em>NCA</em>, <em>37</em>(30), 24591-24605. (<a href='https://doi.org/10.1007/s00521-024-10702-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of addressing the multi-objective vehicle routing problem, a hybrid time window multi-objective vehicle model was established using integer programming and the intelligent water drop algorithm. To overcome the limitation of the intelligent water drop algorithm potentially converging to local optimal solutions, enhancements were proposed through genetic algorithms, particularly by introducing genetic crossover and single-point recombination operators. Subsequently, the intelligent water drop algorithm was refined, and its effectiveness was evaluated through a real-world case study. Comparative analyses were conducted among three algorithms: IWD, GA, and SA. The results demonstrate that the improved algorithm effectively alleviates the common issue of traditional algorithms converging to local optimal solutions. Therefore, an enhanced solution is provided for the discrete hybrid time window problem, achieving superior optimization outcomes.},
  archive      = {J_NCA},
  author       = {Guo, Zhibao and Karimi, Hamid Reza and Jiang, Baoping and Wu, Zhengtian and Cheng, Yukun},
  doi          = {10.1007/s00521-024-10702-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24591-24605},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced intelligent water drops with genetic algorithm for multi-objective mixed time window vehicle routing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe adaptive control for multi-agent systems under uncertain dynamic environments: A robust barrier-certified learning approach. <em>NCA</em>, <em>37</em>(30), 24575-24590. (<a href='https://doi.org/10.1007/s00521-024-10639-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a decentralized safe learning-based adaptive control is developed for multi-agent systems (MAS) operating in uncertain environments. Ensuring the safety of MAS in uncertain environments becomes a challenging task, particularly when the dynamics of the individual agent are unknown and their accurate state information is unavailable to the local agent. Due to the fact that the safe set of the local agent depends on the other agents’ states, the uncertainty of these external systems leads to an uncertain safety set. As a consequence, the safe control design of the local agent system in a multi-agent setting under environmental uncertainties becomes intractable. To address this challenge and ensure the safety of the local agent, a neural network (NN)-based adaptive observer is designed to estimate the state of the unknown external agents in the multi-agent environment. Based on the state estimation of external agents, an adaptive interplay control barrier function (AI-CBF) is formulated. The AI-CBF is designed by considering both the local agent’s state and the estimated states of external agents. Notably, the limitation of forward invariance for the approximated safe set without guaranteeing the same for the actual safe set is acknowledged in AI-CBF design. The AI-CBF incorporates the bounds on state estimation errors of external agents to guarantee the strict safety requirements of the local agent. Based on the safety constraint enforced by the AI-CBF, a control framework is formulated using a quadratic programming (QP) method that integrates the safety and stability of the system. In addition, a stability analysis based on Lyapunov theory is performed to demonstrate the convergence of the neural network-based adaptive observer as well as the closed-loop stability of the overall system. Eventually, experimental validation and comparison study confirm the effectiveness of the developed approach that can ensure multi-agent system safety under challenging conditions in a decentralized manner.},
  archive      = {J_NCA},
  author       = {Dey, Shawon and Xu, Hao},
  doi          = {10.1007/s00521-024-10639-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24575-24590},
  shortjournal = {Neural Comput. Appl.},
  title        = {Safe adaptive control for multi-agent systems under uncertain dynamic environments: A robust barrier-certified learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online multi-agent deep reinforcement learning platform for distributed real-time dynamic control of power systems. <em>NCA</em>, <em>37</em>(30), 24561-24574. (<a href='https://doi.org/10.1007/s00521-024-10488-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-agent deep reinforcement learning (MADRL) has made significant strides in power system decision-making and control. However, there is a scarcity of high-fidelity, real-time platforms for testing various DRL control algorithms in detailed power systems. Motivated by EtherCAT communication and DRL features, this study presents a MADRL online testing platform for distributed real-time dynamic control of power systems. The platform utilizes the Opal-RT real-time simulator for real-time simulation of dynamic power system environments and uses multiple AI workstations for the implementation of MADRL control algorithms. The proposed platform facilitates real-time interaction among AI workstations and the Opal-RT real-time simulator by leveraging the EtherCAT communication protocol to transmit system information and control signals. It enables the online and real-time training of distributed MADRL algorithms for power system dynamic control. The effectiveness and advantages of the proposed platform have been validated through detailed case studies of testing distributed MADRL algorithms for classical power system control problems.},
  archive      = {J_NCA},
  author       = {Zhen, Fan and Zhenghong, Tu and Wenxin, Liu},
  doi          = {10.1007/s00521-024-10488-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24561-24574},
  shortjournal = {Neural Comput. Appl.},
  title        = {Online multi-agent deep reinforcement learning platform for distributed real-time dynamic control of power systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel bi-state deep reinforcement learning approach for SFC placements and deployments. <em>NCA</em>, <em>37</em>(30), 24541-24560. (<a href='https://doi.org/10.1007/s00521-024-10486-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SFC (service function chain) consists of virtual network function chains (VNFs) that generate provisioned network services on demand. To properly structure a network service, SFC traverses a series of VNFs in a predefined order. Thus, the VNF placement problem has become surrounded by other constraints that enshrine the order of VNFs according to their chain type. This restriction requires a method of extracting the best and shortest traffic from a chain of VNFs while considering duplication of the same kind of VNF. In this context, we propose to study and solve the problem of VNF placement and chaining in order to reduce the resources cost and minimize the end-to-end delay (chain of VNFs) while respecting the resources constraints (CPU, memory, and storage) in a multi-instance VNF environment taking into account the bandwidth (traffic congestion). To address these problems, we propose an ILP modeling and a novel model of classification named parallel bistate module deep reinforcement learning (PBDRL) algorithm. This algorithm is based on two modules: The first is MDP (Markov decision process) which is used to capture service chain state transition, and the second is LSTM (long short-term memory) which enables to detection of the long-term history of the service chain and its transitions. Our proposal aims to extract the characteristics of the VNF environment through MDP and LSTM simultaneously taking into account the dynamicity and history of the NFV environment. Simulation results show that our approach achieved $$47\%$$ more reward than VNF deep approach and $$52\%$$ than first-fit algorithm.},
  archive      = {J_NCA},
  author       = {Khemili, Wided and Saidi, Mohand Yazid and Hajlaoui, Jalel Eddine and Omri, Mohamed Nazih and Chen, Ken},
  doi          = {10.1007/s00521-024-10486-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24541-24560},
  shortjournal = {Neural Comput. Appl.},
  title        = {Parallel bi-state deep reinforcement learning approach for SFC placements and deployments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADP-based nonlinear optimal output regulation with nonlinear exosystem. <em>NCA</em>, <em>37</em>(30), 24531-24539. (<a href='https://doi.org/10.1007/s00521-023-09253-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the optimal output regulation problem for nonlinear systems with a nonlinear exosystem. Adaptive dynamic programming and internal model principle are integrated to deal with this problem. The control scheme process is designed in the following two steps. In the first step, a nonlinear internal model is constructed to obtain the feedforward controller, which allows for the conversion of the output regulation problem into a stabilization problem. Compared with feedforward design approach, the common assumption that the exosystem is linear and neutral stable has been eliminated successfully. In the second step, the control cost is taken into account and the optimal feedback controller is learned through adaptive dynamic programming. After that, we demonstrate that the closed-loop system is uniformly ultimately bounded by the Lyapunov stability theory. As a distinctive feature, the proposed control framework can tolerate the nonlinear exosystem. Finally, the effectiveness of the control scheme is shown by a simulation example.},
  archive      = {J_NCA},
  author       = {Jiang, Haoan and Jin, Peng and Ma, Qian and Zhou, Guopeng and Miao, Guoying},
  doi          = {10.1007/s00521-023-09253-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24531-24539},
  shortjournal = {Neural Comput. Appl.},
  title        = {ADP-based nonlinear optimal output regulation with nonlinear exosystem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered synchronization adaptive learning control of nonlinear multi-agent systems with resilience to communication link faults. <em>NCA</em>, <em>37</em>(30), 24517-24530. (<a href='https://doi.org/10.1007/s00521-023-09215-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to propose an event-triggered synchronization adaptive learning scheme for uncertain nonlinear multi-agent systems with communication link faults. In reality, the link faults are ubiquitous and responsible for modeling delays and disturbances in communications and also capturing time-varying communication weights between any two nodes in the network. To address this challenge, based on Lyapunov analysis, adaptive learning neural networks are employed to settle unknown link faults and system uncertainties. To reduce the consumption of communication resources, an event-triggered control strategy is introduced on the input side of the controller, whose triggering events are relative thresholds that can be spontaneously modified with the system states. In addition, a set of smooth functions are inserted in the Lyapunov function that assure the asymptotic stability of the system even with corrupted link faults. Meanwhile, it is rigorously verified that the synchronous tracking errors converge to a user-defined interval. Finally, the effectiveness of the proposed control protocol is demonstrated by a numerical simulation.},
  archive      = {J_NCA},
  author       = {Zheng, Zhiyang and Chen, Ci and Xie, Kan and Li, Zhenni and Xie, Shengli},
  doi          = {10.1007/s00521-023-09215-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24517-24530},
  shortjournal = {Neural Comput. Appl.},
  title        = {Event-triggered synchronization adaptive learning control of nonlinear multi-agent systems with resilience to communication link faults},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive road shoulder traffic control with reinforcement learning approach. <em>NCA</em>, <em>37</em>(30), 24499-24515. (<a href='https://doi.org/10.1007/s00521-023-09134-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce traffic congestion on the highway, variable speed limits, flow control, and traffic light are used in the current traffic control system. Other strategies for easing traffic congestion include controlling the number of vehicles entering the highway by setting up traffic lights on the ramp and extending the number of lanes by opening the shoulder. Although opening the road shoulder to digest the traffic congestion seems to be very efficient, the current system only opens the road shoulder at a fixed schedule. In this research, we proposed a Reinforcement Learning Approach for Adaptive Road Shoulder Traffic Control (ARSTC) to dynamically change the opening and closing time of the road shoulder. The proposed ARSTC technique is able to adjust to different traffic situations and make a suitable decision that is different from the traditional static scheduling approach for opening the road shoulder. The proposed technique is simulated in the Simulation of Urban Mobility. The results showed that ARSTC can reduce traffic congestion time by adaptively controlling the hard shoulders’ opening time and the traffic flow. Our proposed technique (ARSTC) is able to provide safer and more efficient driving conditions while using the hard shoulder to ease traffic congestion.},
  archive      = {J_NCA},
  author       = {Ho, Yao-Hua and Cheng, Tung-Chun},
  doi          = {10.1007/s00521-023-09134-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {30},
  pages        = {24499-24515},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive road shoulder traffic control with reinforcement learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized ensemble machine learning cancer classification system for clinical decision-making. <em>NCA</em>, <em>37</em>(29), 24483-24498. (<a href='https://doi.org/10.1007/s00521-025-11599-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical and healthcare informatics research is accelerating at an unprecedented rate due to the growth and accumulation of large volumes of biological and clinical data. There are new opportunities to use big data to uncover new insights and develop creative approaches to improve the quality of cancer treatment. The microarray gene expression profile is used to efficiently and accurately classify cancer cells for clinical decision-making. In this study, a cancer classification system using an optimized ensemble machine learning approach which is based on artificial bee colony (ABC) optimization and an ensemble of machine learning classifiers is proposed as a result of this effort to distinguish between acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML) utilizing microarray gene expression patterns. The relevant cancer features are optimally selected using the ABC technique to improve the performance of the proposed ensemble learning-based classification system. Moreover, the gene expression dataset is balanced using an upsampling technique and an equal number of ALL and AML records have been used for experimentation. The proposed methodology’s accuracy and other performance metrics are looked at, and the suggested model is contrasted to several base machine learning algorithms based on performance criteria to show how useful it is. Furthermore, to demonstrate the importance of the suggested strategy, receiver operating characteristics (ROC) analysis has been performed, and it is seen that the area under the ROC curve of the proposed approach is higher compared to the existing approaches.},
  archive      = {J_NCA},
  author       = {Amma, N. G. Bhuvaneswari and Amma, N. G. Nageswari},
  doi          = {10.1007/s00521-025-11599-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24483-24498},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized ensemble machine learning cancer classification system for clinical decision-making},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breast cancer classification by converging tumour region probability density and texture feature-based clustering. <em>NCA</em>, <em>37</em>(29), 24461-24481. (<a href='https://doi.org/10.1007/s00521-025-11596-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel technique for the segmentation and classification of regions in breast tumour images by integrating a convergence-based density model, coupled with texture feature-based clustering. The segmentation process starts with an active contour that estimates probability densities for foreground, background, and tumour regions. A key advantage of the proposed method is its independence from an annotated training set. Thus, it reduces sensitivity to dataset variability by using intensity-driven convergence. To address overlapping structures in mammographic images, an edge-path contour-splitting methodology is employed for accurate boundary separation. Finally, a probabilistic neural network (PNN) is used to classify the tumour regions based on texture features. Both qualitative and quantitative results are presented to demonstrate the effectiveness of the proposed method. The proposed method achieves an accuracy of 92%, with a sensitivity of 95.4% and specificity of 94.2%. Performance evaluation with ROC analysis confirms the robustness and diagnostic reliability of the proposed approach in identifying breast tumours.},
  archive      = {J_NCA},
  author       = {Kumari, Bersha and Nandal, Amita and Dhaka, Arvind and Alhudhaif, Adi and Polat, Kemal},
  doi          = {10.1007/s00521-025-11596-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24461-24481},
  shortjournal = {Neural Comput. Appl.},
  title        = {Breast cancer classification by converging tumour region probability density and texture feature-based clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HR-YOLOv8: An innovative model to detect mitosis and identify cancer regions in histopathological images. <em>NCA</em>, <em>37</em>(29), 24441-24460. (<a href='https://doi.org/10.1007/s00521-025-11594-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pathologists use histopathology images to identify breast cancer. Under these circumstances, the identification of mitosis in tissues becomes a potent prognostic marker for breast cancer. Mitosis serves as a vital marker for pinpointing areas of tumor aggressiveness and assessing the probability of disease recurrence. The HR-YOLOv8 model is presented in this paper as a highly accurate way to identify regions of breast cancer and detect mitosis. There are two phases to the study. Through the integration of the HRNet blocks into the YOLOv8 backbone, mitosis is identified in the first stage. The MST algorithm locates the breast cancer region in the second stage. The MST algorithm is employed to merge separate nodes, enabling the identification of cancer regions. HR-YOLOv8 is evaluated on MIDOG21, TUPAC16, and MiDeSeC that datasets are specifically focused on breast cancer, using metrics such as accuracy and F1-score for mitosis detection and AUC, sensitivity, and specificity for breast cancer regions. The results obtained from the study show that the proposed model can identify mitosis and recognize breast cancer regions with high precision.},
  archive      = {J_NCA},
  author       = {Nemati, Nooshin and Samet, Refik},
  doi          = {10.1007/s00521-025-11594-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24441-24460},
  shortjournal = {Neural Comput. Appl.},
  title        = {HR-YOLOv8: An innovative model to detect mitosis and identify cancer regions in histopathological images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for efficient topic modeling. <em>NCA</em>, <em>37</em>(29), 24421-24439. (<a href='https://doi.org/10.1007/s00521-025-11593-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of large language models (LLMs) in research is becoming increasingly prevalent, as they offer advanced capabilities in processing and generating human-like text. However, this advancement comes with a significant trade-off in terms of time and computational costs. In this paper, we demonstrate that analyzing large text datasets with the use of LLMs can be performed efficiently in terms of both time and energy. For this purpose, we utilize the Llama pre-trained model. In more detail, we study the topic modeling task where the goal is to discover and identify topics in large text corpora. The basis of our approach is a hierarchical divisive clustering technique that clusters the data based on their semantic similarity, after employing a Sentence-BERT encoder, pre-trained on a variety of data across different tasks. Then, using an LLM, we identify topics for representative samples from each cluster. Additionally, we introduce a new evaluation method that leverages the capabilities of LLMs to assess the alignment between discovered topics and ground truth labels, providing a robust validation metric. Our findings indicate that it is possible to effectively reduce the computational cost of the topic modeling process compared to the direct application of LLMs and BERTopic, while simultaneously enhancing inference time and overall efficiency, thereby surpassing the current state-of-the-art capabilities of BERTopic.},
  archive      = {J_NCA},
  author       = {Theocharopoulos, Panagiotis C. and Anagnostou, Panagiotis and Georgakopoulos, Spiros V. and Tasoulis, Sotiris K. and Plagianakos, Vassilis P.},
  doi          = {10.1007/s00521-025-11593-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24421-24439},
  shortjournal = {Neural Comput. Appl.},
  title        = {Large language models for efficient topic modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplex network-based representation of vision transformers for visual explainability. <em>NCA</em>, <em>37</em>(29), 24385-24420. (<a href='https://doi.org/10.1007/s00521-025-11591-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enormous growth of artificial intelligence (AI), and deep learning (DL) in particular, has led to the widespread use of these systems in a variety of contexts. One DL model capable of addressing complex computer vision tasks is the vision transformer (ViT). Despite its huge success, the reasoning behind the inferences it makes is often unclear, which poses significant challenges in critical scenarios. In this paper, we propose a new approach called MUltiplex Transformer EXplainer (MUTEX), which aims to explain the inferences made by ViTs. MUTEX combines multiplex network-based representations of attention matrices and mask perturbation approaches to provide insight into the inference process of ViTs. By mapping the attention layers of a ViT into a multiplex network, MUTEX is able to analyze the relationships between different parts of the input image and identify the image patches that most influence the inference process. We tested MUTEX on a subset of ImageNet and on BloodMNIST and compared its performance with that of existing visual explainability approaches. In addition, to assess the robustness and adaptability of MUTEX, we conducted a qualitative analysis, along with a hyperparameter and ablation study, which allowed us to further appreciate its potential in visual explainability of ViT.},
  archive      = {J_NCA},
  author       = {Marchetti, Michele and Traini, Davide and Ursino, Domenico and Virgili, Luca},
  doi          = {10.1007/s00521-025-11591-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24385-24420},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multiplex network-based representation of vision transformers for visual explainability},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced arrhythmia detection using spiking neural networks: An in-depth analysis of ECG data from the MIMIC-IV clinical database. <em>NCA</em>, <em>37</em>(29), 24365-24384. (<a href='https://doi.org/10.1007/s00521-025-11585-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of electrocardiogram (ECG) signals is essential for detecting arrhythmias such as bradycardia, ventricular tachycardia, and atrial fibrillation. This study utilizes MIMIC-IV-ECG dataset, containing over 800,000 recordings, to assess the effectiveness of spiking neural networks (SNNs) in arrhythmia finding. Various deep learning architectures including hybrid, leaky integrate-and-fire networks, spiking convolutional neural networks (SCNNs), convolutional spiking neural networks, S-RNN, and LSTM-SNN, are trained using key ECG features, with performance enhanced through data augmentation and feature engineering. Our findings identified the innovative SCNN demonstrate outstanding arrhythmia classification ability at a highly to notable 97% accuracy; while hybrid models like norse-hybrid and dense-spike show their own capabilities by incorporating traditional deep learning architectures with the concept of spiking neurons and offer additional performance improvements. These findings highlight the potential of neuromorphic computing for ECG analysis, with future work focusing on real-time processing and clinical scalability.},
  archive      = {J_NCA},
  author       = {Verma, Gunjan and Gocher, Honey and Verma, Sweety},
  doi          = {10.1007/s00521-025-11585-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24365-24384},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced arrhythmia detection using spiking neural networks: An in-depth analysis of ECG data from the MIMIC-IV clinical database},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid algorithm: Long short-term memory-genetic algorithm for optimizing uncertain revenue of wind farms in electricity markets. <em>NCA</em>, <em>37</em>(29), 24345-24364. (<a href='https://doi.org/10.1007/s00521-025-11582-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a robust hybrid optimization algorithm that combines artificial intelligence with genetic algorithms (GA) to maximize revenue for electricity generation plants, addressing the challenges posed by wind generation uncertainty in liberalized power markets. The novel method leverages the prediction capabilities of the long short-term memory algorithm, a deep learning methodology, to forecast superior genetic traits. These enhanced individuals are then incorporated into the population, accelerating the evolutionary process of the GA and improving its ability to achieve local optimality. The effectiveness of the proposed algorithm is demonstrated through experimental validation of the revenue optimization problem, aiming to increase wind energy utilization by reducing compensation risks related to electricity production fluctuations in the market. The performance of the algorithm in recommending wind power bidding capacity is evaluated using the IEEE 30-bus and 118-bus power system model. Comparative analysis shows that auctioned wind power consumption increased by 12% compared to the traditional GA and by more than 20% compared to the mixed integer linear programming (MILP) method, with a corresponding revenue increase of 7% compared to the MILP scenario. Furthermore, comparison with previous advanced GA research on the optimal power flow problem indicates not only a reduction in the number of generations but also significant savings in computation time; the effectiveness of the approach is confirmed by a more than 22% reduction in the NFFE index (the number of fitness function evaluations).},
  archive      = {J_NCA},
  author       = {Dinh, Ngoc Sang and Dinh, Le Song Binh and Truong, Viet Anh},
  doi          = {10.1007/s00521-025-11582-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24345-24364},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel hybrid algorithm: Long short-term memory-genetic algorithm for optimizing uncertain revenue of wind farms in electricity markets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting artificial rabbit optimization for solving classification problem: Benchmark dataset analysis. <em>NCA</em>, <em>37</em>(29), 24325-24344. (<a href='https://doi.org/10.1007/s00521-025-11572-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, bio-inspired metaheuristic algorithms have been widespread in various application areas due to their straightforward implementation and ability to handle and solve complex problems. Despite the widespread adoption of metaheuristic algorithms, there is a lack of adaptation of the artificial rabbit optimization (ARO) algorithm to solve classification tasks. Specifically, the ARO algorithm has primarily been used for hyperparameter tuning machine learning algorithms or to select the best feature subset within a wrapper feature selection without cooperating in the classification process. This paper has introduced a new direct adaptation of the ARO algorithm for classification tasks. This adaptation was constructed by directly using the ARO algorithm to find the optimal centroids that could solve the classification task. Therefore, the ARO algorithm was shifted directly to the classification task by identifying the optimal centroid for each class label and minimizing the number of misclassified instances in the training data. The proposed direct adaptation of the ARO algorithm for classification tasks was tested and evaluated using eleven benchmark datasets from various domains. We conducted in-depth investigations of the other seven bio-inspired optimization algorithms alongside the ARO algorithm. The accomplished results demonstrated the robustness and effectiveness of the ARO algorithm for solving the classification problem compared to other bio-inspired optimization algorithms. Consequently, by extending ARO to focus on optimal centroid identification, our approach has enhanced the performance of classification tasks, thereby improving the effectiveness of decision-making models.},
  archive      = {J_NCA},
  author       = {Almseidin, Mohammad},
  doi          = {10.1007/s00521-025-11572-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24325-24344},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adapting artificial rabbit optimization for solving classification problem: Benchmark dataset analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient task offloading based on modified elk herd optimizer for minimizing response times in fog-enabled IoT. <em>NCA</em>, <em>37</em>(29), 24303-24323. (<a href='https://doi.org/10.1007/s00521-025-11569-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fog computing has emerged as a prominent research because of the widespread adoption and continuous advancements in Internet of Things (IoT) technologies. Fog nodes (FNs) offer storage and computational capabilities to resource-constrained IoT devices, enabling them to support IoT applications with high computing needs. Moreover, closeness FNs to IoT devices ensure they meet the latency demands of IoT applications. However, increasing the need to offload the tasks, combined with limited IoT resources, requires to development an efficient task offloading. To address this challenge, a task offloading approach utilizing the modified elk herd optimizer (MEHO) is proposed to assign tasks to FNs. MEHO is designed as an optimization approach aimed at minimizing response time. Comprehensive simulations show that MEHO outperforms other methods under various numbers of FNs, service rate, and rate of arrival data. MEHO achieves a reduction in average response time for maximum tasks by 12%, 16%, 18%, 19%, 26%, and 41% compared to modified sparrow search algorithm, sparrow search algorithm, artificial bee colony optimization, ant colony optimization, particle swarm optimization, and round robin, respectively.},
  archive      = {J_NCA},
  author       = {Alfawaz, Oruba and Khedr, Ahmed M. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-025-11569-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24303-24323},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient task offloading based on modified elk herd optimizer for minimizing response times in fog-enabled IoT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft computing techniques for predicting thermal conductivity of bentonite–fly ash/-sand composite materials. <em>NCA</em>, <em>37</em>(29), 24281-24301. (<a href='https://doi.org/10.1007/s00521-025-11568-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bentonite–sand/fly ash-based thermal backfill materials are used as a heat transfer medium between the heat sources (e.g., underground power cable) and near-field. The characterization of materials in the laboratory, as a thermal backfill material, often requires extensive field as well as laboratory work, which is quite expensive and time-consuming. Therefore, this paper aims to develop a soft computing (SC) technique to predict the thermal conductivity due to its ability to handle complex, nonlinear and uncertain relationships in soil behavior. To achieve this goal, four SC algorithms, namely artificial neural network (ANN), ANN–PSO (particle swarm optimization), adaptive neural network-based fuzzy inference system (ANFIS) and extreme gradient boosting (XGB), were utilized based on the experimental database considering compaction state and physical properties of backfill as input variables. The performance of different SC techniques was evaluated based on scatter plots, statistical indices, Taylor’s diagrams and rank analysis. The results exhibited that XGB predicts more accurately than ANFIS, ANN and ANN–PSO. Finally, the influence and significance of the input parameters on XGB model performance are highlighted using Shapley additive explanation (SHAP) analysis. The findings demonstrated that the water content, dry density and particle size content had the most significant impact on the thermal conductivity of bentonite–sand/fly ash-based backfill material.},
  archive      = {J_NCA},
  author       = {Bharti, Vishakha and Sah, Pawan Kishor and Kumar, Shiv Shankar and Das, Bhabani Shankar},
  doi          = {10.1007/s00521-025-11568-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24281-24301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Soft computing techniques for predicting thermal conductivity of bentonite–fly ash/-sand composite materials},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing monocular depth estimation with an advanced encoder-decoder architecture. <em>NCA</em>, <em>37</em>(29), 24265-24280. (<a href='https://doi.org/10.1007/s00521-025-11566-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant technological advancements have been made in autonomous navigation, impacting various fields such as robotics, autonomous vehicles, and unmanned aerial vehicles. These systems typically use the distances of surrounding objects as input. Monocular depth estimation, which involves estimating depths from a single RGB image, plays a crucial role in this context. In this study, we proposed an encoder-decoder model for monocular depth estimation. Additionally, we introduced a weighted loss function designed to minimize depth image reconstruction errors and penalize distortions in the scene domain of the depth map. The proposed model was evaluated using the NYU Depth V2 dataset, and the results surpassed those of state-of-the-art models on the same dataset. Specifically, our model achieved a validation accuracy of 0.9823, an average relative error (rel) of 0.04713 and a root mean square error (RMSE) of 0.2372, representing significant reductions of 60% and 49%, respectively, compared to contemporary techniques, even with a small training dataset.},
  archive      = {J_NCA},
  author       = {El-Alfy, Yasser and Baroudi, Uthman and Luqman, Hamzah},
  doi          = {10.1007/s00521-025-11566-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24265-24280},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing monocular depth estimation with an advanced encoder-decoder architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViT-DtC: Vision transformer-based design-to-code framework for code generation from generated UI designs and hand-drawn sketches. <em>NCA</em>, <em>37</em>(29), 24243-24264. (<a href='https://doi.org/10.1007/s00521-025-11565-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code generation in software development from various types of user interface (UI) design images can significantly reduce the manual effort required, accelerate development timelines, and facilitate collaboration between designers and developers. Previous studies have restricted the utilized types of UI designs to either generated UI designs or hand-drawn sketches separately, lacking the automatic detection capability for the design type. Moreover, studies frequently necessitated complex datasets with bounding box annotations and sophisticated computer vision preprocessing steps, especially within the context of hand-drawn sketches. In this paper, we introduce the novel comprehensive Vision Transformer-based Design-to-Code (ViT-DtC) framework, which tackles image classification and code generation from both generated UI designs and hand-drawn sketches by harnessing the capabilities of Vision Transformers. The proposed ViT-DtC undergoes fine-tuning to classify UI designs into specific categories, including web, iOS, Android, and hand-drawn sketches. Subsequently, the design is directed to the appropriate code generator to generate domain-specific language (DSL) tokens for UI elements based on the identified design type. The experimental results exhibited an exceptional proficiency in accurately classifying all designs. Employing a greedy search strategy, the proposed ViT-DtC framework achieved an average accuracy of 97.28% in generating UI elements on the modified web dataset (without capturing their state or color information). In iOS and Android designs, an average accuracy of 82.4% and 81.1% was achieved, respectively. Remarkably, when extended to handle hand-drawn sketches, an average accuracy of 84.8% was maintained.},
  archive      = {J_NCA},
  author       = {Ahmed, Areeg and Azab, Shahira and Moussa, Sherin M. and Abdelhamid, Yasser},
  doi          = {10.1007/s00521-025-11565-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24243-24264},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViT-DtC: Vision transformer-based design-to-code framework for code generation from generated UI designs and hand-drawn sketches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EWOSCA: An enhanced walrus optimizer-based secure clustering approach for IoT-based WSNs under adversarial contexts. <em>NCA</em>, <em>37</em>(29), 24209-24242. (<a href='https://doi.org/10.1007/s00521-025-11564-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an integral constituent of the Internet of Things (IoT), wireless sensor networks (WSNs) are revolutionizing different aspects of everyday life through intelligent and cost-effective applications. The dual challenges of security and efficiency are the major concerns facing any WSN deployment. Clustering is widely recognized as one of the most effective strategies for enhancing the WSN lifespan. While cluster heads (CHs) serve a pivotal role by managing data aggregation and communication, if CHs are compromised, the integrity of the collected data is lost, posing a significant risk to the network’s reliability and effectiveness. This work proposes an enhanced walrus optimizer-based secure clustering approach (EWOSCA) for IoT-based WSNs under adversarial contexts. An enhanced walrus optimizer (EWO) is developed to address the key shortcomings of the original WO. It incorporates an adaptive inertia weight strategy to better balance exploration and exploitation, a transverse crossover mechanism to tackle premature stagnation by enhancing diversity and generating high-quality solutions, and a colony predation strategy (CPS) to tackle the limited adaptability by dynamically refining the search process using the best-performing solutions. The EWOSCA approach adapts EWO and prioritizes the selection of secure, reliable, and energy-efficient CHs. Furthermore, an Adaptive weighted average function, AwE(), is devised and utilized while designing the fitness function for adapting the algorithm in response to varying network conditions over time. Simulation results reveal that EWOSCA can effectively handle varying rates of malicious or compromised nodes and surpasses recent schemes in terms of effective clustering, energy efficiency, reliability, and overall WSN lifetime.},
  archive      = {J_NCA},
  author       = {Khedr, Ahmed M. and V., Pravija Raj P. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-025-11564-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24209-24242},
  shortjournal = {Neural Comput. Appl.},
  title        = {EWOSCA: An enhanced walrus optimizer-based secure clustering approach for IoT-based WSNs under adversarial contexts},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal MRI data augmentation and attention-based modeling for interpretable brain tumor classification. <em>NCA</em>, <em>37</em>(29), 24191-24207. (<a href='https://doi.org/10.1007/s00521-025-11561-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from MRI images is crucial for guiding treatment planning and improving clinical outcomes. However, current methods face limitations in generalization due to small datasets and lack of interpretability, both critical in medical applications. To address these challenges, we propose a multi-class brain tumor classification system, exploring three distinct models based on VGG19 architecture. These include an attention-module-based model, a deeper convolutional network, and a linear-boosted model. Each model is trained on a large, aggregated dataset with six types of image augmentations to improve generalization. Among the three models, the attention-based model achieves the highest classification performance by focusing on relevant tumor regions. Local interpretable model-agnostic explanations are used to visualize the decision-making process, enhancing model transparency. Our results demonstrate that the attention-based model outperforms baseline and state-of-the-art methods, making it a robust and interpretable solution for brain tumor diagnosis.},
  archive      = {J_NCA},
  author       = {Alanazi, Mubarak A.},
  doi          = {10.1007/s00521-025-11561-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24191-24207},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-modal MRI data augmentation and attention-based modeling for interpretable brain tumor classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing YOLOv9 for automated detection of stroke lesions in brain CT images. <em>NCA</em>, <em>37</em>(29), 24169-24189. (<a href='https://doi.org/10.1007/s00521-025-11560-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prompt detection and accurate localization of stroke-induced cerebral damage in Computed Tomography (CT) scans are essential for optimal treatment and patient prognosis. Traditional techniques significantly depend on the proficiency of radiologists, which can be labor-intensive and susceptible to inaccuracies. This study presents a novel methodology utilizing the YOLOv9 deep learning architecture, termed StrokeYOLO, for the automated detection of stroke lesions in brain CT images. Although YOLOv9 is primarily employed for general object detection, we have modified it to specifically focus on the nuanced characteristics of stroke lesions. Through the modification of anchor boxes, the refinement of feature extraction, and the fine-tuning of the model, we have improved its capacity to accurately identify smaller stroke regions. The proposed model was trained and assessed on a dataset of annotated brain CT scans, exhibiting outstanding efficacy in identifying ischemic and hemorrhagic stroke regions. The detection accuracy was further corroborated against expert annotations, attaining results that were comparable to or superior to traditional methods. Furthermore, we utilized explainable AI methodologies to enhance transparency in the model's decision-making process, thereby promoting trust and clinical implementation. This study emphasizes the capabilities of YOLOv9 as a real-time, automated instrument for facilitating stroke diagnosis while tackling the challenges and prospects of utilizing object detection models in medical imaging. StrokeYOLO attained an accuracy of 98.7%, precision of 99.92%, recall of 99.1%, and F1-measure of 99.51% on the evaluation dataset, surpassing current methodologies. The findings underscore the capability of YOLOv9 as a real-time, automated instrument for facilitating stroke diagnosis while tackling the challenges and prospects of utilizing object detection models in medical imaging.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Shaban, Warda M.},
  doi          = {10.1007/s00521-025-11560-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24169-24189},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing YOLOv9 for automated detection of stroke lesions in brain CT images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing heart health: An AI-driven analysis of dietary habits, unveiling impacts on human health and attitudes. <em>NCA</em>, <em>37</em>(29), 24149-24167. (<a href='https://doi.org/10.1007/s00521-025-11559-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel deep learning-based approach for analyzing the relationship between eating habits and heart health. The method leverages wearable technology, smartphone applications, and food diaries to gather comprehensive dietary data. This data is then processed by recurrent neural networks (RNNs) and convolutional neural networks (CNNs) to extract significant dietary patterns and features relevant to cardiovascular health. By integrating dietary information with other health-related data, a comprehensive model is constructed to analyze the intricate interactions between lifestyle, nutrition, and cardiovascular outcomes. This approach facilitates the generation of personalized dietary recommendations and enables a more precise and objective evaluation of eating habits. To validate the effectiveness of the proposed methodology, an LSTM model is implemented and achieves a precision of 0.982, indicating a high percentage of true positive predictions. Additionally, the model demonstrates an accuracy of 98.9%, highlighting its ability to classify nearly all instances accurately. These exceptional results suggest the suitability of the modified LSTM model for further investigation and potential real-world implementation.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and ZainEldin, Hanaa and Gamel, Samah A.},
  doi          = {10.1007/s00521-025-11559-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24149-24167},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing heart health: An AI-driven analysis of dietary habits, unveiling impacts on human health and attitudes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting design suitability of box-shaped sustainable timber structural members using machine learning and hyperparameter optimization. <em>NCA</em>, <em>37</em>(29), 24123-24148. (<a href='https://doi.org/10.1007/s00521-025-11556-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement of timber structures is essential for sustainable construction, focusing on enhancing ductility and energy dissipation. While machine learning offers a promising approach to accelerate and simplify the design process for such structures, its application in this field remains underexplored. This study develops digital models of box-shape timber members to assess their suitability for design (TS 647 Building Code for Timber Structures, Türk Standardlari Enstitüsü, Ankara, 1979), considering cross-section, span, loading, screws and material properties. A dataset of 2000 design cases was generated, incorporating variations in these factors based on design calculations in accordance with the relevant standards. After data preprocessing, machine learning (ML) classification algorithms, including Decision Tree (DT), Gaussian Naïve Bayes (GNB), K-Nearest Neighbour (KNN), Linear Discriminant Analysis Classification (LDA), Logistic Regression (LR), Support Vector Machine (SVM) and Voting Ensemble Classification (VEC), were employed to predict design suitability (Fx), where Fx indicates whether the generated design meets structural criteria. The study found that Fx positively correlated with cross-section, span and moment of inertia, while it had a strong negative correlation with moment, stress, shear and deflection. Accuracy scores ranged from 91.7% to 98.6%, with LR performing the best (98.6%), while GNB had the lowest score (91.7%). These results were supported by model performance metrics, namely precision, recall, F1, AUC and MCC scores. Additionally, hyperparameter optimization was applied to improve the performance metrics of the models, resulting in more accurate and reliable predictions. It improved DT, KNN and SVM, while LR, LDA and GNB showed no significant changes. These findings highlight the effectiveness of machine learning in predicting the suitability of timber structure designs, providing a more efficient approach to structural assessment.},
  archive      = {J_NCA},
  author       = {Cosut, Muhammed and Bekdas, Gebrail and Nigdeli, Sinan Melih and Isikdag, Umit},
  doi          = {10.1007/s00521-025-11556-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24123-24148},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predicting design suitability of box-shaped sustainable timber structural members using machine learning and hyperparameter optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PINNs for solving unsteady maxwell’s equations: Convergence issues and comparative assessment with compact schemes. <em>NCA</em>, <em>37</em>(29), 24103-24122. (<a href='https://doi.org/10.1007/s00521-025-11554-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-Informed Neural Networks (PINNs) have recently gained prominence as a mesh-free, physics-integrated framework for solving partial differential equations. In this work, we evaluate the capabilities of PINNs in solving unsteady Maxwell’s equations, benchmarking their performance against two established numerical schemes: the finite-difference time-domain method and a compact Padé scheme. The investigation spans three canonical test cases, involving one-dimensional free-space wave propagation and two-dimensional Gaussian pulse evolution in both periodic and dielectric media. The convergence enhancement strategies including random Fourier feature embeddings, spatio-temporal periodicity enforcement, and temporal causality constraints are assessed systematically through ablation study. The results suggest that architectural design choices must be closely aligned with the governing physics to ensure stable and accurate convergence. Using Neural Tangent Kernel analysis, the intrinsic “uneven learning” behavior of PINNs is uncovered. It was observed that PINNs can fail to prioritize regions with high error, instead converging more rapidly where the loss is already small, contrary to effective optimization principles. Overall, this study demonstrates that PINNs, with appropriate architecture, can match or surpass numerical solvers in accuracy and flexibility. However, challenges remain in addressing spatial inhomogeneity of convergence rate (uneven learning), adapting training to localized high-gradient features, and computational cost.},
  archive      = {J_NCA},
  author       = {Shaviner, Gal G. and Chandravamsi, Hemanth and Pisnoy, Shimon and Chen, Ziv and Frankel, Steven H.},
  doi          = {10.1007/s00521-025-11554-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24103-24122},
  shortjournal = {Neural Comput. Appl.},
  title        = {PINNs for solving unsteady maxwell’s equations: Convergence issues and comparative assessment with compact schemes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based diagnostic model for autism spectrum disorder using blood biomarkers. <em>NCA</em>, <em>37</em>(29), 24075-24102. (<a href='https://doi.org/10.1007/s00521-025-11553-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autistic spectrum disorder (ASD) is a neurological condition characterized by difficulties in social interaction, communication, and repetitive behaviors. Despite its largely hereditary nature, early detection is crucial, and a potential strategy for more efficient and quicker diagnosis is to employ artificial intelligence (AI). In this paper, a new system is introduced that is called automatic screening autism (ASA) system. ASA comprises three primary stages: data preparation (DP), feature selection (FS), and patient detection (PD). In the first stage, the used dataset is preprocessed through several steps: handling missing values and rejecting outliers. Then, these preprocessed features are fed to the FS stage to select the most important features using improved genetic algorithm (IGA). IGA composed of two stages; (i) pre-selection stage (PS2) using information gain (IG) and (ii) conclusive selection stage (CS2) using genetic algorithm (GA). Subsequently, these attributes are input into the proposed classification model using optimized deep neural network (ODNN). Actually, ODNN is based on optimized weights of traditional DNN using various optimization algorithms, and the final decision is derived from the best performance. ASA has been evaluated against contemporary methodologies. Results from experimental studies demonstrate that the proposed ASA outperforms its competitors regarding accuracy, precision, sensitivity, and F-measure, achieving values of approximately 99.10, 98.90, 98.70, and 98.80% in that order.},
  archive      = {J_NCA},
  author       = {Shaban, Warda M.},
  doi          = {10.1007/s00521-025-11553-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24075-24102},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial intelligence-based diagnostic model for autism spectrum disorder using blood biomarkers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of PEM fuel cells via puma optimizer with the aid of practical verifications. <em>NCA</em>, <em>37</em>(29), 24051-24074. (<a href='https://doi.org/10.1007/s00521-025-11552-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents a novel application of the recently developed Puma Optimizer (PO) for identifying the unknown parameters in Mann’s model, which is widely used for characterizing the behavior of Polymer Electrolyte Membrane Fuel Cells (PEMFCs). The proposed PO-based methodology is rigorously evaluated using three test cases. One test case involves experimental I–V measurements under various operating conditions from a commercial PEMFC stack, the Horizon H-100 (100 W), which was assembled and tested in the laboratory. The other two cases are established benchmark PEMFC systems, the Ballard Mark V 5 kW and BCS 500 W units. Comprehensive statistical analyses over multiple independent runs are performed to confirm the consistency and reliability of the PO-based approach. To evaluate the optimizer’s accuracy and robustness, comparisons are made with three other metaheuristic algorithms: two recently developed methods, the Propagation Search Algorithm (PSA) and the Walrus Optimization Algorithm (WOA), and a widely recognized one, the Slime Mold Optimizer (SMO). The comparisons show that the PO optimizer consistently achieved the lowest total square error (TSE) across all test cases, outperforming the other algorithms. Specifically, it develops the lowest values of 0.835811 for the Ballard Mark V 5 kW, 0.011281 for the BCS 0.5 kW, 0.711452 at 30 °C, 0.886144 at 35 °C, and 2.057015 at 40 °C for the Horizon H-100 fuel cell. These findings confirm that the PO is a reliable and effective tool for parameter estimation in PEMFC modeling under both simulated and real-world experimental conditions.},
  archive      = {J_NCA},
  author       = {Ashraf, Hossam and Abdellatif, Sameh O. and Elkholy, Mahmoud M. and El-Fergany, Attia A.},
  doi          = {10.1007/s00521-025-11552-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24051-24074},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance analysis of PEM fuel cells via puma optimizer with the aid of practical verifications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving DNNs for time-series classification using state and gradient abstraction-based preprocessing. <em>NCA</em>, <em>37</em>(29), 24025-24049. (<a href='https://doi.org/10.1007/s00521-025-11550-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series classification is important in various domains and tasks, and has attracted meaningful research over the past decades. Recent years have brought meaningful advancements in using deep neural networks (DNNs) architectures to classify time-series. However, challenges due to measurement errors, missing values, and irregular sampling are still a concern. To minimize their impact on the classification performance, various standardization methods are commonly applied to the raw continuous data as a preprocessing stage to the DNN. Instead, we suggest employing temporal abstraction, wherein the raw time-series is converted into a symbolic representation of time points. The transformed data can then be utilized as input for the DNNs. Specifically, we explore the impact of combining temporal abstraction with convolution-based sequence models and recurrent neural networks. To assess the effectiveness of these methods, we conducted evaluations on a total of 128 univariate and 13 multivariate time-series datasets. Through our framework, which incorporates the temporal abstraction process, we significantly enhanced the performance of various state-of-the-art DNNs used for time-series classification tasks. Our evaluation shows that our methods are significantly superior in classification prediction across all seven evaluation metrics for univariate time-series datasets, outperforming in terms of AUC-ROC for multivariate time-series datasets, compared to predictions using standardized raw data.},
  archive      = {J_NCA},
  author       = {Itzhak, Nevo and Tal, Shahar and Cohen, Hadas and Daniel, Osher and Kopylov, Roze and Moskovitch, Robert},
  doi          = {10.1007/s00521-025-11550-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {24025-24049},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving DNNs for time-series classification using state and gradient abstraction-based preprocessing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing cervical cancer detection: A new optimized explainable artificial intelligence model. <em>NCA</em>, <em>37</em>(29), 23979-24023. (<a href='https://doi.org/10.1007/s00521-025-11548-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a classification model for analyzing cervical cancer images, addressing one of the most prevalent cancers among women worldwide. Early detection is crucial for improving recovery rates and reducing mortality. The integration of artificial intelligence (AI) in medical diagnostics has shown promise in cervical cancer screening by enabling faster results, reducing dependency on specialists, and minimizing biases. While AI-based solutions exist, ongoing research aims to enhance accuracy and efficiency. Advancements in deep learning (DL) have facilitated the development of automated frameworks for medical image analysis, including cervical cancer detection. This research proposes a five-phase model: (1) pre-processing the dataset, (2) extracting features using pre-trained models, (3) optimizing feature selection with the Copula Entropy-based Golden Jackal Optimization (CE-based GJO) algorithm, (4) optimizing hyperparameters using the Sea Horse Optimizer (SHO), and (5) employing explainable AI (XAI) to identify key cytomorphological features in classification. The proposed model is trained on the SipakMed dataset, the largest publicly available cervical cancer dataset on Kaggle. Experimental results demonstrate the proposed model’s superior performance, achieving high precision (0.9985), specificity (0.9996), F-measure (0.9985), and accuracy (0.9985). It outperforms leading benchmark studies, highlighting its potential for precise cervical cancer diagnosis. Additionally, the model offers a secure, cost-effective, and efficient AI-driven solution for early detection and screening.},
  archive      = {J_NCA},
  author       = {Abdel-Salam, Mahmoud and Askr, Heba and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-025-11548-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23979-24023},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing cervical cancer detection: A new optimized explainable artificial intelligence model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating and benchmarking nutritional supplement providers using a multi-criteria decision-making modeling approach. <em>NCA</em>, <em>37</em>(29), 23941-23978. (<a href='https://doi.org/10.1007/s00521-025-11547-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To determine the adequacy and balance of nutritional supplements for children, a high level of assessment and ranking of available Nutritional Supplement Providers (NSPs) is needed. Inherently, the evaluation of nutritional supplements is complex because it involves (1) different criteria, (2) trade-offs in preferences, and (3) the development of models that require data to be handled in a standardized manner. This study aims to develop a robust decision-making framework for assessing and benchmarking NSPs for children, thereby supporting informed choices in pediatric dietary supplementation. The methodology is developed in three phases. Phase 1: Identifying and collecting the dataset. Phase 2: Develop a decision matrix that includes 28 nutritional criteria, following eight NSPs as alternatives. Phase 3: Integration of mathematical process of 2-Tuple Linguistic q-Rung Picture Fuzzy Level-Based Weight Assessment (2TLq-RPF-LBWA), which offers a rigorous evaluation of the nutritional information for the children’s dietary supplements referred to in this study as Nutritional Information for Kids' Dietary Supplements (NIKDSs), and the Additively Ratio Assessment (ARAS) method helps in benchmarking the NSPs. The results from the 2TLq-RPF-LBWA indicated that the most highly weighted nutrients were 'vitamin D' (0.0545), 'iron' (0.0521), 'vitamin B12' (0.0500), and 'vitamin B9' (folic acid) (0.0480), which were the most highly weighted nutrients, whereas 'chromium' had the lowest weight (0.0245). ARAS benchmarking analysis indicated that "Centrum Kids Chewable Multivitamins" had a rating of 0.7626, ranking it in the top position. The "KINDER Multivitamin Syrup" was second, with a rating of 0.2893; "Maddovit Junior" was third, with a rating of 0.2599; and "OLIGOVIT Syrup" was rated lowest, with a rating of 0.0575. To verify the rigor of the results, three sensitivity analysis scenarios were developed, confirming that the methodology was robust and improved decision-making related to the development of safer, effective vitamin and mineral supplements for children. Overall, this study provides valuable benchmarking information to help parents, professionals, and manufacturers select suitable nutritional supplements for children.},
  archive      = {J_NCA},
  author       = {Habeeb, Mustafa Abdulfattah and Khaleel, Yahya Layth and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Sharaf, Iman Mohamad},
  doi          = {10.1007/s00521-025-11547-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23941-23978},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluating and benchmarking nutritional supplement providers using a multi-criteria decision-making modeling approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A benchmark study of optimizers for short-term solar PV power forecasting using neural networks under real-world constraints. <em>NCA</em>, <em>37</em>(29), 23909-23939. (<a href='https://doi.org/10.1007/s00521-025-11546-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term photovoltaic (PV) power forecasts are critical for efficient grid balancing, yet training optimizers, often overlooked compared to neural network architectures, significantly influence prediction accuracy and convergence speed. Prior research primarily focuses on adjusting network architectures, typically employing a single optimizer (commonly Adam), thus leaving optimizer selection underexplored, especially under noisy and incomplete real-world PV data. This study systematically benchmarks four optimizers—Adam, Adaptive Gradient (Adagrad), Rectified Adam (RAdam), and Lookahead—across three deep-learning architectures (Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN)-LSTM, and LSTM-Autoencoder) using data from two distinct PV sites. Unlike prior works, we assess optimizer effectiveness across a wide range of conditions, including varying training data lengths, sampling intervals, and missing data patterns (both random and block-wise). Using two real-world PV datasets representing semi-arid and desert climates, we analyze forecasting accuracy, convergence time, and robustness. Our empirical results demonstrate that RAdam consistently outperforms Adam by achieving up to 36% lower forecasting error under noisy and incomplete data conditions, while Lookahead offers up to 40% faster convergence in deep hybrid models. These gains translate into tighter reserve-margin planning and smoother inverter set-points, advancing state-of-the-art PV forecast pipelines. The paper concludes with optimizer-architecture recommendations for practitioners facing latency or compute constraints.},
  archive      = {J_NCA},
  author       = {Dhingra, Saloni and Gruosso, Giambattista and Storti Gajani, Giancarlo},
  doi          = {10.1007/s00521-025-11546-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23909-23939},
  shortjournal = {Neural Comput. Appl.},
  title        = {A benchmark study of optimizers for short-term solar PV power forecasting using neural networks under real-world constraints},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing aviation safety and efficiency: Applying artificial intelligence (AI) to address navigational challenges. <em>NCA</em>, <em>37</em>(29), 23883-23907. (<a href='https://doi.org/10.1007/s00521-025-11544-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision Navigation and Timing systems, integral to air traffic management, primarily rely on the Global Navigation Satellite System (GNSS) for accurate data transmission. However, GNSS signals, particularly those used in Automatic Dependent Surveillance-Broadcast (ADS-B) systems, are susceptible to interference, leading to potential safety risks in aviation. This study presents a novel AI-driven Flight Trajectory Prediction Framework, leveraging machine learning and deep learning techniques to detect and mitigate GNSS signal interference. By training models on ADS-B data, this framework identifies potential interference and evaluates its impact on air traffic. The implementation of this AI-based system enhances the reliability and security of air navigation, significantly reducing human error and elevating overall safety standards in aviation. Experimental results demonstrate the framework’s efficacy in improving navigational accuracy and operational efficiency within modern air traffic control systems.},
  archive      = {J_NCA},
  author       = {Hamza, Alyaa A. and Yosef, Rehan Ahmed and Rahouma, Kamel Hussien},
  doi          = {10.1007/s00521-025-11544-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23883-23907},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing aviation safety and efficiency: Applying artificial intelligence (AI) to address navigational challenges},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing false alarms by identifying depression-mimicking expressions. <em>NCA</em>, <em>37</em>(29), 23863-23882. (<a href='https://doi.org/10.1007/s00521-025-11543-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, people have been using social media platforms to express their feelings and share their mental health struggles openly and anonymously. This surge has motivated many researchers to take advantage of social media as a valuable resource of data to detect severe depression. However, existing approaches have significant limitations as they rely on datasets with a wide disparity between depressive and non-depressive instances, ignoring depression-mimicking expressions (such as stress, anxiety, sadness, sarcasm, and complaints) that are frequently misclassified as severe depression due to their linguistic overlap. This, in turn, leads to false alarms about inaccurate cases of depression, undermining our confidence in the detection system. We present the first study that aims to detect severe depression while simultaneously differentiating it from other depression-mimicking expressions. We curated and annotated a new dataset and refined existing ones to capture these depression-mimicking expressions. We implement and compare the performance of state-of-the-art large language models (LLMs), with RoBERTa emerging as a top-performing model, achieving an AUC of 98.37% on the validation set and 98.53% on the separate test set. Notably, fine-tuning of the models led to an impressive average AUC increase in around 40% over their original baseline versions, significantly enhancing the models’ ability to distinguish severe depression from depression-mimicking expressions. The fine-tuned RoBERTa model generalized well to an external dataset, increasing AUC from 0.65 to 0.97 and significantly reducing false alarms. The significant improvement highlights the effectiveness of fine-tuning LLMs on carefully curated data, reducing false alarms, and boosting the model’s reliability and applicability in practical settings.},
  archive      = {J_NCA},
  author       = {Ghouch, Baraa Abou and Khreich, Wael},
  doi          = {10.1007/s00521-025-11543-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23863-23882},
  shortjournal = {Neural Comput. Appl.},
  title        = {Reducing false alarms by identifying depression-mimicking expressions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nutriconv: Multitask learning framework for digital dietary tracking trained on EFSA’s pancake dataset. <em>NCA</em>, <em>37</em>(29), 23833-23862. (<a href='https://doi.org/10.1007/s00521-025-11542-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prevalence of nutrition-related health conditions calls for advanced tools to support reliable and efficient dietary monitoring. This paper presents NutriConv, a lightweight multitask convolutional neural network designed to simultaneously perform food classification and weight estimation from single-item food images. Trained on the institutionally validated PANCAKE dataset from the European Food Safety Authority, NutriConv combines classification and regression objectives within a unified architecture, optimized via a hybrid loss function. While its classification accuracy remains lower than that of specialized single-task models, NutriConv achieves competitive regression performance and offers a practical balance between both tasks. Its compact design enables deployment on resource-constrained platforms such as smartglasses and mobile health devices, expanding its usability in real-world dietary tracking scenarios. Extensive experiments confirm its robustness, including external validation on the Nutrition5K dataset, underscoring the model’s generalizability. This work highlights the potential of multitask learning for integrated, scalable, and accessible AI-based nutrition assessment.},
  archive      = {J_NCA},
  author       = {Junquera, Enol and Rico, Noelia and Díaz, Irene and González, Sonia and Remeseiro, Beatriz},
  doi          = {10.1007/s00521-025-11542-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23833-23862},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nutriconv: Multitask learning framework for digital dietary tracking trained on EFSA’s pancake dataset},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced fault detection and diagnosis in wind turbine systems using canonical variate analysis and reconstruction-based contribution method. <em>NCA</em>, <em>37</em>(29), 23811-23832. (<a href='https://doi.org/10.1007/s00521-025-11541-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection and diagnosis (FDD) is critical for ensuring the performance, safety, and reliability of industrial systems, especially in the expanding wind energy sector. As wind turbine installations continue to increase globally, maintaining operational reliability has become increasingly important due to their complex and nonlinear nature. Traditional FDD methods often underperform in such systems due to challenges in handling high-dimensional, nonlinear data. This paper proposes a robust methodology for fault detection and diagnosis in wind turbines. The proposed approach employs canonical variate analysis (CVA) for fault detection by analyzing multivariate data, and reconstruction-based contribution (RBC) for fault isolation by quantifying the contribution of individual variables. The methodology was validated using benchmark data from a real wind turbine system. The results demonstrate high effectiveness in detecting and diagnosing faults, highlighting the approach’s capability to manage complex, nonlinear systems. Simulation results show that the proposed methodology outperforms traditional techniques such as PCA, PLS, EMPRM, and TPCR achieving higher fault detection rates and improved sensitivity, with detection accuracy exceeding $$95\%$$ across multiple fault scenarios. These findings confirm the successful achievement of the research objectives and represent a significant advancement in enhancing the safety, reliability, and operational performance of wind turbine systems under dynamic conditions.},
  archive      = {J_NCA},
  author       = {Elshenawy, Lamiaa M. and Gafar, Ahmed A. and Awad, Hamdi A.},
  doi          = {10.1007/s00521-025-11541-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23811-23832},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced fault detection and diagnosis in wind turbine systems using canonical variate analysis and reconstruction-based contribution method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal object detection: An architecture using feature-level fusion and deep learning. <em>NCA</em>, <em>37</em>(29), 23799-23810. (<a href='https://doi.org/10.1007/s00521-025-11521-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is one of the most fundamental problems to tackle in the computer vision research area. Recent advances in multimodal data streams and deep learning architectures have prompted a fast growth in the field of multimodal learning, which brings several advantages over single-modality approaches for object detection, such as improved accuracy, robustness to noise and ambiguity, handling of complex scenarios and adaptability to diverse data. Some of the biggest challenges when implementing a multimodal learning approach are the selection of the fusion strategy, design of processing architecture, modality alignment/synchronization and interpretability of such high-dimensional representations. To address this challenge, we propose a feature-level fusion architecture for object detection based on extracting YOLO features from images, spectral and rhythm features from sound using Mel-frequency cepstral coefficients, and general descriptors from radar modalities that, after timestamp and homography transformation matrix alignment, are combined with an attention mechanism into a single classification network. Preliminary experiments indicate that the proposed architecture can constitute itself as a base pipeline for several different multimodal object detection tasks in real-world applications.},
  archive      = {J_NCA},
  author       = {Silva, Rui and Coelho, Eduardo and Pimenta, Nuno and Durães, Dalila and Alves, Victor and Bandeira, Lourenço and Machado, José and Novais, Paulo and Melo-Pinto, Pedro},
  doi          = {10.1007/s00521-025-11521-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23799-23810},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal object detection: An architecture using feature-level fusion and deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports. <em>NCA</em>, <em>37</em>(29), 23789-23797. (<a href='https://doi.org/10.1007/s00521-024-10640-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated radiology report generation is an emerging field for improving patient care and alleviating radiologist workload. However, existing methods face a range of challenges such as limited data availability, clinical metric performance, and interpretability. To address these issues, we propose a contrastive concept-phrase pre-training (C2P2) method, which utilizes a phrase-concept grounding task for contrastive learning. C2P2 learns the correspondence between phrases in a report and image concepts by using a phrase classification task to train a multi-label classifier for X-rays and extracting visual concepts of phrases using class activation maps. We then fine-tune a pre-trained BERT model to translate the extracted phrases into reports. Our proposed method outperforms or matches the previous state of the art in clinical efficacy metrics on both internal and external datasets. Moreover, C2P2 leverages more vision language data for pre-training and provides visual explanations of generated phrases.},
  archive      = {J_NCA},
  author       = {Tubaishat, Abdallah and Zia, Tehseen and Windridge, David and Nawaz, Muhammad and Razzaq, Saad},
  doi          = {10.1007/s00521-024-10640-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23789-23797},
  shortjournal = {Neural Comput. Appl.},
  title        = {Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing machine learning algorithms for imputation of missing time series in meteorological data. <em>NCA</em>, <em>37</em>(29), 23773-23787. (<a href='https://doi.org/10.1007/s00521-024-10601-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores advanced feedforward neural networks specifically multi-layer perceptron (MLP), long short-term memory (LSTM), and convolutional neural networks (CNNs) as time-series imputation techniques to address the challenge of missing data in analytical contexts. The study evaluates their performance by introducing artificial data gaps of varying durations 3 days, 1 week, and 1 month. The results reveal that all three algorithms (MLP, LSTM, and CNN) exhibit the ability to estimate incomplete data, yet with differing accuracies. LSTM and CNN outperform in filling short-term gaps (3 days and 1 week) with R2 values of 77% and 70% for LSTM, and 58.4% and 69.7% for CNN. MLP also demonstrates effectiveness, achieving accuracies of 74.9% for a 3-day gap and 67.7% for a 1-week gap. Notably, CNN proves the most accurate for monthly data gaps, attaining an R2 value of 70.1%. The findings suggest that the selection of imputation techniques should consider the specific time gap, with CNN highlighted as particularly effective for monthly gaps. In conclusion, this study provides valuable insights for researchers and practitioners engaged in imputing missing data in time-series analysis.},
  archive      = {J_NCA},
  author       = {Boujoudar, Mohamed and El Ydrissi, Massaab and Abraim, Mounir and Bouarfa, Ibtissam and El Alani, Omaima and Ghennioui, Hicham and Bennouna, El Ghali},
  doi          = {10.1007/s00521-024-10601-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23773-23787},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comparing machine learning algorithms for imputation of missing time series in meteorological data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a model for the study and measurement of consciousness in artificial cognitive systems based on the integrated information theory. <em>NCA</em>, <em>37</em>(29), 23739-23771. (<a href='https://doi.org/10.1007/s00521-024-10584-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study and measurement of consciousness in artificial cognitive systems have been subjects of great interest in the fields of artificial intelligence and neuroscience. This article introduces an innovative model based on the integrated information theory (IIT) aimed at addressing this fundamental issue. The IIT, proposed by Giulio Tononi, offers a robust theoretical approach to comprehend consciousness in terms of information and its integration. Our model focuses on identifying and quantifying key aspects of consciousness in artificial systems, including information complexity and the structure of artificial neural connections. We have developed a conceptual framework to assess the “integrity” of information within a system, enabling us to measure consciousness in terms of its capacity to integrate information coherently, setting it apart from other approaches. Through experiments and simulations conducted with a video game using the Go No Go task, we demonstrate how our model can be applied to artificial cognitive systems, allowing for the evaluation of their level of consciousness in different contexts. This approach carries significant implications for the advancement of more advanced and ethical artificial intelligence systems, as it provides a methodology for evaluating and comparing their level of consciousness. Ultimately, our work contributes to the progress in understanding and measuring consciousness in artificial systems, paving the way for future developments in artificial intelligence and computational neuroscience.},
  archive      = {J_NCA},
  author       = {Guerrero, Luz Enith and Arango-López, Jeferson and Castillo, Luis Fernando and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10584-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23739-23771},
  shortjournal = {Neural Comput. Appl.},
  title        = {Development of a model for the study and measurement of consciousness in artificial cognitive systems based on the integrated information theory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and machine learning approach for detecting spatial and temporal forgeries in videos. <em>NCA</em>, <em>37</em>(29), 23723-23737. (<a href='https://doi.org/10.1007/s00521-024-10558-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video forgery detection has become increasingly critical due to the rise of sophisticated video manipulation techniques. Traditional methods often struggle to keep up with the ever-evolving sophistication of forgery techniques, necessitating innovative and advanced solutions. In response to this challenge, we propose a hybrid approach that combines deep learning and machine learning techniques. This approach leverages the power of DenseNet121 for excellent spatial feature extraction, utilizes a spatiotemporal autoencoder to capture spatiotemporal dependencies, and employs a gradient boosting machine (GBM) to effectively classify the video clips based on the extracted features. This provides a robust and comprehensive solution for detecting spatial and temporal forgeries in videos. By combining these components, this approach offers a more robust and comprehensive solution for detecting spatial and temporal forgeries in videos. To evaluate our approach, we are using a diverse dataset of videos (VTD and VIFFD) with different forgery scenarios and conducting extensive experiments. The results demonstrate the effectiveness and accuracy of our hybrid approach in detecting spatial and temporal forgeries, surpassing the performance of individual classifiers and traditional methods.},
  archive      = {J_NCA},
  author       = {Singh, Upasana and Rathor, Sandeep and Kumar, Manoj},
  doi          = {10.1007/s00521-024-10558-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23723-23737},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid deep learning and machine learning approach for detecting spatial and temporal forgeries in videos},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of causal inference in the DQN sampling process for classical control problems. <em>NCA</em>, <em>37</em>(29), 23709-23721. (<a href='https://doi.org/10.1007/s00521-024-10540-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, causal inference is integrated into deep reinforcement learning to enhance sampling in classical control environments. The problem we’re working on is "classical control," where an agent makes decisions to keep systems balanced. With the help of artificial intelligence and causal inference, we have developed a method that adjusts a deep Q-network’s experience memory by adjusting the priority of transitions. According to the agent’s actions, these priorities are based on the magnitude of causal differences. We have applied our methodology to a reference environment in reinforcement learning. In comparison with a deep Q-network based on conventional random sampling, the results indicate significant improvements in performance and learning efficiency. Our study shows that causal inference can be integrated into the sampling process so that experience transitions can be selected more intelligently, resulting in more effective learning for classical control problems. The study contributes to the convergence between artificial intelligence and causal inference, offering new perspectives for the application of reinforcement learning techniques in real-world applications where precise control is essential.},
  archive      = {J_NCA},
  author       = {Velez Bedoya, Jairo Ivan and Gonzalez Bedia, Manuel and Castillo Ossa, Luis Fernando and Arango Lopez, Jeferson and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10540-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23709-23721},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integration of causal inference in the DQN sampling process for classical control problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model. <em>NCA</em>, <em>37</em>(29), 23707-23708. (<a href='https://doi.org/10.1007/s00521-025-11146-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Islam, Umar and Shah, Babar and Al-Atawi, Abdullah A. and Arnone, Gioia and Abonazel, Mohamed R. and Ali, Ijaz and Moreira, Fernando},
  doi          = {10.1007/s00521-025-11146-0},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23707-23708},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction to: Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model. <em>NCA</em>, <em>37</em>(29), 23683-23706. (<a href='https://doi.org/10.1007/s00521-024-10169-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethereum has emerged as a major platform for decentralized apps and smart contracts with the heightened interest in cryptocurrencies in recent years. Investors and market participants in the cryptocurrency space will find it increasingly important to use reliable price prediction models as Ethereum's popularity grows. To better estimate Ethereum prices around the world, we propose "EtherVoyant," a novel hybrid forecasting model that combines the advantages of ARIMA and SARIMA methods. To improve its forecasting abilities, EtherVoyant uses Ethereum price history to train ARIMA and SARIMA components independently before fusing their predictions. With the help of feature engineering and data preparation, we further improve the model so that it can deal with real-world difficulties like missing values and seasonality in the data. We also investigate hyperparameter optimization for the model's best possible performance. We compare EtherVoyant's forecasts against those of the more conventional ARIMA and SARIMA models to determine its efficacy. By providing more precise and trustworthy price forecasts, our trial results suggest that EtherVoyant is superior to the individual models. The importance of this study resides in the fact that it will lead to the creation of a sophisticated time series forecasting model that will be useful to cryptocurrency investors, traders, and decision-makers. We hope that by making EtherVoyant available on a worldwide scale, we will help advance the field of cryptocurrency analytics and encourage wider adoption of blockchain-based assets.},
  archive      = {J_NCA},
  author       = {Islam, Umar and Shah, Babar and Al-Atawi, Abdullah A. and Arnone, Gioia and Abonazel, Mohamed R. and Ali, Ijaz and Moreira, Fernando},
  doi          = {10.1007/s00521-024-10169-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23683-23706},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering global ethereum price prediction with EtherVoyant: A state-of-the-art time series forecasting model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering privacy and resilience: A decentralized federated learning approach to cyberbullying detection. <em>NCA</em>, <em>37</em>(29), 23667-23682. (<a href='https://doi.org/10.1007/s00521-024-10148-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a rapidly changing digital world, the rise of cyberbullying has become a pressing issue that calls for creative and flexible solutions to detect and prevent it. To address this urgent need, we introduce a novel method: decentralized federated learning for cyberbullying detection using a ring topology network. Traditional federated learning (FL) paradigms have traditionally relied on centralized servers to manage important operations. However, this centralized model faces complex challenges related to privacy vulnerabilities, fairness disparities, and scalability constraints. Our solution brings about a significant change, embracing decentralization as the foundation of a strong and privacy-focused cyberbullying detection framework. This research represents a significant shift away from centralization, as it relies on distributed clients in a ring topology network to collectively carry out crucial FL tasks. Through the redistribution of these responsibilities and the establishment of direct communication channels between neighboring nodes, our approach successfully avoids the challenges related to central server instability and bias. The ring topology architecture creates a decentralized ecosystem, carefully crafted to prioritize the confidentiality of user data while enhancing the resilience and efficiency of the FL process. Our model architecture for cyberbullying detection is carefully crafted, utilizing the powerful capabilities of GRU, LSTM, BERT, and Word2Vec embedding, along with emotional features. This architectural innovation perfectly aligns with the ring topology FL approach, enabling localized updates, efficient aggregation, and flexible adaptability. It is worth mentioning that the BERT model consistently outperforms its competitors, delivering exceptional results.},
  archive      = {J_NCA},
  author       = {Khan, Umair and Khan, Salabat and Mussiraliyeva, Shynar and Samee, Nagwan Abdel and Alabdulhafith, Maali and Shah, Khalid},
  doi          = {10.1007/s00521-024-10148-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {29},
  pages        = {23667-23682},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering privacy and resilience: A decentralized federated learning approach to cyberbullying detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-personalized employment in urban hubs: Multimodal fusion architectures for personality-based job matching. <em>NCA</em>, <em>37</em>(28), 23651-23666. (<a href='https://doi.org/10.1007/s00521-024-10587-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of smart cities, employment strategies have been steering towards a more personalized approach, aiming to enhance job satisfaction and boost economic efficiency. This paper explores an advanced solution by integrating multimodal deep learning to create a hyper-personalized job matching system based on individual personality traits. We employed the First Impressions V2 dataset, a comprehensive collection encompassing various data modalities suitable for extracting personality insights. Among various architectures tested, the fusion of XceptionResNet with BERT emerged as the most promising, delivering unparalleled results. The combined model achieved an accuracy of 92.12%, an R2 score of 54.49%, a mean squared error of 0.0098, and a root mean squared error of 0.0992. These empirical findings demonstrate the effectiveness of the XceptionResNet + BERT in mapping personality traits, paving the way for an innovative, and efficient approach to job matching in urban environments. This work has the potential to revolutionize recruitment strategies in smart cities, ensuring placements that are not only skill-aligned but also personality-congruent, optimizing both individual satisfaction and organizational productivity. A set of theoretical case studies in technology, banking, healthcare, and retail sectors within smart cities illustrate how the model could optimize both individual satisfaction and organizational productivity.},
  archive      = {J_NCA},
  author       = {Jain, Dipika and Sangwan, Saurabh Raj and Kumar, Akshi},
  doi          = {10.1007/s00521-024-10587-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23651-23666},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hyper-personalized employment in urban hubs: Multimodal fusion architectures for personality-based job matching},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based segmentation for medical data hiding with galois field. <em>NCA</em>, <em>37</em>(28), 23635-23650. (<a href='https://doi.org/10.1007/s00521-023-09151-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hiding is of the utmost importance for protecting the copyright of image content, given the widespread use of images in the healthcare domain. Presently, medical image security is important not only for protecting individual privacy but also for accurate diagnosis and treatment. In this paper, a deep learning-based segmentation for a medical data hiding technique with the Galois field is proposed. This technique uses a customised UNet3+ deep learning network to segment a medical image into a Region of Interest and a Non-Region of Interest. Through the proper spatial and transform-based embedding method, multiple marks are embedded into both parts of the medical image. In addition, encryption is utilised to provide additional security for protecting sensitive information when transmitted over an open channel so that the information cannot be retrieved. The extensive experimental results show that the proposed technique for medical images achieves a good balance between imperceptibility and robustness with high security. Further, the obtained results showed the superiority of our technique over state-of-the-art techniques, demonstrating that it can provide a reliable security solution for healthcare data.},
  archive      = {J_NCA},
  author       = {Amrit, P. and Singh, K. N. and Baranwal, N. and Singh, A. K. and Singh, J. P. and Zhou, H.},
  doi          = {10.1007/s00521-023-09151-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23635-23650},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based segmentation for medical data hiding with galois field},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A highly densed deep neural architecture for classification of the multi-organs in fetal ultrasound scans. <em>NCA</em>, <em>37</em>(28), 23619-23633. (<a href='https://doi.org/10.1007/s00521-023-09148-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) makes a substantial contribution to decision-making in many intricate application areas of the medical sciences. One such application is organ classification in maternal–fetal ultrasound scans using sophisticated AI methodology like deep neural networks for better analysis. In this paper, we present a novel and highly dense deep neural architecture specifically designed for the multi-organ classification of fetal ultrasound scans. Our proposed approach introduces a unique combination of densely connected layers, convolution layers, and skip connections, tailored to accurately identify and classify multiple fetal organs simultaneously. To the best of our knowledge, this is the first study to address the comprehensive classification of multiple organs in fetal ultrasound images using such a highly dense neural network. The architecture is designed to capture both local and global features, critical for distinguishing intricate organ structures in ultrasound images. The model also minimizes the gradient loss for faster convergence of the model parameters in the training phase. Through extensive evaluation of a curated dataset of fetal ultrasound scans, we demonstrate that our novel architecture achieves superior classification accuracy—96.85%, precision—97.12%, recall—96.66%, F1-score—96.88%, and AUC-ROC score—97.27%, outperforming state-of-the-art methods in the context of multi-organ classification. Thus, the proposed highly dense deep neural architecture presents a promising avenue for enhancing fetal ultrasound imaging, bringing potential benefits to prenatal care, and contributing to improved neonatal outcomes.},
  archive      = {J_NCA},
  author       = {Srivastava, Somya and Vidyarthi, Ankit and Jain, Shikha},
  doi          = {10.1007/s00521-023-09148-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23619-23633},
  shortjournal = {Neural Comput. Appl.},
  title        = {A highly densed deep neural architecture for classification of the multi-organs in fetal ultrasound scans},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Round trip time meets transformers: High-fidelity human counting in cluttered environments. <em>NCA</em>, <em>37</em>(28), 23591-23617. (<a href='https://doi.org/10.1007/s00521-025-11540-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate human counting in indoor environments is essential for optimizing people-centric applications, such as crowd management, disaster response, and monitoring in settings like shopping malls and healthcare facilities. Traditional vision approaches face challenges with poor lighting conditions and raise privacy concerns. WiFi-based solutions enable device-free human counting by detecting disruptions in wireless signals caused by human presence. However, methods using received signal strength indicator are unreliable due to physical obstructions, multipath fading, radio interference, and fluctuating access point power. While WiFi channel state information-based systems are more sensitive to environmental changes, they lack standardization, limiting their practicality. To overcome these limitations, this paper presents Time4Count, an innovative device-free indoor human counting system that leverages round trip time measurements to achieve high accuracy and scalability. Time4Count capitalizes on human-induced fluctuations in signal propagation time to accurately estimate the number of individuals in a space. By employing a multivariate transformer-based feature extraction method, the system effectively mitigates non-line-of-sight errors and signal distortions, ensuring robust performance even in cluttered indoor environments. Additionally, Time4Count integrates spatial discretization and multi-label classification techniques, enabling it to count an unlimited number of individuals in real-time. The system was rigorously evaluated in two realistic, cluttered environments using commodity hardware, involving up to 15 participants. Experimental results reveal that Time4Count achieves an high counting accuracy of 92.7%. To our knowledge, Time4Count is the first RTT-based indoor counting system, providing a precise solution for indoor monitoring. Implementation is available at: https://github.com/mclab-osaka/time4count .},
  archive      = {J_NCA},
  author       = {Yonekura, Haruki and Rizk, Hamada and Yamaguchi, Hirozumi},
  doi          = {10.1007/s00521-025-11540-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23591-23617},
  shortjournal = {Neural Comput. Appl.},
  title        = {Round trip time meets transformers: High-fidelity human counting in cluttered environments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable information fusion approach to impute meteorological missing values toward cross-domain intelligent forecasting. <em>NCA</em>, <em>37</em>(28), 23533-23589. (<a href='https://doi.org/10.1007/s00521-025-11538-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep robust domain adaptation (DRDA) model for heterogeneous weighted information fusion (HWIF), called DRDAHWIF-gram, is proposed to impute long-term missing meteorological data for cross-domain intelligent forecasting. This study’s core contribution lies in assessing forest climate change risks through explainable machine learning, which leverages a multi-criteria computational performance metric to establish a smart early warning index (SEWI). Two real-world case studies, utilizing the Hyrcanian forest dataset and the comparative climatic data, validate the model’s ability to capture predictable meteorological patterns, including temperature, humidity, solar radiation, and wind speed trends, across diverse climatological domains. Evaluated across multiple forecast horizons, the developed model outperforms conventional methods, demonstrating higher accuracy and reliability in weather variation monitoring. The high SEWI confirms the stability of meteorological domains, supporting the deployment of intelligent climatic hazard early warning systems. This approach advances expert forecasting for meteorological threats under forest climate change.},
  archive      = {J_NCA},
  author       = {Zarchi, Milad and Aslani, Zohreh Hashemi and Tee, Kong Fah},
  doi          = {10.1007/s00521-025-11538-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23533-23589},
  shortjournal = {Neural Comput. Appl.},
  title        = {An interpretable information fusion approach to impute meteorological missing values toward cross-domain intelligent forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking arabic event relation extraction: A comprehensive survey. <em>NCA</em>, <em>37</em>(28), 23513-23531. (<a href='https://doi.org/10.1007/s00521-025-11537-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event relation extraction tasks are paramount in natural language processing (NLP), facilitating a deeper understanding of textual narratives and enhancing language comprehension. However, a notable gap emerges when considering the Arabic language, with limited research dedicated to event-related tasks. This paper addresses this research gap by embarking on a comprehensive exploration of Arabic event relation extraction tasks. It is the first survey to explore Arabic event relation extraction tasks, illustrating the tasks, available corpora, existing works, and associated evaluation metrics. It also addresses the inherent challenges of Arabic event relations and provides insights into established English corpora and research for reference. Furthermore, the paper sets a future research agenda, emphasizing the need for dedicated corpora across tasks to advance the field and automate Arabic text and narrative comprehension.},
  archive      = {J_NCA},
  author       = {Aldawsari, Mohammed},
  doi          = {10.1007/s00521-025-11537-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23513-23531},
  shortjournal = {Neural Comput. Appl.},
  title        = {Unlocking arabic event relation extraction: A comprehensive survey},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of compressive strengths of portland cement with random forest, support vector machine and gradient boosting models. <em>NCA</em>, <em>37</em>(28), 23495-23511. (<a href='https://doi.org/10.1007/s00521-025-11536-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents machine learning models to predict compressive strengths of 924 CEM I 42.5 R type Portland cements. Particularly the utilized machine learning algorithms are adaptive network-based fuzzy inference systems, Random Forest, Support Vector Machine, Extreme Gradient Boosting, Light Gradient Boosting and Categorical Boosting. For machine learning, collected data contained 15 input features that show the physical and chemical properties of the cements. The compressive strengths at 1, 2, 7 and 28 days were defined as the output parameters. Models for each hydration day were trained with 748 data points and tested with 176 data points. Then, compressive strength test results and machine learning predictions were compared using statistical methods such as R-squared, mean absolute percentage error and root-mean-square error. The results indicate that Gradient Boosting models, in particular, accurately predict compressive strength, demonstrating that it is possible to estimate compressive strength without mechanical tests. In our developed Gradient Boosting model, the RMSE accuracy exceeds 95%, further supporting its reliability. The developed machine learning models offer substantial savings in both time and cost for compressive strength estimation.},
  archive      = {J_NCA},
  author       = {Ozcan, Giyasettin and Gulbandilar, Eyyup and Kocak, Yilmaz},
  doi          = {10.1007/s00521-025-11536-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23495-23511},
  shortjournal = {Neural Comput. Appl.},
  title        = {Prediction of compressive strengths of portland cement with random forest, support vector machine and gradient boosting models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable classification by local error amplification. <em>NCA</em>, <em>37</em>(28), 23473-23494. (<a href='https://doi.org/10.1007/s00521-025-11535-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel predictive approach rooted in the field of explainable AI, designed to provide sketches of the action of a non-interpretable machine learning model. The use case is psychological profiling and concerns the alignment between individuals’ personalities and specific job roles within the fields of psychology and career development. Our approach builds upon the foundation of BAPC (Before and After prediction Parameter Correction) (Sobieczky and Geiß in Explainable AI by BAPC—Before and After correction Parameter Comparison, 2023), an explainable AI technique designed to elucidate predictions from models with interpretable parameters of a surrogate ’base’ model. We extend this method by incorporating error amplification (EA), a new technique effectively integrating decision trees as our base model. In the three-step methodology, rooted in the principles of BAPC, enhanced by error amplification, we firstly train a base model using decision trees and identify the model’s errors. Second, we introduce an AI model to predict the errors made by the base model. Subsequently, we amplify these predicted errors in a localized manner around the individual point of interest. In the third step, we retrain the base model using the error-amplified data set. The disparity between the decision boundaries generated by the initial and the retrained decision trees provides explainable insights into enhancing specific personality features. While our method finds its application in personality-job alignment here, its versatility extends to a wide range of scenarios, offering valuable insights into data set feature interpretability by elucidating their effective changes.},
  archive      = {J_NCA},
  author       = {Dudkin, Erika and Sobieczky, Florian},
  doi          = {10.1007/s00521-025-11535-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23473-23494},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable classification by local error amplification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Histogram self-organizing map for cluster detection and visualization. <em>NCA</em>, <em>37</em>(28), 23447-23472. (<a href='https://doi.org/10.1007/s00521-025-11530-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering of histogram data, which preserves important distributional information, remains a significant challenge due to the limitations of existing approaches. This paper presents a novel two-level clustering algorithm explicitly designed for histogram data. It integrates a self-organizing map (SOM) framework with the $$L_2$$ Wasserstein distance to capture distributional features such as location, scale, and shape. By enriching prototypes with local density and connectivity measures, the approach automatically infers cluster numbers without prior knowledge. Experiments on synthetic and real-world datasets underscore the quality of the obtained results in comparison with other distance-based or histogram-based approaches, the computational efficiency of the proposed approach, and its interpretability through topology-preserving visualizations.},
  archive      = {J_NCA},
  author       = {Cabanes, Guénaël and Bennani, Younès and Rastin, Parisa},
  doi          = {10.1007/s00521-025-11530-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23447-23472},
  shortjournal = {Neural Comput. Appl.},
  title        = {Histogram self-organizing map for cluster detection and visualization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in NN: From fitting most to fitting a few. <em>NCA</em>, <em>37</em>(28), 23423-23446. (<a href='https://doi.org/10.1007/s00521-025-11528-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute to the understanding of learning behavior of neural networks by showing that in principle a few possibly noisy samples can significantly alter weights of a network, even after most samples have been fit. Formally, we approach learning dynamics by analyzing prediction accuracy, input reconstruction ability, and prediction performance. A neural network can exhibit a prototype-learning phase decreasing reconstruction loss initially, possibly increasing the loss of a small set of well-defined samples, i.e., defined based on the L2-norm and dot product with the mean of class samples. Toward the end of training, parameter updates might mostly reduce the classification loss of a few samples, which increases reconstruction loss. Aside from providing a mathematical analysis of a linear network, we also assess the behavior using common datasets and architectures from computer vision. On the practical side, our work supports training data selection with low computational effort, i.e., identifying samples that are more likely to be misclassified. This can help in identifying labeling errors, ensuring a diverse dataset, and data valuation. Our work also casts a different view on the notion of outliers in supervised learning.},
  archive      = {J_NCA},
  author       = {Schneider, Johannes},
  doi          = {10.1007/s00521-025-11528-4},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23423-23446},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning in NN: From fitting most to fitting a few},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IntruSafe: A FCNN-LSTM hybrid IoMT intrusion detection system for both string and 2D-spatial data using sandwich architecture. <em>NCA</em>, <em>37</em>(28), 23395-23422. (<a href='https://doi.org/10.1007/s00521-025-11527-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) is a resource-constrained device with limited computational capabilities. However, the market worth of this section is booming rapidly. The IoMT manufacturers need to offer their products at a competitive price, which forces them to use simplified architecture, leaving limited and, to some extent, no scope to employ sophisticated cybersecurity algorithms. As a result, IoMT has become a lucrative practice ground for cybercriminals. The IoMT sector deals with valuable, confidential healthcare-related data and offers convenient, personalized healthcare services. That is why the market demand and IoMT intrusion are experiencing massive growth. An innovative Intrusion Detection System (IDS), IntruSafe, has been studied, developed, and presented in this paper that combines Fully Connected Convolutional Neural Network (FCNN) and Long Short-Term Memory (LSTM) to protect the IoMT network from malicious signals. The IntruSafe combines FCNN and LSTM to ensure the detection of both malicious text and image data. It detects and simultaneously protects the IoMT network from further intrusion with only a 0.18% service interruption rate. This high-performing IDS detects intrusion with 97.66% accuracy, 98.50% precision, 97.33% recall, and 97.85% F1-score. With outstanding performance, IntruSafe is a promising IDS that will facilitate further growth of the IoMT sector while minimizing the risks of a successful intrusion.},
  archive      = {J_NCA},
  author       = {Alazab, Moutaz and Awajan, Albara and Obeidat, Areej and Faruqui, Nuruzzaman and Rehman, Hafeez Ur},
  doi          = {10.1007/s00521-025-11527-5},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23395-23422},
  shortjournal = {Neural Comput. Appl.},
  title        = {IntruSafe: A FCNN-LSTM hybrid IoMT intrusion detection system for both string and 2D-spatial data using sandwich architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaze-guided contrastive unsupervised representations learning. <em>NCA</em>, <em>37</em>(28), 23381-23393. (<a href='https://doi.org/10.1007/s00521-025-11526-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the integration of information-rich prior knowledge, specifically human gaze data, to enhance representation learning through contrastive methods. We propose gaze-guided contrastive unsupervised representation learning, a novel framework harnessing human gaze data to guide the selection of positive and negative samples for contrastive learning. By leveraging human gaze information, we capture meaningful patterns in visual task dynamics, enabling the agent to acquire effective strategies from demonstrations and achieve superior performance. Our findings demonstrate significant improvements over baseline algorithms, highlighting the value of gaze-guided representation learning in reducing data requirements and accelerating learning. This approach offers broad applicability to vision-based tasks, emphasizing the critical role of human gaze in improving task efficiency and generalization.},
  archive      = {J_NCA},
  author       = {Distefano, Joseph P. and Manjunatha, Hemanth and Thammineni, Chaithanya and Dalland, Kristian and Esfahani, Ehsan T.},
  doi          = {10.1007/s00521-025-11526-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23381-23393},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gaze-guided contrastive unsupervised representations learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAHGNet: Modeling hybrid-grained heterogenous dependency efficiently for traffic prediction. <em>NCA</em>, <em>37</em>(28), 23359-23379. (<a href='https://doi.org/10.1007/s00521-025-11525-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction plays a critical role in the intelligent transportation system, and it is also a challenging task because of the underlying complex spatiotemporal patterns and heterogeneities evolving across time. However, most present works mostly concentrate on solely capturing Spatial-temporal dependency or extracting implicit similarity graphs, but the hybrid-granularity evolution is ignored in their modeling process. In this paper, we proposed a novel data-driven end-to-end framework, named spatiotemporal aware hybrid graph network (STAHGNet), to couple the hybrid-grained heterogeneous correlations in series simultaneously through an elaborately hybrid graph attention module (HGAT) and coarse-granularity temporal graph (CTG) generator. Furthermore, an automotive feature engineering with domain knowledge and a random neighbor sampling strategy is utilized to improve efficiency and reduce computational complexity. The MAE, RMSE, and MAPE are used for evaluation metrics. Tested on four real-life datasets, our proposal outperforms eight classical baselines and four state-of-the-art (SOTA) methods (e.g., MAE 14.82 on PeMSD3; MAE 18.92 on PeMSD4). Besides, extensive experiments and visualizations verify the effectiveness of each component in STAHGNet. In terms of computational cost, STAHGNet saves at least four times the space compared to the previous SOTA models. The proposed model will be beneficial for more efficient TFP as well as intelligent transport system construction.},
  archive      = {J_NCA},
  author       = {Wang, Jiyao and Peng, Zehua and Zhang, Yijia and He, Dengbo and Lei, Chen},
  doi          = {10.1007/s00521-025-11525-7},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23359-23379},
  shortjournal = {Neural Comput. Appl.},
  title        = {STAHGNet: Modeling hybrid-grained heterogenous dependency efficiently for traffic prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TA-ASTGCN: A trend-aware adaptive spatio-temporal graph convolutional network for traffic flow prediction. <em>NCA</em>, <em>37</em>(28), 23335-23358. (<a href='https://doi.org/10.1007/s00521-025-11513-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial to improving the safety and efficiency of transportation systems. Although existing deep learning methods have made some progress in traffic flow prediction, most methods fail to effectively capture multi-source spatial relationships and long-term temporal dependencies. To address these problems, we present a traffic prediction model based on convolutional trend-aware attention and adaptive spatio-temporal graph convolution (TA-ASTGCN). The model captures complex spatio-temporal relationships by alternately stacking multiple convolutional trend-aware attention layers and adaptive spatio-temporal graph convolution layers in the encoder and decoder. The convolutional trend-aware attention layer captures the trends in traffic flow sequences and the correlations between nodes. The adaptive spatio-temporal graph convolution layer adaptively adjusts the strength of node relationships to effectively mine spatial correlations in traffic data. Compared with ASTGNN, with the best predictive performance in the baseline models, our TA-ASTGCN showed on average 3.69%, 3.14%, and 3.80% improvement in MAE, RMSE, and MAPE on four datasets. Experimental results showed that the prediction performance of the proposed model is better than the baseline models.},
  archive      = {J_NCA},
  author       = {Cai, Chuang and Guo, Huijie and Zhang, Zhenlin and Dou, Tianfeng and Wu, Dong and Qi, Kaiyuan and Bai, Yuqin and Ren, Chongguang},
  doi          = {10.1007/s00521-025-11513-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23335-23358},
  shortjournal = {Neural Comput. Appl.},
  title        = {TA-ASTGCN: A trend-aware adaptive spatio-temporal graph convolutional network for traffic flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-organizing soft sensor for process control systems: Integrating support vector regression with subtractive clustering. <em>NCA</em>, <em>37</em>(28), 23301-23333. (<a href='https://doi.org/10.1007/s00521-025-11504-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely prediction of critical process variables is crucial for optimizing performance in process control systems. Traditional hardware sensors often face challenges such as high costs, maintenance issues, and response delays. Data-driven soft sensors present a promising alternative by utilizing historical and real-time process data to estimate key variables. This paper introduces an integrated approach that combines support vector regression (SVR) with subtractive clustering and Grey wolf optimizer (GWO) to develop an advanced soft sensor. Subtractive clustering serves as a self-organizing technique, identifying and grouping the most relevant data points from historical data to establish a robust foundation for training the SVR model, thereby capturing complex nonlinear relationships within the process. To further improve the model’s performance, GWO is used to fine-tune SVR hyperparameters, optimizing the search space for the best results. The proposed soft sensor’s prediction accuracy is evaluated using the Tennessee Eastman process benchmark, the Tennessee Eastman process poses significant challenges due to its highly nonlinear nature and the strong interactions among variables, and its uncertainty is analyzed through a quantile regression scheme. Simulation results demonstrate that this approach improves process monitoring, reduces operational costs, and enhances overall system reliability and efficiency.},
  archive      = {J_NCA},
  author       = {Elshenawy, Lamiaa M. and Badawy, Ahmed and AbouOmar, Mahmoud S. and Mahmoud, Tarek A.},
  doi          = {10.1007/s00521-025-11504-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23301-23333},
  shortjournal = {Neural Comput. Appl.},
  title        = {A self-organizing soft sensor for process control systems: Integrating support vector regression with subtractive clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossGCL: Cross-view graph contrastive learning with dual tasks for drug recommendation. <em>NCA</em>, <em>37</em>(28), 23273-23299. (<a href='https://doi.org/10.1007/s00521-025-11497-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug recommendation aims to recommend proper drugs according to patients’ symptoms. A patient usually has multiple symptoms, and a doctor needs to prescribe a combination of drugs rather than an individual drug for a patient. However, existing works mainly model the semantic interactions between symptoms and drugs at the item level but ignore set-level semantic interactions, which are also crucial for drug recommendation tasks. To address the above issues, we propose Cross-view Graph Contrastive Learning with Dual Tasks for Drug Recommendation (CrossGCL) to recommend drugs. Our model learns the representation of symptoms and drugs from the item-level interactions (item view) and the set-level interactions (set view) and then maximizes the mutual information from the two views by cross-view contrastive learning so that the representation from different views can be enhanced by each other. Besides, we construct two structural-duality learning tasks, a symptom-driven drug prediction task and a drug-driven symptom prediction task, to better capture the set-level and item-level semantic information. We conduct experiments on the widely used public MIMIC-III dataset and show that our model outperforms state-of-the-art models.},
  archive      = {J_NCA},
  author       = {Wen, Wushao and Wang, Liang and Fang, Lihuang and Chen, Qiangpu and Qin, Jinghui},
  doi          = {10.1007/s00521-025-11497-8},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23273-23299},
  shortjournal = {Neural Comput. Appl.},
  title        = {CrossGCL: Cross-view graph contrastive learning with dual tasks for drug recommendation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fully convolutional neural network for fast detection, classification, and segmentation of fabric defects. <em>NCA</em>, <em>37</em>(28), 23249-23272. (<a href='https://doi.org/10.1007/s00521-025-11495-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of artificial intelligence and image processing for fabric defect detection is gaining prominence due to its practical significance in enhancing production quality. This study proposes a fast and accurate convolutional neural network (CNN) designed to detect defects in fabric with minimal computational complexity. The model processes input images of size 256 × 256 and generates defect masks of size 64 × 64. To improve detection accuracy, the model incorporates techniques such as ResNet, scheduled learning rate policies, data augmentation, and a weighted cross-entropy loss function. Trained on a diverse dataset of 2,681 defect samples from four fabric types and defect classes (holes, oil stains, color stains, and roller marks), the model achieved an accuracy of over 96%, a loss value below 0.1, and high recall, precision, and F1-Score. Compared to other state-of-the-art models, the proposed model delivers competitive performance with significantly faster prediction times, making it suitable for real-world fabric inspection applications.},
  archive      = {J_NCA},
  author       = {Mohammed, Swash Sami and Clarke, Hülya Gökalp and Mahmood, Sarmad Nozad},
  doi          = {10.1007/s00521-025-11495-w},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23249-23272},
  shortjournal = {Neural Comput. Appl.},
  title        = {A fully convolutional neural network for fast detection, classification, and segmentation of fabric defects},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance evaluation of AI and hybrid-AI models for estimation of evaporation in lesser himalayan valley. <em>NCA</em>, <em>37</em>(28), 23219-23248. (<a href='https://doi.org/10.1007/s00521-025-11492-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaporation holds a significant position in the global hydrological cycle and is one of the intricate phenomena widely affected by various hydrometeorological parameters. Evaporation accounts for 66% of global precipitation losses, profoundly influencing surface and rainfall losses, necessitating its meticulous quantification. Its estimation is resource-intensive, time-consuming, costly and sensitive to climatic and spatial variability. Over 22 numerical-physical methods are available, affected by time, data availability and climatic conditions. Data-driven artificial intelligence (AI) and machine learning (ML) models can be useful where it is very difficult to estimate the evaporation spatially and precisely. The current study uses 10 daily hydrometeorological in situ parameters: temperature (maximum, minimum and mean), vapour pressure (7.19 h, 14.19 h), relative humidity (7.19 h, 14.19 h), rainfall, bright sunshine hours and mean wind velocity for 23 years (2001:2023) for a Lesser Himalayan Valley (Doon Valley), India. The models developed for the estimation are ANN-SGD, ANN-LM, ANN-Adam, SVM and LSTM in addition to two hybrid models; $$R^{2}$$ , $${\text{NSE}}$$ and $${\text{MARE}}$$ measure ANN-PSO and ANN-GA and the performance of the models. The study enlightens on two major outcomes: the application of the various AI-based models for estimation of evaporation and the intercomparison of their outputs with the hybrid-AI models. All models show $$R^{2}$$ > 0.8, 0.15 ≤ $${\text{MARE}}$$ ≤ 0.25 and $${\text{NSE}}$$ ≥ 0.73, signifying robust performances. ANN-PSO and ANN-GA outperformed other models by integrating AI learning with optimization algorithms, addressing single-algorithm limitations. The study’s findings can assist researchers and act as a tool for the local stakeholders in managing water resources.},
  archive      = {J_NCA},
  author       = {Rajkumar, Gupta Abhishek and Nema, Manish Kumar and Khare, Deepak},
  doi          = {10.1007/s00521-025-11492-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23219-23248},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance evaluation of AI and hybrid-AI models for estimation of evaporation in lesser himalayan valley},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An autoencoder-based approach for feature engineering in cardiovascular disease prediction. <em>NCA</em>, <em>37</em>(28), 23185-23218. (<a href='https://doi.org/10.1007/s00521-025-11484-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease remains the leading cause of death worldwide, underscoring the urgent need for accurate, timely diagnosis to improve patient outcomes. Traditional diagnostic methods often struggle with accuracy, delayed intervention, and the complexity of analyzing diverse patient data. Moreover, existing machine learning approaches face challenges due to missing values, class imbalances, and limited features, all of which can negatively affect model performance. To overcome these limitations, this study introduces a novel autoencoder-based framework to enhance feature engineering for heart disease prediction. Autoencoders are employed for both dimensionality reduction and the generation of new, informative features that capture complex, non-linear relationships within the data. The methodology is evaluated using eight diverse heart disease datasets, including the widely recognized Cleveland and Hungarian datasets, with performance assessed through accuracy, balanced accuracy, ROC AUC, F1 score, and computational efficiency. Experimental results demonstrate notable improvements, with the method achieving 91.12% accuracy on the Cleveland dataset and 86.96% on the Hungarian dataset. Comparative analysis across four experimental scenarios—baseline performance with original features, autoencoder-based feature generation, feature reduction, and Bayesian hyperparameter optimization—reveals that the proposed approach substantially outperforms traditional techniques, achieving up to 96% accuracy on concatenated datasets and 95% on a comprehensive dataset. These compelling results highlight the potential of advanced autoencoder-driven feature engineering combined with sophisticated optimization strategies to significantly enhance diagnostic accuracy and generalization in heart disease prediction.},
  archive      = {J_NCA},
  author       = {Rashed, Amr E. Eldin and Badawy, Mahmoud and Elhosseini, Mostafa A. and Bahgat, Waleed M.},
  doi          = {10.1007/s00521-025-11484-z},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23185-23218},
  shortjournal = {Neural Comput. Appl.},
  title        = {An autoencoder-based approach for feature engineering in cardiovascular disease prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A census-based genetic algorithm for target set selection problem in social networks. <em>NCA</em>, <em>37</em>(28), 23155-23183. (<a href='https://doi.org/10.1007/s00521-025-11480-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the target set selection (TSS) problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph is given. A minimum size vertex subset needs to be found to “activate” such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called “a census-based genetic algorithm” for the TSS problem. In our algorithm, the concept of a census is utilized to gather and store information about individuals in a population and collect census data from the individuals constructed during the algorithm’s execution so that greater diversity and avoiding premature convergence can be achieved at locally optimal solutions. Specifically, two distinct census informations have been used: (a) For individuals, the algorithm stores how many times it has been identified during the execution; (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also be self-adjusted by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check individual’s feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, the proposed algorithm has been employed on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.},
  archive      = {J_NCA},
  author       = {Rahman, Md. Samiur and Ahsan, Mohammad Shamim and Chen, Cheng-Wu and Varadarajan, Vijayakumar},
  doi          = {10.1007/s00521-025-11480-3},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23155-23183},
  shortjournal = {Neural Comput. Appl.},
  title        = {A census-based genetic algorithm for target set selection problem in social networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwitchTree: In-network computing and traffic analyses with random forests. <em>NCA</em>, <em>37</em>(28), 23143-23154. (<a href='https://doi.org/10.1007/s00521-020-05440-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of machine learning in different domains is also finding applications in networking. However, this may need real-time analyses of network data which is challenging. The challenge is caused by the big data size and the need for bandwidth to transfer network data to a central location hosting the analyses server. In order to address that challenge, the in-network computing paradigm is gaining popularity with the advances in programmable data plane solutions. In this paper, we perform in-network analysis of the network data by exploiting the power of programmable data plane. We propose SwitchTree which embeds Random Forest algorithm inside a programmable switch such that the Random Forest is configurable and re-configurable at runtime. We show how some flow level stateful features can be estimated, such as the round-trip time and bitrate of each flow. We evaluate the performance of SwitchTree using system level experiments and network traces. Results show that SwitchTree is able to detect network attacks at line speed with high accuracy.},
  archive      = {J_NCA},
  author       = {Lee, Jong-Hyouk and Singh, Kamal},
  doi          = {10.1007/s00521-020-05440-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23143-23154},
  shortjournal = {Neural Comput. Appl.},
  title        = {SwitchTree: In-network computing and traffic analyses with random forests},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TermInformer: Unsupervised term mining and analysis in biomedical literature. <em>NCA</em>, <em>37</em>(28), 23129-23142. (<a href='https://doi.org/10.1007/s00521-020-05335-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terminology is the most basic information that researchers and literature analysis systems need to understand. Mining terms and revealing the semantic relationships between terms can help biomedical researchers find solutions to some major health problems and motivate researchers to explore innovative biomedical research issues. However, how to mine terms from biomedical literature remains a challenge. At present, the research on text segmentation in natural language processing (NLP) technology has not been well applied in the biomedical field. Named entity recognition models usually require a large amount of training corpus, and the types of entities that the model can recognize are limited. Besides, dictionary-based methods mainly use pre-established vocabularies to match the text. However, this method can only match terms in a specific field, and the process of collecting terms is time-consuming and labour-intensive. Many scenarios faced in the field of biomedical research are unsupervised, i.e. unlabelled corpora, and the system may not have much prior knowledge. This paper proposes the TermInformer project, which aims to mine the meaning of terms in an open fashion by calculating terms and find solutions to some of the significant problems in our society. We propose an unsupervised method that can automatically mine terms in the text without relying on external resources. Our method can generally be applied to any document data. Combined with the word vector training algorithm, we can obtain reusable term embeddings, which can be used in any NLP downstream application. This paper compares term embeddings with existing word embeddings. The results show that our method can better reflect the semantic relationship between terms. Finally, we use the proposed method to find potential factors and treatments for lung cancer, breast cancer, and coronavirus.},
  archive      = {J_NCA},
  author       = {Tiwari, Prayag and Uprety, Sagar and Dehdashti, Shahram and Hossain, M. Shamim},
  doi          = {10.1007/s00521-020-05335-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23129-23142},
  shortjournal = {Neural Comput. Appl.},
  title        = {TermInformer: Unsupervised term mining and analysis in biomedical literature},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bio-inspired computation for big data fusion, storage, processing, learning and visualization: State of the art and future directions. <em>NCA</em>, <em>37</em>(28), 23097-23127. (<a href='https://doi.org/10.1007/s00521-021-06332-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This overview gravitates on research achievements that have recently emerged from the confluence between Big Data technologies and bio-inspired computation. A manifold of reasons can be identified for the profitable synergy between these two paradigms, all rooted on the adaptability, intelligence and robustness that biologically inspired principles can provide to technologies aimed to manage, retrieve, fuse and process Big Data efficiently. We delve into this research field by first analyzing in depth the existing literature, with a focus on advances reported in the last few years. This prior literature analysis is complemented by an identification of the new trends and open challenges in Big Data that remain unsolved to date, and that can be effectively addressed by bio-inspired algorithms. As a second contribution, this work elaborates on how bio-inspired algorithms need to be adapted for their use in a Big Data context, in which data fusion becomes crucial as a previous step to allow processing and mining several and potentially heterogeneous data sources. This analysis allows exploring and comparing the scope and efficiency of existing approaches across different problems and domains, with the purpose of identifying new potential applications and research niches. Finally, this survey highlights open issues that remain unsolved to date in this research avenue, alongside a prescription of recommendations for future research.},
  archive      = {J_NCA},
  author       = {Torre-Bastida, Ana I. and Díaz-de-Arcaya, Josu and Osaba, Eneko and Muhammad, Khan and Camacho, David and Del Ser, Javier},
  doi          = {10.1007/s00521-021-06332-9},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23097-23127},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bio-inspired computation for big data fusion, storage, processing, learning and visualization: State of the art and future directions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-sync: A novel deep learning-based tool for semantic-aware subtitling synchronisation. <em>NCA</em>, <em>37</em>(28), 23081-23095. (<a href='https://doi.org/10.1007/s00521-021-05751-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subtitles are a key element to make any media content accessible for people who suffer from hearing impairment and for elderly people, but also useful when watching TV in a noisy environment or learning new languages. Most of the time, subtitles are generated manually in advance, building a verbatim and synchronised transcription of the audio. However, in TV live broadcasts, captions are created in real time by a re-speaker with the help of a voice recognition software, which inevitability leads to delays and lack of synchronisation. In this paper, we present Deep-Sync, a tool for the alignment of subtitles with the audio-visual content. The architecture integrates a deep language representation model and a real-time voice recognition software to build a semantic-aware alignment tool that successfully aligns most of the subtitles even when there is no direct correspondence between the re-speaker and the audio content. In order to avoid any kind of censorship, Deep-Sync can be deployed directly on users’ TVs causing a small delay to perform the alignment, but avoiding to delay the signal at the broadcaster station. Deep-Sync was compared with other subtitles alignment tool, showing that our proposal is able to improve the synchronisation in all tested cases.},
  archive      = {J_NCA},
  author       = {Martín, Alejandro and González-Carrasco, Israel and Rodriguez-Fernandez, Victor and Souto-Rico, Mónica and Camacho, David and Ruiz-Mezcua, Belén},
  doi          = {10.1007/s00521-021-05751-y},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23081-23095},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep-sync: A novel deep learning-based tool for semantic-aware subtitling synchronisation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking vital signs of a patient using channel state information and machine learning for a smart healthcare system. <em>NCA</em>, <em>37</em>(28), 23065-23079. (<a href='https://doi.org/10.1007/s00521-020-05631-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a smart healthcare system, the sensor-embedded wearable devices have the ability to track various vital signs of a patient. However, such devices need to be worn by the patients all the time. These devices have limitations such as their battery lifetime, charging mechanism, and hardware-related cost. Moreover, these devices transmit a huge amount of redundant and inconsistent data. The transmitted data need to be fused to remove any outlier so that only highly- refined data are available for decision-making. In this paper, we use channel state information (CSI) to track the vital signs of a patient and remove any outliers from the gathered data. We monitor the respiration rate of a patient during sleep with minimal hardware-related cost. Our CSI-based approach no longer requires the patients to wear any wearables and can monitor even the minute fluctuations in a WiFi signal. For extracting useful features from the respiratory data, three types of feature extraction techniques are used. In order to select important features from the extracted feature space, three feature selection algorithms, i.e., Relief, mRMR, and Lasso, have been investigated. In addition, for predicting the health conditions of a patient, four machine learning classification algorithms, i.e., KNN, SVM, DT, and RF, are utilized. The use of CSI ensures that highly refined and fused data are available for feature selection, and the selected features are presented to the ML classification algorithms for predicting the health condition of the patient.},
  archive      = {J_NCA},
  author       = {Khan, Muhammad Imran and Jan, Mian Ahmad and Muhammad, Yar and Do, Dinh-Thuan and Rehman, Ateeq ur and Mavromoustakis, Constandinos X. and Pallis, Evangelos},
  doi          = {10.1007/s00521-020-05631-x},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23065-23079},
  shortjournal = {Neural Comput. Appl.},
  title        = {Tracking vital signs of a patient using channel state information and machine learning for a smart healthcare system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convert index trading to option strategies via LSTM architecture. <em>NCA</em>, <em>37</em>(28), 23047-23064. (<a href='https://doi.org/10.1007/s00521-020-05377-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past, most strategies were mainly designed to focus on stocks or futures as the trading target. However, due to the enormous number of companies in the market, it is not easy to select a set of stocks or futures for investment. By investigating each company’s financial situation and the trend of the overall financial market, people can invest precisely in the market and choose to go long or short. Moreover, how to determine the position size of the transaction is also a problematic issue. In the past, many money management theories were based on the Kelly criterion. And they put a certain percentage of their total funds into the market for trading. Nonetheless, three massive problems cannot be overcome. First, futures are leveraged transactions, and extra funds must be deposited as margin. It causes that the position size is hard to be estimated by the Kelly criterion. The second point is that the trading strategy is difficult to determine the winning rate in the financial market and cannot be brought into the Kelly criterion to calculate the optimal fraction. Last, the financial data are always massive. A big data technique should be applied to resolve this issue and enhance the performance of the framework to reveal knowledge in the financial data. Therefore, in this paper, a concept of converting the original futures trading strategy into options trading is proposed. An LSTM (long short-term memory)-based framework is proposed to predict the profit probability of the original futures strategy and convert the corresponding daily take-profit and stop-loss points according to the delta value of the options. Finally, the proposed framework brings the results into the Kelly criterion to get the optimal fraction of options trading. The final research results show that options trading is closer to the optimal fraction calculated by the Kelly criterion than futures trading. If the original futures trading strategy can profit, the benefits after converting to options trading can be further superior.},
  archive      = {J_NCA},
  author       = {Wu, Jimmy Ming-Tai and Wu, Mu-En and Hung, Pang-Jen and Hassan, Mohammad Mehedi and Fortino, Giancarlo},
  doi          = {10.1007/s00521-020-05377-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23047-23064},
  shortjournal = {Neural Comput. Appl.},
  title        = {Convert index trading to option strategies via LSTM architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based driver distraction identification framework over edge cloud. <em>NCA</em>, <em>37</em>(28), 23031-23046. (<a href='https://doi.org/10.1007/s00521-020-05328-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the number of traffic accidents has been increased globally. One of the main reasons for this increase is the distraction of the driver on the road. Distracted driving can cause collisions and cause injury, death, or property damage. New techniques can help to mitigate this problem, and one of the recent approaches is to employ body wearable sensors or camera sensors in the vehicle for real-time monitoring and detection of drivers’ distraction and behaviors, such as cell phone use, talking, eating, drinking, radio tuning, navigation interaction, or even combing hair while driving. However, this type of approach requires not only a powerful training module but also a lightweight module for real-time detection and analyzing the captured data. Data need to be collected from specific wearable or camera sensors in order to detect drivers’ distraction and ensure immediate feedback by the administrator for safe driving. Therefore, in this paper, we propose an effective camera-based framework for real-time identification of drivers’ distraction by using a deep learning approach with edge and cloud computing technologies. More specifically, the framework consists of three modules, including the distraction detection module deployed on edge devices in the vehicle environment, the training module deployed in the cloud environment, and finally the analyzing module implemented in the monitoring environment (administrator side) connected with a telecommunication network. The proposed framework is developed using two deep learning models. The first is a custom deep convolutional neural network (CDCNN) model, and the second one is a visual geometry group-16 (VGG16)-based fine-tuned model. Several experiments are conducted on a public large-scale driver distraction dataset to evaluate the two models. The experimental results show that the accuracy rates were 99.64% for the first model and 99.73% for the second model using a holdout test set of 10%. In addition, the first and second models have achieved accuracy rates of 99.36% and 99.57% using a holdout test set of 30%. The results confirmed the applicability and appropriateness of the adopted deep learning models for designing the proposed driver distraction detection framework.},
  archive      = {J_NCA},
  author       = {Gumaei, Abdu and Al-Rakhami, Mabrook and Hassan, Mohammad Mehedi and Alamri, Atif and Alhussein, Musaed and Razzaque, Md. Abdur and Fortino, Giancarlo},
  doi          = {10.1007/s00521-020-05328-1},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23031-23046},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning-based driver distraction identification framework over edge cloud},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeTrAs: Deep learning-based healthcare framework for IoT-based assistance of alzheimer patients. <em>NCA</em>, <em>37</em>(28), 23017-23029. (<a href='https://doi.org/10.1007/s00521-020-05327-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare 4.0 paradigm aims at realization of data-driven and patient-centric health systems wherein advanced sensors can be deployed to provide personalized assistance. Hence, extreme mentally affected patients from diseases like Alzheimer can be assisted using sophisticated algorithms and enabling technologies. Motivated from this fact, in this paper, DeTrAs: Deep Learning-based Internet of Health Framework for the Assistance of Alzheimer Patients is proposed. DeTrAs works in three phases: (1) A recurrent neural network-based Alzheimer prediction scheme is proposed which uses sensory movement data, (2) an ensemble approach for abnormality tracking for Alzheimer patients is designed which comprises two parts: (a) convolutional neural network-based emotion detection scheme and (b) timestamp window-based natural language processing scheme, and (3) an IoT-based assistance mechanism for the Alzheimer patients is also presented. The evaluation of DeTrAs depicts almost 10–20% improvement in terms of accuracy in contrast to the different existing machine learning algorithms.},
  archive      = {J_NCA},
  author       = {Sharma, Sumit and Dudeja, Rajan Kumar and Aujla, Gagangeet Singh and Bali, Rasmeet Singh and Kumar, Neeraj},
  doi          = {10.1007/s00521-020-05327-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {23017-23029},
  shortjournal = {Neural Comput. Appl.},
  title        = {DeTrAs: Deep learning-based healthcare framework for IoT-based assistance of alzheimer patients},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal medical image fusion algorithm in the era of big data. <em>NCA</em>, <em>37</em>(28), 22995-23015. (<a href='https://doi.org/10.1007/s00521-020-05173-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image-based medical decision-making, different modalities of medical images of a given organ of a patient are captured. Each of these images will represent a modality that will render the examined organ differently, leading to different observations of a given phenomenon (such as stroke). The accurate analysis of each of these modalities promotes the detection of more appropriate medical decisions. Multimodal medical imaging is a research field that consists in the development of robust algorithms that can enable the fusion of image information acquired by different sets of modalities. In this paper, a novel multimodal medical image fusion algorithm is proposed for a wide range of medical diagnostic problems. It is based on the application of a boundary measured pulse-coupled neural network fusion strategy and an energy attribute fusion strategy in a non-subsampled shearlet transform domain. Our algorithm was validated in dataset with modalities of several diseases, namely glioma, Alzheimer’s, and metastatic bronchogenic carcinoma, which contain more than 100 image pairs. Qualitative and quantitative evaluation verifies that the proposed algorithm outperforms most of the current algorithms, providing important ideas for medical diagnosis.},
  archive      = {J_NCA},
  author       = {Tan, Wei and Tiwari, Prayag and Pandey, Hari Mohan and Moreira, Catarina and Jaiswal, Amit Kumar},
  doi          = {10.1007/s00521-020-05173-2},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {22995-23015},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal medical image fusion algorithm in the era of big data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-series forecasting of bitcoin prices using high-dimensional features: A machine learning approach. <em>NCA</em>, <em>37</em>(28), 22979-22993. (<a href='https://doi.org/10.1007/s00521-020-05129-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh–ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.},
  archive      = {J_NCA},
  author       = {Mudassir, Mohammed and Bennbaia, Shada and Unal, Devrim and Hammoudeh, Mohammad},
  doi          = {10.1007/s00521-020-05129-6},
  journal      = {Neural Computing and Applications},
  month        = {10},
  number       = {28},
  pages        = {22979-22993},
  shortjournal = {Neural Comput. Appl.},
  title        = {Time-series forecasting of bitcoin prices using high-dimensional features: A machine learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
