<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSTAT</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cstat">CSTAT - 24</h2>
<ul>
<li><details>
<summary>
(2025). A bootstrap-based bandwidth selection rule for kernel quantile estimators. <em>CSTAT</em>, <em>40</em>(7), 4037-4058. (<a href='https://doi.org/10.1007/s00180-024-01582-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile has been widely used to quantify the uncertainty in many fields. In this paper, we study the estimation of quantiles via kernels, especially for extreme quantiles, and propose a bootstrap-based bandwidth selection (BBS) method for it. This method employs bootstrap sampling of data and least-squares regression to estimate the unknown bandwidth parameter in the kernel, which plays a crucial role in kernel smoothing. From a theoretical perspective, we establish a data-driven and bootstrap-based kernel quantile estimator and provide its asymptotic bias and variance, based on which the proposed method is shown to lead to the asymptotically optimal bandwidth selection in terms of minimizing the mean squared error. Numerical experiments demonstrate that the BBS method works well in both bandwidth selection and extreme quantile estimation.},
  archive      = {J_CSTAT},
  author       = {Liu, Xiaoyu and Song, Yan and Cheng, Hong-Fa and Zhang, Kun},
  doi          = {10.1007/s00180-024-01582-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4037-4058},
  shortjournal = {Comput. Stat.},
  title        = {A bootstrap-based bandwidth selection rule for kernel quantile estimators},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic data generation method providing enhanced covariance matrix estimation. <em>CSTAT</em>, <em>40</em>(7), 4007-4035. (<a href='https://doi.org/10.1007/s00180-025-01643-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data generation is an important tool to ensure data confidentiality. Various synthetic data generators have been developed in the literature. The methods in the literature are mostly for general purposes. They aim to generate data whose distributions are the same as the original data set, and the synthesized data are used for every purpose depending on who uses them. However, it could not be good for all purposes. In this paper, we study the synthetic data generation tailored for a specific purpose. We are particularly interested incovariance matrix estimation, which is a key part of many multivariate statistical analyses. To do it, we first see the connection between the sequential regression model and the modified Cholesky decomposition. We then devise a new synthetic data generator, named SynCov, that controls the error variances of the sequential regression model. We show that the sample covariance matrix of the synthetic data generated by SynCov is equivalent to a shrinkage covariance matrix estimator, which reduces estimation error in Frobenius norm. Our comprehensive numerical study shows that SynCov performs better than other synthetic data generation methods in covariance matrix estimation. Finally, we apply our SynCov to two real data examples, (i) the estimation of the covariance matrix of the (selected) variables of the Los Angeles City Employee Payroll data and (ii) the classification of the Taiwanese Bankruptcy Data.},
  archive      = {J_CSTAT},
  author       = {Kim, Seungkyu and Lim, Johan and Yu, Donghyeon},
  doi          = {10.1007/s00180-025-01643-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4007-4035},
  shortjournal = {Comput. Stat.},
  title        = {Synthetic data generation method providing enhanced covariance matrix estimation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially functional linear expectile regression model with missing observations. <em>CSTAT</em>, <em>40</em>(7), 3981-4005. (<a href='https://doi.org/10.1007/s00180-025-01652-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate estimation for the partially functional linear expectile regression model where observations are missing at random (MAR). First, we construct expectile regression (ER) estimators for both the slope functions and scalar parameters. Second, to obtain confidence intervals for the scalar parameters, we propose both the multiplier bootstrap method and the empirical likelihood (EL) method. Meanwhile, the maximum empirical likelihood (MEL) estimators for the scalar parameters are derived using the empirical log-likelihood ratio function. Furthermore, under mild conditions, we establish several asymptotic properties, including the convergence rates of the ER estimators for the scalar parameters and the slope function, the asymptotic normality of the ER estimators and the MEL estimators for the scalar parameters, and the convergence of the empirical log-likelihood ratio function to the standard chi-squared distribution. Finally, simulation studies and a real data analysis are conducted to evaluate the performance of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Wu, Chengxin and Ling, Nengxiang},
  doi          = {10.1007/s00180-025-01652-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3981-4005},
  shortjournal = {Comput. Stat.},
  title        = {Partially functional linear expectile regression model with missing observations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning causal graphs using variable grouping according to ancestral relationship. <em>CSTAT</em>, <em>40</em>(7), 3947-3979. (<a href='https://doi.org/10.1007/s00180-025-01633-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the sample size is small relative to the number of variables, the accuracy of the conventional causal learning algorithm decreases. Some causal discovery methods are not feasible when the sample size is smaller than the number of variables. To circumvent these problems, some researchers proposed causal discovery algorithms using divide-and-conquer approaches (e.g., Cai et al. in Sada: a general framework to support robust causation discovery. In: International Conference on machine learning, PMLR, pp 208–216, 2013; Zhang et al. in IEEE Trans Cybern 52:3232–3243, 2020). For learning an entire causal graph, divide-and-conquer approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal discovery algorithm is applied, it is expected to improve the estimation accuracy, especially when the sample size is small relative to the number of variables and the model is sparse. However, existing methods are computationally expensive or do not provide sufficient accuracy when the sample size is small. This paper proposes a new algorithm for grouping variables according to the causal ancestral relationships, assuming that the causal model is LiNGAM (Shimizu et al. J Mach Learn Res 7:2003–2030, 2006). We call the proposed algorithm the causal ancestral-relationship-based grouping (CAG). The time complexity of the ancestor finding in the CAG is shown to be cubic in the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM (Shimizu et al. in J Mach Learn Res-JMLR 12:1225–1248, 2011) and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the causal model is sparse or moderately dense. We also apply the proposed method to two real datasets to confirm its usefulness.},
  archive      = {J_CSTAT},
  author       = {Cai, Ming and Hara, Hisayuki},
  doi          = {10.1007/s00180-025-01633-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3947-3979},
  shortjournal = {Comput. Stat.},
  title        = {Learning causal graphs using variable grouping according to ancestral relationship},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model. <em>CSTAT</em>, <em>40</em>(7), 3911-3946. (<a href='https://doi.org/10.1007/s00180-025-01611-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this review article, the term “hierarchy” is related to constrained-ness, but not to superiority. Procedures A and B forming a hierarchy means that A is a constrained variant of B or vice versa. A goal of this article is to present a hierarchy of principal component analysis (PCA) and factor analysis (FA) procedures, which follows from a comprehensive FA (CompFA) model. This model can be regarded as a hybrid of PCA and prevalent FA models. First, we show how a non-random version of the CompFA model leads to the following hierarchy: PCA is a constrained variant of completely decomposed FA, which itself is a constrained variant of matrix decomposition FA. Then, we prove that a random version of the CompFA model leads to minimum rank FA (MRFA) and constraining MRFA leads to random PCA (RPCA), so as to present the following hierarchy: Probabilistic PCA is a constrained variant of prevalent FA, and the latter is a constrained variant of RPCA, which is itself a constrained variant of MRFA. Finally, this hierarchy and the above hierarchy following from the non-random version are unified into one. We further utilize the unified hierarchy to present a strategy for selecting a procedure suitable to a data set.},
  archive      = {J_CSTAT},
  author       = {Adachi, Kohei},
  doi          = {10.1007/s00180-025-01611-8},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3911-3946},
  shortjournal = {Comput. Stat.},
  title        = {Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-expanded ECME algorithms for logistic and penalized logistic regression. <em>CSTAT</em>, <em>40</em>(7), 3883-3909. (<a href='https://doi.org/10.1007/s00180-025-01619-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation in logistic regression is a well-studied problem with the Newton–Raphson method being one of the most prominent optimization techniques used in practice. A number of monotone optimization methods including minorization-maximization (MM) algorithms, expectation-maximization (EM) algorithms and related variational Bayes approaches offer useful alternatives guaranteed to increase the logistic regression likelihood at every iteration. In this article, we propose and evaluate an optimization procedure that is based on a straightforward modification of an EM algorithm for logistic regression. Our method can substantially improve the computational efficiency of the EM algorithm while preserving the monotonicity of EM and the simplicity of the EM parameter updates. By introducing an additional latent parameter and selecting this parameter to maximize the penalized observed-data log-likelihood at every iteration, our iterative algorithm can be interpreted as a parameter-expanded expectation-conditional maximization either (ECME) algorithm, and we demonstrate how to use the parameter-expanded ECME with an arbitrary choice of weights and penalty function. In addition, we describe a generalized version of our parameter-expanded ECME algorithm that can be tailored to the challenges encountered in specific high-dimensional problems, and we study several interesting connections between this generalized algorithm and other well-known methods. Performance comparisons between our method, the EM algorithm, Newton–Raphson, and several other optimization methods are presented using an extensive series of simulation studies based upon both real and synthetic datasets.},
  archive      = {J_CSTAT},
  author       = {Henderson, Nicholas C. and Ouyang, Zhongzhe},
  doi          = {10.1007/s00180-025-01619-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3883-3909},
  shortjournal = {Comput. Stat.},
  title        = {Parameter-expanded ECME algorithms for logistic and penalized logistic regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametric hypothesis testing methods on multiple comparisons. <em>CSTAT</em>, <em>40</em>(7), 3867-3882. (<a href='https://doi.org/10.1007/s00180-025-01615-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Bayesian testing procedures based on the Bayes factor to compare the means across multiple populations in classical nonparametric contexts. The proposed Bayesian methods are designed to maximize the probability of rejecting the null hypothesis when the Bayes factor exceeds a specified evidence threshold. It is shown that these procedures have straightforward closed-form expressions based on classical nonparametric test statistics and their corresponding critical values, allowing for easy computation. We also demonstrate that they effectively control Type I error and enable researchers to make consistent decisions aligned with both frequentist and Bayesian approaches, provided that the evidence threshold for the Bayesian methods is set according to the significance level of the frequentist tests. Importantly, the proposed approaches allow for the quantification of evidence from empirical data in favor of the null hypothesis, an advantage that frequentist methods lack, as they cannot quantify support for the null when the null hypothesis is not rejected. We also present simulation studies and real-world applications to illustrate the performance of the proposed testing procedures.},
  archive      = {J_CSTAT},
  author       = {Hai, Qiuchen and Ma, Zhuanzhuan},
  doi          = {10.1007/s00180-025-01615-4},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3867-3882},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian nonparametric hypothesis testing methods on multiple comparisons},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate. <em>CSTAT</em>, <em>40</em>(7), 3843-3866. (<a href='https://doi.org/10.1007/s00180-024-01587-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Model-X knockoffs is a practical methodology for variable selection, which stands out from other selection strategies since it allows for the control of the false discovery rate, relying on finite-sample guarantees. In this article, we propose a Truncated D-vine Copula Knockoffs (TDCK) algorithm for sampling approximate knockoffs from complex multivariate distributions. Our algorithm enhances and improves features of previous attempts to sample knockoffs under the multivariate setting, with the three main contributions being: (1) the truncation of the D-vine copula, which reduces the dependence between the original variables and their corresponding knockoffs, thus improving the statistical power; (2) the employment of a straightforward non-parametric formulation for marginal transformations, eliminating the need for a specific parametric family or a kernel density estimator; (3) the use of the “rvinecopulib” R package offers better flexibility than the existing fitting vine copula knockoff methods. To eliminate the randomness from the different sets of selected variables in distinct realizations, we wrap the TDCK method with an existing derandomizing procedure for knockoffs, leading to a Derandomized Truncated D-vine Copula Knockoffs with e-values (DTDCKe) procedure. We demonstrate the robustness of the DTDCKe procedure under various scenarios with extensive simulation studies. We further illustrate its efficacy using a gene expression dataset, showing it achieves a more reliable gene selection than other competing methods when the findings are compared with those of a meta-analysis. The results indicate that our Truncated D-vine copula approach is robust and has superior power, representing an appealing approach for variable selection in different multivariate applications, particularly in gene expression analysis.},
  archive      = {J_CSTAT},
  author       = {Vásquez, Alejandro Román and Márquez Urbina, José Ulises and González Farías, Graciela and Escarela, Gabriel},
  doi          = {10.1007/s00180-024-01587-x},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3843-3866},
  shortjournal = {Comput. Stat.},
  title        = {Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated fitting of joint models of survival and longitudinal data with cumulative variations. <em>CSTAT</em>, <em>40</em>(7), 3819-3842. (<a href='https://doi.org/10.1007/s00180-025-01639-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been well recognized that not only biomarkers but also their variability are important for predicting biomarker-related diseases. Understanding and adequately modeling the variability of biomarkers is crucial for detecting and predicting health risks, leading to improved health outcomes and patient care. However, biomarker variability modeling comes with a high computational cost, as statistical models incorporating biomarkers’ variability rely on double integrals with two nested integrations, which must be repeatedly calculated during modeling. To reduce the computational burden, we propose a novel approach aligned with arc length in mathematics to approximate and model biomarker fluctuations. Furthermore, we propose an algorithm that aligns with fast arc length evaluations for the joint modeling of survival and longitudinal data. We synthesize multiple efficient computing methods into a unified framework to accelerate the entire computational process. The core component of the acceleration is the computational efficiency of the double integrals, even when the iterated integral representation of the double integral is not possible. Finally, we illustrate the usage and benefit of our algorithm in joint models in numerical examples and the primary biliary cholangitis clinical study.},
  archive      = {J_CSTAT},
  author       = {Gao, Yan and Sparapani, Rodney A. and Tarima, Sergey},
  doi          = {10.1007/s00180-025-01639-w},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3819-3842},
  shortjournal = {Comput. Stat.},
  title        = {Accelerated fitting of joint models of survival and longitudinal data with cumulative variations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring. <em>CSTAT</em>, <em>40</em>(7), 3781-3817. (<a href='https://doi.org/10.1007/s00180-025-01612-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to estimate the reliability of a stress–strength system using the generalized inverted exponential distribution (GIED). We achieve this by employing an improved adaptive Type-II progressive censoring scheme and utilizing various estimation techniques. The techniques used include maximum likelihood estimation through the EM algorithm and Bayesian inference. We use Markov chain Monte Carlo (MCMC) methods and TK approximation in the Bayesian framework. We compute various intervals, such as asymptotic confidence, arcsin transformed, Bayesian credible, and higher posterior density confidence intervals. To guide the estimation process, we use a generalized entropy loss function. Additionally, we conduct a comprehensive simulation analysis to validate the method’s performance and rigorously assess its applicability through real-life data analysis.},
  archive      = {J_CSTAT},
  author       = {Swaroop, Chatany and Dutta, Subhankar and Saini, Shubham and Tiwari, Neeraj},
  doi          = {10.1007/s00180-025-01612-7},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3781-3817},
  shortjournal = {Comput. Stat.},
  title        = {Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive importance sampling for locally stable point processes. <em>CSTAT</em>, <em>40</em>(7), 3745-3779. (<a href='https://doi.org/10.1007/s00180-025-01609-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the expected value of a statistic of a locally stable point process in a bounded region is addressed. We propose an adaptive importance sampling for solving the problem. In our proposal, we restrict the importance point process to the family of homogeneous Poisson point processes, which enables us to generate quickly independent samples of the importance point process. The optimal intensity of the importance point process is found by applying the cross-entropy minimization method. In the proposed scheme, the expected value of the statistic and the optimal intensity are iteratively estimated in an adaptive manner. We show that the proposed estimator converges to the target value almost surely, and prove the asymptotic normality of it. We explain how to apply the proposed scheme to the estimation of the intensity of a stationary pairwise interaction point process. The performance of the proposed scheme is compared numerically with Markov chain Monte Carlo simulation and perfect sampling.},
  archive      = {J_CSTAT},
  author       = {Kang, Hee-Geon and Kim, Sunggon},
  doi          = {10.1007/s00180-025-01609-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3745-3779},
  shortjournal = {Comput. Stat.},
  title        = {An adaptive importance sampling for locally stable point processes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts. <em>CSTAT</em>, <em>40</em>(7), 3715-3743. (<a href='https://doi.org/10.1007/s00180-025-01602-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fully robust one-sided cross-validation (OSCV) method has versions in the nonparametric regression and density estimation settings. It selects the consistent bandwidths for estimating the continuous regression and density functions that might have finitely many discontinuities in their first derivatives. The theoretical results underlying the method were thoroughly elaborated in the preceding publications, while its practical implementations needed improvement. In particular, until this publication, no appropriate implementation of the method existed in the density estimation context. In the regression setting, the previously proposed implementation has a serious disadvantage of occasionally producing the irregular OSCV functions that complicates the bandwidth selection procedure. In this article, we make a substantial progress towards resolving the aforementioned issues by proposing a suitable implementation of fully robust OSCV for density estimation and providing specific recommendations for the further improvement of the method in the regression setting.},
  archive      = {J_CSTAT},
  author       = {Savchuk, Olga},
  doi          = {10.1007/s00180-025-01602-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3715-3743},
  shortjournal = {Comput. Stat.},
  title        = {On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function. <em>CSTAT</em>, <em>40</em>(7), 3665-3713. (<a href='https://doi.org/10.1007/s00180-024-01581-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acceptance sampling plans (ASPs) for attributes are sometimes misapplied to normal quality characteristics. When inspection costs and quality levels are high, using variable ASPs (VASPs) can be preferable. Among developed approaches to design ASPs, few studies have incorporated losses into the cost objective function. Their limited attention, such as focusing on limited random scenarios, considering only the activation of one specification limit, failing to compare VASPs with military standards still in use, and relying on time-consuming solution procedures, motivated us to utilize the advantages of loss-based economic-statistical design, evaluate four VASPs and two military standards, and presenting detailed results. Additionally, we develop the first Particle swarm optimization (PSO)-based solution procedure for designing VASPs. Numerical and real case studies, which consider the activation of lower and upper specification limits, demonstrate the superior performance of (1) the repetitive group sampling plan, (2) MIL-STD-414 over MIL-STD-105E, and (3) PSO compared to other approaches.},
  archive      = {J_CSTAT},
  author       = {Jafarian-Namin, Samrad and Fattahi, Parviz and Salmasnia, Ali},
  doi          = {10.1007/s00180-024-01581-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3665-3713},
  shortjournal = {Comput. Stat.},
  title        = {Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-based smoothing parameter estimation for neural P-splines. <em>CSTAT</em>, <em>40</em>(7), 3645-3663. (<a href='https://doi.org/10.1007/s00180-024-01593-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the popularity of deep learning models there have recently been many attempts to translate generalized additive models to neural nets. Generalized additive models are usually regularized by a penalty in the loss function and the magnitude of penalization is controlled by one or more smoothing parameters. In the statistical literature these smoothing parameters are estimated by criteria such as generalized cross-validation or restricted maximum likelihood. While the estimation of the primary regression coefficients is well calibrated and investigated for neural net based additive models, the estimation of smoothing parameters is often either based on testing data (and grid search), implicitly estimated or completely neglected. In this paper, we address the issue of explicit smoothing parameter estimation in neural net-based additive models fitted via gradient-based methods, such as the well-known Adam algorithm. We therefore investigate the data-driven smoothing parameter selection via gradient-based optimization of generalized cross-validation and restricted maximum likelihood. Thus we do not need to calculate Hessian information of the smoothing parameters. As an additive model structure, we use a translation of P-splines to neural nets, so-called neural P-splines. The fitting process of neural P-splines as well as the gradient-based smoothing parameter selection are investigated in a simulation study and an application.},
  archive      = {J_CSTAT},
  author       = {Dammann, Lea M. and Freitag, Marei and Thielmann, Anton and Säfken, Benjamin},
  doi          = {10.1007/s00180-024-01593-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3645-3663},
  shortjournal = {Comput. Stat.},
  title        = {Gradient-based smoothing parameter estimation for neural P-splines},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images. <em>CSTAT</em>, <em>40</em>(7), 3613-3643. (<a href='https://doi.org/10.1007/s00180-025-01650-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery is frequently subject to degradation by noise during both image acquisition and transmission processes. The primary goal of noise reduction techniques is to remove Speckle noise while retaining critical features of the images. In remote sensing applications, Synthetic Aperture Radar (SAR) imagery plays a vital role. Speckle, a granular disturbance typically modelled as multiplicative noise, impacts SAR images as well as all coherent images, resulting in a reduction in image quality. Over the past three decades, numerous techniques have been proposed to mitigate Speckle noise in SAR imagery. This study proposes the Kernel-Diffeomorphism Bayesian Bootstrap Filter (KDBBF) as a novel method for satellite image restoration. The method relies on the multivariate Kernel Diffeomorphism estimator and the Bayesian Bootstrap Filter (BBF). Comparative analyses of the results produced by the new method with those of other image restoration techniques reveal superior performance in Speckle noise reduction in SAR imagery, both quantitatively and qualitatively.},
  archive      = {J_CSTAT},
  author       = {Zribi, Mourad and Sadok, Ibrahim and Marhaba, Bassel},
  doi          = {10.1007/s00180-025-01650-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3613-3643},
  shortjournal = {Comput. Stat.},
  title        = {Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting. <em>CSTAT</em>, <em>40</em>(7), 3585-3611. (<a href='https://doi.org/10.1007/s00180-025-01628-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of selecting non-zero partial correlations under the assumption of normality. It is cumbersome to compute variable selection criteria for all subsets of variable pairs when the number of variables is large, even if it is smaller than the sample size. To tackle this problem, we propose a fast and consistent variable selection method based on Bayesian information criterion (BIC). The consistency of the method is provided in a high-dimensional asymptotic framework such that the sample size and the number of variables both tend toward infinity under a certain rule. Through numerical simulations, it is shown that the proposed method has a high probability of selecting the true subset of pairs of non-zero partial correlation.},
  archive      = {J_CSTAT},
  author       = {Yamada, Takayuki and Sakurai, Tetsuro and Fujikoshi, Yasunori},
  doi          = {10.1007/s00180-025-01628-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3585-3611},
  shortjournal = {Comput. Stat.},
  title        = {Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning based regulatory risk index for cryptocurrencies. <em>CSTAT</em>, <em>40</em>(7), 3563-3583. (<a href='https://doi.org/10.1007/s00180-025-01629-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency markets are highly sensitive to regulatory changes, often experiencing sharp price fluctuations in response to new policies and government interventions. Despite this, existing market indices fail to adequately capture the risks associated with regulatory uncertainty. In this paper, we introduce the Cryptocurrency Regulatory Risk Index (CRRIX), a machine learning-based index designed to quantify the impact of regulatory developments on cryptocurrency markets. Our methodology employs Latent Dirichlet Allocation (LDA) to classify policy-related news articles from major cryptocurrency news platforms, providing an objective measure of regulatory risk. We find that the CRRIX exhibits strong synchronicity with VCRIX, a cryptocurrency volatility index, suggesting that regulatory uncertainty plays a significant role in driving market fluctuations. Our results indicate that regulatory risk is a leading factor in market volatility, with major policy shifts triggering significant market movements. The proposed regulatory risk index provides a novel approach to quantifying policy uncertainty in the cryptocurrency sector, offering valuable insights for market participants navigating this rapidly changing environment.},
  archive      = {J_CSTAT},
  author       = {Ni, Xinwen and Xie, Taojun and Härdle, Wolfgang Karl and Zuo, Xiaorui},
  doi          = {10.1007/s00180-025-01629-y},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3563-3583},
  shortjournal = {Comput. Stat.},
  title        = {A machine learning based regulatory risk index for cryptocurrencies},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models. <em>CSTAT</em>, <em>40</em>(7), 3527-3561. (<a href='https://doi.org/10.1007/s00180-025-01642-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a coordinate gradient descent (CGD) algorithm, based on the work of Tseng and Yun (Math Program 117:387–423; 2009a; J Optim Theory Appl 140(3):513–535, 2009b), to optimize the constrained negative quasi maximum likelihood with adaptive LASSO penalization for pure autoregressive conditional heteroscedasticity (ARCH) model and its generalized form (GARCH). The strategy for choosing the appropriate values of the shrinkage parameter through information criteria (IC) is also discussed. We evaluate the numerical efficiency of the proposed algorithm through simulated data. Results of simulation studies show that for moderate sample sizes, the adaptive LASSO with the Bayesian variant of IC correctly estimates the ARCH structure at a high rate, even when model orders are over-specified. On the other hand, the adaptive LASSO has a low rate of correctly estimating true GARCH structure, especially when the model orders are over-specified regardless of the choice of IC. In our case study using daily ASX Ordinary log returns, the adaptive LASSO yields sparser ARCH and GARCH models while maintaining adequate fit for the volatility.},
  archive      = {J_CSTAT},
  author       = {Nasir, Muhammad Jaffri Mohd and Khan, Ramzan Nazim and Nair, Gopalan and Nur, Darfiana},
  doi          = {10.1007/s00180-025-01642-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3527-3561},
  shortjournal = {Comput. Stat.},
  title        = {Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newton-based variant of exclusive lasso for improved sparse solutions. <em>CSTAT</em>, <em>40</em>(7), 3505-3525. (<a href='https://doi.org/10.1007/s00180-025-01630-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exclusive Lasso offers significant advantages in scenarios that require sparse solutions within groups, such as multi-omics or gene expression analysis. These applications involve inherent grouping structures where selecting only a subset of variables from each group is crucial due to high correlations among variables within groups. However, a key challenge in optimizing Exclusive Lasso stems from the non-differentiability of the $$L_{1}$$ -norm within each group. To tackle this issue, we propose a method to transform this norm into a differentiable form using quadratic and sigmoid function approximations. This transformation facilitates the use of a straightforward Newton-based approach to solve the intricate optimization problem. Importantly, our proposed variant of Exclusive Lasso relaxes the strict requirement of selecting at least one variable per group, in contrast to the conventional Exclusive Lasso, and hence enables sparser solutions. Extensive simulation studies underscore the superior performance of our approach compared to both traditional Lasso methods and conventional Exclusive Lasso formulations.},
  archive      = {J_CSTAT},
  author       = {Ravi, Dayasri and Groll, Andreas},
  doi          = {10.1007/s00180-025-01630-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3505-3525},
  shortjournal = {Comput. Stat.},
  title        = {A newton-based variant of exclusive lasso for improved sparse solutions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing non-inferiority for three-arm trials under the PH model. <em>CSTAT</em>, <em>40</em>(7), 3477-3503. (<a href='https://doi.org/10.1007/s00180-025-01624-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of non-inferiority (NI) trials is to show that a new treatment is not worse than a reference treatment by more than a pre-specified margin. For ethical reasons, NI trials usually do not include a placebo arm such that neither the assay sensitivity nor the constancy can be validated. On the other hand, three-arm NI trials consisting of the new treatment, reference treatment, and placebo, can simultaneously test the superiority of the new treatment over placebo and the NI of the new treatment compared with the reference treatment. In this article, we consider assessing NI of a new treatment in three-arm trials with time to event outcomes subject to right censoring. Under the proportional hazards model, we develop a testing procedure for assessing NI based on the infimum of ratio of survival difference between the new treatment and the placebo to that between the reference treatment and the placebo within a specific time period. The proposed test statistics involves the estimates of treatment parameters and survival function evaluated at a specific time point and their corresponding standard error estimates. Simulation study indicates that the proposed test controls the type I error well and has decent power to detect the NI under moderate to large sample settings.},
  archive      = {J_CSTAT},
  author       = {Shen, Pao-sheng},
  doi          = {10.1007/s00180-025-01624-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3477-3503},
  shortjournal = {Comput. Stat.},
  title        = {Testing non-inferiority for three-arm trials under the PH model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors. <em>CSTAT</em>, <em>40</em>(7), 3453-3475. (<a href='https://doi.org/10.1007/s00180-025-01617-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing Bayesian analysis methods of time series with seasonal pattern are based on the normality assumption; however, most of the real time series violate this assumption. With assuming the scale-mixtures of normal (SMN) distribution for the model errors, we introduce the Bayesian estimation and prediction of seasonal autoregressive (SAR) models, using the Gibbs sampler and Metropolis-Hastings algorithms. The SMN distribution is a general class that includes different symmetric heavy-tailed distributions as special cases, such as the Student’s t, slash and contaminated normal distributions. With employing different priors for the SAR parameters, we derive the full conditional posterior distributions of the SAR coefficients and scale parameter to be the multivariate normal and inverse gamma, respectively, and the conditional predictive distribution of the future observations to be the multivariate normal. For the other parameters related to the SMN distribution, we derive their conditional posteriors to be in a closed form but some of them are not standard distributions. Using the derived closed-form conditional posterior and predictive distributions, we propose the Gibbs sampler with the Metropolis-Hastings algorithm to approximate empirically the marginal posterior and predictive distributions. We introduce an extensive simulation study and a real application in order to evaluate the accuracy of the proposed MCMC algorithm.},
  archive      = {J_CSTAT},
  author       = {Amin, Ayman A.},
  doi          = {10.1007/s00180-025-01617-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3453-3475},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bayesian inference in a model for self-generated gradient collective cell movement. <em>CSTAT</em>, <em>40</em>(7), 3399-3452. (<a href='https://doi.org/10.1007/s00180-025-01606-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we explore parameter inference in a novel hybrid discrete-continuum model describing the movement of a population of cells in response to a self-generated chemotactic gradient. The model employs a drift-diffusion stochastic process, rendering likelihood-based inference methods impractical. Consequently, we consider approximate Bayesian computation (ABC) methods, which have gained popularity for models with intractable or computationally expensive likelihoods. ABC involves simulating from the generative model, using parameters from generated observations that are “close enough” to the true data to approximate the posterior distribution. Given the plethora of existing ABC methods, selecting the most suitable one for a specific problem can be challenging. To address this, we employ a simple drift-diffusion stochastic differential equation (SDE) as a benchmark problem. This allows us to assess the accuracy of popular ABC algorithms under known configurations. We also evaluate the bias between ABC-posteriors and the exact posterior for the basic SDE model, where the posterior distribution is tractable. The top-performing ABC algorithms are subsequently applied to the proposed cell movement model to infer its key parameters. This study not only contributes to understanding cell movement but also sheds light on the comparative efficiency of different ABC algorithms in a well-defined context.},
  archive      = {J_CSTAT},
  author       = {Devlin, Jon and Borowska, Agnieszka and Husmeier, Dirk and Mackenzie, John},
  doi          = {10.1007/s00180-025-01606-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3399-3452},
  shortjournal = {Comput. Stat.},
  title        = {Approximate bayesian inference in a model for self-generated gradient collective cell movement},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informative right censoring in nonparametric survival models. <em>CSTAT</em>, <em>40</em>(7), 3385-3397. (<a href='https://doi.org/10.1007/s00180-025-01610-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis models allow us to analyze and predict the time until a certain event occurs. Existing nonparametric models assume that the censoring of observations is random and unrelated to the study conditions. The estimators of the survival and hazard functions assume a constant survival probability between modes, have poor interpretability for datasets with multimodal time distributions, and lead to poor-quality data descriptions. In this paper, we investigate the quality of nonparametric models on four medical datasets with informative censoring and multimodal time distribution and propose a modification to improve the description quality. Proved properties of IBS and AUPRC metrics show that the best quality is achieved at survival function with unimodal time distribution. We propose modifying the nonparametric model based on virtual events from a truncated normal distribution that allows for the suppression of informative censoring. We compared the quality of the nonparametric models on multiple random subsets of datasets of different sizes using the AUPRC and IBS metrics. According to the comparison of the quality using Welch’s test, the proposed model with virtual events significantly outperformed the existing Kaplan–Meier model for all datasets (p-value $$<10^{-6}$$ ). The quality increase of IBS is from 6.91 to 21.92%, and the quality increase of AUPRC is from 12.92 to 18.4%. The nonparametric models with virtual events provide a better interpretation, allow a better description of the observed data, and are stable in terms of the informativeness of censoring. The proposed method is embedded in an open-source survivors Python library.},
  archive      = {J_CSTAT},
  author       = {Vasilev, Iulii and Petrovskiy, Mikhail and Mashechkin, Igor},
  doi          = {10.1007/s00180-025-01610-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3385-3397},
  shortjournal = {Comput. Stat.},
  title        = {Informative right censoring in nonparametric survival models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates. <em>CSTAT</em>, <em>40</em>(7), 3359-3384. (<a href='https://doi.org/10.1007/s00180-024-01573-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a functional partial linear single index model for strong $$\alpha$$ -mixing functional time series data when the responses are missing not at random and the real covariates are observed with measurement errors. We first extend three insertion methods developed for regression models with missing responses in finite dimensions, namely imputation, semiparametric regression surrogate and inverse marginal probability weighted approaches, to functional scenarios when the responses are scalar. Then the attenuation correction method is employed to eliminate the impact of measurement error on model estimation of unknown parameter in linear component, after we completing the missingness of responses by using above insertion methods. Meanwhile, we combine the kNN approach with insertion and attenuation correction approaches to capture the local structure of functional time series data and provide the estimations of unknown operators in the estimation process. The asymptotic properties of unknown parameters in the model are established under some mild assumptions. Furthermore, we make a comparison of three insertion methods, the oracle method and the ignoring method in simulation study and electricity consumption data analysis. All results indicate that our methodology has good performance.},
  archive      = {J_CSTAT},
  author       = {Meng, Shuyu and Huang, Zhensheng and Ling, Nengxiang},
  doi          = {10.1007/s00180-024-01573-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3359-3384},
  shortjournal = {Comput. Stat.},
  title        = {KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
