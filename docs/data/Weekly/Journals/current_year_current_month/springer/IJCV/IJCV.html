<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv">IJCV - 40</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Investigating self-supervised methods for label-efficient learning. <em>IJCV</em>, <em>133</em>(9), 6638. (<a href='https://doi.org/10.1007/s11263-025-02455-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Nandam, Srinivasa Rao and Atito, Sara and Feng, Zhenhua and Kittler, Josef and Awais, Muhammad},
  doi          = {10.1007/s11263-025-02455-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6638},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Investigating self-supervised methods for label-efficient learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation. <em>IJCV</em>, <em>133</em>(9), 6637. (<a href='https://doi.org/10.1007/s11263-025-02456-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Duan, Haoran and Shao, Shuai and Zhai, Bing and Shah, Tejal and Han, Jungong and Ranjan, Rajiv},
  doi          = {10.1007/s11263-025-02456-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6637},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on DAGM GCPR 2023. <em>IJCV</em>, <em>133</em>(9), 6636. (<a href='https://doi.org/10.1007/s11263-025-02490-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-025-02490-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6636},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on DAGM GCPR 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning. <em>IJCV</em>, <em>133</em>(9), 6611-6635. (<a href='https://doi.org/10.1007/s11263-025-02507-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is a crucial technique in the field of computer vision, and its goal is to generate high-quality fused images and improve the performance of downstream tasks. However, existing fusion methods struggle to balance these two factors. Achieving high quality in fused images may result in lower performance in downstream visual tasks, and vice versa. To address this drawback, a novel LVM (large vision model)-guided fusion framework with Object-aware and Contextual COntrastive learning is proposed, termed as OCCO. The pre-trained LVM is utilized to provide semantic guidance, allowing the network to focus solely on fusion tasks while emphasizing learning salient semantic features in form of contrastive learning. Additionally, a novel feature interaction fusion network is also designed to resolve information conflicts in fusion images caused by modality differences. By learning the distinction between positive samples and negative samples in the latent feature space (contextual space), the integrity of target information in fused image is improved, thereby benefiting downstream performance. Finally, compared with eight state-of-the-art methods on four datasets, the effectiveness of the proposed method is validated, and exceptional performance is also demonstrated on downstream visual task.},
  archive      = {J_IJCV},
  author       = {Li, Hui and Bian, Congcong and Zhang, Zeyang and Song, Xiaoning and Li, Xi and Wu, Xiao-Jun},
  doi          = {10.1007/s11263-025-02507-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6611-6635},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance. <em>IJCV</em>, <em>133</em>(9), 6590-6610. (<a href='https://doi.org/10.1007/s11263-025-02466-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) super-resolution (SR) aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment (TSIA) with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation (FA) module and an Attention Fusion (AF) module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation (IDFA) block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations. Our code is publicly available to the community ( https://github.com/BITYKZhang/SSC-HSR ).},
  archive      = {J_IJCV},
  author       = {Zhang, Yingkai and Lai, Zeqiang and Zhang, Tao and Fu, Ying and Zhou, Chenghu},
  doi          = {10.1007/s11263-025-02466-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6590-6610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos. <em>IJCV</em>, <em>133</em>(9), 6578-6589. (<a href='https://doi.org/10.1007/s11263-025-02493-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using unmanned aerial vehicles (UAVs) to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding the collective behavior of primates. Previous studies have demonstrated the feasibility of automating primate behavior classification from video data, but these studies have been carried out in captivity or from ground-based cameras. However, to understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. To tackle this challenge, this study presents a novel dataset for baboon detection, tracking, and behavior recognition from drone videos where troops are observed on-the-move in their natural environment as they move to and from their sleeping sites. Videos were captured from drones at Mpala Research Centre, a research station located in Laikipia County, in central Kenya. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The baboon tracking dataset is derived from the baboon detection dataset, where bounding boxes are consistently assigned the same ID throughout the video. This process resulted in half an hour of dense tracking data. The baboon behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal. These mini-scenes were annotated with 12 distinct behavior types and one additional category for occlusion, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62% for the YOLOv8-X detection model, multiple object tracking precision (MOTP) of 87.22% for the DeepSORT tracking algorithm, and micro top-1 accuracy of 64.89% for the X3D behavior recognition model. Using deep learning to rapidly and accurately classify wildlife behavior from drone footage facilitates non-invasive data collection on behavior enabling the behavior of a whole group to be systematically and accurately recorded. The dataset can be accessed at https://baboonland.xyz .},
  archive      = {J_IJCV},
  author       = {Duporge, Isla and Kholiavchenko, Maksim and Harel, Roi and Wolf, Scott and Rubenstein, Daniel I and Crofoot, Margaret C and Berger-Wolf, Tanya and Lee, Stephen J and Barreau, Julie and Kline, Jenna and Ramirez, Michelle and Stewart, Charles V},
  doi          = {10.1007/s11263-025-02493-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6578-6589},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised salient object detection with oversize bounding box. <em>IJCV</em>, <em>133</em>(9), 6558-6577. (<a href='https://doi.org/10.1007/s11263-025-02482-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to laborious pixel-level annotations, scribbles, bounding boxes, and points are much more efficient in salient object detection (SOD). However, the annotation cost of these forms linearly increases with the number of salient objects in the image, which is not ideal for real-world scenarios. To address this issue, we propose a novel annotation form called oversize bounding box (OBB), i.e., a box that encompasses all salient objects without the need for tight enclosure. It has two characteristics: (1) All pixels outside the box are from the background. (2) A subset of the pixels inside the box belongs to salient objects. Therefore, the core issue is how to highlight integral and accurate object regions as pseudo-labels. Inspired by the powerful visual understanding and vision-language correlations demonstrated by large multimodal models, we devise the first multimodal model-based pseudo-label generation method for SOD. It utilizes MiniGPT-4 to generate descriptions of salient objects as text prompts for CLIP, combined with the CAM-based technique and proposed refinement algorithm based on the multi-head self-attention and superpixel to activate the object regions. Then we use OBB to further correct the activation regions based on its two characteristics. Given the potential errors of pseudo-labels within the box, we propose a center-pixel-based cross entropy loss, as activated pixels closer to the center are generally more reliable. Moreover, we establish an OBB-supervised training dataset by relabeling the DUTS dataset. Extensive experiments on six benchmarks demonstrate that our method achieves the state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Wu, Zhihao and Xu, Yong and Yang, Jian and Zhang, David},
  doi          = {10.1007/s11263-025-02482-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6558-6577},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weakly supervised salient object detection with oversize bounding box},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit diffusion models for continuous super-resolution. <em>IJCV</em>, <em>133</em>(9), 6535-6557. (<a href='https://doi.org/10.1007/s11263-025-02462-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) has attracted increasing attention due to its widespread applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most of them work only with fixed magnifications. To address these problems, this paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Moreover, we design a scale-adaptive conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The LR conditioning network adopts a parallel architecture to provide multi-resolution LR conditions for the denoising model. The scaling factor further regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Furthermore, we accelerate the inference process by adjusting the denoising equation and employing post-training quantization to compress the learned denoising network in a training-free manner. Extensive experiments on six benchmark datasets validate the effectiveness of our IDM and demonstrate its superior performance over prior arts. The source code is available at https://github.com/Ree1s/IDM .},
  archive      = {J_IJCV},
  author       = {Liu, Xuhui and Gao, Sicheng and Zeng, Bohan and Zhang, Luping and Wang, Tian and Liu, Jianzhuang and Zhang, Baochang},
  doi          = {10.1007/s11263-025-02462-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6535-6557},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Implicit diffusion models for continuous super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation. <em>IJCV</em>, <em>133</em>(9), 6513-6534. (<a href='https://doi.org/10.1007/s11263-025-02497-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from source domain to target domain, which always struggles with severe domain shift between the data. The recent progress on visual-language models (VLMs) have provided a promising way to address UDA, leveraging the knowledge from text for more guided adaptation. However, directly deploying such models on downstream UDA tasks with conventional prompt learning can be challenging, which neglects the diversity of visual samples and can cause mis-alignment between modalities, thus lacking flexibility to adapt both modalities dynamically and limiting the cross-domain knowledge transfer. In this paper, we propose an innovative domain-invariant prompt learning method to align the prompts from different modalities. Specifically, we first introduce a hybrid-modality guided prompting module that leverages the prompted multi-modal representation to synergistically help uni-modal learning, thus mutually aligning visual and textual embeddings. We also take advantage of the the category-wise attributes derived from Large Language Model (LLM) to incorporate fine-grained semantic knowledge into prompt learning, ensuring better discrimination among different classes. Besides, to further minimize domain discrepancy, we propose to fuse the textual prototypes with the visual prototypes from each domain, thus to make the input attend to overall domain distribution, which effectively integrates self-enhanced and cross-domain features into the model prediction. With our framework, the two modalities can be mutually promoted to better enhance the adaptation of VLMs for UDA. Experiments on several different benchmarks demonstrate the superiority of our method over previous approaches.},
  archive      = {J_IJCV},
  author       = {Xing, Bowei and Ying, Xianghua and Wang, Ruibin and Guo, Ruohao},
  doi          = {10.1007/s11263-025-02497-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6513-6534},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ricci curvature tensor-based volumetric segmentation. <em>IJCV</em>, <em>133</em>(9), 6491-6512. (<a href='https://doi.org/10.1007/s11263-025-02492-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing level set models employ regularization based only on gradient information, 1D curvature or 2D curvature. For 3D image segmentation, however, an appropriate curvature-based regularization should involve a well-defined 3D curvature energy. This is the first paper to introduce a regularization energy that incorporates 3D scalar curvature for 3D image segmentation, inspired by the Einstein-Hilbert functional. To derive its Euler-Lagrange equation, we employ a two-step gradient descent strategy, alternately updating the level set function and its gradient. The paper also establishes the existence and uniqueness of the viscosity solution for the proposed model. Experimental results demonstrate that our proposed model outperforms other state-of-the-art models in 3D image segmentation.},
  archive      = {J_IJCV},
  author       = {Huang, Jisui and Chen, Ke and Alpers, Andreas and Lei, Na},
  doi          = {10.1007/s11263-025-02492-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6491-6512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Ricci curvature tensor-based volumetric segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion. <em>IJCV</em>, <em>133</em>(9), 6473-6490. (<a href='https://doi.org/10.1007/s11263-025-02496-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN has excelled in 2D face reconstruction and semantic editing, but the extension to 3D lacks a generic inversion framework, limiting its applications in 3D reconstruction. In this paper, we address the challenge of 3D GAN inversion, focusing on predicting a latent code from a single 2D image to faithfully recover 3D shapes and textures. The inherent ill-posed nature of the problem, coupled with the limited capacity of global latent codes, presents significant challenges. To overcome these challenges, we introduce an efficient self-training scheme that does not rely on real-world 2D-3D pairs but instead utilizes proxy samples generated from a 3D GAN. Additionally, our approach goes beyond the global latent code by enhancing the generation network with a local branch. This branch incorporates pixel-aligned features to accurately reconstruct texture details. Furthermore, we introduce a novel pipeline for 3D view-consistent editing. The efficacy of our method is validated on two representative 3D GANs, namely StyleSDF and EG3D. Through extensive experiments, we demonstrate that our approach consistently outperforms state-of-the-art inversion methods, delivering superior quality in both shape and texture reconstruction.},
  archive      = {J_IJCV},
  author       = {Lan, Yushi and Meng, Xuyi and Yang, Shuai and Loy, Chen Change and Dai, Bo},
  doi          = {10.1007/s11263-025-02496-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6473-6490},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {$$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models. <em>IJCV</em>, <em>133</em>(9), 6456-6472. (<a href='https://doi.org/10.1007/s11263-025-02495-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers’ emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers’ emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs’ reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers’ emotional responses to videos and providing coherent and insightful explanations.},
  archive      = {J_IJCV},
  author       = {Guo, Yuxiang and Siddiqui, Faizan and Zhao, Yang and Chellappa, Rama and Lo, Shao-Yuan},
  doi          = {10.1007/s11263-025-02495-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6456-6472},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EventEgo3D++: 3D human motion capture from a head-mounted event camera. <em>IJCV</em>, <em>133</em>(9), 6432-6455. (<a href='https://doi.org/10.1007/s11263-025-02489-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de .},
  archive      = {J_IJCV},
  author       = {Millerdurai, Christen and Akada, Hiroyasu and Wang, Jian and Luvizon, Diogo and Pagani, Alain and Stricker, Didier and Theobalt, Christian and Golyanik, Vladislav},
  doi          = {10.1007/s11263-025-02489-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6432-6455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EventEgo3D++: 3D human motion capture from a head-mounted event camera},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast sampling through the reuse of attention maps in diffusion models. <em>IJCV</em>, <em>133</em>(9), 6422-6431. (<a href='https://doi.org/10.1007/s11263-025-02463-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image diffusion models have demonstrated unprecedented capabilities for flexible and realistic image synthesis. Nevertheless, these models rely on a time-consuming sampling procedure, which has motivated attempts to reduce their latency. When improving efficiency, researchers often use the original diffusion model to train an additional network designed specifically for fast image generation. In contrast, our approach seeks to reduce latency directly, without any retraining, fine-tuning, or knowledge distillation. In particular, we find the repeated calculation of attention maps to be costly yet redundant, and instead suggest reusing them during sampling. Our specific reuse strategies are based on ODE theory, which implies that the later a map is reused, the smaller the distortion in the final image. We empirically compare our reuse strategies with few-step sampling procedures of comparable latency, finding that reuse generates images that are closer to those produced by the original high-latency diffusion model.},
  archive      = {J_IJCV},
  author       = {Hunter, Rosco and Dudziak, Łukasz and Abdelfattah, Mohamed S. and Mehrotra, Abhinav and Bhattacharya, Sourav and Wen, Hongkai},
  doi          = {10.1007/s11263-025-02463-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6422-6431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast sampling through the reuse of attention maps in diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RML++: Regroup median loss for combating label noise. <em>IJCV</em>, <em>133</em>(9), 6400-6421. (<a href='https://doi.org/10.1007/s11263-025-02494-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) typically necessitates large-scale, high-quality annotated datasets. However, due to the inherent challenges of precisely annotating vast numbers of training samples, label noise—characterized by potentially erroneous annotations—is common yet detrimental in practice. Currently, to combat the negative impacts of label noise, mainstream studies follow a pipeline that begins with data sampling and is followed by loss correction. Data sampling aims to partition the original training dataset into clean and noisy subsets, but it often suffers from biased sampling that can mislead models. Additionally, loss correction typically requires knowledge of the noise rate as a priori information, of which the precise estimation can be challenging. To this end, we propose a novel method, Regroup Median Loss Plus Plus (RML++), that addresses both of the previous drawbacks. Specifically, the training dataset is partitioned into clean and noisy subsets using a newly designed separation approach, which synergistically combines prediction consistency with an adaptive threshold to ensure a reliable sampling. Moreover, to ensure the noisy subsets can be robustly learned by models, we suggest to estimate the losses of noisy training samples by utilizing the same-class samples from the clean subset. Subsequently, the proposed method corrects the labels of noisy samples based on the model predictions with the regularization of RML++. Compared to state-of-the-art (SOTA) methods, RML++ achieves significant improvements on both synthetic and challenging real-world datasets. The source code is available at https://github.com/Feng-peng-Li/RML-Extension .},
  archive      = {J_IJCV},
  author       = {Li, Fengpeng and Li, Kemou and Wang, Qizhou and Han, Bo and Tian, Jinyu and Zhou, Jiantao},
  doi          = {10.1007/s11263-025-02494-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6400-6421},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RML++: Regroup median loss for combating label noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed deep learning deformable medical image registration method based on neural ODEs. <em>IJCV</em>, <em>133</em>(9), 6374-6399. (<a href='https://doi.org/10.1007/s11263-025-02476-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An unsupervised machine learning method is introduced to align medical images in the context of the large deformation elasticity coupled with growth and remodeling biophysics. The technique, which stems from the principle of minimum potential energy in solid mechanics, consists of two steps: Firstly, in the predictor step, the geometric registration is achieved by minimizing a loss function composed of a dissimilarity measure and a regularizing term. Secondly, the physics of the problem, including the equilibrium equations along with growth mechanics, are enforced in a corrector step by minimizing the potential energy corresponding to a Dirichlet problem, where the predictor solution defines the boundary condition and is maintained by distance functions. The features of the new solution procedure, as well as the nature of the registration problem, are highlighted by considering several examples. In particular, registration problems containing large non-uniform deformations caused by extension, shearing, and bending of multiply-connected regions are used as benchmarks. In addition, we analyzed a benchmark biological example (registration for brain data) to showcase that the new deep learning method competes with available methods in the literature. We then applied the method to various datasets. First, we analyze the regrowth of the zebrafish embryonic fin from confocal imaging data. Next, we evaluate the quality of the solution procedure for two examples related to the brain. For one, we apply the new method for 3D image registration of longitudinal magnetic resonance images of the brain to assess cerebral atrophy, where a first-order ODE describes the volume loss mechanism. For the other, we explore cortical expansion during early fetal brain development by coupling the elastic deformation with morphogenetic growth dynamics. The method and examples show the ability of our framework to attain high-quality registration and, concurrently, solve large deformation elasticity balance equations and growth and remodeling dynamics.},
  archive      = {J_IJCV},
  author       = {Amiri-Hezaveh, Amirhossein and Tan, Shelly and Deng, Qing and Umulis, David and Cunniff, Lauren and Weickenmeier, Johannes and Buganza Tepole, Adrian},
  doi          = {10.1007/s11263-025-02476-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6374-6399},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A physics-informed deep learning deformable medical image registration method based on neural ODEs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. <em>IJCV</em>, <em>133</em>(9), 6362-6373. (<a href='https://doi.org/10.1007/s11263-025-02481-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Neural Radiance Fields (NeRF) have been successful in high-fidelity 3D modeling of talking portraits. However, slow training and inference speed have obstructed their potential usage. This paper proposes an efficient NeRF-based framework, which enables faster convergence and real-time synthesizing of stable talking portraits, by utilizing the recent success of grid-based NeRF. This is accomplished by decomposing the inherently high-dimensional talking portrait representation into three low-dimensional feature grids. Specifically, a Decomposed Audio-Spatial Encoding Module models the dynamic head with a 3D spatial grid and a 2D audio grid, where audio dynamics are modeled in a spatial-dependent manner to avoid undesirable flickering. The torso is handled with another 2D grid in a lightweight Pseudo-3D Deformable Module. Extensive experiments demonstrate that our method can generate realistic and audio-lips synchronized talking portrait videos, while also being highly efficient. Our project page is available at https://me.kiui.moe/radnerf/ .},
  archive      = {J_IJCV},
  author       = {Tang, Jiaxiang and Wang, Kaisiyuan and Zhou, Hang and Chen, Xiaokang and He, Dongliang and Hu, Tianshu and Liu, Jingtuo and Liu, Ziwei and Zeng, Gang and Wang, Jingdong},
  doi          = {10.1007/s11263-025-02481-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6362-6373},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Real-time neural radiance talking portrait synthesis via audio-spatial decomposition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skim then focus: Integrating contextual and fine-grained views for repetitive action counting. <em>IJCV</em>, <em>133</em>(9), 6347-6361. (<a href='https://doi.org/10.1007/s11263-025-02471-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to action counting is accurately locating each video’s repetitive actions. Instead of estimating the probability of each frame belonging to an action directly, we propose a dual-branch network, i.e., SkimFocusNet, working in a two-step manner. The model draws inspiration from empirical observations indicating that humans initially engage in coarse skimming of entire sequences to quickly locate potential target action frames and grasp general motion patterns. This is followed by finer, frame-by-frame focusing to precisely determine whether the located frames align with the target actions. Specifically, SkimFocusNet incorporates a skim branch and a focus branch. The skim branch scans the global contextual information throughout the sequence to identify potential target action for guidance. Subsequently, the focus branch utilizes the guidance to diligently identify repetitive actions using a long-short adaptive guidance (LSAG) block. Additionally, we have observed that videos in existing datasets often feature only one type of repetitive action, which inadequately represents real-world scenarios. To more accurately describe real-life situations, we establish the Multi-RepCount dataset, which includes videos containing multiple repetitive motions. On Multi-RepCount, our SkimFoucsNet can perform specified action counting, that is, to enable counting a particular action type by referencing an exemplary video. This capability substantially exhibits our method’s robustness, particularly in accurately performing action counting despite the presence of interfering actions. Extensive experiments demonstrate that SkimFocusNet achieves state-of-the-art performances with significant improvements. We also conduct a thorough ablation study to evaluate the network components. The source code will be published upon acceptance https://github.com/isotopezzq/SkimFocusNet .},
  archive      = {J_IJCV},
  author       = {Zhao, Zhengqi and Huang, Xiaohu and Zhou, Hao and Yao, Kun and Ding, Errui and Wang, Jingdong and Wang, Xinggang and Liu, Wenyu and Bin, Feng},
  doi          = {10.1007/s11263-025-02471-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6347-6361},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Skim then focus: Integrating contextual and fine-grained views for repetitive action counting},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image. <em>IJCV</em>, <em>133</em>(9), 6332-6346. (<a href='https://doi.org/10.1007/s11263-025-02485-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle animatable 3D dog reconstruction from a single image, noting the overlooked potential of animals. Particularly, we focus on dogs, emphasizing their intrinsic characteristics that complicate 3D observation. First, the considerable variation in shapes across breeds presents a complexity for modeling. Additionally, the nature of quadrupeds leads to frequent joint occlusions compared to humans. These challenges make 3D reconstruction from 2D observations difficult, and it becomes dramatically harder when constrained to a single image. To address these challenges, our insight is to combine the acquisition of appearance from generative models, without additional data, with geometric guidance provided by a parametric representation, aiming to achieve complete geometry. To this end, we present DogRecon, our framework consists of two key components: Canine-centric novel view synthesis with canine prior for multi-view generation of dog and a reliable sampling weight strategy with Gaussian Splatting for animatable 3D dog reconstruction. Extensive experiments on the GART, DFA, and internet-sourced datasets confirm our framework has state-of-the-art performance in image-to-3D generation and comparable performance in animatable 3D reconstruction. Additionally, we demonstrate novel pose animation and text-to-3D dog reconstruction as applications. Project page: https://vision3d-lab.github.io/dogrecon/},
  archive      = {J_IJCV},
  author       = {Cho, Gyeongsu and Kang, Changwoo and Soon, Donghyeon and Joo, Kyungdon},
  doi          = {10.1007/s11263-025-02485-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6332-6346},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thread counting in plain weave for old paintings using regression deep learning models. <em>IJCV</em>, <em>133</em>(9), 6316-6331. (<a href='https://doi.org/10.1007/s11263-025-02473-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel algorithm designed to improve thread density estimation in canvas analysis. Our approach incorporates three major contributions. First, we eliminate the need for post-segmentation processing by integrating regression techniques, enabling the deep learning (DL) model to directly compute thread density. This does not only reduce computational time but also shifts the training focus from locating crossing points to minimizing thread counting errors, thereby enhancing accuracy. We develop and rigorously evaluate various models, selecting the one with optimal performance through a hyperparameter search. Second, we refine the data generation process by dynamically adjusting filter lengths based on initial thread density estimates and incorporating equalization. We also enhance data augmentation. Third, we implement semi-supervised training to expand the dataset and fine-tune model weights. This involves incorporating new inputs into the training set when both the DL model and Fourier transform yield similar density estimates for new paintings. Our proposed algorithm demonstrates superior performance in thread density error reduction and operational efficiency compared to previous DL segmentation solutions for masterpieces from Ribera, Velázquez, or Poussin. Additionally, it has been effectively applied to identify fabric matches between canvases attributed to different authors, showcasing its practical applicability in art analysis.},
  archive      = {J_IJCV},
  author       = {Delgado, Antonio and Murillo-Fuentes, Juan José and Alba-Carcelén, Laura},
  doi          = {10.1007/s11263-025-02473-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6316-6331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Thread counting in plain weave for old paintings using regression deep learning models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). About time: Advances, challenges, and outlooks of action understanding. <em>IJCV</em>, <em>133</em>(9), 6251-6315. (<a href='https://doi.org/10.1007/s11263-025-02478-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.},
  archive      = {J_IJCV},
  author       = {Stergiou, Alexandros and Poppe, Ronald},
  doi          = {10.1007/s11263-025-02478-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6251-6315},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {About time: Advances, challenges, and outlooks of action understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for text recognition: A critical survey. <em>IJCV</em>, <em>133</em>(9), 6221-6250. (<a href='https://doi.org/10.1007/s11263-025-02487-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Recognition (TR) refers to the research area that focuses on retrieving textual information from images, a topic that has seen significant advancements in the last decade due to the use of Deep Neural Networks (DNN). However, these solutions often necessitate vast amounts of manually labeled or synthetic data. Addressing this challenge, Self-Supervised Learning (SSL) has gained attention by utilizing large datasets of unlabeled data to train DNN, thereby generating meaningful and robust representations. Although SSL was initially overlooked in TR because of its unique characteristics, recent years have witnessed a surge in the development of SSL methods specifically for this field. This rapid development, however, has led to many methods being explored independently, without taking previous efforts in methodology or comparison into account, thereby hindering progress in the field of research. This paper, therefore, seeks to consolidate the use of SSL in the field of TR, offering a critical and comprehensive overview of the current state of the art. We will review and analyze the existing methods, compare their results, and highlight inconsistencies in the current literature. This thorough analysis aims to provide general insights into the field, propose standardizations, identify new research directions, and foster its proper development.},
  archive      = {J_IJCV},
  author       = {Penarrubia, Carlos and Valero-Mas, Jose J. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s11263-025-02487-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6221-6250},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised learning for text recognition: A critical survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification. <em>IJCV</em>, <em>133</em>(9), 6201-6220. (<a href='https://doi.org/10.1007/s11263-025-02474-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tabular classifiers provide explainable decision-making with interpretable features (concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, the Img2Tab classifier should automatically discover class-relevant concepts from the StyleGAN features. Thus, we propose a novel algorithm using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. By this method of concept visualization, we quantitatively investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can interactively debug Img2Tab classifier to prevent erroneous decision-making from data bias without sacrificing accuracy. The source and demo code for Img2Tab are available at https://github.com/songsnim/Img2Tab_pytorch},
  archive      = {J_IJCV},
  author       = {Song, Youngjae and Shyn, Sung Kuk and Kim, Kwang-su},
  doi          = {10.1007/s11263-025-02474-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6201-6220},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport with arbitrary prior for dynamic resolution network. <em>IJCV</em>, <em>133</em>(9), 6187-6200. (<a href='https://doi.org/10.1007/s11263-025-02483-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resolution network is proved to be crucial in reducing computational redundancy by automatically assigning satisfactory resolution for each input image. However, it is observed that resolution choices are often collapsed, where prior works tend to assign images to the resolution routes whose computational cost is close to the required FLOPs. In this paper, we propose a novel optimal transport dynamic resolution network (OTD-Net) by establishing an intrinsic connection between resolution assignment and optimal transport problem. In this framework, each sample owns a resolution assignment choice viewed as supplier, and each resolution requires unallocated images considered as demander. With two assignment priors, OTD-Net benefits from the non-collapse division under theoretical support, and produces the desired assignment policy by balancing the computation budget and prediction accuracy. On that basis, a multi-resolution inference is proposed to ensemble low-resolution predictions. Extensive experiments including image classification, object detection and depth estimation, show our approach is both efficient and effective for both ResNet and Transformer, achieving state-of-the-art performance on various benchmarks.},
  archive      = {J_IJCV},
  author       = {Zhang, Zhizhong and Li, Shujun and Zhang, Chenyang and Ma, Lizhuang and Tan, Xin and Xie, Yuan},
  doi          = {10.1007/s11263-025-02483-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6187-6200},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Optimal transport with arbitrary prior for dynamic resolution network},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search. <em>IJCV</em>, <em>133</em>(9), 6170-6186. (<a href='https://doi.org/10.1007/s11263-025-02480-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their impressive performance on various tasks, vision transformers (ViTs) are heavy for mobile vision applications. Recent works have proposed combining the strengths of ViTs and convolutional neural networks (CNNs) to build lightweight networks. Still, these approaches rely on hand-designed architectures with a pre-determined number of parameters. In this work, we address the challenge of finding optimal light-weight ViTs given constraints on model size and computational cost using neural architecture search. We use a search algorithm that considers both model parameters and on-device deployment latency. This method analyzes network properties, hardware memory access pattern, and degree of parallelism to directly and accurately estimate the network latency. To prevent the need for extensive testing during the search process, we use a lookup table based on a detailed breakdown of the speed of each component and operation, which can be reused to evaluate the whole latency of each search structure. Our approach leads to improved efficiency compared to testing the speed of the whole model during the search process. Extensive experiments demonstrate that, under similar parameters and FLOPs, our searched lightweight ViTs achieve higher accuracy and lower latency than state-of-the-art models. For instance, on ImageNet-1K, AutoViT_XXS (71.3% Top-1 accuracy, 10.2ms latency) outperforms MobileViTv3_XXS (71.0% Top-1 accuracy, 12.5ms latency) with 0.3% higher accuracy and 2.3ms lower latency.},
  archive      = {J_IJCV},
  author       = {Kong, Zhenglun and Xu, Dongkuan and Li, Zhengang and Dong, Peiyan and Tang, Hao and Wang, Yanzhi and Mukherjee, Subhabrata},
  doi          = {10.1007/s11263-025-02480-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6170-6186},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking open-set object detection: Issues, a new formulation, and taxonomy. <em>IJCV</em>, <em>133</em>(9), 6145-6169. (<a href='https://doi.org/10.1007/s11263-025-02479-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing “what to detect,” which contradicts the idea of identifying “unknown” objects. This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods’ performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. The results show that existing methods fail to accurately detect unknown objects due to misclassification of known and unknown classes rather than incorrect bounding box prediction. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.},
  archive      = {J_IJCV},
  author       = {Hosoya, Yusuke and Suganuma, Masanori and Okatani, Takayuki},
  doi          = {10.1007/s11263-025-02479-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6145-6169},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking open-set object detection: Issues, a new formulation, and taxonomy},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight structure-aware attention for visual understanding. <em>IJCV</em>, <em>133</em>(9), 6129-6144. (<a href='https://doi.org/10.1007/s11263-025-02475-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention operator has been widely used as a basic brick in visual understanding since it provides some flexibility through its adjustable kernels. However, this operator suffers from inherent limitations: (1) the attention kernel is not discriminative enough, resulting in high redundancy, and (2) the complexity in computation and memory is quadratic in the sequence length. In this paper, we propose a novel attention operator, called Lightweight Structure-aware Attention (LiSA), which has a better representation power with log-linear complexity. Our operator transforms the attention kernels to be more discriminative by learning structural patterns. These structural patterns are encoded by exploiting a set of relative position embeddings (RPEs) as multiplicative weights, thereby improving the representation power of the attention kernels. Additionally, the RPEs are approximated to obtain log-linear complexity. Our experiments and analyses demonstrate that the proposed operator outperforms self-attention and other existing operators, achieving state-of-the-art results on ImageNet-1K and other downstream tasks such as video action recognition on Kinetics-400, object detection & instance segmentation on COCO, and semantic segmentation on ADE-20K.},
  archive      = {J_IJCV},
  author       = {Kwon, Heeseung and Castro, Francisco M. and Marin-Jimenez, Manuel J. and Guil, Nicolas and Alahari, Karteek},
  doi          = {10.1007/s11263-025-02475-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6129-6144},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight structure-aware attention for visual understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection. <em>IJCV</em>, <em>133</em>(9), 6108-6128. (<a href='https://doi.org/10.1007/s11263-025-02486-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for oriented object detection (OOD), recent studies on point-supervised OOD have attracted significant interest. In this paper, we propose PointOBB-v3, a stronger single point-supervised OOD framework. Compared to existing methods, it generates pseudo rotated boxes without additional priors and incorporates support for the end-to-end paradigm. PointOBB-v3 functions by integrating three unique image views: the original view, a resized view, and a rotated/flipped (rot/flp) view. Based on the views, a scale augmentation module and an angle acquisition module are constructed. In the first module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive Feature Fusion (SSFF) module are introduced to improve the model’s ability to estimate object scale. To achieve precise angle predictions, the second module employs symmetry-based self-supervised learning. Additionally, we introduce an end-to-end version that eliminates the pseudo-label generation process by integrating a detector branch and introduces an Instance-Aware Weighting (IAW) strategy to focus on high-quality predictions. We conducted extensive experiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR datasets. Across all these datasets, our method achieves an average improvement in accuracy of 3.56% in comparison to previous state-of-the-art methods. The code will be available at https://github.com/ZpyWHU/PointOBB-v3 .},
  archive      = {J_IJCV},
  author       = {Zhang, Peiyuan and Luo, Junwei and Yang, Xue and Yu, Yi and Li, Qingyun and Zhou, Yue and Jia, Xiaosong and Lu, Xudong and Chen, Jingdong and Li, Xiang and Yan, Junchi and Li, Yansheng},
  doi          = {10.1007/s11263-025-02486-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6108-6128},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling scattering effect for under-display camera image restoration. <em>IJCV</em>, <em>133</em>(9), 6088-6107. (<a href='https://doi.org/10.1007/s11263-025-02454-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-display camera (UDC) technology furnishes users with an uninterrupted full-screen viewing experience, eliminating the need for notches or punch holes. However, the translucent properties of the display lead to substantial degradation in UDC images. This work addresses the challenge of restoring UDC images by specifically targeting the scattering effect induced by the display. We explicitly model this scattering phenomenon by treating the display as a homogeneous scattering medium. Leveraging this physical model, the image formation pipeline is enhanced to synthesize more realistic UDC images alongside corresponding ground-truth images, thereby constructing a more accurate UDC dataset. To counteract the scattering effect in the restoration process, we propose a dual-branch network. The scattering branch employs channel-wise self-attention to estimate the scattering parameters, while the image branch capitalizes on the local feature representation capabilities of CNNs to restore the degraded UDC images. Additionally, we introduce a novel channel-wise cross-attention fusion block that integrates global scattering information into the image branch, facilitating improved restoration. To further refine the model, we design a dark channel regularization loss during training to reduce the gap between the dark channel distributions of the restored and ground-truth images. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the superiority of our approach over current state-of-the-art UDC restoration methods. Our source code is publicly available at: https://github.com/NamecantbeNULL/SRUDC_pp .},
  archive      = {J_IJCV},
  author       = {Song, Binbin and Zhou, Jiantao and Chen, Xiangyu and Xu, Shuning},
  doi          = {10.1007/s11263-025-02454-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6088-6107},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Modeling scattering effect for under-display camera image restoration},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIM4D: Masked modeling with multi-view video for autonomous driving representation learning. <em>IJCV</em>, <em>133</em>(9), 6074-6087. (<a href='https://doi.org/10.1007/s11263-025-02464-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information, which is fundamental for the ultimate application, i.e., end-to-end planning. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including end-to-end planning( $$9\%$$ collision decrease), BEV segmentation ( $$8.7\%$$ IoU), 3D object detection ( $$3.5\%$$ mAP), and HD map construction ( $$1.4\%$$ mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D .},
  archive      = {J_IJCV},
  author       = {Zou, Jialv and Liao, Bencheng and Zhang, Qian and Liu, Wenyu and Wang, Xinggang},
  doi          = {10.1007/s11263-025-02464-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6074-6087},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MIM4D: Masked modeling with multi-view video for autonomous driving representation learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RigNet++: Semantic assisted repetitive image guided network for depth completion. <em>IJCV</em>, <em>133</em>(9), 6051-6073. (<a href='https://doi.org/10.1007/s11263-025-02470-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to recover dense depth maps from sparse ones, where color images are often used to facilitate this task. Recent depth methods primarily focus on image guided learning frameworks. However, blurry guidance in the image and unclear structure in the depth still impede their performance. To tackle these challenges, we explore a repetitive design in our image guided network to gradually and sufficiently recover depth values. Specifically, the repetition is embodied in both the image guidance branch and depth generation branch. In the former branch, we design a dense repetitive hourglass network (DRHN) to extract discriminative image features of complex environments, which can provide powerful contextual instruction for depth prediction. In the latter branch, we present a repetitive guidance (RG) module based on dynamic convolution, in which an efficient convolution factorization is proposed to reduce the complexity while modeling high-frequency structures progressively. Furthermore, in the semantic guidance branch, we utilize the well-known large vision model, i.e., segment anything (SAM), to supply RG with semantic prior. In addition, we propose a region-aware spatial propagation network (RASPN) for further depth refinement based on the semantic prior constraint. Finally, we collect a new dataset termed TOFDC for the depth completion task, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Extensive experiments demonstrate that our method achieves state-of-the-art performance on KITTI, NYUv2, Matterport3D, 3D60, VKITTI, and our TOFDC.},
  archive      = {J_IJCV},
  author       = {Yan, Zhiqiang and Li, Xiang and Hui, Le and Zhang, Zhenyu and Li, Jun and Yang, Jian},
  doi          = {10.1007/s11263-025-02470-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6051-6073},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RigNet++: Semantic assisted repetitive image guided network for depth completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized contour vibration model for building extraction. <em>IJCV</em>, <em>133</em>(9), 6025-6050. (<a href='https://doi.org/10.1007/s11263-025-02468-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic active contour models (ACMs) are becoming a great promising solution to the contour-based object extraction with the progress of deep learning recently. Inspired by the wave vibration theory in physics, we propose a Generalized Contour Vibration Model (G-CVM) by inheriting the force and motion principle of contour wave for automatically estimating building contours. The contour estimation problems, conventionally solved by snake and level-set based ACMs, are unified to formulate as second-order partial differential equation to model the contour evolution. In parallel with the current ACM methods, we propose two types of evolution paradigms: curve-CVM and surface-CVM, from the perspective of the vibration spaces of contour waves. To tailor personalization contours for specific targets, we parameterize the constant coefficient wave differential equation through a convolutional network, and hereby integrate them into a unified learnable model for contour extraction. Through adopting finite difference optimization, we can progressively perform the contour evolution from an initial state through a recursive computation on the contour vibration model. Both the building contour evolution and the model optimization are modulated to form a close-looping end-to-end network. Besides, we make a discussion of ours vs the conventional ACMs, all which can be interpreted uniformly from the view of differential equation in different evolution domains. Comprehensive evaluations on several building datasets demonstrate the effectiveness and superiority of our proposed G-CVM when compared with other state-of-the-art building extraction networks and deep active contour solutions.},
  archive      = {J_IJCV},
  author       = {Xu, Chunyan and Yao, Shuaizhen and Xu, Ziqiang and Cui, Zhen and Yang, Jian},
  doi          = {10.1007/s11263-025-02468-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6025-6050},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A generalized contour vibration model for building extraction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects. <em>IJCV</em>, <em>133</em>(9), 6015-6024. (<a href='https://doi.org/10.1007/s11263-025-02469-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and effective method that views the problem of single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects as an aligned sequential point cloud prediction problem. Our method does not require additional data transformations (truncated signed distance function or deformation graphs, etc.), alignment constraints (handcrafted features or optical flow, etc.), and prior regularities (as-rigid-as-possible or embedded deformation, etc.). We propose an end-to-end model architecture that is TRansformer for synchronous Tracking and Reconstruction of non-rigid dynamic target based on RGB-D images from a monocular camera, called TR4TR. We use a spatial-temporal combined 2D image encoder that directly encodes features from RGB-D sequence images, and a 3D point decoder to generate aligned sequential point cloud containing tracking and reconstruction results. The TR4TR model outperforms the baselines on the DeepDeform non-rigid dataset, and outperforms the state-of-the-art method by 8.82% on the deformation error evaluation metric. In addition, TR4TR is more robust when the target undergoes large inter-frame deformation. The code is available at https://github.com/xfliu1998/tr4tr-main .},
  archive      = {J_IJCV},
  author       = {Liu, Xiaofei and Yi, Zhengkun and Wu, Xinyu and Shang, Wanfeng},
  doi          = {10.1007/s11263-025-02469-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6015-6024},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID. <em>IJCV</em>, <em>133</em>(9), 5992-6014. (<a href='https://doi.org/10.1007/s11263-025-02461-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/FranklinLingfeng/code-for-SALCR .},
  archive      = {J_IJCV},
  author       = {Cheng, De and He, Lingfeng and Wang, Nannan and Zhang, Dingwen and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02461-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5992-6014},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to deblur polarized images. <em>IJCV</em>, <em>133</em>(9), 5976-5991. (<a href='https://doi.org/10.1007/s11263-025-02459-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A polarization camera can capture four linear polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of linear polarization (DoLP) and the angle of linear polarization (AoLP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoLP and AoLP. Deblurring methods for conventional images often show degraded performance when handling the polarized images since they only focus on deblurring without considering the polarization constraints. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.},
  archive      = {J_IJCV},
  author       = {Zhou, Chu and Teng, Minggui and Zhou, Xinyu and Xu, Chao and Sato, Imari and Shi, Boxin},
  doi          = {10.1007/s11263-025-02459-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5976-5991},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to deblur polarized images},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching. <em>IJCV</em>, <em>133</em>(9), 5958-5975. (<a href='https://doi.org/10.1007/s11263-025-02457-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patch-based matching is a technique meant to measure the disparity between pixels in a source and target image and is at the core of various methods in computer vision. When the subpixel disparity between the source and target images is required, the cost function or the target image has to be interpolated. While cost-based interpolation is easier to implement, multiple works have shown that image-based interpolation can increase the accuracy of the disparity estimate. In this paper we review closed-form formulae for subpixel disparity computation for one dimensional matching, e.g., rectified stereo matching, for the standard cost functions used in patch-based matching. We then propose new formulae to generalize to high-dimensional search spaces, which is necessary for unrectified stereo matching and optical flow. We also compare the image-based interpolation formulae with traditional cost-based formulae, and show that image-based interpolation brings a significant improvement over the cost-based interpolation methods for two dimensional search spaces, and small improvement in the case of one dimensional search spaces. The zero-mean normalized cross correlation cost function is found to be preferable for subpixel alignment. A new error model, based on very broad assumptions is outlined in the Supplementary Material to demonstrate why these image-based interpolation formulae outperform their cost-based counterparts and why the zero-mean normalized cross correlation function is preferable for subpixel alignement.},
  archive      = {J_IJCV},
  author       = {Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Bennamoun, Mohammed},
  doi          = {10.1007/s11263-025-02457-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5958-5975},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HumanLiff: Layer-wise 3D human diffusion model. <em>IJCV</em>, <em>133</em>(9), 5938-5957. (<a href='https://doi.org/10.1007/s11263-025-02477-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human generation from 2D images has achieved remarkable progress through the synergistic utilization of neural rendering and generative models. Existing 3D human generative models mainly generate a clothed 3D human as an inseparable 3D model in a single pass, while rarely considering the layer-wise nature of a clothed human body, which often consists of the human body and various clothes such as underwear, outerwear, trousers, shoes, etc. In this work, we propose HumanLiff, the first layer-wise 3D human generative model with a unified diffusion process. Specifically, HumanLiff firstly generates minimal-clothed humans, represented by tri-plane features, in a canonical space, and then progressively generates clothes in a layer-wise manner. In this way, the 3D human generation is thus formulated as a sequence of diffusion-based 3D conditional generation. To reconstruct more fine-grained 3D humans with tri-plane representation, we propose a tri-plane shift operation that splits each tri-plane into three sub-planes and shifts these sub-planes to enable feature grid subdivision. To further enhance the controllability of 3D generation with 3D layered conditions, HumanLiff hierarchically fuses tri-plane features and 3D layered conditions to facilitate the 3D diffusion model learning. Extensive experiments on two layer-wise 3D human datasets, SynBody (synthetic) and TightCap (real-world), validate that HumanLiff significantly outperforms state-of-the-art methods in layer-wise 3D human generation. Our code and datasets are available at https://skhu101.github.io/HumanLiff .},
  archive      = {J_IJCV},
  author       = {Hu, Shoukang and Hong, Fangzhou and Hu, Tao and Pan, Liang and Mei, Haiyi and Xiao, Weiye and Yang, Lei and Liu, Ziwei},
  doi          = {10.1007/s11263-025-02477-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5938-5957},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HumanLiff: Layer-wise 3D human diffusion model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against adversarial examples via modeling adversarial noise. <em>IJCV</em>, <em>133</em>(9), 5920-5937. (<a href='https://doi.org/10.1007/s11263-025-02467-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have become a major threat to the reliable application of deep learning models. Meanwhile, this issue promotes the development of adversarial defenses. Adversarial noise contains well-generalizing and misleading features, which can manipulate predicted labels to be flipped maliciously. Motivated by this, we study modeling adversarial noise for defending against adversarial examples by learning the transition relationship between adversarial labels (i.e., flipped labels caused by adversarial noise) and natural labels (i.e., real labels of natural samples). In this work, we propose an adversarial defense method from the perspective of modeling adversarial noise. Specifically, we construct an instance-dependent label transition matrix to represent the label transition relationship for explicitly modeling adversarial noise. The label transition matrix is obtained from the input sample by leveraging a label transition network. By exploiting the label transition matrix, we can infer the natural label from the adversarial label and thus correct wrong predictions misled by adversarial noise. Additionally, to enhance the robustness of the label transition network, we design an adversarial robustness constraint at the transition matrix level. Experimental results demonstrate that our method effectively improves the robust accuracy against multiple attacks and exhibits great performance in detecting adversarial input samples.},
  archive      = {J_IJCV},
  author       = {Zhou, Dawei and Wang, Nannan and Han, Bo and Liu, Tongliang and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02467-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5920-5937},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Defending against adversarial examples via modeling adversarial noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring bidirectional bounds for minimax-training of energy-based models. <em>IJCV</em>, <em>133</em>(9), 5898-5919. (<a href='https://doi.org/10.1007/s11263-025-02460-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-based models (EBMs) estimate unnormalized densities in an elegant framework, but they are generally difficult to train. Recent work has linked EBMs to generative adversarial networks, by noting that they can be trained through a minimax game using a variational lower bound. To avoid the instabilities caused by minimizing a lower bound, we propose to instead work with bidirectional bounds, meaning that we maximize a lower bound and minimize an upper bound when training the EBM. We investigate four different bounds on the log-likelihood derived from different perspectives. We derive lower bounds based on the singular values of the generator Jacobian and on mutual information. To upper bound the negative log-likelihood, we consider a gradient penalty-like bound, as well as one based on diffusion processes. In all cases, we provide algorithms for evaluating the bounds. We compare the different bounds to investigate, the pros and cons of the different approaches. Finally, we demonstrate that the use of bidirectional bounds stabilizes EBM training and yields high-quality density estimation and sample generation.},
  archive      = {J_IJCV},
  author       = {Geng, Cong and Wang, Jia and Chen, Li and Gao, Zhiyong and Frellsen, Jes and Hauberg, Søren},
  doi          = {10.1007/s11263-025-02460-0},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5898-5919},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring bidirectional bounds for minimax-training of energy-based models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A norm regularization training strategy for robust image quality assessment models. <em>IJCV</em>, <em>133</em>(9), 5883-5897. (<a href='https://doi.org/10.1007/s11263-025-02458-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Quality Assessment (IQA) models predict the quality score of input images. They can be categorized into Full-Reference (FR-) and No-Reference (NR-) IQA models based on the availability of reference images. These models are essential for performance evaluation and optimization guidance in the media industry. However, researchers have observed that introducing imperceptible perturbations to input images can notably influence the predicted scores of both FR- and NR-IQA models, resulting in inaccurate assessments of image quality. This phenomenon is known as adversarial attacks. In this paper, we initially define attacks targeted at both FR-IQA and NR-IQA models. Subsequently, we introduce a defense approach applicable to both types of models, aimed at enhancing the stability of predicted scores and boosting the adversarial robustness of IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $$\ell _1$$ norm of the model’s gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $$\ell _1$$ norm of the gradient, thereby boosting the robustness of IQA models. Experiments conducted on three FR-IQA and four NR-IQA models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on both FR- and NR-IQA models. Our study offers valuable insights into the adversarial robustness of IQA models and provides a foundation for future research in this area.},
  archive      = {J_IJCV},
  author       = {Liu, Yujia and Yang, Chenxi and Li, Dingquan and Jiang, Tingting and Huang, Tiejun},
  doi          = {10.1007/s11263-025-02458-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5883-5897},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A norm regularization training strategy for robust image quality assessment models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
