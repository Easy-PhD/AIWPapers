<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PAAA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="paaa">PAAA - 26</h2>
<ul>
<li><details>
<summary>
(2025). ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization. <em>PAAA</em>, <em>28</em>(4), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01516-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of cybersecurity, malware detection stands at the forefront of defense against malicious software. This study introduces an innovative strategy to tackle the ever-evolving cyber threats that characterize the current landscape, transcending traditional methodologies. We present a hybridized approach that combines the advanced capabilities of Vision Transformer (ViT) model, genetic algorithms, and cutting-edge deep learning techniques, marking a new era in cybersecurity. The proposed process begins by transforming complex malware source code into grayscale images, effectively bridging the gap between linear code analysis and spatial image recognition. These grayscale images are analyzed using the ViT_b16 model, renowned for its exceptional ability to uncover subtle intricacies within images. The next steps involve leveraging deep learning to scrutinize the features identified by the ViT, facilitating precise detection of malicious code. To enhance the efficiency of the proposed deep learning model, a genetic algorithm is employed for end-to-end hyperparameter optimization for both ViT and deep learning phases. this process aims at calibrating essential parameters such as the Image Size, Number of Attention Heads, Hidden Size (Embedding Dimension), MLP (Feedforward) Dimension, activation function, architectural depth, neuron count, optimizers, initializers, dropout layers, batch normalization, and learning rates of the ViT_b16 and deep learning models. After extensive training on a dataset comprising 25 diverse malware families, the proposed model exhibits remarkable performance, consistently achieving an accuracy rate exceeding 99% in differentiating among these malware variants. A comprehensive evaluation and benchmarking against both state-of-the-art malware detection methodologies and widely used baseline models, including CNNs and traditional machine learning algorithms, demonstrating superior detection performance across all metrics.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit and Bakır, Rezan and Alkhaldi, Tareq and Darem, Abdulbasit A. and Alhashmi, Asma A. and Alqhatani, Abdulmajeed},
  doi          = {10.1007/s10044-025-01516-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01526-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface defects of printed circuit boards (PCB) that occur during the manufacturing process seriously affect product quality. So it is important to detect PCB surface defects quickly and accurately. However, existing defect detection methods still have room to be improved for PCB surface defect detection. This paper proposes an advanced model, MDUA-YOLO, based on YOLOv5 to increase the detection accuracy of defects on PCB surface. Firstly, we introduce the C3 Mobile Vision Transformer (C3MobileViT) module in the backbone of YOLOv5, improving the feature extraction capability of the model. Secondly, the Deformable Convolutional-Receptive Field Block (DC-RFB) module is incorporated into the neck of YOLOv5 to dynamically expands the receptive field and more accurately capture the location information of small defects. Additionally, we design the Union Attention Block (UAB) module in the neck of YOLOv5 to optimize the fusion of low-level and high-level feature maps. Finally, an extra prediction head and new feature fusion layer are also added to enhance the ability of the model to detect small defects. On the benchmark PKU-PCB and DeepPCB datasets, numerous experimental results show that the MDUA-YOLO surpasses other comparison state-of-the-art models and meets the real-time detection requirement of industrial environment.},
  archive      = {J_PAAA},
  author       = {Liu, Xiaowei and Wang, Haichao},
  doi          = {10.1007/s10044-025-01526-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing video salient object detection via SAM-based multimodal energy prompting. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01531-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Salient Object Detection (VSOD) aims to identify the most visually conspicuous objects in videos and extract key information from complex visual scenes. Recent studies combine optical flow (OF) and depth for complementary feature extraction. However, suboptimal fusion strategies often treat these modalities merely as extensions of the RGB stream, failing to fully leverage their unique semantic contributions. To address this limitation, we propose a novel SAM-based Multimodal Energy Prompting Network (MEPNet), which utilizes implicit prompts derived from OF and depth within a pre-trained Segment Anything Model (SAM). This approach enhances VSOD by effectively integrating the complementary dynamic and structural information from these modalities. Particularly, we introduce a Spectrogram Energy Generator to extract Spectrogram Energy from OF and depth. These energy-driven prompts fine-tune SAM via the Modality Energy Adapter, effectively mitigating noise interference and improving segmentation accuracy. In addition, we propose a Circular High-frequency Filter to enhance RGB modality details using an adaptive circular mask. Extensive experiments on five VSOD benchmark datasets demonstrate that our MEPNet outperforms state-of-the-art approaches. Furthermore, our MEPNet generalizes effectively to the Video-Camouflaged Object Detection task and achieves competitive results. The module and predicted maps are publicly available at https://github.com/TOMMYWHY/MEPNet .},
  archive      = {J_PAAA},
  author       = {Jiang, Tao and Wang, Yi and Hou, Feng and Liu, Li-li},
  doi          = {10.1007/s10044-025-01531-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing video salient object detection via SAM-based multimodal energy prompting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with adaptive aggregation within clusters. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01533-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows clients to collaboratively train models while keeping client data local. Initially, it trains a global model to serve all clients, but when the distribution of each local dataset differs significantly from the global dataset, the local objectives of each client may diverge from the globally optimal values, leading to drift in local updates. This phenomenon greatly impacts model performance. The primary purpose of client participation in federated learning is to obtain personalized models with better local performance. In order to solve this problem, this paper proposes a new federated learning algorithm - Federated Learning with Adaptive Intra - cluster Aggregation (FedACC). This algorithm utilizes the inference correlation of client-uploaded models to the server and divides clients with similar data distributions into clusters. During the weighted aggregation of models within each cluster, we introduce an adaptive weight learning algorithm and use the obtained weights to perform the weighted aggregation of cluster models. The algorithm can cluster clients with similar data distributions and utilize adaptive weight learning within the cluster to obtain optimal aggregation weights, enabling more efficient and personalized federated learning through a weighted aggregation cluster model. Our experiments are conducted on three public image datasets, namely MNIST, Fashion - MNIST, and CIFAR − 10, and in a data - heterogeneous environment. Compared with three baseline algorithms, the Federated Averaging algorithm (FedAvg), the Federated Proximal algorithm (FedProx), and the Federated Learning with Intra - cluster Similarity algorithm (FLIS), the global model of the FedACC algorithm proposed in this paper converges faster and has a higher accuracy. On the Fashion - MNIST dataset, compared with FedAvg, FedProx, and FLIS algorithms, the accuracy of FedACC is improved by 11.6%, 10.5%, and 3.0% respectively, which proves the effectiveness of the FedACC algorithm.},
  archive      = {J_PAAA},
  author       = {Ding, Shiyuan and Liu, Yanhong and Shi, Haobin and Gao, Yingying},
  doi          = {10.1007/s10044-025-01533-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Personalized federated learning with adaptive aggregation within clusters},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01534-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate retinal vessel segmentation is crucial for ophthalmic image analysis, providing key structural information for diagnosis and treatment planning. However, existing methods struggle with multi-scale vessel variability, complex curvatures, and ambiguous boundaries. CNNs, Transformer, and Mamba-based approaches have shown promise, yet still struggle to maintain vascular continuity and accurately delineate fine vessel boundaries, especially in thin or tortuous regions, leading to structural discontinuities and edge ambiguity. To address these limitations, we propose a novel hybrid framework that synergistically integrates CNNs and Mamba for high-precision retinal vessel segmentation. Our approach introduces three key innovations: (1) The proposed High-Resolution Edge Fuse Network is a high-resolution preserving hybrid segmentation framework that enhances edge features to ensure accurate and robust vessel segmentation. (2) The Dynamic Snake Visual State Space block is designed to adaptively capture vessel curvature details and long-range dependencies. An improved eight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting strategy enhance the perception of complex vascular topologies. (3) The MREF module enhances boundary precision through multi-scale edge feature aggregation, suppressing noise while emphasizing critical vessel structures across scales. Experiments on DRIVE, STARE, and CHASE_DB1 demonstrate the robust and effective performance of our method. Specifically, our approach attains Dice scores of 82.14%, 76.29%, and 80.46%; clDice scores of 82.40%, 80.30%, and 82.93%; and AUC values of 98.56%, 98.05%, and 98.78%, respectively. This work provides a robust method for clinical applications requiring accurate retinal vessel analysis. The code is available at https://github.com/frank-oy/HREFNet .},
  archive      = {J_PAAA},
  author       = {Ouyang, Yihao and Kuang, Xunheng and Xiong, Mengjia and Wang, Zhida and Wang, Yuanquan},
  doi          = {10.1007/s10044-025-01534-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial translucent patch: A robust physical attack technique against object detectors. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01535-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of computer vision-based autonomous driving technology in daily life necessitates further evaluation of its safety. Current physical attack techniques, using non-transparent stickers as perturbations, lack stealth and therefore may not be effective in real-world applications. Some studies use translucent patches on camera lenses to attack deep neural networks (DNNs), but accessing a victim’s camera is impractical. Light-based attacks also struggle to achieve robust effects under varying environmental conditions. To address these issues within the domain of applied pattern recognition, we propose Adversarial Translucent Patch (AdvTP). This method utilizes translucent color patches optimized with a differential evolution algorithm to create effective physical perturbations. These patches are applied to target objects for black-box attacks on object detectors, a key area in computer vision and image processing. Extensive experiments validate the method’s effectiveness, stealth, and robustness. The proposed method achieves a 91.04% success rate in digital attacks and a 100% success rate in most physical attack cases. It demonstrates superior stealth compared to baseline methods and achieves an average attack success rate of 94.04% against advanced object detectors. We also analyze the method’s generalization capabilities across different pattern recognition tasks, including attacking image classifiers and vehicle detectors, as well as its performance in transfer attacks and against adversarial defenses. Given the significant security threats posed by this method to vision-based applications, which are critical in various applied domains, we believe this work will attract considerable attention from the pattern recognition community. The code can be found in https://github.com/kalbinur90/AdvTP.git .},
  archive      = {J_PAAA},
  author       = {Tiliwalidi, Kalibinuer and Hu, Chengyin and Shi, Weiwen and Lu, Guangxi and Wu, Hao},
  doi          = {10.1007/s10044-025-01535-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adversarial translucent patch: A robust physical attack technique against object detectors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-capacity reversible data hiding in encrypted HDR images with multiple data hiders. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01536-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a reversible data hiding algorithm for encrypted high-dynamic-range images, with the potential to significantly impact the field. The algorithm supports high embedding capacity, high embedding rate, and allows for multiple data hiders. It begins by using a median edge detector to predict the value of each processing pixel, and determines the embedding capacity through multi-MSB prediction and leading zero count prediction strategies. Multiple label maps are generated and encoded using Huffman coding to reduce transmission overhead. Furthermore, secret sharing is employed to generate several image shares containing pre-embedded label maps, which are distributed to participants who can independently embed secret message. The proposed algorithm enhances the confidentiality of high-dynamic-range images through secret sharing and improves robustness by using multiple image shares, preventing a single image attack from compromising message recovery. Overall, the algorithm significantly increases embedding capacity. Experimental results demonstrate that this approach makes substantial contributions to reversible data hiding in encrypted high-dynamic-range images.},
  archive      = {J_PAAA},
  author       = {Lin, Alfrindo and Lin, Yun-Ting and Jao, Wen-Ting and Tsai, Yuan-Yu},
  doi          = {10.1007/s10044-025-01536-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {High-capacity reversible data hiding in encrypted HDR images with multiple data hiders},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boostis:boosting image semi-supervised learning through pseudo-label quality assessment. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01537-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in semi-supervised learning methods that leverage pseudo-labels and consistency regularization. However, two key issues with existing approaches have been identified. Firstly, these methods often focus on maintaining inter-class consistency between strong and weak augmentations, but they tend to overlook intra-class differences. This oversight results in a waste of valuable intra-class information. Secondly, the selection of a fixed high threshold for pseudo-label confidence restricts the quantity and utilization of pseudo-labels. On the contrary, using an initial low threshold introduces a large number of erroneous pseudo-labels, resulting in a degradation of the model’s performance. To address these issues, we propose a novel semi-supervised learning framework that combines contrastive learning and feature discrepancy loss. Our approach introduces a new loss function that facilitates intra-class discrimination by emphasizing inter-class differences. Additionally, we tackle the threshold problem in pseudo-label consistency loss by introducing dynamic weighting coefficients. These coefficients help balance the impact of both the quantity and quality of pseudo-labels on the model. Our experimental results demonstrate that our method effectively harnesses the feature differences among samples with different perturbations. This enhancement boosts the model’s feature generation capability while mitigating the negative effects of erroneous pseudo-labels on model’s performance. Overall, our proposed framework provides a more comprehensive and effective solution to semi-supervised learning in classification applications by addressing the issues of intra-class differences and the selection of pseudo-label thresholds.},
  archive      = {J_PAAA},
  author       = {Liu, Pingping and Chen, Pengfei and Liu, Xiaofeng and Zhou, Qiuzhan},
  doi          = {10.1007/s10044-025-01537-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Boostis:boosting image semi-supervised learning through pseudo-label quality assessment},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multi-view deep image clustering via contrastive learning for global and local consistency. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01538-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) is a data clustering method with many applications, including but not limited to image and video analysis, text and language processing, bioinformatics, and signal processing. The objective of multi-view deep clustering is to enhance the efficacy of clustering algorithms by integrating data from disparate views. However, discrepancies and inconsistencies between different views frequently reduce the precision of the clustering outcomes. In the recent popular contrastive learning, it has been observed that the processing of positive and negative samples does not consider the multi-view consistency information, ultimately resulting in a decline in clustering accuracy. In this paper, we put forth a global and local consistency-based contrastive learning framework to enhance the efficacy of multi-view deep clustering. First, a global consistency constraint is designed to ensure that the global representations of different views can be aligned to capture the data’s main features. Secondly, we introduce a local consistency mechanism, which aims to preserve the unique local information in each view and obtain efficient, positive samples to improve the complementarity and robustness of the inter-view representations through contrastive learning. The experimental results demonstrate that the proposed method markedly enhances the clustering performance on several real benchmark datasets, mainly when dealing with multi-view data with incompleteness.},
  archive      = {J_PAAA},
  author       = {Shi, Fuhao and Lu, Hu},
  doi          = {10.1007/s10044-025-01538-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing multi-view deep image clustering via contrastive learning for global and local consistency},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-preserving image smoothing via sparse gradient enhancement. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01539-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing is fundamental in the fields of computer vision and image processing. The primary challenge is to smooth low-amplitude details while retaining critical structural information. Existing global filtering methods typically incorporate a data fidelity term and a gradient smoothness term. However, preserving the full semantic information of an image solely through data fidelity remains difficult. To address this, we propose a novel generalized smoothing model that integrates a data fidelity term, a structural fidelity term, and a sparse smoothing term to enhance edge-preserving smoothing performance. The structural fidelity term is designed to ensure that the gradient of the output image closely matches the preprocessed gradient of the input image, thereby achieving structural fidelity. Simultaneously, the smoothing term with sparse regularity constraints is employed to smooth detailed information while preserving significant structural elements. Extensive experimental validation demonstrates that our proposed method outperforms existing techniques and is applicable across various fields, including image smoothing, detail enhancement, edge extraction, HDR tone mapping, clip-art compression artifact removal, and image abstraction. The source code is available at: https://github.com/kxZhang1016/EPSGEF .},
  archive      = {J_PAAA},
  author       = {Long, Jianwu and Zhang, Kaixin and Liu, Yuanqin and Chen, Shuang and Luo, Qi},
  doi          = {10.1007/s10044-025-01539-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Edge-preserving image smoothing via sparse gradient enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01540-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of stains in digital pathological images is prone to be affected by variations in imaging protocols, dyes, scanners, and illumination conditions. This inconsistency will prevent robustness and generalization of computer-aided diagnostic algorithms. Thus, many researchers have proposed efficient methods to normalize different stained images, among which the CycleGAN method prevails. In practice, we have found that this method can cause the background region to be mistaken for the foreground region or can result in the nucleus being considered cytoplasm, a phenomenon we refer to as Stain Region Inversion (SRI). To address the problem and improve its structure-preserving performance in stain normalization tasks, this paper proposes a novel stain normalization method called Structure-Preserving Self-Attention CycleGAN (SPSA-CycleGAN), which enhances the performance of CycleGAN in processing histological and cytological images. We demonstrate how to utilize multi-head self-attention to capture local features and use grey-scaled images to address the issue of SRI, enhancing the pixel-level structure-preserving capability of the original CycleGAN model. Our method is then verified in five experiments and compared with six other state-of-the-art stain normalization methods. The experimental results demonstrated that our SPSA-CycleGAN has better or comparable performance compared to all the other methods. Code available at: https://github.com/Smile-We/SPSA-CycleGAN},
  archive      = {J_PAAA},
  author       = {Chen, Zheng and Jiang, Peng and Duan, Wensi and Wang, Lang and Li, Cheng and Wang, Junfeng and Liu, Juan},
  doi          = {10.1007/s10044-025-01540-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced group convolution: An improved group convolution based on approximability estimates. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01542-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the balanced group convolution over other variants of group convolution.},
  archive      = {J_PAAA},
  author       = {Lee, Youngkyu and Park, Jongho and Lee, Chang-Ock},
  doi          = {10.1007/s10044-025-01542-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced group convolution: An improved group convolution based on approximability estimates},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction. <em>PAAA</em>, <em>28</em>(4), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01499-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is a challenging task due to the subtle and local movements of facial muscles. To get rid of redundant video frames, studies have been conducted on the use of apex frames for MER. However, these studies mainly focused on 2D frontal apex frames, ignoring side face information, which can result in insufficient extraction of features?. To address this issue, we propose a novel dual-stream network with coordinate attention (DSNCA) framework for MER, which comprises a multi-view coordinate attention module (MVCAM) and an apex frame coordinate attention module (AFCAM). The MVCAM initially employs 3D face reconstruction to acquire unobstructed multi-view images that are rich in micro-expression information and enhanced with geometric details. Subsequently, it adaptively learns micro-expression features from multiple angle views with coordinate attention, thus leveraging supplementary face information. In contrast, the AFCAM aims to adaptively locate key regions where micro-expressions occur, thereby minimizing redundant information. The proposed method achieved UF1 and UAR of 76.45, 75.16, 59.21, 59.6, 62.77, 62.06, 66.74, and 69.31 on the Chinese Academy of Sciences Micro-expression DatabaseII (CASMEII), the Spontaneous Micro-expression Corpus (SMIC), the Spontaneous Actions and Micro-Movements (SAMM), and the composite databases, respectively. The code is available for research purposes at https://github.com/pennypppp/DSNCA .},
  archive      = {J_PAAA},
  author       = {Ma, Pianpian and Chen, Jingying and Liu, Xu and Liu, Xiaodi},
  doi          = {10.1007/s10044-025-01499-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multi-feature fusion trajectory prediction method. <em>PAAA</em>, <em>28</em>(4), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01541-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is critical in fields such as autonomous driving and robot navigation. It enables autonomous vehicles and robots to make proactive decisions, avoid collisions with pedestrians, and ensure pedestrian safety. Some methods utilize Generative Adversarial Networks (GAN) for trajectory prediction. However, there are still two major drawbacks that hinder the improvement of trajectory prediction accuracy. 1) The complex interactions among multiple pieces of information have not been fully captured, preventing the information from being fully utilized. 2) The uncertainty of network outputs has not been fully modeled and processed. To address these challenges and enhance the model’s adaptability to unknown data, especially to provide an effective solution for managing complex tasks, a novel multi-feature fusion trajectory prediction method is introduced. Firstly, this method designs a new feature fusion submodule. It considers the correlation between multi-feature information through the Power Average (P-A) operator. It also enables the model to better learn the differences among multiple feature information and effectively capture the complex interactions among them. Then, the uncertainty of the network’s output is modeled by incorporating a conservative output state through triangular fuzzy numbers, enhancing the flexibility of the network. Evidence theory is crucial in practical applications due to its effectiveness in representing and processing uncertain information. Therefore, we adopt the belief measure of evidence theory as the uncertainty output of the network. Finally, considering that evidence distance can accurately assess the differences between uncertain information, we embed the evidence distance formula into the loss function of the model as a loss term. This loss term is used to address the discrepancy between the model’s predicted output and the actual distribution, thereby optimizing the information loss of the model and enhancing prediction accuracy. Experiments conducted on the public ETH/UCY datasets reveal that the proposed method achieves higher accuracy, particularly for trajectory prediction in complex situations. Additionally, we have not significantly reduced the model’s efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Tian and Wang, Gang and Lai, Jian and Wang, Yang},
  doi          = {10.1007/s10044-025-01541-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new multi-feature fusion trajectory prediction method},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01543-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling functional brain networks is essential for revealing the functional mechanisms of the human brain. Deep Neural Network (DNN) models have been widely employed for extracting multi-scale spatiotemporal features from functional magnetic resonance imaging (fMRI) data. However, current approaches face two fundamental challenges. Firstly, existing deep neural network-based approaches exhibit significant limitations in learning cross-task common representations when dealing with the variable sequence length characteristics inherent in task-fMRI multi-task data. Secondly, existing approaches often neglect the dynamic variability of neural activity across different time points in fMRI data. To overcome these challenges, a novel framework based on $$\:{\mathbf{L}}_{2}$$ -Normalized Attention Fully Convolutional Recurrent Autoencoder ( $$\:{\mathbf{L}}_{2}$$ -FCRAAE) is proposed for modeling hierarchical functional brain networks (FBNs). Specifically, the $$\:{\mathbf{L}}_{2}$$ -FCRAAE is trained in an unsupervised manner, where the autoencoder architecture guides the attention modules to focus on task-activated regions. The architecture incorporates two synergistic design principles: First, its fully convolutional recurrent structure inherently adapts to variable-length tfMRI time series while effectively capturing long-range temporal dynamics and recognizing brain state transitions. Second, the integrated $$\:{\mathbf{L}}_{2}$$ -normalized temporal-channel attention module weights task-relevant neural activation patterns, substantially enhancing representational capacity. Comprehensive experiments demonstrate that the proposed $$\:{\mathbf{L}}_{2}$$ -FCRAAE exhibits superior capability and generalizability in characterizing spatial and temporal patterns of FBNs in a hierarchical manner. Overall, this study presents a novel approach for understanding the hierarchical organization of functional brain architecture. The code for this paper is available at: https://github.com/beiweizai111/FCAAE .},
  archive      = {J_PAAA},
  author       = {Liu, Huan and Cui, Puwang and Zhang, Minye and Li, Li and Han, Fei},
  doi          = {10.1007/s10044-025-01543-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01546-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing technology has gained widespread attention due to its low storage cost and high retrieval efficiency. Although existing cross-modal hashing methods have achieved good retrieval results, there is still the problem of facing modal heterogeneity leading to semantic separation. Therefore, this paper proposes a deep hashing method based on prototype-aware hardness-weighted, which uses the CLIP model after fine-tuning the large language model as a feature extractor to better obtain the feature information of the two modalities. And a set of loss functions are designed to better handle difficult samples by prototype-aware hardness-weighted loss to ensure that the data can be embedded in the appropriate location, bringing relevant data closer and pulling irrelevant data away. After conducting experiments on three datasets and comparing with some advanced cross-modal retrieval methods in recent years, it can be shown that our proposed DPHS method has excellent performance. The code and datasets used in this article can be obtained from https://github.com/chmy180/dphs-main .},
  archive      = {J_PAAA},
  author       = {Ge, Bin and Cheng, Mengyan and Xia, Chenxing},
  doi          = {10.1007/s10044-025-01546-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot segmentation network based on class-aware prototype fusion. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01547-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing few-shot segmentation methods based on the support-query matching framework suffer from insufficient support information, where the limited number and coverage of annotated samples often produce prototypes that are incomplete or contaminated by background features, leading to incomplete activation of target regions in the query image and false activation of irrelevant areas. To address this issue, we propose a class-aware prototype fusion network (CAPFN) for few-shot segmentation, comprising a class-aware module (CAM) and a prototype fusion module (PFM). The CAM extracts class-specific information by jointly utilizing support features, masks, and preliminary query predictions, thereby guiding the model to attend precisely to target regions. To further mitigate semantic gap between the support and query domains, the PFM constructs a hybrid prototype by fusing support-derived and query-derived prototypes based on initial predictions. This fusion enhances prototype quality, reduces information loss, and improves the discriminative capacity of query activation. Extensive experiments on benchmark datasets (PASCAL-5 $${}^i$$ and COCO-20 $${}^i$$ ) demonstrate the superiority of our approach. Notably, with ResNet50 as the backbone, our model achieves a 2.32% improvement in mIoU over the baseline on the COCO-20 $${}^i$$ dataset. The code is available at https://github.com/ShuoWang011/CAPFN.git .},
  archive      = {J_PAAA},
  author       = {Yang, Aiping and Wang, Shuo and Sang, Zijia and Zhou, Yaran},
  doi          = {10.1007/s10044-025-01547-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Few-shot segmentation network based on class-aware prototype fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localised ensemble learning (LEL) – A localised approach to class imbalance. <em>PAAA</em>, <em>28</em>(4), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01545-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a persistent challenge in machine learning, often degrading model performance by skewing predictions toward majority classes. Traditional approaches typically focus on global correction strategies, which may overlook important Localised irregularities within the data. These global methods may fail to adapt to the varying characteristics of individual samples, limiting their effectiveness in complex real-world scenarios.To address these limitations, we propose Localised Ensemble Learning (LEL), a novel framework that incorporates local structural information into the learning process. LEL begins by applying K-Nearest Neighbors (KNN) to assign each sample a Sample Type based on specific rules that capture neighbourhood distribution, distance-based imbalance, and sample quality. This Sample Type feature is then used to partition the dataset into distinct subsets, each of which is treated using tailored imbalance mitigation strategies. Individual models are trained on these subsets, and their predictions are integrated into a final ensemble, allowing LEL to address different forms of localised imbalance in a principled and modular way. The effectiveness of LEL is validated through a comprehensive evaluation against global correction strategies namely SMOTE, NOGAN, Cost-Sensitive Learning (CSL) and Ensemble methods, across multiple metrics including Recall, Precision, F1 Score, Kappa, G Mean and quantitatively. Statistical significance of the results is assessed using paired T-tests and Wilcoxon signed rank test. SHAP (SHapley Additive exPlanations) values are employed to analyze feature contributions, revealing the Sample Type feature as a critical determinant of model performance. Additionally, an ablation study highlights the impact of key parameters, such as the $$ k $$ value in KNN providing further insights into the robustness and adaptability of the LEL framework. Experimental results demonstrate that LEL consistently outperforms global approaches across all tested classifiers, including Random Forest, Decision Tree, XGBoost, and Naive Bayes. LEL achieves statistically significant improvements in recall and precision, underscoring its ability to handle localised forms of imbalance effectively which translates to global imbalances. The findings emphasize the importance of addressing localised data defects and leveraging features like Sample Type, which capture complex relationships and enhance predictive accuracy.},
  archive      = {J_PAAA},
  author       = {Olabisi, Olayemi and Maurya, Lalit and Bader-El-Den, Mohamed},
  doi          = {10.1007/s10044-025-01545-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Localised ensemble learning (LEL) – A localised approach to class imbalance},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised graph convolutional deep embedded clustering. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01548-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is an important field of study in data analysis, and semi-supervised clustering, as a sub-category of clustering, has received extensive attention for significantly improving performance with only a small amount of prior information introduced. In recent years, many studies have incorporated semi-supervision into their field. Although good results have been obtained, most of these methods, when introducing additional information (for example, multi-view information), could not make full use of the given knowledge to mine intra-class similarity, and achieved suboptimal performance. For this situation, we put forward a new method of Semi-supervised Graph Convolutional Deep Embedding Clustering (SGDEC), which is a single-view algorithm. Specifically, an autoencoder network (implemented by linear layers) is pre-trained. We further propose to employ a Graph Convolutional Network (GCN) to learn the data similarity based on prior pairwise information, and make clear the training direction of the latent variable. Clustering label assignment and feature representation can be optimized by clustering loss alone. Unlike previous methods that only use pairwise information as a constraint in neural network, this architecture can make full use of pairwise information and capture similarities from it, optimizing the entire network for better clustering performance. We conduct many experiments on four popular benchmark datasets of images using the same set of learning rates, and it is shown that our SGDEC achieves a significant improvement over the new single-view algorithms, even better than the latest multi-view methods.},
  archive      = {J_PAAA},
  author       = {Cao, Chao and Li, Mengli and Li, Chungui},
  doi          = {10.1007/s10044-025-01548-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised graph convolutional deep embedded clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSDFSA: A two-stage dynamic feature selection method based on attention mechanism for multi-sensor data. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01550-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-sensor behavior recognition, data redundancy and irrelevant features severely impair model training and optimization, leading to degraded performance. Feature selection enhances model performance by reducing dimensionality and retaining representative features. Existing dynamic selection methods iteratively update feature weights via forward propagation but are sensitive to initial parameters, which may result in early selections that reflect only local feature importance. This issue hinders the characterization of global contributions, undermines subset representativeness, and impairs model generalization. To address these challenges, we propose a Two-Stage Dynamic Feature Selection Approach (TSDFSA) based on attention mechanisms. TSDFSA adopts a two-stage feature selection strategy comprising a pre-training stage and a dynamic selection stage. The pre-training stage alleviates the impact of initial parameter settings on the feature selection process. In the dynamic selection stage, a soft–hard coupling gating mechanism is employed to adaptively adjust feature weights. This mechanism progressively refines the feature subset and avoids the limitations of one-off selection. The Hadamard product-based weighting strategy further improves the discrimination of feature contributions. Subsequently, a binary feature mask is constructed using one-hot encoding to eliminate irrelevant features. Experimental results on two multi-sensor datasets demonstrate that TSDFSA significantly enhances behavior recognition accuracy, achieving 94.10 and 91.01, respectively. Compared to other deep learning-based feature selection methods, TSDFSA achieves superior performance, confirming its effectiveness in both feature selection and behavior recognition.},
  archive      = {J_PAAA},
  author       = {Zhang, Guozhi and Dai, Gaoyang and Liu, Zixuan and Shi, Jiajia and Lin, Yuanzhe},
  doi          = {10.1007/s10044-025-01550-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TSDFSA: A two-stage dynamic feature selection method based on attention mechanism for multi-sensor data},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Internal–external networks for image quality improvement. <em>PAAA</em>, <em>28</em>(4), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01551-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of deep learning has led to a notable improvement in super resolution (SR) performance in recent years. Because supervised deep learning SR methods often rely on training data generated by an assumed or predetermined degradation model, they perform exceptionally well in the ideal scenario, where the degradation model of a test low-resolution image does, in fact, conform with the assumed model (e.g., bicubic down-scaling) without unknown parameters like sensor noise, non-ideal point spread function (PSF), etc. However, this ideal setting is rarely suitable for actual low-resolution photographs. In this paper, we propose a hybrid method for the SR problem that considers both internal information to a given image and exterior information obtained by a pretrained SR network. Consequently, a hybrid network for SR is produced, which utilizes both information channels and adapts to the given image (and possibly its specific degradation). According to the experimental results on the standard dataset, the proposed approach has improved PSNR and SSIM. The maximum values of the two PSNR and SSIM measurements in the unknown downscaling kernel case are 29.50 and 0.93 respectively. The results of the experiments indicate that the proposed approach outperforms the state-of-the-art methods, especially when the degradation model is ambiguous or not ideal. Our source code is available at https://github.com/cvloc/FusionNet_SR .},
  archive      = {J_PAAA},
  author       = {Loc, Cu Vinh and Hai, Nguyen Thanh and Viet, Truong Xuan and Viet, Tran Hoang and Thao, Le Hoang and Viet, Nguyen Hoang},
  doi          = {10.1007/s10044-025-01551-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Internal–external networks for image quality improvement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dile: A distribution-based incremental learning approach. <em>PAAA</em>, <em>28</em>(4), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01552-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification has thrived with the arrival of Deep Learning. However, the learned models are mostly static (cannot handle new classes), require a large number of images and long training times; and, in general, heavily depend on the user who defines the classes and provides labeled training data. Recently, class incremental learning has gained popularity by removing the need to retrain a model once data from new classes is available. Most incremental systems know when new classes are given and focus on not decreasing their global performance as these become available. In this paper, we describe DILE, an incremental learner based on distributions of embedded features and their comparisons using the Fréchet distance. DILE uses a limited amount of memory per class by using statistics of these feature vectors. We also introduce an incremental tree-based representation that is gradually built as more classes are known to the system, and which significantly reduces the classification times while placing semantically closer classes in the same branches of the tree. We tested the proposed approach on several databases with very competitive performance against state-of-the-art systems. Additional experiments were performed to see how DILE can work with a single image per class and automatically obtaining additional data from the Internet, and how well DILE can work if it has no knowledge if the examples are from a known or unknown class.},
  archive      = {J_PAAA},
  author       = {Morales, Eduardo F. and Herrera-Vega, Javier and Serrano-Cuevas, Jonathan and Aburto, Yareli and Sucar, L. Enrique and Escalante, Hugo J.},
  doi          = {10.1007/s10044-025-01552-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dile: A distribution-based incremental learning approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exacter set-level nonlocal attention mechanism based on learning weights for image denoising. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01553-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models based on non-local attention (NLA) have achieved remarkable success in image denoising. The success of NLA stems from its ability to capture long-range interactions guided by both the data itself and learnable parameters within the attention mechanism. Building on the classical insight from traditional non-local filtering methods, it has been observed that more accurate calculation of interactions or similarities between tokens can significantly improve denoising performance. However, existing non-local attention networks designed for denoising have failed to consider the trustworthiness of these interaction computations. In this paper, motivated by the statistic principles underlying similarity calculation, we propose a exacter set-level non-local attention based on the learning weights (ESNLA). This approach leverages similar tokens within the local neighborhood to mitigate the impact of noise and enhance the robustness of similarity calculations under noisy conditions. Designed as an efficient modular component, ESNLA can be seamlessly integrated into any deep convolutional architecture for image denoising. Ablation studies validate the superiority of ESNLA over existing NLA modules. By embedding multiple ESNLA blocks into a ResNet backbone, we further construct a deep ESNLA Network (ESNLANet). Extensive experiments demonstrate that ESNLANet achieves highly competitive denoising performance, outperforming a wide range of state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Chen, Dai-Qiang and Chen, Ying-Chun},
  doi          = {10.1007/s10044-025-01553-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exacter set-level nonlocal attention mechanism based on learning weights for image denoising},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient framework for text-to-image retrieval using complete feature aggregation and cross-knowledge conversion. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01554-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven person retrieval aims to find a particular person within an array of images using a natural language description of that individual. The main challenge in this task lies in calculating a similarity score that measures the likeness between the person’s image and the given description. This procedure involves uncovering the intricate, underlying connections between various levels of image sub-regions and textual phrases. Previous efforts have sought to tackle this issue by utilizing individual pre-trained models for visual and textual data to extract features. However, these methods lack essential alignment capabilities regarding efficient multimodal data matching. Moreover, these approaches rely on prior information to investigate explicit part alignments, potentially causing the disruption of information within the same modality. To increase the performance of existing state-of-the-art text-to-image retrieval systems, we proposed a novel technique aimed at enhancing the effectiveness of the text-image pedestrian matching pipeline, referred to as Complete Feature Aggregation and Cross-Knowledge Conversion (CFA-CKC), which integrates diverse supervisory signals from various textual and visual perspectives. Our proposed algorithm achieves new state-of-the-art results on all three public datasets, including CUHK-PEDES, ICFG-PEDES, and RSTPReid. Experimental results show that our CFA-CKC outperforms the state-of-the-art methods by up to 6.0% in rank-1 accuracy. The code and pre-trained models are available at this link https://github.com/AIVIETNAMResearch/Pedestrian-Matching .},
  archive      = {J_PAAA},
  author       = {Ngo, Bach Hoang and An, Minh-Hung and Anh, Khoa Nguyen Tho and Bui, Minh-Duc and Dinh, Quang-Vinh and Nguyen, Vinh Dinh},
  doi          = {10.1007/s10044-025-01554-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An efficient framework for text-to-image retrieval using complete feature aggregation and cross-knowledge conversion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Future occupancy guidance under multi-view collaboration for trajectory prediction. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01555-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle trajectory prediction is a critical component of autonomous driving technology, aiming to predict plausible future trajectories for surrounding agents in dynamic traffic scenarios. However, recent approaches merely fit certain elements of interest using the agent’s local poses, which is insufficient for effectively perceiving large-scale static maps. Meanwhile, some models rely on historical or current map states, assigning excessive attention to traversed road segments. This leads to severe limitations in vehicles’ autonomous decision-making capabilities. Furthermore, single-stage prediction fails to evaluate intersecting future spatial distributions, consequently inducing hazardous behaviors. To address these challenges, this paper proposes a Future Occupancy Guidance(FOG) framework based on multi-view collaboration. Firstly, the model leverages edge relationships to construct spatio-temporal interactions among elements across multiple viewpoint graphs. Then, we generate future occupancy markers along the driving direction and adaptively shift the map query perspective to accommodate the distance disparities caused by agents traveling at different speeds, thereby providing road-level guidance. Finally, we design a refinement module to further correct the rough predicted trajectories with collision risks through multi-head attention. The experimental results on Argoverse 1 and Argoverse 2 motion forecasting benchmarks demonstrate that our FOG achieves outstanding performance in reducing both FDE and MR metrics. The code implementation is available at: https://github.com/alien-HL/FOG.git},
  archive      = {J_PAAA},
  author       = {Sang, Haifeng and Huang, Lai},
  doi          = {10.1007/s10044-025-01555-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Future occupancy guidance under multi-view collaboration for trajectory prediction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point-pad: Point cloud upsampling with kernel representation and attention. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01558-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor generated point clouds are often noisy and sparse, which pose challenges in various downstream tasks. Recent approaches have tackled the problem through various upsampling approaches. Existing methods suffer from outlier and variable point cloud density leading to loss in object geometry. This work introduces Point-PAD, an architecture which utilizes kernel points and positional encodings in order to encode the local geometry, which is then processed by a multi-head attention block for adjusting predicted points with the entire architecture supported by a density aware loss function. The architecture, when tested on the PU-GAN dataset, achieves state-of-the-art results. The model outperformed the state-of-the-art by 3.22% and was able to upsample points even at sharp curves while preserving the geometry. We also report results on one of the first outdoor dataset comprising real life LiDAR (Light Detection and Ranging) scans of outdoor scenes for this task. Our model outperforms the existing work, proving its generalization, which is crucial for applications related to digital twins and autonomous systems. The code is available at: https://github.com/geoai4cities/PointPAD},
  archive      = {J_PAAA},
  author       = {Verma, Sameer and Kumar, Vaibhav},
  doi          = {10.1007/s10044-025-01558-y},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Point-pad: Point cloud upsampling with kernel representation and attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
