<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam">MAM - 3</h2>
<ul>
<li><details>
<summary>
(2025). Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival. <em>MAM</em>, <em>35</em>(4), 1-23. (<a href='https://doi.org/10.1007/s11023-025-09736-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of large language models (LLMs) into our conversational infrastructure presents a critical inflection point for democratic practice. While contemporary digital platforms systematically erode transitional conversational spaces—interfaces between private intuition and public deliberation where tentative thoughts can be explored—this paper argues that specialized LLM interfaces could potentially reconstruct these essential democratic environments. I propose a design framework for ‘transitional conversational spaces’ that leverages uncertainty expression not merely to prevent unwarranted epistemic confidence but to create communicative environments conducive to democratic capability development. Drawing on theories of democratic deliberation and moral perception, this paper distinguishes between epistemic uncertainty (addressable through additional information) and hermeneutic uncertainty (concerning the inherently contestable nature of interpretation). The proposed framework emphasizes ‘ensemble interfaces’ that make visible the contingent nature of value judgments by presenting outputs from multiple models trained on different datasets. The design principles outlined challenge tokenistic participation by advocating for substantive participatory infrastructure with features like ‘tinkerability’—enabling communities to experiment with system configurations—and mechanisms that counter designer-centric development models. These principles stand in contrast to conventional ‘participatory AI’ approaches that treat engagement as merely instrumental to system optimization rather than as constitutive of democratic practice. This paper does not claim to solve all challenges of democratic participation but rather identifies one valuable design direction that could potentially enhance our collective capacity for exploratory dialogue. Implementation would require institutional transformations that align technological development with democratic values beyond current procedural approaches to AI governance.},
  archive      = {J_MAM},
  author       = {Delacroix, Sylvie},
  doi          = {10.1007/s11023-025-09736-x},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented democracy in action: AI systems for legislative innovation in the italian parliament. <em>MAM</em>, <em>35</em>(4), 1-29. (<a href='https://doi.org/10.1007/s11023-025-09743-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Debates on AI governance often focus on regulating risks. This article shifts perspective to examine how AI can augment democratic processes, presenting a critical analysis of the Italian Chamber of Deputies’ pioneering AI initiative. We detail the 2024 project that produced three prototype systems—NORMA (legislative analysis), MSE (drafting assistance), and DepuChat (citizen engagement)—which embed principles of transparency, human oversight, and privacy-by-design. We introduce the project’s third way development model, a public-academic partnership that contrasts with full in-house or commercial approaches. Using this initiative as a critical case study, we move beyond merely applying the concept of “augmented democracy”. We argue that this real-world implementation reveals key tensions between the goals of efficiency and the preservation of deliberative friction essential to democratic practice. The analysis highlights risks, including staff deskilling and the digitalisation of inequalities, and situates the Italian approach in an international context. We conclude by offering a theoretical refinement of augmented democracy, informed by the practical lessons of its implementation in a complex legislative environment.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano and Ascani, Anna},
  doi          = {10.1007/s11023-025-09743-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Augmented democracy in action: AI systems for legislative innovation in the italian parliament},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clarifying the opacity of neural networks. <em>MAM</em>, <em>35</em>(4), 1-30. (<a href='https://doi.org/10.1007/s11023-025-09745-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Deep Neural Networks (DNNs) can perform a wide range of tasks at human or greater-than-human level of competence, they are also notoriously opaque. This paper aims to shed light on both the specific nature of this opacity and what it would take to fully or partially remove it. We begin by drawing a clarificatory distinction between two basic dimensions of opacity of complex systems – internal and relational – and explain how various kinds of opacity invoked in recent discussions of DNNs can be understood in terms of these basic dimensions. We then discuss the prospects of removing opacity from DNNs following the methods of two subfields of research in AI – mechanistic interpretability and explainable artificial intelligence (XAI). We finish with a critical discussion of the relevance of what Sullivan (2022a) calls ‘link uncertainty’.},
  archive      = {J_MAM},
  author       = {Raleigh, Thomas and Knoks, Aleks},
  doi          = {10.1007/s11023-025-09745-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Clarifying the opacity of neural networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
