<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NPL</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="npl">NPL - 10</h2>
<ul>
<li><details>
<summary>
(2025). Detecting bitcoin sentiment: Leveraging language model applications in sentiment analysis for bitcoin price prediction. <em>NPL</em>, <em>57</em>(5), 1-25. (<a href='https://doi.org/10.1007/s11063-025-11787-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Bitcoin continues to establish itself as a global asset and discussions around relevant regulations become more active, there is an increasing demand for a comprehensive price prediction framework. To address this necessity, this study aims to enhance the accuracy of Bitcoin price predictions by integrating sentiment information with technical indicators, on-chain data, and cryptocurrency price data. Recognizing Bitcoin’s sensitivity to market sentiment, the proposed framework incorporates sentiment features derived from both lexicon-based methods and large language models. As unsupervised sentiment tools can introduce label noise particularly in domain-specific or ambiguous financial contexts, this study combines the outputs of multiple sentiment models at the feature level to construct a more stable representation. This design improves the robustness of downstream regression performance and distinguishes the framework from previous hybrid models that relied on a single sentiment source without component-wise evaluation. Experimental results using a dataset spanning 2700 days showed that the long short-term memory (LSTM) model with a 3-day window achieves the best performance with mean absolute percentage error (MAPE) of 3.93% and R-squared value of 0.99106. Feature importance analysis further demonstrates sentiment index as the most impactful feature, as excluding it resulted in the largest decline in predictive accuracy. Additionally, the model's performance was evaluated under four major volatility periods, revealing MAPE values ranging from 1.49 to 4.03%, highlighting the framework’s practical capability in rapidly adapting to sudden market shifts. In summary, integrating sentiment information attained from multiple language models significantly enhanced prediction accuracy compared to single source approaches. These findings highlight the framework’s practical value for sentiment-informed investment strategies and risk alerts, with a modular design that enables flexible adaptation and potential integration into automated trading systems.},
  archive      = {J_NPL},
  author       = {Jung, Hae Sun and Lee, Haein and Kim, Jang Hyun},
  doi          = {10.1007/s11063-025-11787-1},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Neural Process. Lett.},
  title        = {Detecting bitcoin sentiment: Leveraging language model applications in sentiment analysis for bitcoin price prediction},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDNet: A novel image focus discriminative network for enhancing camera autofocus. <em>NPL</em>, <em>57</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11063-025-11788-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate activation and optimization of autofocus (AF) functions are essential for capturing high-quality images and minimizing camera response time. Traditional contrast detection autofocus (CDAF) methods suffer from a trade-off between accuracy and robustness, while learning-based methods often incur high spatio-temporal computational costs. To address these issues, we propose a lightweight focus discriminative network (FDNet) tailored for AF tasks. Built upon the ShuffleNet V2 backbone, FDNet leverages a genetic algorithm optimization (GAO) strategy to automatically search for efficient network structures, and incorporates coordinate attention (CA) and multi-scale feature fusion (MFF) modules to enhance spatial, directional, and contextual feature extraction. A dedicated focus stack dataset is constructed with high-quality annotations to support training and evaluation. Experimental results show that FDNet outperforms mainstream methods by up to 4% in classification accuracy while requiring only 0.2 GFLOPs, 0.5 M parameters, a model size of 2.1 MB, and an inference time of 0.06 s, achieving a superior balance between performance and efficiency. Ablation studies further confirm the effectiveness of the GAO, CA, and MFF components in improving the accuracy and robustness of focus feature classification.},
  archive      = {J_NPL},
  author       = {Kou, Chenhao and Xiao, Zhaolin and Jin, Haiyan and Guo, Qifeng and Su, Haonan},
  doi          = {10.1007/s11063-025-11788-0},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Neural Process. Lett.},
  title        = {FDNet: A novel image focus discriminative network for enhancing camera autofocus},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-band video deblurring via efficient chunked additive attention and dynamic channel adaptive module. <em>NPL</em>, <em>57</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11063-025-11798-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based deblurring methods perform well in the visible spectrum but suffer from high computational costs due to complex network designs. For infrared images, texture deficiency and edge degradation hinder effective feature learning. To overcome these problems, a Dynamic Channel Adaptive Network (EDCANet) is proposed, which is based on the Dynamic Channel Adaptive Module (DCAM) and Efficient Chunked Additive Attention (ECAA) integrated into a multi—scale network with an encoder—decoder architecture. The DCAM adaptively recalibrates input frame contributions through dynamic channel weighting. Additionally, the ECAA mechanism that decomposes attention operations into partitioned spatial and frequency domains is adopted to enhance contour recovery performance. Images across multiple spectral bands are used in experiments to verify the generalization ability of the proposed method and the effectiveness of structural contour and fine-grained detail recovery. Experimental results show that the EDCANet can process images in real time at a speed of 100 frames per second, with the Peak Signal-to-Noise Ratio (PSNR) exceeding 31 dB and the Structural Similarity Index (SSIM) surpassing 0.92 on infrared datasets. For visible light datasets, its PSNR reaches above 32 dB and SSIM exceeds 0.93.},
  archive      = {J_NPL},
  author       = {Ao, Yongqi and Zhu, Deyan and Li, Chengcheng and Zhang, Yufan and Xu, Jiayi},
  doi          = {10.1007/s11063-025-11798-y},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Neural Process. Lett.},
  title        = {Multi-band video deblurring via efficient chunked additive attention and dynamic channel adaptive module},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STP-Cube—Multivariate time series forecasting based on spatiotemporal correlations and periodicity. <em>NPL</em>, <em>57</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11063-025-11799-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of multivariate time series (MTS) forecasting aims to utilize historical time series data to predict future trends and variations. MTS data possesses numerous latent features, with spatiotemporal correlations and periodicity being particularly significant. Most existing methods focused on either spatiotemporal correlations or periodicity, limiting their effectiveness. The paper proposes a novel STP-cube model by simultaneously capturing both, offering a more robust solution for MTS forecasting. MTS data is first transformed into a three-dimensional structure—the cube, through special stacking. Then graph convolution is utilized to capture spatiotemporal dependencies among sensors and Conv2Ds in another dimension to extract periodic features within sensors. By incorporating both features, the forecasting effectiveness is significantly enhanced. STP-cube model shows robust performance across six public MTS datasets, particularly excelling on the ETTh1 dataset where it reduces MSE by 0.177 and MAE by 0.084 comparing to the SOTA model.},
  archive      = {J_NPL},
  author       = {Zhang, Yidong and Jing, Jie and Liu, Luqi and Lan, Chengming and Shi, Peng},
  doi          = {10.1007/s11063-025-11799-x},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {STP-Cube—Multivariate time series forecasting based on spatiotemporal correlations and periodicity},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and exponential lag-synchronization of a class of neural network with state dependent and distributed delays over a time scale. <em>NPL</em>, <em>57</em>(5), 1-24. (<a href='https://doi.org/10.1007/s11063-025-11802-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the stability and synchronization challenges of neural networks with distributed and state-dependent delays on a temporal domain, leveraging the powerful framework of time scales theory. By formulating the problem within this unified framework, we enable applications to both uniform and non-uniform time domains. Our investigation begins with a thorough analysis of the local exponential stability of the zero solution, employing a combination of pure analysis method, reduction to absurdity technique, and time scale theory. We derive a set of sufficient conditions that guarantee local exponential stability of neural networks with distributed and state-dependent delays. Furthermore, we examine exponential lag synchronization results, utilizing a range of analytical tools, including time scale theory, matrix norm theory, unified matrix-measure theory, and the Halanay inequality. To demonstrate the efficacy and broad applicability of our findings, we present a simulated example on random time scales. Specifically, the time scales theory allows us to effectively handle time scales by providing a unified framework that can seamlessly integrate both continuous and discrete time domains, thereby enabling the analysis of complex systems with varying time scales. Moreover, our approach leverages the flexibility of time scales theory to accommodate non-uniform time domains, making it an ideal tool for tackling real-world problems with intricate temporal dynamics.},
  archive      = {J_NPL},
  author       = {Abbas, Muhammad and Zada, Akbar and Popa, Ioan-Lucian and Kallekh, Afef},
  doi          = {10.1007/s11063-025-11802-5},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {Stability and exponential lag-synchronization of a class of neural network with state dependent and distributed delays over a time scale},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From task-aware to task-agnostic parameter isolation for incremental learning. <em>NPL</em>, <em>57</em>(5), 1-28. (<a href='https://doi.org/10.1007/s11063-025-11792-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitigating catastrophic forgetting in continual learning is a long-standing challenge for artificial intelligence. Often, methods used to alleviate forgetting make use of either rehearsal buffers, pretrained backbones or task-id knowledge. However, these requirements result in severe limitations regarding scalability, privacy preservation, and efficient deployment. In this work, we explore how to eliminate the need for such requirements in incremental learning approaches based on parameter isolation. We propose Low Interference Feature Extraction Subnetworks (LIFES), a method that learns a subnetwork per task and uses all of them concurrently at inference time. This solution minimises requirements; however, it creates the need to address certain challenges. To formalize them, we break down the catastrophic forgetting problem into 4 distinct causes, and address them with a novel lateral classifiers regularization, weight standardization, and subnetwork interference connection pruning. Specifically, the use of lateral classification shows very promising results, forcing the model to learn distributions with higher inter-class distance. Using these components, LIFES achieves competitive results in standard task-agnostic scenarios, demonstrating the viability of this new perspective for parameter isolation, which has minimal requirements. Finally, we discuss how future work can improve this new paradigm further, and how the strategies defined can be complementary to other approaches.},
  archive      = {J_NPL},
  author       = {Vicente-Sola, Alex and Kirkland, Paul and Di Caterina, Gaetano and Bihl, Trevor J and Masana, Marc},
  doi          = {10.1007/s11063-025-11792-4},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Neural Process. Lett.},
  title        = {From task-aware to task-agnostic parameter isolation for incremental learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FEFM-YOLO11: Underwater object detection algorithm based on improved YOLO11. <em>NPL</em>, <em>57</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11063-025-11805-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection technology is extensively applied in subsea resource exploration and benthic environmental monitoring; however, the underwater environment presents significant challenges due to limited visibility caused by insufficient illumination and turbid water quality, severely impeding detection tasks. To address these challenges and enhance precision, this paper proposes FEFM-YOLO11—an enhanced YOLO11-based underwater object detection algorithm. Primarily, the algorithm substitutes two C3K2 modules in the backbone with a Feature Enhancement Module (FEM). Employing a multi-branch dilated convolutional structure, the FEM increases feature diversity, expands the network’s local receptive field, and enhances semantic representation of small objects, thereby strengthening detection capabilities. Subsequently, a Context-Aware Fusion Module (CAFM) is introduced in the Neck, which effectively models global and local features through integrated local feature capture (convolutional operations) and global feature extraction (attention mechanisms), consequently improving denoising performance. Furthermore, a dedicated small-target detection layer is incorporated to specifically boost detection performance for smaller underwater objects. Finally, the Wise IoU loss function was used for comprehensive evaluation and performance optimization, and the collaborative integration of these components effectively reduced the problem of missed detections caused by occlusion in dense cluster targets. Experimental results on the URPC2020 dataset demonstrate that the improved algorithm achieves enhancements of 2.0% points in mAP@50 and 1.9% points in mAP@50:95 compared to the baseline, confirming that FEFM-YOLO11 elevates detection precision and validates the feasibility and effectiveness of the proposed methodology.},
  archive      = {J_NPL},
  author       = {Wang, Qi and Liu, Zhichuan},
  doi          = {10.1007/s11063-025-11805-2},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {FEFM-YOLO11: Underwater object detection algorithm based on improved YOLO11},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ghost-connect net: A generalization-enhanced guidance for sparse deep networks under distribution shifts. <em>NPL</em>, <em>57</em>(5), 1-40. (<a href='https://doi.org/10.1007/s11063-025-11758-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse deep neural networks (DNNs) excel in real-world applications like robotics and computer vision, by reducing computational demands that hinder usability. However, recent studies aim to boost DNN efficiency by trimming redundant neurons or filters based on task relevance, but neglect their adaptability to distribution shifts. We aim to enhance these existing techniques by introducing a companion network, Ghost Connect-Net (GC-Net), to monitor the connections in the original network with distribution generalization advantage. GC-Net’s weights represent connectivity measurements between consecutive layers of the original network. After pruning GC-Net, the pruned locations are mapped back to the original network as pruned connections, allowing for the combination of magnitude and connectivity-based pruning methods. Experimental results using common DNN benchmarks, such as CIFAR-10, Fashion MNIST, and Tiny ImageNet show promising results for hybridizing the method, and using GC-Net guidance for later layers of a network and direct pruning on earlier layers. We provide theoretical foundations for GC-Net’s approach to improving generalization under distribution shifts.},
  archive      = {J_NPL},
  author       = {Wisell, Mary and Sekeh, Salimeh},
  doi          = {10.1007/s11063-025-11758-6},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Neural Process. Lett.},
  title        = {Ghost-connect net: A generalization-enhanced guidance for sparse deep networks under distribution shifts},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global dissipative examination of delayed memristive inertial neural networks with uncertain parameters. <em>NPL</em>, <em>57</em>(5), 1-27. (<a href='https://doi.org/10.1007/s11063-025-11784-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present article addresses the dissipative examination of Dynamic Systems, namely Inertial Neural Networks with memristor, parameter uncertainty and delays such as time-varying, distributed. A suitable variable substitution is implemented to convert the inertial system to the first order differential system. Exploiting the concept of matrix measure and properties to the Lyapunov function, a sufficient criteria for dissipative of the dynamical system is achieved through a generalized Halanay Inequality. Concurrently, the globally attractive sets are extracted from the network system with bound. The derived results are new-fangled concerning the dynamical systems with complex- the inertial and memristor term along with the mixed delays and parameter uncertainties. Finally, the numerical simulations are presented for better clarification and testimonial of obtained dissipative criteria with pictorial representation.},
  archive      = {J_NPL},
  author       = {Premalatha, S. and Shanmugapriya, M. M. and Indumathi, P. and Kumar, S. Santhosh},
  doi          = {10.1007/s11063-025-11784-4},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Neural Process. Lett.},
  title        = {Global dissipative examination of delayed memristive inertial neural networks with uncertain parameters},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual decoder mathematical word problem solving model based on lie group intrinsic mean feature matrix. <em>NPL</em>, <em>57</em>(5), 1-24. (<a href='https://doi.org/10.1007/s11063-025-11804-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Math Word Problems (MWP) solving involves language comprehension and mathematical reasoning. Most of the existing models are primarily based on deep learning methods. However, deep learning models often exhibit large model scales (e.g., large number of parameters), high feature dimensions, and an imbalance between accuracy and computational efficiency. To address these issues, We propose a novel dual-decoder model for solving Mathematical Word Problems (MWP), which is constructed using a Lie Group intrinsic mean feature matrix and named the Dual-Decoder Neural Symbolic Machine (DDNSM). This model projects data samples onto a Lie Group manifold to effectively reduce the output feature dimensions of the encoder through a feature matrix built around the Lie Group intrinsic mean, thereby reducing the computational load in the decoding stage and improving model inference efficiency. Additionally, we designed a dual-decoder system, comprising a decoder based on global and local attention mechanisms and another structured around a tree built from the Lie Group intrinsic mean. Extensive experiments on multiple challenging public datasets demonstrate that our model achieves competitive results in MWP tasks, achieving competitive results in MWP tasks with improved accuracy and computational efficiency.},
  archive      = {J_NPL},
  author       = {Jian, Pengpeng and Sun, Tianhao and Ma, Bin and Xi, Hui and Yang, Yangrui},
  doi          = {10.1007/s11063-025-11804-3},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {Dual decoder mathematical word problem solving model based on lie group intrinsic mean feature matrix},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
