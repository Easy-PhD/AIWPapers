<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst">MLST - 17</h2>
<ul>
<li><details>
<summary>
(2025). Physics-inspired self-learning framework for unsupervised depth estimation on non-lambertian surfaces. <em>MLST</em>, <em>6</em>(3), 035059. (<a href='https://doi.org/10.1088/2632-2153/ae054b'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of scene depth from monocular images has become crucial in fields such as spatial perception and computer vision. However, unsupervised depth estimation methods based on view synthesis often ignore the significant impact of non-Lambertian surfaces and ghosting artifacts. In this study, we propose a self-learning depth reconstruction framework. This framework introduces a depth consistency loss to compensate for the failure of the photometric assumption in non-Lambertian regions. Additionally, we design an intrinsic consistency loss that leverages variance as a game-theoretic strategy to ensure the robustness of our model. Finally, we introduce a physics-inspired ghosting mask to eliminate ghosting artifacts. We also design a multi-path transformer layer that integrates the Transformer’s global dependency modeling capability into CNNs, thereby enhancing the model’s performance. Experimental results show that our model demonstrates excellent performance in non-Lambertian regions. Compared with state-of-the-art methods that merely rely on the photometric assumption, our method achieves average improvements of 9.29% and 2.86% on the Sq Rel and RMSE metrics across three network models. Furthermore, it exhibits outstanding zero-shot generalization capability on external datasets. The source code is available at https://github.com/IkeFwd/Icdepth .},
  archive      = {J_MLST},
  author       = {Ke Li and Bolin Song and Naiyao Wang and Zhen Wang and Ruijie Tian},
  doi          = {10.1088/2632-2153/ae054b},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics-inspired self-learning framework for unsupervised depth estimation on non-lambertian surfaces},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D–MOPH–25: Diverse MOF–molecule pairs for henry’s constants prediction*. <em>MLST</em>, <em>6</em>(3), 035058. (<a href='https://doi.org/10.1088/2632-2153/ae0241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational methods like grand-canonical Monte Carlo simulations and machine learning (ML) have accelerated metal–organic frameworks (MOF) exploration but are typically limited to a narrow range of adsorbates due to data availability and force field constraints. In this study, we introduce a dataset of diverse MOF–molecule pairs for Henry’s constant prediction, D–MOPH–25, which systematically explores a diverse chemical space by combining 113 molecular adsorbates with over 5000 MOF structures through an active learning process. D–MOPH–25 constitutes the most diverse adsorbate dataset used in any ML study of molecular adsorption in MOFs to date. Our workflow builds a benchmark for predicting Henry’s constants at 300 K, leveraging conformal prediction for uncertainty quantification. Assessment through Shannon entropy and uniform manifold approximation and projection confirms the comprehensiveness of D–MOPH–25 while highlighting the importance of robust classification to filter out unphysical data points in regression tasks. Although future enhancements in model architecture and sampling criteria could improve predictive performance, our dataset already spans the target space using only 2.31% of total possibilities. This comprehensive dataset facilitates assessment of model generalizability across adsorbate species and can establish a foundation for high-throughput MOF screening and ML-driven separation processes.},
  archive      = {J_MLST},
  author       = {Sihoon Choi and David S Sholl and Andrew J Medford},
  doi          = {10.1088/2632-2153/ae0241},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {D–MOPH–25: Diverse MOF–molecule pairs for henry’s constants prediction*},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving turbulent magnetohydrodynamics: A hybrid operator-diffusion framework. <em>MLST</em>, <em>6</em>(3), 035057. (<a href='https://doi.org/10.1088/2632-2153/ae054c'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hybrid machine learning framework that combines physics-informed neural operators (PINOs) with score-based generative diffusion models to simulate the full spatio-temporal evolution of two-dimensional, incompressible, resistive magnetohydrodynamic turbulence across a broad range of Reynolds numbers ( \mathrm{Re} ). The framework leverages the equation-constrained generalization capabilities of PINOs to predict coherent, low-frequency dynamics, while a conditional diffusion model stochastically corrects high-frequency residuals, enabling accurate modeling of fully developed turbulence. Trained on a comprehensive ensemble of high-fidelity simulations with \mathrm{Re} \in \{100, 250, 500, 750, 1000, 3000, 10\,000\} , the approach achieves state-of-the-art accuracy in regimes previously inaccessible to deterministic surrogates. At \mathrm{Re} = 1000 and 3000, the model faithfully reconstructs the full spectral energy distributions of both velocity and magnetic fields late into the simulation, capturing non-Gaussian statistics, intermittent structures, and cross-field correlations with high fidelity. At extreme turbulence levels ( \mathrm{Re} = 10\,000 ), it remains the first surrogate capable of recovering the high-wavenumber evolution of the magnetic field, preserving large-scale morphology and enabling statistically meaningful predictions.},
  archive      = {J_MLST},
  author       = {Semih Kacmaz and E A Huerta and Roland Haas},
  doi          = {10.1088/2632-2153/ae054c},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Resolving turbulent magnetohydrodynamics: A hybrid operator-diffusion framework},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modular post-hoc enhancements for zero-shot histopathology classification using vision-language models. <em>MLST</em>, <em>6</em>(3), 035056. (<a href='https://doi.org/10.1088/2632-2153/ae0557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning with vision-language models (VLMs) has shown promising results in histopathological image classification. However, existing approaches often under-utilize domain-specific adaptations that could enhance performance in specialized medical tasks. In this study, we systematically evaluate three modular, post-hoc enhancements to standard zero-shot VLM pipelines: (1) cosine similarity calibration for affinity matrix construction, (2) prompt template enhancement via diverse clinical prompts, and (3) adaptive hyperparameter tuning using a Gaussian mixture model-based method for adjusting pseudo-labeling weights ( λ ) and support set contributions ( γ ), following Zanella et al We conduct extensive zero-shot experiments on the NCT-CRC-HE-100 K dataset, using 60 000 patches for validation of inference-time hyperparameters and reserving 40 000 patches as an independent test set. No training or fine-tuning was performed. The evaluation was carried out across five VLMs: CLIP, CONCH, PLIP, Quilt-B16, and Quilt-B32. Results show that each enhancement module yields significant accuracy gains. Cosine calibration improves CLIP (+7.8%), CONCH (+1.53%), PLIP (+14.3%), Quilt-B16 (+16.1%), and Quilt-B32 (+11.4%). Prompt enhancement yields accuracy gains in PLIP (+16.8%) and Quilt-B32 (+29.9%), with limited effect on CLIP or CONCH. Adaptive tuning yields large improvements in PLIP (+28.4%). The best performance is achieved with the full combination of enhancements, with PLIP reaching 87.88% (+27.88%, p < 0.001) and Quilt-B32 reaching 88.38% (+36.1%) over their baselines. CLIP showed significance only with cosine calibration ( p < 0.001), reflecting its contrastive learning backbone. Although CONCH reached the highest raw accuracy (92.63%) under the full enhancement setting, this performance appears primarily driven by its histopathology-specific pretraining; statistically significant gains emerged only in cosine-based combinations, indicating limited responsiveness to prompt or tuning. Our findings highlight the critical role of structured modular enhancements in optimizing VLM performance for specialized clinical domains.},
  archive      = {J_MLST},
  author       = {Shahd M Noman and Mayar Tarek Henedak and Mustafa Elattar},
  doi          = {10.1088/2632-2153/ae0557},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035056},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Modular post-hoc enhancements for zero-shot histopathology classification using vision-language models},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-type point cloud autoencoder: A complete equivariant embedding for molecule conformation and pose. <em>MLST</em>, <em>6</em>(3), 035055. (<a href='https://doi.org/10.1088/2632-2153/adff35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representations are a foundational component of any modeling protocol, including on molecules and molecular solids. For tasks that depend on knowledge of both molecular conformation and 3D orientation, such as the modeling of molecular dimers, clusters, or condensed phases, we desire a rotatable representation that is provably complete in the types and positions of atomic nuclei and roto-inversion equivariant with respect to the input point cloud. In this paper, we develop, train, and evaluate a new type of autoencoder, molecular O(3) encoding net (Mo3ENet), for multi-type point clouds, for which we propose a new reconstruction loss, capitalizing on a Gaussian mixture representation of the input and output point clouds. Mo3ENet is end-to-end equivariant, meaning the learned representation can be manipulated on O(3), a practical bonus. An appropriately trained Mo3ENet latent space comprises a universal embedding for scalar, vector, and tensorial molecule property prediction tasks, as well as other downstream tasks incorporating the 3D molecular pose, and we demonstrate its fitness on several such tasks.},
  archive      = {J_MLST},
  author       = {Michael Kilgour and Mark E Tuckerman and Jutta Rogal},
  doi          = {10.1088/2632-2153/adff35},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035055},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-type point cloud autoencoder: A complete equivariant embedding for molecule conformation and pose},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deterministic versus stochastic dynamical classifiers: Opposing random adversarial attacks with noise. <em>MLST</em>, <em>6</em>(3), 035054. (<a href='https://doi.org/10.1088/2632-2153/ae0244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous-variable firing rate (CVFR) model, widely used in neuroscience to describe the complex dynamics of excitatory biological neurons, is here trained and tested as a dynamical classifier. To this end the model is supplied with a set of attractors which are a priori embedded in the inter-node coupling matrix, via its spectral decomposition. Learning amounts to tuning the residual parameters, in order to shape a non-equilibrium path which bridges the input (the data to be classified) and the output (the target memory slot). The imposed attractors are unaltered by the training, and this enables for ex post comparisons to be eventually drawn, e.g. as it concerns the size of their associated basins of attraction. A stochastic variant of the CVFR model is also studied and found to be robust to non-targeted adversarial attacks, which corrupt with a random perturbation the items to be eventually classified. Taken as a whole, here we show that a family of biologically plausible models written in terms of coupled ODEs can efficiently cope with a non-trivial classification task.},
  archive      = {J_MLST},
  author       = {Lorenzo Chicchi and Duccio Fanelli and Diego Febbe and Lorenzo Buffoni and Francesca Di Patti and Lorenzo Giambagli and Raffaele Marino},
  doi          = {10.1088/2632-2153/ae0244},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035054},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deterministic versus stochastic dynamical classifiers: Opposing random adversarial attacks with noise},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A frequentist simulation-based inference treatment of sterile neutrino global fits. <em>MLST</em>, <em>6</em>(3), 035053. (<a href='https://doi.org/10.1088/2632-2153/ae040c'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical challenge in particle physics is combining results from diverse experimental setups that measure the same physical quantity to enhance precision and statistical power, a process known as a global fit. Global fits of sterile neutrino searches, hunts for additional neutrino oscillation frequencies and amplitudes, present an intriguing case study. In such a scenario, the key assumptions underlying Wilks’ theorem, a cornerstone of most classic frequentist analyses, do not hold. The method of Feldman and Cousins, a trials-based approach which does not assume Wilks’ theorem, becomes computationally prohibitive for complex or intractable likelihoods. To bypass this limitation, we borrow a technique from simulation-based inference (SBI) to estimate likelihood ratios for use in building trials-based confidence intervals, speeding up test statistic evaluations by a factor \gt\!\!\!10^4 per grid point, resulting in a faster, but approximate, frequentist fitting framework. Applied to a subset of sterile neutrino search data involving the disappearance of muon-flavor (anti)neutrinos, our method leverages machine learning (ML) to compute frequentist confidence intervals while significantly reducing computational expense. In addition, the SBI-based approach holds additional value by recognizing underlying systematic uncertainties that the Wilks approach does not. Thus, our method allows for more robust ML-based analyses critical to performing accurate but computationally feasible global fits. This allows, for the first time, a global fit to sterile neutrino data without assuming Wilks’ theorem. While we demonstrate the utility of such a technique studying sterile neutrino searches, it is applicable to both single-experiment and global fits of all kinds.},
  archive      = {J_MLST},
  author       = {Joshua Villarreal and Julia Woodward and John M Hardin and Janet M Conrad},
  doi          = {10.1088/2632-2153/ae040c},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035053},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A frequentist simulation-based inference treatment of sterile neutrino global fits},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-$T_\mathrm c$ superconductor candidates proposed by machine learning. <em>MLST</em>, <em>6</em>(3), 035052. (<a href='https://doi.org/10.1088/2632-2153/ae04c1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We cast the relation between the chemical composition of a solid-state material and its superconducting critical temperature ( T_\mathrm c ) as a statistical learning problem with reduced complexity. Training of query-aware similarity-based ridge regression models on experimental SuperCon data achieves average T_\mathrm c prediction errors of ±5 K for unseen out-of-sample materials. Two models were trained with one excluding high pressure data in training (‘ambient’ model) and a second also including high pressure data (‘implicit’ model). Subsequent utilization of the approach to scan ∼153 k materials in the Materials Project enables the ranking of candidates by T_\mathrm c while accounting for thermodynamic stability and small band gap. The ambient model is used to predict stable top three high- T_\mathrm c candidate materials that include those with large band gaps of LiCuF 4 (316 K), Ag 2 H 12 S(NO) 4 (316 K), and Na 2 H 6 PtO 6 (315 K). Filtering these candidates for those with small band gaps correspondingly yields LiCuF 4 (316 K), Cu 2 P 2 O 7 (311 K), and Cu 3 P 2 H 2 O 9 (307 K).},
  archive      = {J_MLST},
  author       = {Siwoo Lee and Jason Hattrick-Simpers and Young-June Kim and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/ae04c1},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035052},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {High-$T_\mathrm c$ superconductor candidates proposed by machine learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal cascade feature transfer for polymer property prediction. <em>MLST</em>, <em>6</em>(3), 035051. (<a href='https://doi.org/10.1088/2632-2153/ae023e'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction. Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.},
  archive      = {J_MLST},
  author       = {Kiichi Obuchi and Yuta Yahagi and Kiyohiko Toyama and Shukichi Tanaka and Kota Matsui},
  doi          = {10.1088/2632-2153/ae023e},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035051},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-modal cascade feature transfer for polymer property prediction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DINAMO: Dynamic and INterpretable anomaly MOnitoring for large-scale particle physics experiments. <em>MLST</em>, <em>6</em>(3), 035050. (<a href='https://doi.org/10.1088/2632-2153/ae0240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring reliable data collection in large-scale particle physics experiments demands data quality monitoring (DQM) procedures to detect possible detector malfunctions and preserve data integrity. Traditionally, this resource-intensive task has been handled by human shifters who struggle with frequent changes in operational conditions. We present D ynamic and IN terpretable A nomaly MO nitoring: a novel, interpretable, robust, and scalable DQM framework designed to automate anomaly detection in time-dependent settings. Our approach constructs evolving histogram templates with built-in uncertainties, featuring both a statistical variant—extending the classical exponentially weighted moving average—and a machine learning-enhanced version that leverages a transformer encoder for improved adaptability. Experimental validations on synthetic datasets demonstrate the high accuracy, adaptability, and interpretability of these methods. The statistical variant is being commissioned in the LHCb experiment at the large hadron collider, underscoring its real-world impact.},
  archive      = {J_MLST},
  author       = {Arsenii Gavrikov and Julián García Pardiñas and Alberto Garfagnini},
  doi          = {10.1088/2632-2153/ae0240},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035050},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {DINAMO: Dynamic and INterpretable anomaly MOnitoring for large-scale particle physics experiments},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric GNNs for charged particle tracking at GlueX. <em>MLST</em>, <em>6</em>(3), 035049. (<a href='https://doi.org/10.1088/2632-2153/ae02df'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nuclear physics experiments are aimed at uncovering the fundamental building blocks of matter. The experiments involve high-energy collisions that produce complex events with many particle trajectories. Tracking charged particles resulting from collisions in the presence of a strong magnetic field is critical to enable the reconstruction of particle trajectories and precise determination of interactions. It is traditionally achieved through combinatorial approaches that scale worse than linearly as the number of hits grows. Since particle hit data naturally form a point cloud and can be structured as graphs, graph neural networks (GNNs) emerge as an intuitive and effective choice for this task. In this study, we evaluate the GNN model for track finding on the data from the GlueX experiment at Jefferson Lab. We use simulation data to train the model and test on both simulation and real GlueX measurements. We demonstrate that GNN-based track finding outperforms the currently used traditional method at GlueX in terms of segment-based efficiency at a fixed purity while providing faster inferences. We show that the GNN model can achieve significant speedup by processing multiple events in batches, which exploits the parallel computation capability of graphical processing units (GPUs). Finally, we compare the GNN implementation on GPU and field-programmable gate array and describe the trade-off.},
  archive      = {J_MLST},
  author       = {Ahmed Hossam Mohammed and Kishansingh Rajput and Simon Taylor and Denis Furletov and Sergey Furletov and Malachi Schram},
  doi          = {10.1088/2632-2153/ae02df},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035049},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Geometric GNNs for charged particle tracking at GlueX},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided graph compression for quantum graph neural networks. <em>MLST</em>, <em>6</em>(3), 035048. (<a href='https://doi.org/10.1088/2632-2153/adffe2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are effective for processing graph-structured data but face challenges with large graphs due to high memory requirements and inefficient sparse matrix operations on GPUs. Quantum computing offers a promising avenue to address these issues and inspires new algorithmic approaches. In particular, quantum GNNs (QGNNs) have been explored in recent literature. However, current quantum hardware limits the dimension of the data that can be effectively encoded. Existing approaches either simplify datasets manually or use artificial graph datasets. This work introduces the guided graph compression (GGC) framework, which uses a graph autoencoder to reduce both the number of nodes and the dimensionality of node features. The compression is guided to enhance the performance of a downstream classification task, which can be applied either with a quantum or a classical classifier. The framework is evaluated on the Jet Tagging task, a classification problem of fundamental importance in high energy physics that involves distinguishing particle jets initiated by quarks from those by gluons. We compare GGC to a model that uses the autoencoder as a preprocessing step and to a baseline classical GNN classifier. Our numerical results demonstrate that GGC outperforms both alternatives, while also facilitating the testing of novel QGNN ansatzes on realistic datasets.},
  archive      = {J_MLST},
  author       = {Mikel Casals and Vasilis Belis and Elias F Combarro and Eduard Alarcón and Sofia Vallecorsa and Michele Grossi},
  doi          = {10.1088/2632-2153/adffe2},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035048},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Guided graph compression for quantum graph neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development and application of a high-dimensional neural network potential for boron carbide. <em>MLST</em>, <em>6</em>(3), 035047. (<a href='https://doi.org/10.1088/2632-2153/adfffd'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A high-dimensional neural network potential (HDNNP) is developed to accurately model the potential energy surface of boron carbide. The HDNNP is trained on an extensive dataset of structures generated using a Monte Carlo algorithm, covering a wide range of stoichiometries and structural configurations. Density functional theory (DFT) calculations provided energy and atomic force data, which were encoded using atom-centered symmetry functions to capture local atomic environments. The resulting HDNNP exhibits exceptional predictive performance, accurately estimating energies and forces across diverse structural configurations of boron carbide. The model effectively captures essential structural, energetic, and mechanical properties, including elastic behavior and the influence of stoichiometry on stability. The HDNNP development followed an iterative refinement strategy, in which the initial training set was systematically expanded to 2931 structures by incorporating finite-temperature AIMD snapshots at 300 K, 600 K, and 1000 K, as well as extrapolative configurations from MD simulations and defect-containing cells. This approach resulted in an HDNNP model that is numerically stable and physically consistent up to 500 K, with only minor drift observed at 600 K. The HDNNP achieves exceptional accuracy in predicting energies, forces, and mechanical properties across temperatures, reproducing Young’s modulus values of 416 GPa at 10 K and 420\pm20 GPa at 300 K, in close agreement with DFT and experimental ranges. The predicted ultimate tensile strength of 43 GPa at a failure strain of 0.14 at 300 K is consistent with atomistic simulations (40.23 GPa, strain 0.12). Comparative MD simulations show that the HDNNP outperforms Tersoff and ReaxFF in both accuracy and computational efficiency, enabling timesteps up to an order of magnitude larger, achieving 43% faster performance than Tersoff and 79% faster than ReaxFF. These results highlight the HDNNP as a robust and efficient tool for simulating boron carbide across a range of temperatures and deformation conditions, offering quantum-level accuracy for large-scale atomistic modeling in extreme environments.},
  archive      = {J_MLST},
  author       = {Sara Sheikhi and Wylie Stroberg and James D Hogan},
  doi          = {10.1088/2632-2153/adfffd},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035047},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Development and application of a high-dimensional neural network potential for boron carbide},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel quintile multi-model ensemble approach for improving future extreme precipitation projections using XGBoost. <em>MLST</em>, <em>6</em>(3), 035046. (<a href='https://doi.org/10.1088/2632-2153/adfcb1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel quintile multi-model ensemble (QMME) framework is introduced to improve future precipitation projections from MMEs. The QMME method divides daily precipitation into five quintiles, thereby capturing not only extreme rainfall but also the full spectrum of precipitation intensities. This approach surpasses conventional ensembles, such as the equal MME (EMME) and the weighted MME (WMME), in accurately reflecting the distinct characteristics of individual climate models. To develop the QMME, this study first applies empirical quantile mapping to correct biases in daily precipitation outputs from 14 coupled model intercomparison project 6 (CMIP6) general circulation models (GCMs). Historical observations (1980–2014) from 61-gauge stations in South Korea are then used to evaluate GCM performance within each quintile. The QMME framework integrates each GCM’s historical performance and future uncertainty into a quintile-specific weights and combines them with XGBoost to generate enhanced precipitation time series. Four shared socioeconomic pathway (SSP) scenarios (SSP1-2.6, SSP2-4.5, SSP3-7.0, and SSP5-8.5) are considered to assess a range of future scenarios. As a result, the QMME consistently outperforms both EMME and WMME in capturing both extreme precipitation events and moderate rainfall conditions. An evaluation using reliability ensemble averaging confirms that the QMME more effectively accounts for inter-model variability and reduces uncertainty, thus providing robust projections of future precipitation. This quintile-based methodology can be readily extended to other hydroclimatic variables and geographic regions, offering significant potential for improving climate impact assessments and guiding risk management in water resources planning.},
  archive      = {J_MLST},
  author       = {Young Hoon Song and Mohamed Sanusi Shiru and Eun-Sung Chung},
  doi          = {10.1088/2632-2153/adfcb1},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035046},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A novel quintile multi-model ensemble approach for improving future extreme precipitation projections using XGBoost},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning and interpreting gravitational-wave features from CNNs with a random forest approach. <em>MLST</em>, <em>6</em>(3), 035045. (<a href='https://doi.org/10.1088/2632-2153/adfc27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have attracted increasing attention in gravitational wave (GW) data analysis due to their ability to automatically learn hierarchical features from raw strain data. However, the physical meaning of these learned features remains underexplored, limiting the interpretability of such models. In this work, we propose a hybrid architecture that combines a CNN-based feature extractor with a random forest (RF) classifier to improve both detection performance and interpretability. Unlike prior approaches that directly connect classifiers to CNN outputs, our method introduces four physically interpretable metrics-variance, signal-to-noise ratio (SNR), waveform overlap, and peak amplitude-computed from the final convolutional layer. These are jointly used with the CNN output probability in the RF classifier to enable more informed decision boundaries. Tested on long-duration strain datasets, our hybrid model outperforms a baseline-CNN model, achieving a relative improvement of 21% in sensitivity at a fixed false alarm rate of 10 events per month. Notably, it also shows improved detection of low-SNR signals (SNR \unicode{x2A7D} 10), which are especially vulnerable to misclassification in noisy environments. Feature importance analysis and ablation studies reveal that handcrafted features play a significant role in classification decisions and contribute to improved performance. These findings suggest that physically motivated post-processing of CNN feature maps can serve as a valuable tool for interpretable and efficient GW detection, bridging the gap between deep learning and domain knowledge.},
  archive      = {J_MLST},
  author       = {Jun Tian and He Wang and Jibo He and Yu Pan and Shuo Cao and Qingquan Jiang},
  doi          = {10.1088/2632-2153/adfc27},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035045},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning and interpreting gravitational-wave features from CNNs with a random forest approach},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating reaction rate predictions: A machine learning approach using a novel quantum chemical property for nucleophilic aromatic substitutions. <em>MLST</em>, <em>6</em>(3), 035044. (<a href='https://doi.org/10.1088/2632-2153/adfd37'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting reaction rate coefficients is crucial for various chemical engineering tasks, such as kinetic modeling and drug synthesis planning. Traditional methods like group additivity and rate rules have limitations, prompting the exploration of machine learning methods to predict these coefficients. These machine learning models are often combined with quantum chemical calculations to improve their performance. While often accurate, these approaches slow down predictions due to the need for quantum chemical calculations, particularly for transition states. This study addresses the issue by introducing a quantum chemical property that does not require transition state calculations but still correlates well with the rate coefficient. We further enhance the prediction speed by training a machine learning model to predict this property. Finally, we propose two approaches using this machine learning model to predict the rate coefficients of nucleophilic aromatic substitution reactions in fractions of a second.},
  archive      = {J_MLST},
  author       = {Lowie Tomme and István Lengyel and Florence H Vermeire and Christian V Stevens and Kevin M Van Geem},
  doi          = {10.1088/2632-2153/adfd37},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035044},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerating reaction rate predictions: A machine learning approach using a novel quantum chemical property for nucleophilic aromatic substitutions},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical physics benchmark (TPBench)—a dataset and study of AI reasoning capabilities in theoretical physics. <em>MLST</em>, <em>6</em>(3), 030505. (<a href='https://doi.org/10.1088/2632-2153/adfcb0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics (TP), focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted TP research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org .},
  archive      = {J_MLST},
  author       = {Daniel J H Chung and Zhiqi Gao and Yurii Kvasiuk and Tianyi Li and Moritz Münchmeyer and Maja Rudolph and Frederic Sala and Sai Chaitanya Tadepalli},
  doi          = {10.1088/2632-2153/adfcb0},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {030505},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Theoretical physics benchmark (TPBench)—a dataset and study of AI reasoning capabilities in theoretical physics},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
