<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst">MLST - 4</h2>
<ul>
<li><details>
<summary>
(2025). A high-performance and portable implementation of the SISSO method for CPUs and GPUs. <em>MLST</em>, <em>6</em>(4), 047001. (<a href='https://doi.org/10.1088/2632-2153/ae0ab3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sure-independence screening and sparsifying operator (SISSO) is an artificial intelligence (AI) method based on symbolic regression and compressed sensing widely used in materials science research. SISSO++ is its C++ implementation that employs MPI and OpenMP for parallelization, rendering it well-suited for high-performance computing (HPC) environments. As heterogeneous hardware becomes mainstream in the HPC and AI fields, we chose to port the SISSO++ code to GPUs using the Kokkos performance-portable library. Kokkos allows us to maintain a single codebase for both Nvidia and AMD GPUs, significantly reducing the maintenance effort. In this work, we summarize the necessary code changes we did to achieve hardware and performance portability. This is accompanied by performance benchmarks on Nvidia and AMD GPUs. We demonstrate the speedups obtained from using GPUs across the three most time-consuming parts of our code.},
  archive      = {J_MLST},
  author       = {Sebastian Eibl and Yi Yao and Matthias Scheffler and Markus Rampp and Luca M Ghiringhelli and Thomas A R Purcell},
  doi          = {10.1088/2632-2153/ae0ab3},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {047001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A high-performance and portable implementation of the SISSO method for CPUs and GPUs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-correlated noise in epoch-based stochastic gradient descent: Implications for weight variances in flat directions. <em>MLST</em>, <em>6</em>(4), 045003. (<a href='https://doi.org/10.1088/2632-2153/ae0ab2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the gradient noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional predictions of isotropic weight variance under stationarity, based on uncorrelated and curvature-proportional noise, are recovered. Anti-correlations have negligible effect here. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we present a numerical experiment where training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training may play a role in finding flatter minima that generalize better.},
  archive      = {J_MLST},
  author       = {Marcel Kühn and Bernd Rosenow},
  doi          = {10.1088/2632-2153/ae0ab2},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Anti-correlated noise in epoch-based stochastic gradient descent: Implications for weight variances in flat directions},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum generative adversarial networks with dual generators. <em>MLST</em>, <em>6</em>(4), 045002. (<a href='https://doi.org/10.1088/2632-2153/ae0bf7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum generative adversarial networks (QGANs) have demonstrated strong capabilities in tasks like synthetic data generation and detecting anomalies. Recent developments have increasingly integrated traditional machine learning techniques to boost the performance of QGANs. Motivated by this progress, we propose an innovative QGAN architecture that incorporates a classical learning component and employs a dual-generator design. Our approach improves upon the traditional hybrid quantum–classical GAN structure and introduces a redesigned loss function tailored for the new model. Experiments on multiple datasets indicate that our method surpasses previous techniques in image generation quality, achieving a 1.38% average reduction in Fréchet inception distance scores compared to the current state-of-the-art, and improvements of 6.52%, 0.36%, and 0.38% in structural similarity index, cosine similarity, and peak signal-to-noise ratio metrics, respectively. Additionally, our architecture supports the generation of larger images (up to 78\,\times\,78 ), as verified on the CelebA dataset. Simulations conducted in noisy conditions further confirm the robustness and effectiveness of both the proposed architecture and loss function.},
  archive      = {J_MLST},
  author       = {Quangong Ma and Chaolong Hao and NianWen Si and Dan Qu},
  doi          = {10.1088/2632-2153/ae0bf7},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum generative adversarial networks with dual generators},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor network for anomaly detection in the latent space of proton collision events at the LHC. <em>MLST</em>, <em>6</em>(4), 045001. (<a href='https://doi.org/10.1088/2632-2153/ae0243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of discovering new phenomena at the Large Hadron Collider (LHC) requires constant innovation in algorithms and technologies. Tensor networks are mathematical models at the intersection of classical and quantum machine learning, which present a promising and efficient alternative for tackling these challenges. In this study, we propose a tensor network-based strategy for anomaly detection at the LHC and demonstrate its superior performance in identifying new phenomena compared to established quantum methods. Our model is a parameterized matrix product state with an isometric feature map, processing a latent representation of simulated LHC data generated by an autoencoder. Our results highlight the potential of tensor networks to enhance new-physics discovery.},
  archive      = {J_MLST},
  author       = {Ema Puljak and Maurizio Pierini and Artur Garcia-Saez},
  doi          = {10.1088/2632-2153/ae0243},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Tensor network for anomaly detection in the latent space of proton collision events at the LHC},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
