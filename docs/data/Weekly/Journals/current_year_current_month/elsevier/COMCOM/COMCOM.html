<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom">COMCOM - 5</h2>
<ul>
<li><details>
<summary>
(2025). Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice. <em>COMCOM</em>, <em>243</em>, 108321. (<a href='https://doi.org/10.1016/j.comcom.2025.108321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of optimizing LoRa network success probability by proposing an optimized allocation strategy for Spreading Factors (SFs) under both uniform and Gaussian network deployments with a single or multiple gateways. More specifically, we solve the problem of finding the best SF allocations in dense network deployments whose EDs are first assigned with the minimum SF. Theoretical models are developed to quantify the success probability of transmissions, considering the capture effect as well as intra- and inter-SF interference. A mathematical optimization framework is introduced to determine the optimal SF distribution that maximizes the average probability of packet reception. The problem is solved using Mixed Integer Linear Programming (MILP), and then evaluated using simulations. Even though optimal SF allocation strategies have been proposed in the literature, no practical insights have been discovered and no real-world deployments have been considered. To this extent, the practical benefits of using improved or optimal SF settings are discovered in this paper. Simulation results confirm the theoretical findings while they demonstrate an up to 10 percentage points improvements in Packet Reception Ratio (PRR) in the real-world use-case.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Aruzhan Sabyrbek and Luigi Di Puglia Pugliese},
  doi          = {10.1016/j.comcom.2025.108321},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108321},
  shortjournal = {Comput. Commun.},
  title        = {Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS. <em>COMCOM</em>, <em>243</em>, 108320. (<a href='https://doi.org/10.1016/j.comcom.2025.108320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-assisted edge computing provides low-latency and low-energy consumption computing capabilities for sparsely distributed Internet of Things (IoT) networks. In addition, the assisted UAVs provide line-of-sight links to further improve communication quality. However, the existing offloading strategies have low efficiency and high costs. Motivated by this, we propose a novel UAV-assisted multi-layer mobile edge computing network with active transmissive reconfigurable intelligent surface (RIS). The introduced an active transmissive RIS not only receives data from UAVs but also performs computing functionality. We establish an optimization to minimize the total system energy consumption under delay constraints by jointly planning UAV positions and allocating computing bits, sub-carriers, time slots, transmission power, and RIS transmission coefficient. To tackle this problem, we first use the block coordinate descent (BCD) algorithm to decouple it into four sub-problems. Then, we solve them by adopting successive convex approximation (SCA), difference-convex (DC) programming, and introducing slack variables. Experimental results demonstrate that the proposed network is superior to the other five baselines concerning energy consumption reduction. Also, the influences of system parameters are verified, including the number of IoT devices, the number of RIS elements, and the delay threshold.},
  archive      = {J_COMCOM},
  author       = {Kexin Yang and Yaxi Liu and Boxin He and Jiahao Huo and Wei Huangfu},
  doi          = {10.1016/j.comcom.2025.108320},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108320},
  shortjournal = {Comput. Commun.},
  title        = {Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks. <em>COMCOM</em>, <em>243</em>, 108315. (<a href='https://doi.org/10.1016/j.comcom.2025.108315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-distance data transmission between Internet of Things (IoT) devices and remote cloud center often leads to unacceptable latency for certain tasks. Fog computing has emerged as a promising solution for low-latency tasks. Consequently, the concept of cloud-fog collaborative networks has garnered significant attention. However, existing research primarily focuses on heterogeneous tasks, overlooking the crucial aspect of considering both task priority and second unloading. To address this gap, this paper proposes a novel task unloading scheme that concurrently takes preemptive priority and second optional unloading into account. In this scheme, delay-sensitive tasks (DSTs) are given preemptive priority over delay-tolerant tasks (DTTs). Furthermore, some DTTs may undergo preprocessing in the fog layer to optimize resource utilization. Moreover, tasks encountering blocking or preemption in the fog layer can also be secondarily unloaded to the cloud layer. In this framework, we devise a four-dimensional Markov chain (4DMC) to model and analyze this process. Through numerical experiments, we assess performance indicators under various parameters. Ultimately, our proposed strategy is compared with the unloading scheme that does not incorporate second unloading through both numerical analysis and simulation validation. The results indicate that our scheme notably enhances the throughput of DTTs, albeit at a marginal performance trade-off.},
  archive      = {J_COMCOM},
  author       = {Yuan Zhao and Hongmin Gao and Shuaihua Liu},
  doi          = {10.1016/j.comcom.2025.108315},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108315},
  shortjournal = {Comput. Commun.},
  title        = {A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning. <em>COMCOM</em>, <em>243</em>, 108314. (<a href='https://doi.org/10.1016/j.comcom.2025.108314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive resource provisioning has become crucial for cloud-based applications, especially those managing real-time traffic like Voice over IP (VoIP), which experience rapidly fluctuating workloads. Traditional static provisioning methods often fall short in these dynamic environments, leading to inefficiencies and potential service disruptions. Existing solutions struggle to maintain performance under varying traffic conditions, particularly for time-sensitive applications. This paper introduces ScaleIP, a hybrid autoscaling solution for containerized VoIP services that offers real-time adaptability and efficient resource management. ScaleIP leverages Deep Reinforcement Learning to make dynamic and efficient scaling decisions, improving call latency, increasing the number of successfully routed calls, and maximizing resource utilization. We evaluated ScaleIP through extensive experiments conducted on a real testbed utilizing the customer Call Detail Record (CDR) from 2023 provided by World Direct, encompassing over 89 million calls. The results show that ScaleIP consistently maintains call latency below 2 s, increases the number of successfully routed calls by 3.26 ×, and increases the resource utilization up to 60 % compared to state-of-the-art autoscaling methods.},
  archive      = {J_COMCOM},
  author       = {Zahra Najafabadi Samani and Juan Aznar Poveda and Dominik Gratz and Rene Hueber and Philipp Kalb and Thomas Fahringer},
  doi          = {10.1016/j.comcom.2025.108314},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108314},
  shortjournal = {Comput. Commun.},
  title        = {ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond performance comparing the costs of applying deep and shallow learning. <em>COMCOM</em>, <em>243</em>, 108312. (<a href='https://doi.org/10.1016/j.comcom.2025.108312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of mobile network traffic and the emergence of complex applications, such as self-driving cars and augmented reality, demand ultra-low latency, high throughput, and massive device connectivity, which traditional network design approaches struggle to meet. These issues were initially addressed in Fifth-Generation (5G) and Beyond-5G (B5G) networks, where Artificial Intelligence (AI), particularly Deep Learning (DL), is proposed to optimize the network and to meet these demanding requirements. However, the resource constraints and time limitations inherent in telecommunication networks raise questions about the practicality of deploying large Deep Neural Networks (DNNs) in these contexts. This paper analyzes the costs of implementing DNNs by comparing them with shallow ML models across multiple datasets and evaluating factors such as execution time and model interpretability. Our findings demonstrate that shallow ML models offer comparable performance to DNNs, with significantly reduced training and inference times, achieving up to 90% acceleration. Moreover, shallow models are more interpretable, as explainability metrics struggle to agree on feature importance values even for high-performing DNNs.},
  archive      = {J_COMCOM},
  author       = {Rafael Teixeira and Leonardo Almeida and Pedro Rodrigues and Mário Antunes and Diogo Gomes and Rui L. Aguiar},
  doi          = {10.1016/j.comcom.2025.108312},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108312},
  shortjournal = {Comput. Commun.},
  title        = {Beyond performance comparing the costs of applying deep and shallow learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
