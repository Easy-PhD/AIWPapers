<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom">COMCOM - 11</h2>
<ul>
<li><details>
<summary>
(2025). An effective method for data aggregation point placement in LoRaWAN network for smart metering service. <em>COMCOM</em>, <em>243</em>, 108335. (<a href='https://doi.org/10.1016/j.comcom.2025.108335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric grids have been restructured with Smart Grids (SGs), and the deployment of Advanced Metering Infrastructure (AMI) systems is a fundamental part of this process. An AMI system consists of Smart Meters (SMs) that collect energy consumption data and send it to the utility company through Data Aggregation Points (DAPs). Thus, methods to determine the appropriate quantity and positions of DAPs become necessary. In this context, this paper proposes a method called DAP Placement (DPlace), which determines the minimum number of DAPs and applies the Fuzzy C-Means algorithm to position the DAPs at coordinates that enhance the successful reception of packets, ensuring that smart metering applications can adequately transmit data through a Long Range Wide-Area Network (LoRaWAN) network. The proposed method is compared to three state-of-the-art solutions through simulations, and the results show that the DPlace method reduces the required number of DAPs by up to 37.04% while achieving communication performance – evaluated through metrics such as packet delivery ratio and delay – similar to related methods, even with less DAPs.},
  archive      = {J_COMCOM},
  author       = {Thiago A.R. da Silva and Geraldo A. Sarmento N. and Luís H. de O. Mendes and Pedro F. de Abreu and Artur F. da S. Veloso and Fernando J.V. Santos and José Valdemir dos R. Júnior},
  doi          = {10.1016/j.comcom.2025.108335},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108335},
  shortjournal = {Comput. Commun.},
  title        = {An effective method for data aggregation point placement in LoRaWAN network for smart metering service},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). META: Multi-classified encrypted traffic anomaly detection with fine-grained flow and interaction analysis. <em>COMCOM</em>, <em>243</em>, 108333. (<a href='https://doi.org/10.1016/j.comcom.2025.108333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive implementation of encryption mechanisms has introduced considerable obstacles to anomalous traffic detection, rendering conventional attack detection methodologies that rely on packet payload characteristics ineffectual. In the absence of plaintext information, current anomaly encrypted traffic detection mainly relies on traffic data analysis to identify and characterize anomalous attack patterns in encrypted traffic, employing machine learning or deep learning models. However, the existing methods still suffer from limited detection capabilities, especially the ability to classify multi-class attacks due to insufficient internal and external features. In this paper, we propose a Multi-classified Encrypted Traffic Anomaly Detection (META) method. META refines and extends the available feature dimensions in encrypted traffic by leveraging two key aspects: the internal interaction behavior information within the traffic and the external interaction behavior information in network topology. Specifically, an in-depth examination of the internal packet interaction features is undertaken, resulting in a novel feature set, designated as META-Features, encompassing 278 fine-grained statistical features. Furthermore, a Graph Neural Network (GNN) is employed to learn the external interaction behavior in the network topology from the embedding of the IP node graph and flow edge graph. The results of the experiments demonstrate that the refined feature set META-Features significantly enhances the model’s detection capabilities. Thereby, the META-GNN model exhibits superior performance compared to the traditional approaches, with an accuracy of 91.90% and an F1-score of 87.41%.},
  archive      = {J_COMCOM},
  author       = {Boyu Kuang and Yuchi Chen and Yansong Gao and Yaqian Xu and Anmin Fu and Willy Susilo},
  doi          = {10.1016/j.comcom.2025.108333},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108333},
  shortjournal = {Comput. Commun.},
  title        = {META: Multi-classified encrypted traffic anomaly detection with fine-grained flow and interaction analysis},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating scalability of median-based ADR under different mobility conditions. <em>COMCOM</em>, <em>243</em>, 108322. (<a href='https://doi.org/10.1016/j.comcom.2025.108322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The LoRaWAN protocol is widely used in Internet of Things (IoT) applications due to its ability to provide long-range, low-power communication. The Adaptive Data Rate (ADR) mechanism dynamically adjusts transmission parameters to optimize energy consumption. However, ADR is primarily designed for static devices, which limits its effectiveness in mobile environments, where fluctuating signal conditions can degrade performance. To address this limitation, the Median-Based ADR (MB-ADR) scheme was introduced, leveraging statistical measures to improve ADR adaptability to changing channel conditions. This study evaluates the scalability of MB-ADR in networks with up to 1,000 end devices and node speeds of up to 20 m/s, considering mobility models such as Random Walk and Gauss–Markov. The results show that MB-ADR demonstrates superior performance in scenarios with realistic mobility patterns, particularly under the Steady-State Random Waypoint model, resulting in improvements of up to 15% in Packet Delivery Ratio (PDR) and 55% in energy efficiency compared to a Kalman filter-based scheme under the same mobility model. Additionally, the analysis demonstrates the effectiveness of MB-ADR in improving throughput and reducing collisions by promoting an efficient distribution of spreading factors. Overall, the study confirms the potential of MB-ADR to enhance communication reliability and energy efficiency in mobile IoT networks, making it a viable solution for large-scale, high-density IoT deployments with variable mobility.},
  archive      = {J_COMCOM},
  author       = {Geraldo A. Sarmento Neto and Thiago A.R. da Silva and Artur F. da S. Veloso and Pedro Felipe de Abreu and Luis H. de O. Mendes and J. Valdemir dos Reis Jr},
  doi          = {10.1016/j.comcom.2025.108322},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108322},
  shortjournal = {Comput. Commun.},
  title        = {Evaluating scalability of median-based ADR under different mobility conditions},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice. <em>COMCOM</em>, <em>243</em>, 108321. (<a href='https://doi.org/10.1016/j.comcom.2025.108321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of optimizing LoRa network success probability by proposing an optimized allocation strategy for Spreading Factors (SFs) under both uniform and Gaussian network deployments with a single or multiple gateways. More specifically, we solve the problem of finding the best SF allocations in dense network deployments whose EDs are first assigned with the minimum SF. Theoretical models are developed to quantify the success probability of transmissions, considering the capture effect as well as intra- and inter-SF interference. A mathematical optimization framework is introduced to determine the optimal SF distribution that maximizes the average probability of packet reception. The problem is solved using Mixed Integer Linear Programming (MILP), and then evaluated using simulations. Even though optimal SF allocation strategies have been proposed in the literature, no practical insights have been discovered and no real-world deployments have been considered. To this extent, the practical benefits of using improved or optimal SF settings are discovered in this paper. Simulation results confirm the theoretical findings while they demonstrate an up to 10 percentage points improvements in Packet Reception Ratio (PRR) in the real-world use-case.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Aruzhan Sabyrbek and Luigi Di Puglia Pugliese},
  doi          = {10.1016/j.comcom.2025.108321},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108321},
  shortjournal = {Comput. Commun.},
  title        = {Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS. <em>COMCOM</em>, <em>243</em>, 108320. (<a href='https://doi.org/10.1016/j.comcom.2025.108320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-assisted edge computing provides low-latency and low-energy consumption computing capabilities for sparsely distributed Internet of Things (IoT) networks. In addition, the assisted UAVs provide line-of-sight links to further improve communication quality. However, the existing offloading strategies have low efficiency and high costs. Motivated by this, we propose a novel UAV-assisted multi-layer mobile edge computing network with active transmissive reconfigurable intelligent surface (RIS). The introduced an active transmissive RIS not only receives data from UAVs but also performs computing functionality. We establish an optimization to minimize the total system energy consumption under delay constraints by jointly planning UAV positions and allocating computing bits, sub-carriers, time slots, transmission power, and RIS transmission coefficient. To tackle this problem, we first use the block coordinate descent (BCD) algorithm to decouple it into four sub-problems. Then, we solve them by adopting successive convex approximation (SCA), difference-convex (DC) programming, and introducing slack variables. Experimental results demonstrate that the proposed network is superior to the other five baselines concerning energy consumption reduction. Also, the influences of system parameters are verified, including the number of IoT devices, the number of RIS elements, and the delay threshold.},
  archive      = {J_COMCOM},
  author       = {Kexin Yang and Yaxi Liu and Boxin He and Jiahao Huo and Wei Huangfu},
  doi          = {10.1016/j.comcom.2025.108320},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108320},
  shortjournal = {Comput. Commun.},
  title        = {Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIS-assisted LoRa networks with diversity: Impact of hardware impairments and phase noise. <em>COMCOM</em>, <em>243</em>, 108319. (<a href='https://doi.org/10.1016/j.comcom.2025.108319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the performance of downlink LoRa networks assisted by reconfigurable intelligent surfaces (RIS) and diversity techniques. We derive closed-form expressions for the coverage probability (Pcov) under four scenarios: phase noise at the RIS only, hardware impairments at both the gateway and end devices (EDs), the combined effect of both impairments, and an ideal benchmark case. The analysis is carried out within a unified framework that is valid for any number of RIS elements, providing key insights into the influence of hardware impairment levels, gateway transmit power, and the diversity order as the number of RIS elements grows large. The results reveal that coverage probability improves with transmit power but deteriorates under more severe hardware impairments, while the diversity order scales directly with the number of RIS elements. Monte Carlo simulations validate the analytical findings and confirm that the ideal scenario achieves the best performance, followed in order by the phase noise, hardware impairment, and combined impairment cases.},
  archive      = {J_COMCOM},
  author       = {Thi-Phuong-Anh Hoang and Thien Huynh-The and Tien Hoa Nguyen and Trong-Thua Huynh and Nguyen-Son Vo and Lam-Thanh Tu},
  doi          = {10.1016/j.comcom.2025.108319},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108319},
  shortjournal = {Comput. Commun.},
  title        = {RIS-assisted LoRa networks with diversity: Impact of hardware impairments and phase noise},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel-hopping sequence generation for blind rendezvous in cognitive radio-enabled internet of vehicles: A multi-agent twin delayed deep deterministic policy gradient-based method. <em>COMCOM</em>, <em>243</em>, 108318. (<a href='https://doi.org/10.1016/j.comcom.2025.108318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient spectrum utilization is a major challenge in highly dynamic vehicular environments due to the scarcity of spectrum resources. Cognitive Radio (CR) has emerged as a solution to improve spectrum utilization by enabling opportunistic access in IoV. In this context, channel-hopping based blind rendezvous offers a practical approach for decentralized spectrum access in CR-enabled IoV (CR-IoV). This paper presents a novel Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3PG)-based strategy for generating channel sequences in channel-hopping-based blind rendezvous. Unlike existing methods that overlook the quality of licensed spectrum, our approach ensures spectrum efficiency and QoS awareness in dynamic channel sequence generation. We formulate the channel sequence selection problem as a multi-objective optimization, aiming to maximize spectrum efficiency and minimize Time-To-Rendezvous (TTR) while meeting stringent latency and reliability requirements for vehicular communications. Each vehicle independently generates a channel-hopping sequence using a learning agent, which considers key channel quality metrics such as availability, reliability, and capacity. The generated sequences are employed in an asynchronous and asymmetric blind rendezvous process, enhancing adaptability to dynamic network conditions. Simulation results demonstrate that the proposed method significantly outperforms existing approaches, including Enhanced Jump-Stay (EJS), Single-radio Sunflower Set (SSS), Zero-type, One-type, and S-type (ZOS), Multi-Agent Q-Learning based Rendezvous (MAQLR), Exponential-weight algorithm for Exploration and Exploitation (Exp3), and Reinforcement Learning-based Channel-Hopping Rendezvous (RLCH) in terms of Expected TTR (ETTR), Maximum TTR (MTTR), delay, capacity, and reliability.},
  archive      = {J_COMCOM},
  author       = {Mehri Asadi Vasfi and Behrouz Shahgholi Ghahfarokhi},
  doi          = {10.1016/j.comcom.2025.108318},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108318},
  shortjournal = {Comput. Commun.},
  title        = {Channel-hopping sequence generation for blind rendezvous in cognitive radio-enabled internet of vehicles: A multi-agent twin delayed deep deterministic policy gradient-based method},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor-based task allocation using multi-objective optimization in GECC environment. <em>COMCOM</em>, <em>243</em>, 108316. (<a href='https://doi.org/10.1016/j.comcom.2025.108316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green Edge-Cloud Computing (GECC) has emerged as a promising paradigm to meet the diverse requirements of modern applications by integrating edge and cloud resources. Existing task allocation strategies in GECC environment often fail to adequately address the problems of low resource utilization and high economic cost in multi-objective conflicts. Therefore, this paper proposes a tensor-based task allocation scheme using multi-objective optimization in GECC environment. We first extend the task allocation problem in GECC environment to a multi-objective optimization problem and construct five optimization models, i.e., energy, system reliability, quality of experience, economic cost, and latency. Then, to address the complex relationship among these objectives, we develop a tensor-based representation and calculation model for task allocation across cloud, edge service, and edge device platforms. Furthermore, we propose a tensor-based multi-objective beetle swarm optimization algorithm combined speed limiting and dynamic step strategies (TMOBSO-SLDS) that dynamically adjusts the step size and limit speed to improve the global search efficiency and the diversity of solution set. Extensive experimental results in various task allocation scenarios demonstrate that our proposed TMOBSO-SLDS algorithm outperforms existing approaches, as measured by the HV value. It can significantly enhance the diversity of the solution set and improve resource utilization.},
  archive      = {J_COMCOM},
  author       = {Huazhong Liu and Longtao Huang and Yuan Tian and Jihong Ding and Xiaoxue Yin and Guangshun Zhang},
  doi          = {10.1016/j.comcom.2025.108316},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108316},
  shortjournal = {Comput. Commun.},
  title        = {Tensor-based task allocation using multi-objective optimization in GECC environment},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks. <em>COMCOM</em>, <em>243</em>, 108315. (<a href='https://doi.org/10.1016/j.comcom.2025.108315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-distance data transmission between Internet of Things (IoT) devices and remote cloud center often leads to unacceptable latency for certain tasks. Fog computing has emerged as a promising solution for low-latency tasks. Consequently, the concept of cloud-fog collaborative networks has garnered significant attention. However, existing research primarily focuses on heterogeneous tasks, overlooking the crucial aspect of considering both task priority and second unloading. To address this gap, this paper proposes a novel task unloading scheme that concurrently takes preemptive priority and second optional unloading into account. In this scheme, delay-sensitive tasks (DSTs) are given preemptive priority over delay-tolerant tasks (DTTs). Furthermore, some DTTs may undergo preprocessing in the fog layer to optimize resource utilization. Moreover, tasks encountering blocking or preemption in the fog layer can also be secondarily unloaded to the cloud layer. In this framework, we devise a four-dimensional Markov chain (4DMC) to model and analyze this process. Through numerical experiments, we assess performance indicators under various parameters. Ultimately, our proposed strategy is compared with the unloading scheme that does not incorporate second unloading through both numerical analysis and simulation validation. The results indicate that our scheme notably enhances the throughput of DTTs, albeit at a marginal performance trade-off.},
  archive      = {J_COMCOM},
  author       = {Yuan Zhao and Hongmin Gao and Shuaihua Liu},
  doi          = {10.1016/j.comcom.2025.108315},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108315},
  shortjournal = {Comput. Commun.},
  title        = {A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning. <em>COMCOM</em>, <em>243</em>, 108314. (<a href='https://doi.org/10.1016/j.comcom.2025.108314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive resource provisioning has become crucial for cloud-based applications, especially those managing real-time traffic like Voice over IP (VoIP), which experience rapidly fluctuating workloads. Traditional static provisioning methods often fall short in these dynamic environments, leading to inefficiencies and potential service disruptions. Existing solutions struggle to maintain performance under varying traffic conditions, particularly for time-sensitive applications. This paper introduces ScaleIP, a hybrid autoscaling solution for containerized VoIP services that offers real-time adaptability and efficient resource management. ScaleIP leverages Deep Reinforcement Learning to make dynamic and efficient scaling decisions, improving call latency, increasing the number of successfully routed calls, and maximizing resource utilization. We evaluated ScaleIP through extensive experiments conducted on a real testbed utilizing the customer Call Detail Record (CDR) from 2023 provided by World Direct, encompassing over 89 million calls. The results show that ScaleIP consistently maintains call latency below 2 s, increases the number of successfully routed calls by 3.26 ×, and increases the resource utilization up to 60 % compared to state-of-the-art autoscaling methods.},
  archive      = {J_COMCOM},
  author       = {Zahra Najafabadi Samani and Juan Aznar Poveda and Dominik Gratz and Rene Hueber and Philipp Kalb and Thomas Fahringer},
  doi          = {10.1016/j.comcom.2025.108314},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108314},
  shortjournal = {Comput. Commun.},
  title        = {ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond performance comparing the costs of applying deep and shallow learning. <em>COMCOM</em>, <em>243</em>, 108312. (<a href='https://doi.org/10.1016/j.comcom.2025.108312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of mobile network traffic and the emergence of complex applications, such as self-driving cars and augmented reality, demand ultra-low latency, high throughput, and massive device connectivity, which traditional network design approaches struggle to meet. These issues were initially addressed in Fifth-Generation (5G) and Beyond-5G (B5G) networks, where Artificial Intelligence (AI), particularly Deep Learning (DL), is proposed to optimize the network and to meet these demanding requirements. However, the resource constraints and time limitations inherent in telecommunication networks raise questions about the practicality of deploying large Deep Neural Networks (DNNs) in these contexts. This paper analyzes the costs of implementing DNNs by comparing them with shallow ML models across multiple datasets and evaluating factors such as execution time and model interpretability. Our findings demonstrate that shallow ML models offer comparable performance to DNNs, with significantly reduced training and inference times, achieving up to 90% acceleration. Moreover, shallow models are more interpretable, as explainability metrics struggle to agree on feature importance values even for high-performing DNNs.},
  archive      = {J_COMCOM},
  author       = {Rafael Teixeira and Leonardo Almeida and Pedro Rodrigues and Mário Antunes and Diogo Gomes and Rui L. Aguiar},
  doi          = {10.1016/j.comcom.2025.108312},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108312},
  shortjournal = {Comput. Commun.},
  title        = {Beyond performance comparing the costs of applying deep and shallow learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
