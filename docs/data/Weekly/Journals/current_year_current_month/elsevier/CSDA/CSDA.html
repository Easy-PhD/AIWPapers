<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda">CSDA - 7</h2>
<ul>
<li><details>
<summary>
(2026). An algorithm for estimating threshold boundary regression models. <em>CSDA</em>, <em>214</em>, 108274. (<a href='https://doi.org/10.1016/j.csda.2025.108274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative iterative two-stage algorithm designed for estimating threshold boundary regression (TBR) models. By transforming the non-differentiable least-squares (LS) problem inherent in fitting TBR models into an optimization framework, our algorithm combines the optimization of a weighted classification error function for the threshold model with obtaining LS estimators for regression models. To improve the efficiency and flexibility of TBR model estimation, we integrate the weighted support vector machine (WSVM) as a surrogate method for solving the weighted classification problem. The TBR-WSVM algorithm offers several key advantages over recently developed methods: it eliminates pre-specification requirements for threshold parameters, accommodates flexible estimation of nonlinear threshold boundaries, and streamlines the estimation process. We conducted several simulation studies to illustrate the finite-sample performance of TBR-WSVM. Finally, we demonstrate the practical applicability of the TBR model through a real data analysis.},
  archive      = {J_CSDA},
  author       = {Chih-Hao Chang and Takeshi Emura and Shih-Feng Huang},
  doi          = {10.1016/j.csda.2025.108274},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108274},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An algorithm for estimating threshold boundary regression models},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rate accelerated inference for integrals of multivariate random functions. <em>CSDA</em>, <em>214</em>, 108273. (<a href='https://doi.org/10.1016/j.csda.2025.108273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of integrals is a fundamental task in the analysis of functional data, where the data are typically considered as random elements in a space of squared integrable functions. Effective unbiased estimation and inference procedures are proposed for integrals of uni- and multivariate random functions. Applications to key problems in functional data analysis involving random design points are examined and illustrated. In the absence of noise, the proposed estimates converge faster than the sample mean and standard numerical integration algorithms. The estimator also supports effective inference by generally providing better coverage with shorter confidence and prediction intervals in both noisy and noiseless settings.},
  archive      = {J_CSDA},
  author       = {Valentin Patilea and Sunny G․ W․ Wang},
  doi          = {10.1016/j.csda.2025.108273},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108273},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rate accelerated inference for integrals of multivariate random functions},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust selection of the number of change-points via FDR control. <em>CSDA</em>, <em>214</em>, 108272. (<a href='https://doi.org/10.1016/j.csda.2025.108272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust quantification of uncertainty regarding the number of change-points presents a significant challenge in data analysis, particularly when employing false discovery rate (FDR) control techniques. Emphasizing the detection of genuine signals while controlling false positives is crucial, especially for identifying shifts in location parameters within flexible distributions. Traditional parametric methods often exhibit sensitivity to outliers and heavy-tailed data. Addressing this limitation, a robust method accommodating diverse data structures is proposed. The approach constructs component-wise sign-based statistics. Leveraging the global symmetry inherent in these statistics enables the derivation of data-driven thresholds suitable for multiple testing scenarios. Method development occurs within the framework of U-statistics, which naturally encompasses existing cumulative sum-based procedures. Theoretical guarantees establish FDR control for the component-wise sign-based method under mild assumptions. Demonstrations of effectiveness utilize simulations with synthetic data and analyses of real data.},
  archive      = {J_CSDA},
  author       = {Hui Chen and Chengde Qian and Qin Zhou},
  doi          = {10.1016/j.csda.2025.108272},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108272},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust selection of the number of change-points via FDR control},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Kernel density estimation with a markov chain monte carlo sample. <em>CSDA</em>, <em>214</em>, 108271. (<a href='https://doi.org/10.1016/j.csda.2025.108271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference relies on the posterior distribution, which is often estimated with a Markov chain Monte Carlo sampler. The sampler produces a dependent stream of variates from the limiting distribution of the Markov chain, the posterior distribution. When one wishes to display the estimated posterior density, a natural choice is the histogram. However, abundant literature has shown that the kernel density estimator is more accurate than the histogram in terms of mean integrated squared error for an i.i.d. sample. With this as motivation, a kernel density estimation method is proposed that is appropriate for the dependence in the Markov chain Monte Carlo output. To account for the dependence, the cross-validation criterion is modified to select the bandwidth in standard kernel density estimation approaches. A data-driven adjustment to the biased cross-validation method is suggested with introducing the integrated autocorrelation time of the kernel. The convergence of the modified bandwidth to the optimal bandwidth is shown by adapting theorems from the time series literature. Simulation studies show that the proposed method finds the bandwidth close to the optimal value, while standard methods lead to smaller bandwidths under Markov chain samples and hence to undersmoothed density estimates. A study with real data shows that the proposed method has a considerably smaller integrated mean squared error than standard methods. The R package KDEmcmc to implement the suggested algorithm is available on the Comprehensive R Archive Network.},
  archive      = {J_CSDA},
  author       = {Hang J. Kim and Steven N. MacEachern and Young Min Kim and Yoonsuh Jung},
  doi          = {10.1016/j.csda.2025.108271},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108271},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Kernel density estimation with a markov chain monte carlo sample},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Measure selection for functional linear model. <em>CSDA</em>, <em>214</em>, 108270. (<a href='https://doi.org/10.1016/j.csda.2025.108270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in modern science have led to an increased prevalence of functional data, which are usually viewed as elements of the space of square-integrable functions L 2 . Core methods in functional data analysis, such as functional principal component analysis, are typically grounded in the Hilbert structure of L 2 and rely on inner products based on integrals with respect to the Lebesgue measure over a fixed domain. A more flexible framework is proposed, where the measure can be arbitrary, allowing natural extensions to unbounded domains and prompting the question of optimal measure choice. Specifically, a novel functional linear model is introduced that incorporates a data-adaptive choice of the measure that defines the space, alongside an enhanced function principal component analysis. Selecting a good measure can improve the model’s predictive performance, especially when the underlying processes are not well-represented when adopting the default Lebesgue measure. Simulations, as well as applications to COVID-19 data and the National Health and Nutrition Examination Survey data, show that the proposed approach consistently outperforms the conventional functional linear model.},
  archive      = {J_CSDA},
  author       = {Su I Iao and Hans-Georg Müller},
  doi          = {10.1016/j.csda.2025.108270},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108270},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Measure selection for functional linear model},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Overview of normal-reference tests for high-dimensional means with implementation in the r package ‘HDNRA’. <em>CSDA</em>, <em>214</em>, 108269. (<a href='https://doi.org/10.1016/j.csda.2025.108269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of testing for equal mean vectors in high-dimensional data poses significant difficulties in statistical inference. Much of the existing literature introduces methods that often rely on stringent regularity conditions for the underlying covariance matrices, enabling asymptotic normality of test statistics. However, this can lead to complications in controlling test size. To address these issues, a new set of tests has emerged, leveraging the normal-reference approach to improve reliability. The latest normal-reference methods for testing equality of mean vectors in high-dimensional samples, potentially with differing covariance structures, are reviewed. The theoretical underpinnings of these tests are revisited, providing a new unified justification for the validity of centralized L 2 -norm-based normal-reference tests (NRTs) by deriving the convergence rate of the distance between the null distribution of the test statistic and its corresponding normal-reference distribution. To facilitate practical application, an R package, HDNRA , is introduced, implementing these NRTs and extending beyond the two-sample problem to accommodate general linear hypothesis testing (GLHT). The package, designed with user-friendliness in mind, achieves efficient computation through a core implemented in C++ using Rcpp , OpenMP , and RcppArmadillo . Examples with real datasets are included, showcasing the application of various tests and providing insights into their practical utility.},
  archive      = {J_CSDA},
  author       = {Pengfei Wang and Tianming Zhu and Jin-Ting Zhang},
  doi          = {10.1016/j.csda.2025.108269},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108269},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Overview of normal-reference tests for high-dimensional means with implementation in the r package ‘HDNRA’},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-dimensional subgroup functional quantile regression with panel and dependent data. <em>CSDA</em>, <em>214</em>, 108268. (<a href='https://doi.org/10.1016/j.csda.2025.108268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional additive functional partial linear single-index quantile regression with high-dimensional parameters under subgroup panel data is investigated. Based on spline-based approach, we construct oracle estimators of the unknown parameter and functions, and discuss their consistency with rates and asymptotic normality under α -mixing assumptions. A penalized estimation method by using the SCAD technique is introduced to estimate the additive functions and parameter, enabling variable selection and automatic identification of the number of groups. Hypothesis testing for the parameter is also considered, and the asymptotic distributions of the restricted estimators and the test statistic are derived under both the null and local alternative hypotheses. Simulation studies and real data analysis are conducted to verify the validity of the proposed methods and applications.},
  archive      = {J_CSDA},
  author       = {Xiao-Ge Yu and Han-Ying Liang},
  doi          = {10.1016/j.csda.2025.108268},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108268},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional subgroup functional quantile regression with panel and dependent data},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
