<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor">EJOR - 49</h2>
<ul>
<li><details>
<summary>
(2026). Optimistic and pessimistic approaches for cooperative games. <em>EJOR</em>, <em>328</em>(2), 725-733. (<a href='https://doi.org/10.1016/j.ejor.2025.09.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative game theory explores how to fairly allocate the joint value generated by a group of decision-makers, but its application is compromised by the large number of counterfactuals needed to compute the value of all coalitions, a problem made even more complicated when externalities are present. We provide a theoretical foundation for a simplification used in many applications, in which the value of a coalition is computed assuming that they either select before or after the complement set of agents, providing optimistic and pessimistic values on what a coalition should receive. In a vast set of problems exhibiting what we call feasibility externalities, we show that ensuring a coalition does not receive more than its optimistic value is always at least as difficult as ensuring it receives its pessimistic value. Furthermore, under the presence of negative externalities, we establish the existence of stable allocations that respect these bounds. Finally, we examine well-known optimization-based applications and their corresponding cooperative games to show how our results lead to new insights and allow the derivation of further results from the existing literature.},
  archive      = {J_EJOR},
  author       = {Ata Atay and Christian Trudeau},
  doi          = {10.1016/j.ejor.2025.09.002},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {725-733},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimistic and pessimistic approaches for cooperative games},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sustainable product development under profit-sharing crowdfunding: An analytical approach to market structure and government policy. <em>EJOR</em>, <em>328</em>(2), 704-724. (<a href='https://doi.org/10.1016/j.ejor.2025.08.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops an integrated analytical framework to examine how crowdfunding, market structure, and government policy interact to shape environmentally sustainable product development (SPD). Focusing on profit-sharing and securities-based crowdfunding, we model how investor opportunity costs, risk preferences, platform fees, and regulatory schemes (voluntary vs. mandatory under fiscal vs. non-fiscal policy regimes) influence firm strategies and outcomes across economic, social, and environmental (ESE) dimensions. Firm behavior is analyzed under monopoly and duopoly settings to explore variation in market power and competitive intensity. Findings reveal that voluntary greening can achieve strong ESE outcomes in monopolistic markets with low financial frictions and environmentally aware consumers. In contrast, competitive or uncertain environments often require benchmark-based regulation and fiscal instruments to sustain environmental investments. Two dominant firm profiles emerge: the Voluntary sustainability leader, which performs well under favorable market and investor conditions without policy intervention, and the Policy-driven strategist, which depends on regulatory standards and fiscal tools to overcome competitive pressures and risk constraints. By formalizing investor-entrepreneur interactions and embedding environmental quality as a strategic variable, this research advances the literature on crowdfunding and market-driven sustainability. It provides actionable insights for aligning crowdfunding design and policy frameworks with the broader goals of green innovation and public sustainability.},
  archive      = {J_EJOR},
  author       = {Raziyeh Reza-Gharehbagh and Madeleine Pullman},
  doi          = {10.1016/j.ejor.2025.08.020},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {704-724},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sustainable product development under profit-sharing crowdfunding: An analytical approach to market structure and government policy},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimal dividend and scale of business strategies with reinsurance and premium pricing for insurance company. <em>EJOR</em>, <em>328</em>(2), 694-703. (<a href='https://doi.org/10.1016/j.ejor.2025.07.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the optimal dividend and business scale strategies aimed at maximizing the value of an insurance company. While prior studies typically assume that insurers can only adjust their business scale through reinsurance, this study extends the framework by allowing the insurer to control the premium rate. Under more realistic market assumptions, we examine the joint optimization problem for two common types of reinsurance — proportional and excess-of-loss — across both arbitrage and non-arbitrage scenarios. We derive the optimal strategies for dividends and premium pricing, along with their corresponding value functions. The results show that the insurer should decrease the premium rate and reduce reinsurance coverage as the surplus increases. The optimal dividend policy follows a barrier strategy. Economic interpretations and numerical examples are provided to illustrate the findings.},
  archive      = {J_EJOR},
  author       = {Dingjun Yao and Bo Yang and Xin Xu and Youwei Li and Yizhi Wang},
  doi          = {10.1016/j.ejor.2025.07.039},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {694-703},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal dividend and scale of business strategies with reinsurance and premium pricing for insurance company},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Anticipating delays in recruitment: Explainable machine learning for the prediction of hard-to-fill online job vacancies. <em>EJOR</em>, <em>328</em>(2), 680-693. (<a href='https://doi.org/10.1016/j.ejor.2025.06.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online job vacancy (OJV) platforms have transformed the labor market by enabling employers to advertise jobs to a wide audience. Particularly in tight labor markets, quickly identifying vacancies likely to suffer prolonged durations is crucial. This study utilizes data from the Flemish public employment service's OJV platform to examine the effectiveness of machine learning in predicting hard-to-fill vacancies. We achieve notable predictive performance with XGBoost in forecasting recruitment delays and demonstrate the importance of capturing non-linear patterns in OJV data. SHAP (SHapley Additive exPlanations) values reveal that the textual content of vacancies and latent company characteristics are key predictors of hiring delays. Counterfactual-SHAP insights provide practical guidance for refining recruitment strategies, enhancing labor market forecasts, and informing targeted policies.},
  archive      = {J_EJOR},
  author       = {Wouter Dossche and Sarah Vansteenkiste and Bart Baesens and Wilfried Lemahieu},
  doi          = {10.1016/j.ejor.2025.06.027},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {680-693},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Anticipating delays in recruitment: Explainable machine learning for the prediction of hard-to-fill online job vacancies},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Using diet optimization and machine learning for the design of healthy and acceptable menu plans. <em>EJOR</em>, <em>328</em>(2), 668-679. (<a href='https://doi.org/10.1016/j.ejor.2025.06.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of dietary plans relies on understanding and modelling consumer acceptance, yet quantifying this poses a challenge due to the complexity of individual preferences. Recent research is focused on deriving acceptability constraints directly from data, as demonstrated by its application in designing food baskets with a limited number of commodities. In this study, we applied diet optimization with machine learning to the more complex task of menu planning. This involved considering hundreds of potential food alternatives and assessing their compatibility within a meal using a recipe completion algorithm. Compared to the traditional diet modelling approach of food group filtering, the recipe completion model delivered diets with either higher nutritional adequacy or greater substitute acceptability, depending on the number of food groups used in the traditional method. While more research is needed to further improve the acceptability of substitutions, combining diet optimization with recipe completion presents a promising approach to enhance the nutritional adequacy of individual diets while maintaining the acceptability of food combinations within meals.},
  archive      = {J_EJOR},
  author       = {Dominique van Wonderen and Johanna C. Gerdessen and Alida Melse-Boonstra and Marleen C. Onwezen},
  doi          = {10.1016/j.ejor.2025.06.015},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {668-679},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Using diet optimization and machine learning for the design of healthy and acceptable menu plans},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Project monitoring and control with an empirically grounded budget-release model. <em>EJOR</em>, <em>328</em>(2), 646-667. (<a href='https://doi.org/10.1016/j.ejor.2025.06.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Project monitoring and control (PMC) is a process of measuring a project’s progress and taking corrective action when necessary to ensure successful project completion. However, most existing models lack empirical validation of their assumptions and effectiveness, which limits their practical use. We fill in this gap by using empirical data from 97 real projects to calibrate activity-duration distributions and assess activity-duration dependencies, integrating these empirical foundations into an enhanced PMC model. We further improve the model by incorporating budget-release timing constraints and introducing two new policies for crashing and fast-tracking based on a project’s specific time and cost characteristics. Extensive computational experiments using empirical and artificial data evaluate the effectiveness of these policies. Because the budget-release policies and corrective action types depend on project characteristics such as activity-duration dependencies and topological network structure, key insights from this study can be usefully applied by project managers as heuristics even without a detailed model of their project.},
  archive      = {J_EJOR},
  author       = {Jie Song and Jinbo Song and Tyson Browning and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2025.06.007},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {646-667},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Project monitoring and control with an empirically grounded budget-release model},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). The continuous roll-on roll-off dual cycling problem with tugs and driver-handled cargo units. <em>EJOR</em>, <em>328</em>(2), 633-645. (<a href='https://doi.org/10.1016/j.ejor.2025.05.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roll-on roll-off vessels are a popular mode of transport in short-sea shipping. In this domain, appropriate stevedoring procedures are crucial to enhance efficiency. This includes dual cycling, where tugs simultaneously load and unload the vessel. Dual cycling reduces turnaround time, thereby giving the vessel more time to travel and allowing for slow steaming and reduced emissions. We extend the roll-on roll-off dual cycling problem by incorporating a continuous time horizon and differentiating cargo units that are handled by their own drivers and units that must be handled by a tug. We propose a mixed integer linear programming model for generating an efficient schedule that minimizes overall makespan by optimizing the sequence of cargo units and the assignment of cargo units to tugs. To solve instances of real-world size with acceptable computational effort, we provide a range of heuristics, including a biased random-key genetic algorithm. Compared to the linear programming model and on instances of real-world size, the genetic algorithm finds good solutions quickly. We derive managerial insights from a sensitivity analysis and show that dual cycling and strategic positioning of driver-handled units can reduce turnaround time by 14.5%, reducing emissions of the considered vessel by more than 8%. We demonstrate the robustness of these insights in uncertain environments through a simulation study.},
  archive      = {J_EJOR},
  author       = {Teresa Marquardt and Arne Heinold and Catherine Cleophas and Frank Meisel},
  doi          = {10.1016/j.ejor.2025.05.050},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {633-645},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The continuous roll-on roll-off dual cycling problem with tugs and driver-handled cargo units},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimizing treatment allocation in the presence of interference. <em>EJOR</em>, <em>328</em>(2), 620-632. (<a href='https://doi.org/10.1016/j.ejor.2025.09.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Influence Maximization (IM), the objective is to — given a budget — select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover, effects that follow from targeting these customers. Recently, new methods to estimate treatment effects in the presence of network interference have been proposed. However, the issue of how to leverage these models to make better treatment allocation decisions has been largely overlooked. Traditionally, in Uplift Modeling (UM), entities are ranked according to estimated treatment effect, and the top entities are allocated treatment. Since, in a network context, entities influence each other, the UM ranking approach will be suboptimal. The problem of finding the optimal treatment allocation in a network setting is NP-hard, and generally has to be solved heuristically. To fill the gap between IM and UM, we propose OTAPI: Optimizing Treatment Allocation in the Presence of Interference to find solutions to the IM problem using treatment effect estimates. OTAPI consists of two steps. First, a causal estimator is trained to predict treatment effects in a network setting. Second, this estimator is leveraged to identify an optimal treatment allocation by integrating it into classic IM algorithms. We demonstrate that this novel method outperforms classic IM and UM approaches on both synthetic and semi-synthetic datasets.},
  archive      = {J_EJOR},
  author       = {Daan Caljon and Jente Van Belle and Jeroen Berrevoets and Wouter Verbeke},
  doi          = {10.1016/j.ejor.2025.09.015},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {620-632},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing treatment allocation in the presence of interference},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Soft regression trees: A model variant and a decomposition training algorithm. <em>EJOR</em>, <em>328</em>(2), 607-619. (<a href='https://doi.org/10.1016/j.ejor.2025.08.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are widely used for classification and regression tasks in a variety of application fields due to their interpretability and good accuracy. During the past decade, growing attention has been devoted to globally optimized decision trees with deterministic or soft splitting rules at branch nodes, which are trained by optimizing the error function over all the tree parameters. In this work, we propose a new variant of soft multivariate regression trees (SRTs) where, for every input vector, the prediction is defined as the linear regression associated to a single leaf node, namely, the leaf node obtained by routing the input vector from the root along the branches with higher probability. SRTs exhibit the conditional computational property, i.e., each prediction depends on a small number of nodes (parameters), and our nonlinear optimization formulation for training them is amenable to decomposition. After showing a universal approximation result for SRTs, we present a decomposition training algorithm including a clustering-based initialization procedure and a heuristic for rerouting the input vectors along the tree. Under mild assumptions, we establish asymptotic convergence guarantees. Experiments on 15 well-known datasets indicate that our SRTs and decomposition algorithm yield higher accuracy and robustness compared with traditional soft regression trees trained using the nonlinear optimization formulation of Blanquero et al. (2021), and a significant reduction in training times as well as a slightly better average accuracy compared with the mixed-integer optimization approach of Bertsimas and Dunn (2019). We also report a comparison with the Random Forest ensemble method.},
  archive      = {J_EJOR},
  author       = {Antonio Consolo and Edoardo Amaldi and Andrea Manno},
  doi          = {10.1016/j.ejor.2025.08.050},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {607-619},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Soft regression trees: A model variant and a decomposition training algorithm},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An explainable machine learning framework for recurrent event data analysis. <em>EJOR</em>, <em>328</em>(2), 591-606. (<a href='https://doi.org/10.1016/j.ejor.2025.09.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel explainable temporal point process (TPP) model, Stratified Hawkes Point Process (SHPP), for modelling recurrent event data (RED). Unlike existing approaches that treat temporal influence as a black box or rely on post-hoc explanations, SHPP structurally decomposes event intensities into semantically meaningful components for describing self-, Markovian, and joint influences. This decomposition enables direct quantification of how past events contribute to future event risks, termed as influence values. We further provide a sufficient condition for mean-square stability based on kernel decay, ensuring long-term boundedness of intensities and realistic behavioural predictions. Experiments and an e-commerce case study demonstrate SHPP’s ability to deliver accurate, interpretable, and stable modelling of complex event-driven systems.},
  archive      = {J_EJOR},
  author       = {Qi Lyu and Shaomin Wu},
  doi          = {10.1016/j.ejor.2025.09.005},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {591-606},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An explainable machine learning framework for recurrent event data analysis},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Decision space dynamic niching-based method for constrained multiobjective evolutionary optimization. <em>EJOR</em>, <em>328</em>(2), 574-590. (<a href='https://doi.org/10.1016/j.ejor.2025.07.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a set with a good approximation to the Pareto-optimal solutions in the multiobjective optimization problem (MOP) is a challenging task in terms of convergence toward and diversity across the Pareto optimal front (PoF). In some cases, solving MOPs requires satisfying certain constraints, which significantly increases the complexity of the problem. Such problems are constrained multiobjective optimization problems (CMOPs) and pose considerable computational challenges. Many constrained multiobjective evolutionary algorithms (CMOEAs) face challenges in avoiding becoming trapped in local optima, which impacts convergence, and offer solutions that lack good coverage of the PoF, implying weak diversity. All these nonoptimal or partially optimal solutions in the objective space are essentially clustered in local optimality dilemmas in the decision space. To better eliminate the convergence and diversity challenges caused by clustered solutions, this paper proposes a decision space dynamic niching-based (DSDN) method to better address CMOPs. Specifically, the DSDN method adds a dynamic decision space niche as an additional criterion to the traditional Pareto-constrained dominance principle (Pareto-CDP). The better preserved solutions must satisfy the Pareto-CDP and the condition within the niche radius of other solutions, which strictly meets the original dominance relationship requirement while relaxing the nondominance threshold. As a result, the dynamic adjustment of the niche radius ( N R ) effectively balances the exploitation and exploration of solutions in the decision space while enhancing both convergence and diversity in the objective space. Experiments conducted on four widely recognized test suites and three real-world case studies have demonstrated that the DSDN method yields significantly better results than the original Pareto-CDP algorithms. Furthermore, the proposed approach is competitive with or comparable to seven other state-of-the-art CMOEAs.},
  archive      = {J_EJOR},
  author       = {Fan Yu and Qun Chen and Jinlong Zhou},
  doi          = {10.1016/j.ejor.2025.07.002},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {574-590},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Decision space dynamic niching-based method for constrained multiobjective evolutionary optimization},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Impact of technology spillovers and subsidies on innovation consortia dynamics. <em>EJOR</em>, <em>328</em>(2), 560-573. (<a href='https://doi.org/10.1016/j.ejor.2025.06.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In innovation consortia, governments and enterprises view technology spillovers and subsidies differently, which reduces the effectiveness of policies and undermines cooperation. This work investigates the dynamics of cooperation among leading enterprises (LEs) and small/medium enterprises (SMEs), using an evolutionary game model with intrinsic, extrinsic and mixed strategies. We show that there may exist more than one evolutionary equilibrium of enterprise cooperation under multi-strategy choices. A high subsidy may induce enterprises to speculate and reach a suboptimal evolutionary equilibrium. Furthermore, our findings indicate that SMEs are particularly sensitive to changes in subsidy levels, income distribution ratios, and project income, with their speculative behavior exhibiting a lagging effect. Also, increasing the income distribution ratio of LEs can promote active cooperation between the groups and reduce the speculative tendency of SMEs. This occurs because it magnifies the effect of technology spillover on the income of SMEs and enhances the income of the leading enterprises. Our work improves understanding of cooperative factors and provides new recommendations for government and leading enterprises to improve cooperation and managerial performance.},
  archive      = {J_EJOR},
  author       = {Zheng Yang and Lin Li and Nicholas G. Hall and Yongzeng Lai and Yijiang Zhou},
  doi          = {10.1016/j.ejor.2025.06.031},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {560-573},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Impact of technology spillovers and subsidies on innovation consortia dynamics},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Online quality endorsement to improve consumer trust: Blockchain or self-hosted livestream?. <em>EJOR</em>, <em>328</em>(2), 545-559. (<a href='https://doi.org/10.1016/j.ejor.2025.04.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online products have long suffered from consumer distrust, putting them at a disadvantage when online brands compete with offline brands. To address this issue, many online brands have adopted blockchain as a means of quality endorsement to improve consumer trust. Alternatively, livestream e-commerce has shown the capability of both quality endorsement and demand creation by real-time interactions with consumers because the application of AR/VR technology and the use of online sales force can effectively induce the herding mentality. For many small and medium-sized online brands, it remains unclear which approach is better, so we formulate the key tradeoffs in the online brand's choice to improve consumer trust in the presence of offline brand's competition. Our research delves into the influence of three key factors on livestream e-commerce: the cost associated with adopting blockchain technology, the strength of positive network externality, and the potential downside of consumer returns. Contrary to conventional wisdom, we find that when the return cost is high and the network externality in livestream is weak, opting for livestream as a quality endorsement can actually benefit the online brand. We also find that the online brand is capable of mitigating the return cost by transferring it to consumers through charging a high retail price, which increases the likelihood of favoring livestream. Our findings shed light on the building and improvement of online consumer trust, contributing to the high-quality development of online-offline business in the new era of consumption.},
  archive      = {J_EJOR},
  author       = {Baozhuang Niu and Jian Dong and Xinhu Yu and Yulan Wang},
  doi          = {10.1016/j.ejor.2025.04.010},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {545-559},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Online quality endorsement to improve consumer trust: Blockchain or self-hosted livestream?},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Production trade-offs in free disposal hull technologies. <em>EJOR</em>, <em>328</em>(2), 530-544. (<a href='https://doi.org/10.1016/j.ejor.2025.06.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data envelopment analysis, production trade-offs are value judgements that represent simultaneous changes to the inputs and outputs assumed to be technologically possible for any production unit in the technology. The specification of production trade-offs generally leads to an enlargement of the model of technology and increasing its discriminating power on efficiency. In conventional convex variable and constant returns-to-scale models, production trade-offs are the dual forms of weight restrictions. In this paper, we extend the use of production trade-offs to the free disposal hull model of technology and its constant, non-increasing and non-decreasing returns-to-scale variants, in a single unifying development. We provide an axiomatic definition of the new nonconvex technologies, explore the notion of consistent trade-offs in such technologies and develop methods for its testing. We further develop different computational approaches for nonconvex models with production trade-offs. We illustrate the new models by an application in the context of higher education.},
  archive      = {J_EJOR},
  author       = {Mahmood Mehdiloo and Grammatoula Papaioannou and Victor V. Podinovski},
  doi          = {10.1016/j.ejor.2025.06.032},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {530-544},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Production trade-offs in free disposal hull technologies},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Price optimization for round trip car sharing. <em>EJOR</em>, <em>328</em>(2), 511-529. (<a href='https://doi.org/10.1016/j.ejor.2025.06.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Car sharing, car clubs and short-term rentals could support the transition toward net zero but their success depends on them being financially sustainable for service providers and attractive to end users. Dynamic pricing could support this by incentivizing users while balancing supply and demand. We describe the usage of a round trip car sharing fleet by a continuous time Markov chain model, which reduces to a multi-server queuing model where hire duration is assumed independent of the hourly rental price. We present analytical and simulation optimization models that allow the development of dynamic pricing strategies for round trip car sharing systems; in particular identifying the optimal hourly rental price. The analytical tractability of the queuing model enables fast optimization to maximize expected hourly revenue for either a single fare system or a system where the fare depends on the number of cars on hire, while accounting for stochasticity in customer arrival times and durations of hire. Simulation optimization is used to optimize prices where the fare depends on the time of day or hire duration depends on price. We present optimal prices for a given customer population and show how the expected revenue and car availability depend on the customer arrival rate, willingness-to-pay distribution, dependence of the hire duration on price, and size of the customer population. The results provide optimal strategies for pricing of car sharing and inform strategic managerial decisions such as whether to use time- or state-dependent pricing and optimizing the fleet size.},
  archive      = {J_EJOR},
  author       = {Christine S.M. Currie and Rym M’Hallah and Beatriz Brito Oliveira},
  doi          = {10.1016/j.ejor.2025.06.024},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {511-529},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Price optimization for round trip car sharing},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). The development strategy of supply chain intelligent technology considering technology development uncertainty. <em>EJOR</em>, <em>328</em>(2), 496-510. (<a href='https://doi.org/10.1016/j.ejor.2025.07.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It’s crucial for both manufacturing and logistics industries to improve logistics efficiency and reduce logistics loss during transportation, storage, and other processes through the development and application of intelligent technology. By focusing on three potential modes of intelligent technology development cooperation between a manufacturer and its logistics provider, we examine the impact of such collaboration on reducing Logistics loss, as well as explore the optimal mode of cooperation for both firms. Our analytical results indicate that compared to independent technology development, collaborative development of intelligent technology can mitigate the adverse effects of double-marginalization. Comparing the three modes of cooperation, we find that higher development cost can incentivize the collaboration between two firms, while higher integration cost and price elasticity may make the cost sharing mode preferable. It is noteworthy that the uncertainty of intelligent technology development exerts a significant moderating effect on the choice of cooperation mode. Heightened technology development uncertainty tends to incentivize both firms to pursue joint development in order to alleviate the negative impact of the uncertainty.},
  archive      = {J_EJOR},
  author       = {Peng Han and Yanfang Huo and Weihua Liu and Ershi Qi and Helen Cai},
  doi          = {10.1016/j.ejor.2025.07.026},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {496-510},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The development strategy of supply chain intelligent technology considering technology development uncertainty},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimal price, quantity, and return policy decisions of a two-period newsvendor with product reviews. <em>EJOR</em>, <em>328</em>(2), 477-495. (<a href='https://doi.org/10.1016/j.ejor.2025.06.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retailers (e-tailers) have high product return rates. To decrease product returns and consumers’ purchase risk, e-tailers can increase available product information through product reviews and/or offer money back guarantees (MBGs). We examine the rational expectations equilibrium of an e-tailer selling to myopic and strategic consumers over two periods. By the end of period 1, product reviews become available, providing a signal to strategic consumers who may wait to purchase in period 2. The e-tailer decides order quantity, prices, and return policy. We find that under both no returns and MBG, as strategic consumers’ patience increases, the optimal range of having them purchase in period 2 increases. For low signal accuracy, it is optimal to have strategic consumers purchase in period 1, whereas for high signal accuracy, it is optimal to have them purchase in period 2. However, under no returns and for a low signal accuracy, it may be optimal to have strategic consumers purchase in period 2 if their proportion is medium and they are patient. We also find that as consumers’ patience increases, the range where MBG dominates no returns increases. For heterogeneous signal accuracy among strategic consumers, equilibrium strategies are similar to the homogeneous case, except that when consumers’ patience is low, high heterogeneity allows the e-tailer to price discriminate, making low-signal accuracy consumers purchase in period 1 and high-signal accuracy consumers purchase in period 2. Also, as the proportion of low-signal accuracy consumers increases, price discrimination increases. Therefore, heterogeneity increases profit.},
  archive      = {J_EJOR},
  author       = {Huirong Fan and Moutaz Khouja and Jing Zhou},
  doi          = {10.1016/j.ejor.2025.06.023},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {477-495},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal price, quantity, and return policy decisions of a two-period newsvendor with product reviews},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Probabilistic forecast aggregation with statistical depth. <em>EJOR</em>, <em>328</em>(2), 460-476. (<a href='https://doi.org/10.1016/j.ejor.2025.06.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers aggregation methods for interval forecasts and forecasts of cumulative distribution functions (CDFs) when there are many forecasters, and past forecast accuracy may not be known. For aggregation, the median and trimmed means have been proposed as simple and robust alternatives to the mean, with some trimmed mean approaches enabling recalibration to widen or narrow the resulting interval or CDF forecast. For interval forecast aggregation, the median and trimming are applied to each bound separately. To try to use the available information better, we treat the bounds as a bivariate point with statistical depth used to order the points in terms of centrality. The deepest point can be viewed as the median interval forecast, and the depth of each point can be used as the basis for trimming. For CDF forecasts, the literature presents aggregation methods for which the median or trimmed mean are obtained for each point on the domain of the distribution. However, if one part of a CDF forecast is outlying, the appeal of using the rest of the CDF forecast is perhaps reduced. We use functional depth to provide a measure of centrality for each CDF forecast, and hence identify the deepest function, which can be viewed as the median forecast. We also use functional depth as the basis for trimming, and consider weighted depth to control the width of the resulting aggregated interval or CDF forecast. We provide empirical illustration using data from surveys of professional macroeconomic forecasters, and an application to growth-at-risk.},
  archive      = {J_EJOR},
  author       = {James W. Taylor},
  doi          = {10.1016/j.ejor.2025.06.028},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {460-476},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Probabilistic forecast aggregation with statistical depth},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A data-driven approach for strategic inventory placement in multi-echelon supply networks. <em>EJOR</em>, <em>328</em>(2), 446-459. (<a href='https://doi.org/10.1016/j.ejor.2025.06.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a data-driven solution for optimizing inventory buffers in large-scale supply networks. We study the placement and sizing of strategic inventories in multi-echelon supply chains where the decision maker faces uncertain demand with an unknown distribution influenced by explanatory variables. State-of-the-art multi-echelon inventory optimization models, such as the well-known guaranteed-service model (GSM), are non-linear and typically informed by distributional and parametric assumptions. They often rely on dynamic programming and are difficult to solve for large networks. We adapt the GSM to introduce a nonparametric, feature-driven approach to supply chain safety stock optimization that is based on mixed-integer linear programming (MILP). The MILP formulation sets cost-optimal base stocks, which are learned as linear functions of feature data under consideration of service level requirements. This integrated estimation and optimization approach is solved with commercial mathematical programming solvers and is enhanced by a Benders decomposition method for large networks. We extend the literature on data-driven inventory control by a multi-period and multi-echelon approach for safety stock planning in general, acyclic networks. On the real-world networks from Willems (2008), we find that incorporating feature information when setting safety stocks in large supply chains, on average, reduces operational costs out-of-sample. This value of feature information that the proposed model offers to decision-makers increases in demand volatility and is dependent on certain network characteristics.},
  archive      = {J_EJOR},
  author       = {Josef Svoboda and Stefan Minner},
  doi          = {10.1016/j.ejor.2025.06.022},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {446-459},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A data-driven approach for strategic inventory placement in multi-echelon supply networks},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Minimizing the maximum flow loss in the network maintenance scheduling problem with flexible arc outages. <em>EJOR</em>, <em>328</em>(2), 430-445. (<a href='https://doi.org/10.1016/j.ejor.2025.07.056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a network maintenance scheduling problem where maintenance tasks are carried out on the arcs of a network within flexible time windows. During maintenance, an arc is interrupted and no flow can pass through it. Such arc outages introduce flow loss and thus affect network capacity and service capability. For some public services operated on networks, possible blackouts and serious flow loss introducing extreme risks are generally unacceptable. The problem is to find a feasible schedule of maintenance tasks so that the maximum flow loss during the planning horizon is minimized. We introduce a mixed integer programming formulation and a Benders reformulation for the problem. A Benders decomposition algorithm based on a branch-and-cut framework is designed. Strengthened initial cuts and effective cuts are introduced to reduce feasible region so that the exact algorithm is accelerated. An efficient separation procedure is proposed to generate Benders optimality cuts. Computational experiments were conducted on a set of benchmark instances and a set of simulated instances based on telecommunication networks. Computational results show that our algorithm performs much better than applying a solver to the formulation and an existing Benders decomposition algorithm for a related problem. Optimal schedules can reduce extreme risks caused by large flow loss on the network. Since multiple optimal solutions may exist, hierarchical optimization is used to further select a desirable schedule, by either minimizing total flow loss or minimizing the duration of maximum flow loss. With adaptations, our algorithm also performs well for two extensions.},
  archive      = {J_EJOR},
  author       = {Shuang Jin and Ying Liu and Jing Zhou and Qian Hu},
  doi          = {10.1016/j.ejor.2025.07.056},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {430-445},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Minimizing the maximum flow loss in the network maintenance scheduling problem with flexible arc outages},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Stochastic quay partitioning problem. <em>EJOR</em>, <em>328</em>(2), 415-429. (<a href='https://doi.org/10.1016/j.ejor.2025.07.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the problem of dividing a quay of a container terminal into berth segments so that the quality of service for future ship arrivals is as good as possible. Since future arrivals are unknown, the alternative solutions are evaluated on various arrival scenarios generated for certain arrival intensity from a stochastic model referred to as a ship traffic model (STM). This problem will be referred to as a stochastic quay partitioning problem (SQPP). SQPP is defined by an STM, arrival intensity, quay length and a set of admissible berth lengths. Evaluation of an SQPP solution on one scenario is a problem of scheduling the arriving vessels on the berths, which is a classic berth allocation problem (BAP). In SQPP the sizes of BAP instances that must be solved by far exceed capabilities of the methods presented in the existing literature. Therefore, a novel approach to solving BAP is applied. Tailored portfolios of algorithms capable of solving very large BAP instances under limited runtime are used. Features of SQPP solutions are studied experimentally: patterns in selected berth lengths, dispersion of solutions quality and solutions similarity. We demonstrate, that partitioning a quay into equal-length berths is not the best approach. The largest vessel traffic is dominating in defining best quay partitions, but dedicating quays for shorter vessels give lower dispersion of solution quality. A set of algorithms to partition a quay is proposed and evaluated: methods based on integer linear programming (ILP) to match vessel classes arrival intensities with berth availability, hill climber, tabu search and a greedy approach. Only under high arrival intensity can these methods show their prowess. ILP methods have an advantage of low solution evaluation cost. Tabu is most flexible, but at high evaluation costs. To the best of our knowledge, SQPP is posed and solved for the first time in the operations research.},
  archive      = {J_EJOR},
  author       = {Jakub Wawrzyniak and Maciej Drozdowski and Éric Sanlaville},
  doi          = {10.1016/j.ejor.2025.07.043},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {415-429},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic quay partitioning problem},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Scheduling mixed batch machines with inclusive processing set restrictions and non-identical capacities. <em>EJOR</em>, <em>328</em>(2), 407-414. (<a href='https://doi.org/10.1016/j.ejor.2025.07.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new batch scheduling problem, name mixed batch scheduling problem, is received attentions recently. In a mixed batch scheduling model, the processing time of a job batch H is defined as α max j ∈ H { p j } + ( 1 − α ) ∑ j ∈ H p j , where α ∈ [ 0 , 1 ] is a constant. In other words, the processing time of a job batch is the weighted sum of the maximum processing time and the total processing time of jobs in the batch. In this paper, we study the problem of scheduling mixed batch machines with non-identical capacities under inclusive processing set restrictions, where the objective is to minimize the makespan of finishing all the jobs. We present a fast approximation algorithm with a performance ratio of 4 / 3 + α for the problem, which improves up the existing performance bounds in the literature. By providing a technical lemma, we are able to develop the first polynomial time approximation scheme (PTAS) for the problem. We also design linear-time approximation schemes for two important special cases of the problem.},
  archive      = {J_EJOR},
  author       = {Jinwen Ou and Weidong Li},
  doi          = {10.1016/j.ejor.2025.07.012},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {407-414},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scheduling mixed batch machines with inclusive processing set restrictions and non-identical capacities},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A matheuristic approach for the robust coloured travelling salesman problem with multiple depots. <em>EJOR</em>, <em>328</em>(2), 390-406. (<a href='https://doi.org/10.1016/j.ejor.2025.06.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a special type of the travelling salesman problem (TSP) called the coloured TSP (CTSP) is considered. The CTSP, which has many real-world applications, involves a set of salesmen, each assigned a specific colour, and cities that may have one or multiple colours. Salesmen are restricted to visiting only cities that share their colour. We consider a specific depot for each salesman, and the edge weights are uncertain, meaning that there is a set of possible scenarios for their values. A robust objective is considered and minimised using an artificial intelligence (AI)-driven matheuristic approach due to the high computational complexity of the problem. This approach integrates a variable neighbourhood search (VNS) framework with genetic algorithm (GA) and simulated annealing (SA) operators. More importantly, local improvements based on mathematical programming are applied to different parts of a proportion of the solutions using the concept of partial optimisation metaheuristic under special intensification conditions (POPMUSIC). A key innovation of our method is the use of an artificial neural network to guide the POPMUSIC procedure by selecting only solution segments with high improvement potential, thereby reducing computation time. Extensive computational experiments demonstrate the effectiveness of the proposed algorithm, which outperforms four state-of-the-art methods in solution quality and runs faster than three of them. We also investigate the contribution of individual algorithmic components and the cost of robustness. Furthermore, our method improves upon the best-known results for the single-depot deterministic version of the CTSP from the literature.},
  archive      = {J_EJOR},
  author       = {Abtin Nourmohammadzadeh and Stefan Voß},
  doi          = {10.1016/j.ejor.2025.06.018},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {390-406},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A matheuristic approach for the robust coloured travelling salesman problem with multiple depots},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fifty years of research on resource-constrained project scheduling explored from different perspectives. <em>EJOR</em>, <em>328</em>(2), 367-389. (<a href='https://doi.org/10.1016/j.ejor.2025.03.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource-constrained project scheduling problem is one of the most investigated problems in the project scheduling literature, and has a rich history. This article provides a perspective on this challenging scheduling problem, without having the ambition to provide a complete overview. Instead, the article does aim to summarize a number of reasons why this problem has been so intensely investigated from different perspectives. It will be shown that this scheduling problem has many faces, and therefore deserves a lot of research time from a computational and theoretical point of view as well as from a practical point of view. An overview of possible extensions to other problems and a detailed overview of the used (both heuristic and exact) solution methods will be given. In addition, the data used will be discussed and interesting avenues for further research will be mentioned throughout the different sections.},
  archive      = {J_EJOR},
  author       = {Christian Artigues and Sönke Hartmann and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2025.03.024},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {367-389},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of research on resource-constrained project scheduling explored from different perspectives},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic mode decomposition for online portfolio selection task. <em>EJOR</em>, <em>328</em>(1), 349-365. (<a href='https://doi.org/10.1016/j.ejor.2025.04.049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online portfolio selection (OPS) is a complex task aimed at maximizing investment returns through strategic allocation of capital among risky assets. Traditional Follow the Winner (FTW) strategies, grounded in the Best Constant Rebalanced Portfolios strategy, assume market is independent and identically distributed (i.i.d.), which often fails to capture real-world financial market dynamics, leading to sub-optimal performance in practical applications. To address this limitation, we propose integrating Dynamic Mode Decomposition (DMD) into FTW strategies. DMD is a powerful data-driven technique that originated in the field of fluid dynamics. It is designed to extract coherent structures and identify temporal patterns within complex data. By applying DMD to financial market data, we can uncover underlying patterns and trends that are not apparent under the i.i.d. assumption. Significantly, the integrated DMD in this paper allows for efficient recursion, which is particularly crucial for OPS task. To illustrate the effectiveness of the proposed idea, we consider the Exponential Gradient (EG) strategy as an example and proposed Exponential Gradient with Dynamic Mode Decomposition (EGDMD). The results demonstrate that the proposed EGDMD outperforms traditional EG-type strategies, significantly improves risk-adjusted returns, and maintains computational efficiency. The integration of DMD allows for more accurate identification of market patterns, leading to more effective investment decisions and enhanced portfolio performance.},
  archive      = {J_EJOR},
  author       = {Jiahao Li and Yong Zhang and Xiaoteng Zheng},
  doi          = {10.1016/j.ejor.2025.04.049},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {349-365},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic mode decomposition for online portfolio selection task},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Choice-based crowdshipping for next-day delivery services: A dynamic task display problem. <em>EJOR</em>, <em>328</em>(1), 336-348. (<a href='https://doi.org/10.1016/j.ejor.2025.05.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies integrating the crowd workforce into next-day home delivery services. In this setting, both crowd drivers and contract drivers collaborate in making deliveries. Crowd drivers have limited capacity and can choose not to deliver if the presented tasks do not align with their preferences. The central question addressed is: How can the platform minimize the total task fulfilment cost, which includes payouts to crowd drivers and additional payouts to contract drivers for delivering the unselected tasks by customizing task displays to crowd drivers? To tackle this problem, we formulate it as a finite-horizon Stochastic Decision Problem, capturing crowd drivers’ utility-driven task preferences, with the option of not choosing a task based on the displayed options. An inherent challenge is approximating the non-constant marginal cost of serving orders not chosen by crowd drivers, which are then assigned to contract drivers. We address this by leveraging a common approximation technique, dividing the service region into zones. Furthermore, we devise a stochastic look-ahead strategy that tackles the curse of dimensionality issues arising in dynamic task display execution and a non-linear (problem specifically concave) boundary condition associated with the cost of hiring contract drivers. In experiments inspired by Singapore’s geography, we demonstrate that choice-based crowd shipping can reduce next-day delivery fulfilment costs by up to 16.9%. The observed cost savings are closely tied to the task display policies and the task choice behaviours of drivers.},
  archive      = {J_EJOR},
  author       = {Alp Arslan and Fırat Kılcı and Shih-Fen Cheng and Archan Misra},
  doi          = {10.1016/j.ejor.2025.05.046},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {336-348},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Choice-based crowdshipping for next-day delivery services: A dynamic task display problem},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). When cost metafrontiers are nonconvex in the outputs, then the production metafrontier is nonconvex: The price of a convexification strategy. <em>EJOR</em>, <em>328</em>(1), 324-335. (<a href='https://doi.org/10.1016/j.ejor.2025.05.048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metafrontier analysis is widely used to account for technological heterogeneity among producers. The approach involves combining a number of group-specific production possibilities sets to form a production possibilities metaset. Even though the union of the group sets normally results in a nonconvex metaset, most authors proceed as if this metaset is convex. Kerstens, O’Donnell and Van de Woestyne (2019) obtain new results on the union operator on sets under various assumptions and empirically illustrate that the popular convexification strategy is highly questionable. In this paper we transpose their results on the union operator from a production to a cost context: this is new. We then explore the extent to which convexity of the cost function is corroborated using a newly developed test. Furthermore, we check to which extent a convexification strategy is tenable when estimating a cost metafrontier. We use an original banking data set from China and the USA to illustrate the main issues. We establish that the cost function is not convex in the outputs for China and that the convexification strategy leads to potentially-biased estimates of the cost metafrontier and associated measures of efficiency.},
  archive      = {J_EJOR},
  author       = {Kristiaan Kerstens and Christopher O’Donnell and Ignace Van de Woestyne and Shirong Zhao},
  doi          = {10.1016/j.ejor.2025.05.048},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {324-335},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {When cost metafrontiers are nonconvex in the outputs, then the production metafrontier is nonconvex: The price of a convexification strategy},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mitigating adversarial attacks on transformer models in credit scoring. <em>EJOR</em>, <em>328</em>(1), 309-323. (<a href='https://doi.org/10.1016/j.ejor.2025.05.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of unstructured data, such as text created by borrowers, offers new opportunities for improving credit default prediction but also introduces new risks. This study examines the robustness of transformer-based credit scoring models that utilize textual data and assesses their vulnerability to adversarial attacks. Using peer-to-peer lending data, we show that small, semantically neutral changes in loan descriptions can substantially alter model outputs. These vulnerabilities expose lenders and borrowers to economic risks through distorted risk assessments and mispriced loans. We evaluate two mitigation strategies: adversarial training and topic modeling. Adversarial training improves robustness without compromising predictive performance. Topic modeling provides a more interpretable and stable representation of borrower narratives. An economic analysis confirms that robust models reduce mispricing and improve outcomes for all parties. The findings underscore the importance of robustness as the use of unstructured data in credit scoring becomes more accessible.},
  archive      = {J_EJOR},
  author       = {Brandon Schwab and Johannes Kriebel},
  doi          = {10.1016/j.ejor.2025.05.029},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {309-323},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Mitigating adversarial attacks on transformer models in credit scoring},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cost-sensitive single-index classification model. <em>EJOR</em>, <em>328</em>(1), 295-308. (<a href='https://doi.org/10.1016/j.ejor.2025.08.058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-index models (SIMs) are a type of semiparametric model in which a response variable is assumed to be related to a linear combination of explanatory variables by an unknown function, on which any restriction is imposed. Thus, they provide both interpretability and flexibility to capture complex data relationships. In this paper, SIMs are extended to the cost-sensitive classification problem by minimizing the different misclassification costs. The flexibility of SIMs combined with a cost-sensitive approach results in a powerful model to minimize losses and optimize decision making. This is demonstrated through an extensive simulation study and the analysis of five real data sets, where the proposed approach outperforms both parametric and semi-parametric previous approaches.},
  archive      = {J_EJOR},
  author       = {Jorge C-Rella and Ricardo Cao and Juan M. Vilar},
  doi          = {10.1016/j.ejor.2025.08.058},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {295-308},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cost-sensitive single-index classification model},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Diverse ensemble cost-sensitive logistic regression. <em>EJOR</em>, <em>328</em>(1), 282-294. (<a href='https://doi.org/10.1016/j.ejor.2025.07.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cost-sensitive methods have become increasingly crucial for decision-making in various real-world applications. These methods have been developed for the purpose of minimizing costs or risks for stakeholders. Moreover, the interpretability of cost-sensitive methods has gained considerable attention in critical domains such as finance and medical care. In this article, we propose a diverse ensemble of cost-sensitive logistic regression models to reduce costs for binary classification tasks, as well as a novel algorithm based on the partial conservative convex separable quadratic approximation to solve this non-convex optimization problem. The proposed method demonstrates substantial cost savings through extensive simulations and real-world applications, including fraud detection and gene expression analysis. Additionally, unlike other ensembling techniques, the resulting model of the proposed method is fully interpretable as a logistic regression model and achieves a high level of sparsity induced by the proposed algorithm. We believe this approach offers deeper insights into the relationship between predictors and response, enabling more informed decision-making in practical scenarios.},
  archive      = {J_EJOR},
  author       = {Bing Yang and Stefan Van Aelst and Tim Verdonck},
  doi          = {10.1016/j.ejor.2025.07.028},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {282-294},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Diverse ensemble cost-sensitive logistic regression},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-view ensemble feature selection via SemiDefinite programming. <em>EJOR</em>, <em>328</em>(1), 269-281. (<a href='https://doi.org/10.1016/j.ejor.2025.07.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning faces significant challenges in selecting discriminative features while managing redundancy and noise across heterogeneous data sources. To address these issues, this paper introduces Multi-view Ensemble Feature Selection (MEFS), a novel framework that systematically integrates view generation (VG) and view selection (VS) through a unified optimization paradigm. By reformulating feature selection as a MaxCut problem and leveraging SemiDefinite Programming (SDP) relaxation, MEFS dynamically balances the generalization capability of individual views with their pairwise diversity, eliminating the need for manual parameter tuning. A key innovation is the proposed pairwise diversity metric, which quantifies inter-view dissimilarity using between-class scatter matrices to ensure complementary feature subsets. Extensive experiments on ten benchmark datasets demonstrate that MEFS consistently outperforms state-of-the-art methods in accuracy, robustness, and computational efficiency. Ablation studies validate the synergistic effect of combining VG and VS modules.},
  archive      = {J_EJOR},
  author       = {Xiaojian Ding and Xin Wang and Pengcheng Shi},
  doi          = {10.1016/j.ejor.2025.07.014},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {269-281},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-view ensemble feature selection via SemiDefinite programming},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On the price of diversity for multiwinner elections under (weakly) separable scoring rules. <em>EJOR</em>, <em>328</em>(1), 258-268. (<a href='https://doi.org/10.1016/j.ejor.2025.06.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a model of multi-winner elections, where each voter expresses a linear preference over a finite set of alternatives. Based on voters’ preferences, the primary goal is to select a subset of admissible alternatives, forming what is referred to as a committee. We explore (weakly) separable committee scoring rules, the voting mechanisms that assess each alternative individually using a scoring vector and select the top k alternatives, where k represents the committee’s size. Furthermore, we operate under the assumption that alternatives are categorized based on specific attributes. Within each attribute category, there exists a targeted minimum number of alternatives that the selected committee should encompass, emphasizing the necessity for diversity. In this context, we assess the cost associated with imposing such a diversity constraint on the voting process. This assessment is conducted through two methodologies, referred to as the “price of diversity” and the “individual price of diversity”. We set the upper bounds for both prices across all (weakly) separable committee scoring rules. Additionally, we show how the maximum price of diversity can be used to discriminate between different voting rules in this context. Ultimately, we illustrate that concentrating on the candidates’ performance yields a more accurate estimation of the price of diversity compared to a focus on the enforced diversity constraint.},
  archive      = {J_EJOR},
  author       = {Mostapha Diss and Clinton Gubong Gassi and Eric Kamwa},
  doi          = {10.1016/j.ejor.2025.06.013},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {258-268},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On the price of diversity for multiwinner elections under (weakly) separable scoring rules},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nonconvex truncated conditional value at risk-based sparse linear regression. <em>EJOR</em>, <em>328</em>(1), 246-257. (<a href='https://doi.org/10.1016/j.ejor.2025.06.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional value at risk (CVaR) is a widely recognized risk measure used to manage data uncertainty within risk management. In this paper, we study a class of sparse linear regression models based on truncated CVaR measure and ℓ 0 -norm regularization. Due to the nonconvexity and nonsmoothness of the objective functions, as well as the NP-hardness of the problem with the ℓ 0 -norm regularization, we propose an approximation model that employs a tight relaxation of the ℓ 0 -norm. The solution equivalence between the proposed model and its approximation model is explored. To efficiently solve the approximation model, we develop a semismooth Newton-based proximal majorization-minimization algorithm. Furthermore, the convergence analysis of the proposed algorithm is presented, and the convergence rate for the reduced CVaR-based sparse linear regression model is established. Moreover, extensive numerical experiments conducted on both synthetic and real datasets validate the stability and effectiveness of the proposed algorithm, demonstrating significant improvements in both sparsity and accuracy compared to existing state-of-the-art methods.},
  archive      = {J_EJOR},
  author       = {Boyi Xie and Zhongming Wu and Min Li},
  doi          = {10.1016/j.ejor.2025.06.004},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {246-257},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Nonconvex truncated conditional value at risk-based sparse linear regression},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Designing and computing explanations for comparisons inferred from an additive value model. <em>EJOR</em>, <em>328</em>(1), 232-245. (<a href='https://doi.org/10.1016/j.ejor.2025.05.058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many decision models are based on an additive representation of preferences. Recommendations obtained from such additive decision models are sometimes considered as self-evident. On the contrary, we claim that these recommendations deserve an explanation so as to be fully understood by the user/decision-maker and to foster her trust. We propose to explain a preference statement x preferred to y by decomposing this statement into simpler ones. Arguments in favor of x (Pros), and arguments in favor of y (Cons) are decomposed using a covering scheme in which each Con is covered by a Pro. We use a decomposition language in which elementary self-evident statements involve ( i ) one Pro against one Con, ( ii ) one pro against several Cons, or ( iii ) several Pros against one Con. We prove that computing such explanations is computationally difficult in case ( ii ) and ( iii ), and propose a mathematical programming formulation to solve it. Numerical experiments provide insights on the actual behavior of our algorithm. We also illustrate the usefulness of our approach in the context of multicriteria decision aid but also for machine learning approaches.},
  archive      = {J_EJOR},
  author       = {Manuel Amoussou and Khaled Belahcene and Nicolas Maudet and Vincent Mousseau and Wassila Ouerdane},
  doi          = {10.1016/j.ejor.2025.05.058},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {232-245},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Designing and computing explanations for comparisons inferred from an additive value model},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bayesian variable selection in kriging metamodeling for quality design. <em>EJOR</em>, <em>328</em>(1), 216-231. (<a href='https://doi.org/10.1016/j.ejor.2025.06.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of production processes and rapid developments in digital technology have fueled the adoption of metamodels in quality design. Kriging has emerged as one of the most popular emulation methods for both deterministic and stochastic simulations. Conventional Kriging models with predetermined mean functions, such as ordinary or universal Kriging, may exhibit subpar predictive performance when strong trends exist. This paper proposes a novel variable selection procedure for the mean function that ensures prediction accuracy while using only a limited number of variables to capture the potential existing trends in deterministic simulations. The proposed method integrates the benefits of Bayesian variable selection and frequentist statistical tests. Initially, a group of potential models is chosen to build the mean function, employing the Bayesian method with priors designed to guarantee sparsity. This results in a significant reduction in the number of models to be considered in the next stage. Subsequently, each candidate model undergoes rigorous frequentist tests to thoroughly assess its reliability and validity. Extensive simulation studies are conducted using the well-known Borehole function and a real-life case. The results demonstrate the superiority of the proposed method over several existing approaches, establishing its effectiveness in achieving robust parameter design.},
  archive      = {J_EJOR},
  author       = {Baoping Tao and Zifei Han and Wen Shi and Min Wang and Linhan Ouyang},
  doi          = {10.1016/j.ejor.2025.06.003},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {216-231},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bayesian variable selection in kriging metamodeling for quality design},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Design and pricing of extended warranty menus with reference effects. <em>EJOR</em>, <em>328</em>(1), 201-215. (<a href='https://doi.org/10.1016/j.ejor.2025.05.056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer durables increasingly come with extended warranty menus—beyond manufacturers’ base warranties—that offer multiple options with differentiated protection lengths and prices. When choosing from an extended warranty menu, consumers might not only evaluate the intrinsic utility of each option but also form a reference point against which the available options are compared. In this paper, we posit that consumers compare the price-length ratios of available options when determining their willingness to pay, and investigate the design and pricing of extended warranty menus under such reference effects. To this end, we adapt the standard multinomial logit choice model to incorporate two types of reference point that are generated endogenously and exogenously, respectively, with respect to the given menu. We show that if the warranty options are ordered in the protection length, then the optimal pricing policies prescribe the same adjusted markup and the same price-length ratio alternately. We further extend our analysis to price competition under reference effects, where multiple firms compete in the aftermarket, each offering a single extended warranty. We prove the existence of a unique Nash equilibrium and develop an efficient method to identify it. Numerical examples are presented to illustrate the analytical findings, and sensitivity analyses are conducted to examine the impact of reference-effect coefficients on the optimal pricing policies. Overall, this work highlights the importance of incorporating reference effects into the design and pricing of extended warranty menus.},
  archive      = {J_EJOR},
  author       = {Xiao-Lin Wang and Chenglong Li and Junjie Wang},
  doi          = {10.1016/j.ejor.2025.05.056},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {201-215},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Design and pricing of extended warranty menus with reference effects},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Planning bayesian reliability demonstration tests via a generalized test statistic. <em>EJOR</em>, <em>328</em>(1), 189-200. (<a href='https://doi.org/10.1016/j.ejor.2025.08.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability demonstration testing (RDT) has been extensively employed to verify whether a product meets specific reliability requirements at a desired confidence level. Driven by intense market competition and constrained test resources, manufacturers are motivated to seek effective strategies to reduce the testing efforts required for RDT. In this paper, we propose a method that utilizes existing knowledge and information obtained from the product design and development phase to construct a Bayesian prior distribution of the product’s reliability. Based on this prior, a preliminary disposition decision on whether to accept or reject the product is made. A subsequent demonstration test is needed only when the prior information is deemed insufficient for an immediate disposition. A RDT planning method is developed based on the posterior distribution of the product’s reliability, which is applicable to general cases involving non-conjugate priors. We study two types of demonstration testing: binomial and exponential. For each, we prove the existence of an optimal test plan and develop an efficient searching algorithm to determine it. Numerical studies are conducted to demonstrate the effectiveness of the proposed method, supplemented by a case study on RDT for systems of different configurations. Overall, this work provides a unified and effective framework for reliability demonstration under the Bayesian paradigm.},
  archive      = {J_EJOR},
  author       = {Zan Li and Jianyu Xu and Chengjie Wang and Xiao-Lin Wang},
  doi          = {10.1016/j.ejor.2025.08.011},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {189-200},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Planning bayesian reliability demonstration tests via a generalized test statistic},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A primal–dual policy iteration algorithm for constrained markov decision processes. <em>EJOR</em>, <em>328</em>(1), 174-188. (<a href='https://doi.org/10.1016/j.ejor.2025.08.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution algorithms of Constrained Markov Decision Process (CMDP), a widely adopted model for sequential decision-making, have been intensively studied in the literature. Despite increasing effort, the Linear Programming (LP) formulation of CMDP remains the dominant exact method that leads to the optimal solution without constraint violations. However, the LP formulation is computationally inefficient due to the curse of dimensionality in CMDP state and action spaces. In this study, we introduce a novel policy iteration method for CMDP, based on decomposition and row-generation techniques. We design a Primal–Dual Policy Iteration (PDPI) algorithm that utilizes state values and Lagrangian multipliers to improve randomized stationary policies in an iterative fashion. We analytically show that upon convergence, PDPI produces the optimal solution for CMDP. An upper bound of the convergence iterations is also given. To validate the algorithm performance, we conduct comprehensive computational experiments on six benchmarking problems curated from the literature. Results show that PDPI outperforms conventional methods considerably, improving the total algorithm runtime by up to 89.19%. The improvement becomes more significant as the problem size grows larger. We further provide insights and discuss the impact of the developed method.},
  archive      = {J_EJOR},
  author       = {Zeyu Liu and Xueping Li and Anahita Khojandi},
  doi          = {10.1016/j.ejor.2025.08.038},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {174-188},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A primal–dual policy iteration algorithm for constrained markov decision processes},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Controlling antithetic variates. <em>EJOR</em>, <em>328</em>(1), 162-173. (<a href='https://doi.org/10.1016/j.ejor.2025.08.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish and investigate a theoretical framework for controlling covariance matrices in the method of antithetic variates through control variates to further reduce estimator variance. Instead of preemptively and carefully designing an estimator vector with negatively correlated components, the proposed framework starts with a predefined estimator vector that incorporates specified control variates. The weights and control matrix are then analytically determined through matrix algebra. The joint optimality of the resulting estimator variance is ensured with respect to both the weights and the control matrix, with closed-form implementable formulas derived for the optimal parameter pair. Numerical results are provided for various typical examples to illustrate the effectiveness, potential, and challenges of the proposed framework.},
  archive      = {J_EJOR},
  author       = {Reiichiro Kawai},
  doi          = {10.1016/j.ejor.2025.08.027},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {162-173},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Controlling antithetic variates},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust multi-period blood inventory routing under multiple uncertainties. <em>EJOR</em>, <em>328</em>(1), 137-161. (<a href='https://doi.org/10.1016/j.ejor.2025.05.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multi-period blood inventory routing problem that integrates production, inventory, and distribution decisions under uncertainties in demand, donation supply, and travel times, all while accounting for the limited shelf life of blood products. Our model captures transportation efficiency through a disutility measure based on vehicles’ arrival times at hospitals, and addresses supply–demand imbalances by allowing selective rejection of service requests at a high penalty cost. We formulate a robust optimization model that simultaneously determines production quantities, inventory levels, hospital service selections, and vehicle routing for each period. The objective is to minimize the total cost over the planning horizon, which includes worst-case inventory holding, wastage, and transportation costs, unserved demand penalties, and overall transportation disutility. To obtain an exact solution, we propose an integrated algorithm within the L -shaped framework that combines Benders decomposition with a branch-and-price-and-cut (BPC) scheme. This approach decomposes the robust model into a master problem and period-specific subproblems. For a given master solution, we first use constraint programming to verify the feasibility of the subproblems, and then, if feasible, solve them with a tailored BPC algorithm to generate Benders cuts that eliminate suboptimal master solutions. Extensive numerical experiments, including a case study at the Blood Center in Chongqing, demonstrate the effectiveness of our approach. Our analysis quantifies the benefits of incorporating uncertainty and robustness while providing managerial insights through a systematic evaluation of various parameters.},
  archive      = {J_EJOR},
  author       = {Ling Qing and Yunqiang Yin and Joshua Ignatius and Dujuan Wang},
  doi          = {10.1016/j.ejor.2025.05.036},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {137-161},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust multi-period blood inventory routing under multiple uncertainties},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Data-driven joint optimization of maintenance and spare parts provisioning: A distributionally robust approach. <em>EJOR</em>, <em>328</em>(1), 122-136. (<a href='https://doi.org/10.1016/j.ejor.2025.06.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the joint optimization of condition-based maintenance and spare provisioning, incorporating insights obtained from sensor data. Prognostic models estimate components’ remaining lifetime distributions (RLDs), which are integrated into an optimization model to coordinate maintenance and spare provisioning. The existing literature addressing this problem assumes that prognostic models provide accurate estimates of RLDs, thereby allowing a direct adoption of Stochastic Programming or Markov Decision Process methodologies. Nevertheless, this assumption often does not hold in practice since the estimated distributions can be inaccurate due to noisy sensors or scarcity of training data. To tackle this issue, we develop a Distributionally Robust Chance Constrained (DRCC) formulation considering general discrepancy-based ambiguity sets that capture potential distribution perturbations of the estimated RLDs. The proposed formulation admits a Mixed-Integer Linear Programming (MILP) reformulation, where explicit formulas are provided to simplify the general discrepancy-based ambiguity sets. Finally, for the numerical illustration, we test a type- ∞ Wasserstein ambiguity set and derive closed-form expressions for the parameters of the MILP reformulation. The efficacy of our methodology is showcased in a wind turbine case study, where the proposed DRCC formulation outperforms other benchmarks based on stochastic programming and robust optimization.},
  archive      = {J_EJOR},
  author       = {Heraldo Rozas and Weijun Xie and Nagi Gebraeel and Stephen Robinson},
  doi          = {10.1016/j.ejor.2025.06.025},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {122-136},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven joint optimization of maintenance and spare parts provisioning: A distributionally robust approach},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards blockchain-enabled circular closed-loop supply chain and impact of consumers’ distrust in price, product greenness sensitivity and carbon tax and subsidy. <em>EJOR</em>, <em>328</em>(1), 105-121. (<a href='https://doi.org/10.1016/j.ejor.2025.06.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing emphasis on environmental sustainability, both governments and consumers are more concerned than ever about the greenness of products. In this complex landscape, Supply Chains (SCs) face challenges in building trust and avoiding greenwashing accusations. Blockchain technology offers a promising solution by ensuring transparency and circularity within SCs, particularly in identifying customers for product recycling. This study pioneers the exploration of consumers' distrust in pricing and product greenness, alongside the impact of carbon policies (taxes and subsidies) within a closed-loop supply chain (CLSC). Using classical Stackelberg game theory, we develop two models that identify equilibrium decisions for SC members, focusing on pricing, green production investment, circularity, and blockchain adoption. Additionally, we propose an evolutionary game theory model to find the optimal government policies and identify the long-term behaviour of the CLSC and government in two heterogeneous populations. Our findings reveal that if the retailer's share of blockchain costs falls below a certain threshold, blockchain adoption becomes less profitable than exclusive investment in green production. A higher (lower) subsidy rate benefits (harms) the retailer but disadvantages (benefits) the collector. Blockchain adoption is generally more profitable for manufacturers and retailers, though less so for collectors, and it also drives greater investment in green production. While subsidies encourage blockchain adoption, they are not a sustainable long-term strategy for governments. Ultimately, the evolutionarily stable strategy for SCs involves a balanced investment in both green production and blockchain or green production alone, depending on market characteristics and cost-sharing structures.},
  archive      = {J_EJOR},
  author       = {Mohammad Akbarzadeh Sarabi and Ata Allah Taleizadeh and Arijit Bhattacharya},
  doi          = {10.1016/j.ejor.2025.06.030},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {105-121},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Towards blockchain-enabled circular closed-loop supply chain and impact of consumers’ distrust in price, product greenness sensitivity and carbon tax and subsidy},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A split-embedded metaheuristic for the heterogeneous inventory routing problem with batch size. <em>EJOR</em>, <em>328</em>(1), 91-104. (<a href='https://doi.org/10.1016/j.ejor.2025.05.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling transportation and inventory management simultaneously is a challenging problem. The most famous optimization problem in this domain, known as the Inventory Routing Problem (IRP), aims to determine the routes and quantities to be delivered by a set of vehicles to meet customer demands at a minimum total inventory and transportation cost. The vast majority of works carried out so far on the IRP consider a homogeneous fleet of vehicles. This paper addresses, instead, a new IRP variant that considers intrinsic characteristics of real supply chains, such as a period-dependent heterogeneous fleet of vehicles and batch sizes for the delivered quantities. We model the problem with a Mixed Integer Linear Programming (MILP) formulation and propose a Split-Embedded Metaheuristic with a Post-Optimization phase (SEMPO) to solve it. Extensive computational experiments are conducted on a set of 80 new benchmark instances with up to 183 customers and a challenging time horizon of up 7 to 28 time periods to evaluate the performance of our approaches. The proposed SEMPO algorithm provides high-quality solutions and faster convergence compared to the MILP formulation.},
  archive      = {J_EJOR},
  author       = {Diego Perdigão Martino and Philippe Lacomme and Katyanne Farias},
  doi          = {10.1016/j.ejor.2025.05.044},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {91-104},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A split-embedded metaheuristic for the heterogeneous inventory routing problem with batch size},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). The role of scarcity behavior in inventory management. <em>EJOR</em>, <em>328</em>(1), 78-90. (<a href='https://doi.org/10.1016/j.ejor.2025.05.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-known phenomenon related to inventory, but often neglected in inventory management, is the scarcity effect, i.e., the increase in the demand if inventory is low. We consider a repeated purchase setting with a single firm and multiple buyers and address the question of whether inventory management decisions concerning the control policy and the service configuration (determining when and how much to order) impact scarcity behavior arising from buyers’ stock-out perception. We study common inventory control policies that assume a demand independent of the inventory management, and we challenge this critical assumption of an exogenous demand. This research explores two prevalent classes of inventory policies widely used in practice (periodic and continuous) configured according to fill rates, a popular way of measuring service. We conduct a laboratory experiment with four automated inventory management treatments (2 policies × 2 configurations) where participants act as buyers. We observe stock-out induced scarcity and find support for the hypothesis that the periodic policy leads to a stronger effect compared to the continuous policy if the service level is low. The study also supports the hypothesis that buyers act forward-looking as their demand peaks before the inventory reaches its lowest level. Overall, our research provides a new perspective on inventory management as it reveals that the chosen control policy and the selected service configuration influence stock-out pressure induced inventory runs. Inventory managers should be aware of scarcity effects and its consequences, like the disadvantages of a periodic policy for low service level.},
  archive      = {J_EJOR},
  author       = {Sebastian Schiffels and Christian Jost},
  doi          = {10.1016/j.ejor.2025.05.043},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {78-90},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The role of scarcity behavior in inventory management},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Green scheduling with time-of-use tariffs and machine states: Optimizing energy cost via branch-and-bound and bin packing strategies. <em>EJOR</em>, <em>328</em>(1), 64-77. (<a href='https://doi.org/10.1016/j.ejor.2025.06.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a branch-and-bound algorithm, enhanced with bin packing strategies, for scheduling under variable energy pricing and power-saving states. The proposed algorithm addresses the 1 , TOU | states | TEC problem, which involves scheduling jobs to minimize total energy cost (TEC) while considering time-of-use (TOU) electricity prices and different machine states (e.g., processing, idle, off). Key innovations include instance pre-processing for rapid lower bound calculations, a novel branching scheme combined with initializations, a block-finding primal heuristic, and a tighter lower bound for jobs with non-coprime processing times. These enhancements result in an efficient algorithm capable of solving benchmark instances with real energy prices with 200 jobs more than 100 times faster than existing state-of-the-art methods.},
  archive      = {J_EJOR},
  author       = {Ondřej Benedikt and István Módos and Antonin Novak and Zdeněk Hanzálek},
  doi          = {10.1016/j.ejor.2025.06.026},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {64-77},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Green scheduling with time-of-use tariffs and machine states: Optimizing energy cost via branch-and-bound and bin packing strategies},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Flight test scheduling: A generic model, lower bounds, and iterated local search. <em>EJOR</em>, <em>328</em>(1), 49-63. (<a href='https://doi.org/10.1016/j.ejor.2025.06.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flight tests play a critical role in the R&D process for new aircraft as they help verify the airworthiness and capabilities and expose design and manufacturing defects. During the test course, a large number of tasks need to be scheduled appropriately so that the flight tests can be completed with minimum cost and high efficiency. Therefore, there is a strong need for developing an efficient method that can generate high-quality test schedules. In this paper, we study flight test scheduling to minimize the number of required test flights, thereby decreasing the cost and time required during the entire test course. We establish a mixed-integer programming model to formally describe the problem, propose several computationally efficient lower bounds to help verify the quality of obtained solutions, and develop an iterated local search algorithm for generating a high-quality solution in an effective manner. Comprehensive computational experiments are performed to demonstrate the efficiency of our proposed algorithm. We report some general managerial insights based on the obtained computational results.},
  archive      = {J_EJOR},
  author       = {Hanqiao Tao and Guopeng Song and Roel Leus and Zhe Liang and Jiang Jiang},
  doi          = {10.1016/j.ejor.2025.06.001},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {49-63},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flight test scheduling: A generic model, lower bounds, and iterated local search},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nested logic-based benders decomposition for an integrated home healthcare problem. <em>EJOR</em>, <em>328</em>(1), 32-48. (<a href='https://doi.org/10.1016/j.ejor.2025.06.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we apply nested logic-based Benders decomposition to solve an integrated home healthcare staffing, assignment, routing, and scheduling problem with application in Norway. The proposed method operates at two decomposition levels. Consequently, the entire problem is decomposed into three hierarchical sub-problems: the staffing problem, the assignment problem, and the routing and scheduling problem. These sub-problems are interrelated through two levels of logic-based Benders cuts. Computational experiments on 40 test instances demonstrate the superior performance of nested logic-based Benders decomposition compared to directly solving a mixed-integer linear programming model available in the literature. Specifically, the proposed solution method achieved proven optimality in 28 instances and provided feasible solutions for the remaining 12 instances. In contrast, directly solving the mixed-integer linear programming model yielded proven optimality in 16 instances, provided feasible solutions for 20 instances, and failed to find feasible solutions for 4 instances within the same computational time limit.},
  archive      = {J_EJOR},
  author       = {Abdalrahman Algendi and Sebastián Urrutia and Lars Magnus Hvattum and Rafael A. Melo},
  doi          = {10.1016/j.ejor.2025.06.006},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {32-48},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Nested logic-based benders decomposition for an integrated home healthcare problem},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A reduction approach for the parallel machine scheduling problem with a separate server for loading and unloading operations. <em>EJOR</em>, <em>328</em>(1), 15-31. (<a href='https://doi.org/10.1016/j.ejor.2025.05.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the non-preemptive identical parallel machine scheduling problem with a dedicated loading server and a dedicated unloading server. Each job has to be loaded by the dedicated loading server immediately before being processed on one of the identical parallel machines, and unloaded immediately by the dedicated unloading server after its processing. The objective function involves the minimization of the makespan. This problem arises in the semiconductor industry, plastic injection industry, kitchen production systems, healthcare, container terminals, and many other industrial fields. We prove the problem to be strongly NP-hard, analyze a special case with identical loading, processing, and unloading times, and establish a tight lower bound. In addition, we propose two novel mixed-integer linear programming formulations: one utilizing time-indexed variables with an iterative strengthening algorithm, and the other employing linear-ordering variables along with two enhanced valid inequalities. Given the complexity of the problem, we introduce a reduction approach that simplifies the problem by modifying certain constraints, enabling the determination of a feasible solution much more quickly. Building on this reduction, we provide a linear-time reduction algorithm and a fast formulation based on assignment-and-positional date variables. Furthermore, we study a special case involving regular jobs. To solve large-scale instances of the problem with up to 250 jobs and up to 5 machines, we design a hybrid approach combining an iterated greedy algorithm with variable neighborhood descent. As shown in the computational experiments on two sets of benchmark instances, the reduction approach significantly outperforms all previous methods existing in the literature.},
  archive      = {J_EJOR},
  author       = {Abdelhak Elidrissi and Jatinder N.D. Gupta and Rachid Benmansour and Bertrand M.T. Lin},
  doi          = {10.1016/j.ejor.2025.05.030},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {15-31},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A reduction approach for the parallel machine scheduling problem with a separate server for loading and unloading operations},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cutting stock problem with usable leftovers: A review. <em>EJOR</em>, <em>328</em>(1), 1-14. (<a href='https://doi.org/10.1016/j.ejor.2025.03.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive literature review of the Cutting Stock Problem with Usable Leftovers (CSPUL). The most recent review on this topic dates to 2014, covering articles published before 2013. Since then, the number of publications on CSPUL has increased significantly, driven by new applications and more efficient solution approaches. We analyze fifty two relevant articles from twenty four different journals, focusing on works published after 2008 while acknowledging foundational contributions from the 1980s and 1990s. This review categorizes variations of CSPUL based on their dimensions (1D, 2D, and 3D), planning period characteristics (single-period and multi-period), objective functions, and solution methods. The article provides a detailed summary of the key features in the mathematical models and solution methods proposed in these studies. Additionally, it highlights several industrial applications of CSPUL, illustrating its practical relevance. Through this analysis, we identify important applications and propose promising directions for future research. The findings and insights presented here have practical implications for optimizing resource utilization and promoting sustainability in industries facing cutting challenges.},
  archive      = {J_EJOR},
  author       = {Victor Senergues and Nadjib Brahimi and Adriana Cristina Cherri and François Klein and Olivier Péton},
  doi          = {10.1016/j.ejor.2025.03.014},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cutting stock problem with usable leftovers: A review},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
