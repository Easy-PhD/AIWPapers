<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr">PR - 153</h2>
<ul>
<li><details>
<summary>
(2026). Heterogeneous graph contrastive learning with spectral augmentation and dual aggregation. <em>PR</em>, <em>172</em>, 112505. (<a href='https://doi.org/10.1016/j.patcog.2025.112505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs effectively model complex entity relationships in real-world scenarios. However, existing methods primarily focus on topological structures, overlooking the spectral domain, which limits their ability to capture rich, multi-dimensional graph information. Many rely on meta-path schemes to encode semantic details of specific node types, neglecting others and local structural nuances. Thus, they fail to capture comprehensive structural information. To address these issues, a novel combined d ual a ggregation and s pectral a ugmented algorithm, the h eterogeneous g raph c ontrast l earning model (DasaHGCL), is proposed. It applies adaptive spectral augmentation introduced from homogeneous graph learning to the meta-path view of heterogeneous graphs, capturing their spectral invariance for the first time. It also creates an intra-scheme contrast mechanism in dual aggregation algorithms for meta-path and network schema, which circumvents the effect of differences between different aggregation schemes on the model to effectively capture higher-order semantic information and local heterogeneous structural features. Experiments on multiple real-world datasets demonstrate the clear advantages of DasaHGCL.},
  archive      = {J_PR},
  author       = {Jing Zhang and Wan Zhang and Xiaoqian Jiang and Yingjie Xie and Yali Yuan and Shunmei Meng and Cangqi Zhou},
  doi          = {10.1016/j.patcog.2025.112505},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112505},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous graph contrastive learning with spectral augmentation and dual aggregation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Conformal prediction of molecule-induced cancer cell growth inhibition challenged by strong distribution shifts. <em>PR</em>, <em>172</em>, 112501. (<a href='https://doi.org/10.1016/j.patcog.2025.112501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The drug discovery process often employs phenotypic and target-based virtual screening to identify potential drug candidates. Despite the longstanding dominance of target-based approaches, phenotypic virtual screening is undergoing a resurgence due to its potential being now better understood. In the context of cancer cell lines, a well-established experimental system for phenotypic screens, molecules are tested to identify their whole-cell activity, as summarized by their half-maximal inhibitory concentrations. Machine learning has emerged as a potent tool for computationally guiding such screens, yet important research gaps persist, including generalization and uncertainty quantification. To address this, we leverage a clustering-based validation approach, called Leave Dissimilar Molecules Out (LDMO). This strategy enables a more rigorous assessment of model generalization to structurally novel compounds. This study focuses on applying Conformal Prediction (CP), a model-agnostic framework, to predict the activities of novel molecules on specific cancer cell lines. A total of 4320 independent models were evaluated across 60 cell lines, 5 CP variants, 2 set features, and training-test splits, providing strong and consistent results. From this comprehensive evaluation, we concluded that, regardless of the cell line or model, novel molecules with smaller CP-calculated confidence intervals tend to have smaller predicted errors once measured activities are revealed. It was also possible to anticipate the activities of dissimilar test molecules across 50 or more cell lines. These outcomes demonstrate the robust efficacy that LDMO-based models can achieve in realistic and challenging scenarios, thereby providing valuable insights for enhancing decision-making processes in drug discovery.},
  archive      = {J_PR},
  author       = {Saiveth Hernandez-Hernandez and Qianrong Guo and Pedro J. Ballester},
  doi          = {10.1016/j.patcog.2025.112501},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112501},
  shortjournal = {Pattern Recognition},
  title        = {Conformal prediction of molecule-induced cancer cell growth inhibition challenged by strong distribution shifts},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Salient object ranking with reinforcement learning. <em>PR</em>, <em>172</em>, 112499. (<a href='https://doi.org/10.1016/j.patcog.2025.112499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects in an image inherently draw attention due to their vivid colors and larger sizes, indicating their high saliency. The salient object ranking (SOR) task aims to prioritize multiple salient objects within a scene based on their saliency levels. Past research has predominantly treated the SOR task as a static process, typically formulating it as a regression or classification problem. However, these approaches overlook the dynamic nature of human attention, which shifts as context and inter-object correlations influence focus. To address this, we employ a reinforcement learning strategy, modeling SOR as a dynamic iterative sequence process. We train an actor to select salient objects from the environment. Additionally, we design a reward strategy that encourages the selection of the most prominent object among those not previously chosen. The selection order generated by the actor directly determines the ranking of object saliency in the scene. Furthermore, we identify limitations in existing SOR evaluation metrics, which may falter in certain scenarios. To address this, we introduce a simple and useful metric, referred to as the F1-Sor, into SOR tasks, improving the evaluation accuracy of the SOR tasks. Our model achieves state-of-the-art performance on publicly available SOR datasets.},
  archive      = {J_PR},
  author       = {Qi Gao and Heng Li and Jianpin Chen and Xinyu Chai},
  doi          = {10.1016/j.patcog.2025.112499},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112499},
  shortjournal = {Pattern Recognition},
  title        = {Salient object ranking with reinforcement learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Affinity-aware uncertainty quantification for learning with noisy labels. <em>PR</em>, <em>172</em>, 112495. (<a href='https://doi.org/10.1016/j.patcog.2025.112495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) with noisy labels is a challenging task that significantly degenerates the model’s performance. Most existing methods mitigate this problem by identifying and eliminating noisy samples or correcting their labels according to statistical properties like confidence values. However, these methods often overlook the impact of inherent noise, such as sample quality, which can mislead DNNs to focus on incorrect regions, adulterate the softmax classifier, and generate low-quality pseudo-labels. In this paper, we propose a novel Affinity-aware Uncertainty Quantification (AUQ) framework to explore the perception ambiguity and rectify the salient bias by quantifying the uncertainty. Concretely, we construct the dynamic prototypes to represent intra-class semantic spaces and estimate the uncertainty based on sample-prototype pairs, where the observed affinities between sample-prototype pairs are converted to probabilistic representations as the estimated uncertainty. Samples with higher uncertainty are likely to be hard samples and we design an uncertainty-aware loss to emphasize the learning from those samples with high uncertainty, which helps DNNs to gradually concentrate on the critical regions. Besides, we further utilize sample-prototype affinities to adaptively refine pseudo-labels, enhancing the quality of supervisory signals for noisy samples. Extensive experiments conducted on the CIFAR-10, CIFAR-100 and Clothing1M datasets demonstrate the efficacy and effectiveness of AUQ. Notably, we achieve an average performance gain of 0.4 % on CIFAR-10 and a substantial average improvement of 2.3 % over the second-best method on the more challenging CIFAR-100 dataset. Moreover, there is a 0.6 % improvement over the sub-optimal method on Clothing1M. These results validate AUQ’s capability in enhancing DNN robustness against noisy labels.},
  archive      = {J_PR},
  author       = {Zhihao Zhou and Rui Li and Wenjie Ai and Xueying Li and Zhu Teng and Baopeng Zhang and Junwei Du},
  doi          = {10.1016/j.patcog.2025.112495},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112495},
  shortjournal = {Pattern Recognition},
  title        = {Affinity-aware uncertainty quantification for learning with noisy labels},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning hierarchical uncertainty from hybrid representations for neural active reconstruction. <em>PR</em>, <em>172</em>, 112493. (<a href='https://doi.org/10.1016/j.patcog.2025.112493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active reconstruction is a key area for the robotics and computer vision communities, enabling autonomous agents to dynamically reconstruct scenes or objects from multiple viewpoints for navigation and manipulation tasks. Although existing methods have achieved promising results in 3D reconstruction, the hierarchical uncertainty-aware active reconstruction based on hybrid implicit representations remains underexplored, particularly in balancing accuracy, efficiency, and adaptability. To address this gap, we propose a neural active reconstruction system that combines hybrid neural representations with uncertainty. Specifically, we explore a novel scheme that integrates occupancy, signed distance function, and neural radiance fields for high-fidelity 3D reconstruction. Additionally, we utilize hierarchical uncertainty associated with different representations to select the next best viewpoint for trajectory planning and optimization. Our system has been extensively evaluated on benchmark datasets including Replica and MP3D, demonstrating qualitatively and quantitatively improved reconstruction quality and view planning efficiency compared to baseline approaches.},
  archive      = {J_PR},
  author       = {Shuaixian Wang and Yaokun Li and Chenhui Guo and Guang Tan},
  doi          = {10.1016/j.patcog.2025.112493},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112493},
  shortjournal = {Pattern Recognition},
  title        = {Learning hierarchical uncertainty from hybrid representations for neural active reconstruction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Language model encoded multi-scale feature fusion and transformation for predicting protein-peptide binding sites. <em>PR</em>, <em>172</em>, 112487. (<a href='https://doi.org/10.1016/j.patcog.2025.112487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein-peptide interactions serve as a pivotal and indispensable role in diverse biological functions and cellular processes. Although recent studies have begun to employ language models for predicting protein-peptide binding sites (PPBS), the majority of previous approaches have persisted in utilizing intricate sequence-based feature engineering or incorporating costly experimental structural information. To overcome these limitations, we develop a novel sequence-based end-to-end PPBS predictor using deep learning, named Language model encoded Multi-scale Feature Fusion and Transformation (LMFFT). The proposed model starts with a single protein language model for comprehensive multi-scale feature extraction, including residue, dipeptide, and fragment-level representations, which are implemented by the dipeptide embedding-based fragment fusion and further enhanced through the dipeptide contextual encoding. Moreover, multi-scale convolutional neural networks are applied to transform multi-scale features by capturing intricate interactions between local and global information. Our LMFFT achieves state-of-the-art performance across three benchmark datasets, outperforming existing sequence-based methods and demonstrating competitive advantages over certain structure-based baselines. This work provides a cost-effective and efficient solution for PPBS prediction, advancing revealing the sequence-function relationship of proteins.},
  archive      = {J_PR},
  author       = {Hua Zhang and Pengliang Chen and Xiaoqi Yang and Junhao Wang and Guogen Shan and Bi Chen and Bo Jiang},
  doi          = {10.1016/j.patcog.2025.112487},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112487},
  shortjournal = {Pattern Recognition},
  title        = {Language model encoded multi-scale feature fusion and transformation for predicting protein-peptide binding sites},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-AD: Cross-domain unsupervised anomaly detection for medical and industrial applications. <em>PR</em>, <em>172</em>, 112486. (<a href='https://doi.org/10.1016/j.patcog.2025.112486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, an unsupervised convolutional neural network (CNN) model for robust anomaly detection across medical and industrial domain images. Our approach utilizes the squeeze-and-excitation (SE) block to enhance feature extraction by applying channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Additionally, knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model’s capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model gains the ability to detect anomalies of varying sizes. Teacher-student ( T - S ) architecture ensures consistency in representing high-dimensional features while adapting these features to improve anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average accuracy for anomaly localization at both the image level (81.4 % for medical and 99.6 % for industrial) and pixel level (97.0 % for medical and 98.4 % for industrial), making it effective for real-world applications.},
  archive      = {J_PR},
  author       = {Wahyu Rahmaniar and Kenji Suzuki},
  doi          = {10.1016/j.patcog.2025.112486},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112486},
  shortjournal = {Pattern Recognition},
  title        = {Multi-AD: Cross-domain unsupervised anomaly detection for medical and industrial applications},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A collaborative spatial–frequency learning network for infrared and visible image fusion. <em>PR</em>, <em>172</em>, 112480. (<a href='https://doi.org/10.1016/j.patcog.2025.112480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep fusion models operate predominantly in the spatial domain, which limits their ability to effectively preserve texture details. In contrast, methods that incorporate frequency-domain information often suffer from inadequate interaction with spatial-domain features, thereby constraining overall fusion performance. To address these limitations, we propose a Collaborative Spatial-Frequency Learning Network (CSFNet) for infrared and visible image fusion. In the frequency-domain learning branch, we introduce a frequency refinement module based on wavelet transform to enable cross-band feature interaction and facilitate effective multi-scale feature fusion. In the spatial-domain branch, we embed a learnable low-rank decomposition model that extracts low-rank features from infrared images and sparse detail features from visible images, forming the basis of a dedicated spatial feature extraction module. Additionally, an information aggregation module is designed to learn complementary representations and integrate cross-domain features efficiently. To validate the effectiveness of the proposed approach, we conducted extensive experiments on three publicly available datasets: MSRS, TNO, and RoadScene, and compared CSFNet with sixteen state-of-the-art (SOTA) fusion methods. On the MSRS dataset, CSFNet achieved favorable results, with a mean and standard deviation of SF = 12.2108 ± 3.8706, VIF = 1.0232 ± 0.1397, Qabf = 0.7112 ± 0.0397, SSIM = 0.6909 ± 0.0859, PSNR = 17.6517 ± 3.8767, and AG = 4.0243 ± 1.5465. The minimum performance improvement over SOTA methods was 1.64 %, while the maximum gain reached 108.82 %. Furthermore, CSFNet demonstrated superior performance on a downstream semantic segmentation task.},
  archive      = {J_PR},
  author       = {Hongbin Yu and Xiangcan Du and Wei Song and Haojie Zhou and Junyi Zhang},
  doi          = {10.1016/j.patcog.2025.112480},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112480},
  shortjournal = {Pattern Recognition},
  title        = {A collaborative spatial–frequency learning network for infrared and visible image fusion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Error-resilient incomplete multi-view clustering: Mitigating imputation-induced error accumulation. <em>PR</em>, <em>172</em>, 112477. (<a href='https://doi.org/10.1016/j.patcog.2025.112477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMC) plays a pivotal role in integrating and analyzing multi-view data with missing information. Most of existing IMC methods improve clustering performance by inherently incorporating a data recovery step to derive a common representation or consensus graph. However, the imputation of missing data may introduce biased errors, which can accumulate and amplify during iterative optimization, ultimately distorting clustering results. To tackle this critical issue, we propose a novel unified optimization framework that jointly learns data completion and error removal in a mutually reinforcing manner. Specifically, our method introduces a dual-path architecture: one path reconstructs missing views via self-representation, while the other path explicitly models and eliminates biased errors. Crucially, these two components interact via an alternating minimization scheme, enabling them to mutually enhance each other. This synergy effectively reduces error accumulation, leading to a more accurate graph for clustering. Experiments on real-world datasets show that the proposed framework achieves state-of-the-art performance under extremely high missing rates (up to 90 %), significantly reducing error propagation while outperforming existing baselines.},
  archive      = {J_PR},
  author       = {Xuanlong Ma and Yanhong She and Fenfang Xie and Guo Zhong},
  doi          = {10.1016/j.patcog.2025.112477},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112477},
  shortjournal = {Pattern Recognition},
  title        = {Error-resilient incomplete multi-view clustering: Mitigating imputation-induced error accumulation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CLIP can understand depth. <em>PR</em>, <em>172</em>, 112475. (<a href='https://doi.org/10.1016/j.patcog.2025.112475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we demonstrate that CLIP can also be adapted to downstream tasks where its vision-language alignment is suboptimally learned during pre-training on web-crawled data, all without requiring fine-tuning. We explore the case of monocular depth estimation, where CLIP’s contrastive prior struggles to generalize, compared to its success in domains such as generative modeling and semantic segmentation. Since CLIP fails to consistently capture similarities between image patches and natural language prompts describing distance, we eliminate the use of its pre-trained natural language token embeddings and distill the semantic prior of its frozen text encoder into a single learnable embedding matrix called “mirror” . The main design goal of mirror is to derive a non-human language prompt that approximates an optimal natural language prompt: “ How far is this location from the camera? ” Using this approach, we jointly train two lightweight modules, a mirror and a compact decoder, on top of a frozen CLIP for dense depth prediction. Compared to conventional depth models, our framework is significantly more efficient in terms of parameters and computation. The resulting model exhibits impressive performance, matching several state-of-the-art vision models on the NYU Depth v2 and KITTI benchmark datasets, while outperforming all vision-language depth models based on a frozen CLIP prior. Specifically, our method reduces the Absolute Relative Error (Abs Rel) by 68.7 % on NYU Depth v2 and by 75.6 % on KITTI compared to the method of Auty et al. , a representative CLIP-based baseline. Experiments demonstrate that the suboptimal depth understanding of CLIP in terms of spatial and temporal consistency can be significantly corrected without either fine-tuning it or concatenating mirror with its pre-trained subword token embeddings. Furthermore, an ablation study on the convergence status of mirror shows that it is implicitly trained to capture objects, such as humans and windows, where semantic cues play an important role in detection.},
  archive      = {J_PR},
  author       = {Sohee Kim and Jisu Kang and Dunam Kim and Seokju Lee},
  doi          = {10.1016/j.patcog.2025.112475},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112475},
  shortjournal = {Pattern Recognition},
  title        = {CLIP can understand depth},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving adversarial transferability via semantic-style joint expectation perturbations. <em>PR</em>, <em>172</em>, 112474. (<a href='https://doi.org/10.1016/j.patcog.2025.112474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style and content information, which are model-independent inherent properties of an image, serve as crucial information that deep neural networks depend on for classification tasks. However, most existing gradient-based attacks mainly distort content-related information through semantic distortion of the model’s final output, neglecting the role of style information. To fully distort the inherent intrinsic information of the image, this paper proposes Semantic-Style joint Expectation Perturbations (SSEPs). Specifically, we first establish a style loss based on the kernel function from the feature space of the surrogate model and inject it into gradient-based attacks to form a Semantics-Style joint Loss (SSL) for generating joint perturbations. Subsequently, we use gradient normalization and the proposed dynamic gradient decomposition scheme to address the problems of multi-objective gradient magnitude differences and gradient conflicts that occur in SSL during optimization. Finally, we generate SSEPs by motivating the maximization of the expected loss, thereby enhancing the transferability of Adversarial Examples (AEs). On the ImageNet sub-dataset, extensive experiments show that AEs covered with SSEPs have high transferability. Compared to the baseline attack (MI-FGSM), our method achieves at least a 14 % and 5 % higher attack success rate for normally trained models and defense models, respectively. Compared with other classic and advanced gradient-based attacks and feature-level attacks, our method still has advantages in attack performance. Our code is available at: https://github.com/OUTOFTEN/TransferAttack-ssep},
  archive      = {J_PR},
  author       = {Zhi Lin and Bingwen Wang and Xixi Wang and Yu Zhang and Xiao Wang and Kang Deng and Anjie Peng and Jin Tang and Xing Yang},
  doi          = {10.1016/j.patcog.2025.112474},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112474},
  shortjournal = {Pattern Recognition},
  title        = {Improving adversarial transferability via semantic-style joint expectation perturbations},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SAR image change detection based on saliency region guidance and SIFT keypoint extraction. <em>PR</em>, <em>172</em>, 112471. (<a href='https://doi.org/10.1016/j.patcog.2025.112471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic Aperture Radar (SAR) can operate under all-weather, all-day conditions, playing a crucial role in regional change detection (CD). However, due to its unique imaging principles, SAR images contain significant speckle noise and blurred boundary and detail features, which reduces the detection accuracy and leads to missed detection and false detection. To address these issues, this paper proposes a SAR image CD method based on saliency region guidance and Scale-Invariant Feature Transform (SIFT) keypoint extraction to reduce the interference of speckle noise. First, a saliency region guidance method is introduced to analyze the saliency of local features in SAR images, extracting potentially changed regions and reducing the interference of speckle noise. Second, the SIFT is employed to extract keypoints in regions significantly different from the background in the difference map, leveraging its robustness to speckle noise. By extracting keypoints, the approximate location and extent of the changed regions are determined. These are, then, fused with the saliency region information, enhancing the saliency weights of pixels around keypoints for more extraction of change regions. Finally, a Vision Transformer (ViT) detection network is used for SAR image CD, utilizing the combined saliency information from the original saliency map and SIFT keypoints. This approach effectively integrates SIFT’s stable description of local features with ViT’s modeling capability for global features, improving the model’s accuracy and robustness.},
  archive      = {J_PR},
  author       = {Lu Wang and Bailiang Sun and Chunhui Zhao and Suleman Mazhar and Tomoaki Ohtsuki and P. Takis Mathiopoulos and Fumiyuki Adachi},
  doi          = {10.1016/j.patcog.2025.112471},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112471},
  shortjournal = {Pattern Recognition},
  title        = {SAR image change detection based on saliency region guidance and SIFT keypoint extraction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Anisotropic pth-order TV-based retinex decomposition with adaptive reflectance regularizer for low-light image enhancement. <em>PR</em>, <em>172</em>, 112468. (<a href='https://doi.org/10.1016/j.patcog.2025.112468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement plays a fundamental role in image processing and computer vision. Its primary purpose is to improve the visual quality of an image by enhancing its contrast and brightness. However, most existing enhancement methods tend to amplify the imaging noise, especially in very dark regions of the image, leading to undesirable artifacts in the enhanced result. To address this problem, this paper aims to develop a method that enhances low-light images without introducing these artifacts. We propose a novel anisotropic p th-order total variation-based (ApTV-based) Retinex decomposition with an adaptive reflectance regularizer for low-light image enhancement, where p represents the exponent in our regularization term, controlling the degree of structure preservation in the resulting image. Specifically, for 0 < p ≤ 1 , the ApTV with a smaller p -value can effectively extract strong structures of the image, making it suitable for piecewise smooth illumination estimation. In contrast, a larger p -value can help preserve the image’s fine details and suppress noise, making it favorable for accurate reflectance estimation. More importantly, since the degree of noise amplification varies across different regions, we incorporate the obtained illumination into the reflectance regularizer to enable adaptive denoising. Extensive numerical experiments and comparisons with state-of-the-art low-light image enhancement methods demonstrate that the proposed adaptive Retinex decomposition approach achieves superior performance both qualitatively and quantitatively. It effectively addresses noise amplification and artifact issues while enhancing overall image quality.},
  archive      = {J_PR},
  author       = {Po-Wen Hsieh and Suh-Yuh Yang},
  doi          = {10.1016/j.patcog.2025.112468},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112468},
  shortjournal = {Pattern Recognition},
  title        = {Anisotropic pth-order TV-based retinex decomposition with adaptive reflectance regularizer for low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cooperative multi-task learning and reliability assessment for glioma segmentation and IDH genotyping. <em>PR</em>, <em>172</em>, 112467. (<a href='https://doi.org/10.1016/j.patcog.2025.112467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high heterogeneity of gliomas presents significant challenges in distinguishing isocitrate dehydrogenase (IDH) genotypes based on magnetic resonance imaging (MRI) features. To address this issue, we propose a joint optimization framework based on multi-task learning (MLNet), which enables the simultaneous optimization of glioma segmentation and IDH genotype prediction within a unified framework. First, we design a glioma segmentation network based on a CNN-Transformer hybrid architecture to extract glioma features. Second, feature fusion is employed to provide feature support for the IDH genotyping task. A reliability assessment mechanism is introduced to evaluate the IDH genotyping results, determining whether a secondary assessment is necessary. Finally, we construct a multi-task learning loss function and achieve end-to-end joint training through feature sharing across tasks. We evaluate the proposed method on the BraTs2020 dataset, and comparisons with state-of-the-art methods demonstrate that the multi-task learning method offers superior performance.},
  archive      = {J_PR},
  author       = {Meng Li and Du Jiang and Juntong Yun and Rong Liu and Ying Sun and Gongfa Li},
  doi          = {10.1016/j.patcog.2025.112467},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112467},
  shortjournal = {Pattern Recognition},
  title        = {Cooperative multi-task learning and reliability assessment for glioma segmentation and IDH genotyping},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Gaussian splitting attack: Gaussian splatting-based multi-view 3D adversarial attack. <em>PR</em>, <em>172</em>, 112466. (<a href='https://doi.org/10.1016/j.patcog.2025.112466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-view adversarial attack methods utilize Neural Radiance Fields (NeRF) to generate adversarial samples from different viewpoints of an object effectively deceiving deep neural networks. However, these methods simply add noise to the rendered images and fail to construct explicit 3D adversarial samples limited by the implicit representation of NeRF. To address the above limitation, we propose a novel G aussian S plitting Attack ( GSAttack ) scheme based on Gaussian Splatting to generate explicit 3D adversarial samples that deceive the classifier in various viewpoints . Specifically, we first quantify the contribution of each Gaussian based on its gradient in adversarial attack. Subsequently, we split tiny Gaussians from the high contribution Gaussians as initial 3D perturbations, which are then optimized by adversarial loss to ensure deception in diverse viewpoints. Furthermore, to ensure the invisibility of 3D perturbation, we devise position and color losses to make the perturbations tightly bound to the object surface and minimize the color differences. Owing to these ingenious designs, our 3D perturbations are more natural in space and effective attack neural network. Experimental results show that the 3D adversarial samples generated by our GSAttack can effectively deceive the classifier over a wider range of viewpoints and achieve superior visualization compared to existing schemes.},
  archive      = {J_PR},
  author       = {Lingzhuang Meng and Mingwen Shao and Yuanjian Qiao and Wenjie Liu and Xiang Lv},
  doi          = {10.1016/j.patcog.2025.112466},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112466},
  shortjournal = {Pattern Recognition},
  title        = {Gaussian splitting attack: Gaussian splatting-based multi-view 3D adversarial attack},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hypergraph regularization-based anchor learning for multi-view clustering. <em>PR</em>, <em>172</em>, 112465. (<a href='https://doi.org/10.1016/j.patcog.2025.112465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current anchor graph-based multi-view clustering methods can effectively address the problem of high computational cost for clustering large-scale multimedia data. However, they have the following shortcomings: (1) The relationships between anchor points are not adequately considered. (2) The correlations between the consistent anchor graph and diverse anchor graphs are ignored. To handle these issues, we propose a novel multi-view clustering method named Hypergraph Regularization-Based Anchor Learning (HRFAL). Specifically, we first process the original data to obtain a consistent anchor graph and diverse anchor graphs, which can explore more comprehensive consistent and complementary information. Meanwhile, the hyper-Laplacian regularization is applied to the anchor points to explore the higher-order relationships between the anchor points, thus enabling the generation of high-quality anchor graphs. Furthermore, the orthogonal diversity constraints are imposed on the consistent and diverse anchor graphs to enhance the distinction between the consistent and diverse components, resulting in better exploitation of consistent and complementary information. Finally, the Schatten p -norm constraint is implemented on the consistent anchor graph to maintain its low-rank structure, thus obtaining more robust consistent information. Experimental results on eight multi-view datasets show that HRFAL exhibits superior performance in terms of accuracy and speed.},
  archive      = {J_PR},
  author       = {Yunpeng Zeng and Peng Song and Beihua Yang and Changjia Wang and Guanghao Du and Yanwei Yu and Wenming Zheng},
  doi          = {10.1016/j.patcog.2025.112465},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112465},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph regularization-based anchor learning for multi-view clustering},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A multi-layer processing and coarse filtering network for accurate feature matching. <em>PR</em>, <em>172</em>, 112464. (<a href='https://doi.org/10.1016/j.patcog.2025.112464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core task of feature matching is establishing correspondences between two images. The methods based on Transformers have achieved impressive results, which can directly capture the relationships among all features without relying on the distances between them. However, it also reduce the weight of long-distance texture features and ignore simultaneous integration of global, local, and multi-scale features, leading to limited matching accuracy. To address this issue, we propose a detector-free feature matching method based on Transformer with multi-level processing and coarse-grained filtering. First, we apply a local window aggregation module to minimize irrelevant interference through window attention and combine local self-attention with global self-attention to ensure the features have a global perspective but not lose local details. Then, the multi-scale features are processed in layers, integrating multi-scale information into the matching phase, allowing each layer to perform feature matching at different scales for more precise matches. Additionally, we designed a filter to discard incorrectly matched points in the global context, thereby improving the accuracy of the matching points. Extensive experiments demonstrate that our method delivers excellent results comparing with the current state-of-the-art techniques in the tasks of pose estimation, homography estimation, and visual localization. Compared with the baseline method LoFTR, our method achieves an average improvement of 16.07 % in pose estimation, 6.52 % in homography estimation, and 9.69 % in visual localization. Meanwhile, our method also demonstrates superior performance compared to other state-of-the-art feature matching approaches.},
  archive      = {J_PR},
  author       = {Yuan Guo and Wenpeng Li and Ping Zhai},
  doi          = {10.1016/j.patcog.2025.112464},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112464},
  shortjournal = {Pattern Recognition},
  title        = {A multi-layer processing and coarse filtering network for accurate feature matching},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-level noise augmentation for graph clustering with triplet-wise contrastive learning. <em>PR</em>, <em>172</em>, 112463. (<a href='https://doi.org/10.1016/j.patcog.2025.112463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive deep graph clustering has attracted widespread attention due to its self-supervised representation learning mechanism and excellent clustering performance. Although, most existing methods rely on low-pass filtering to achieve denoising, ignoring the potential benefit of high-frequency information and noise in obtaining comprehensive and robust representation. Second, commonly used contrastive learning strategies generally treat non-target samples as negative samples, which is prone to triggering contrastive bias and weakening the representation quality. To this end, this paper proposes a novel contrastive graph clustering framework, Dual-level Noise Augmentation for Graph Clustering with triplet-wise Contrastive learning (DNA-CGC), strengthening the benefit of noise to enrich the representation learning and amplify the contrastive learning efficacy. It consists of two core modules, Hybrid Noise Representation Augmentation (HNRA) and Noise-Aware Contrastive Learning (NACL). The HNRA module integrates low- and high-frequency graph signals to capture both shared and distinctive node characteristics, while introducing Gaussian noise as beneficial perturbation to enrich the representation diversity, thereby achieving multi-information fusion under hybrid noise. The NACL module, on the other hand, generates exclusive negative samples through Gaussian noise and constructs the triplet-wise contrastive pairs (Target, Positive, Negative), mitigating the contrastive bias by preventing false negatives and further facilitating more accurate semantic alignment. Extensive experiments on six benchmark datasets validate the significant advantages of DNA-CGC in terms of clustering performance and representation quality. The code could be available at https://github.com/TianxiangZhao0474/DNA-CGC.git .},
  archive      = {J_PR},
  author       = {Tianxiang Zhao and Youqing Wang and Shilong Xu and Tianchuan Yang and Junbin Gao and Jipeng Guo},
  doi          = {10.1016/j.patcog.2025.112463},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112463},
  shortjournal = {Pattern Recognition},
  title        = {Dual-level noise augmentation for graph clustering with triplet-wise contrastive learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving lesion segmentation in medical images by global and regional feature compensation. <em>PR</em>, <em>172</em>, 112461. (<a href='https://doi.org/10.1016/j.patcog.2025.112461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated lesion segmentation of medical images has made tremendous improvements in recent years due to deep learning advancements. However, accurately capturing fine-grained global and regional feature representations remains a challenge. Many existing methods achieve suboptimal performance in complex lesion segmentation due to information loss during typical downsampling operations and insufficient capture of either regional or global features. To address these issues, we propose the Global and Regional Compensation Segmentation Framework (GRCSF), which introduces two key innovations: the Global Compensation Unit (GCU) and the Region Compensation Unit (RCU). The proposed GCU addresses resolution loss in the U-shaped backbone by preserving global contextual features and fine-grained details during multiscale downsampling. Meanwhile, the RCU introduces a self-supervised learning (SSL) residual map generated by Masked Autoencoders (MAE), obtained as pixel-wise differences between reconstructed and original images, to highlight regions with potential lesions. These SSL residual maps guide precise lesion localization and segmentation through a patch-based cross-attention mechanism that integrates regional spatial and pixel-level features. Additionally, the RCU incorporates patch-level importance scoring to enhance feature fusion by leveraging global spatial information from the backbone. Experiments on three publicly available medical image segmentation datasets, including brain stroke lesion, lung tumor and coronary artery calcification datasets, demonstrate that our GRCSF outperforms state-of-the-art methods, confirming its effectiveness across diverse lesion types and its potential as a generalizable lesion segmentation solution.},
  archive      = {J_PR},
  author       = {Chuhan Wang and Zhenghao Chen and Jean Y.H. Yang and Jinman Kim},
  doi          = {10.1016/j.patcog.2025.112461},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112461},
  shortjournal = {Pattern Recognition},
  title        = {Improving lesion segmentation in medical images by global and regional feature compensation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RGBX-DiffusionDet: A framework for multi-modal RGB-X object detection using DiffusionDet. <em>PR</em>, <em>172</em>, 112460. (<a href='https://doi.org/10.1016/j.patcog.2025.112460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenge of object detection using multimodal heterogeneous sensors by extending the recently proposed DiffusionDet framework, initially designed for RGB-only detection. We propose RGBX-DiffusionDet, a generalized diffusion-based object detection framework that enables seamless fusion of heterogeneous 2D modalities (denoted as “X”, e.g., depth, infrared, and polarimetric data) with RGB imagery. The proposed approach adopts a mid-level feature fusion strategy to address the heterogeneous nature of multimodal data, characterized by varying spatial resolutions, noise profiles, and semantic content. Instead of commonly used brute-force feature concatenation, we introduce two novel architectural components: (1) a dynamic channel reduction convolutional block attention module (DCR-CBAM), which enhances cross-modal fusion by emphasizing salient channel features while reducing the dimensionality of merged RGB-X features, and (2) a dynamic multi-level aggregation block (DMLAB), which addresses a limitation of the baseline DiffusionDet decoder by adaptively fusing spatial features to improve object localization. Additionally, we incorporate novel regularization losses that promote channel saliency and spatial selectivity, resulting in compact and discriminative feature embeddings. Extensive experiments on RGB-depth (KITTI), a newly annotated RGB-polarimetric (RGB-P) dataset, and RGB-infrared (M3FD) benchmarks demonstrate consistent superiority of the proposed approach over RGB-only baselines, while maintaining decoding efficiency. We further show that RGBX-DiffusionDet exhibits improved robustness and generalization capability in visually-corrupted conditions, demonstrating its practical efficiency for robust multimodal object detection.},
  archive      = {J_PR},
  author       = {Eliraz Orfaig and Inna Stainvas and Igal Bilik},
  doi          = {10.1016/j.patcog.2025.112460},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112460},
  shortjournal = {Pattern Recognition},
  title        = {RGBX-DiffusionDet: A framework for multi-modal RGB-X object detection using DiffusionDet},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FedFAT: Frequency adpative interpolation for federated domain generalization on heterogeneous medical images. <em>PR</em>, <em>172</em>, 112459. (<a href='https://doi.org/10.1016/j.patcog.2025.112459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple distributed medical institutions can leverage federated learning (FL) to collaboratively build a shared prediction model with privacy protection. However, the presence of non-independent and identically distributed (non-IID) data in medical imaging leads to data drift in practical learning scenarios, detrimentally affecting both convergence and generalization to the unseen domain. In this paper, we propose a novel framework named Federated Frequency Adaptive Interpolation(FedFAT), which leverages a frequency space adaptive interpolation mechanism to mitigate data drift in federated domain generalization. FedFAT enables clients to adaptively exchange partial amplitude information, leveraging multi-source data distributions to enhance generalization. Crucially, local phase information is retained to preserve privacy. To mitigate data drift, FedFAT employs cross-client feature alignment via amplitude normalization, which effectively batch-normalizes images from diverse source distributions. Furthermore, we introduce a client-specific weight perturbation mechanism designed to guide local models toward a consistent low-loss region. We have theoretically analyzed the proposed method and empirically conducted extensive experiments on two medical image classification and segmentation tasks, showing that FedFAT outperforms a set of recent state-of-the-art methods with average Dice improvement of 2.42 % and 10.61 % on the prostate MRI segmentation datasets (PROMISE12, PROSTATEx, and NCI-ISBI) and breast cancer classification datasets(CAMELYON17), respectively. FedFAT also has an improvement of 4.15 % on generalization performance. These results demonstrate the superiority of FedFAT in handling data drift and improving generalization performance.},
  archive      = {J_PR},
  author       = {Donghao Wang and Yingchun Cui and Mingyang Li and Heran Xi and Jinghua Zhu},
  doi          = {10.1016/j.patcog.2025.112459},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112459},
  shortjournal = {Pattern Recognition},
  title        = {FedFAT: Frequency adpative interpolation for federated domain generalization on heterogeneous medical images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards unified molecule-enhanced pathology image representation learning via integrating spatial transcriptomics. <em>PR</em>, <em>172</em>, 112458. (<a href='https://doi.org/10.1016/j.patcog.2025.112458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in multimodal pre-training have advanced computational pathology, but current visual-language approaches lack molecular perspective and face performance bottlenecks in clinical settings. Here, we introduce a U nified M olecule-enhanced P athology I mage RE presentation Learning framework ( UMPIRE ) that enhances the robustness and generalization capabilities of pathology image analysis across diverse tissue types and sequencing platforms. UMPIRE leverages complementary information from gene expression profiles to guide multimodal pre-training, addressing the challenge of distribution shifts between research and clinical environments. To overcome the scarcity of paired data, we collected more than 4 million entries of spatial transcriptomics gene expression to train the gene encoder. UMPIRE aligns modalities across 697K pathology image-gene expression pairs, creating a foundation model that demonstrates superior generalization across multiple sequencing platforms and downstream tasks without additional fine-tuning. Comprehensive evaluation shows UMPIRE ’s effectiveness in gene expression prediction, spot classification, and mutation state prediction in whole slide images, with significant improvements over state-of-the-art methods. Our findings demonstrate how molecular data integration enhances visual pattern recognition in computational pathology, providing a resilient approach for bench-to-bedside translation. The code and pre-trained weights are available at https://github.com/Hanminghao/Umpire .},
  archive      = {J_PR},
  author       = {Minghao Han and Dingkang Yang and Jiabei Cheng and Xukun Zhang and Zizhi Chen and Haopeng Kuang and Lihua Zhang},
  doi          = {10.1016/j.patcog.2025.112458},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112458},
  shortjournal = {Pattern Recognition},
  title        = {Towards unified molecule-enhanced pathology image representation learning via integrating spatial transcriptomics},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HMSNet: Hilbert curve enhanced mamba for real-time semantic segmentation. <em>PR</em>, <em>172</em>, 112457. (<a href='https://doi.org/10.1016/j.patcog.2025.112457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a core technology for vehicle perception of the surrounding environment in autonomous driving. However, existing real-time semantic segmentation models face two major challenges: loss of local detail information and inconsistency of intra-class semantic information. To address these issues, we propose a novel network architecture, HMSNet. The network mainly consists of the following three core modules: the Hilbert curve enhanced Visual Mamba Block (HVM Block), Selective Attention Fusion Module (SAFM), and Multi-scale Context-Aware Module (MCAM). The HVM Block utilizes the Hilbert curve to reduce the dimensionality of two-dimensional images and applies a selective scanning algorithm in Mamba, enabling the network to effectively capture local dependencies while maintaining a global receptive field, thereby optimizing the consistency of intra-class semantic information. The SAFM module effectively merges local detail information from shallow networks with global semantic information from deep networks, alleviating the problem of local detail information loss. Finally, the MCAM module, introduced at the end of the network, enhances the model,s ability to judge contextual information, thereby improving segmentation accuracy. Experimental results show that HMSNet achieves an excellent balance between segmentation accuracy and inference speed on challenging public datasets, including CamVid, Cityscapes, and ADE20K.},
  archive      = {J_PR},
  author       = {Lianyin Jia and Aoxiang Gao and Mengjuan Li and Xiaodong Fu and Haihe Zhou and Jiaman Ding},
  doi          = {10.1016/j.patcog.2025.112457},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112457},
  shortjournal = {Pattern Recognition},
  title        = {HMSNet: Hilbert curve enhanced mamba for real-time semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Trend-aware time series clustering via self-attentive LSTM. <em>PR</em>, <em>172</em>, 112455. (<a href='https://doi.org/10.1016/j.patcog.2025.112455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series clustering aims to partition time series into subsets with similar patterns, uncovering their underlying structures and dynamics. This paper proposes a novel clustering method that integrates polynomial curve fitting, an enhanced self-attention mechanism, and a long short-term memory (LSTM) network. First, the Hodrick-Prescott (HP) filter is applied to denoise the raw time series. Then, polynomial curve fitting (PCF) is employed to extract multi-order derivative features at each time point, capturing local trend information and constructing a high-dimensional feature space. An enhanced self-attention LSTM model is designed to encode both raw and trend-based features into a hidden state sequence, enabling the model to capture key patterns and long-range dependencies. Finally, a distance metric based on the hidden states is defined and incorporated into a hierarchical clustering (HC) algorithm. Experiments on several public univariate datasets with long sequences demonstrate that the proposed method outperforms conventional approaches, offering a robust solution for modeling and interpreting complex time series.},
  archive      = {J_PR},
  author       = {Chongyan Wu and Bin Yu},
  doi          = {10.1016/j.patcog.2025.112455},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112455},
  shortjournal = {Pattern Recognition},
  title        = {Trend-aware time series clustering via self-attentive LSTM},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-view biclustering via non-negative matrix tri-factorisation. <em>PR</em>, <em>172</em>, 112454. (<a href='https://doi.org/10.1016/j.patcog.2025.112454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data is ever more apparent as methods for production, collection and storage of data become more feasible both practically and fiscally. However, not all features are relevant to describe the patterns for all individuals. Multi-view biclustering aims to simultaneously cluster both rows and columns, discovering clusters of rows as well as their view-specific identifying features. A novel multi-view biclustering approach based on non-negative matrix factorisation is proposed named ResNMTF. Demonstrated through extensive experiments on both synthetic and real datasets, ResNMTF successfully identifies both overlapping and non-exhaustive biclusters, without pre-existing knowledge of the number of biclusters present, and is able to incorporate any combination of shared dimensions across views. Further, to address the lack of a suitable bicluster-specific intrinsic measure, the popular silhouette score is extended to the bisilhouette score. The bisilhouette score is demonstrated to align well with known extrinsic measures, and proves useful as a tool for hyperparameter tuning as well as visualisation.},
  archive      = {J_PR},
  author       = {Ella S.C. Orme and Theodoulos Rodosthenous and Marina Evangelou},
  doi          = {10.1016/j.patcog.2025.112454},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112454},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view biclustering via non-negative matrix tri-factorisation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Corrigendum to “Efficient multi-view discrete co-clustering with learned graph” [Pattern recognition 168 (2025) 111811]. <em>PR</em>, <em>172</em>, 112453. (<a href='https://doi.org/10.1016/j.patcog.2025.112453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Jiaqi Nie and Qianyao Qiang and Jason Chen Zhang and Fei Hao},
  doi          = {10.1016/j.patcog.2025.112453},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112453},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “Efficient multi-view discrete co-clustering with learned graph” [Pattern recognition 168 (2025) 111811]},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LayerCLIP: A fine-grained class activation map for weakly supervised semantic segmentation. <em>PR</em>, <em>172</em>, 112452. (<a href='https://doi.org/10.1016/j.patcog.2025.112452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) using image-level labels aims to create pseudo-labels leveraging Class Activation Maps (CAM) to train a separate segmentation model. Recent methods that utilize Contrastive Language-Image Pre-training (CLIP) models have achieved significant advancements. These approaches take advantage of CLIP’s capability to identify various categories without requiring additional training. However, due to the limited local information of the final embedding layer, the CAM generated by the CLIP model is still a rough region with an under-activated or over-activated issue. Furthermore, the abundant multi-layer information of CLIP, which plays a vital role in dense prediction, has been ignored. In this paper, we proposed a LayerCLIP model for a fine-grained CAM generation via hierarchical features, which consists of two consecutive components: a dynamic hierarchical CAMs module and an adaptive affinity module. Specifically, the dynamic hierarchical CAMs module utilizes the hierarchical features to produce two complementary CAMs, along with a dynamic strategy to fuse these CAMs. Subsequently, the affinity based on multi-head self-attention is adaptively reweighted to refine CAM by the CAM itself in the adaptive affinity module. LayerCLIP significantly enhances the quality of CAM. Our method achieves a new state-of-the-art performance on PASCAL VOC 2012 (75.1 % mIoU) and MS COCO 2014 (46.9 % mIoU) through extensive benchmark experiments.},
  archive      = {J_PR},
  author       = {Lingma Sun and Le Zou and Xianghu Lv and Zhize Wu and Xiaofeng Wang},
  doi          = {10.1016/j.patcog.2025.112452},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112452},
  shortjournal = {Pattern Recognition},
  title        = {LayerCLIP: A fine-grained class activation map for weakly supervised semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Generative models for noise-robust training in unsupervised domain adaptation. <em>PR</em>, <em>172</em>, 112450. (<a href='https://doi.org/10.1016/j.patcog.2025.112450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent unsupervised domain adaptation (UDA) methods show the effectiveness of pseudo-labels for unlabeled target domain. However, pseudo-labels inevitably contain noise, which can degrade adaptation performance. This paper thus propose a Generative models for Noise-Robust Training (GeNRT), a method designed to mitigate label noise while reducing domain shift. The key idea is leveraging the class-wise distributions of the target domain, modeled by generative models, provide more reliable pseudo-labels than individual pseudo-labeled instances. This is because the distributions statistically better represent class-wise information than a single instance. Based on this observation, GeNRT incorporates a Distribution-based Class-wise Feature Augmentation (D-CFA), which enhances feature representations by sampling features from target class distributions modeled by generative models. These augmented features serve a dual purpose: (1) providing class-level knowledge from generative models to train a noise-robust discriminative classifier, and (2) acting as intermediate features to bridge the domain gap at the class level. Furthermore, GeNRT leverages Generative and Discriminative Consistency (GDC), enforcing consistency regularization between a generative classifier (formed by all class-wise generative models) and the learned discriminative classifier. By aggregating knowledge across target class distributions, GeNRT improves pseudo-label reliability and enhances robustness against label noise. Extensive experiments on Office-Home, VisDA-2017, PACS, and Digit-Five show that our GeNRT achieves comparable performance to state-of-the-art methods under both single-source and multi-source UDA settings.},
  archive      = {J_PR},
  author       = {Zhongying Deng and Da Li and Junjun He and Xiaojiang Peng and Yi-Zhe Song and Tao Xiang},
  doi          = {10.1016/j.patcog.2025.112450},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112450},
  shortjournal = {Pattern Recognition},
  title        = {Generative models for noise-robust training in unsupervised domain adaptation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Tiny object detection based on dynamic scale-awareness label assignment and contextual enhancement. <em>PR</em>, <em>172</em>, 112449. (<a href='https://doi.org/10.1016/j.patcog.2025.112449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prosperity of recent object detection can not camouflage the deficiencies of tiny object detection. The generic object detectors suffer a dramatic performance degradation on tiny object detection. For this purpose, we present a tiny object detection approach based on Dynamic scale-awareness label assignment and Contextual enhancement (DCNet), which improves the tiny object detection performance from label assignment and feature enhancement perspectives. Considering the IoU-based label assignment seriously harms the positive samples for tiny objects, we design a Dynamic Scale-Awareness (DSA) label assignment to replace it in the region proposal network. The DSA label assignment adaptively rescales preset anchors and introduces the regression information to better assign the preset anchors for tiny objects. Furthermore, the tiny objects often exhibit weak feature responses due to their poor-quality appearance. Therefore, we propose a contextual enhancement module that aggregates contextual information at different scales to enhance tiny objects’ feature responses. Comprehensive experimental analyses on multiple datasets confirm the effectiveness and good generality of our proposed DCNet in tiny object detection.},
  archive      = {J_PR},
  author       = {Tianyang Zhang and Xiangrong Zhang and Chaozhuo Hua and Guanchun Wang and Xiao Han and Licheng Jiao},
  doi          = {10.1016/j.patcog.2025.112449},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112449},
  shortjournal = {Pattern Recognition},
  title        = {Tiny object detection based on dynamic scale-awareness label assignment and contextual enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DBL: Dual-level balanced learning for long-tailed classification. <em>PR</em>, <em>172</em>, 112448. (<a href='https://doi.org/10.1016/j.patcog.2025.112448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data are typically long-tailed, causing neural networks to over-fit head classes and underperform on rare tails. We propose Dual-Level Balanced Learning (DBL), an efficient training framework that balances gradients at both the class and instance levels. DBL combines Class-aware Balancing (CB), which corrects class-level imbalance by re-weighting gradients according to prediction bias; Instance-aware Balancing (IB), which alleviates instance-level imbalance by emphasising the learning of hard examples; and a lightweight Cross-Level Collaboration (CC) scheme that harmonises the two losses. By jointly addressing class- and instance-level imbalance, DBL delivers consistent gains across all classes and most individual samples. Extensive experiments on CIFAR10/100-LT, ImageNet-LT, Places-LT, and iNaturalist18 show that DBL sets new state-of-the-art accuracy on all five benchmarks, confirming its robustness to severe long-tailed distributions.},
  archive      = {J_PR},
  author       = {Zheng Wu and Kehua Guo and Sheng Ren and Bin Hu and Xiangyuan Zhu and Rui Ding},
  doi          = {10.1016/j.patcog.2025.112448},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112448},
  shortjournal = {Pattern Recognition},
  title        = {DBL: Dual-level balanced learning for long-tailed classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-frequency shared-feature-learning based diffusion model for removing surgical smoke. <em>PR</em>, <em>172</em>, 112447. (<a href='https://doi.org/10.1016/j.patcog.2025.112447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surgical smoke in laparoscopic surgery can deteriorate visibility for surgeons. This work aims to simultaneously remove the surgical smoke and restore true-to-life image colors with deep learning. However, deep learning-based smoke removal remains a challenge due to: 1) the non-homogeneous distribution of surgical smoke, 2) higher frequency modes being hindered from being learned due to spectral bias. In this work, we propose the multi-frequency shared-feature-learning based conditional diffusion model with adaptive smoke attention for removing surgical smoke. The proposed model learns to map both the smoky and smokeless images into a shared inherent feature by the forward learning and synthesize the smokeless image by the reverse learning, and the input noisy image used for the forward learning is wrapped by the smoke attention learning to ease sampling steps and facilitate shared feature optimization. The smoke attention learning employs smoke segmentation and convolutional block attention modules to capture the non-homogeneous features of smoke. The multi-frequency learning is introduced to incorporate with shared feature learning to enhance the mid-to-high frequency features. In addition, the multi-task learning incorporates shared feature loss, smoke perception loss, dark channel prior loss, and contrast enhancement loss to help the model optimization. The experimental results show that the proposed method outperforms other state-of-the-art methods on both synthetic/real laparoscopic surgical images, with the potential to be embedded in laparoscopic devices for de-smoking.},
  archive      = {J_PR},
  author       = {Hao Li and Xiangyu Zhai and Ziwei Liang and Jie Xue and Bin Jin and Haitao Niu and Guangyong Zhang and Huanxin Ding and Dengwang Li and Pu Huang},
  doi          = {10.1016/j.patcog.2025.112447},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112447},
  shortjournal = {Pattern Recognition},
  title        = {Multi-frequency shared-feature-learning based diffusion model for removing surgical smoke},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SeeD: Online similarity-preserving pattern discovery for streaming trajectories. <em>PR</em>, <em>172</em>, 112446. (<a href='https://doi.org/10.1016/j.patcog.2025.112446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid accumulation of fresh trajectory data has fueled a growing interest in the analysis of such data. There has been a notable economic and social value attributed to effectively uncovering mobility behaviors within rich, streaming trajectory data for applications like urban planning, marketing and intelligence. Despite extensive research on pattern discovery, existing methods often confine themselves to fixed patterns, neglecting the potential synergy between pattern discovery and similarity queries. This synergy can be bidirectional: similarity results could be the foundation of pattern discovery, while pattern discovery can accelerate the similarity queries. To bridge this gap, we propose the Online S imilarity-preserving Traj e ctory Patt e rn D iscovery, called SeeD . This framework consists of three core modules: (1) The composite windowing strategy, which extracts multi-scale trajectory information and maintains correlation patterns, ensuring data relevance across various scales. (2) The C lustering-based S imilarity Q uery (CSQ) module, which accelerates similarity computation based on pattern discovery results, thus improving query efficiency. (3) The E volution D etection and A nalysis (EDA) module, which enhances overall performance by analyzing pattern evolution, providing insights into dynamic changes within trajectory data. Extensive experimental results conducted on well-established datasets unequivocally demonstrate the effectiveness of SeeD, indicating its potential to revolutionize the field by offering a robust solution for pattern discovery.},
  archive      = {J_PR},
  author       = {Junhua Fang and Jiayi Li and Chunhui Feng and Zhicheng Pan and Pingfu Chao and Jiajie Xu and Pengpeng Zhao},
  doi          = {10.1016/j.patcog.2025.112446},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112446},
  shortjournal = {Pattern Recognition},
  title        = {SeeD: Online similarity-preserving pattern discovery for streaming trajectories},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Decoding the brain via multi-view brain topology contrastive learning. <em>PR</em>, <em>172</em>, 112445. (<a href='https://doi.org/10.1016/j.patcog.2025.112445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Graph Neural Networks (GNNs) have been widely used in neural decoding due to strong topological feature mining and interpretability. GNNs are heavily based on manually defined brain topology; if there are false connections or noise, it will greatly affect the decoding performance. To address the aforementioned challenges, a series of GNN-based graph topology learning (GTL) methods have received widespread attention due to their ability to automatically optimize brain topology. However, existing GTL methods are usually implemented in a supervised manner and rely on a large amount of annotated data, making it difficult to directly transfer them to different decoding scenarios. Therefore, in this paper, a Brain Topology Inference framework based on Multi-View Contrastive Self-supervised Learning (BTI-MVCSL) is proposed for neural decoding. Specifically, BTI-MVCSL first designs a series of graph learners, which can infer brain topological connections as “learner”, generate topology learning objectives as “instructor” from the original fMRI data, and maximize consistency between “instructor” and “learner” to extract the rich information in hidden connections. Furthermore, in order to achieve fully automated topology learning guidance, BTI-MVCSL develops a new self-learning mechanism that can use the “learner”-view brain topology to update the “instructor”-view brain topology during model optimization and further achieves comparative constraints through the “instructor” topology. The proposed BTI-MVCSL has been extensively evaluated in two publicly available fMRI datasets, demonstrating superior performance and revealing potential changes in brain topology under different decoding tasks.},
  archive      = {J_PR},
  author       = {Ziyu Li and Zhiyuan Zhu and Qing Li and Xia Wu},
  doi          = {10.1016/j.patcog.2025.112445},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112445},
  shortjournal = {Pattern Recognition},
  title        = {Decoding the brain via multi-view brain topology contrastive learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic clustering transformer for LiDAR-based 3D object detection. <em>PR</em>, <em>172</em>, 112444. (<a href='https://doi.org/10.1016/j.patcog.2025.112444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR perception is a critical task in 3D computer vision. Currently, inspired by the success of vision transformers in 2D images, many LiDAR-based detectors also partition the whole scene point cloud into non-overlapping windows, and perform window attention and window shifting to capture local and global information respectively. While these methods improved performance of LiDAR detection task, they often fail to account for the intrinsic separability of 3D LiDAR point clouds. Unlike 2D images, where objects can overlap and blend into one another, objects in LiDAR are distinct and non-overlapping. In this paper, building upon this insight, we propose the Dynamic Cluster Transformer (DCT), a clustering-based point cloud backbone that incorporates transformer architecture. Our approach is designed to exploit the unique characteristics of LiDAR point clouds, enabling a more efficient 3D feature extraction. Specifically, the DCT architecture comprises two primary modules: Sparse Cluster Generation (SCG) and Cluster Feature Interaction (CFI). The Sparse Cluster Generation is responsible for producing initial sparse cluster features from the entire scene point cloud, providing a basis for local and global feature propagation. The Cluster Feature Interaction then facilitates information propagation between these clusters and surrounding voxels, allowing for a more comprehensive understanding of the spatial relationships. This proposed clustering-based learning process is simple yet effective, conforming to the physical characteristics of LiDAR point clouds. Empirical results demonstrate that DCT achieves state-of-the-art performance on the large-scale Waymo Open Dataset and nuScenes dataset.},
  archive      = {J_PR},
  author       = {Yubo Cui and Zhiheng Li and Zheng Fang},
  doi          = {10.1016/j.patcog.2025.112444},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112444},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic clustering transformer for LiDAR-based 3D object detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Condense loss: Exploiting vector magnitude during person re-identification training process. <em>PR</em>, <em>172</em>, 112443. (<a href='https://doi.org/10.1016/j.patcog.2025.112443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The magnitudes of features and weights significantly affect the gradients during the training process. L2 normalized softmax losses (such as NormFace, CosFace, ArcFace, etc.) and Naive softmax losses both reduce the magnitudes of image features in the training process and achieve good results in face recognition and person re-identification tasks, respectively. In this paper, we fully utilize the feature vector magnitudes and propose Condense loss for Re-ID tasks, which replaces the inner production of Naive softmax loss with the negative Euclidean distance. Condense loss generates negative radial gradients when updating weight parameters to push all features compacter. Because the coefficients of tangential gradients (the tangential component of the gradients) are related to feature magnitudes, it ideally provides monotonically decreasing tangential gradients, resulting in gradually diminishing updates that enhance the stability of the training process. We also introduce a margin parameter into Condense loss to enlarge inter-class distances and thus help the model learn more discriminative features. Mathematical analysis is given in this paper, and we have conducted sufficient experiments focusing on Re-ID tasks to prove the corresponding conclusion. The experimental results demonstrate that the Condense loss achieves competitive results compared to the state-of-the-art methods in the person re-identification task. At the same time, it also has a good performance in face recognition tasks.},
  archive      = {J_PR},
  author       = {Xi Yang and Wenjiao Dong and Yingzhi Tang and Gu Zheng and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2025.112443},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112443},
  shortjournal = {Pattern Recognition},
  title        = {Condense loss: Exploiting vector magnitude during person re-identification training process},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Entropy-informed weighting channel normalizing flow for deep generative models. <em>PR</em>, <em>172</em>, 112442. (<a href='https://doi.org/10.1016/j.patcog.2025.112442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalizing Flows (NFs) are widely used in deep generative models for their exact likelihood estimation and efficient sampling. However, they require substantial memory since the latent space matches the input dimension. Multi-scale architectures address this by progressively reducing latent dimensions while preserving reversibility. Existing multi-scale architectures use simple, static channel-wise splitting, limiting expressiveness. To improve this, we introduce a regularized, feature-dependent Shuffle operation and integrate it into vanilla multi-scale architecture. This operation adaptively generates channel-wise weights and shuffles latent variables before splitting them. We observe that such operation guides the variables to evolve in the direction of entropy increase, hence we refer to NFs with the Shuffle operation as Entropy-Informed Weighting Channel Normalizing Flow (EIW-Flow). Extensive experiments on CIFAR-10, CelebA, ImageNet, and LSUN demonstrate that EIW-Flow achieves state-of-the-art density estimation and competitive sample quality for deep generative modeling, with minimal computational overhead.},
  archive      = {J_PR},
  author       = {Wei Chen and Shian Du and Shigui Li and Delu Zeng and John Paisley},
  doi          = {10.1016/j.patcog.2025.112442},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112442},
  shortjournal = {Pattern Recognition},
  title        = {Entropy-informed weighting channel normalizing flow for deep generative models},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Buffer-free class-incremental learning with out-of-distribution detection. <em>PR</em>, <em>172</em>, 112441. (<a href='https://doi.org/10.1016/j.patcog.2025.112441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must learn new classes over time without forgetting previous ones and handle inputs from unknown classes that a closed-set model would misclassify. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. When post hoc OOD detection is applied at inference time, we discover that it can effectively replace buffer-based strategies. We examine the performance of these methods in terms of classification accuracy of seen samples and rejection rates of unseen samples. We show that our approach achieves competitive performance compared to recent multi-head and single-head methods that rely on memory buffers and other buffer-free approaches. The results show that the proposed approach outperforms them in a closed-world setting and detects unseen samples while being significantly resource-efficient. Experimental results on CIFAR-10, CIFAR-100, and Tiny ImageNet support our findings and offer new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.},
  archive      = {J_PR},
  author       = {Srishti Gupta and Daniele Angioni and Maura Pintor and Ambra Demontis and Lea Schönherr and Fabio Roli and Battista Biggio},
  doi          = {10.1016/j.patcog.2025.112441},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112441},
  shortjournal = {Pattern Recognition},
  title        = {Buffer-free class-incremental learning with out-of-distribution detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Corrigendum to “DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction” [Pattern recognition 172 (2026) 112339]. <em>PR</em>, <em>172</em>, 112440. (<a href='https://doi.org/10.1016/j.patcog.2025.112440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Chengcheng Li and Luqi Gong and Leiheng Xu and Xin Wang},
  doi          = {10.1016/j.patcog.2025.112440},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112440},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction” [Pattern recognition 172 (2026) 112339]},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A progressive attention network with transformer for multi-label image recognition. <em>PR</em>, <em>172</em>, 112439. (<a href='https://doi.org/10.1016/j.patcog.2025.112439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research typically improves the performance of multi-label image recognition by constructing higher-order pairwise label correlations. However, these methods lack the ability to effectively learn multi-scale features, which makes it difficult to distinguish small-scale objects. Moreover, most current attention-based methods to capture local salient features may ignore many useful non-salient features. To address the aforementioned issues, we propose a Transformer-based Progressive Attention Network (TPANet) for multi-label image recognition. Specifically, we first design a new adaptive multi-scale feature attention (AMSA) module to learn cross-scale features in multi-level features. Then, to excavate various useful object features, we introduce the transformer encoder to construct a semantic spatial attention (ESA) module and also propose a context-aware feature enhanced (CAFE) module. The former ESA module is used to discover complete object regions and capture discriminative features, and the latter CAFE module leverages object-local features to enhance pixel-level global features. The proposed TPANet model can generate more accurate object labels in three popular benchmark datasets (i.e., MS-COCO 2014, Pascal VOC 2007 and Visual Genome), and is competitive to state-of-the-art models (e.g., SST and FL-Tran, etc.).},
  archive      = {J_PR},
  author       = {Sulan Zhang and Zhenwen Liao and Jianeng Li and Lihua Hu and Jifu Zhang},
  doi          = {10.1016/j.patcog.2025.112439},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112439},
  shortjournal = {Pattern Recognition},
  title        = {A progressive attention network with transformer for multi-label image recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Layer-wise correlation and attention discrepancy distillation for semantic segmentation. <em>PR</em>, <em>172</em>, 112438. (<a href='https://doi.org/10.1016/j.patcog.2025.112438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has recently garnered increased attention in segmentation tasks due to its effective balance between accuracy and computational efficiency. Nonetheless, existing methods mainly rely on structured knowledge from a single layer, overlooking the valuable discrepant knowledge that captures the diversity and distinctiveness of features across various layers, which is essential for the KD process. We present Layer-wise Correlation and Attention Discrepancy Distillation (LCADD) to tackle this issue, training compact and accurate semantic segmentation networks by considering layer-wise discrepancy knowledge. Specifically, we employ two distillation schemes: (i) correlation discrepancy distillation, which constructs a pixel-wise correlation discrepancy matrix across various layers to seize more detailed spatial dependencies, and (ii) attention discrepancy self-distillation, which aims to guide the shallower layers of the student network to emulate the attention discrepancy maps of the deeper layers, facilitating self-learning of attention discrepancy knowledge within the student network. Each proposed method is designed to work collaboratively in learning discrepancy knowledge, allowing the student network to better imitate the teacher from the perspective of layer-wise discrepancy. Our method has demonstrated superior performance on various semantic segmentation datasets, including Cityscapes, Pascal VOC 2012, and CamVid, compared to the latest knowledge distillation techniques, thereby validating its effectiveness.},
  archive      = {J_PR},
  author       = {Jianping Gou and Kaijie Chen and Cheng Chen and Weihua Ou and Xin Luo and Zhang Yi},
  doi          = {10.1016/j.patcog.2025.112438},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112438},
  shortjournal = {Pattern Recognition},
  title        = {Layer-wise correlation and attention discrepancy distillation for semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Gradient semi-masking for improving adversarial robustness. <em>PR</em>, <em>172</em>, 112433. (<a href='https://doi.org/10.1016/j.patcog.2025.112433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In gradient masking, certain complex signal processing and probabilistic optimization strategies exhibit favorable characteristics such as nonlinearity, irreversibility, and feature preservation, thereby providing new solutions for adversarial defense. Inspired by this, this paper proposes a plug-and-play gradient semi-masking module ( GSeM ) to improve the adversarial robustness of neural networks. GSeM primarily contains a feature straight-through pathway that allows for normal gradient propagation and a feature mapping pathway that interrupts gradient flow. The multi-pathway and semi-masking characteristics cause GSeM to exhibit opposing behaviors when processing data and gradients. Specifically, during data processing, GSeM compresses the state space of features while introducing white noise augmentation. However, during gradient processing, it leads to inefficient updates to certain parameters and ineffective generation of training examples. To address this shortcoming, we correct gradient propagation and introduce gradient-corrected adversarial training. Extensive experiments demonstrate that GSeM differs fundamentally from earlier gradient masking methods: it can genuinely enhance the adversarial defense performance of neural networks, surpassing previous state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Xinlei Liu and Tao Hu and Peng Yi and Baolin Li and Jichao Xie and Hailong Ma},
  doi          = {10.1016/j.patcog.2025.112433},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112433},
  shortjournal = {Pattern Recognition},
  title        = {Gradient semi-masking for improving adversarial robustness},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Structural-prior guided bi-generative network for image inpainting. <em>PR</em>, <em>172</em>, 112432. (<a href='https://doi.org/10.1016/j.patcog.2025.112432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is a great challenge when reconstructed with realistic textures and required to enhance the consistency of semantic structures in large-scale missing regions. However, popular structural prior guidance methods primarily rely on the reconstruction of structural features. Due to the Markovian property inherent in purely feedforward architectures, noise undergoes persistent accumulation and propagation in early network layers. Without intermediate feedback mechanisms, minor artifacts in shallow layers would be nonlinearly amplified through successive convolution operations and cannot be timely corrected, thereby hindering the extraction of valid structural information. To this end, we presents a bi-generative network (Bi-GNet) guided by specific semantic structures, including an auxiliary network N s and an inpainting network N inp . Here N s provides the structural prior information to N inp for reconstructing the texture details of images. Additionally, we provide the spatial coordinate attention (SCA) and the adaptive feature filtering (AFF) module to ensure structural consistency and texture plausibility in the reconstructed content. Experiments demonstrate that Bi-GNet significantly outperforms other state-of-the-art approaches on three datasets and achieves good inpainting results on the Mogao Grottoes mural dataset.},
  archive      = {J_PR},
  author       = {Jiajun Zhang and Jizhao Liu and Huaikun Zhang and Jibao Zhang and Jing Lian},
  doi          = {10.1016/j.patcog.2025.112432},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112432},
  shortjournal = {Pattern Recognition},
  title        = {Structural-prior guided bi-generative network for image inpainting},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning from majority label: A novel problem in multi-class multiple-instance learning. <em>PR</em>, <em>172</em>, 112425. (<a href='https://doi.org/10.1016/j.patcog.2025.112425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a novel multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag-level label. The goal of LML is to train a classification model that estimates the class of each instance using the majority label. This problem is valuable in a variety of applications, including pathology image segmentation, political voting prediction, customer sentiment analysis, and environmental monitoring. To solve LML, we propose a Counting Network trained to produce bag-level majority labels, estimated by counting the number of instances in each class. Furthermore, analysis experiments on the characteristics of LML revealed that bags with a high proportion of the majority class facilitate learning. Based on this result, we developed a Majority Proportion Enhancement Module (MPEM) that increases the proportion of the majority class by removing minority class instances within the bags. Experiments demonstrate the superiority of the proposed method on four datasets compared to conventional MIL methods. Moreover, ablation studies confirmed the effectiveness of each module. The code is available at here .},
  archive      = {J_PR},
  author       = {Kaito Shiku and Shinnosuke Matsuo and Daiki Suehiro and Ryoma Bise},
  doi          = {10.1016/j.patcog.2025.112425},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112425},
  shortjournal = {Pattern Recognition},
  title        = {Learning from majority label: A novel problem in multi-class multiple-instance learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature subset weighting for distance-based supervised learning. <em>PR</em>, <em>172</em>, 112424. (<a href='https://doi.org/10.1016/j.patcog.2025.112424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces feature subset weighting using monotone measures for distance-based supervised learning. The Choquet integral is used to define a distance function that incorporates these weights. This integration enables the proposed distances to effectively capture non-linear relationships and account for interactions both between conditional and decision attributes and among conditional attributes themselves, resulting in a more flexible distance measure. In particular, we show how this approach ensures that the distances remain unaffected by the addition of duplicate and strongly correlated features. Another key point of this approach is that it makes feature subset weighting computationally feasible, since only m feature subset weights should be calculated each time instead of calculating all feature subset weights ( 2 m ), where m is the number of attributes. Next, we also examine how the use of the Choquet integral for measuring similarity leads to a non-equivalent definition of distance. The relationship between distance and similarity is further explored through dual measures. Additionally, symmetric Choquet distances and similarities are proposed, preserving the classical symmetry between similarity and distance. Finally, we introduce a concrete feature subset weighting distance, evaluate its performance in a k -nearest neighbours (KNN) classification setting, and compare it against Mahalanobis distances and weighted distance methods.},
  archive      = {J_PR},
  author       = {Adnan Theerens and Yvan Saeys and Chris Cornelis},
  doi          = {10.1016/j.patcog.2025.112424},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112424},
  shortjournal = {Pattern Recognition},
  title        = {Feature subset weighting for distance-based supervised learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A simple yet lightweight module for enhancing domain generalization through relative representation. <em>PR</em>, <em>172</em>, 112423. (<a href='https://doi.org/10.1016/j.patcog.2025.112423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) learns a model from multiple source domains to combat individual domain differences and ensure generalization to unseen domains. Most existing methods focus on learning domain-invariant absolute representations. However, we empirically observe that such representations often suffer from notable distribution divergence, leading to unstable performance in diverse unseen domains. In contrast, relative representations, constructed w.r.t. a set of anchors, naturally capture geometric relationships and exhibit intrinsic stability within a dataset. Despite this potential, their application to DG remains largely unexplored, due to their common transductive assumption that anchors require access to target-domain data, which is incompatible with the inductive setting of DG. To address this issue, we design Re2SL, a simple and lightweight plug-in module that follows a pre-trained encoder and constructs anchors solely from source-domain prototypes, thereby ensuring a completely inductive design. To our knowledge, Re2SL is the first to explore relative representation for DG. This design is inspired by the insight that ReS idual differences between absolute and domain-specific representations can spontaneously seek stable representations within the same distribution shared across all domains . Leveraging these stable representations, we construct cross-domain ReL ative representation to enhance stability and transferability without accessing any target data during training or anchor computation. Empirical studies show that our constructed representation exhibits minimal H -divergence, confirming its stability. Notably, Re2SL achieves up to 4.3 % improvement while reducing computational cost by 90 %, demonstrating its efficiency.},
  archive      = {J_PR},
  author       = {Meng Cao and Songcan Chen},
  doi          = {10.1016/j.patcog.2025.112423},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112423},
  shortjournal = {Pattern Recognition},
  title        = {A simple yet lightweight module for enhancing domain generalization through relative representation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). 4DStyleGaussian: Generalizable 4D style transfer with gaussian splatting. <em>PR</em>, <em>172</em>, 112422. (<a href='https://doi.org/10.1016/j.patcog.2025.112422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D neural style transfer has gained significant attention for its potential to provide user-friendly stylization with 3D spatial consistency. However, existing 3D style transfer methods often struggle with inference efficiency, generalization, and maintaining temporal consistency when handling dynamic scenes. In this paper, we introduce 4DStyleGaussian, a novel 4D style transfer framework designed to achieve real-time stylization of arbitrary style references while maintaining reasonable content affinity, multi-view consistency, and temporal coherence. Our approach leverages an embedded 4D Gaussian Splatting technique, which is trained utilizing a reversible neural network for reducing content loss and artifacts in the feature distillation process. With the pre-trained 4D embedded Gaussians for efficient and view-consistent rendering, we predict a 4D style transformation matrix that facilitates spatially and temporally consistent style transfer. Experiments demonstrate that our method can achieve high-quality and generalizable stylization for 4D scenarios with enhanced efficiency and spatial-temporal consistency, with 7.1 % lower LPIPS and 2.5× faster inference compared to existing methods.},
  archive      = {J_PR},
  author       = {Wanlin Liang and Hongbin Xu and Weitao Chen and Feng Xiao and Wenxiong Kang},
  doi          = {10.1016/j.patcog.2025.112422},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112422},
  shortjournal = {Pattern Recognition},
  title        = {4DStyleGaussian: Generalizable 4D style transfer with gaussian splatting},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Retinex-guided generative diffusion prior for low-light image enhancement. <em>PR</em>, <em>172</em>, 112421. (<a href='https://doi.org/10.1016/j.patcog.2025.112421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Retinex-based training-free low-light image enhancement (LLIE) methods often rely on complex architectures or lack support for text-controlled personalization. In this paper, we propose RetinexGDP, a training-free and text-controllable LLIE framework that uniquely integrates Retinex-based image modeling with generative diffusion priors. First, we introduce a simplified Retinex decomposition by embedding weighted total variation optimization into a single Gaussian convolutional layer, enabling robust illumination estimation without the need for training. Next, we guide the diffusion denoising process using the estimated reflectance map, employing patch-wise inversion and reflectance-conditioned sampling to effectively suppress noise while preserving structural details. Finally, unlike previous diffusion-based LLIE methods that perform only monotonous global brightness enhancement, we incorporate text guidance into the sampling process, enabling controllable enhancement that aligns with user-specific stylistic preferences. RetinexGDP thus provides a modular, interpretable, and text-controllable solution for low-light image enhancement. Experimental results show that RetinexGDP achieves state-of-the-art performance in terms of NIQMC and CPCQI metrics across seven real-world datasets. Code will be available at: https://github.com/zhaozunjin/PLIE},
  archive      = {J_PR},
  author       = {Zunjin Zhao and Daming Shi},
  doi          = {10.1016/j.patcog.2025.112421},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112421},
  shortjournal = {Pattern Recognition},
  title        = {Retinex-guided generative diffusion prior for low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive integration of textual context and visual embeddings for underrepresented vision classification. <em>PR</em>, <em>172</em>, 112420. (<a href='https://doi.org/10.1016/j.patcog.2025.112420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of deep learning has significantly improved image classification performance; however, handling long-tail distributions remains challenging due to the limited data available for rare classes. Existing approaches predominantly focus on visual features, often neglecting the valuable contextual information provided by textual data, which can be especially beneficial for classes with sparse visual examples. In this work, we introduce a novel method addressing this limitation by integrating textual data generated by advanced language models with visual inputs through our newly proposed Adaptive Integration Block for Vision-Text Synergy (AIB-VTS). Specifically designed for Vision Transformer architectures, AIB-VTS adaptively balances visual and textual information during inference, effectively utilizing textual descriptions generated from large language models. Extensive experiments on benchmark datasets demonstrate substantial performance improvements across all class groups, particularly in underrepresented (tail) classes. These results confirm the effectiveness of our approach in leveraging textual context to mitigate data scarcity issues and enhance model robustness.},
  archive      = {J_PR},
  author       = {Seongyeop Kim and Hyung-Il Kim and Yong Man Ro},
  doi          = {10.1016/j.patcog.2025.112420},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112420},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive integration of textual context and visual embeddings for underrepresented vision classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PCFFusion: Progressive cross-modal feature fusion network for infrared and visible images. <em>PR</em>, <em>172</em>, 112419. (<a href='https://doi.org/10.1016/j.patcog.2025.112419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion (IVIF) aims to fuse thermal target information in infrared images and spatial texture information in visible images, improving the observability and comprehensibility of the fused images. Currently, most IVIF methods suffer from the loss of salient target information and texture details in fused images. To alleviate this problem, a progressive cross-modal feature fusion network (PCFFusion) for IVIF is proposed, which comprises two stages: feature extraction and feature fusion. In the feature extraction stage, to enhance the network’s feature representation capability, a feature decomposition module (FDM) is constructed to extract two modal features of different scales by defining a feature decomposition operation (FDO). In addition, by establishing correlations between the high- frequency and low-frequency components of two modal features, a cross-modal feature enhancement module (CMFEM) is built to realize correction and enhancement of the two features at each scale. The feature fusion stage achieves the fusion of two modal features at each scale and the supplementation of adjacent scale features by constructing three cross-domain fusion module (CDFMs). To constrain the fused results preserve more salient targets and richer texture details, a dual-feature fidelity loss function is defined by constructing a salient weight map to balance the two loss terms. Extensive experiments demonstrate that fusion results of the proposed method highlight prominent targets from infrared images while retaining rich background details from visible images, and the performance of PCFFusion is superior to some advanced methods. Specifically, compared to the optimal results obtained by other comparison methods, the proposed network achieves an average increase of 30.35 % and 10.9 % in metrics Mutual Information (MI) and Standard deviation (SD) on the TNO dataset, respectively.},
  archive      = {J_PR},
  author       = {Shuying Huang and Kai Zhang and Yong Yang and Weiguo Wan},
  doi          = {10.1016/j.patcog.2025.112419},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112419},
  shortjournal = {Pattern Recognition},
  title        = {PCFFusion: Progressive cross-modal feature fusion network for infrared and visible images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MonoA2: Adaptive depth with augmented head for monocular 3D object detection. <em>PR</em>, <em>172</em>, 112418. (<a href='https://doi.org/10.1016/j.patcog.2025.112418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection is a hot direction due to its low cost and configuration simplicity. Achieving accurate instance depth prediction from monocular images is a challenging problem in monocular 3D object detection. Many existing methods perform instance depth prediction based on fixed rules, which are not flexible for various objects. Furthermore, these methods ignore the design of more discriminative task heads. To address these issues, we propose the MonoA 2 , which consists of the Adaptive Depth Module (ADM) and the Augmented Head Module (AHM). The ADM is used to achieve more accurate depth prediction by learning adaptive offsets to decouple the depth prediction from object center constraints. The AHM is proposed to obtain more discriminative task heads through task-aware attention and task-interaction attention. The task-aware attention can generate different weights adapted to different tasks and the task-interaction attention can guide depth tasks to interact with other tasks. Experimental results on the KITTI and Waymo datasets demonstrate the effectiveness of the proposed method. Our method achieves superior performance on the KITTI and Waymo benchmarks.},
  archive      = {J_PR},
  author       = {Jinpeng Dong and Sanping Zhou and Yufeng Hu and Yuhao Huang and Jingjing Jiang and Weiliang Zuo and Shitao Chen and Nanning Zheng},
  doi          = {10.1016/j.patcog.2025.112418},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112418},
  shortjournal = {Pattern Recognition},
  title        = {MonoA2: Adaptive depth with augmented head for monocular 3D object detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Time series adaptive mode decomposition (TAMD): Method for improving forecasting accuracy in the apparel industry. <em>PR</em>, <em>172</em>, 112417. (<a href='https://doi.org/10.1016/j.patcog.2025.112417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of apparel sales is critical for inventory management, supply chain optimization, and market strategy planning. However, existing forecasting models often struggle to effectively capture the complex characteristics of apparel sales data, such as distinct seasonality, cyclicality, and strongly nonlinear fluctuations, which significantly hinder prediction accuracy and generalization ability. To address these challenges, this study introduces a novel Time series Adaptive Mode Decomposition (TAMD)-based forecasting algorithm. The proposed method: (1) employs Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and sample entropy-guided Variational Mode Decomposition (VMD) to separate the input time series into noise components and multiple smooth Intrinsic Mode Functions (IMFs), to better capture intrinsic data dynamics; (2) refines the sub-series distribution features via an adaptive module guided by sample entropy, dividing each sub-series into subsequences with maximal distribution difference to improve adaptability to periodic changes and market volatility; (3) predicts each subsequence with adaptive distribution matching based on discontinuous random subsequence combinations, and then linearly superposes the prediction results as a final output, thereby boosting accuracy and generalizability. Comprehensive experiments on both public and self-constructed datasets (including four years of Taobao sales data for dresses, jeans, sweatshirts, and sweaters, totaling over 44.7 million records) demonstrate that TAMD outperforms existing methods significantly, highlighting its effectiveness in revealing the complexity of apparel market data and enhancing prediction performance.},
  archive      = {J_PR},
  author       = {Guangbao Zhou and Pengliang Liu and Quanle Lin and Miao Qian and Zhong Xiang and Zeyu Zheng and Lixian Liu},
  doi          = {10.1016/j.patcog.2025.112417},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112417},
  shortjournal = {Pattern Recognition},
  title        = {Time series adaptive mode decomposition (TAMD): Method for improving forecasting accuracy in the apparel industry},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A framework for bias-aware dataset evaluation in soft facial attribute recognition. <em>PR</em>, <em>172</em>, 112416. (<a href='https://doi.org/10.1016/j.patcog.2025.112416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft Facial Attribute Recognition (FAR) remains largely unexplored in terms of demographic fairness. To the best of our knowledge, this study presents one of the first comprehensive analyses of demographic bias in FAR, proposing a systematic framework to detect, quantify, and promote awareness of both representational and stereotypical biases, supporting their mitigation. Leveraging established taxonomies, we evaluate state-of-the-art datasets using a rigorous set of interpretable bias metrics to uncover hidden demographic imbalances. To support reliable fairness assessment, we first enrich the datasets with standardized demographic annotations using the FairFace model. We then address label inconsistencies through the integration of predictions from advanced Vision-Language Models (VLMs). Our analysis reveals substantial imbalances across gender, age, and racial categories-specifically White, Black, and Asian- affecting dataset composition. Furthermore, we show that conventional fairness metrics often yield divergent assessments, highlighting the importance of multi-metric evaluation. This study provides a replicable methodology and actionable insights to support bias-aware facial analysis.},
  archive      = {J_PR},
  author       = {Lucia Cascone and Michele Nappi and Chiara Pero and Xinggang Wang},
  doi          = {10.1016/j.patcog.2025.112416},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112416},
  shortjournal = {Pattern Recognition},
  title        = {A framework for bias-aware dataset evaluation in soft facial attribute recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fast multi-view discrete clustering with two solvers. <em>PR</em>, <em>172</em>, 112415. (<a href='https://doi.org/10.1016/j.patcog.2025.112415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph clustering follows a three-phase process: constructing view-specific similarity graphs, fusing information from different views, and conducting eigenvalue decomposition followed by post-processing to obtain the clustering indicators. However, it encounters two key challenges: the high computational cost of graph construction and eigenvalue decomposition, and the inevitable information deviation introduced by the last process. To tackle these obstacles, we propose Fast Multi-view Discrete Clustering with two solvers (FMDC), to directly and efficiently solve the multi-view graph clustering problem. FMDC involves: (1) generating a compact set of representative anchors to construct anchor graphs, (2) automatically weighting them into a symmetric and doubly stochastic aggregated similarity matrix, (3) executing clustering on the aggregated form with the discrete indicator matrix directly computed through two efficient solvers that we devised. The linear computational complexity of FMDC w.r.t. data size is a notable improvement over traditional quadratic or cubic complexity. Extensive experiments confirm the superior performance of FMDC both in efficiency and in effectiveness.},
  archive      = {J_PR},
  author       = {Qianyao Qiang and Bin Zhang and Jason Chen Zhang and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112415},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112415},
  shortjournal = {Pattern Recognition},
  title        = {Fast multi-view discrete clustering with two solvers},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Clinical knowledge enhanced medical image classification. <em>PR</em>, <em>172</em>, 112414. (<a href='https://doi.org/10.1016/j.patcog.2025.112414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scarcity of data in medical field, deep learning-based medical image classification faces challenges in both accuracy and reliability. Foundation models (FMs) provide a promising enhancement strategy by extracting the text medical knowledge embeddings from FMs and use it to guide the specific classification model. However, the clinical knowledge is generally structurized, and the use of pure text as knowledge representation may not be significant enough for enhancing downstream model. Moreover, the lesion areas are generally subtle, combining FMs to downstream model in a coarse-grained manner still faces challenge in precisely attending the lesions. To tackle these challenges, we propose a novel medical image classification model that effectively embeds clinical knowledge through combining graphs and FMs. First, we represent the clinical rules as graphs, where the node describes the critical characteristics of disease. During training, we use FMs to extract the embeddings of node text description, and use graph transformer to extract global representation of graphs. By employing vision transformer to encode input images, we propose a global-local alignment module to transfer clinical knowledge where the embeddings of image branch and graph branch are aligned from image-to-graph level and patch-to-vertex level, respectively. Moreover, we propose a dynamic image patch selection method to reduce the attention of the model to irrelevant and noisy regions. Experimental results on bladder tumor classification dataset verifies that even with limited training data, the proposed method can not only achieve SOTA performance, but also accurately attend the lesion areas, thus improving the trustworthiness.},
  archive      = {J_PR},
  author       = {Zhikang Xu and Jiye Liang and Zhipeng Wei and Xiaodong Yue and Deyu Li},
  doi          = {10.1016/j.patcog.2025.112414},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112414},
  shortjournal = {Pattern Recognition},
  title        = {Clinical knowledge enhanced medical image classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Asymmetric simulation-enhanced flow reconstruction for incomplete multimodal learning. <em>PR</em>, <em>172</em>, 112413. (<a href='https://doi.org/10.1016/j.patcog.2025.112413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multimodal learning addresses the common real-world challenge of missing modalities, which undermines the performance of standard multimodal methods. Existing solutions struggle with distribution mismatches between reconstructed and observed data, asymmetric cross-modal structures, and insufficient cross-modal knowledge sharing. To tackle these issues, we propose an asymmetric simulation-enhanced flow reconstruction (ASE-FR) framework, which contains following contributions: (1) Distribution-consistent flow reconstruction module that align available and missing modality distributions by normalizing flows; (2) Asymmetric simulation module that perturbs and randomly masks features to mimic real-world modality absence and improve robustness; (3) Modal-shared knowledge distillation that transfers shared representations from teacher encoders to a student encoder through contrastive learning. This framework is applicable to a range of real-world scenarios, such as multi-sensor networks in smart manufacturing, medical diagnostic systems combining imaging and electronic health records, and autonomous driving platforms that integrate camera and LiDAR data. The experimental results show that our ASE-FR method achieves 94.71 %, 41.85 % and 81.90 % accuracy on Audiovision-MNIST, MM-IMDb and IEMOCAP datasets, as well as 1.1376 error rate on CMU-MOSI dataset, which exhibits competitive performance.},
  archive      = {J_PR},
  author       = {Jiacheng Yao and Jing Zhang and Yixiao Wang and Li Zhuo},
  doi          = {10.1016/j.patcog.2025.112413},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112413},
  shortjournal = {Pattern Recognition},
  title        = {Asymmetric simulation-enhanced flow reconstruction for incomplete multimodal learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Benchmarking the spatial robustness of DNNs via natural and adversarial localized corruptions. <em>PR</em>, <em>172</em>, 112412. (<a href='https://doi.org/10.1016/j.patcog.2025.112412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remained underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.},
  archive      = {J_PR},
  author       = {Giulia Marchiori Pietrosanti and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1016/j.patcog.2025.112412},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112412},
  shortjournal = {Pattern Recognition},
  title        = {Benchmarking the spatial robustness of DNNs via natural and adversarial localized corruptions},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preserving privacy without compromising accuracy: Machine unlearning for handwritten text recognition. <em>PR</em>, <em>172</em>, 112411. (<a href='https://doi.org/10.1016/j.patcog.2025.112411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten Text Recognition (HTR) is crucial for document digitization, but handwritten data can contain user-identifiable features, like unique writing styles, posing privacy risks. Regulations such as the “right to be forgotten” require models to remove these sensitive traces without full retraining. We introduce a practical encoder-only transformer baseline as a robust reference for future HTR research. Building on this, we propose a two-stage unlearning framework for multihead transformer HTR models. Our method combines neural pruning with machine unlearning applied to a writer classification head, ensuring sensitive information is removed while preserving the recognition head. We also present Writer-ID Confusion (WIC), a method that forces the forget set to follow a uniform distribution over writer identities, unlearning user-specific cues while maintaining text recognition performance. We compare WIC to Random Labeling, Fisher Forgetting, Amnesiac Unlearning, and DELETE within our prune-unlearn pipeline and consistently achieve better privacy and accuracy trade-offs. This is the first systematic study of machine unlearning for HTR. Using metrics such as Accuracy, Character Error Rate (CER), Word Error Rate (WER), and Membership Inference Attacks (MIA) on the IAM and CVL datasets, we demonstrate that our method achieves state-of-the-art or superior performance for effective unlearning. These experiments show that our approach effectively safeguards privacy without compromising accuracy, opening new directions for document analysis research. Our code is publicly available at https://github.com/leitro/WIC-WriterIDConfusion-MachineUnlearning .},
  archive      = {J_PR},
  author       = {Lei Kang and Xuanshuo Fu and Lluis Gomez and Alicia Fornés and Ernest Valveny and Dimosthenis Karatzas},
  doi          = {10.1016/j.patcog.2025.112411},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112411},
  shortjournal = {Pattern Recognition},
  title        = {Preserving privacy without compromising accuracy: Machine unlearning for handwritten text recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Federated automatic latent variable selection in multi-output gaussian processes. <em>PR</em>, <em>172</em>, 112410. (<a href='https://doi.org/10.1016/j.patcog.2025.112410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a federated learning approach that automatically selects the number of latent processes in multi-output Gaussian processes (MGPs). The MGP has seen great success as a transfer learning tool when data is generated from multiple sources/units/entities. A common approach in MGPs to transfer knowledge across units involves gathering all data from each unit to a central server and extracting common independent latent processes to express each unit as a linear combination of the shared latent patterns. However, this approach poses key challenges in (i) determining the adequate number of latent processes and (ii) relying on centralized learning which leads to potential privacy risks and significant computational burdens on the central server. To address these issues, we propose a hierarchical model that places spike-and-slab priors on the coefficients of each latent process. These priors help automatically select only needed latent processes by shrinking the coefficients of unnecessary ones to zero. To estimate the model while avoiding the drawbacks of centralized learning, we propose a variational inference-based approach, that formulates model inference as an optimization problem compatible with federated settings. We then design a federated learning algorithm that allows units to jointly select and infer the common latent processes without sharing their data. We also discuss an efficient learning approach for a new unit within our proposed federated framework. Simulation and case studies on Li-ion battery degradation and air temperature data demonstrate the advantageous features of our proposed approach.},
  archive      = {J_PR},
  author       = {Jingyi Gao and Seokhyun Chung},
  doi          = {10.1016/j.patcog.2025.112410},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112410},
  shortjournal = {Pattern Recognition},
  title        = {Federated automatic latent variable selection in multi-output gaussian processes},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive multi-view consistency clustering via structure-enhanced contrastive learning. <em>PR</em>, <em>172</em>, 112409. (<a href='https://doi.org/10.1016/j.patcog.2025.112409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art deep multi-view clustering methods resort to contrastive learning to learn consensus representations with Cross-View Consistency ( CVC ). However, contrastive learning has inherent limitations when being applied to the multi-view clustering. On one hand, contrastive learning suffers from class collision issue, compromising the discriminability of consensus representation. On the other hand, contrastive alignment of two views of different quality could lead to representation degradation for the higher-quality view, weakening the robustness of the consensus representation. To alleviate these issues, this paper presents an Adaptive Multi-view consistency clustering method via structure-enhanced contrastive learning ( A da M ), which learns multi-faceted consensus representation that balances view-consistency, discriminability and robustness, forming an optimal consensus representation. Specifically, we first design a view fusion module and a structural learning module to learn view weights and structural relationships among samples, respectively, to derive the consensus representation. Second, beyond CVC , we propose a novel clustering framework called Adaptive Multi-View Consistency ( AMVC ), which adaptively aligns specific view representation with consensus representation based on the learned view weights. Furthermore, compared to CVC , we theoretically demonstrate the superiority of AMVC in learning robust consensus representation. Third, A da M leverages the structural relationships among samples to refine the conventional contrastive loss, further enhancing the discriminability of the consensus representation. Extensive experimental results on eight datasets demonstrate the superior performance of A da M over eight advanced multi-view clustering baselines.},
  archive      = {J_PR},
  author       = {Xuqian Xue and Qi Cai and Zhanwei Zhang and Yiming Lei and Hongming Shan and Junping Zhang},
  doi          = {10.1016/j.patcog.2025.112409},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112409},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive multi-view consistency clustering via structure-enhanced contrastive learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A text-only weakly supervised learning framework for text spotting via text-to-polygon generator. <em>PR</em>, <em>172</em>, 112408. (<a href='https://doi.org/10.1016/j.patcog.2025.112408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced text spotting methods typically rely on large-scale, meticulously labeled datasets to achieve satisfactory performance. However, annotating fine-grained positional information of texts in real-world scene images is extremely costly and time-consuming. Although some weakly supervised methods have been developed to reduce annotation costs, they face two major challenges: 1) their performance significantly lags behind the fully supervised counterparts, and 2) They are tightly coupled with specific text spotting models, meaning that switching to a different model would require retraining and incur substantial computational costs. To address these limitations, we propose a novel text-only weakly supervised learning framework for text spotting via text-to-polygon generator. In the first stage, we pretrain a text-to-polygon generator on an auxiliary dataset, e.g., synthetic or public datasets, where full annotations are readily accessible. In the second stage, given real-world target datasets annotated with text-only labels, we employ the pretrained generator to produce pseudo polygon labels, thereby constructing a pseudo-labeled supervised dataset for training text spotting models. To ensure high-quality pseudo polygon labels, the text-to-polygon generator first identifies all candidate text regions, then filters those that are relevant to the target text, and finally predicts their precise spatial locations. Notably, this generator requires only a single pretraining session and can subsequently be applied to any text spotting model and target text-only dataset without incurring additional costs. Extensive experiments on public benchmarks demonstrate that our method can significantly reduce labeling costs while maintaining competitive performance.},
  archive      = {J_PR},
  author       = {Gege Zhang and Zhiyong Gan and Ling Deng and Shuaicheng Niu and Zhenghua Peng and Gang Dai and Shuangping Huang and Xiangmin Xu},
  doi          = {10.1016/j.patcog.2025.112408},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112408},
  shortjournal = {Pattern Recognition},
  title        = {A text-only weakly supervised learning framework for text spotting via text-to-polygon generator},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging synthetic data for zero–shot and few–shot circle detection in real–world domains. <em>PR</em>, <em>172</em>, 112407. (<a href='https://doi.org/10.1016/j.patcog.2025.112407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circle detection plays a pivotal role in computer vision, underpinning applications from industrial inspection and bioinformatics to autonomous driving. Traditional methods, however, often struggle with real–world complexities, as they demand extensive parameter tuning and adaptation across different domains. In this paper, we present the Synthetic Circle Dataset (SynCircle), a large synthetic image dataset designed to train a YOLO v10 network for circle detection. The YOLO v10 network, pre–trained solely on synthetic data, demonstrates remarkable off–the–shelf performance that surpasses conventional methods in various practical scenarios. Furthermore, we show that incorporating just a few labeled real images for fine–tuning can significantly boost performance, reducing the need for large annotated datasets. To promote reproducibility and streamline adoption, we publicly release both the trained YOLO v10 weights and the full SynCircle dataset.},
  archive      = {J_PR},
  author       = {Paolo Andreini and Marco Tanfoni and Simone Bonechi and Monica Bianchini},
  doi          = {10.1016/j.patcog.2025.112407},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112407},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging synthetic data for zero–shot and few–shot circle detection in real–world domains},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Aegis: A domain generalization framework for medical image segmentation by mitigating feature misalignment. <em>PR</em>, <em>172</em>, 112406. (<a href='https://doi.org/10.1016/j.patcog.2025.112406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain shift caused by variations in data acquisition significantly impedes the deployment of medical image segmentation models in clinical settings. Domain generalization aims to mitigate performance degradation induced by domain shift by training a model using source domain data and generalize well to unseen target domain. In this work, we have an interesting observation: domain shift results in significantly different activation patterns across domains even they have semantically identical input. This cross-domain “feature misalignment” phenomenon motivates us to develop a hypothesis: mitigating cross-domain feature misalignment may enhance domain generalization. To this end, we propose a framework called Aegis , which employs style augmentation to generate augmented image features that simulate domain shift. Subsequently, we introduce a dual attention-guided feature calibration (DAFC) module to facilitate feature interaction between source and augmented images, thereby establishing an implicit alignment constraint within the shared feature space. Furthermore, we propose an uncertainty-guided feature alignment (UFA) loss, which quantifies segmentation discrepancies caused by domain shift and incorporates an uncertainty-weighting mechanism to enhance the alignment of hard-to-classify pixel regions. These components work in synergy to effectively mitigate cross-domain feature misalignment, promote robust feature alignment, and ultimately improve cross-domain generalization. Extensive experiments conducted on three widely used benchmarks demonstrate that the proposed framework significantly outperforms existing methods in domain generalization. Code is available at https://github.com/Zerua-bit/Aegis .},
  archive      = {J_PR},
  author       = {Yuheng Xu and Taiping Zhang and Yuqi Fang},
  doi          = {10.1016/j.patcog.2025.112406},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112406},
  shortjournal = {Pattern Recognition},
  title        = {Aegis: A domain generalization framework for medical image segmentation by mitigating feature misalignment},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preference isolation forest for structure-based anomaly detection. <em>PR</em>, <em>172</em>, 112405. (<a href='https://doi.org/10.1016/j.patcog.2025.112405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest ( PIF ), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: i ) Voronoi- iForest , the most general solution, ii ) RuzHash - iForest , that avoids explicit computation of distances via Local Sensitive Hashing, and iii ) Sliding- PIF , that leverages a locality prior to improve efficiency and effectiveness.},
  archive      = {J_PR},
  author       = {Filippo Leveni and Luca Magri and Cesare Alippi and Giacomo Boracchi},
  doi          = {10.1016/j.patcog.2025.112405},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112405},
  shortjournal = {Pattern Recognition},
  title        = {Preference isolation forest for structure-based anomaly detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DURN: Data uncertainty-driven robust network for mural sketch detection. <em>PR</em>, <em>172</em>, 112404. (<a href='https://doi.org/10.1016/j.patcog.2025.112404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mural sketches reveal both the content and structure of the murals and are crucial for the preservation of murals. However, existing methods lack robustness, making it difficult to suppress noise while preserving sketches on damaged murals and fully capturing details on clear murals. To address this, we propose a Data Uncertainty-Driven Robust Network (DURN) for mural sketch detection. DURN uses uncertainty to quantify noise in the murals, converting prediction into a learnable normal distribution, where the mean represents the sketch and the variance denotes the uncertainty. This enables the model to learn both the sketch and the noise simultaneously, achieving noise suppression while preserving the sketches. To enhance sketches, we design an Adaptive Fusion Feature Enhancement Module (AFFE) to dynamically adjust the fusion strategy according to the contribution of features at different scales and reduce the information loss caused by feature dimensionality reduction to maximize the utility of each feature. We develop a novel Deep-Shallow Supervision (DSS) module to mitigate background noise using deep semantic information to guide shallow features without adding parameters. Additionally, we achieve model lightweighting through pruning techniques, ensuring competitive performance while reducing the number of parameters to only 4.5 % of the original. The experimental results show an improvement of 10. 4 % AP over existing methods, demonstrating the robustness of DURN for complex and damaged murals. The source code is available at https://github.com/TIVEN-Z/DURN .},
  archive      = {J_PR},
  author       = {Shenglin Peng and Xingguo Zhao and Jun Wang and Lin Wang and Shuyi Qu and Jingye Peng and Xianlin Peng},
  doi          = {10.1016/j.patcog.2025.112404},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112404},
  shortjournal = {Pattern Recognition},
  title        = {DURN: Data uncertainty-driven robust network for mural sketch detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learn depth space from light field via a distance-constraint query mechanism. <em>PR</em>, <em>172</em>, 112403. (<a href='https://doi.org/10.1016/j.patcog.2025.112403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Light Field (LF) captures both spatial and angular information of scenes, enabling precise depth estimation. Recent advancements in deep learning have led to significant success in this field; however, existing methods primarily focus on modeling surface characteristics (e.g., depth maps) while overlooking the depth space, which contains additional valuable information. The depth space consists of numerous space points and provides substantially more geometric data than a single depth map. In this paper, we conceptualize depth prediction as a spatial modeling problem, aiming to learn the entire depth space rather than merely a single depth map. Specifically, we define space points as signed distances relative to the scene surface and propose a novel distance-constraint query mechanism for LF depth estimation. To model the depth space effectively, we first develop a mixed sampling strategy to approximate its data representation. Subsequently, we introduce an encoder-decoder network architecture to query the distances of each point, thereby implicitly embedding the depth space. Finally, to extract the target depth map from this space, we present a generation algorithm that iteratively invokes the decoder network. Through extensive experiments, our approach achieves the highest performance on LF depth estimation benchmarks, and also demonstrates superior performance on various synthetic and real-world scenes.},
  archive      = {J_PR},
  author       = {Hao Sheng and Rongshan Chen and Ruixuan Cong and Da Yang and Zhenglong Cui and Sizhe Wang},
  doi          = {10.1016/j.patcog.2025.112403},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112403},
  shortjournal = {Pattern Recognition},
  title        = {Learn depth space from light field via a distance-constraint query mechanism},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised instance segmentation with superpixels. <em>PR</em>, <em>172</em>, 112402. (<a href='https://doi.org/10.1016/j.patcog.2025.112402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Cuong Manh Hoang},
  doi          = {10.1016/j.patcog.2025.112402},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112402},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised instance segmentation with superpixels},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MIGF-net: Multimodal interaction-guided fusion network for image aesthetics assessment. <em>PR</em>, <em>172</em>, 112401. (<a href='https://doi.org/10.1016/j.patcog.2025.112401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social media, people like to post images and comments to share their ideas, which provides rich visual and textural semantic information for image aesthetics assessment (IAA). However, most previous works either extracted the unimodal aesthetic features from image due to the difficulty of obtaining comments, or combined multimodal information together but ignoring the interactive relationship between image and comment, which limits the overall performance. To solve the above problem, we propose a Multimodal Interaction-Guided Fusion Network (MIGF-Net) for image aesthetics assessment based on both image and comment semantic information, which can not only solve the challenge of comment generating, but also provide the multimodal feature interactive information. Specifically, considering the coupling mechanism of the image theme, we construct a visual semantic fusion module to extract the visual semantic feature based on the visual attributes and the theme features. Then, a textural semantic feature extractor is designed to mine the semantic information hidden in comments, which not only addresses the issue of missing comments but also effectively complements the visual semantic features. Furthermore, we establish a Dual-Stream Interaction-Guided Fusion module to fuse the semantic features of images and comments, fully exploring the interactive relationship between images and comments in the human brain’s perception mechanism. Experimental results on two public image aesthetics evaluation datasets demonstrate that our model outperforms the current state-of-the-art methods. Our code will be released at https://github.com/wenzhipeng123/MIGF-Net .},
  archive      = {J_PR},
  author       = {Yun Liu and Zhipeng Wen and Leida Li and Peiguang Jing and Daoxin Fan},
  doi          = {10.1016/j.patcog.2025.112401},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112401},
  shortjournal = {Pattern Recognition},
  title        = {MIGF-net: Multimodal interaction-guided fusion network for image aesthetics assessment},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IRIS: An information path planning method based on reinforcement learning and information-directed sampling. <em>PR</em>, <em>172</em>, 112400. (<a href='https://doi.org/10.1016/j.patcog.2025.112400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information Path Planning (IPP) is a critical aspect of robotics, aimed at intelligently selecting information-rich paths to optimize robot trajectories and significantly enhance the efficiency and quality of data collection. However, in the process of maximizing information acquisition, IPP must also account for energy consumption, time constraints, and physical obstacles, which often lead to inefficiencies. To address these challenges, we propose an Information Path Planning method based on Reinforcement Learning and Information-Directed Sampling (IRIS). This model is the first to integrate Reinforcement Learning (RL) with Information-Directed Sampling (IDS), ensuring both immediate rewards and the potential for greater information gain through exploratory actions. IRIS employs an off-policy deep reinforcement learning framework, effectively overcoming the limitations observed in on-policy methods, thereby enhancing the model’s adaptability and efficiency. Simulation results demonstrate that the IRIS algorithm performs exceptionally well across various IPP scenarios. Once training stabilizes, IDS will dominate decision-making with a probability of approximately 1.3 % to yield better outcomes, highlighting its significant potential in this field. The relevant code is available at https://github.com/SUTLZY/IRIS .},
  archive      = {J_PR},
  author       = {Ziyuan Liu and Yan Zhuang and Peng Wu and Yuanchang Liu},
  doi          = {10.1016/j.patcog.2025.112400},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112400},
  shortjournal = {Pattern Recognition},
  title        = {IRIS: An information path planning method based on reinforcement learning and information-directed sampling},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Knowledge tailoring: Bridging the teacher-student gap in semantic segmentation. <em>PR</em>, <em>172</em>, 112399. (<a href='https://doi.org/10.1016/j.patcog.2025.112399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation transfers knowledge from a high-capacity teacher network to a compact student network, but a large capacity gap often limits the student’s ability to fully benefit from the teacher’s guidance. In semantic segmentation, another major challenge is the difficulty in predicting accurate object boundaries, as even strong teacher models can produce ambiguous or imprecise outputs. To address both challenges, we present Knowledge Tailoring, a novel distillation framework that adapts the teacher’s knowledge to better match the student’s representational capacity and learning dynamics. Much like a tailor adjusts an oversized suit to fit the wearer’s shape, our method reshapes the teacher’s abundant but misaligned knowledge into a form more suitable for the student. KT introduces feature tailoring, which restructures intermediate features based on channel-wise correlation to narrow the representation gap, and logit tailoring, which improves boundary prediction by refining class-specific logits. The tailoring strategy evolves throughout training, offering guidance that aligns with the student’s progress. Experiments on Cityscapes, Pascal VOC, and ADE20K confirm that KT consistently enhances performance across a variety of architectures including DeepLabV3, PSPNet, and SegFormer. Our code is available for https://github.com/seok-hwa/KT .},
  archive      = {J_PR},
  author       = {Seokhwa Cheung and Seungbeom Woo and Taehoon Kim and Wonjun Hwang},
  doi          = {10.1016/j.patcog.2025.112399},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112399},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge tailoring: Bridging the teacher-student gap in semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Understanding and tackling the modality imbalance problem in multimodal survival prediction. <em>PR</em>, <em>172</em>, 112398. (<a href='https://doi.org/10.1016/j.patcog.2025.112398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the in-depth integration of multimodal data, survival prediction has emerged as a pivotal task in cancer prognosis by facilitating personalized treatment planning and medical resource allocation. In this study, we report an intriguing phenomenon of inter-modality capability gap (ICG) enlargement during joint survival modelling of genomics data and pathology images. This observation, supported by our dedicated theoretical analysis, uncovers a previously unrecognized modality imbalance problem, where pathology modality suffers from limited gradient propagation and insufficient learning while genomics modality dominates in reducing survival loss. To tackle this problem, we further propose a balanced multimodal learning approach for survival prediction named BMLSurv, which introduces two innovative auxiliary learning strategies: self-enhancement learning (SEL) and peer-assistance learning (PAL). The SEL strategy exploits a real-time imbalance measure to guide extra task-aware supervision, therefore dynamically strengthening pathology-specific gradient propagation in a self-enhanced manner. Meanwhile, the PAL strategy leverages the stronger genomics modality as a “helpful peer” to assist the sufficient learning of pathology modality via a new risk-ranking distillation technique. Extensive experiments on representative cancer datasets demonstrate that by successfully address the modality imbalance problem, BMLSurv remarkably narrows the ICG in joint survival modelling and consistently outperforms state-of-the-art methods by a large margin. These results underscore the potential of BMLSurv to advance multimodal survival prediction and enhance clinical decision-making in cancer prognosis.},
  archive      = {J_PR},
  author       = {Chicheng Zhou and Minghui Wang and Yi Shi and Anli Zhang and Ao Li},
  doi          = {10.1016/j.patcog.2025.112398},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112398},
  shortjournal = {Pattern Recognition},
  title        = {Understanding and tackling the modality imbalance problem in multimodal survival prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyper-network curvature: A new representation method for high-order brain network analysis. <em>PR</em>, <em>172</em>, 112397. (<a href='https://doi.org/10.1016/j.patcog.2025.112397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human brain is a complex system and contains abundant high-order interactions among multiple brain regions, which can be described by brain hyper-network. In brain hyper-networks, nodes represent brain regions of interest (ROIs), while edges describe the interactions of multiple ROIs, providing important high-order information for brain disease analysis and diagnosis. However, most of the existing hyper-network studies focused on the hyper-connection (i.e. hyper-edge) analysis and ignored the local topological information on nodes. To address this problem, we propose a new representation method (i.e., hyper-network curvature) for brain hyper-network analysis. Compared with the existing hyper-network representation methods, the proposed hyper-network curvature can be used to analyze the local topologies of nodes in brain hyper-networks. Based on hyper-network curvature, we further propose a novel graph kernel called brain hyper-network curvature kernel to measure the similarity of a pair of brain hyper-networks. We have proved that the proposed hyper-network curvature is bounded and brain hyper-network curvature kernel is positive definite. To evaluate the effectiveness of our proposed method, we perform the classification experiments on functional magnetic resonance imaging data of brain diseases. The experimental results demonstrate that our proposed method can significantly improve classification accuracy compared to the state-of-the-art graph kernels and graph neural networks for classifying brain diseases.},
  archive      = {J_PR},
  author       = {Kai Ma and Tianyu Du and Qi Zhu and Xuyun Wen and Jiashuang Huang and Xibei Yang and Daoqiang Zhang},
  doi          = {10.1016/j.patcog.2025.112397},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112397},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-network curvature: A new representation method for high-order brain network analysis},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Federated cross-source learning for lung nodule segmentation with data characteristic-aware weight optimization. <em>PR</em>, <em>172</em>, 112396. (<a href='https://doi.org/10.1016/j.patcog.2025.112396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables multiple medical institutions to undertake distributed training while protecting patient privacy. Nevertheless, the significant variance in data distributions across diverse sites results in imbalanced knowledge acquisition, thereby affecting the performance of the global model. To tackle this challenge, we propose a novel federated algorithm for lung nodule segmentation, incorporating a Cross-source Learning (CSL) method. This method generates pseudo nodules by synthesizing the nodule phase spectrum with the nodule amplitude spectrum from other clients. These pseudo nodules are subsequently embedded into pulmonary regions to augment the data. By incorporating knowledge from various clients, which alleviates the challenges posed by non-IID data. On the server side, a Data Characteristic-aware Weight Optimization (DCWO) method is proposed to incorporate client data quality assessment and the size of lung nodule volume as weights to optimize both model performance and fairness. On the client side, we design a Multi-scale Attention Dynamic Convolution (MADC) lightweight network, which dynamically adapts attention to different spatial regions and extracts features at multiple scales. The performance of our method is superior to the state-of-the-art methods on six public and in-house CT datasets of lung cancer.},
  archive      = {J_PR},
  author       = {Xinjun Bian and Huan Lin and Yumeng Wang and Lingqiao Li and Zhenbing Liu and Huadeng Wang and Zhenwei Shi and Yi Qian and Zaiyi Liu and Rushi Lan and Xipeng Pan},
  doi          = {10.1016/j.patcog.2025.112396},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112396},
  shortjournal = {Pattern Recognition},
  title        = {Federated cross-source learning for lung nodule segmentation with data characteristic-aware weight optimization},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Joint luminance-chrominance learning for quality assessment of low-light image enhancement. <em>PR</em>, <em>172</em>, 112395. (<a href='https://doi.org/10.1016/j.patcog.2025.112395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for low-light enhancement quality assessment (LEQA) often underperform across diverse scenarios. One reason is that most of them rely on shallow feature respresentations, while another is that deep-learning-based counterparts fail to make full use of the unique characteristics of low-light enhanced images (LEIs), such as luminance enhancement and color refinement. In this paper, we propose a novel Joint Luminance-Chrominance Learning Network (JLCLNet) for LEQA to comprehensively assess the effects of low-light image enhancement (LLIE) algorithms. Specifically, we construct a two-branch network architecture consisting of a luminance learning branch and a chrominance learning branch. In the luminance learning branch, the low- and high-frequency subbands of the luminance channel in the CIELAB color space, derived from the dual-tree complex wavelet transform (DTCWT), focus on measuring contrast enhancement and structure preservation. Meanwhile, the chrominance learning branch addresses potential color distortions by integrating perceptual information from the two parallel chrominance channels of the CIELAB color space. Finally, the complementary features from both branches are fused to predict quality scores. Experimental results on four public LEQA databases demonstrate the performance advantages of the proposed method compared to the state-of-the-art approaches. The source code of JLCLNet is available at https://github.com/li181119/JLCLNET .},
  archive      = {J_PR},
  author       = {Tuxin Guan and Qiuping Jiang and Xiongli Chai and Chaofeng Li},
  doi          = {10.1016/j.patcog.2025.112395},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112395},
  shortjournal = {Pattern Recognition},
  title        = {Joint luminance-chrominance learning for quality assessment of low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A graph contrastive learning network for change detection with heterogeneous remote sensing images. <em>PR</em>, <em>172</em>, 112394. (<a href='https://doi.org/10.1016/j.patcog.2025.112394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover change detection (LCCD) with heterogeneous remote sensing images (Hete-RSIs) is an attractive topic in the community of remote sensing applications. Intuitively, Hete-RSIs are acquired with different remote sensors, and they cannot be compared directly for LCCD because of the different imaging modalities. In this paper, a graph contrastive learning network (GCLN) is proposed for LCCD with bitemporal Hete-RSIs. First, with the motivation of smoothing the noise and utilizing contextual information, the k-nearest neighbor algorithm is used to improve the spectral homogeneity of the pixels within a superpixel. Then, a pairwise graph is constructed on the basis of each superpixel from spectral similarity and dissimilarity perspectives, and a graph feature learning network is designed to learn the near-far dependencies of graph features for change detection. Finally, the similarity and dissimilarity loss functions are coupled as a contrastive loss function to expand the difference between similar and dissimilar features. Comparisons with seven advanced methods on five pairs of Hete-RSIs demonstrate the feasibility and superiority of the proposed GCLN for LCCD with Hete-RSIs. For example, the improvements on the five datasets are 3.63 % , 8.47 % , 4.17 % , 8.23 % , and 4.98 % in terms of overall accuracy. The code of the proposed approach can be available at: https://github.com/ImgSciGroup/2024-GCLN .},
  archive      = {J_PR},
  author       = {Zhiyong Lv and Sizhe Cheng and Linfu Xie and Junhuai Li and Minghua Zhao},
  doi          = {10.1016/j.patcog.2025.112394},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112394},
  shortjournal = {Pattern Recognition},
  title        = {A graph contrastive learning network for change detection with heterogeneous remote sensing images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AM40: Enhancing action recognition through matting-driven interaction analysis. <em>PR</em>, <em>172</em>, 112393. (<a href='https://doi.org/10.1016/j.patcog.2025.112393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition models frequently face challenges from complex video backgrounds, where actors may blend into their surroundings and complicate motion analysis. Human interactions with action-related elements vary across scenarios, with backgrounds serving as both contextual cues and sources of interference. To address these issues, we introduce video matting techniques to separate foreground subjects from the background. This enables the model to focus on the subject of interest while suppressing irrelevant regions, thereby enhancing the extraction of interactions between the subject and associated objects. To support this methodology, we present ActionMatting40 ( AM40 ) dataset, which comprises 40 action categories annotated with alpha mattes to distinguish human actions and related objects from the background. Furthermore, we propose Matting-Driven Interaction Recognition (MIR), integrating an Action Background Decoupling (ABD) module to mitigate background interference and a Semantic-aware Feature Communication (SFC) module to selectively extract informative features for improved action recognition. Our code and dataset are publicly available at https://github.com/lwxfight/actionmatting .},
  archive      = {J_PR},
  author       = {Siqi Liang and Wenxuan Liu and Zhe Li and Kui Jiang and Siyuan Yang and Chia-Wen Lin and Xian Zhong},
  doi          = {10.1016/j.patcog.2025.112393},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112393},
  shortjournal = {Pattern Recognition},
  title        = {AM40: Enhancing action recognition through matting-driven interaction analysis},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Edge craft odyssey: Navigating guided super-resolution with a fast, precise, and lightweight network. <em>PR</em>, <em>172</em>, 112392. (<a href='https://doi.org/10.1016/j.patcog.2025.112392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal imaging technology is exceptionally valuable in environments where visibility is limited or nonexistent. However, the high cost and technological limitations of high-resolution thermal imaging sensors restrict their widespread use. Many thermal cameras are now paired with high-resolution visible cameras, which can help improve low-resolution thermal images. However, aligning thermal and visible images is challenging due to differences in their spectral ranges, making pixel-wise alignment difficult. Therefore, we present the Edge Craft Odyssey Network (ECONet), a lightweight transformer-based network designed for Guided Thermal Super-Resolution (GTSR) to address these challenges. Our approach introduces a Progressive Edge Prediction module that extracts edge features from visible images using an adaptive threshold within our innovative Edge-Weighted Gradient Blending technique. This technique provides precise control over the blending intensity between low-resolution thermal and visible images. Additionally, we introduce a lightweight Cascade Deep Feature Extractor that focuses on efficient feature extraction and edge weight highlighting, enhancing the representation of high-frequency details. Experimental results show that ECONet outperforms state-of-the-art methods across various datasets while maintaining a relatively low computational and memory requirements. ECONet improves performance by up to 0.20 to 1.3 dB over existing methods and generates super-resolved images in a fraction of a second, approximately 91 % faster than the other methods. The code is available at https://github.com/Rm1n90/ECONet .},
  archive      = {J_PR},
  author       = {Armin Mehri and Parichehr Behjati and Dario Carpio and Angel D. Sappa},
  doi          = {10.1016/j.patcog.2025.112392},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112392},
  shortjournal = {Pattern Recognition},
  title        = {Edge craft odyssey: Navigating guided super-resolution with a fast, precise, and lightweight network},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Jailbreak attack with multimodal virtual scenario hypnosis for vision-language models. <em>PR</em>, <em>172</em>, 112391. (<a href='https://doi.org/10.1016/j.patcog.2025.112391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent vulnerabilities of large Vision-Language Models (VLMs), security governance has emerged as a critical concern, particularly given the risks posed by noisy and biased training data as well as adversarial attacks, including data poisoning and prompt injection. These perturbations can significantly degrade model performance and introduce multifaceted societal risks. To verify the safe robustness of VLMs and further inspire the design of defensive AI frameworks, we propose Virtual Scenario Hypnosis (VSH), a multimodal prompt injection jailbreak method that embeds malicious queries into prompts through a deceptive narrative framework. This approach strategically distracts the model while compromising its resistance to jailbreak attempts. Our methodology features two key innovations: 1) Targeted adversarial image prompts that transform textual content into visual layouts through optimized typographic designs, circumventing safety alignment mechanisms to elicit harmful responses; and 2) An information veil encrypted In-Context Learning (ICL) method for text prompts that systematically evades safety detection protocols. To streamline evaluation, we employ Large Language Models (LLMs) to facilitate an efficient assessment of jailbreak success rates, supported by a meticulously designed prompt template incorporating multi-dimensional scoring rules and evaluation metrics. Extensive experiments demonstrate the efficacy of VSH, achieving an overall success rate exceeding 82% on 500 harmful queries spanning multiple domains when tested against LLaVA-v1.5-13B and GPT-4o mini.},
  archive      = {J_PR},
  author       = {Xiayang Shi and Shangfeng Chen and Gang Zhang and Wei Wei and Yinlin Li and Zhaoxin Fan and Jingjing Liu},
  doi          = {10.1016/j.patcog.2025.112391},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112391},
  shortjournal = {Pattern Recognition},
  title        = {Jailbreak attack with multimodal virtual scenario hypnosis for vision-language models},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A locality-sensitive hashing based instance selection method with its application to acceleration of feature selection. <em>PR</em>, <em>172</em>, 112390. (<a href='https://doi.org/10.1016/j.patcog.2025.112390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of important data preprocessing techniques, feature selection aims to remove redundant and irrelevant features and has been extensively applied to many fields. At present, however, the evaluation of existing feature selection algorithms focuses mainly on the scale of the selected features and the performance of models formulated by the selected features, while the running time of feature selection algorithms is usually neglected. It is noted that the computation complexity of the majority of feature selection algorithms is the square order of the number of instances, resulting in an exponential increase of the running time for large-scale data. In this paper, we propose an algorithm of core instance selection based on the locality-sensitive hashing (CISLSH) to improve the computation efficiency of feature selection algorithms by alleviating the instances used for feature selection. Specifically, all the instances are firstly considered to map them into the one-dimensional integer space using a locality-sensitive hashing (LSH) function. Given a set of hash functions families, a bucket index matrix is constructed to integrate all the mapping results of the set of hash functions families. Then, a voting mechanism is designed according to the bucket index matrix, which motivates to present a novel data partitioning method dividing similar instances into the same bucket (partition) as many as possible. Furthermore, the CISLSH algorithm is developed by selecting a core instance from each non-empty bucket. Finally, numerical experiments are conducted to assess the performance of CISLSH. The experimental results show that the execution of feature selection using the representative instances selected by CISLSH can not only significantly reduce the running time of feature selection but also guarantee the effectiveness of the selected features.},
  archive      = {J_PR},
  author       = {Fan Song and Xiao Zhang and Jinhai Li and Changlin Mei},
  doi          = {10.1016/j.patcog.2025.112390},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112390},
  shortjournal = {Pattern Recognition},
  title        = {A locality-sensitive hashing based instance selection method with its application to acceleration of feature selection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive latent disease state learning for multimodal alzheimer’s disease biomarker detection with missing modalities. <em>PR</em>, <em>172</em>, 112389. (<a href='https://doi.org/10.1016/j.patcog.2025.112389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal neuroimaging genetics is a crucial approach for identifying biomarkers of Alzheimer’s disease (AD) by leveraging the inherent relationships between genetic and neuroimaging data. However, existing methods are limited by susceptibility to input noises, underutilization of complementary information across neuroimaging modalities, and ineffective handling of samples with incomplete modalities. To address these challenges, we propose an Adaptive Latent Disease State Learning (ALDSL) method, which integrates noise reduction, latent space learning, adaptive regularization, and feature selection into a unified framework for detecting AD biomarkers from incomplete multimodal data. ALDSL introduces a noise reduction strategy based on inter-variable correlations and tailored distance metrics to eliminate noises in the input data, thereby obtaining high-quality representations for each modality. Additionally, latent disease state learning with adaptive regularization is proposed to capture inter-modality correlations by projecting the high-quality representations from multiple modalities into a common latent space. To utilize samples with incomplete modalities, we design a modality-specific weight matrix that accounts for the missing information in the latent disease state learning. Furthermore, an adaptive weighting determination strategy is developed to ensure that the modalities with different data types and varying sample sizes contribute on the same scale. We develop an efficient alternating optimization algorithm to solve the objective function of ALDSL. Experimental results on synthetic datasets and the ADNI GO/2 dataset demonstrate the effectiveness of ALDSL in detecting AD biomarkers.},
  archive      = {J_PR},
  author       = {Zhi Chen and Fengli Zhang and Yun Zhang and Jiajing Zhu and Qiaoqin Li and Yongguo Liu},
  doi          = {10.1016/j.patcog.2025.112389},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112389},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive latent disease state learning for multimodal alzheimer’s disease biomarker detection with missing modalities},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MMP: Enhancing unsupervised graph anomaly detection with multi-view message passing. <em>PR</em>, <em>172</em>, 112388. (<a href='https://doi.org/10.1016/j.patcog.2025.112388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complementary and conflicting relationships between views are two fundamental issues when applying Graph Neural Networks (GNNs) to multi-view attributed graph anomaly detection. Most existing approaches do not address the inherent multi-view properties in the attribute space or leverage complementary information through simple representation fusion, which overlooks the conflicting information among different views. In this paper, we argue that effectively applying GNNs to multi-view anomaly detection necessitates reinforcing complementary information between views and, more importantly, managing conflicting information. Building on this perspective, this paper introduces Multi-View Message Passing (MMP), a novel and effective message passing paradigm specifically designed for multi-view anomaly detection. In the multi-view aggregation phase of MMP, views containing different types of information are integrated using view-specific aggregation functions. This approach enables the model to dynamically adjust the amount of information aggregated from complementary and conflicting views, thereby mitigating issues arising from insufficient complementary information and excessive conflicting information, which can lead to suboptimal representation learning. Furthermore, we propose an innovative aggregation loss mechanism that enhances model performance by optimizing the reconstruction differences between aggregated representations and the original views, thereby improving both detection accuracy and model interpretability. Extensive experiments on synthetic and real-world datasets validate the effectiveness and robustness of our method. The source code is available at https://github.com/weihus/MMP .},
  archive      = {J_PR},
  author       = {Weihu Song and Lei Li and Mengxiao Zhu and Yue Pei and Haogang Zhu},
  doi          = {10.1016/j.patcog.2025.112388},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112388},
  shortjournal = {Pattern Recognition},
  title        = {MMP: Enhancing unsupervised graph anomaly detection with multi-view message passing},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HopGAT: A multi-hop graph attention network with heterophily and degree awareness. <em>PR</em>, <em>172</em>, 112387. (<a href='https://doi.org/10.1016/j.patcog.2025.112387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In highly heterophilic graphs, where nodes frequently connect across categories, the attention learning mechanism by dynamically adjusting neighboring node weights, may struggle to capture intricate node relationships. Furthermore, first-hop neighbor information is usually insufficient to encompass the global structure, but multi-hop increases complexity. To address these challenges, we propose HopGAT, a multi-hop graph attention network with heterophily and degree awareness. Firstly, we design heterophily-based neighbor sampling to sequentially filter high-hop neighbors by degree. Next, to obtain comprehensive global information, we construct a multi-hop recursive learning method with head and tail attention vectors to learn multi-hop neighbor features. Finally, we combine the average node degree of the graph with hop decay modeling to learn importance coefficients at different hops and adaptively aggregate the learned multi-hop features. Experimental results demonstrate that HopGAT significantly improves performance across 9 benchmark datasets with various heterophily and different average degrees.},
  archive      = {J_PR},
  author       = {Han Zhang and Huan Wang and Mingjing Han},
  doi          = {10.1016/j.patcog.2025.112387},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112387},
  shortjournal = {Pattern Recognition},
  title        = {HopGAT: A multi-hop graph attention network with heterophily and degree awareness},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A boundary-enhanced and target-driven deformable convolutional network for abdominal multi-organ segmentation. <em>PR</em>, <em>172</em>, 112386. (<a href='https://doi.org/10.1016/j.patcog.2025.112386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to accurately segment organs from abdominal CT images for clinical diagnosis, treatment planning, and surgical guidance, which remains an extremely challenging task due to low contrast between organs and surrounding tissues and the difference of organ size and shape. Previous works mainly focused on complex network architectures or task-specific modules but frequently failed to learn irregular boundaries and did not consider that different slices from the same case might contain targets of different numbers of categories. To tackle these issues, this paper proposes UAMSNet for abdominal multi-organ segmentation. In UAMSNet, a hybrid receptive field extraction (HRFE) module is introduced to adaptively learn the features of irregular targets, which has an adaptive dilation factor containing distance information to facilitate spatial and channel attention. The HRFE module can simultaneously learn multiple scales and deformations of different organs. Furthermore, a multi-organ boundary-enhanced attention (MBA) module in the encoder and decoder is designed to provide effective boundary information for feature extraction based on the large peak of the organ edge. Finally, the difference in the number of organ categories between different slices is first considered using a loss function, which can adjust the loss computation based on organ categories in the image. The loss function mitigates the effect of false positives during training to ensure the model can adapt to small organ segmentation. Experimental results on WORD and Synapse datasets demonstrate that our UAMSNet outperforms the existing state-of-the-art methods. Ablation experiments confirm the effectiveness of our designed modules and loss function. Our code is publicly available on https://github.com/HeyJGJu/UAMSNet .},
  archive      = {J_PR},
  author       = {Jianguo Ju and Menghao Liu and Wenhuan Song and Tongtong Zhang and Jindong Liu and Pengfei Xu and Ziyu Guan},
  doi          = {10.1016/j.patcog.2025.112386},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112386},
  shortjournal = {Pattern Recognition},
  title        = {A boundary-enhanced and target-driven deformable convolutional network for abdominal multi-organ segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Distantly supervised reinforcement localization for real-world object distribution estimation. <em>PR</em>, <em>172</em>, 112385. (<a href='https://doi.org/10.1016/j.patcog.2025.112385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the distribution of objects in the real world from monocular images is a challenging task due to the disparity between object distributions in perspective images and reality. Many researchers focus on predicting object distributions by converting perspective images into Bird’s-Eye View (BEV) images. In scenarios where camera parameter information is unavailable, the prediction of vanishing lines becomes critical for performing inverse perspective transformations. However, accurately predicting vanishing lines necessitates accounting for variations in object size, which cannot be effectively captured through simple regression models. Therefore, this paper proposes a size variation-aware method, utilizing expert knowledge from object detection to build a reinforcement learning framework for predicting vanishing lines in traffic scenes. Specifically, this method leverages size information from trained detectors to convert perspective images into BEV images without the need for additional camera intrinsic parameters. First, we design a novel reward mechanism that utilizes prior knowledge of scale differences between similar objects in perspective images, allowing the network to automatically update and learn specific vanishing line positions. Second, we propose a fast inverse perspective transformation method, which accelerates the training speed of the proposed approach. To evaluate the effectiveness of the method, experiments are conducted on two traffic flow datasets. The experimental results demonstrate that the proposed algorithm accurately predicts vanishing line positions and successfully transforms perspective images into BEV images. Furthermore, the proposed algorithm performs competitively with directly supervised methods. The code is available at: https://github.com/HotChieh/DDRL.},
  archive      = {J_PR},
  author       = {Haojie Guo and Junyu Gao and Yuan Yuan},
  doi          = {10.1016/j.patcog.2025.112385},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112385},
  shortjournal = {Pattern Recognition},
  title        = {Distantly supervised reinforcement localization for real-world object distribution estimation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A variable gaussian kernel scale active contour model based on jeffreys divergence for ICT image segmentation. <em>PR</em>, <em>172</em>, 112384. (<a href='https://doi.org/10.1016/j.patcog.2025.112384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial computed tomography (ICT), factors like beam scattering, insufficient beam intensity, and detector dark current often lead to weak edges, scattering artifacts, and severe Gaussian noise in ICT images. These issues pose significant difficulties for accurate segmentation of high-density complex structures using existing active contour models (ACMs). To address these limitations, this paper presents a variable Gaussian kernel scale active contour model based on Jeffreys divergence (VGJD). Firstly, the Jeffreys divergence (JD) is incorporated into the energy function to replace the conventional Euclidean distance, enhancing the contour’s ability to quantify pixel value disparity during evolution. Additionally, a filter weight is introduced to minimize the impact of noise. Moreover, a variable Gaussian kernel scale strategy is adopted to effectively integrate both global and local image information, thereby enhancing the robustness of the initial contour and improving the precision of detail segmentation. Finally, optimized length and regularity terms are employed to enforce constraints on the level set function. Extensive experimental results demonstrate that the VGJD model can effectively segment various complex ICT images, achieving superior precision in comparison to other ACM models. The code is available at https://github.com/LiuZX599/ACM-VGJD.git},
  archive      = {J_PR},
  author       = {Zexin Liu and Qi Li and Junyao Wang and Tingyuan Deng and Rifeng Zhou and Yufang Cai and Fenglin Liu},
  doi          = {10.1016/j.patcog.2025.112384},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112384},
  shortjournal = {Pattern Recognition},
  title        = {A variable gaussian kernel scale active contour model based on jeffreys divergence for ICT image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust spatio-temporal graph neural networks with sparse structure learning. <em>PR</em>, <em>172</em>, 112383. (<a href='https://doi.org/10.1016/j.patcog.2025.112383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of spatio-temporal graph classification by introducing sparse structure learning to enhance its robustness and explainability. Spatio-temporal graph neural networks (STGNN) integrate spatial structure and temporal sequential features into GNN learning, resulting in promising performance in many applications. However, current STGNN models often fail to capture the discriminative sparse substructure and the smooth distribution of these samples. To this end, this paper introduces RostGNN, robust spatio-temporal graph neural networks, for achieving more discriminative graph representations. Concretely, RostGNN extracts the spatial and temporal features by performing gated recurrent units on the given time series data and calculating adjacent matrixes for graphs. Then, we impose the iterative hard-thresholding approach on the final association matrix to obtain a sparse graph. Meanwhile, we calculate a similarity matrix from the side information of samples to smooth the achieved data representations and use fully connected networks for graph classification. We finally applied RostGNN to brain graph classification in experiments on real-world datasets. The results demonstrate that RostGNN delivers robust and discriminative graph representations and performs better than compared methods, benefiting from the sparsity and manifold regularizers. Furthermore, RostGNN can potentially yield useful findings for data understanding.},
  archive      = {J_PR},
  author       = {Yupei Zhang and Yuxin Li and Shuhui Liu and Xuequn Shang},
  doi          = {10.1016/j.patcog.2025.112383},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112383},
  shortjournal = {Pattern Recognition},
  title        = {Robust spatio-temporal graph neural networks with sparse structure learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiscDC: Unsupervised discriminative deep image clustering via confidence-driven self-labeling. <em>PR</em>, <em>172</em>, 112382. (<a href='https://doi.org/10.1016/j.patcog.2025.112382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering, as an important research topic in machine learning and data mining, has been widely applied in many real-world scenarios. However, existing deep clustering methods primarily rely on implicit optimization objectives such as contrastive learning or reconstruction, which do not explicitly enforce cluster-level discrimination. This limitation restricts their ability to achieve compact intra-cluster structures and distinct inter-cluster separations. To overcome this limitation, we propose a novel unsupervised discriminative deep clustering (discDC) method, which explicitly integrates cluster-level discrimination into the learning process. The proposed discDC framework projects data into a nonlinear latent space with compact and well-separated cluster representations. It explicitly optimizes clustering objectives by minimizing intra-cluster discrepancy and maximizing inter-cluster discrepancy. Additionally, to tackle the lack of label information in unsupervised scenarios, we introduce a confidence-driven self-labeling mechanism, which iteratively derives reliable pseudo-labels to enhance discriminative analysis. Extensive experiments on five benchmark datasets demonstrate the superiority of discDC over state-of-the-art deep clustering approaches.},
  archive      = {J_PR},
  author       = {Jinyu Cai and Wenzhong Guo and Yunhe Zhang and Jicong Fan},
  doi          = {10.1016/j.patcog.2025.112382},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112382},
  shortjournal = {Pattern Recognition},
  title        = {DiscDC: Unsupervised discriminative deep image clustering via confidence-driven self-labeling},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MCoCa: Towards fine-grained multimodal control in image captioning. <em>PR</em>, <em>172</em>, 112381. (<a href='https://doi.org/10.1016/j.patcog.2025.112381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable image captioning (CIC) models have traditionally focused on generating controlled descriptions using specific text styles. However, these approaches are limited as they rely solely on text control signals, which often fail to align with complex human intentions, such as selecting specific areas in images. To enhance multimodal interactivity, we propose to augment current CIC systems with diverse and joint visual-text controls. To achieve this, we first create a comprehensive Multimodal Controllable Image Captioning Corpus (MCoCa) dataset by leveraging language rewriting ability of GPT-3.5, containing 0.97M image-captions pairs along with 21 visual-text control signals. By training the visual and textual adapters equipped on the multimodal large language model with newly proposed instructional prompts on MCoCa, we observe emergent combinatory multimodal controllability and significant improvement in text controllability. We present exhaustive quantitative and qualitative results, benchmarking our trained model’s state-of-the-art zero-shot captioning performance on SentiCap and FlickrStyle10K in terms of both fidelity and controllability. For regional understanding ability of visual-controlled captioning, our method achieves obvious improvement compared with the baseline models.},
  archive      = {J_PR},
  author       = {Shanshan Zhao and Teng Wang and Jinrui Zhang and Xiangchen Wang and Feng Zheng},
  doi          = {10.1016/j.patcog.2025.112381},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112381},
  shortjournal = {Pattern Recognition},
  title        = {MCoCa: Towards fine-grained multimodal control in image captioning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning interpretable binary codes via semantic alignment for customized image retrieval. <em>PR</em>, <em>172</em>, 112380. (<a href='https://doi.org/10.1016/j.patcog.2025.112380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single modality hashing (SMH) has achieved impressive performance on image retrieval task in recent years. The only fly in the ointment is that most of the methods mainly measure the image similarity based on the high-level class labels. The retrieval needs in the real world are diverse in form of different subsets of the semantics (not only the category labels) presented in the query image. However, existing SMH methods fail to account for such customized image retrieval task that allows users to select visual semantics or their combinations present in the query and retrieve similar images based on such selected semantic descriptions. To address such practical issues, we propose a deep hashing to learn Interpretable Binary Codes (IBC), endowing the hashing bits with semantic interpretability rather than purely entangling the class information in the whole codes, i.e., aligning the criteria of binary space partition of each bit with a particular visual semantic concept. Specifically, binary encoding is a highly non-linear operation of dimension reduction, the semantic and spatial information of which has respectively been abstract and lost heavily. In light of the rich semantic interpretability and binary concept detection ability of convolutional filters, we innovatively transfer the semantic knowledge from filters to hashing bits by align the distributions of the binary codes and filter activations that capture the presence/absence of visual patterns in images. To further improve the semantics of filters/bits, the shared and learnable classification rules are introduced and optimized to disentangle the sparse composition between the category label and encoded semantics in filters/bits. With high interpretability, we can selectively combine bits corresponding to the target semantics during retrieval, thereby enabling flexible and customized similarity searches. Extensive experiments on several large-scale datasets covering general objects and scenes, single and multiple label scenarios, demonstrate the interpretability and functionalities of learned binary codes for the customized image retrieval tasks.},
  archive      = {J_PR},
  author       = {Shishi Qiao and Ruiping Wang and Xilin Chen},
  doi          = {10.1016/j.patcog.2025.112380},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112380},
  shortjournal = {Pattern Recognition},
  title        = {Learning interpretable binary codes via semantic alignment for customized image retrieval},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AFFusion: Atmospheric scattering enhancement and frequency integrated spatial-channel attention for infrared and visible image fusion. <em>PR</em>, <em>172</em>, 112379. (<a href='https://doi.org/10.1016/j.patcog.2025.112379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion (IVIF) seeks to generate fused images that combine rich texture details with distinct thermal radiation features by integrating and leveraging complementary information from multiple sources. However, existing fusion methods frequently neglect the challenges posed by illumination degradation and inaccurate color contrast, which arise due to light energy loss and light scattering during atmospheric transmission. To address these limitations, this study introduces an innovative IVIF framework, termed AFFusion, which integrates an atmospheric scattering physical model with a frequency-domain feature component. By accurately predicting and estimating two key physical parameters-the transmission map and atmospheric light-within the scattering model, AFFusion harnesses atmospheric scattering principles to produce enhanced visible images, thereby mitigating the adverse effects of energy attenuation and scattering. Furthermore, to resolve artifacts and texture loss caused by traditional atmospheric scattering models, AFFusion incorporates Fourier transform in conjunction with spatial and channel attention mechanisms to selectively amplify amplitude and phase features in the frequency domain, thereby enhancing texture fidelity and detail representation within the fused images. Comprehensive experimental evaluations demonstrate that AFFusion surpasses state-of-the-art methods in both qualitative and quantitative performance metrics, while also providing robust support for high-level visual tasks. The implementation code is publicly accessible at https://github.com/cici0206/AFFusion .},
  archive      = {J_PR},
  author       = {Jiwei Hu and Chengcheng Song and Qiwen Jin and Kin-Man Lam},
  doi          = {10.1016/j.patcog.2025.112379},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112379},
  shortjournal = {Pattern Recognition},
  title        = {AFFusion: Atmospheric scattering enhancement and frequency integrated spatial-channel attention for infrared and visible image fusion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Vision-by-prompt: Context-aware dual prompts for composed video retrieval. <em>PR</em>, <em>172</em>, 112378. (<a href='https://doi.org/10.1016/j.patcog.2025.112378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed video retrieval (CoVR) is a challenging task of retrieving relevant videos in a corpus by using a query that integrates both a relative change text and a reference video. Most existing CoVR models simply rely on the late-fusion strategy to combine visual and change text. Furthermore, various methods have been proposed to generate pseudo-word tokens from the reference video, which are then integrated into the relative change text for CoVR. However, these pseudo-word-based techniques exhibit limitations when the target video involves complex changes from the reference video, e.g. , object removal. In this work, we propose a novel CoVR framework that learns context information via context-aware dual prompts for relative change text to achieve effective composed video retrieval. The dual prompts cater to two aspects: 1) Global descriptive prompts generated from the pretrained V-L models, e.g. , BLIP-2, to get concise textual representations of the reference video. 2) Local target prompts to learn the target representations that the change text pays attention to. By connecting these prompts with relative change text, one can easily use existing text-to-video retrieval models to enhance CoVR performance. Our proposed framework can be flexibly used for both composed video retrieval (CoVR) and composed image retrieval (CoIR) tasks. Moreover, we take a pioneering approach by adopting the CoVR model to achieve zero-shot CoIR for remote sensing. Experiments on four datasets show that our approach achieves state-of-the-art performance in both CoVR and zero-shot CoIR tasks, with improvements of as high as around 3.5 % in terms of recall@K=1 score.},
  archive      = {J_PR},
  author       = {Hao Wang and Fang Liu and Licheng Jiao and Jiahao Wang and Shuo Li and Lingling Li and Puhua Chen and Xu Liu},
  doi          = {10.1016/j.patcog.2025.112378},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112378},
  shortjournal = {Pattern Recognition},
  title        = {Vision-by-prompt: Context-aware dual prompts for composed video retrieval},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient and compact tensor wheel decomposition for tensor completion. <em>PR</em>, <em>172</em>, 112377. (<a href='https://doi.org/10.1016/j.patcog.2025.112377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor wheel (TW) decomposition has recently emerged as a powerful technique for achieving state-of-the-art recovery performance in tensor completion tasks. However, its widespread application has been hindered by issues related to rank sensitivity and high computational cost. To address these limitations, we introduce an efficient and compact TW decomposition method for low-rank tensor completion. Specifically, we demonstrate that the model complexity of TW decomposition is controlled simultaneously by two elements, namely, the explicit TW rank and implicit sparsity in the core tensor. Therefore, low-rank and sparsity regularization are introduced to ring factors and core factor, respectively, to achieve a compact TW decomposition. Furthermore, to alleviate the computational bottleneck of TW decomposition, we propose a novel generalized inverse operation, which reduces the computational complexity of vanilla TW decomposition from O ( I N R 2 N ) to O ( I N R N ) . Subsequently, we develop an efficient alternating direction method of multipliers (ADMM) algorithm with theoretical convergence guarantees. Numerical tensor completion experiments on color images, multispectral images, and color videos demonstrate that the proposed method achieves superior performance while significantly reducing runtime compared to state-of-the-art methods. The code is available at: https://github.com/justicbro/TWLRS .},
  archive      = {J_PR},
  author       = {Peilin Yang and Yuning Qiu and Zhenhao Huang and Guoxu Zhou and Qibin Zhao},
  doi          = {10.1016/j.patcog.2025.112377},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112377},
  shortjournal = {Pattern Recognition},
  title        = {Efficient and compact tensor wheel decomposition for tensor completion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning to complement with multiple humans. <em>PR</em>, <em>172</em>, 112376. (<a href='https://doi.org/10.1016/j.patcog.2025.112376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solution for addressing real-world image classification challenges. Human-AI collaborative classification (HAI-CC) aims to synergise the efficiency of machine learning classifiers and the reliability of human experts to support decision making. Learning to defer (L2D) has been one of the promising HAI-CC approaches, where the system assesses a sample and decides to defer to one of human experts when it is not confident. Despite recent progress, existing L2D methods rely on the strong assumption of ground truth label availability for training, while in practice, most datasets often contain multiple noisy annotations per data sample without well-curated ground truth labels. In addition, current L2D methods either consider the setting of a single human expert or defer the decision to one human expert, even though there may be multiple experts available, resulting in a suboptimal utilisation of available resources. Furthermore, current HAI-CC evaluation frameworks often overlook processing costs, making it difficult to assess the trade-off between computational efficiency and performance when benchmarking different methods. To address these gaps, this paper introduces LECOMH – a new HAI-CC method that learns from noisy labels without depending on clean labels for training, simultaneously maximising collaborative accuracy with either one or multiple human experts, while minimising the cost of human collaboration. The paper also introduces benchmarks featuring multiple noisy labels per data sample for both training and testing to evaluate HAI-CC methods. Through quantitative comparisons on these benchmarks, LECOMH consistently outperforms HAI-CC methods and baselines, including human experts alone, multi-rater learning and noisy-label learning methods across both synthetic and real-world datasets.},
  archive      = {J_PR},
  author       = {Zheng Zhang and Cuong Nguyen and Kevin Wells and Thanh-Toan Do and Gustavo Carneiro},
  doi          = {10.1016/j.patcog.2025.112376},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112376},
  shortjournal = {Pattern Recognition},
  title        = {Learning to complement with multiple humans},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Noise-aware state-space method for underwater object detection. <em>PR</em>, <em>172</em>, 112375. (<a href='https://doi.org/10.1016/j.patcog.2025.112375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Object Detection (UOD) faces significant challenges due to complex degradation factors, such as color shifts caused by light absorption and scattering, spatially varying noise induced by plankton and sea snow, and motion blur resulting from dynamic water currents. Among existing methods, Convolutional Neural Networks (CNNs) are limited by fixed receptive fields, making it difficult to model long-range noise patterns; while Transformers excel at modeling global dependencies, they suffer from high computational complexity and weak capability in restoring fine-grained local features. Neither can effectively address the demands of detecting underwater-specific noise and small objects. To tackle these issues, we propose UOD-Mamba, a state space model (SSM)-based framework for underwater object detection. At its core is the Noise-Aware Dual-path Mamba (NADM) module, which integrates a global-local dual-path fusion strategy to enable both long-range noise modeling and local feature enhancement. The global path balances noise in input features through the Noise-Balanced Preprocessing Module (NBPM) and leverages Mamba’s long-range modeling capability to extract global noise patterns; the local path fuses the Underwater Enhanced Multi-scale Attention Module (UEMA) with CSP convolution to model edge and detail features at a fine-grained level, thereby compensating for the loss of local information. By explicitly learning the distribution characteristics of underwater noise and capturing the differences between noise and target features, the framework enhances detection robustness in noisy environments. Experimental validation on the DUO and RUOD datasets demonstrates that UOD-Mamba sets a new state-of-the-art in detection performance. It also exhibits advantages in explicit modeling of diverse noises, preservation of local details, and computational efficiency across multi-noise scenarios, enabling effective handling of complex underwater interference environments.},
  archive      = {J_PR},
  author       = {Jingchun Zhou and Xudong Wang and Mingjie Li and Zongxin He and Wentian Xin and Xiuguo Zhang},
  doi          = {10.1016/j.patcog.2025.112375},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112375},
  shortjournal = {Pattern Recognition},
  title        = {Noise-aware state-space method for underwater object detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-domain-aware deep unfolding transformer for hyperspectral image super-resolution. <em>PR</em>, <em>172</em>, 112374. (<a href='https://doi.org/10.1016/j.patcog.2025.112374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing low-spatial-resolution hyperspectral images with high-spatial-resolution (HSR) multispectral images is pivotal for generating HSR hyperspectral images (HSR-HSIs). While current deep unrolling-based multi-stage frameworks have shown notable advancements due to their robustness and interpretability, they still exhibit limitations in adequately harnessing the HSI prior knowledge. This deficiency is principally attributed to three factors: (1) prior knowledge learned from training samples often overlooks target-specific characteristics; (2) insufficient feature representation within and across stages; and (3) insufficient modeling of spatial–spectral dependencies. To address these issues, we propose a novel Cross-domain-aware Transformer (CaFormer). Specifically, a cross-domain aware attention mechanism is investigated to capture intrinsic joint spatial–spectral dependencies through unified cross-domain feature representation. The attention mechanism models HSI eigenfeatures to derive spatial and spectral representations while preserving their mutual correlations. Furthermore, we introduce a Fourier Domain Perception Block to enhance structural and semantic representations by exploiting amplitude and phase components in the frequency domain, thereby strengthening feature aggregation across stages. To further improve adaptability while preserving the interpretability of deep unrolling networks, CaFormer employs a dual-stage prior learning strategy, transferring prior knowledge learned from general training data to the specific observed scene. Our experimental evaluations on four public datasets and Worldview-2 satellite images confirm that our proposed method outperformed eleven state-of-the-art methods. The code is available at https://github.com/Caoxuheng/HIFtool .},
  archive      = {J_PR},
  author       = {Xuheng Cao and Xuquan Wang and Xiong Dun and Yusheng Lian and Xinbin Cheng and Xiaopeng Hao},
  doi          = {10.1016/j.patcog.2025.112374},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112374},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain-aware deep unfolding transformer for hyperspectral image super-resolution},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-teacher self-distillation registration for multi-modality medical image fusion. <em>PR</em>, <em>172</em>, 112373. (<a href='https://doi.org/10.1016/j.patcog.2025.112373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misaligned multimodal medical images pose challenges to the fusion task, resulting in structural distortions and edge artifacts in the fusion results. Existing registration networks primarily consider single-scale deformation fields at each stage, thereby neglecting long-range connections between non-adjacent stages. Moreover, in the fusion task, due to the quadratic computational complexity faced by Transformers during feature extraction, they are unable to effectively capture long-range correlated features. To address these problems, we propose an image registration and fusion method called DTMFusion. DTMFusion comprises two main networks: a Dual-Teacher Self-Distillation Registration (DTSDR) network and a Mamba-Conv-based Fusion (MCF) network. The registration network employs a pyramid progressive architecture to generate independent deformation fields at each layer. We introduce a dual-teacher self-distillation scheme that leverages past learning history and the current network structure as teacher guidance to constrain the generated deformation fields. For the fusion network, we introduced Mamba to address the quadratic complexity problem of Transformers. Specifically, the fusion network involves two key components: the Shallow Fusion Module (SFM) and the Cross-Modality Fusion Module (CFM). The SFM achieves lightweight cross-modality interaction through channel exchange, while the CFM leverages inherent cross-modality relationships to enhance the representation capability of fusion results. Through the collaborative effort of these components, the network can effectively integrate cross-modality complementary information and maintain appropriate apparent strength from a global perspective. Extensive experimental analysis demonstrates the superiority of this method in fusing misaligned medical images.},
  archive      = {J_PR},
  author       = {Aimei Dong and Jingyuan Xu and Long Wang},
  doi          = {10.1016/j.patcog.2025.112373},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112373},
  shortjournal = {Pattern Recognition},
  title        = {Dual-teacher self-distillation registration for multi-modality medical image fusion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FSF-net: Enhance 4D occupancy forecasting with coarse BEV scene flow for autonomous driving. <em>PR</em>, <em>172</em>, 112372. (<a href='https://doi.org/10.1016/j.patcog.2025.112372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {4D occupancy forecasting is one of the important techniques for autonomous driving, which can avoid potential risk in the complex traffic scenes. Scene flow is a crucial element to describe 4D occupancy map tendency. However, an accurate scene flow is difficult to predict in the real scene. In this paper, we find that BEV scene flow can approximately represent 3D scene flow in most traffic scenes. And coarse BEV scene flow is easy to generate. Under this thought, we propose 4D occupancy forecasting method FSF-Net based on coarse BEV scene flow. At first, we develop a general occupancy forecasting architecture based on coarse BEV scene flow. Then, to further enhance 4D occupancy feature representation ability, we propose a vector quantized based Mamba (VQ-Mamba) network to mine spatial-temporal structural scene feature. After that, to effectively fuse coarse occupancy maps forecasted from BEV scene flow and latent features, we design a U-Net based quality fusion (UQF) network to generate the fine-grained forecasting result. Extensive experiments are conducted on public Occ3D dataset. FSF-Net has achieved IoU and mIoU 9.56 % and 10.87 % higher than state-of-the-art method. Hence, we believe that proposed FSF-Net benefits to the safety of autonomous driving.},
  archive      = {J_PR},
  author       = {Erxin Guo and Pei An and You Yang and Qiong Liu and An-An Liu},
  doi          = {10.1016/j.patcog.2025.112372},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112372},
  shortjournal = {Pattern Recognition},
  title        = {FSF-net: Enhance 4D occupancy forecasting with coarse BEV scene flow for autonomous driving},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A novel image enhancement method based on image decomposition and deep neural networks. <em>PR</em>, <em>172</em>, 112371. (<a href='https://doi.org/10.1016/j.patcog.2025.112371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image decomposition and deep learning are active research areas in computer vision tasks, such as cartoon texture decomposition, low-light image enhancement, rain streak removal, image recovery, etc. This paper proposes a novel low-light image enhancement method by joining image decomposition and deep neural network techniques. We introduce a new image decomposition-based optimization model by incorporating the Tikhonov regularization and multi-scale convolutional sparse coding (MSCSC) to enhance image visual effects. To enhance robustness performance, we introduce a noise-free image decomposition error term to effectively suppress noise in low-light images. To effectively implement the proposed method, we incorporate a deep-unfolding neural network and an adaptive denoiser into the alternating direction method of multipliers (ADMM) framework. Since the deep unfolding network can effectively simulate the optimization algorithm process, the interpretability of the network model is increased. Moreover, through end-to-end training, we can automatically estimate the two priors and parameter settings from training samples. Finally, qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art image enhancement methods in terms of visual quality and robustness. The source code is available at https://github.com/cassiopeia-yxx/LLIE .},
  archive      = {J_PR},
  author       = {Yao Xiao and Youshen Xia},
  doi          = {10.1016/j.patcog.2025.112371},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112371},
  shortjournal = {Pattern Recognition},
  title        = {A novel image enhancement method based on image decomposition and deep neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nonuniform low-light image enhancement via noise-aware decomposition and adaptive correction. <em>PR</em>, <em>172</em>, 112370. (<a href='https://doi.org/10.1016/j.patcog.2025.112370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured under low-illumination conditions often exhibit low brightness with nonuniform distribution, low contrast, and noise, negatively affecting the human visual experience and the accuracy of image-based computer vision tasks. Enhancing nonuniform low-light images is challenging considering the requirement of simultaneously reducing noise, enhancing low-light regions, and suppressing high-light regions. To address these challenges, we innovatively propose a noise-aware decomposition and adaptive correction method (NDAC) to enhance the nonuniform low-light images without the need for paired high-quality training data. Specifically, a noise-aware image decomposition network (NIDNet) is first presented to decompose the input images into illumination, reflection, and noise components, while suppressing the noise in the reflection component through a variable gradient operator and estimating the noise component. Besides, we devise a novel nonlinear adaptive brightness mapping function (NABM), whose parameters are optimized via a designed automatic light enhancement network (ALENet) to brighten the illumination component. The enhancements are obtained by fusing the noiseless reflection component with the brightened illumination component. Extensive experiments on both public and industrial datasets demonstrate that the proposed NDAC method outperforms state-of-the-art approaches in both qualitative and quantitative evaluations.},
  archive      = {J_PR},
  author       = {Jiancai Huang and Zhaohui Jiang and Xingjian Liu and Yap-Peng Tan and Weihua Gui},
  doi          = {10.1016/j.patcog.2025.112370},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112370},
  shortjournal = {Pattern Recognition},
  title        = {Nonuniform low-light image enhancement via noise-aware decomposition and adaptive correction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fourier-enhanced semi-supervised proxy learning for ultra-fine-grained novel class discovery. <em>PR</em>, <em>172</em>, 112369. (<a href='https://doi.org/10.1016/j.patcog.2025.112369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operating in open-world environments requires recognizing known categories and discovering new ones, especially in ultra-fine-grained task, where distinguishing similar categories is challenging. The task of Ultra-Fine-Grained Novel Class Discovery (UFG-NCD) intensifies this challenge by requiring systems to identify previously unseen classes within unlabeled data. However, existing UFG-NCD methods fall short in extracting critical visual cues and efficiently transferring knowledge from known to novel categories. To overcome these limitations, this paper proposes Fourier-Enhanced Semi-supervised Proxy Learning (FESPL), a novel framework for UFG-NCD. FESPL incorporates a Fourier amplitude guided block that leverages frequency domain analysis to capture high-frequency details often missed by traditional approaches, enhancing ultra-fine-grained discrimination. Additionally, the semi-supervised proxy learning strategy maximizes information extraction from limited labeled data and promotes robust generalization across known and unseen categories. Our approach achieves substantial improvements in both novel category discovery and known category classification on seven popular UFG-NCD datasets, with average performance gains of 10.41 % in the accuracy of the old class and 4.27 % in the accuracy of the new class in task-agnostic evaluation, while with average performance gains of 4.40 % in clustering accuracy on the unlabeled training data in task-aware evaluation.},
  archive      = {J_PR},
  author       = {Qiupu Chen and Hongkui Jiang and Lin Jiao and Zhou Li and Taosheng Xu and Xue Wang and Rujing Wang},
  doi          = {10.1016/j.patcog.2025.112369},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112369},
  shortjournal = {Pattern Recognition},
  title        = {Fourier-enhanced semi-supervised proxy learning for ultra-fine-grained novel class discovery},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An adaptive weighted active contour based HRNet for underwater image segmentation. <em>PR</em>, <em>172</em>, 112368. (<a href='https://doi.org/10.1016/j.patcog.2025.112368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring optimal results in underwater environments remains challenging due to light absorption, scattering, and suspended particles. Furthermore, the low-resolution outputs from traditional semantic segmentation often result in spatial information loss and blurred segmentation boundaries. To address these issues, we first propose a low-level image enhancement preprocessing module as an independent preliminary stage to improve underwater image quality, thereby enhancing subsequent high-level semantic segmentation performance. Second, leveraging the region-based active contour model-which is independent of image gradients and adept at handling complex contour topology changes-we design a novel level set function to serve as the level set in the geometric active contour model. While this new level set exhibits formal similarity to classical level sets in representing binary segmentation contours, its formulation is derived from network prediction outputs. Third, we construct an adaptive weighted active contour energy function as a loss function within HRNet for multi-class segmentation. This loss function preserves geometric information while penalizing deviations between network-predicted probabilities and ground truth, effectively mitigating spatial information loss and optimizing boundary. Comparative experiments demonstrate that our model outperforms classical methods on objective metrics including mIoU and mPA.},
  archive      = {J_PR},
  author       = {Bo Chen and Jing Ji and Junwei Li and Xiaoli Sun and Feng Gong},
  doi          = {10.1016/j.patcog.2025.112368},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112368},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive weighted active contour based HRNet for underwater image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). BORT2: Bi-level optimization for robust target training in multi-source domain adaptation. <em>PR</em>, <em>172</em>, 112367. (<a href='https://doi.org/10.1016/j.patcog.2025.112367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both conventional and source-free multi-source domain adaptation (MSDA) tasks often face bias toward source domains, because more numerous labeled data in these domains, compared with a single unlabeled target domain, can dominate the training process. To alleviate the bias, target adaptation techniques train a target model on the pseudo-labeled target domain data only, with the source-domain-biased models used as the labeling function for pseudo-label generation. However, the pseudo labels may contain noise and harm performance when directly used for supervision. To tackle label noise, we introduce a novel Bi-level Optimization for Robust Target Training (BORT 2 ) scheme. BORT 2 trains a noise-robust target model on pseudo-labeled target data only and meanwhile updates the labeling function (i.e., the source-domain-biased models) to improve pseudo-label quality. Specifically, the target model is a stochastic network designed to be robust to label noise. Such a stochastic network exploits a Gaussian distribution to model the feature of each target instance and deploys an entropy maximization regularizer to the Gaussian to quantify the uncertainty of each pseudo-label, where the uncertainty is utilized to mitigate the negative effects of label noise. In addition, BORT 2 leverages the entropy to update the labeling function for better pseudo-label quality. Updating both the labeling function and the stochastic network involves a nested bi-level optimization problem, addressed using implicit differentiation. Extensive experiments demonstrate that BORT 2 achieves state-of-the-art performance for both conventional and source-free MSDA, as verified on Office-Home, Office-Caltech, PACS, Digit-Five, and the large-scale DomainNet datasets.},
  archive      = {J_PR},
  author       = {Zhongying Deng and Da Li and Xiaojiang Peng and Yi-Zhe Song and Tao Xiang},
  doi          = {10.1016/j.patcog.2025.112367},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112367},
  shortjournal = {Pattern Recognition},
  title        = {BORT2: Bi-level optimization for robust target training in multi-source domain adaptation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Ultra-efficient 3D shape reconstruction: Line-coded absolute phase unwrapping algorithm. <em>PR</em>, <em>172</em>, 112366. (<a href='https://doi.org/10.1016/j.patcog.2025.112366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absolute phase unwrapping-based fringe projection profilometry (APU-FPP) has the advantages of pixel-wise calculation, high precision, and full-field sensing of 3D shape information. To the best of our knowledge, existing APU-FPP methods have a general contradiction between accuracy and efficiency because of projecting extra auxiliary coded fringes (ACFs). In this paper, a line-coded absolute phase unwrapping (LCAPU) algorithm is presented for absolute 3D shape reconstruction of the scene with non-uniform reflectivity and complex surfaces. Firstly, a sequence of single-pixel lines is successively embedded into two sets of 3-step phase-shifting patterns to mark fringe periods, which can thoroughly avoid extra ACFs to disrupt the coherence of adjacent morphological information. Secondly, two line-coded phase-shifting patterns with the same phase shift are used to recognize the corresponding coded lines containing the fringe order cue, which can be simultaneously used to guide fringe mutual compensation, thereby extracting a high-quality phase. Finally, according to the pixel positions and the fringe indices of the decoded lines, a multi-layer decoding (MLD) algorithm is developed to iteratively generate a fringe order map, which can adapt to the randomness of morphological changes. Compared to other methods, the proposed LCAPU can not only perform a one-shot 3D shape reconstruction with a single image acquisition, but also automatically correct phase errors, balancing ultra-efficiency and high accuracy. Experimental results demonstrate the superior performance and the practical application potential in dynamic complex scenes.},
  archive      = {J_PR},
  author       = {Haihua An and Yiping Cao and Hechen Zhang},
  doi          = {10.1016/j.patcog.2025.112366},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112366},
  shortjournal = {Pattern Recognition},
  title        = {Ultra-efficient 3D shape reconstruction: Line-coded absolute phase unwrapping algorithm},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning label-specific features for multi-dimensional classification. <em>PR</em>, <em>172</em>, 112365. (<a href='https://doi.org/10.1016/j.patcog.2025.112365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-dimensional classification (MDC), instances are associated with multiple class variables that are assumed in the output space, and each class variable corresponds to one heterogeneous class space and characterizes the objects’ semantics from one dimension. Learning from MDC examples poses challenges due to the heterogeneity of class spaces, since the outputs from different class spaces are not directly comparable. Moreover, existing approaches often use identical data representation for all labels in a class, which may lead to suboptimal results as each label might be determined by its own specific characteristics. Critically, the inherent incomparability of raw heterogeneous labels prevents existing methods from effectively capturing label correlations, which are essential for guiding feature learning. In this paper, we propose a novel algorithm named LEAD, i.e., learning Label-spEcific feAtures for multi-Dimensional classification. LEAD first resolves label heterogeneity by transforming the original output space into a unified encoded label space through one-hot label encoding. This critical alignment enables explicit extraction of label correlations from the encoded space. To enhance the reliability of the estimation of label correlations, LEAD then leverages feature-space manifold structures via locally linear embedding, propagating labeling information across similar instances to counteract sparsity. Finally, LEAD jointly learns label-specific feature representations and constructs the classifier through sparse learning while incorporating label correlations. Experimental comparisons on fifteen datasets demonstrate that our proposed method outperforms state-of-the-art multi-dimensional classification methods. The code is available at https://github.com/ZhangZan-source/LEAD .},
  archive      = {J_PR},
  author       = {Zan Zhang and Jialin Zhou and Jialu Yao and Lin Liu and Jiuyong Li and Lei Li and Xindong Wu},
  doi          = {10.1016/j.patcog.2025.112365},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112365},
  shortjournal = {Pattern Recognition},
  title        = {Learning label-specific features for multi-dimensional classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TSAR: A two-stage approach to motion artifact reduction in OCTA images. <em>PR</em>, <em>172</em>, 112364. (<a href='https://doi.org/10.1016/j.patcog.2025.112364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Coherence Tomography Angiography (OCTA) is an innovative and non-invasive imaging technique that leverages motion contrast imaging to generate angiographic images from high-resolution volumetric blood flow data rapidly. However, OCTA imaging is vulnerable to various artifacts induced by eye movements, including displacement artifacts, duplicated scanning artifacts, and white line artifacts. Previous methods that attempted to mitigate eye motion artifacts necessitated costly hardware upgrades. However, despite the availability of advanced eye-tracking hardware and software correction in commercial machines, motion artifacts persist in real-world usage. Recently developed cost-effective learning-based methods only focus on the removal of white line artifacts while neglecting the displacement artifacts and duplicated scanning artifacts. To address this challenge, we propose a comprehensive framework, TSAR, to remove three types of eye motion artifacts in OCTA images. In the first stage, we leverage the intrinsic axial and directional attributes of these artifacts in the first phase to develop an innovative hierarchical transformer network. This network is designed to capture global-wise, local-wise, and vertical-wise features effectively while also removing displacement and duplicate scanning artifacts. Afterward, we leverage the contextual information and develop a residual conditional diffusion model (RCDM) to remove the white line artifacts. By applying our TSAR to the degraded OCTA images, we aim to eliminate all three types of motion artifacts. We evaluate the superior performance of our proposed methodology in artifact removal and image quality enhancement compared to other methods by conducting experiments on both synthetic and real-world OCTA images. The code is available at https://github.com/btma48/TSAR},
  archive      = {J_PR},
  author       = {Benteng Ma and Xiaomeng Li and Xu Lin and Xiaoyu Bai and Dongping Shao and Chubin Ou and Lin An and Jia Qin and Kwang-Ting Cheng},
  doi          = {10.1016/j.patcog.2025.112364},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112364},
  shortjournal = {Pattern Recognition},
  title        = {TSAR: A two-stage approach to motion artifact reduction in OCTA images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quaternionic reweighted amplitude flow for phase retrieval in image reconstruction. <em>PR</em>, <em>172</em>, 112363. (<a href='https://doi.org/10.1016/j.patcog.2025.112363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Ren Hu and Pan Lian},
  doi          = {10.1016/j.patcog.2025.112363},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112363},
  shortjournal = {Pattern Recognition},
  title        = {Quaternionic reweighted amplitude flow for phase retrieval in image reconstruction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust scene text understanding with OCR token and word alignment for text-VQA and text-caption. <em>PR</em>, <em>172</em>, 112362. (<a href='https://doi.org/10.1016/j.patcog.2025.112362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve vision-language tasks incorporating scene text, such as Text-VQA and Text-Caption, recognizing and understanding scene text within image is the first priority. However, the scene text recognized by Optical Character Recognition (OCR) systems often includes spelling errors, such as “pepsi” being recognized as “peosi”. These OCR errors are one of the major challenges for Text-VQA and Text-Caption systems. To address this, we propose a novel multi-modal OCR Token and Word Alignment (TWA) method to alleviate OCR errors in these tasks. First, we artificially create the misspelled OCR tokens and render them onto the RGB images, which can effectively simulates OCR errors. Second, we propose an OCR token-word contrastive learning task to pre-train OCR token representation, making the system more robust to OCR errors. Finally, we introduce a vocabulary predictor with character-level semantic matching, which enables the model to recover the correct word from the vocabulary even with misspelled OCR tokens. A variety of experimental evaluations demonstrate that our method outperforms the state-of-the-art methods on both Text-VQA and Text-Caption datasets.},
  archive      = {J_PR},
  author       = {Zan-Xia Jin and Pinle Qin and Suzhen Lin and Jia Qin and Shuangjiao Zhai and Jianchao Zeng and Xu-Cheng Yin},
  doi          = {10.1016/j.patcog.2025.112362},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112362},
  shortjournal = {Pattern Recognition},
  title        = {Robust scene text understanding with OCR token and word alignment for text-VQA and text-caption},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Prior tokenization-based interactive segmentation with vision transformers. <em>PR</em>, <em>172</em>, 112361. (<a href='https://doi.org/10.1016/j.patcog.2025.112361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively leveraging the provided priors is crucial for interactive segmentation. Existing approaches typically encode clicks via distance-based maps, which are then concatenated or added to the original image as network input. However, these methods do not fully exploit the semantic information embedded in the provided priors, leading to confusion in the feature distribution of different targets and reducing the segmentation quality. To address this issue, we propose a prior tokenization-based interactive segmentation method that uses simple Vision Transformers. By extending the original image tokens with prior tokens, each token represents the semantic features of the foreground and background related to the priors. These tokens participate in the self-attention operation alongside regular image tokens, gradually extracting semantic features from the image tokens to the prior tokens. In addition, we introduce a discriminative loss function to enforce inter-class separation and intra-class compactness of the prior tokens. Subsequently, we employ a cross-attention mechanism to couple the prior tokens with the regular image block token features, ensuring that the features extracted by the network are aligned with the user’s intent. Finally, we use the register method to suppress artifacts and enhance the segmentation performance further. Extensive experiments demonstrate that our method achieves superior interaction efficiency, robustness, and generalization ability across various medical image segmentation benchmarks. The source codes are available at https://github.com/dzyha2011/PT-SimpleClick},
  archive      = {J_PR},
  author       = {Zongyuan Ding and Boyu Wang and Hongyuan Wang and Tao Wang},
  doi          = {10.1016/j.patcog.2025.112361},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112361},
  shortjournal = {Pattern Recognition},
  title        = {Prior tokenization-based interactive segmentation with vision transformers},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CTNet: Color transformation network for low-light image enhancement. <em>PR</em>, <em>172</em>, 112360. (<a href='https://doi.org/10.1016/j.patcog.2025.112360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images are often plagued by low visibility, poor contrast, and high noise levels, which significantly impair both subjective visual quality and the performance of downstream tasks. Existing enhancement methods typically struggle with color-related degradations such as color casting, artifacts, and distortion. To address these challenges, we propose an end-to-end Color Transformation Network for low-light image enhancement, with a specific focus on improving color restoration. By leveraging the complementary strengths of the HSV and RGB color spaces in capturing color attributes, our approach enables effective interaction between these color spaces at the feature level. The HSV branch simultaneously enhances the V component while extracting features from the H and S components, thereby providing a more comprehensive set of cues for color recovery. To facilitate interaction, we design a learnable Color Transformation Block that bridges the HSV and RGB feature domains, effectively simulating the HSV-to-RGB conversion. Furthermore, a Cross-Integration Block, employing an attention-based cross-guidance mechanism, enables bi-directional information flow between the two color spaces. Extensive experiments on both real and synthetic datasets demonstrate that our method achieves superior performance, surpassing existing approaches both qualitatively and quantitatively. The project is available at https://github.com/1013990424/CTNet .},
  archive      = {J_PR},
  author       = {Lidong Xie and Runmin Cong and Ju Dai and Wenhan Yang and Junjun Pan and Hao Wu},
  doi          = {10.1016/j.patcog.2025.112360},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112360},
  shortjournal = {Pattern Recognition},
  title        = {CTNet: Color transformation network for low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Joint adversarial attack: An effective approach to evaluate robustness of 3D object tracking. <em>PR</em>, <em>172</em>, 112359. (<a href='https://doi.org/10.1016/j.patcog.2025.112359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have widely been used in 3D object tracking, thanks to its superior capabilities to learn from geometric training samples and locate tracking targets. Although the DNN based trackers show vulnerability to adversarial examples, their robustness in real-world scenarios with potentially complex data defects has rarely been studied. To this end, a joint adversarial attack method against 3D object tracking is proposed, which simulates defects of the point cloud data in the form of point filtration and perturbation simultaneously. Specifically, a voxel-based point filtration module is designed to filter points of the tracking template, which is described by the voxel-wise binary distribution regarding the density of the point cloud. Furthermore, a voxel-based point perturbation module adds voxel-wise perturbations to the filtered template, whose direction is constrained by local geometrical information of the template. Experiments conducted on popular 3D trackers demonstrate that the proposed joint attack have decreased the success and precision of existing 3D trackers by 30.2% and 35.4% respectively in average, which made an improvement of 30.5% over existing attack methods.},
  archive      = {J_PR},
  author       = {Riran Cheng and Xupeng Wang and Ferdous Sohel and Hang Lei},
  doi          = {10.1016/j.patcog.2025.112359},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112359},
  shortjournal = {Pattern Recognition},
  title        = {Joint adversarial attack: An effective approach to evaluate robustness of 3D object tracking},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-channel blur invariants of color and multispectral images. <em>PR</em>, <em>172</em>, 112358. (<a href='https://doi.org/10.1016/j.patcog.2025.112358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the recognition of blurred color/multispectral images directly without any deblurring. We present a general theory of invariants of multispectral images with respect to blur. The paper is a significant non-trivial extension of the recent theory of blur invariants of graylevel images. The main original contribution of the paper lies in introducing cross-channel blur invariants in Fourier domain. We also developed an algorithm for their stable and fast calculation in the moment domain. Moreover, the cross-channel invariants can be found for blurs for which single-channel invariants do not exist. The experiments on simulated and real data demonstrate that incorporating the new cross-channel invariants significantly improves the recognition power and surpasses other existing approaches. The outlook for a possible implementation of the blur invariants into neural networks is briefly sketched in the conclusion.},
  archive      = {J_PR},
  author       = {Václav Košík and Jan Flusser and Filip Šroubek},
  doi          = {10.1016/j.patcog.2025.112358},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112358},
  shortjournal = {Pattern Recognition},
  title        = {Cross-channel blur invariants of color and multispectral images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). UniForCE: The unimodality forest method for clustering and estimation of the number of clusters. <em>PR</em>, <em>172</em>, 112357. (<a href='https://doi.org/10.1016/j.patcog.2025.112357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of clusters k while clustering the data is a challenging task. An incorrect cluster assumption indicates that the number of clusters k gets wrongly estimated. Consequently, the model fitting becomes less important. In this work, we focus on the concept of unimodality and propose a flexible cluster definition called locally unimodal cluster . A locally unimodal cluster extends for as long as unimodality is locally preserved across pairs of subclusters of the data. Then, we propose the UniForCE method for locally unimodal clustering. The method starts with an initial overclustering of the data and relies on the unimodality graph that connects subclusters forming unimodal pairs. Such pairs are identified using an appropriate statistical test. UniForCE identifies maximal locally unimodal clusters that are statistically significant by computing a spanning forest in the unimodality graph. Experimental results on both real and synthetic datasets illustrate that the proposed methodology is particularly flexible and robust in discovering regular and highly complex cluster shapes. Most importantly, it automatically provides an adequate estimation of the number of clusters.},
  archive      = {J_PR},
  author       = {Georgios Vardakas and Argyris Kalogeratos and Aristidis Likas},
  doi          = {10.1016/j.patcog.2025.112357},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112357},
  shortjournal = {Pattern Recognition},
  title        = {UniForCE: The unimodality forest method for clustering and estimation of the number of clusters},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SequencePAR: Understanding pedestrian attributes via a sequence generation paradigm. <em>PR</em>, <em>172</em>, 112356. (<a href='https://doi.org/10.1016/j.patcog.2025.112356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current pedestrian attribute recognition (PAR) algorithms use multi-label or multi-task learning frameworks with specific classification heads. These models often struggle with imbalanced data and noisy samples. Inspired by the success of generative models, we propose Sequence Pedestrian Attribute Recognition (SequencePAR), a novel sequence generation paradigm for PAR. SequencePAR extracts pedestrian features using a language-image pre-trained model and embeds the attribute set into query tokens guided by text prompts. A Transformer decoder generates human attributes by integrating visual features and attribute query tokens. The masked multi-head attention layer in the decoder prevents the model from predicting the next attribute during training. The extensive experiments on multiple PAR datasets validate the effectiveness of SequencePAR. Specifically, we achieve 84.92 %, 90.44 %, 90.73 %, and 90.46 % in accuracy, precision, recall, and F1-score on the PETA dataset.},
  archive      = {J_PR},
  author       = {Jiandong Jin and Xiao Wang and Yin Lin and Chenglong Li and Lili Huang and Aihua Zheng and Jin Tang},
  doi          = {10.1016/j.patcog.2025.112356},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112356},
  shortjournal = {Pattern Recognition},
  title        = {SequencePAR: Understanding pedestrian attributes via a sequence generation paradigm},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SATE: Efficient knowledge distillation with implicit student-aware teacher ensembles. <em>PR</em>, <em>172</em>, 112355. (<a href='https://doi.org/10.1016/j.patcog.2025.112355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent findings suggest that with the same teacher architecture, a fully converged or “stronger” checkpoint surprisingly leads to a worse student. This can be explained by the Information Bottleneck (IB) principle, as the features of a weaker teacher transfer more “dark” knowledge because they maintain higher mutual information with the inputs. Meanwhile, various works have shown that severe teacher-student structural disparity or capability mismatch often leads to worse student performance. To deal with these issues, we propose a generalizable and efficient Knowledge Distillation (KD) framework with implicit Student-Aware Teacher Ensembles (SATE). The SATE framework simultaneously trains a student network and a student-aware intermediate teacher as a learning companion. With the proposed co-training strategy, the intermediate teacher is trained gradually and forms implicit ensembles of weaker teachers along the learning process. Such a design enables the student model to retain more dark knowledge for better generalization ability. The proposed framework improves the training scheme in a plug-and-play way so that it can be applied to improve various classic and state-of-the-art KD methods on both intra-domain (up to 2.184 % ) and cross-domain (up to 7.358 % ) settings, under a diversified configurations on teacher-student architectures, and achieves a major efficient advantage over other generic frameworks. The code is available at https://github.com/diqichen91/SATE.git .},
  archive      = {J_PR},
  author       = {Diqi Chen and Yang Li and Jiajun Liu and Jun Zhou and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2025.112355},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112355},
  shortjournal = {Pattern Recognition},
  title        = {SATE: Efficient knowledge distillation with implicit student-aware teacher ensembles},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Discriminative attention based weighted sparse representation of visual objects in complex scenarios. <em>PR</em>, <em>172</em>, 112354. (<a href='https://doi.org/10.1016/j.patcog.2025.112354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse subspace representation (SSR) is an attractive technique for subspace segmentation of high-dimensional data through a self-representation manner to reveal its algebraic structure. Numerous generalizations of SSR have been developed to meet different applications. However, a fatal limitation in those extensions is their neglect of feature weights in visual samples, which play crucial roles in segmenting or recognizing specific objects. This paper introduces a discriminative attention based weighted SSR model to tackle visual objects. In the proposed model, the prior information is empirically constructed for intra-cluster features and inter-cluster ones, aided by the sparse representation of samples. An attention mechanism is introduced to learn weights of features of samples. The attention based weights of objects in samples and sparse representation of samples are collaboratively learned from the prior information. A hard version and a soft one of attention based sparse subspace representation, abbreviated as HDAWSSR and SDAWSSR, are specified by assigning attention of features by a Boolean matrix and a fuzzy matrix. Algorithms for solving both models are meticulously developed, respectively. Applications of both algorithms in clustering and moving object detection within high-dimensional image data are investigated. Experimental results show that both models outperform the state-of-the-art subspace based segmentation methods.},
  archive      = {J_PR},
  author       = {Ge Yang and Tingquan Deng and Ming Yang and Changzhong Wang},
  doi          = {10.1016/j.patcog.2025.112354},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112354},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative attention based weighted sparse representation of visual objects in complex scenarios},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale feature sharing and collaborative sampling for unsupervised vehicle re-identification. <em>PR</em>, <em>172</em>, 112353. (<a href='https://doi.org/10.1016/j.patcog.2025.112353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Re-identification (Re-ID) retrieves target vehicle images from non-overlapping cameras. To address label noise in pseudo-labels, we propose the Multi-scale Feature Sharing and Collaborative Sampling (MFSCS) method. Specifically, the designed multi-scale feature sharing module moves beyond reliance on global features, efficiently promoting the exchange of characteristics between global and local aspects. This shared feature approach collectively mitigates the label noise arising from clustering. Recognizing that clustering methods are highly sensitive to outliers, we introduce a collaborative sampling module that cooperatively combines samples in the clustering process before training the model. This cooperative sampling module is better equipped to handle outliers in the samples and update label information more efficiently. As a result, it asymptotically improves the accuracy and stability of the model. The effectiveness of the proposed method in terms of performance is demonstrated through extensive experiments conducted on both the latest challenging truck Re-ID dataset, Truck-ID and VeRi-776.},
  archive      = {J_PR},
  author       = {Jia-Jia Li and Si-Bao Chen and Chris Ding and Bin Luo},
  doi          = {10.1016/j.patcog.2025.112353},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112353},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale feature sharing and collaborative sampling for unsupervised vehicle re-identification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). You look from old classes: Towards accurate few shot class-incremental learning. <em>PR</em>, <em>172</em>, 112352. (<a href='https://doi.org/10.1016/j.patcog.2025.112352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class incremental learning (FSCIL) is a common but difficult task that faces two challenges: catastrophic forgetting of old classes and insufficient learning of new classes with limited samples. Recent wisdom focuses on preventing catastrophic forgetting yet overlooks the limited samples issue, resulting in poor new class performance. In this paper, we argue that old class samples contain rich knowledge, which can be exploited to supplement the learning of new classes. To this end, we propose to Look from Old Classes (YLOC) for FSCIL, enhancing both the base and incremental sessions. In the base session, we develop a prototype centered loss (PCL) to obtain a compact distribution of old classes. During incremental sessions, we devise a prototype augmentation learning (PAL) method to aid the learning of new classes by exploiting old classes. Extensive experiments on three FSCIL benchmark datasets demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Yijie Hu and Kaizhu Huang and Wei Wang and Xiaowei Huang and Qiufeng Wang},
  doi          = {10.1016/j.patcog.2025.112352},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112352},
  shortjournal = {Pattern Recognition},
  title        = {You look from old classes: Towards accurate few shot class-incremental learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-decoder collaborative learning with multi-hybrid view augmentation for self-supervised 3D action recognition. <em>PR</em>, <em>172</em>, 112351. (<a href='https://doi.org/10.1016/j.patcog.2025.112351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised methods, including contrastive learning and masked skeleton modeling, have demonstrated considerable potential in the field of skeleton-based action recognition. While contrastive learning captures fine-grained details at the instance level, masked skeleton modeling emphasizes joint-level features. Recent studies have begun to combine these two approaches. However, existing combination methods primarily focus on integrating the tasks within the skeleton space. Moreover, existing contrastive learning methods often fail to exploit the comprehensive interaction information in skeletal structures, resulting in suboptimal performance when recognizing actions involving multiple individuals. To overcome these limitations, we introduce the Dual-Decoder Collaborative Learning (DDC) with Multi-Hybrid View Augmentation (MHGNA) method, which connects these two tasks across multiple spaces. Specifically, the masked skeleton modeling task provides diverse views for the contrastive learning task in the skeleton space, while the contrastive method aligns the features generated by both tasks within the feature space. We further present an innovative view augmentation method that enhances the model’s capacity to understand human interaction relationships by shuffling and replacing data across temporal, spatial, and personal dimensions. Extensive experiments on four downstream tasks across three large-scale datasets demonstrate that DDC exhibits stronger representational capabilities compared to state-of-the-art methods. Our code is available at https://github.com/Yingfei-Wu/DDC .},
  archive      = {J_PR},
  author       = {Wenming Cao and Yingfei Wu and Xinpeng Yin},
  doi          = {10.1016/j.patcog.2025.112351},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112351},
  shortjournal = {Pattern Recognition},
  title        = {Dual-decoder collaborative learning with multi-hybrid view augmentation for self-supervised 3D action recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). D3PD: Dual distillation and dynamic fusion for camera-radar 3D perception. <em>PR</em>, <em>172</em>, 112350. (<a href='https://doi.org/10.1016/j.patcog.2025.112350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving perception is driving rapid advancements in Bird’s-Eye-View (BEV) technology. The synergy of surround-view imagery and radar is seen as a cost-friendly approach that enhances the understanding of driving scenarios. However, current methods for fusing radar and camera features lack effective environmental perception guidance and dynamic adjustment capabilities, which restricts their performance in real-world scenarios. In this paper, we introduce the D3PD framework, which combines fusion techniques with knowledge distillation to tackle the dynamic guidance deficit in existing radar-camera fusion methods. Our method includes two key modules: Radar-Camera Feature Enhancement (RCFE) and Dual Distillation Knowledge Transfer. The RCFE module enhances the areas of interest in BEV, addressing the poor object perception performance of single-modal features. The Dual Distillation Knowledge Transfer includes four distinct modules: Camera Radar Sparse Distillation (CRSD) for sparse feature knowledge transfer and teacher-student network feature alignment. Position-guided Sampling Distillation(SamD) for refining the knowledge transfer of fused features through dynamic sampling. Detection Constraint Result Distillation (DcRD) for strengthening the positional correlation between teacher and student network outputs in forward propagation, achieving more precise detection perception. and Self-learning Mask Focused Distillation (SMFD) for focusing perception detection results on knowledge transfer through self-learning, concentrating on the reinforcement of local key areas. The D3PD framework outperforms existing methods on the nuScenes benchmark, achieving 49.6 % mAP and 59.2 % NDS performance. Moreover, in the occupancy prediction task, D3PD-Occ has achieved an advanced performance of 37.94 % mIoU. This provides insights for the design and model training of camera and radar-based 3D object detection and occupancy network prediction methods. The code will be available at https://github.com/no-Name128/D3PD .},
  archive      = {J_PR},
  author       = {Junyin Wang and Chenghu Du and Tongao Ge and Bingyi Liu and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2025.112350},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112350},
  shortjournal = {Pattern Recognition},
  title        = {D3PD: Dual distillation and dynamic fusion for camera-radar 3D perception},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TBiGAN-based parallel networks for remaining useful life prediction of multi-stage degraded bearings. <em>PR</em>, <em>172</em>, 112349. (<a href='https://doi.org/10.1016/j.patcog.2025.112349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of the remaining useful life (RUL) of rolling bearings is crucial for ensuring the safe and reliable operation of rotating machinery. However, existing methods generally overlook the correlation between different degradation stages and RUL, thereby limiting the accuracy of RUL prediction for rolling bearings. To address this challenge, a novel adaptive RUL prediction method for multi-stage degrading rolling bearings is proposed. Specifically, a new Transformer-based network is designed to classify the degradation stages of bearings. Additionally, a parallel RUL prediction model incorporating attention mechanisms is introduced, which integrates Temporal Convolutional Networks (TCN) and Bidirectional Gated Recurrent Units (BiGRU) to capture degradation features from multiple dimensions automatically and enhance the model’s ability to capture long-term dependencies in sequence tasks. Finally, the RUL prediction results from different stages are adaptively integrated using a smoothing technique to generate the final RUL. The accuracy and superiority of the proposed method are validated on the PHM2012 bearing dataset.},
  archive      = {J_PR},
  author       = {Zheng Jianfei and Chen Dongnan and Hu Changhua and Han Qihui and Pei Hong},
  doi          = {10.1016/j.patcog.2025.112349},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112349},
  shortjournal = {Pattern Recognition},
  title        = {TBiGAN-based parallel networks for remaining useful life prediction of multi-stage degraded bearings},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MChartQA and mChartQABench: A multimodal-only solution for complex chart question-answering. <em>PR</em>, <em>172</em>, 112348. (<a href='https://doi.org/10.1016/j.patcog.2025.112348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal chart question-answering (QA) is essential for applications such as financial report analysis, decision support, and invoice parsing. Current methods typically convert charts to text for processing by large language models (LLMs) or use direct multimodal processing. This raises an important question: under what conditions is a multimodal approach essential for chart question-answering? We observe that these traditional approaches often struggle with complex color patterns, structural intricacies, and implicit numerical data. Yet, limited research addresses these challenges. To bridge this gap, we introduce a new multimodal chart dataset, mChartQABench, constructed by consolidating data from existing open-source datasets to address challenges with color, structure, and textless chart data. To handle these complex multimodal scenarios effectively, we propose mChartQA, a framework integrating the advanced language processing of LLMs with a state-of-the-art table-to-text engine. This framework excels in aligning visual and textual data, enhancing deep reasoning and contextual understanding within charts. Experimental results show that mChartQA achieves superior performance across four datasets, with over 20 % overall accuracy improvement on mChartQABench.},
  archive      = {J_PR},
  author       = {Jingxuan Wei and Nan Xu and Guiyong Chang and Yin Luo and Bihui Yu and Ruifeng Guo},
  doi          = {10.1016/j.patcog.2025.112348},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112348},
  shortjournal = {Pattern Recognition},
  title        = {MChartQA and mChartQABench: A multimodal-only solution for complex chart question-answering},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Copula-based conformal prediction for prioritized heterogeneous multi-task learning. <em>PR</em>, <em>172</em>, 112347. (<a href='https://doi.org/10.1016/j.patcog.2025.112347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction (CP) has emerged as a standard for finite-sample and distribution-free uncertainty quantification (UQ). Although CP is widely used as a post-processing step (on the outputs of machine learning models) to produce reliable set-valued predictions, it is still challenging to post-process heterogeneous (i.e., categorical & numerical) predictions since the traditional CP procedures are either exclusively designed for classification or only tailored to regression. This article proposes the use of a simple yet novel copula-based CP method that jointly produces (discrete) set-valued predictions and (continuous) interval-valued predictions. This approach offers flexibility by allowing the prioritization of specific outputs’ reliability and applies to general heterogeneous multi-task problems. We demonstrate its effectiveness in the context of autonomous driving, on two popular multi-class object detection benchmarks, where it effectively infers set values for object classes and bounding boxes with the specified confidence levels. Experimental results validate our method’s ability in handling heterogeneous multi-task conformal predictions: we achieve high confidence levels without losing the informativeness of the prediction regions.},
  archive      = {J_PR},
  author       = {Bruce Cyusa Mukama and Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
  doi          = {10.1016/j.patcog.2025.112347},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112347},
  shortjournal = {Pattern Recognition},
  title        = {Copula-based conformal prediction for prioritized heterogeneous multi-task learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A deep spatio-temporal architecture for dynamic ECN analysis with granger causality based causal discovery. <em>PR</em>, <em>172</em>, 112346. (<a href='https://doi.org/10.1016/j.patcog.2025.112346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurobrain science provides the motivation for research on causal modeling. The existing causal discovery methods have shown promising results in effective connectivity network analysis, however, they often overlook the dynamics of causality, in addition to the incorporation of spatio-temporal information in data. Dynamic effective connectivity networks (dECNs) reveal the changing directed brain activity and the dynamic causal influences among brain regions, which facilitate the identification of individual differences and enhance the understanding of human brain. To learn dynamic causality, we propose a deep spatio-temporal fusion architecture, which employs a dynamic causal deep encoder to incorporate spatio-temporal information into dynamic causality modeling, and a dynamic causal deep decoder to verify the discovered causality. The effectiveness of the proposed method is first illustrated with simulated data. Then, experimental results from Philadelphia Neurodevelopmental Cohort (PNC) demonstrate the superiority of the proposed method in inferring dECNs, which reveal the dynamic evolution of directed flow between brain regions. The analysis shows the difference of dECNs between young adults and children. Specifically, the directed brain functional networks transit from fluctuating undifferentiated systems to more stable specialized networks as one grows. This observation provides further evidence on the modularization and adaptation of brain networks during development, leading to higher cognitive abilities observed in young adults.},
  archive      = {J_PR},
  author       = {Faming Xu and Yiding Wang and Gang Qu and Vince D. Calhoun and Julia M. Stephen and Tony W. Wilson and Yu-Ping Wang and Chen Qiao},
  doi          = {10.1016/j.patcog.2025.112346},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112346},
  shortjournal = {Pattern Recognition},
  title        = {A deep spatio-temporal architecture for dynamic ECN analysis with granger causality based causal discovery},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised domain adaptation via style-aware self-intermediate domain. <em>PR</em>, <em>172</em>, 112344. (<a href='https://doi.org/10.1016/j.patcog.2025.112344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) has garnered significant attention for its ability to transfer knowledge from a label-rich source domain to a related but unlabeled target domain, with minimizing inter-domain discrepancies being crucial, especially when a substantial gap exists between the domains. To address this, we introduce the novel Style-aware Self-Intermediate Domain (SSID), which effectively bridges large domain gaps by facilitating knowledge transfer while preserving class-discriminative information. Inspired by human transitive inference and learning capabilities, SSID connects seemingly unrelated concepts through a sequence of intermediate, auxiliary synthesized concepts. Meanwhile, an external memory bank is designed to store and update designated labeled features, ensuring the stability of class-specific and class-wise style features. Additionally, we also proposed a novel intra- and inter-domain loss functions that enhance class recognition and feature compatibility, with their convergence rigorously validated through a novel analytical approach. Comprehensive experiments demonstrate that SSID achieves accuracies of 85.4 % and 85.3 % on two widely recognized UDA benchmarks, outperforming the second-best methods by 0.94 % and 1.17 %, respectively. As a plug-and-play solution, SSID integrates seamlessly with various backbone networks, showcasing its effectiveness and versatility in domain adaptation scenarios.},
  archive      = {J_PR},
  author       = {Lianyu Wang and Meng Wang and Daoqiang Zhang and Huazhu Fu},
  doi          = {10.1016/j.patcog.2025.112344},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112344},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation via style-aware self-intermediate domain},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing visual representation of untrimmed videos by counteracting visuality threatening content. <em>PR</em>, <em>172</em>, 112343. (<a href='https://doi.org/10.1016/j.patcog.2025.112343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the remarkable growth of the video platform industry and the surge in video uploads, Content-Based Video Retrieval (CBVR), which finds videos on desired topics from a collection of untrimmed videos using solely the visual modality, is gaining increased attention. However, the challenge of accurate retrieval persists due to the varied and complex content in untrimmed videos, and there has been a lack of discussion on which types of content compromise visual representations. In this paper, we found that text and blur texture are of this nature, grounded in empirical observations. Indeed, in models focusing on the visual modality, both the visual structure of text (without semantics) and the smoothness of blur texture (with few edges and corners) interfere with decision-making. To address them, we propose two strategies: text-masking learning, which excludes the effect of text in the descriptor for inputs that may contain text content, and blur texture filtering, a re-scaling strategy that mitigates the impact of blur textures by exploiting the neural network’s insensitivity to the smoothed pixel-wise gradients. Furthermore, through empirical observations, we demonstrate that our proposed method effectively handles visuality-threatening content. Additionally, we show that our method can lead to state-of-the-art performance across multiple benchmarks of untrimmed videos.},
  archive      = {J_PR},
  author       = {Gwangjin Lee and Won Jo and Hyunwoo Kim and Yukyung Choi},
  doi          = {10.1016/j.patcog.2025.112343},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112343},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing visual representation of untrimmed videos by counteracting visuality threatening content},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Texture-aware transformer with pose-patch mapping for occluded person re-identification. <em>PR</em>, <em>172</em>, 112341. (<a href='https://doi.org/10.1016/j.patcog.2025.112341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (re-ID) aims to retrieve the target person from occluded images captured by different cameras, where the challenges lie in identity loss caused by different types of occlusion. To alleviate the occlusion interference, some methods rely on external clues or generate more occlusion samples. However, these methods fail to address the issues of pose misalignment under extreme occlusion and identity confusion caused by non-target pedestrian occlusion. To solve these problems, we design a novel T exture-Aware T ransformer with P ose-Patch M apping (TTPM), which does not require generating any occlusion samples. Specifically, a Multi-patch Feature Encoder is proposed to encode discriminative features from inter patches and intra patches. Afterwards, the Pose-Patch Mapping is designed to construct a positional mapping between poses and patches, which highlights human patches and weakens the impact of occluded patches. Finally, to mitigate the non-target pedestrian occlusion, a Texture-Aware Decoder is introduced to perceive texture features and leverage their distinctiveness to enhance the representation of important regions. Extensive experiments show that our method achieves state-of-the-art results on Occluded-Duke and Occluded-REID datasets.},
  archive      = {J_PR},
  author       = {Dengwen Wang and Guanyu Xing and Yanli Liu},
  doi          = {10.1016/j.patcog.2025.112341},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112341},
  shortjournal = {Pattern Recognition},
  title        = {Texture-aware transformer with pose-patch mapping for occluded person re-identification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An efficient community-aware pre-training method for graph neural networks. <em>PR</em>, <em>172</em>, 112340. (<a href='https://doi.org/10.1016/j.patcog.2025.112340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While graph neural networks (GNNs) have demonstrated widespread success in various domains, their pre-training techniques lag behind those in computer vision and natural language processing, typically exhibiting limited performance gains and high computational costs. This paper introduces Community-Aware Pre-training (CAP), a novel approach that leverages the inherent community structures prevalent in real-world networks to enhance GNN pre-training efficiency and effectiveness. CAP employs a self-supervised contrastive learning framework to learn node representations that are highly discriminative of their respective communities. To further optimize the pre-training process, we introduce a Monte Carlo Tree Search-based community sampler that efficiently extracts representative subgraphs, mitigating noise and enhancing sample quality. CAP is versatile and can be applied to a broad range of node classification tasks due to the commonly existing community structures within networks. Extensive evaluations on diverse node classification benchmarks demonstrate that CAP consistently outperforms state-of-the-art methods, achieving accuracy improvements of up to 4.34 % while significantly reducing pre-training time by up to 14.87 times compared to existing techniques. Furthermore, CAP enhances the predictive confidence and visualization distinctiveness of node representations, paving a new path for effective and efficient GNN pre-training.},
  archive      = {J_PR},
  author       = {Zhenhua Huang and Wenhao Zhou and Yihang Jiang and Zhaohong Jia and Linyuan Lü and Yunjie Ma},
  doi          = {10.1016/j.patcog.2025.112340},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112340},
  shortjournal = {Pattern Recognition},
  title        = {An efficient community-aware pre-training method for graph neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction. <em>PR</em>, <em>172</em>, 112339. (<a href='https://doi.org/10.1016/j.patcog.2025.112339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have made significant progress in trajectory prediction tasks but still face several critical challenges. The ordinary differential equation (ODE) solving methods used in standard diffusion models often suffer from error accumulation during multi-step iterations. Additionally, the denoising process is highly time-consuming due to the large number of computational steps, which significantly hinders inference efficiency and makes real-time applications challenging. To address these issues, we propose a diffusion-based method, DiffTrajectory, which integrates the Runge-Kutta (RK4) method, a Leap Initializer Module (LIM), and an Adaptive Dynamic Step-size Strategy (ADSS) to enhance generation accuracy and greatly optimize inference efficiency. Specifically, to tackle the problem of error accumulation, DiffTrajectory formalizes the denoising process as an ODE-solving problem and adopts the RK4 as a numerical solution. By computing multiple intermediate points at each iteration, this approach significantly reduces error accumulation. To improve the efficiency of the denoising process, DiffTrajectory introduces LIM, which leverages a pre-trained initial model to quickly generate a high-quality starting point for denoising, thereby reducing the computational burden during the initial denoising stages. Furthermore, we design the ADSS that adjusts the step size dynamically based on the results of each denoising stage, ensuring the quality of the generated results while substantially shortening inference time. Extensive experiments on the ETH/UCY and NBA datasets demonstrate that DiffTrajectory achieves substantial improvements in both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Chengcheng Li and Luqi Gong and Leiheng Xu and Xin Wang},
  doi          = {10.1016/j.patcog.2025.112339},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112339},
  shortjournal = {Pattern Recognition},
  title        = {DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A mondrian conformal predictive system with improved decision trees for uncertainty quantification under heteroscedasticity. <em>PR</em>, <em>172</em>, 112338. (<a href='https://doi.org/10.1016/j.patcog.2025.112338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing application of machine learning in industrial production, model uncertainty quantification has become a critical tool to evaluate the prediction reliability and guide decision-making. The Conformal Predictive System (CPS), which generates Cumulative Distribution Functions (CDFs), provides valuable support for uncertainty quantification. However, CPS faces limitations when addressing heteroskedasticity. This paper proposes a Mondrian Conformal Predictive System (LWT-MCPS) based on an enhanced Decision Tree. The proposed approach constructs decision trees using splitting criteria derived from Levene’s test and Welch’s t -test, ensuring that the variance and mean within each partition remain as homogeneous as possible. Furthermore, it incorporates predicted values and prediction variances estimated using the k -Nearest Neighbors (KNN) as splitting features, effectively mitigating the impact of high-dimensional data on tree partitioning and enhancing the model’s ability to identify heterogeneous regions. Experiments conducted on simulated data, public datasets, and blast furnace ironmaking data demonstrate that LWT-MCPS generates CDFs with lower Continuous Ranked Probability Scores (CRPS) than traditional CPS. These results validate its significant advantages in addressing heteroskedasticity challenges.},
  archive      = {J_PR},
  author       = {Ruiyao Zhang and Ping Zhou},
  doi          = {10.1016/j.patcog.2025.112338},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112338},
  shortjournal = {Pattern Recognition},
  title        = {A mondrian conformal predictive system with improved decision trees for uncertainty quantification under heteroscedasticity},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spatial multi-semantic features guided spectral-friendly transformer network for hyperspectral image classification. <em>PR</em>, <em>172</em>, 112337. (<a href='https://doi.org/10.1016/j.patcog.2025.112337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image classification (HSIC) is a foundational topic in remote sensing. However, the high correlations between bands and the spectral correlations often result in redundant data. Moreover, traditional convolutional neural networks (CNNs) compress spatial dimensions through pooling layers or strides during spatial information extraction, resulting in the loss of spatial information. To overcome these challenges, we propose a spatial multi-semantic features guided spectral-friendly Transformer network (SFTN), which effectively extracts the spectral and spatial features of HSIs. Specifically, a multi-semantic spatial attention (MsSA) module applies unidirectional spatial compression along the height and width dimensions. Thus, this module maintains spatial structure in one direction while aggregating global spatial information, thereby minimizing information loss during compression. It then employs multi-scale depth-shared 1D convolutions to capture multi-semantic spatial information. Furthermore, the spectral-friendly Transformer replaces the traditional multi-head self-attention (MHSA) with spectral correlation self-attention (ECSa), which effectively captures spectral differences and thus reduces the redundancy of spectral information. Extensive experiments on several HSI datasets show that the proposed SFTN method outperforms other state-of-the-art methods in HSIC applications. The source code for this work will be released later.},
  archive      = {J_PR},
  author       = {Xiaoyan Yu and Mingzhu Tai and Yuyang Wang and Zhenqiu Shu and Liehuang Zhu},
  doi          = {10.1016/j.patcog.2025.112337},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112337},
  shortjournal = {Pattern Recognition},
  title        = {Spatial multi-semantic features guided spectral-friendly transformer network for hyperspectral image classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Backdoor defense based on adversarial prediction proximity and contrastive knowledge distillation. <em>PR</em>, <em>172</em>, 112336. (<a href='https://doi.org/10.1016/j.patcog.2025.112336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become indispensable across various fields; however, their susceptibility to backdoor attacks poses significant security risks. In this paper, we propose a backdoor defense scheme based on adversarial prediction proximity and contrastive knowledge distillation. This scheme not only detects poisoned models and labels but also effectively unlearns backdoors while preserving the model’s benign functionality. Based on the observation that untargeted adversarial examples and poisoned samples exhibit proximity in feature space within poisoned models (i.e., adversarial prediction proximity), we first detect backdoors by analyzing changes in the prediction behavior of untargeted adversarial examples for models before and after fine-tuning. Next, we purify the poisoned model using a triplet loss that incorporates clean samples and untargeted adversarial examples. This process is guided by contrastive knowledge distillation, where a fine-tuned model acts as a “benign teacher”, and a backdoor-retained model serves as a “malicious teacher”, encouraging the poisoned model to align its feature representations with clean behavior. Comprehensive experimental results demonstrate that our scheme achieves high accuracy in detecting poisoned models and labels, even with limited access to clean samples. Furthermore, our scheme provides effective backdoor purification, while preserving the integrity and performance of models.},
  archive      = {J_PR},
  author       = {Lin Huang and Leo Yu Zhang and Ching-Chun Chang and Wei Wang and Chuan Qin},
  doi          = {10.1016/j.patcog.2025.112336},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112336},
  shortjournal = {Pattern Recognition},
  title        = {Backdoor defense based on adversarial prediction proximity and contrastive knowledge distillation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyperspectral space transformations for texture classification. <em>PR</em>, <em>172</em>, 112335. (<a href='https://doi.org/10.1016/j.patcog.2025.112335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D color space transformations are widely used in color imaging to enhance the results of various tasks, including image segmentation, object recognition, and texture classification. However, such useful transformations are much more limited in hyperspectral imaging, where images contain hundreds to thousands of spectral bands. To improve the performance of hyperspectral image analysis, we introduce four new hyperspectral space transformations in this paper: the Hyper-Chrominance-Luminance (H-CL), the Hyper-Hue-Chroma-Luminance (H-HCL), the Hyper-Hue-Saturation-Intensity (H-HSI), and the Hyper-Hue-Saturation-Value (H-HSV). These transformations extend the corresponding CL, HCL, HSI, and HSV 3D color spaces to multiple dimensions. To investigate their suitability in the context of texture classification, several well-known texture descriptors, including both theory-driven (handcrafted) and data-driven (deep learning) methods, are used in the experiments. Ten hyperspectral datasets are considered: HyTexila, SpecTex, HyperPlastic, and seven datasets extracted from the Timbers database. Among these datasets, six new ones are introduced in this paper. The proposed H-CL, H-HCL, H-HSI, and H-HSV transformations are also compared with state-of-the-art transformation strategies. The experiments conducted in this paper demonstrate the efficacy of the proposed space transformations with an accuracy improvement that can reach +43.47 %.},
  archive      = {J_PR},
  author       = {Alice Porebski and Souraya Ouaidar Hadir and Thierry Gensane and Nicolas Vandenbroucke},
  doi          = {10.1016/j.patcog.2025.112335},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112335},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral space transformations for texture classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-target federated backdoor attack based on feature aggregation. <em>PR</em>, <em>172</em>, 112333. (<a href='https://doi.org/10.1016/j.patcog.2025.112333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current federated backdoor attacks focus on collaboratively training backdoor triggers, where multiple compromised clients train their local trigger patches and then merge them into a global trigger during the inference phase. However, these methods require careful design of the shape and position of trigger patches and lack the feature interactions between trigger patches during training, resulting in poor backdoor attack success rates. Moreover, the pixels of the patches remain untruncated, thereby making abrupt areas in backdoor examples easily detectable by the detection algorithm. To this end, we propose a novel benchmark for the federated backdoor attack based on feature aggregation. Specifically, we align the dimensions of triggers with images, constrain the trigger’s pixel boundaries so that it is within a small range to avoid being detected, and aggregate trigger features from multiple compromised clients to enhance the global trigger’s ability to capture distributed data patterns. Furthermore, leveraging the intra-class attack strategy to train specific triggers for each class of samples, we propose the simultaneous generation of backdoor triggers for all target classes, significantly reducing the overall production time for triggers across all target classes and increasing the risk of the federated model being attacked. Experiments demonstrate that our method can not only bypass the detection of defense methods while patch-based methods fail, but also achieve a zero-shot backdoor attack with a success rate of 77.39 %. To the best of our knowledge, our work is the first to implement such a zero-shot attack in federated learning. Finally, we evaluate attack performance by varying the trigger’s training factors, including poison location, ratio, pixel bound, and trigger training duration (local epochs and communication rounds).},
  archive      = {J_PR},
  author       = {Lingguag Hao and Kuangrong Hao and Bing Wei and Xue-Song Tang},
  doi          = {10.1016/j.patcog.2025.112333},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112333},
  shortjournal = {Pattern Recognition},
  title        = {Multi-target federated backdoor attack based on feature aggregation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LayerMix: Enhanced data augmentation for robust deep learning. <em>PR</em>, <em>172</em>, 112332. (<a href='https://doi.org/10.1016/j.patcog.2025.112332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) models have demonstrated remarkable performance across various computer vision tasks, yet their vulnerability to distribution shifts remains a critical challenge. Despite sophisticated neural network architectures, existing models often struggle to maintain consistent performance when confronted with Out-of-Distribution (OOD) samples, including natural corruptions, adversarial perturbations, and anomalous patterns. We introduce LayerMix, an innovative Data Augmentation (DA) approach that systematically enhances model robustness through structured fractal-based image synthesis. By meticulously integrating structural complexity into training datasets, our method generates semantically consistent synthetic samples that significantly improve neural network generalization capabilities. Unlike traditional augmentation techniques that rely on random transformations, LayerMix employs a structured mixing pipeline that preserves original image semantics while introducing controlled variability. Extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K demonstrate LayerMix’s superior performance in classification accuracy and substantially enhances critical Machine Learning (ML) safety metrics, including resilience to natural image corruptions, robustness against adversarial attacks, improved model calibration and enhanced prediction consistency. LayerMix represents a significant advancement toward developing more reliable and adaptable artificial intelligence systems by addressing the fundamental challenges of DL generalization. The code is available at https://github.com/ahmadmughees/layermix .},
  archive      = {J_PR},
  author       = {Hafiz Mughees Ahmad and Dario Morle and Afshin Rahimi},
  doi          = {10.1016/j.patcog.2025.112332},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112332},
  shortjournal = {Pattern Recognition},
  title        = {LayerMix: Enhanced data augmentation for robust deep learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A query-driven twin network framework with optimization-based meta-learning for few-shot hyperspectral image classification. <em>PR</em>, <em>172</em>, 112331. (<a href='https://doi.org/10.1016/j.patcog.2025.112331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved remarkable results in hyperspectral image (HSI) classification due to its powerful deep feature extraction and nonlinear relationship processing capabilities. However, the success of deep learning methods is largely dependent on extensive labeled samples, which is both time-consuming and labor-intensive. To address this issue, a novel query-driven meta-learning twin network (QMTN) framework is proposed for HSI few-shot learning. QMTN uses two meta-learning channels, allowing for the comprehensive learning of meta-knowledge across diverse meta-tasks and enhancing learning efficiency. Within the QMTN framework, a lightweight spectral-spatial attention residual network is proposed for extraction of HSI features. The network incorporates a residual mechanism in both spectral and spatial feature extraction processes and includes an attention block to improve network performance by focusing on key locations in the spatial features. To maximize the use of the limited samples for constructing diverse meta-tasks, two meta-task generation approaches are employed, with and without simulated noise. Experiments on three public HSI datasets demonstrate that the QMTN framework effectively reduces the dependence on labeled samples in a single scene and significantly improves the classification performance and convergence of the internal network. The meta-task generation method with simulated noise can improve the classification performance of the QMTN.},
  archive      = {J_PR},
  author       = {Jian Zhu and Pengxin Wang and Jian Hui and Xin Ye},
  doi          = {10.1016/j.patcog.2025.112331},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112331},
  shortjournal = {Pattern Recognition},
  title        = {A query-driven twin network framework with optimization-based meta-learning for few-shot hyperspectral image classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reliable classification through rank-based conformal prediction sets. <em>PR</em>, <em>172</em>, 112330. (<a href='https://doi.org/10.1016/j.patcog.2025.112330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning classification tasks often benefit from predicting a set of possible labels with confidence scores to capture uncertainty. However, existing methods struggle with the high-dimensional nature of the data and the lack of well-calibrated probabilities from modern classification models. We propose a novel conformal prediction method that utilizes a rank-based score function suitable for classification models that predict the order of labels correctly, even if not well-calibrated. Our approach constructs prediction sets that achieve the desired coverage rate while managing their size. We provide a theoretical analysis of the expected size of the conformal prediction sets based on the rank distribution of the underlying classifier. Through extensive experiments, we demonstrate that our method outperforms existing techniques on various datasets, providing reliable uncertainty quantification. Our contributions include a novel conformal prediction method, theoretical analysis, and empirical evaluation. This work advances the practical deployment of machine learning systems by enabling reliable uncertainty quantification.},
  archive      = {J_PR},
  author       = {Rui Luo and Zhixin Zhou},
  doi          = {10.1016/j.patcog.2025.112330},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112330},
  shortjournal = {Pattern Recognition},
  title        = {Reliable classification through rank-based conformal prediction sets},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised domain adaptation for cardiac MRI segmentation via adversarial learning in latent space. <em>PR</em>, <em>172</em>, 112328. (<a href='https://doi.org/10.1016/j.patcog.2025.112328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is crucial for visualizing myocardial infarction (MI), with accurate segmentation of the ventricles and myocardium being essential for effective MI treatment. However, due to the complex myocardial structure and the limited availability of pixel-level annotations in LGE CMR images, accurate segmentation using supervised deep learning methods remains challenging. To address this, we propose an unsupervised domain adaptation framework for LGE CMR segmentation, utilizing CMR images from other modalities. First, we transform balanced Steady-State Free Precession (bSSFP) CMR images, which have abundant annotations, into LGE-like images using an enhanced CycleGAN. This CycleGAN incorporates an adversarial sample mining technique in the latent space to improve the quality of synthetic images. Next, we modify the nnU-Net architecture by introducing non-local blocks to train on these synthetic images, enabling precise segmentation of the myocardium and ventricular regions. We evaluate our method on the MS-CMRSeg 2019 dataset and MyoPS 2020 dataset, achieving an average Dice score of 88.0 % and 82.6 % respectively. Our experimental results demonstrate superior performance compared to state-of-the-art methods. The code for our approach is available at https://github.com/Lucarqi/Adv-CycleGAN .},
  archive      = {J_PR},
  author       = {Fan Zheng and Hengfei Cui and Yanning Zhang and Yong Xia},
  doi          = {10.1016/j.patcog.2025.112328},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112328},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation for cardiac MRI segmentation via adversarial learning in latent space},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficacy of varying sensing features for enhanced performance of deep-learning-informed multidimensional force platform. <em>PR</em>, <em>172</em>, 112327. (<a href='https://doi.org/10.1016/j.patcog.2025.112327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-informed vision-based 3D force platforms have demonstrated significant potential for simultaneous assessment of pressure and shear stresses. However, the enhancement of force decoupling capacity is widely recognized as a difficult challenge in the field. For vision-based designs, the marker-embedded sensing layer serves as the pivotal element of the force platform, unveiling diverse sensing characteristics throughout the learning process. However, none of the previous studies have thoroughly investigated the differences among these sensing features and leveraged them to optimize DL models for enhanced performance in multidimensional force detection. This study addresses this gap by systematically evaluating five distinct features (including optical flow, original images, and their derivatives) using four classic CNN architectures. Our comparative analysis reveals a clear feature-force specialization: gray images are most effective for pressure decoupling, while arrow images are superior in decoupling shear stress. Based on this finding, we proposed and validated a dual-branch DL model that fuses these two specialized features. The model achieves a strong, comprehensive performance on both tasks simultaneously, demonstrating the efficacy of our evidence-based feature-fusion strategy. This study provides new insights into sensing feature selection and evidence-based neural network design for vision-based multidimensional force platforms. These advancements have the potential to expedite the deployment of high-performance multidimensional force platforms in real-life applications.},
  archive      = {J_PR},
  author       = {Hu Luo and Yuxin Ma and Zesheng Wang and Jiewen Li and Xin Ma and Wen-Ming Chen},
  doi          = {10.1016/j.patcog.2025.112327},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112327},
  shortjournal = {Pattern Recognition},
  title        = {Efficacy of varying sensing features for enhanced performance of deep-learning-informed multidimensional force platform},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TP-LReID: Lifelong person re-identification using text prompts. <em>PR</em>, <em>172</em>, 112326. (<a href='https://doi.org/10.1016/j.patcog.2025.112326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong person re-identification (LReID) aims to develop a single model that is capable of continuously learning from new domain (present) while retaining knowledge from previously encountered ones (past) and generalizing to unseen domains (future). However, distribution shifts across these domains pose a significant challenge in maintaining performance across past, present, and future domains, that is, causing the catastrophic forgetting on previously seen domains and limited generalization to unseen ones. To address the above issues, we propose to guide consistent feature extraction to bridge distribution shifts using text prompts designed to remain invariant across domains. First, identity-consistent text prompts capturing high-level image semantics are extracted and aligned with image features throughout the lifelong learning pipeline. Moreover, to enhance generalization to unseen domains, we introduce an adversarial training that text features are contrastively aligned with both original and future-style image features, the latter generated by applying gradient-based perturbations in the feature space. Compared with 21 representative models on 11 benchmark datasets, our proposed model, trained without access to historical data, achieves performance comparable to the model trained using a joint training approach, and it performs well on all of the past, present, and future domains. We further explored the forgetting of the first historical domain and the generalization to all unseen domains under all 24 orders, and the results confirmed the superiority of our model. Codes will be released if this paper is accepted.},
  archive      = {J_PR},
  author       = {Zhaoshuo Liu and Zhiwei Guo and Chaolu Feng and Wei Li and Kun Yu and Jun Hu and Jinzhu Yang},
  doi          = {10.1016/j.patcog.2025.112326},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112326},
  shortjournal = {Pattern Recognition},
  title        = {TP-LReID: Lifelong person re-identification using text prompts},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Zoom-shot: Fast, efficient and unsupervised zero-shot knowledge transfer from CLIP to vision encoders. <em>PR</em>, <em>172</em>, 112323. (<a href='https://doi.org/10.1016/j.patcog.2025.112323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models like CLIP demonstrate exceptional capabilities over a broad domain of knowledge, such as with zero-shot classification; however, they also require significant computational resources, narrowing their real-world utility. Recent studies have shown that mapping features from pre-trained vision encoders into CLIP’s latent space can transfer some of CLIP’s abilities to smaller vision encoders, offering a promising alternative. Yet, the performance of these vision encoders still falls short of CLIP’s native capabilities, particularly in low-data regimes. In this work, we argue that enhancing training data coverage/diversity significantly improves mapping efficacy. We achieve this using tailored loss functions rather than relying on data augmentation or increasing training samples. For instance, we exploit the inherent multimodal nature of CLIP’s latent space, by incorporating cycle-consistency loss as one of our loss functions. Moreover, the mapping is learned using entirely unlabelled and unpaired data, eliminating the need for manual labelling or data pairing in novel domains. From these findings, our resulting method (Zoom-shot) offers a viable path to flexible zero-shot models for resource-limited, data-scarce settings. We test Zoom-shot’s zero-shot performance across various pre-trained vision encoders on coarse- and fine-grained datasets and achieve superior performance compared to recent works. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; allowing for a significant reduction in required training data. All code and models are available on GitHub.},
  archive      = {J_PR},
  author       = {Jordan Shipard and Arnold Wiliem and Kien Nguyen Thanh and Wei Xiang and Clinton Fookes},
  doi          = {10.1016/j.patcog.2025.112323},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112323},
  shortjournal = {Pattern Recognition},
  title        = {Zoom-shot: Fast, efficient and unsupervised zero-shot knowledge transfer from CLIP to vision encoders},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging. <em>PR</em>, <em>172</em>, 112322. (<a href='https://doi.org/10.1016/j.patcog.2025.112322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-pixel imaging provides significant advantages for non-visible wavelength detection and ultra-compressed sensing. However, accurate reconstruction from severely under-sampled measurements remains challenging. To tackle this, we propose a novel wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging (WGDNet), which hierarchically reconstructs images through wavelet component-aware reinforcement. Specifically, we design a sampling-guided model to capture essential textures and produce an initial image decomposed into high- and low-frequency components. The low-frequency part is enhanced with adaptive diffusion to preserve structure, while the high-frequency part is directionally incorporated through a multi-frequency adaptive fusion attention (MAFA) mechanism to refine details. Building on this, we develop a residual spatial adaptive fusion (RSAF) module to effectively combine low-frequency structures and high-frequency details. Extensive experiments on five public datasets demonstrate that our method achieves superior performance in both structural preservation and detail recovery. Successful implementation in the imaging system validates the applicability in real scenarios.},
  archive      = {J_PR},
  author       = {Dawei Song and Qiurong Yan and Hui Wang and Jian Yang and Xiaolong Luo},
  doi          = {10.1016/j.patcog.2025.112322},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112322},
  shortjournal = {Pattern Recognition},
  title        = {Wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hierarchical community-based graph generation model for improving structural diversity. <em>PR</em>, <em>172</em>, 112320. (<a href='https://doi.org/10.1016/j.patcog.2025.112320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph generation remains a challenging task due to the high dimensionality of graphs and the complex dependencies among their edges. Existing models often struggle to produce structurally diverse graphs. To address this limitation, we propose a novel generative framework specifically designed to capture structural diversity in graph generation. Our approach follows a sequential process: initially, a community detection algorithm partitions the input graph into distinct communities. Each community is then generated independently using deep generative models, while a dedicated module concurrently learns the interconnections between communities. To scale to graphs with a larger number of communities, we extend our approach into a hierarchical generative model. The proposed framework not only improves generation accuracy but also significantly reduces generation time for large-scale graphs. Moreover, it enables the application of prior methods that were previously incapable of handling such graphs. To highlight the shortcomings of existing approaches, we conduct experiments on a synthetic dataset comprising diverse graph structures. The results demonstrate substantial improvements in standard evaluation metrics as well as in the quality of the generated graphs.},
  archive      = {J_PR},
  author       = {Masoomeh Sadat Razavi and Abdolreza Mirzaei and Mehran Safayani},
  doi          = {10.1016/j.patcog.2025.112320},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112320},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical community-based graph generation model for improving structural diversity},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GLGF-CR: A gated local-global fusion approach for cloud removal in real-world remote sensing. <em>PR</em>, <em>172</em>, 112319. (<a href='https://doi.org/10.1016/j.patcog.2025.112319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical satellite imagery is a critical data source for Earth observation in remote sensing. However, cloud cover often degrades image quality, hindering its application and analysis. Therefore, effective cloud removal from optical satellite images has become a prominent research direction. In real-world scenarios, thick clouds act as pure noise, completely obscuring underlying information, while thin clouds provide partially beneficial information that can be leveraged for reconstruction. Traditional cloud removal methods often fail to distinguish between these two types of noise, leading to suboptimal performance. To address this limitation, we propose a novel cloud removal model, GLGF-CR, which incorporates a Gated Local-Global Fusion module. This module is designed to effectively separate and process the distinct characteristics of thick and thin clouds. For thick clouds, which contain no recoverable information, the model focuses on robust reconstruction using complementary data sources. For thin clouds, the model extracts and utilizes the beneficial information embedded in the partially obscured regions, enabling more accurate and detailed reconstruction. Additionally, a Dual Cross-Attention mechanism is introduced to establish robust mappings between SAR and optical modalities, further improving fusion accuracy. To handle domain shifts between source and target domains, we incorporate a domain adaptation module, which enhances the model’s ability to generalize across diverse real-world scenarios. The proposed algorithm not only outperforms existing methods on the large-scale real-world dataset SEN12MS-CR but also demonstrates strong cross-domain transferability on the Henan flood dataset. By explicitly addressing the dual nature of cloud noise–pure noise in thick clouds and partially beneficial information in thin clouds–this work advances the field of beneficial noise learning, demonstrating how noise can be systematically analyzed and utilized to improve model performance in complex scenarios.},
  archive      = {J_PR},
  author       = {Ganchao Liu and Jiawei Qiu and Jincheng Huang and Yuan Yuan},
  doi          = {10.1016/j.patcog.2025.112319},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112319},
  shortjournal = {Pattern Recognition},
  title        = {GLGF-CR: A gated local-global fusion approach for cloud removal in real-world remote sensing},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Semi-supervised feature selection with concept factorization and robust label learning. <em>PR</em>, <em>172</em>, 112317. (<a href='https://doi.org/10.1016/j.patcog.2025.112317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is essential for improving model performance in high-dimensional data by identifying the most relevant features. Concept Factorization (CF), building on Non-negative Matrix Factorization (NMF), is valued for revealing meaningful data structure and producing interpretable concept vectors. However, existing CF-based FS methods are typically unsupervised and do not leverage label information, leading to a bias toward high-variance features. This bias can result in the omission of low-variance features that may be highly discriminative, ultimately reducing the effectiveness of FS and compromising model performance, especially in tasks where subtle or rare patterns are important. To address these limitations, this paper proposes SCFLR, a novel semi-supervised FS method that combines CF with robust label learning. SCFLR establishes the CF framework based on the feature space by expressing each concept vector as a conic combination of the feature vectors, thereby leveraging both the underlying data structure and available label information to select a more informative and balanced set of features. To this end, SCFLR defines a linear regression-based loss function derived from the generated concept vectors to leverage information from labeled data. This loss function is further enhanced through a label learning framework based on the L 2 , 1 -norm to ensure a robust label approximation. SCFLR also utilizes the dual-graph regularization to maintain the local geometric structures in both feature and data spaces. In order to tackle the optimization problem of SCFLR, an efficient algorithm, with proof of its convergence, is introduced. Finally, the experimental validation of the SCFLR method on multiple datasets highlights its effectiveness and superior performance compared to other FS methods.},
  archive      = {J_PR},
  author       = {Razieh Sheikhpour and Farid Saberi-Movahed and Mahdi Jalili and Kamal Berahmand},
  doi          = {10.1016/j.patcog.2025.112317},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112317},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised feature selection with concept factorization and robust label learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learning for DBT classification with saliency-guided 2D synthesis. <em>PR</em>, <em>172</em>, 112316. (<a href='https://doi.org/10.1016/j.patcog.2025.112316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Breast Tomosynthesis (DBT) is a key imaging modality for breast cancer detection, improving lesion visibility by reducing tissue overlap inherent in conventional mammography. In this work, we propose a novel deep learning framework that classifies DBT volumes as malignant or non-malignant, while simultaneously generating a synthetic 2D image to assist diagnostic interpretation. This image is derived from a 3D saliency map computed by the internal attention mechanisms of the model, which highlights and preserves the most diagnostically relevant regions from the original volume. A surface is defined in this saliency space, enabling sampling and projection into a 2D diagnostic representation. This projection offers a compact summary of the volumetric scan, assisting clinicians in diagnostic interpretation and potentially alleviating the cognitive workload. A standard convolutional neural network trained on these synthetic 2D images achieves classification performance comparable to models operating directly on full 3D volumes. We train and evaluate our method on the OPTIMAM dataset and assess generalization through external validation on the independent BCS-DBT dataset without retraining. Results show that the model performs robustly across different clinical sources and provides an interpretable, computationally efficient tool for DBT-based breast cancer diagnosis.},
  archive      = {J_PR},
  author       = {Marco Cantone and Ciro Russo and Federico~V.~L. Dell’Ascenza and Claudio Marrocco and Alessandro Bria},
  doi          = {10.1016/j.patcog.2025.112316},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112316},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning for DBT classification with saliency-guided 2D synthesis},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Noise-tolerant scheme and explicit regularizer for deep active learning with noisy oracles. <em>PR</em>, <em>172</em>, 112313. (<a href='https://doi.org/10.1016/j.patcog.2025.112313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the query strategies based on deep learning shows promising results in terms of designing the criteria for active learning. However, the labels provided by the oracles might be noisy (inaccurate) due to similarities across several classes causing ambiguity, leading to unreliable results. To address this issue, we propose a noise-tolerant deep active learning method. Specifically, we design a consistency regularization for deep attention network as explicit regularizer, which is used to measure the uncertainty of examples. Besides, we develop the robust model for dealing with the noisy oracles , which first take the associations that make from embeddings of labeled data to those of unlabeled data and back, then we employ the association probability as a weighting fusion schema into angular margin based loss. Moreover, we design the submodular maximization function for reducing the redundancy of selected batch examples. Finally, the formulation is encapsulated into the multi-task framework that helps to adaptive learning towards more generalizable performance. Experimentally, we conduct extensive experiments on classification and segmentation tasks, and the results clearly demonstrate the superiority of the proposed method to the existing state-of-the-art deep active learning approaches.},
  archive      = {J_PR},
  author       = {Yanchao Li and Ziteng Xie and Hongwu Zhong and Guangwei Gao},
  doi          = {10.1016/j.patcog.2025.112313},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112313},
  shortjournal = {Pattern Recognition},
  title        = {Noise-tolerant scheme and explicit regularizer for deep active learning with noisy oracles},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Linguistic query-guided mask generation for referring image segmentation. <em>PR</em>, <em>172</em>, 112306. (<a href='https://doi.org/10.1016/j.patcog.2025.112306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation aims to segment the image region of interest according to the given language expression, which is a typical multi-modal task. Existing methods either adopt the pixel classification-based or the learnable query-based framework for mask generation, both of which are insufficient to deal with various text-image pairs with a fix number of parametric prototypes. The motivation of this work is to propose an end-to-end framework built on transformer to perform Linguistic query-Guided mask generation, dubbed LGFormer. It views the linguistic features as query to generate a specialized prototype for arbitrary input image-text pair, thus generating more consistent segmentation results. Moreover, we design several cross-modal interaction modules (e.g. vision-language bidirectional attention module, VLBA) in both encoder and decoder to achieve better cross-modal alignment. Extensive experiments demonstrate that our LGFormer achieves a new state-of-the-art performance on ReferIt, RefCOCO+, and RefCOCOg by large margins. Code is available at https://github.com/mqchen1993/LGFormer .},
  archive      = {J_PR},
  author       = {Zhichao Wei and Xiaohao Chen and Mingqiang Chen and Hao Li and Zilong Dong and Siyu Zhu},
  doi          = {10.1016/j.patcog.2025.112306},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112306},
  shortjournal = {Pattern Recognition},
  title        = {Linguistic query-guided mask generation for referring image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TransSTC: Transformer tracker meets efficient spatial-temporal cues. <em>PR</em>, <em>172</em>, 112303. (<a href='https://doi.org/10.1016/j.patcog.2025.112303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, researchers have started developing trackers using the powerful global modeling capabilities of transformer networks. However, existing transformer trackers usually model all template spatial cues indiscriminately and ignore temporal cues of target state changes. This distracts the tracker’s attention and gradually fails to understand the target’s latest state. Therefore, we propose a new tracker called TransSTC, which explores the effective spatial cues in the template and temporal cues during tracking to improve the tracker’s performance. Specifically, we design the target-aware focused coding network to emphasize the efficient spatial cues in the templates, alleviating the impact of spatial cues with low associations of targets in templates on the tracker’s localization accuracy. Additionally, we employ the multi-temporal template update structure that accurately captures variations in the target’s appearance. Within this structure, the collected samples are assessed for target appearance similarity and environmental interference, followed by a three-level sample selection process to ensure the accurate template update. Finally, we introduce the motion constraint framework to dynamically adjust the classification results based on the target’s historical motion trajectory. Extensive experimental results on seven tracking benchmarks demonstrate that TransSTC achieves competitive tracking performance.},
  archive      = {J_PR},
  author       = {Hong Zhang and Wanli Xing and Yifan Yang and Hanyang Liu and Ding Yuan},
  doi          = {10.1016/j.patcog.2025.112303},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112303},
  shortjournal = {Pattern Recognition},
  title        = {TransSTC: Transformer tracker meets efficient spatial-temporal cues},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGFNet: Multi-granularity medical pattern fusion network for patient risk prediction. <em>PR</em>, <em>172</em>, 112302. (<a href='https://doi.org/10.1016/j.patcog.2025.112302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of patient risk prediction tasks is to predict a patient’s future disease or mortality risk based on his/her historical electronic health record (EHR). Most prior works focus on learning patient evolution patterns from longitudinal EHR data, while ignoring the differences in temporal granularity in medical data, resulting in insufficient information exploitation. To address these limitations, we propose the M ulti- G ranularity Medical Pattern F usion Net work (MGFNet) for patient risk prediction based on temporal data. It learns the evolutionary patterns of medical data at different temporal granularities (both at the vital sign-level and visit-level), and introduces a gated filtering function and a contrastive learning strategy for multi-granularity fusion, which captures fused information from different temporal granularities and supervises each other to obtain a more effective information representation. In addition, for patients with variable visit lengths, we introduce a soft curriculum learning method to learn these patterns by assigning different weights to medical samples to improve prediction accuracy. The final experimental results demonstrate that MGFNet effectively improves the performance of risk prediction compared with state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Lin Cheng and Yuliang Shi and Xiaojing Yu and Xinyu Li and Xinjun Wang and Zhongmin Yan and Zhiyong Chen},
  doi          = {10.1016/j.patcog.2025.112302},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112302},
  shortjournal = {Pattern Recognition},
  title        = {MGFNet: Multi-granularity medical pattern fusion network for patient risk prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning multi-scale spatial-frequency features for image denoising. <em>PR</em>, <em>172</em>, 112300. (<a href='https://doi.org/10.1016/j.patcog.2025.112300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.},
  archive      = {J_PR},
  author       = {Xu Zhao and Chen Zhao and Xiantao Hu and Hongliang Zhang and Ying Tai and Jian Yang},
  doi          = {10.1016/j.patcog.2025.112300},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112300},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-scale spatial-frequency features for image denoising},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep intrinsic image decomposition via physics-aware neural networks. <em>PR</em>, <em>172</em>, 112299. (<a href='https://doi.org/10.1016/j.patcog.2025.112299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition (IID) aims to separate an observed image into its underlying reflectance and shading components. This task is challenging due to the complex interplay of lighting, surface geometry, and material reflectance in real-world scenes. To address these challenges, this paper proposes a physics-aware deep neural network with a single-encoder, double-decoder architecture. The encoder incorporates an explicit alternating process inspired by a physics-guided model, enabling iterative decoupling of image features into reflectance and shading. Two asymmetric decoders are designed to reconstruct reflectance and shading maps based on their distinct properties. In addition, we introduce a shading loss function leverages spatial distributions of texture and structure. Unlike standard total variation (TV) losses, it employs a texture-likelihood-weighted TV norm, where weights are derived via a patch-matching scheme to distinguish isotropic textures from anisotropic image edges. This design enhances the model’s ability to suppress texture while preserving structure. Experimental results on three datasets (MIT, MPI-Sintel, and IIW) show the effectiveness of our method: on MIT and MPI-Sintel, it reduces the mean-squared-errors of both reflectance and shading by over 40 % compared to existing works, and on IIW, it achieves a superior WHDR score of 13.2, outperforming all existing methods.},
  archive      = {J_PR},
  author       = {Yan Huang and Kangjie Liu and Tengyue Chen and Yong Xu and Hui Ji},
  doi          = {10.1016/j.patcog.2025.112299},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112299},
  shortjournal = {Pattern Recognition},
  title        = {Deep intrinsic image decomposition via physics-aware neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain adapter for visual object tracking based on hyperspectral video. <em>PR</em>, <em>172</em>, 112296. (<a href='https://doi.org/10.1016/j.patcog.2025.112296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking based on hyperspectral video attracts increasing attention due to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by converting the hyperspectral images into false-color images and fine-tuning the whole network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the spectral information loss during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method based on domain adaption, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the spectral information in the original hyperspectral images into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which reduce the negative impact of the spectral information loss caused by the transformation. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifying the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets.},
  archive      = {J_PR},
  author       = {Long Gao and Yunhe Zhang and Langkun Chen and Yan Jiang and Gang He and Weiying Xie and Yunsong Li},
  doi          = {10.1016/j.patcog.2025.112296},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112296},
  shortjournal = {Pattern Recognition},
  title        = {Domain adapter for visual object tracking based on hyperspectral video},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Few-shot image generation via information transfer from the built geodesic surface. <em>PR</em>, <em>172</em>, 112293. (<a href='https://doi.org/10.1016/j.patcog.2025.112293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models trained with limited data often struggle with poor fidelity and diversity. While adapting large pre-trained models is a common solution, such an approach requires significant resources and suitable source domains, which are often unavailable. To address these limitations, we propose Information Transfer from the Built Geodesic Surface (ITBGS), a framework that generates high-quality images from scratch. The core of ITBGS is our Feature Augmentation on Geodesic Surface (FAGS) module, which constructs a Geodesic surface to create a diverse pseudo-source domain from the initial samples. By transferring structural information from the augmented domain to guide the generator’s training, our method completely removes the need for pre-trained models. To refine the output, a supporting Interpolation and Regularization (I&R) module is also introduced to enhance the smoothness and perceptual quality of generated images. Extensive experiments demonstrate that ITBGS achieves state-of-the-art or comparable performance on various few-shot datasets, successfully balancing image fidelity and diversity.},
  archive      = {J_PR},
  author       = {Yuexing Han and Liheng Ruan and Bing Wang},
  doi          = {10.1016/j.patcog.2025.112293},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112293},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot image generation via information transfer from the built geodesic surface},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FADMB: Fully attention-based dual memory bank network for weakly supervised video anomaly detection. <em>PR</em>, <em>172</em>, 112288. (<a href='https://doi.org/10.1016/j.patcog.2025.112288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is crucial for analyzing surveillance videos and plays a significant role in maintaining public safety. Recent advances in weakly supervised methods, utilizing video-level labels, have improved performance based on techniques like multi-instance learning and temporal modeling. Furthermore, memory banks demonstrate great potential in unsupervised anomaly detection, prompting their integration into weakly supervised setups. However, these methods depend on the Top- k selection mechanism to update the prototypes within memory banks, which has limitations such as overlooking valuable prototypes, leading to a biased updating process, and requiring hyperparameters. To tackle these challenges, we introduce a novel video anomaly detection model, FADMB ( F ully A ttention-based D ual M emory B ank network), which replaces the Top- k selection mechanism with an innovative attention-based prototype updating paradigm to obtain a more comprehensive and robust memory bank. Additionally, we design a Hybrid Encoder that encodes local and global temporal information to produce superior video representations. Extensive experiments demonstrate the superiority of FADMB, achieving 85.79 % AUC on UCF-Crime dataset and 83.29 % AP on XD-Violence dataset.},
  archive      = {J_PR},
  author       = {Zhiming Luo and Shuheng Huang and Kun Yang and Jianzhe Gao and Shaozi Li},
  doi          = {10.1016/j.patcog.2025.112288},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112288},
  shortjournal = {Pattern Recognition},
  title        = {FADMB: Fully attention-based dual memory bank network for weakly supervised video anomaly detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
