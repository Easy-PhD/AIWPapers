<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv">ICV - 11</h2>
<ul>
<li><details>
<summary>
(2025). Mining fine-grained attributes for vision–semantics integration in few-shot learning. <em>ICV</em>, <em>163</em>, 105739. (<a href='https://doi.org/10.1016/j.imavis.2025.105739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Few-Shot Learning (FSL) have been significantly driven by leveraging semantic descriptions to enhance feature discrimination and recognition performance. However, existing methods, such as SemFew, often rely on verbose or manually curated attributes and apply semantic guidance only to the support set, limiting their effectiveness in distinguishing fine-grained categories. Inspired by human visual perception, which emphasizes crucial features for accurate recognition, this study introduces concise, fine-grained semantic attributes to address these limitations. We propose a Visual Attribute Enhancement (VAE) mechanism that integrates enriched semantic information into visual features, enabling the model to highlight the most relevant visual attributes and better distinguish visually similar samples. This module enhances visual features by aligning them with semantic attribute embeddings through a cross-attention mechanism and optimizes this alignment using an attribute-based cross-entropy loss. Furthermore, to mitigate the performance degradation caused by methods that supply semantic information exclusively to the support set, we propose a semantic attribute reconstruction (SAR) module. This module predicts and integrates semantic features for query samples, ensuring balanced information distribution between the support and query sets. Specifically, SAR enhances query representations by aligning and reconstructing semantic and visual attributes through regression and optimal transport losses to ensure semantic–visual consistency. Experiments on five benchmark datasets, including both general datasets and more challenging fine-grained Few-Shot datasets consistently demonstrate that our proposed method outperforms state-of-the-art methods in both 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_ICV},
  author       = {Juan Zhao and Lili Kong and Deshang Sun and Deng Xiong and Jiancheng Lv},
  doi          = {10.1016/j.imavis.2025.105739},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105739},
  shortjournal = {Image Vis. Comput.},
  title        = {Mining fine-grained attributes for vision–semantics integration in few-shot learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deepfake detection across different modalities: An overview of methods and challenges. <em>ICV</em>, <em>163</em>, 105738. (<a href='https://doi.org/10.1016/j.imavis.2025.105738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of deepfake technology enables the creation of realistic and deceptive content, raising concerns about several serious issues, including biometric authentication, misinformation, politics, privacy, and trust. Many Deepfake Detection (DD) models are entering the market to combat the misuse of deepfakes. With these developments, one primary issue occurs in ensuring the explainability of the proposed detection models to understand the rationale of the decision. This paper aims to investigate the state-of-the-art explainable DD models across multiple modalities, including image, video, audio, and text. Unlike existing surveys that focus on detection methodologies with minimal attention to explainability and limited modality coverage, this paper directly focuses on these gaps. It offers a comprehensive analysis of advanced explainability techniques, including Grad-CAM, LIME, SHAP, LRP, Saliency Maps, and Anchors, for detecting deceptive content across the modalities. It identifies the strengths and limitations of existing models and outlines research directions to enhance explainability and interpretability in future works. By exploring these models, we aim to enhance transparency, provide deeper insights into model decisions, and bridge the gap between detection accuracy with explainability in DD models.},
  archive      = {J_ICV},
  author       = {MD Sarfaraz Momin and Abu Sufian and Debaditya Barman and Marco Leo and Cosimo Distante and Naser Damer},
  doi          = {10.1016/j.imavis.2025.105738},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105738},
  shortjournal = {Image Vis. Comput.},
  title        = {Explainable deepfake detection across different modalities: An overview of methods and challenges},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance. <em>ICV</em>, <em>163</em>, 105736. (<a href='https://doi.org/10.1016/j.imavis.2025.105736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs , addressing five critical ITS tasks : object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS’s effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5’s performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6’s from 0.678 to 0.921 (+35.8%), Qwen2-VL’s from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL’s from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source , providing high-value resources to advance both ITS and LMM research.},
  archive      = {J_ICV},
  author       = {Kaikai Zhao and Zhaoxiang Liu and Peng Wang and Xin Wang and Zhicheng Ma and Yajun Xu and Wenjing Zhang and Yibing Nan and Kai Wang and Shiguo Lian},
  doi          = {10.1016/j.imavis.2025.105736},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105736},
  shortjournal = {Image Vis. Comput.},
  title        = {MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data. <em>ICV</em>, <em>163</em>, 105734. (<a href='https://doi.org/10.1016/j.imavis.2025.105734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring underwater images affected by non-uniform illumination (NUI) is essential to improve visual quality and usability in marine applications. Conventional methods often fall short in handling complex illumination patterns, while learning-based approaches face challenges due to the lack of targeted datasets. To address these limitations, the Underwater Non-uniform Illumination Restoration Network (UNIR-Net) is proposed. UNIR-Net integrates multiple components, including illumination enhancement, attention mechanisms, visual refinement, and contrast correction, to effectively restore underwater images affected by NUI. In addition, the Paired Underwater Non-uniform Illumination (PUNI) dataset is introduced, specifically designed for training and evaluating models under NUI conditions. Experimental results on PUNI and the large-scale real-world Non-Uniform Illumination Dataset (NUID) show that UNIR-Net achieves superior performance in both quantitative metrics and visual outcomes. UNIR-Net also improves downstream tasks such as underwater semantic segmentation, highlighting its practical relevance. The code is available at https://github.com/xingyumex/UNIR-Net .},
  archive      = {J_ICV},
  author       = {Ezequiel Pérez-Zarate and Chunxiao Liu and Oscar Ramos-Soto and Diego Oliva and Marco Pérez-Cisneros},
  doi          = {10.1016/j.imavis.2025.105734},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105734},
  shortjournal = {Image Vis. Comput.},
  title        = {UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FFENet: A frequency fusion and enhancement network for camouflaged object detection. <em>ICV</em>, <em>163</em>, 105733. (<a href='https://doi.org/10.1016/j.imavis.2025.105733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of camouflaged object detection (COD) is to accurately find camouflaged objects hidden in their surroundings. Although most of the existing frequency-domain based COD models can boost the performance of COD to a certain extent by utilizing the frequency domain information, the frequency feature fusion strategies they adopt tend to ignore the complementary effects between high-frequency features and low-frequency features. In addition, most of the existing frequency-domain based COD models also do not consider enhancing camouflaged objects using low-level frequency-domain features. In order to solve these problems, we present a frequency fusion and enhancement network (FFENet) for camouflaged object detection, which mainly includes three stages. In the frequency feature extraction stage, we design a frequency feature learning module (FLM) to extract corresponding high-frequency features and low-frequency features. In the frequency feature fusion stage, we design a frequency feature fusion module (FFM) that can increase the representation ability of the fused features by adaptively assigning weights to the high-frequency features and the low-frequency features using a cross-attention mechanism. In the frequency feature guidance information enhancement stage, we design a frequency feature guidance information enhancement module (FGIEM) to enhance the contextual information and detail information of camouflaged objects in the fused features under the guidance of the low-level frequency features. Extensive experimental results on the COD10K, CHAMELEON, NC4K and CAMO datasets show that our model is superior to most existing COD models.},
  archive      = {J_ICV},
  author       = {Haishun Du and Wenzhe Zhang and Sen Wang and Zhengyang Zhang and Linbing Cao},
  doi          = {10.1016/j.imavis.2025.105733},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105733},
  shortjournal = {Image Vis. Comput.},
  title        = {FFENet: A frequency fusion and enhancement network for camouflaged object detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UpAttTrans: Upscaled attention based transformer for facial image super-resolution. <em>ICV</em>, <em>163</em>, 105731. (<a href='https://doi.org/10.1016/j.imavis.2025.105731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) aims to reconstruct high-quality images from low-resolution inputs, a task particularly challenging in face-related applications due to extreme degradations and modality differences (e.g., visible, low-resolution, near-infrared). Conventional convolutional neural networks (CNNs) and GAN-based approaches have achieved notable success; however, they often struggle with preserving identity and fine structural details at high upscaling factors. In this work, we introduce UpAttTrans, a novel attention mechanism that connects original and upsampled features for better detail recovery based on vision transformer for SR. The core generator leverages a custom UpAttTrans module that translates input image patches into embeddings, processes them through transformer layers enhanced with connector-up attention, and reconstructs high-resolution outputs with improved detail retention. We evaluate our model on the CelebA dataset across multiple upscaling factors ( 4 × , 8 × , 16 × , 32 × , and 64 × ). UpAttTrans achieves a 24.63% increase in PSNR, 21.56% in SSIM, and 19.61% reduction in FID for 4 × and 8 × SR, outperforming state-of-the-art baselines. Additionally, for higher magnification levels, our model maintains strong performance, with average gains of 6.20% in PSNR and 21.49% in SSIM, indicating its robustness in extreme SR settings. These findings suggest that UpAttTrans holds significant promise for real-world applications such as face recognition in surveillance, forensic image enhancement, and cross-spectral matching, where high-quality reconstruction from severely degraded inputs is critical.},
  archive      = {J_ICV},
  author       = {Neeraj Baghel and Shiv Ram Dubey and Satish Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105731},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105731},
  shortjournal = {Image Vis. Comput.},
  title        = {UpAttTrans: Upscaled attention based transformer for facial image super-resolution},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation. <em>ICV</em>, <em>163</em>, 105729. (<a href='https://doi.org/10.1016/j.imavis.2025.105729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov–Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov–Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN .},
  archive      = {J_ICV},
  author       = {Shuang Zeng and Chee Hong Lee and Micky C. Nnamdi and Wenqi Shi and J. Ben Tamo and Hangzhou He and Xinliang Zhang and Qian Chen and May D. Wang and Lei Zhu and Yanye Lu and Qiushi Ren},
  doi          = {10.1016/j.imavis.2025.105729},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105729},
  shortjournal = {Image Vis. Comput.},
  title        = {Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence content detection techniques using watermarking: A survey. <em>ICV</em>, <em>163</em>, 105728. (<a href='https://doi.org/10.1016/j.imavis.2025.105728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement in AI-generated content has catalyzed artistic creation, advertising, and media dissemination. Despite their widespread applications across several domains, AI-generated content inherently poses risks of identity fraud, copyright violation and unauthorized use. Watermarking has emerged as a critical tool for copyright protection, allowing embedding of identification information in AI-generated content, and enhances traceability and verification without hurting user experience. In this study, we provide a systematic literature review of the technique for detecting AI content, especially text and images, using watermarking, spanning studies from 2010 to 2025. Studies included in this review were peer-reviewed articles that applied watermarking to effectively distinguish AI-generated content from real or human-written content. We report strong past and current approaches to detecting watermarking-based AI content, especially text and images. This includes an analysis of how watermarking methods are used on AI-generated content, their role in enhancing performance, and a detail comparative analysis of notable techniques. Furthermore, we discuss how these methods have been evaluated, identify the research gaps and potential solutions. Our findings provide valuable insights for future watermarking-based AI content detection researchers, applications and organizations seeking to implement watermarking solutions in potential applications. To the best of our knowledge, we are the first to explore the detection of AI content, especially text and image, detection using watermarking.},
  archive      = {J_ICV},
  author       = {Nishant Kumar and Amit Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105728},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105728},
  shortjournal = {Image Vis. Comput.},
  title        = {Artificial intelligence content detection techniques using watermarking: A survey},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Your image generator is your new private dataset. <em>ICV</em>, <em>163</em>, 105727. (<a href='https://doi.org/10.1016/j.imavis.2025.105727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, existing approaches for synthetic dataset generation face significant limitations: previous methods like Knowledge Recycling rely on label-conditioned generation with models trained from scratch, limiting flexibility and requiring extensive computational resources, while simple class-based conditioning fails to capture the semantic diversity and intra-class variations found in real datasets. Additionally, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper proposes the Text-Conditioned Knowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines dynamic image captioning, parameter-efficient diffusion model fine-tuning, and Generative Knowledge Distillation techniques to create synthetic datasets tailored for image classification. The pipeline is rigorously evaluated on ten diverse image classification benchmarks. The results demonstrate that models trained solely on TCKR-generated data achieve classification accuracies on par with (and in several cases exceeding) models trained on real images. Furthermore, the evaluation reveals that these synthetic-data-trained models exhibit substantially enhanced privacy characteristics: their vulnerability to Membership Inference Attacks is significantly reduced, with the membership inference AUC lowered by 5.49 points on average compared to using real training data, demonstrating a substantial improvement in the performance-privacy trade-off. These findings indicate that high-fidelity synthetic data can effectively replace real data for training classifiers, yielding strong performance whilst simultaneously providing improved privacy protection as a valuable emergent property. The code and trained models are available in the accompanying open-source repository .},
  archive      = {J_ICV},
  author       = {Nicolò Francesco Resmini and Eugenio Lomurno and Cristian Sbrolli and Matteo Matteucci},
  doi          = {10.1016/j.imavis.2025.105727},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105727},
  shortjournal = {Image Vis. Comput.},
  title        = {Your image generator is your new private dataset},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the noise robustness of class activation maps: A framework for reliable model interpretability. <em>ICV</em>, <em>163</em>, 105717. (<a href='https://doi.org/10.1016/j.imavis.2025.105717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Activation Maps (CAMs) are one of the important methods for visualizing regions used by deep learning models. Yet their robustness to different noise remains underexplored. In this work, we evaluate and report the resilience of various CAM methods for different noise perturbations across multiple architectures and datasets. By analyzing the influence of different noise types on CAM explanations, we assess the susceptibility to noise and the extent to which dataset characteristics may impact explanation stability. The findings highlight considerable variability in noise sensitivity for various CAMs. We propose a robustness metric for CAMs that captures two key properties: consistency and responsiveness. Consistency reflects the ability of CAMs to remain stable under input perturbations that do not alter the predicted class, while responsiveness measures the sensitivity of CAMs to changes in the prediction caused by such perturbations. The metric is evaluated empirically across models, different perturbations, and datasets along with complementary statistical tests to exemplify the applicability of our proposed approach.},
  archive      = {J_ICV},
  author       = {Syamantak Sarkar and Revoti P. Bora and Bhupender Kaushal and Sudhish N. George and Kiran Raja},
  doi          = {10.1016/j.imavis.2025.105717},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105717},
  shortjournal = {Image Vis. Comput.},
  title        = {Assessing the noise robustness of class activation maps: A framework for reliable model interpretability},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images. <em>ICV</em>, <em>163</em>, 105693. (<a href='https://doi.org/10.1016/j.imavis.2025.105693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion has emerged as a critical technique for the diagnosis of Alzheimer’s Disease (AD), with the aim of effectively extracting and utilising complementary information from diverse modalities. Current fusion methods frequently cause the precise alignment of source images and do not adequately address parallax issues. This oversight can result in artifacts during the fusion process when images are misaligned. In response to this challenge, we propose a refined registration fusion technique, termed MURF, which integrates multimodal image registration and fusion within a cohesive framework. The Vision Transformer (ViT) has inspired the application of large-kernel convolutions in the diagnosis of Alzheimer’s disease (AD) because of its ability to model long-range dependencies. This approach aims to expand the receptive field and enhance the performance of diagnostic models. Despite requiring a minimal number of floating-point operations (FLOPs), these deep operators encounter challenges associated with over-parameterisation because of high memory access costs, which ultimately compromises computational efficiency. By utilising wavelet transform convolutions (WTConv), we decompose large-kernel depth-wise convolutions into four parallel branches. One branch employs a wavelet-transform convolution with square kernels, while the other two branches incorporate orthogonal wavelet-transform kernels with an identity mapping. This innovative method, with a Mixed Local Channel Attention mechanism, has facilitated the development of the InceptionWTConvolutions network. This network maintains a receptive field comparable to that of large-kernel convolutions, while concurrently minimising over-parameterisation and enhancing computational efficiency. InceptionWTMNet classified AD, MCI, and NC using MRI and PET data from ADNI dataset with 98.69% accuracy, 98.65% recall, 98.70% F1-score, and 98.98% AUC. and provide Graphical abstract in correct format.},
  archive      = {J_ICV},
  author       = {Zenan Xu and Zhengyao Bai and Han Ma and Mingqiang Xu and Qiqin Huang and Tao Lin},
  doi          = {10.1016/j.imavis.2025.105693},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105693},
  shortjournal = {Image Vis. Comput.},
  title        = {InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
