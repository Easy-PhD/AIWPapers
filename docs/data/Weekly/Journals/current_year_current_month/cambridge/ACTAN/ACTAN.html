<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ACTAN</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="actan">ACTAN - 8</h2>
<ul>
<li><details>
<summary>
(2025). Sparse linear least-squares problems. <em>ACTAN</em>, <em>34</em>, 891-1010. (<a href='https://doi.org/10.1017/S0962492924000059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least-squares problems are a cornerstone of computational science and engineering. Over the years, the size of the problems that researchers and practitioners face has constantly increased, making it essential that sparsity is exploited in the solution process. The goal of this article is to present a broad review of key algorithms for solving large-scale linear least-squares problems. This includes sparse direct methods and algebraic preconditioners that are used in combination with iterative solvers. Where software is available, this is highlighted.},
  archive      = {J_ACTAN},
  author       = {Jennifer Scott and Miroslav Tůma},
  doi          = {10.1017/S0962492924000059},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {891-1010},
  shortjournal = {Acta Numer.},
  title        = {Sparse linear least-squares problems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration methods for fixed-point iterations. <em>ACTAN</em>, <em>34</em>, 805-890. (<a href='https://doi.org/10.1017/S0962492924000096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pervasive approach in scientific computing is to express the solution to a given problem as the limit of a sequence of vectors or other mathematical objects. In many situations these sequences are generated by slowly converging iterative procedures, and this led practitioners to seek faster alternatives to reach the limit. ‘Acceleration techniques’ comprise a broad array of methods specifically designed with this goal in mind. They started as a means of improving the convergence of general scalar sequences by various forms of ‘extrapolation to the limit’, i.e. by extrapolating the most recent iterates to the limit via linear combinations. Extrapolation methods of this type, the best-known of which is Aitken’s delta-squared process, require only the sequence of vectors as input. However, limiting methods to use only the iterates is too restrictive. Accelerating sequences generated by fixed-point iterations by utilizing both the iterates and the fixed-point mapping itself has proved highly successful across various areas of physics. A notable example of these fixed-point accelerators (FP-accelerators) is a method developed by Donald Anderson in 1965 and now widely known as Anderson acceleration (AA). Furthermore, quasi-Newton and inexact Newton methods can also be placed in this category since they can be invoked to find limits of fixed-point iteration sequences by employing exactly the same ingredients as those of the FP-accelerators. This paper presents an overview of these methods – with an emphasis on those, such as AA, that are geared toward accelerating fixed-point iterations. We will navigate through existing variants of accelerators, their implementations and their applications, to unravel the close connections between them. These connections were often not recognized by the originators of certain methods, who sometimes stumbled on slight variations of already established ideas. Furthermore, even though new accelerators were invented in different corners of science, the underlying principles behind them are strikingly similar or identical. The plan of this article will approximately follow the historical trajectory of extrapolation and acceleration methods, beginning with a brief description of extrapolation ideas, followed by the special case of linear systems, the application to self-consistent field (SCF) iterations, and a detailed view of Anderson acceleration. The last part of the paper is concerned with more recent developments, including theoretical aspects, and a few thoughts on accelerating machine learning algorithms.},
  archive      = {J_ACTAN},
  author       = {Yousef Saad},
  doi          = {10.1017/S0962492924000096},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {805-890},
  shortjournal = {Acta Numer.},
  title        = {Acceleration methods for fixed-point iterations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust optimization. <em>ACTAN</em>, <em>34</em>, 579-804. (<a href='https://doi.org/10.1017/S0962492924000084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally robust optimization (DRO) studies decision problems under uncertainty where the probability distribution governing the uncertain problem parameters is itself uncertain. A key component of any DRO model is its ambiguity set, that is, a family of probability distributions consistent with any available structural or statistical information. DRO seeks decisions that perform best under the worst distribution in the ambiguity set. This worst case criterion is supported by findings in psychology and neuroscience, which indicate that many decision-makers have a low tolerance for distributional ambiguity. DRO is rooted in statistics, operations research and control theory, and recent research has uncovered its deep connections to regularization techniques and adversarial training in machine learning. This survey presents the key findings of the field in a unified and self-contained manner.},
  archive      = {J_ACTAN},
  author       = {Daniel Kuhn and Soroosh Shafiee and Wolfram Wiesemann},
  doi          = {10.1017/S0962492924000084},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {579-804},
  shortjournal = {Acta Numer.},
  title        = {Distributionally robust optimization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization problems governed by systems of PDEs with uncertainties. <em>ACTAN</em>, <em>34</em>, 491-577. (<a href='https://doi.org/10.1017/S0962492925000029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews current theoretical and numerical approaches to optimization problems governed by partial differential equations (PDEs) that depend on random variables or random fields. Such problems arise in many engineering, science, economics and societal decision-making tasks. This paper focuses on problems in which the governing PDEs are parametrized by the random variables/fields, and the decisions are made at the beginning and are not revised once uncertainty is revealed. Examples of such problems are presented to motivate the topic of this paper, and to illustrate the impact of different ways to model uncertainty in the formulations of the optimization problem and their impact on the solution. A linear–quadratic elliptic optimal control problem is used to provide a detailed discussion of the set-up for the risk-neutral optimization problem formulation, study the existence and characterization of its solution, and survey numerical methods for computing it. Different ways to model uncertainty in the PDE-constrained optimization problem are surveyed in an abstract setting, including risk measures, distributionally robust optimization formulations, probabilistic functions and chance constraints, and stochastic orders. Furthermore, approximation-based optimization approaches and stochastic methods for the solution of the large-scale PDE-constrained optimization problems under uncertainty are described. Some possible future research directions are outlined.},
  archive      = {J_ACTAN},
  author       = {Matthias Heinkenschloss and Drew P. Kouri},
  doi          = {10.1017/S0962492925000029},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {491-577},
  shortjournal = {Acta Numer.},
  title        = {Optimization problems governed by systems of PDEs with uncertainties},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time parallelization for hyperbolic and parabolic problems. <em>ACTAN</em>, <em>34</em>, 385-489. (<a href='https://doi.org/10.1017/S0962492924000072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time parallelization, also known as PinT (parallel-in-time), is a new research direction for the development of algorithms used for solving very large-scale evolution problems on highly parallel computing architectures. Despite the fact that interesting theoretical work on PinT appeared as early as 1964, it was not until 2004, when processor clock speeds reached their physical limit, that research in PinT took off. A distinctive characteristic of parallelization in time is that information flow only goes forward in time, meaning that time evolution processes seem necessarily to be sequential. Nevertheless, many algorithms have been developed for PinT computations over the past two decades, and they are often grouped into four basic classes according to how the techniques work and are used: shooting-type methods; waveform relaxation methods based on domain decomposition; multigrid methods in space–time; and direct time parallel methods. However, over the past few years, it has been recognized that highly successful PinT algorithms for parabolic problems struggle when applied to hyperbolic problems. We will therefore focus on this important aspect, first by providing a summary of the fundamental differences between parabolic and hyperbolic problems for time parallelization. We then group PinT algorithms into two basic groups. The first group contains four effective PinT techniques for hyperbolic problems: Schwarz waveform relaxation (SWR) with its relation to tent pitching; parallel integral deferred correction; ParaExp; and ParaDiag. While the methods in the first group also work well for parabolic problems, we then present PinT methods specifically designed for parabolic problems in the second group: Parareal; the parallel full approximation scheme in space–time (PFASST); multigrid reduction in time (MGRiT); and space–time multigrid (STMG). We complement our analysis with numerical illustrations using four time-dependent PDEs: the heat equation; the advection–diffusion equation; Burgers’ equation; and the second-order wave equation.},
  archive      = {J_ACTAN},
  author       = {Martin J. Gander and Shu-Lin Wu and Tao Zhou},
  doi          = {10.1017/S0962492924000072},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {385-489},
  shortjournal = {Acta Numer.},
  title        = {Time parallelization for hyperbolic and parabolic problems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The discontinuous Petrov–Galerkin method. <em>ACTAN</em>, <em>34</em>, 293-384. (<a href='https://doi.org/10.1017/S0962492924000102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discontinuous Petrov–Galerkin (DPG) method is a Petrov–Galerkin finite element method with test functions designed for obtaining stability. These test functions are computable locally, element by element, and are motivated by optimal test functions which attain the supremum in an inf-sup condition. A profound consequence of the use of nearly optimal test functions is that the DPG method can inherit the stability of the (undiscretized) variational formulation, be it coercive or not. This paper combines a presentation of the fundamentals of the DPG ideas with a review of the ongoing research on theory and applications of the DPG methodology. The scope of the presented theory is restricted to linear problems on Hilbert spaces, but pointers to extensions are provided. Multiple viewpoints to the basic theory are provided. They show that the DPG method is equivalent to a method which minimizes a residual in a dual norm, as well as to a mixed method where one solution component is an approximate error representation function. Being a residual minimization method, the DPG method yields Hermitian positive definite stiffness matrix systems even for non-self-adjoint boundary value problems. Having a built-in error representation, the method has the out-of-the-box feature that it can immediately be used in automatic adaptive algorithms. Contrary to standard Galerkin methods, which are uninformed about test and trial norms, the DPG method must be equipped with a concrete test norm which enters the computations. Of particular interest are variational formulations in which one can tailor the norm to obtain robust stability. Key techniques to rigorously prove convergence of DPG schemes, including construction of Fortin operators, which in the DPG case can be done element by element, are discussed in detail. Pointers to open frontiers are presented.},
  archive      = {J_ACTAN},
  author       = {Leszek Demkowicz and Jay Gopalakrishnan},
  doi          = {10.1017/S0962492924000102},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {293-384},
  shortjournal = {Acta Numer.},
  title        = {The discontinuous Petrov–Galerkin method},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble kalman methods: A mean-field perspective. <em>ACTAN</em>, <em>34</em>, 123-291. (<a href='https://doi.org/10.1017/S0962492924000060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble Kalman methods, introduced in 1994 in the context of ocean state estimation, are now widely used for state estimation and parameter estimation (inverse problems) in many arenae. Their success stems from the fact that they take an underlying computational model as a black box to provide a systematic, derivative-free methodology for incorporating observations; furthermore the ensemble approach allows for sensitivities and uncertainties to be calculated. Analysis of the accuracy of ensemble Kalman methods, especially in terms of uncertainty quantification, is lagging behind empirical success; this paper provides a unifying mean-field-based framework for their analysis. Both state estimation and parameter estimation problems are considered, and formulations in both discrete and continuous time are employed. For state estimation problems, both the control and filtering approaches are considered; analogously for parameter estimation problems, the optimization and Bayesian perspectives are both studied. As well as providing an elegant framework, the mean-field perspective also allows for the derivation of a variety of methods used in practice. In addition it unifies a wide-ranging literature in the field and suggests open problems.},
  archive      = {J_ACTAN},
  author       = {Edoardo Calvello and Sebastian Reich and Andrew M. Stuart},
  doi          = {10.1017/S0962492924000060},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {123-291},
  shortjournal = {Acta Numer.},
  title        = {Ensemble kalman methods: A mean-field perspective},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cut finite element methods. <em>ACTAN</em>, <em>34</em>, 1-121. (<a href='https://doi.org/10.1017/S0962492925000017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cut finite element methods (CutFEM) extend the standard finite element method to unfitted meshes, enabling the accurate resolution of domain boundaries and interfaces without requiring the mesh to conform to them. This approach preserves the key properties and accuracy of the standard method while addressing challenges posed by complex geometries and moving interfaces. In recent years, CutFEM has gained significant attention for its ability to discretize partial differential equations in domains with intricate geometries. This paper provides a comprehensive review of the core concepts and key developments in CutFEM, beginning with its formulation for common model problems and the presentation of fundamental analytical results, including error estimates and condition number estimates for the resulting algebraic systems. Stabilization techniques for cut elements, which ensure numerical robustness, are also explored. Finally, extensions to methods involving Lagrange multipliers and applications to time-dependent problems are discussed.},
  archive      = {J_ACTAN},
  author       = {Erik Burman and Peter Hansbo and Mats G. Larson and Sara Zahedi},
  doi          = {10.1017/S0962492925000017},
  journal      = {Acta Numerica},
  month        = {7},
  pages        = {1-121},
  shortjournal = {Acta Numer.},
  title        = {Cut finite element methods},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
