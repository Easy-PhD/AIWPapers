<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa">JASA - 59</h2>
<ul>
<li><details>
<summary>
(2025). Objective bayesian inference. <em>JASA</em>, <em>120</em>(550), 1321-1322. (<a href='https://doi.org/10.1080/01621459.2025.2454051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jaeyong Lee},
  doi          = {10.1080/01621459.2025.2454051},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1321-1322},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Objective bayesian inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soccer analytics: An introduction using r. <em>JASA</em>, <em>120</em>(550), 1320-1321. (<a href='https://doi.org/10.1080/01621459.2024.2435110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Alexander Aue},
  doi          = {10.1080/01621459.2024.2435110},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1320-1321},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Soccer analytics: An introduction using r},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handbook of bayesian, fiducial, and frequentist inference. <em>JASA</em>, <em>120</em>(550), 1318-1320. (<a href='https://doi.org/10.1080/01621459.2025.2454048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mengyang Gu},
  doi          = {10.1080/01621459.2025.2454048},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1318-1320},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of bayesian, fiducial, and frequentist inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep regression learning with optimal loss function. <em>JASA</em>, <em>120</em>(550), 1305-1317. (<a href='https://doi.org/10.1080/01621459.2024.2412364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a novel efficient and robust nonparametric regression estimator under a framework of a feedforward neural network (FNN). There are several interesting characteristics for the proposed estimator. First, the loss function is built upon an estimated maximum likelihood function, which integrates the information from observed data as well as the information from the data distribution. Consequently, the resulting estimator has desirable optimal properties, such as efficiency. Second, different from the traditional maximum likelihood estimation (MLE), the proposed method avoids the specification of the distribution, making it adaptable to various distributions such as heavy tails and multimodal or heterogeneous distributions. Third, the proposed loss function relies on probabilities rather than direct observations as in least square loss, contributing to the robustness of the proposed estimator. Finally, the proposed loss function involves a nonparametric regression function only. This enables the direct application of the existing packages, simplifying the computational and programming requirements. We establish the large sample property of the proposed estimator in terms of its excess risk and minimax near-optimal rate. The theoretical results demonstrate that the proposed estimator is equivalent to the true MLE where the density function is known in terms of excess risk. Our simulation studies show that the proposed estimator outperforms the existing methods based on prediction accuracy, efficiency and robustness. Particularly, it is comparable to the MLE with the known density and even gets slightly better as the sample size increases. This implies that the adaptive and data-driven loss function from the estimated density may offer an additional avenue for capturing valuable information. We further apply the proposed method to four real data examples, resulting in significantly reduced out-of-sample prediction errors compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xuancheng Wang and Ling Zhou and Huazhen Lin},
  doi          = {10.1080/01621459.2024.2412364},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1305-1317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep regression learning with optimal loss function},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust permutation tests in linear instrumental variables regression. <em>JASA</em>, <em>120</em>(550), 1294-1304. (<a href='https://doi.org/10.1080/01621459.2024.2412363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops permutation versions of identification-robust tests in linear instrumental variables regression. Unlike the existing randomization and rank-based tests in which independence between the instruments and the error terms is assumed, the permutation Anderson-Rubin (AR), Lagrange Multiplier (LM) and Conditional Likelihood Ratio (CLR) tests are asymptotically similar and robust to conditional heteroscedasticity under standard exclusion restriction, that is, the orthogonality between the instruments and the error terms. Moreover, when the instruments are independent of the structural error term, the permutation AR tests are exact, hence, robust to heavy tails. As such, these tests share the strengths of the rank-based tests and the wild bootstrap AR tests. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Purevdorj Tuvaandorj},
  doi          = {10.1080/01621459.2024.2412363},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1294-1304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust permutation tests in linear instrumental variables regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparse beta regression model for network analysis. <em>JASA</em>, <em>120</em>(550), 1281-1293. (<a href='https://doi.org/10.1080/01621459.2024.2411073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For statistical analysis of network data, the ùõΩ -model has emerged as a useful tool, thanks to its flexibility in incorporating nodewise heterogeneity and theoretical tractability. To generalize the Œ≤ -model, this article proposes the Sparse Œ≤ -Regression Model (S Œ≤ RM) that unites two research themes developed recently in modeling homophily and sparsity. In particular, we employ differential heterogeneity that assigns weights only to important nodes and propose penalized likelihood with an l 1 penalty for parameter estimation. While our estimation method is closely related to the LASSO method for logistic regression, we develop a new theory emphasizing the use of our model for dealing with a parameter regime that can handle sparse networks usually seen in practice. More interestingly, the resulting inference on the homophily parameter demands no debiasing normally employed in LASSO type estimation. We provide extensive simulation and data analysis to illustrate the use of the model. As a special case of our model, we extend the Erd≈ës-R√©nyi model by including covariates and develop the associated statistical inference for sparse networks, which may be of independent interest. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Stefan Stein and Rui Feng and Chenlei Leng},
  doi          = {10.1080/01621459.2024.2411073},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1281-1293},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A sparse beta regression model for network analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative methods for vecchia-laplace approximations for latent gaussian process models. <em>JASA</em>, <em>120</em>(550), 1267-1280. (<a href='https://doi.org/10.1080/01621459.2024.2410004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Gaussian process (GP) models are flexible probabilistic nonparametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, that is, on large datasets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a 3-fold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite dataset. All methods are implemented in a free C++ software library with high-level Python and R packages. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pascal K√ºndig and Fabio Sigrist},
  doi          = {10.1080/01621459.2024.2410004},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1267-1280},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Iterative methods for vecchia-laplace approximations for latent gaussian process models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive learning of the latent space of wasserstein generative adversarial networks. <em>JASA</em>, <em>120</em>(550), 1254-1266. (<a href='https://doi.org/10.1080/01621459.2024.2408778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models based on latent variables, such as generative adversarial networks (GANs) and variational auto-encoders (VAEs), have gained lots of interests due to their impressive performance in many fields. However, many data such as natural images usually do not populate the ambient Euclidean space but instead reside in a lower-dimensional manifold. Thus an inappropriate choice of the latent dimension fails to uncover the structure of the data, possibly resulting in mismatch of latent representations and poor generative qualities. Toward addressing these problems, we propose a novel framework called the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein auto-encoder and the Wasserstein GAN so that the intrinsic dimension of the data manifold can be adaptively learned by a modified informative latent distribution. We prove that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encoding distribution is equal to the dimension of the data manifold. We theoretically establish that our estimated intrinsic dimension is a consistent estimate of the true dimension of the data manifold. Meanwhile, we provide an upper bound on the generalization error of LWGAN, implying that we force the synthetic data distribution to be similar to the real data distribution from a population perspective. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by sampling from the learned latent distribution. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yixuan Qiu and Qingyi Gao and Xiao Wang},
  doi          = {10.1080/01621459.2024.2408778},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1254-1266},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive learning of the latent space of wasserstein generative adversarial networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring covariance structure from multiple data sources via subspace factor analysis. <em>JASA</em>, <em>120</em>(550), 1239-1253. (<a href='https://doi.org/10.1080/01621459.2024.2408777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis provides a canonical framework for imposing lower-dimensional structure such as sparse covariance in high-dimensional data. High-dimensional data on the same set of variables are often collected under different conditions, for instance in reproducing studies across research groups. In such cases, it is natural to seek to learn the shared versus condition-specific structure. Existing hierarchical extensions of factor analysis have been proposed, but face practical issues including identifiability problems. To address these shortcomings, we propose a class of SUbspace Factor Analysis (SUFA) models, which characterize variation across groups at the level of a lower-dimensional subspace. We prove that the proposed class of SUFA models lead to identifiability of the shared versus group-specific components of the covariance, and study their posterior contraction properties. Taking a Bayesian approach, these contributions are developed alongside efficient posterior computation algorithms. Our sampler fully integrates out latent variables, is easily parallelizable and has complexity that does not depend on sample size. We illustrate the methods through application to integration of multiple gene expression datasets relevant to immunology. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Noirrit Kiran Chandra and David B. Dunson and Jason Xu},
  doi          = {10.1080/01621459.2024.2408777},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1239-1253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring covariance structure from multiple data sources via subspace factor analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-agnostic graph neural network for integrating local and global information. <em>JASA</em>, <em>120</em>(550), 1225-1238. (<a href='https://doi.org/10.1080/01621459.2024.2404668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, however, existing GNNs suffer from two significant limitations: a lack of interpretability in their results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to effectively integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and demonstrate its power to represent layer-wise neighborhood mixing. We conduct comprehensive numerical studies using simulated data to demonstrate the superior performance of MaGNet in comparison to several state-of-the-art alternatives. Furthermore, we apply MaGNet to a real-world case study aimed at extracting task-critical information from brain activity data, thereby highlighting its effectiveness in advancing scientific research. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wenzhuo Zhou and Annie Qu and Keiland W. Cooper and Norbert Fortin and Babak Shahbaba},
  doi          = {10.1080/01621459.2024.2404668},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1225-1238},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A model-agnostic graph neural network for integrating local and global information},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating higher-order mixed memberships via the l2,‚àû tensor perturbation bound. <em>JASA</em>, <em>120</em>(550), 1214-1224. (<a href='https://doi.org/10.1080/01621459.2024.2404265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this article we propose the sub-Gaussian) tensor mixed-membership blockmodel , a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (HOOI) for tensor SVD composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound under sub-Gaussian noise, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the l 2 , ‚àû tensor perturbation bound for HOOI under independent, heteroscedastic, sub-Gaussian noise that may be of independent interest. Our analysis uses a novel leave-one-out construction for the iterates, and our bounds depend only on spectral properties of the underlying low-rank tensor under nearly optimal signal-to-noise ratio conditions such that tensor SVD is computationally feasible. Finally, we apply our methodology to real and simulated data, demonstrating some effects not identifiable from the model with discrete community memberships. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Joshua Agterberg and Anru R. Zhang},
  doi          = {10.1080/01621459.2024.2404265},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1214-1224},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating higher-order mixed memberships via the l2,‚àû tensor perturbation bound},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive transfer learning framework for functional classification. <em>JASA</em>, <em>120</em>(550), 1201-1213. (<a href='https://doi.org/10.1080/01621459.2024.2403788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the transfer learning problem in functional classification, aiming to improve the classification accuracy of the target data by leveraging information from related source datasets. To facilitate transfer learning, we propose a novel transferability function tailored for classification problems, enabling a more accurate evaluation of the similarity between source and target dataset distributions. Interestingly, we find that a source dataset can offer more substantial benefits under certain conditions than another dataset with an identical distribution to the target dataset. This observation renders the commonly-used debiasing step in the parameter-based transfer learning algorithm unnecessary under some circumstances to the classification problem. In particular, we propose two adaptive transfer learning algorithms based on the functional Distance Weighted Discrimination (DWD) classifier for scenarios with and without prior knowledge regarding informative sources. Furthermore, we establish the upper bound on the excess risk of the proposed classifiers, providing the statistical gain via transfer learning mathematically provable. Simulation studies are conducted to thoroughly examine the finite-sample performance of the proposed algorithms. Finally, we implement the proposed method to Beijing air-quality data, and significantly improve the prediction of the PM 2.5 level of a target station by effectively incorporating information from source datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Caihong Qin and Jinhan Xie and Ting Li and Yang Bai},
  doi          = {10.1080/01621459.2024.2403788},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1201-1213},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An adaptive transfer learning framework for functional classification},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale low-rank gaussian process prediction with support points. <em>JASA</em>, <em>120</em>(550), 1189-1200. (<a href='https://doi.org/10.1080/01621459.2024.2403188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank approximation is a popular strategy to tackle the ‚Äúbig n problem‚Äù associated with large-scale Gaussian process regressions. Basis functions for developing low-rank structures are crucial and should be carefully specified. Predictive processes simplify the problem by inducing basis functions with a covariance function and a set of knots. The existing literature suggests certain practical implementations of knot selection and covariance estimation; however, theoretical foundations explaining the influence of these two factors on predictive processes are lacking. In this article, the asymptotic prediction performance of the predictive process and Gaussian process predictions are derived and the impacts of the selected knots and estimated covariance are studied. The use of support points as knots, which best represent data locations, is advocated. Extensive simulation studies demonstrate the superiority of support points and verify our theoretical results. Real data of precipitation and ozone are used as examples, and the efficiency of our method over other widely used low-rank approximation methods is verified. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yan Song and Wenlin Dai and Marc G. Genton},
  doi          = {10.1080/01621459.2024.2403188},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1189-1200},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale low-rank gaussian process prediction with support points},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based clustering of categorical data based on the hamming distance. <em>JASA</em>, <em>120</em>(550), 1178-1188. (<a href='https://doi.org/10.1080/01621459.2024.2402568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-based approach is developed for clustering categorical data with no natural ordering. The proposed method exploits the Hamming distance to define a family of probability mass functions to model the data. The elements of this family are then considered as kernels of a finite mixture model with an unknown number of components. Conjugate Bayesian inference has been derived for the parameters of the Hamming distribution model. The mixture is framed in a Bayesian nonparametric setting, and a transdimensional blocked Gibbs sampler is developed to provide full Bayesian inference on the number of clusters, their structure, and the group-specific parameters, facilitating the computation with respect to customary reversible jump algorithms. The proposed model encompasses a parsimonious latent class model as a special case when the number of components is fixed. Model performances are assessed via a simulation study and reference datasets, showing improvements in clustering recovery over existing approaches. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Raffaele Argiento and Edoardo Filippi-Mazzola and Lucia Paci},
  doi          = {10.1080/01621459.2024.2402568},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1178-1188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based clustering of categorical data based on the hamming distance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neyman-pearson multi-class classification via cost-sensitive learning. <em>JASA</em>, <em>120</em>(550), 1164-1177. (<a href='https://doi.org/10.1080/01621459.2024.2402567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package npcs , which is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ye Tian and Yang Feng},
  doi          = {10.1080/01621459.2024.2402567},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1164-1177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neyman-pearson multi-class classification via cost-sensitive learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On optimality of mallows model averaging. <em>JASA</em>, <em>120</em>(550), 1152-1163. (<a href='https://doi.org/10.1080/01621459.2024.2402566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, model averaging (MA) has attracted much attention as it has emerged as an alternative tool to the model selection (MS) statistical approach. Hansen introduced a Mallows model averaging (MMA) method with model weights selected by minimizing a Mallows‚Äô C p criterion. The main theoretical justification for MMA is an asymptotic optimality (AOP), which states that the risk/loss of the resulting MA estimator is asymptotically equivalent to that of the best but infeasible averaged model. MMA‚Äôs AOP is proved in the literature by either constraining weights in a special discrete weight set or limiting the number of candidate models. In this work, it is first shown that under these restrictions, however, the optimal risk of MA becomes an unreachable target, and MMA may converge more slowly than MS. In this background, a foundational issue that has not been addressed is: When a suitably large set of candidate models is considered, and the model weights are not harmfully constrained, can the MMA estimator perform asymptotically as well as the optimal convex combination of the candidate models? We answer this question in both nested and non-nested settings. In the nested setting, we provide finite sample inequalities for the risk of MMA and show that without unnatural restrictions on the candidate models, MMA‚Äôs AOP holds in a general continuous weight set under certain mild conditions. In the non-nested setting, a sufficient condition and a negative result are established for the achievability of the optimal MA risk. Implications on minimax adaptivity are given as well. The results from simulations and real data analysis back up our theoretical findings. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jingfu Peng and Yang Li and Yuhong Yang},
  doi          = {10.1080/01621459.2024.2402566},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1152-1163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On optimality of mallows model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix. <em>JASA</em>, <em>120</em>(550), 1139-1151. (<a href='https://doi.org/10.1080/01621459.2024.2402565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the number of factors in high-dimensional factor modeling is essential but challenging, especially when the data are heavy-tailed. In this article, we introduce a new estimator based on the spectral properties of Spearman sample correlation matrix under the high-dimensional setting, where both dimension and sample size tend to infinity proportionally. Our estimator is robust against heavy tails in either the common factors or idiosyncratic errors. The consistency of our estimator is established under mild conditions. Numerical experiments demonstrate the superiority of our estimator compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiaxin Qiu and Zeng Li and Jianfeng Yao},
  doi          = {10.1080/01621459.2024.2402565},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1139-1151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid inference after causal discovery. <em>JASA</em>, <em>120</em>(550), 1127-1138. (<a href='https://doi.org/10.1080/01621459.2024.2402089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to ‚Äúdouble dipping,‚Äù invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while allowing for a trade-off between causal discovery accuracy and confidence interval width. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Paula Gradu and Tijana Zrnic and Yixin Wang and Michael I. Jordan},
  doi          = {10.1080/01621459.2024.2402089},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1127-1138},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Valid inference after causal discovery},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving tensor regression by optimal model averaging. <em>JASA</em>, <em>120</em>(550), 1115-1126. (<a href='https://doi.org/10.1080/01621459.2024.2398164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors have broad applications in neuroimaging, data mining, digital marketing, etc. CANDECOMP/PARAFAC (CP) tensor decomposition can effectively reduce the number of parameters to gain dimensionality-reduction and thus plays a key role in tensor regression. However, in CP decomposition, there is uncertainty about which rank to use. In this article, we develop a model averaging method to handle this uncertainty by weighting the estimators from candidate tensor regression models with different ranks. When all candidate models are misspecified, we prove that the model averaging estimator is asymptotically optimal. When correct models are included in the set of candidate models, we prove the consistency of parameters and the convergence of the model averaging weight. Simulations and empirical studies illustrate that the proposed method has superiority over the competition methods and has promising applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qiushi Bu and Hua Liang and Xinyu Zhang and Jiahui Zou},
  doi          = {10.1080/01621459.2024.2398164},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1115-1126},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improving tensor regression by optimal model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-policy evaluation in doubly inhomogeneous environments. <em>JASA</em>, <em>120</em>(550), 1102-1114. (<a href='https://doi.org/10.1080/01621459.2024.2395593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions‚Äîtemporal stationarity and individual homogeneity are both violated. To handle the ‚Äúdouble inhomogeneities‚Äù, we propose a class of latent factor models for the reward and transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first article that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms state-of-the-art methods. Finally, we illustrate our method on a dataset from the Medical Information Mart for Intensive Care. An R implementation of the proposed procedure is available at https://github.com/ZeyuBian/2FEOPE . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zeyu Bian and Chengchun Shi and Zhengling Qi and Lan Wang},
  doi          = {10.1080/01621459.2024.2395593},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1102-1114},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Off-policy evaluation in doubly inhomogeneous environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based causal feature selection for general response types. <em>JASA</em>, <em>120</em>(550), 1090-1101. (<a href='https://doi.org/10.1080/01621459.2024.2395588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships from observational data is a fundamental yet challenging task. Invariant causal prediction (ICP, Peters, B√ºhlmann, and Meinshausen) is a method for causal feature selection which requires data from heterogeneous settings and exploits that causal models are invariant. ICP has been extended to general additive noise models and to nonparametric settings using conditional independence tests. However, the latter often suffer from low power (or poor Type I error control) and additive noise models are not suitable for applications in which the response is not measured on a continuous scale, but reflects categories or counts. Here, we develop transformation-model ( tram ) based ICP, allowing for continuous, categorical, count-type, and uninformatively censored responses (these model classes, generally, do not allow for identifiability when there is no exogenous heterogeneity). As an invariance test, we propose tram -GCM based on the expected conditional covariance between environments and score residuals with uniform asymptotic level guarantees. For the special case of linear shift tram s, we also consider tram -Wald, which tests invariance based on the Wald statistic. We provide an open-source R package tramicp and evaluate our approach on simulated data and in a case study investigating causal features of survival in critically ill patients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Lucas Kook and Sorawit Saengkyongam and Anton Rask Lundborg and Torsten Hothorn and Jonas Peters},
  doi          = {10.1080/01621459.2024.2395588},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1090-1101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based causal feature selection for general response types},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process. <em>JASA</em>, <em>120</em>(550), 1077-1089. (<a href='https://doi.org/10.1080/01621459.2024.2395587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but resample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Akihiko Nishimura and Zhenyu Zhang and Marc A. Suchard},
  doi          = {10.1080/01621459.2024.2395587},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1077-1089},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monte carlo inference for semiparametric bayesian regression. <em>JASA</em>, <em>120</em>(550), 1063-1076. (<a href='https://doi.org/10.1080/01621459.2024.2395586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transformations are essential for broad applicability of parametric regression models. However, for Bayesian analysis, joint inference of the transformation and model parameters typically involves restrictive parametric transformations or nonparametric representations that are computationally inefficient and cumbersome for implementation and theoretical analysis, which limits their usability in practice. This article introduces a simple, general, and efficient strategy for joint posterior inference of an unknown transformation and all regression model parameters. The proposed approach directly targets the posterior distribution of the transformation by linking it with the marginal distributions of the independent and dependent variables, and then deploys a Bayesian nonparametric model via the Bayesian bootstrap. Crucially, this approach delivers (a) joint posterior consistency under general conditions, including multiple model misspecifications, and (b) efficient Monte Carlo (not Markov chain Monte Carlo) inference for the transformation and all parameters for important special cases. These tools apply across a variety of data domains, including real-valued, positive, and compactly-supported data. Simulation studies and an empirical application demonstrate the effectiveness and efficiency of this strategy for semiparametric Bayesian analysis with linear models, quantile regression, and Gaussian processes. The R package SeBR is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Daniel R. Kowal and Bohan Wu},
  doi          = {10.1080/01621459.2024.2395586},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1063-1076},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Monte carlo inference for semiparametric bayesian regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network pairwise comparison. <em>JASA</em>, <em>120</em>(550), 1048-1062. (<a href='https://doi.org/10.1080/01621459.2024.2393471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the problem of two-sample network hypothesis testing: given two networks with the same set of nodes, we wish to test whether the underlying Bernoulli probability matrices of the two networks are the same or not. We propose Interlacing Balance Measure (IBM) as a new two-sample testing approach. We consider the Degree-Corrected Mixed-Membership (DCMM) model for undirected networks, where we allow severe degree heterogeneity, mixed-memberships, flexible sparsity levels, and weak signals. In such a broad setting, how to find a test that has a tractable limiting null and optimal testing performances is a challenging problem. We show that IBM is such a test: in a broad DCMM setting with only mild regularity conditions, IBM has N ( 0 , 1 ) as the limiting null and achieves the optimal phase transition. While the above is for undirected networks, IBM is a unified approach and is directly implementable for directed networks. For a broad directed-DCMM (extension of DCMM for directed networks) setting, we show that IBM has N ( 0 , 1 / 2 ) as the limiting null and continues to achieve the optimal phase transition. We have also applied IBM to the Enron email network and a gene co-expression network, with interesting results. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiashun Jin and Zheng Tracy Ke and Shengming Luo and Yucong Ma},
  doi          = {10.1080/01621459.2024.2393471},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1048-1062},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network pairwise comparison},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised triply robust inductive transfer learning. <em>JASA</em>, <em>120</em>(550), 1037-1047. (<a href='https://doi.org/10.1080/01621459.2024.2393463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a Semi-supervised Triply Robust Inductive transFer LEarning (STRIFLE) approach, which integrates heterogeneous data from a label-rich source population and a label-scarce target population and uses a large amount of unlabeled data simultaneously to improve the learning accuracy in the target population. Specifically, we consider a high dimensional covariate shift setting and employ two nuisance models, a density ratio model and an imputation model, to combine transfer learning and surrogate-assisted semi-supervised learning strategies effectively and achieve triple robustness. While the STRIFLE approach assumes the target and source populations to share the same conditional distribution of outcome Y given both the surrogate features S and predictors X , it allows the true underlying model of Y‚èß X to differ between the two populations due to the potential covariate shift in S and X . Different from double robustness, even if both nuisance models are misspecified or the distribution of Y‚èß S , X is not the same between the two populations when the shifted source population and the target population share enough similarities, the triply robust STRIFLE estimator can still partially use the source population when the shifted source population and the target population share enough similarities. Moreover, it is guaranteed to be no worse than the target-only surrogate-assisted semi-supervised estimator with an additional error term from transferability detection. These desirable properties of our estimator are established theoretically and verified in finite samples via extensive simulation studies. We use the STRIFLE estimator to train a Type II diabetes polygenic risk prediction model for the African American target population by transferring knowledge from electronic health records linked genomic data observed in a larger European source population. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Tianxi Cai and Mengyan Li and Molei Liu},
  doi          = {10.1080/01621459.2024.2393463},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1037-1047},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semi-supervised triply robust inductive transfer learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Euclidean mirrors and dynamics in network time series. <em>JASA</em>, <em>120</em>(550), 1025-1036. (<a href='https://doi.org/10.1080/01621459.2024.2392912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing changes in network evolution is central to statistical network inference. We consider a dynamic network model in which each node has an associated time-varying low-dimensional latent vector of feature data, and connection probabilities are functions of these vectors. Under mild assumptions, the evolution of latent vectors exhibits low-dimensional manifold structure under a suitable distance. This distance can be approximated by a measure of separation between the observed networks themselves, and there exist Euclidean representations for underlying network structure, as characterized by this distance. These Euclidean representations, called Euclidean mirrors, permit the visualization of network dynamics and lead to methods for change point and anomaly detection in networks. We illustrate our methodology with real and synthetic data, and identify change points corresponding to massive shifts in pandemic policies in a communication network of a large organization. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Avanti Athreya and Zachary Lubberts and Youngser Park and Carey Priebe},
  doi          = {10.1080/01621459.2024.2392912},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1025-1036},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Euclidean mirrors and dynamics in network time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for networks of high-dimensional point processes. <em>JASA</em>, <em>120</em>(550), 1014-1024. (<a href='https://doi.org/10.1080/01621459.2024.2392907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled in part by recent applications in neuroscience, the multivariate Hawkes process has become a popular tool for modeling the network of interactions among high-dimensional point process data. While evaluating the uncertainty of the network estimates is critical in scientific applications, existing methodological and theoretical work has primarily addressed estimation. To bridge this gap, we develop a new statistical inference procedure for high-dimensional Hawkes processes. The key ingredient for the inference procedure is a new concentration inequality on the first- and second-order statistics for integrated stochastic processes, which summarize the entire history of the process. Combining recent martingale central limit theorem with the new concentration inequality, we then characterize the convergence rate of the test statistics in a continuous time domain. Finally, to account for potential non-stationarity of the process in practice, we extend our statistical inference procedure to a flexible class of Hawkes processes with time-varying background intensities and unknown transition functions. The finite sample validity of the inferential tools is illustrated via extensive simulations and further applied to a neuron spike train dataset. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Wang and Mladen Kolar and Ali Shojaie},
  doi          = {10.1080/01621459.2024.2392907},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1014-1024},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for networks of high-dimensional point processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust regression with covariate filtering: Heavy tails and adversarial contamination. <em>JASA</em>, <em>120</em>(550), 1002-1013. (<a href='https://doi.org/10.1080/01621459.2024.2392906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of linear regression where both covariates and responses are potentially (i) heavy-tailed and (ii) adversarially contaminated. Several computationally efficient estimators have been proposed for the simpler setting where the covariates are sub-Gaussian and uncontaminated; however, these estimators may fail when the covariates are either heavy-tailed or contain outliers. In this work, we show how to modify the Huber regression, least trimmed squares, and least absolute deviation estimators to obtain estimators which are simultaneously computationally and statistically efficient in the stronger contamination model. Our approach is quite simple, and consists of applying a filtering algorithm to the covariates, and then applying the classical robust regression estimators to the remaining data. We show that the Huber regression estimator achieves near-optimal error rates in this setting, whereas the least trimmed squares and least absolute deviation estimators can be made to achieve near-optimal error after applying a postprocessing step. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ankit Pensia and Varun Jog and Po-Ling Loh},
  doi          = {10.1080/01621459.2024.2392906},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1002-1013},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust regression with covariate filtering: Heavy tails and adversarial contamination},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural gradient variational bayes without fisher matrix analytic calculation and its inversion. <em>JASA</em>, <em>120</em>(550), 990-1001. (<a href='https://doi.org/10.1080/01621459.2024.2392904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a method for efficiently approximating the inverse of the Fisher information matrix, a crucial step in achieving effective variational Bayes inference. A notable aspect of our approach is the avoidance of analytically computing the Fisher information matrix and its explicit inversion. Instead, we introduce an iterative procedure for generating a sequence of matrices that converge to the inverse of Fisher information. The natural gradient variational Bayes algorithm without analytic expression of the Fisher matrix and its inversion is provably convergent and achieves a convergence rate of order O ( log s / s ) , with s the number of iterations. We also obtain a central limit theorem for the iterates. Implementation of our method does not require storage of large matrices, and achieves a linear complexity in the number of variational parameters. Our algorithm exhibits versatility, making it applicable across a diverse array of variational Bayes domains, including Gaussian approximation and normalizing flow Variational Bayes. We offer a range of numerical examples to demonstrate the efficiency and reliability of the proposed variational Bayes method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {A. Godichon-Baggioni and D. Nguyen and M.-N. Tran},
  doi          = {10.1080/01621459.2024.2392904},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {990-1001},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Natural gradient variational bayes without fisher matrix analytic calculation and its inversion},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity. <em>JASA</em>, <em>120</em>(550), 976-989. (<a href='https://doi.org/10.1080/01621459.2024.2392903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity is a challenging issue for modern statistical data analysis. There are different types of data heterogeneity in practice. In this article, we consider potential structural changes and complicated tail distributions. There are various existing methods proposed to handle either structural changes or heteroscedasticity. However, it is difficult to handle them simultaneously. To overcome this limitation, we consider statistically and computationally efficient change point detection and localization in high-dimensional quantile regression models. Our proposed framework is general and flexible since the change points and the underlying regression coefficients are allowed to vary across different quantile levels. The model parameters, including the data dimension, the number of change points, and the signal jump size, can be scaled with the sample size. Under this framework, we construct a novel two-step estimation of the number and locations of the change points as well as the underlying regression coefficients. Without any moment constraints on the error term, we present theoretical results, including consistency of the change point number, oracle estimation of change point locations, and estimation for the underlying regression coefficients with the optimal convergence rate. Finally, we present simulation results and an application to the S&P 100 dataset to demonstrate the advantage of the proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xianru Wang and Bin Liu and Xinsheng Zhang and Yufeng Liu},
  doi          = {10.1080/01621459.2024.2392903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {976-989},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel sampling of decomposable graphs using markov chains on junction trees. <em>JASA</em>, <em>120</em>(550), 963-975. (<a href='https://doi.org/10.1080/01621459.2024.2388908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference for undirected graphical models is mostly restricted to the class of decomposable graphs, as they enjoy a rich set of properties making them amenable to high-dimensional problems. While parameter inference is straightforward in this setup, inferring the underlying graph is a challenge driven by the computational difficulty in exploring the space of decomposable graphs. This work makes two contributions to address this problem. First, we provide sufficient and necessary conditions for when multi-edge perturbations maintain decomposability of the graph. Using these, we characterize a simple class of partitions that efficiently classify all edge perturbations by whether they maintain decomposability. Second, we propose a novel parallel nonreversible Markov chain Monte Carlo sampler for distributions over junction tree representations of the graph. At every step, the parallel sampler executes simultaneously all edge perturbations within a partition. Through simulations, we demonstrate the efficiency of our new edge perturbation conditions and class of partitions. We find that our parallel sampler yields improved mixing properties in comparison to the single-move variate, and outperforms current state-of-the-art methods in terms of accuracy and computational efficiency. The implementation of our work is available in the Python package parallelDG. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Mohamad Elmasri},
  doi          = {10.1080/01621459.2024.2388908},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {963-975},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Parallel sampling of decomposable graphs using markov chains on junction trees},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network membership estimation under severe degree heterogeneity. <em>JASA</em>, <em>120</em>(550), 948-962. (<a href='https://doi.org/10.1080/01621459.2024.2388903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real networks often have severe degree heterogeneity, with maximum, average, and minimum node degrees differing significantly. This article examines the impact of degree heterogeneity on statistical limits of network data analysis. Introducing the heterogeneity distribution (HD) under a degree-corrected mixed membership model, we show that the optimal rate of mixed membership estimation is an explicit functional of the HD. This result confirms that severe degree heterogeneity decelerates the error rate, even when the overall sparsity remains unchanged. To obtain a rate-optimal method, we modify an existing spectral algorithm, Mixed-SCORE, by adding a pre-PCA normalization step. This step normalizes the adjacency matrix by a diagonal matrix consisting of the b th power of node degrees, for some b ‚àà R . We discover that b =‚Äâ1/2 is universally favorable. The resulting spectral algorithm is rate-optimal for networks with arbitrary degree heterogeneity. A technical component in our proofs is entry-wise eigenvector analysis of the normalized graph Laplacian. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zheng Tracy Ke and Jingming Wang},
  doi          = {10.1080/01621459.2024.2388903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {948-962},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network membership estimation under severe degree heterogeneity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling the false split rate in tree-based aggregation. <em>JASA</em>, <em>120</em>(550), 935-947. (<a href='https://doi.org/10.1080/01621459.2024.2376285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many domains, data measurements can naturally be associated with the leaves of a tree, expressing the relationships among these measurements. For example, companies belong to industries, which in turn belong to ever coarser divisions such as sectors; microbes are commonly arranged in a taxonomic hierarchy from species to kingdoms; street blocks belong to neighborhoods, which in turn belong to larger-scale regions. The problem of tree-based aggregation that we consider in this article asks which of these tree-defined subgroups of leaves should really be treated as a single entity and which of these entities should be distinguished from each other. We introduce the false split rate , an error measure that describes the degree to which subgroups have been split when they should not have been. While expressible as the false discovery rate in a special case, we show that these measures can be quite different for the general tree structures common in our setting. We then propose a multiple hypothesis testing algorithm for tree-based aggregation, which we prove controls this error measure. We focus on two main examples of tree-based aggregation, one which involves aggregating means and the other hich involves aggregating regression coefficients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Simeng Shao and Jacob Bien and Adel Javanmard},
  doi          = {10.1080/01621459.2024.2376285},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {935-947},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlling the false split rate in tree-based aggregation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust matrix completion with heavy-tailed noise. <em>JASA</em>, <em>120</em>(550), 922-934. (<a href='https://doi.org/10.1080/01621459.2024.2375037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies noisy low-rank matrix completion in the presence of heavy-tailed and possibly asymmetric noise, where we aim to estimate an underlying low-rank matrix given a set of highly incomplete noisy entries. Though the matrix completion problem has attracted much attention in the past decade, there is still lack of theoretical understanding when the observations are contaminated by heavy-tailed noises. Prior theory falls short of explaining the empirical results and is unable to capture the optimal dependence of the estimation error on the noise level. In this article, we adopt an adaptive Huber loss to accommodate heavy-tailed noise, which is robust against large and possibly asymmetric errors when the parameter in the Huber loss function is carefully designed to balance the Huberization biases and robustness to outliers. Then, we propose an efficient nonconvex algorithm via a balanced low-rank Burer-Monteiro matrix factorization and gradient descent with robust spectral initialization. We prove that under merely a bounded second-moment condition on the error distributions, rather than the sub-Gaussian assumption, the Euclidean errors of the iterates generated by the proposed algorithm decrease geometrically fast until achieving a minimax-optimal statistical estimation error, which has the same order as that in the sub-Gaussian case. The key technique behind this significant advancement is a powerful leave-one-out analysis framework. The theoretical results are corroborated by our numerical studies. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bingyan Wang and Jianqing Fan},
  doi          = {10.1080/01621459.2024.2375037},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {922-934},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust matrix completion with heavy-tailed noise},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for H√ºsler‚ÄìReiss graphical models through matrix completions. <em>JASA</em>, <em>120</em>(550), 909-921. (<a href='https://doi.org/10.1080/01621459.2024.2371978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severity of multivariate extreme events is driven by the dependence between the largest marginal observations. The H√ºsler‚ÄìReiss distribution is a versatile model for this extremal dependence, and it is usually parameterized by a variogram matrix. In order to represent conditional independence relations and obtain sparse parameterizations, we introduce the novel H√ºsler‚ÄìReiss precision matrix. Similarly to the Gaussian case, this matrix appears naturally in density representations of the H√ºsler‚ÄìReiss Pareto distribution and encodes the extremal graphical structure through its zero pattern. For a given, arbitrary graph we prove the existence and uniqueness of the completion of a partially specified H√ºsler‚ÄìReiss variogram matrix so that its precision matrix has zeros on non-edges in the graph. Using suitable estimators for the parameters on the edges, our theory provides the first consistent estimator of graph structured H√ºsler‚ÄìReiss distributions. If the graph is unknown, our method can be combined with recent structure learning algorithms to jointly infer the graph and the corresponding parameter matrix. Based on our methodology, we propose new tools for statistical inference of sparse H√ºsler‚ÄìReiss models and illustrate them on large flight delay data in the United States, as well as Danube river flow data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Manuel Hentschel and Sebastian Engelke and Johan Segers},
  doi          = {10.1080/01621459.2024.2371978},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {909-921},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for H√ºsler‚ÄìReiss graphical models through matrix completions},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual dynamic pricing with strategic buyers. <em>JASA</em>, <em>120</em>(550), 896-908. (<a href='https://doi.org/10.1080/01621459.2024.2370613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this article, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer‚Äôs true feature, but a manipulated feature according to buyers‚Äô strategic behavior. In addition, the seller does not observe the buyers‚Äô valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers‚Äô strategic behavior into the online learning to maximize the seller‚Äôs cumulative revenue. We first prove that existing nonstrategic pricing policies that neglect the buyers‚Äô strategic behavior result in a linear Œ© ( T ) regret with T the total time horizon, indicating that these policies are not better than a random pricing policy. We then establish an O ( T ) regret upper bound of our proposed policy and an Œ© ( T ) regret lower bound for any pricing policy within our problem setting. This underscores the rate optimality of our policy. Importantly, our policy is not a mere amalgamation of existing dynamic pricing policies and strategic behavior handling algorithms. Our policy can also accommodate the scenario when the marginal cost of manipulation is unknown in advance. To account for it, we simultaneously estimate the valuation parameter and the cost parameter in the online pricing policy, which is shown to also achieve an O ( T ) regret bound. Extensive experiments support our theoretical developments and demonstrate the superior performance of our policy compared to other pricing policies that are unaware of the strategic behaviors. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pangpang Liu and Zhuoran Yang and Zhaoran Wang and Will Wei Sun},
  doi          = {10.1080/01621459.2024.2370613},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {896-908},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Contextual dynamic pricing with strategic buyers},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic likelihood in misspecified models. <em>JASA</em>, <em>120</em>(550), 884-895. (<a href='https://doi.org/10.1080/01621459.2024.2370594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian synthetic likelihood is a widely used approach for conducting Bayesian analysis in complex models where evaluation of the likelihood is infeasible but simulation from the assumed model is tractable. We analyze the behavior of the Bayesian synthetic likelihood posterior when the assumed model differs from the actual data generating process. We demonstrate that the Bayesian synthetic likelihood posterior can display a wide range of nonstandard behaviors depending on the level of model misspecification, including multimodality and asymptotic non-Gaussianity. Our results suggest that likelihood tempering, a common approach for robust Bayesian inference, fails for synthetic likelihood whilst recently proposed robust synthetic likelihood approaches can ameliorate this behavior and deliver reliable posterior inference under model misspecification. All results are illustrated using a simple running example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {David T. Frazier and David J. Nott and Christopher Drovandi},
  doi          = {10.1080/01621459.2024.2370594},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {884-895},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Synthetic likelihood in misspecified models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised dynamic PCA: Linear dynamic forecasting with many predictors. <em>JASA</em>, <em>120</em>(550), 869-883. (<a href='https://doi.org/10.1080/01621459.2024.2370592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel dynamic forecasting method using a new supervised Principal Component Analysis (PCA) when a large number of predictors are available. The new supervised PCA provides an effective way to bridge the gap between predictors and the target variable of interest by scaling and combining the predictors and their lagged values, resulting in an effective dynamic forecasting. Unlike the traditional diffusion-index approach, which does not learn the relationships between the predictors and the target variable before conducting PCA, we first rescale each predictor according to their significance in forecasting the targeted variable in a dynamic fashion, and a PCA is then applied to a rescaled and additive panel, which establishes a connection between the predictability of the PCA factors and the target variable. We also propose to use penalized methods such as the LASSO to select the significant factors that have superior predictive power over the others. Theoretically, we show that our estimators are consistent and outperform the traditional methods in prediction under some mild conditions. We conduct extensive simulations to verify that the proposed method produces satisfactory forecasting results and outperforms most of the existing methods using the traditional PCA. An example of predicting U.S. macroeconomic variables using a large number of predictors showcases that our method fares better than most of the existing ones in applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zhaoxing Gao and Ruey S. Tsay},
  doi          = {10.1080/01621459.2024.2370592},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {869-883},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Supervised dynamic PCA: Linear dynamic forecasting with many predictors},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced response envelope via envelope regularization. <em>JASA</em>, <em>120</em>(550), 859-868. (<a href='https://doi.org/10.1080/01621459.2024.2368844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The response envelope model provides substantial efficiency gains over the standard multivariate linear regression by identifying the material part of the response to the model and by excluding the immaterial part. In this article, we propose the enhanced response envelope by incorporating a novel envelope regularization term based on a nonconvex manifold formulation. It is shown that the enhanced response envelope can yield better prediction risk than the original envelope estimator. The enhanced response envelope naturally handles high-dimensional data for which the original response envelope is not serviceable without necessary remedies. In an asymptotic high-dimensional regime where the ratio of the number of predictors over the number of samples converges to a nonzero constant, we characterize the risk function and reveal an interesting double descent phenomenon for the envelope model. A simulation study confirms our main theoretical findings. Simulations and real data applications demonstrate that the enhanced response envelope does have significantly improved prediction performance over the original envelope method, especially when the number of predictors is close to or moderately larger than the number of samples. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Oh-Ran Kwon and Hui Zou},
  doi          = {10.1080/01621459.2024.2368844},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {859-868},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Enhanced response envelope via envelope regularization},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tyranny-of-the-minority regression adjustment in randomized experiments. <em>JASA</em>, <em>120</em>(550), 846-858. (<a href='https://doi.org/10.1080/01621459.2024.2366043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression adjustment is widely used in the analysis of randomized experiments to improve the estimation efficiency of the treatment effect. This article reexamines a weighted regression adjustment method termed t yranny- o f-the- m inority (ToM), wherein units in the minority group are given greater weights. We demonstrate that ToM regression adjustment is more robust than Lin ‚Äôs regression adjustment with treatment-covariate interactions, even though these two regression adjustment methods are asymptotically equivalent in completely randomized experiments. Moreover, ToM regression adjustment can be easily extended to stratified randomized experiments and completely randomized survey experiments. We obtain the design-based properties of the ToM regression-adjusted average treatment effect estimator under such designs. In particular, we show that the ToM regression-adjusted estimator improves the asymptotic estimation efficiency compared to the unadjusted estimator, even when the regression model is misspecified, and is optimal in the class of linearly adjusted estimators. We also study the asymptotic properties of various heteroscedasticity-robust standard errors and provide recommendations for practitioners. Simulation studies and real data analysis demonstrate ToM regression adjustment‚Äôs superiority over existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xin Lu and Hanzhong Liu},
  doi          = {10.1080/01621459.2024.2366043},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {846-858},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tyranny-of-the-minority regression adjustment in randomized experiments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test and measure for partial mean dependence based on machine learning methods. <em>JASA</em>, <em>120</em>(550), 833-845. (<a href='https://doi.org/10.1080/01621459.2024.2366030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of importance to investigate the significance of a subset of covariates W for the response Y given covariates Z in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard Chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of Y given W after controlling for the nonlinear effect of Z . We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root- N convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates Z , we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Leheng Cai and Xu Guo and Wei Zhong},
  doi          = {10.1080/01621459.2024.2366030},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {833-845},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Test and measure for partial mean dependence based on machine learning methods},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric multiple-output center-outward quantile regression. <em>JASA</em>, <em>120</em>(550), 818-832. (<a href='https://doi.org/10.1080/01621459.2024.2366029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on recent measure-transportation-based concepts of multivariate quantiles, we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content, the graphs of which constitute nested center-outward quantile regression tubes with given unconditional probability content; these (conditional and unconditional) probability contents do not depend on the underlying distribution‚Äîan essential property of quantile concepts. Empirical counterparts of these concepts are constructed, yielding interpretable empirical contours, regions, and tubes which are shown to consistently reconstruct (in the Pompeiu-Hausdorff topology) their population versions. Our method is entirely nonparametric and performs well in simulations‚Äîwith possible heteroscedasticity and nonlinear trends. Its potential as a data-analytic tool is illustrated on some real datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Eustasio del Barrio and Alberto Gonz√°lez Sanz and Marc Hallin},
  doi          = {10.1080/01621459.2024.2366029},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {818-832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric multiple-output center-outward quantile regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values. <em>JASA</em>, <em>120</em>(550), 805-817. (<a href='https://doi.org/10.1080/01621459.2024.2359739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing, as their validity often requires the deployment of symmetric decision rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and demonstrate improvements in power compared to existing model-free methods in various scenarios. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zinan Zhao and Wenguang Sun},
  doi          = {10.1080/01621459.2024.2359739},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {805-817},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with the mediator and outcome missing not at random. <em>JASA</em>, <em>120</em>(550), 794-804. (<a href='https://doi.org/10.1080/01621459.2024.2359132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is widely used for investigating direct and indirect causal pathways through which an effect arises. However, many mediation analysis studies are challenged by missingness in the mediator and outcome. In general, when the mediator and outcome are missing not at random, the direct and indirect effects are not identifiable without further assumptions. We study the identifiability of the direct and indirect effects under some interpretable mechanisms that allow for missing not at random in the mediator and outcome. We evaluate the performance of statistical inference under those mechanisms through simulation studies and illustrate the proposed methods via the National Job Corps Study. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuozhi Zuo and Debashis Ghosh and Peng Ding and Fan Yang},
  doi          = {10.1080/01621459.2024.2359132},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {794-804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mediation analysis with the mediator and outcome missing not at random},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients. <em>JASA</em>, <em>120</em>(550), 779-793. (<a href='https://doi.org/10.1080/01621459.2024.2359131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial heterogeneity is of great importance in social, economic, and environmental science studies. The spatially varying coefficient model is a popular and effective spatial regression technique to address spatial heterogeneity. However, accounting for heterogeneity comes at the cost of reducing model parsimony. To balance flexibility and parsimony, this article develops a class of generalized partially linear spatially varying coefficient models which allow the inclusion of both constant and spatially varying effects of covariates. Another significant challenge in many applications comes from the enormous size of the spatial datasets collected from modern technologies. To tackle this challenge, we design a novel distributed heterogeneity learning (DHL) method based on bivariate spline smoothing over a triangulation of the domain. The proposed DHL algorithm has a simple, scalable, and communication-efficient implementation scheme that can almost achieve linear speedup. In addition, this article provides rigorous theoretical support for the DHL framework. We prove that the DHL constant coefficient estimators are asymptotic normal and the DHL spline estimators reach the same convergence rate as the global spline estimators obtained using the entire dataset. The proposed DHL method is evaluated through extensive simulation studies and analyses of U.S. loan application data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shan Yu and Guannan Wang and Li Wang},
  doi          = {10.1080/01621459.2024.2359131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {779-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node-level community detection within edge exchangeable models for interaction processes. <em>JASA</em>, <em>120</em>(550), 764-778. (<a href='https://doi.org/10.1080/01621459.2024.2358560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists are increasingly interested in discovering community structure from modern relational data arising on large-scale social networks. While many methods have been proposed for learning community structure, few account for the fact that these modern networks arise from processes of interactions in the population. We introduce block edge exchangeable models (BEEM) for the study of interaction networks with latent node-level community structure. The block vertex components model (B-VCM) is derived as a canonical example. Several theoretical and practical advantages over traditional vertex-centric approaches are highlighted. In particular, BEEMs allow for sparse degree structure and power-law degree distributions within communities. Our theoretical analysis bounds the misspecification rate of block assignments while supporting simulations show the properties of the network can be recovered. A computationally tractable Gibbs algorithm is derived. We demonstrate the proposed model using post-comment interaction data from Talklife, a large-scale online peer-to-peer support network, and contrast the learned communities from those using standard algorithms including degree-corrected stochastic block models, popularity-adjusted block models, and weighted stochastic block models. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuhua Zhang and Walter Dempsey},
  doi          = {10.1080/01621459.2024.2358560},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {764-778},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Node-level community detection within edge exchangeable models for interaction processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Population-level balance in signed networks. <em>JASA</em>, <em>120</em>(550), 751-763. (<a href='https://doi.org/10.1080/01621459.2024.2356894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical network models are useful for understanding the underlying formation mechanism and characteristics of complex networks. However, statistical models for signed networks have been largely unexplored. In signed networks, there exist both positive (e.g., like, trust) and negative (e.g., dislike, distrust) edges, which are commonly seen in real-world scenarios. The positive and negative edges in signed networks lead to unique structural patterns, which pose challenges for statistical modeling. In this article, we introduce a statistically principled latent space approach for modeling signed networks and accommodating the well-known balance theory , that is, ‚Äúthe enemy of my enemy is my friend‚Äù and ‚Äúthe friend of my friend is my friend.‚Äù The proposed approach treats both edges and their signs as random variables, and characterizes the balance theory with a novel and natural notion of population-level balance. This approach guides us towards building a class of balanced inner-product models, and toward developing scalable algorithms via projected gradient descent to estimate the latent variables. We also establish non-asymptotic error rates for the estimates, which are further verified through simulation studies. In addition, we apply the proposed approach to an international relation network, which provides an informative and interpretable model-based visualization of countries during World War II. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Weijing Tang and Ji Zhu},
  doi          = {10.1080/01621459.2024.2356894},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {751-763},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Population-level balance in signed networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rate-optimal rank aggregation with private pairwise rankings. <em>JASA</em>, <em>120</em>(550), 737-750. (<a href='https://doi.org/10.1080/01621459.2025.2484843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various real-world scenarios, such as recommender systems and political surveys, pairwise rankings are commonly collected and used for rank aggregation to derive an overall ranking of items. However, preference rankings can reveal individuals‚Äô personal preferences, highlighting the need to protect them from exposure in downstream analysis. In this article, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from a general comparison model. A common privacy protection strategy in practice is the use of the randomized response mechanism to perturb raw pairwise rankings. However, a critical challenge arises because the privatized rankings no longer adhere to the original model, resulting in significant bias in downstream rank aggregation tasks. To address this, we propose an adaptive debiasing method for rankings from the randomized response mechanism, ensuring consistent estimation of true preferences and enhancing the utility of downstream rank aggregation. Theoretically, we provide insights into the relationship between overall privacy guarantees and estimation errors in private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection affects the specification of top- K item sets and complete rankings. Our findings are validated through extensive simulations and a real-world application. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shirong Xu and Will Wei Sun and Guang Cheng},
  doi          = {10.1080/01621459.2025.2484843},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {737-750},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rate-optimal rank aggregation with private pairwise rankings},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse bayesian group factor model for feature interactions in multiple count tables data. <em>JASA</em>, <em>120</em>(550), 723-736. (<a href='https://doi.org/10.1080/01621459.2025.2449721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group factor models have been developed to infer relationships between multiple co-occurring multivariate continuous responses. Motivated by complex count data from multi-domain microbiome studies using next-generation sequencing, we develop a sparse Bayesian group factor model (Sp-BGFM) for multiple count table data that captures the interaction between microorganisms in different domains. Sp-BGFM uses a rounded kernel mixture model using a Dirichlet process (DP) prior with log-normal mixture kernels for count vectors. A group factor model is used to model the covariance matrix of the mixing kernel that describes microorganism interaction. We construct a Dirichlet-Horseshoe (Dir-HS) shrinkage prior and use it as a joint prior for factor loading vectors. Joint sparsity induced by a Dir-HS prior greatly improves the performance in high-dimensional applications. We further model the effects of covariates on microbial abundances using regression. The semiparametric model flexibly accommodates large variability in observed counts and excess zero counts and provides a basis for robust estimation of the interaction and covariate effects. We evaluate Sp-BGFM using simulation studies and real data analysis, comparing it to popular alternatives. Our results highlight the necessity of joint sparsity induced by the Dir-HS prior, and the benefits of a flexible DP model for baseline abundances. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuangjie Zhang and Yuning Shen and Irene A. Chen and Juhee Lee},
  doi          = {10.1080/01621459.2025.2449721},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {723-736},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse bayesian group factor model for feature interactions in multiple count tables data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoWarp: Warped spatial processes for inferring subsea sediment properties. <em>JASA</em>, <em>120</em>(550), 710-722. (<a href='https://doi.org/10.1080/01621459.2024.2445874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For offshore structures like wind turbines, subsea infrastructure, pipelines, and cables, it is crucial to quantify the properties of the seabed sediments at a proposed site. However, data collection offshore is costly, so analysis of the seabed sediments must be made from measurements that are spatially sparse. Adding to this challenge, the structure of the seabed sediments exhibits both nonstationarity and anisotropy. To address these issues, we propose GeoWarp, a hierarchical spatial statistical modeling framework for inferring the 3-D geotechnical properties of subsea sediments. GeoWarp decomposes the seabed properties into a region-wide vertical mean profile (modeled using B-splines), and a nonstationary 3-D spatial Gaussian process. Process nonstationarity and anisotropy are accommodated by warping space in three dimensions and by allowing the process variance to change with depth. We apply GeoWarp to measurements of the seabed made using cone penetrometer tests (CPTs) at six sites on the North West Shelf of Australia. We show that GeoWarp captures the complex spatial distribution of the sediment properties, and produces realistic 3-D simulations suitable for downstream engineering analyses. Through cross-validation, we show that GeoWarp has predictive performance superior to other state-of-the-art methods, demonstrating its value as a tool in offshore geotechnical engineering. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Michael Bertolacci and Andrew Zammit-Mangion and Juan Valderrama Giraldo and Michael O‚ÄôNeill and Fraser Bransby and Phil Watson},
  doi          = {10.1080/01621459.2024.2445874},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {710-722},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {GeoWarp: Warped spatial processes for inferring subsea sediment properties},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk. <em>JASA</em>, <em>120</em>(550), 698-709. (<a href='https://doi.org/10.1080/01621459.2024.2441519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Does having firearms in the home increase suicide risk? To test this hypothesis, a matched case-control study can be performed, in which suicide case subjects are compared to living controls who are similar in observed covariates in terms of their retrospective exposure to firearms at home. In this application, cases can be defined using a broad case definition (suicide) or a narrow case definition (suicide occurred at home). The broad case definition offers a larger number of cases, but the narrow case definition may offer a larger effect size, which can reduce sensitivity to bias from unmeasured confounding. However, when the goal is to test whether there is a treatment effect based on the broad case definition, restricting to the narrow case definition may introduce selection bias (i.e., bias due to selecting samples based on characteristics affected by the treatment) because exposure to firearms in the home may affect the location of suicide and thus the type of a case a subject is. We propose a new sensitivity analysis framework for combining broad and narrow case definitions in matched case-control studies, that considers the unmeasured confounding bias and selection bias simultaneously. We develop a valid randomization-based testing procedure using only the narrow case matched sets when the effect of the unmeasured confounder on receiving treatment and the effect of the treatment on case definition among the always-cases are controlled by sensitivity parameters. We then use the Bonferroni method to combine the testing procedures using the broad and narrow case definitions. With the proposed methods, we find robust evidence that having firearms at home increases suicide risk. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ting Ye and Kan Chen and Dylan Small},
  doi          = {10.1080/01621459.2024.2441519},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {698-709},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance. <em>JASA</em>, <em>120</em>(550), 685-697. (<a href='https://doi.org/10.1080/01621459.2024.2435655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations are increasingly relying on digital communications, such as targeted e-mails and mobile notifications, to engage with their audiences. Despite the evident advantages like cost-effectiveness and customization, assessing the effectiveness of such communications from observational data poses various statistical challenges. An immediate challenge is to adjust for targeting rules used in these communications. When digital communications involve a sequence of e-mails or notifications, however, further adjustments are required to correct for selection bias arising from previous communications influencing the subsequent ones and to deal with noncompliance issues, for example, not opening the e-mail. This article addresses these challenges in a study of promotional e-mail sequences sent by a U.S. retailer. We use a Bayesian methodology for causal inference from longitudinal data, considering targeting, noncompliance, and sequential confounding with unmeasured variables. The methodology serves three objectives: to evaluate the average treatment effect of any deterministic e-mailing strategy, to compare the effectiveness of these strategies across varying compliance behaviors, and to infer optimal strategies for distinct customer segments. Our analysis finds, among other things, that certain promotional e-mails effectively maintain engagement among individuals who have regularly received such incentives, and individuals who consistently open their e-mails exhibit reduced sensitivity to promotional content. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuki Ohnishi and Bikram Karmakar and Wreetabrata Kar},
  doi          = {10.1080/01621459.2024.2435655},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {685-697},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immune profiling among colorectal cancer subtypes using dependent mixture models. <em>JASA</em>, <em>120</em>(550), 671-684. (<a href='https://doi.org/10.1080/01621459.2024.2427936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison of transcriptomic data across different conditions is of interest in many biomedical studies. In this article, we consider comparative immune cell profiling for early-onset (EO) versus late-onset (LO) colorectal cancer (CRC). EOCRC, diagnosed between ages 18‚Äì45, is a rising public health concern that needs to be urgently addressed. However, its etiology remains poorly understood. We work toward filling this gap by identifying homogeneous T cell sub-populations that show significantly distinct characteristics across the two tumor types, and identifying others that are shared between EOCRC and LOCRC. We develop dependent finite mixture models where immune subtypes enriched under a specific condition are characterized by terms in the mixture model with common atoms but distinct weights across conditions, whereas common subtypes are characterized by sharing both atoms and relative weights. The proposed model facilitates the desired comparison across conditions by introducing highly structured multi-layer Dirichlet priors. We illustrate inference with simulation studies and data examples. Results identify EO- and LO-enriched T cells subtypes whose biomarkers are found to be linked to mechanisms of tumor progression, and potentially motivate insights into treatment of CRC. Code implementing the proposed method is available at: https://github.com/YunshanDYS/SASCcode . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yunshan Duan and Shuai Guo and Wenyi Wang and Peter M√ºller},
  doi          = {10.1080/01621459.2024.2427936},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {671-684},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Immune profiling among colorectal cancer subtypes using dependent mixture models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking retrospective prevalent information in EHRs‚ÄîA revisit to the pairwise pseudolikelihood. <em>JASA</em>, <em>120</em>(550), 658-670. (<a href='https://doi.org/10.1080/01621459.2024.2427431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records offer abundant data on various diseases and health conditions, enabling researchers to explore the relationship between disease onset age and underlying risk factors. Unlike mortality data, the event of interest is nonterminal, hence, individuals can retrospectively report their disease-onset-age upon recruitment to the study. These individuals, diagnosed with the disease before entering the study, are termed ‚Äúprevalent.‚Äù The ascertainment imposes a left truncation condition, also known as a ‚Äúdelayed entry,‚Äù because individuals had to survive a certain period before being eligible for enrollment. The standard method to accommodate delayed entry conditions on the entire history up to recruitment, hence, the retrospective prevalent failure times are conditioned upon and cannot participate in estimating the disease-onset-age distribution. Other methods that condition on less information and allow the incorporation of the prevalent observations either bring about numerical and computational difficulties or require statistical assumptions that are violated by most biobanks. This work presents a novel estimator of the coefficients in a regression model for the age-at-onset, successfully using the prevalent data. Asymptotic results are provided, and simulations are conducted to showcase the substantial efficiency gain. In particular, the method is highly useful in leveraging large-scale repositories for replication analysis of genetic variants. Indeed, analysis of urinary bladder cancer data reveals that the proposed approach yields about twice as many replicated discoveries compared to the popular approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Nir Keret and Malka Gorfine},
  doi          = {10.1080/01621459.2024.2427431},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {658-670},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Unlocking retrospective prevalent information in EHRs‚ÄîA revisit to the pairwise pseudolikelihood},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal modeling for record-breaking temperature events in spain. <em>JASA</em>, <em>120</em>(550), 645-657. (<a href='https://doi.org/10.1080/01621459.2024.2427430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record-breaking temperature events are now very frequently in the news, viewed as evidence of climate change. With this as motivation, we undertake the first substantial spatial modeling investigation of temperature record-breaking across years for any given day within the year. We work with a dataset consisting of over 60 years (1960‚Äì2021) of daily maximum temperatures across peninsular Spain. Formal statistical analysis of record-breaking events is an area that has received attention primarily within the probability community, dominated by results for the stationary record-breaking setting with some additional work addressing trends. Such effort is inadequate for analyzing actual record-breaking data. Resulting from novel and detailed exploratory data analysis, we propose rich hierarchical conditional modeling of the indicator events which define record-breaking sequences. After suitable model selection, we discover explicit trend behavior, necessary autoregression, significance of distance to the coast, useful interactions, helpful spatial random effects, and very strong daily random effects. Illustratively, the model estimates that global warming trends have increased the number of records expected in the past decade almost 2-fold, 1.93 ( 1.89 , 1.98 ) , but also estimates highly differentiated climate warming rates in space and by season. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jorge Castillo-Mateo and Alan E. Gelfand and Zeus Gracia-Tabuenca and Jes√∫s As√≠n and Ana C. Cebri√°n},
  doi          = {10.1080/01621459.2024.2427430},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {645-657},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatio-temporal modeling for record-breaking temperature events in spain},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ‚Ñì1-based bayesian ideal point model for multidimensional politics. <em>JASA</em>, <em>120</em>(550), 631-644. (<a href='https://doi.org/10.1080/01621459.2024.2425461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ideal point estimation methods in the social sciences lack a principled approach for identifying multidimensional ideal points. We present a novel method for estimating multidimensional ideal points based on l 1 distance. In the Bayesian framework, the use of l 1 distance transforms the invariance problem of infinite rotational turns into the signed perpendicular problem, yielding posterior estimates that contract around a small area. Our simulation shows that the proposed method successfully recovers planted multidimensional ideal points in a variety of settings including non-partisan, two-party, and multi-party systems. The proposed method is applied to the analysis of roll call data from the United States House of Representatives during the late Gilded Age (1891‚Äì1899) when legislative coalitions were distinguished not only by partisan divisions but also by sectional divisions that ran across party lines. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Sooahn Shin and Johan Lim and Jong Hee Park},
  doi          = {10.1080/01621459.2024.2425461},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {631-644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {‚Ñì1-based bayesian ideal point model for multidimensional politics},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed, deep double reservoir network for forecasting boundary layer velocity. <em>JASA</em>, <em>120</em>(550), 618-630. (<a href='https://doi.org/10.1080/01621459.2024.2422131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data in a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing improved forecasting by the proposed approach in terms of mass conservation and variability of velocity fluctuation against non-physics-informed methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Matthew Bonas and David H. Richter and Stefano Castruccio},
  doi          = {10.1080/01621459.2024.2422131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {618-630},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A physics-informed, deep double reservoir network for forecasting boundary layer velocity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies. <em>JASA</em>, <em>120</em>(550), 605-617. (<a href='https://doi.org/10.1080/01621459.2024.2422124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation, pleiotropy, and replication analyses are three highly popular genetic study designs. Although these analyses address different scientific questions, the underlying statistical inference problems all involve large-scale testing of composite null hypotheses. The goal is to determine whether all null hypotheses‚Äîas opposed to at least one‚Äîin a set of individual tests should simultaneously be rejected. Recently, various methods have been proposed for each of these situations, including an appealing two-group empirical Bayes approach that calculates local false discovery rates (lfdr). However, lfdr estimation is difficult due to the need for multivariate density estimation. Furthermore, the multiple testing rules for the empirical Bayes lfdr approach can disagree with conventional frequentist z-statistics, which is troubling for a field that ubiquitously uses summary statistics. This work proposes a framework to unify two-group testing in genetic association composite null settings, the conditionally symmetric multidimensional Gaussian mixture model (csmGmm). The csmGmm is shown to demonstrate more robust operating characteristics than recently-proposed alternatives. Crucially, the csmGmm also offers interpretability guarantees by harmonizing lfdr and z-statistic testing rules. We extend the base csmGmm to cover each of the mediation, pleiotropy, and replication settings, and we prove that the lfdr z-statistic agreement holds in each situation. We apply the model to a collection of translational lung cancer genetic association studies that motivated this work. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ryan Sun and Zachary R. McCaw and Xihong Lin},
  doi          = {10.1080/01621459.2024.2422124},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {605-617},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-time extremes of severe U.S. thunderstorm environments. <em>JASA</em>, <em>120</em>(550), 591-604. (<a href='https://doi.org/10.1080/01621459.2024.2421582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe thunderstorms cause substantial economic and human losses in the United States. Simultaneous high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are favorable to severe weather, and both they and the composite variable PROD = CAPE √ó SRH can be used as indicators of severe thunderstorm activity. Their extremal spatial dependence exhibits temporal non-stationarity due to seasonality and large-scale atmospheric signals such as El Ni√±o-Southern Oscillation (ENSO). In order to investigate this, we introduce a space-time model based on a max-stable, Brown‚ÄìResnick, field whose range depends on ENSO and on time through a tensor product spline. We also propose a max-stability test based on empirical likelihood and the bootstrap. The marginal and dependence parameters must be estimated separately owing to the complexity of the model, and we develop a bootstrap-based model selection criterion that accounts for the marginal uncertainty when choosing the dependence model. In the case study, the out-sample performance of our model is good. We find that extremes of PROD, CAPE, and SRH are generally more localized in summer and, in some regions, less localized during El Ni√±o and La Ni√±a events, and give meteorological interpretations of these phenomena. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jonathan Koh and Erwan Koch and Anthony C. Davison},
  doi          = {10.1080/01621459.2024.2421582},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {591-604},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Space-time extremes of severe U.S. thunderstorm environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
