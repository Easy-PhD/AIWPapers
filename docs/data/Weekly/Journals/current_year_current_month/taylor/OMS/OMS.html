<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms">OMS - 11</h2>
<ul>
<li><details>
<summary>
(2025). Correction. <em>OMS</em>, <em>40</em>(4), 1014. (<a href='https://doi.org/10.1080/10556788.2025.2516933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  doi          = {10.1080/10556788.2025.2516933},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {1014},
  shortjournal = {Optim. Methods Softw.},
  title        = {Correction},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential convergence rates of a second-order dynamic system and algorithm for a linear equality constrained optimization problem. <em>OMS</em>, <em>40</em>(4), 977-1013. (<a href='https://doi.org/10.1080/10556788.2025.2517174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The O ( 1 / t 2 ) convergence rate of second-order dynamical systems with asymptotic vanishing viscous damping is faster than the O ( 1 / t ) rate of systems with fixed viscous damping in unconstrained and linear equality constrained optimization problems. We explore whether the performance of systems with vanishing viscous damping remains superior when both use time scaling. We compare the best polynomial convergence rates of vanishing damping systems with the best exponential convergence rates of a fixed damping system with time scaling for linear equality constrained problems. We prove that the primal-dual trajectory weakly converges to an optimal solution. Additionally, we present an inertial algorithm derived from the implicit discretization of the dynamical system, establishing exponential convergence rates for the primal-dual gap, feasibility measure, and objective value without assuming strong convexity. The sequence of iterates generated by the inertial algorithm weakly converges to an optimal solution when the objective function is proper, convex, and lower semicontinuous. These results align with those in the continuous setting.},
  archive      = {J_OMS},
  author       = {Ke-wei Ding and Lingling Liu and Phan Tu Vuong},
  doi          = {10.1080/10556788.2025.2517174},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {977-1013},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exponential convergence rates of a second-order dynamic system and algorithm for a linear equality constrained optimization problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proximal-gradient inertial algorithm with tikhonov regularization: Strong convergence to the minimal norm solution. <em>OMS</em>, <em>40</em>(4), 947-976. (<a href='https://doi.org/10.1080/10556788.2025.2517172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the strong convergence properties of a proximal-gradient inertial algorithm with two Tikhonov regularization terms in connection with the minimization problem of the sum of a convex lower semi-continuous function f and a smooth convex function g . For the appropriate setting of the parameters, we provide the strong convergence of the generated sequence ( x k ) k ≥ 0 to the minimum norm minimizer of our objective function f + g . Further, we obtain fast convergence to zero of the objective function values in a generated sequence but also for the discrete velocity and the sub-gradient of the objective function. We also show that for another setting of the parameters the optimal rate of order O ( k − 2 ) for the potential energy ( f + g ) ( x k ) − min ( f + g ) can be obtained.},
  archive      = {J_OMS},
  author       = {Szilárd Csaba László},
  doi          = {10.1080/10556788.2025.2517172},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {947-976},
  shortjournal = {Optim. Methods Softw.},
  title        = {A proximal-gradient inertial algorithm with tikhonov regularization: Strong convergence to the minimal norm solution},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach for solving a class of diffusion identification problems. <em>OMS</em>, <em>40</em>(4), 920-946. (<a href='https://doi.org/10.1080/10556788.2025.2506176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to the formulation and solution of a class of elliptic diffusion identification problems in the framework of the Pontryagin maximum principle (PMP) is investigated. The proposed approach considers twice continuously differentiable diffusion coefficients defined as the convolution with a square-integrable optimization function, which allows to prove the PMP by spike variation and to construct and analyze an efficient PMP-based iterative algorithm that efficiently solves diffusion identification problems approximated by finite elements.},
  archive      = {J_OMS},
  author       = {Ştefana-Lucia Aniţa and Alfio Borzì},
  doi          = {10.1080/10556788.2025.2506176},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {920-946},
  shortjournal = {Optim. Methods Softw.},
  title        = {A novel approach for solving a class of diffusion identification problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-point feedback for composite optimization with applications to distributed and federated learning. <em>OMS</em>, <em>40</em>(4), 904-919. (<a href='https://doi.org/10.1080/10556788.2025.2502829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is devoted to solving the composite optimization problem with the mixture oracle: for the smooth part of the problem, we have access to the gradient, and for the non-smooth part, only the one-point zero-order oracle is available. For such a setup, we present a new method based on the sliding algorithm. Our method allows to separate the oracle complexities and to compute the gradient for one of the functions as rarely as possible. This paper also presents the applicability of our new method to the problems of distributed optimization and federated learning. Experimental results confirm the theory.},
  archive      = {J_OMS},
  author       = {Aleksandr Beznosikov and Ivan Stepanov and Artyom Voronov and Alexander Gasnikov},
  doi          = {10.1080/10556788.2025.2502829},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {904-919},
  shortjournal = {Optim. Methods Softw.},
  title        = {One-point feedback for composite optimization with applications to distributed and federated learning},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2MPJ and CUTEst optimization problems for matlab, python and julia. <em>OMS</em>, <em>40</em>(4), 871-903. (<a href='https://doi.org/10.1080/10556788.2025.2490640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new suite of test problems for optimization is presented, which contains a large fraction of the problems in the CUTEst collection. The problems are supplied in the form of Matlab, Python and Julia files allowing the computation of values and derivatives of the objective function and constraints directly within ‘native’ Matlab, Python or Julia, without any additional installation or interfacing with MEX files or Fortran programs. These files are produced by a new decoder (written in Matlab) for the original SIF descriptions in the CUTEst collection. When used within Matlab, the new problem files optionally support reduced-precision computations.},
  archive      = {J_OMS},
  author       = {S. Gratton and Ph. L. Toint},
  doi          = {10.1080/10556788.2025.2490640},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {871-903},
  shortjournal = {Optim. Methods Softw.},
  title        = {S2MPJ and CUTEst optimization problems for matlab, python and julia},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum computing and the stable set problem. <em>OMS</em>, <em>40</em>(4), 837-870. (<a href='https://doi.org/10.1080/10556788.2025.2490639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected graph, the stable set problem asks to determine the cardinality of the largest subset of pairwise non-adjacent vertices. This value is called the stability number of the graph, and its computation is an NP-hard problem. In this paper, we solve the stable set problem using the D-Wave quantum annealer. By formulating the problem as a quadratic unconstrained binary optimization problem with the penalty method, we show its optimal value equals the graph's stability number for specific penalty values. However, D-Wave's quantum annealer is a heuristic, so the solutions may be far from the optimum and may not represent stable sets. To address these, we introduce a post-processing procedure that identifies samples that could lead to improved solutions. Additionally, we propose a partitioning method to handle larger instances that cannot be embedded on D-Wave's quantum processing unit. Finally, we investigate how different penalty parameter values affect the solutions' quality. Extensive computational results show that the post-processing procedure significantly improves the solution quality, while the partitioning method successfully extends our approach to medium-size instances.},
  archive      = {J_OMS},
  author       = {Aljaž Krpan and Janez Povh and Dunja Pucher},
  doi          = {10.1080/10556788.2025.2490639},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {837-870},
  shortjournal = {Optim. Methods Softw.},
  title        = {Quantum computing and the stable set problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bundle trust-region algorithm for nonsmooth nonconvex constrained optimization. <em>OMS</em>, <em>40</em>(4), 813-836. (<a href='https://doi.org/10.1080/10556788.2025.2475518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an algorithm based on the idea of the bundle trust-region method to solve nonsmooth nonconvex constrained optimization problems. The resulting algorithm inherits some attractive features from both bundle and trust-region methods. Moreover, it allows effective control of the size of trust-region subproblems via the compression and aggregation techniques of bundle methods. On the other hand, the trust-region strategy is used to manage the search region and accept a candidate point as a new successful iterate. Global convergence of the developed algorithm is studied under some mild assumptions and its encouraging preliminary computational results are reported.},
  archive      = {J_OMS},
  author       = {N. Hoseini Monjezi and S. Nobakhtian},
  doi          = {10.1080/10556788.2025.2475518},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {813-836},
  shortjournal = {Optim. Methods Softw.},
  title        = {A bundle trust-region algorithm for nonsmooth nonconvex constrained optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft quasi-newton: Guaranteed positive definiteness by relaxing the secant constraint. <em>OMS</em>, <em>40</em>(4), 783-812. (<a href='https://doi.org/10.1080/10556788.2025.2475406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm, termed soft quasi-Newton (soft QN), for optimization in the presence of bounded noise. Traditional quasi-Newton algorithms are vulnerable to such noise-induced perturbations. To develop a more robust quasi-Newton method, we replace the secant condition in the matrix optimization problem for the Hessian update with a penalty term in its objective and derive a closed-form update formula. A key feature of our approach is its ability to maintain positive definiteness of the Hessian inverse approximation throughout the iterations. Furthermore, we establish the following properties of soft QN: it recovers the BFGS method under specific limits, it treats positive and negative curvature equally, and it is scale invariant. Collectively, these features enhance the efficacy of soft QN in noisy environments. For strongly convex objective functions and Hessian approximations obtained using soft QN, we develop an algorithm that exhibits linear convergence toward a neighborhood of the optimal solution even when gradient and function evaluations are subject to bounded perturbations. Through numerical experiments, we demonstrate that soft QN consistently outperforms state-of-the-art methods across a range of scenarios.},
  archive      = {J_OMS},
  author       = {Erik Berglund and Jiaojiao Zhang and Mikael Johansson},
  doi          = {10.1080/10556788.2025.2475406},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {783-812},
  shortjournal = {Optim. Methods Softw.},
  title        = {Soft quasi-newton: Guaranteed positive definiteness by relaxing the secant constraint},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal subgradient method for non-lipschitz objective functions. <em>OMS</em>, <em>40</em>(4), 755-782. (<a href='https://doi.org/10.1080/10556788.2025.2475405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a convergence analysis framework of the proximal subgradient method for optimization problems involving non-Lipschitz continuous objective functions. In the conventional analysis of the various subgradient methods, including the proximal subgradient method, the Lipschitz continuity assumption has been placed to guarantee the boundedness of the subgradient and derive the convergence rate. However, the Lipschitz continuity does not hold in practical problems, including the sum-of- ℓ 2 -norms (SO ℓ 2 N) optimal control problem, which is examined in the numerical experiments of this paper. Without the Lipschitz continuity assumption, this paper provides the convergence analysis for strongly convex and non-strongly convex objective functions under mild assumptions. Suitable stepsize rules and resulting convergence rates are established; non-strongly convex cases result in a rate close to the rate of the existing subgradient method, and strongly convex cases achieve the same rate as the existing convergence analysis.},
  archive      = {J_OMS},
  author       = {Mitsuru Toyoda and Mirai Tanaka},
  doi          = {10.1080/10556788.2025.2475405},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {755-782},
  shortjournal = {Optim. Methods Softw.},
  title        = {Proximal subgradient method for non-lipschitz objective functions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified polak-ribière-polyak type conjugate gradient method for vector optimization. <em>OMS</em>, <em>40</em>(4), 725-754. (<a href='https://doi.org/10.1080/10556788.2025.2475402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a modified Polak-Ribière-Polyak conjugate gradient method for solving vector optimization problems. Unlike the existing methods, it is not necessary to use the inner loop with sufficient accurate line search to generate the descent direction at each iteration. Moreover, it does not ignore the fact that the proposed algorithm may still generate the descent direction when the conjugate parameter is negative. We prove that the generated direction is sufficiently descent independent of any line search or convexity. Under the standard Wolfe line search, we also prove the global convergence of the modified Polak-Ribière-Polyak conjugate gradient method without restart or convexity assumption. Finally, through numerical experiments and comparative analysis with the existing methods, we validate the numerical performance of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Qingjie Hu and Yanyan Zhang and Ruyun Li and Zhibin Zhu},
  doi          = {10.1080/10556788.2025.2475402},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {725-754},
  shortjournal = {Optim. Methods Softw.},
  title        = {A modified polak-ribière-polyak type conjugate gradient method for vector optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
