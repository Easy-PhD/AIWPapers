<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JCGS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jcgs">JCGS - 31</h2>
<ul>
<li><details>
<summary>
(2025). Quantile regression and homogeneity identification of a semiparametric panel data model. <em>JCGS</em>, <em>34</em>(3), 1169-1187. (<a href='https://doi.org/10.1080/10618600.2024.2433672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we delve into the quantile regression and homogeneity detection of a varying index coefficient panel data model, which incorporates fixed individual effects and exhibits nonlinear time trends. Using spline approximation, we obtain estimators for the trend functions, link functions, and index parameters, and subsequently establish the corresponding convergence rates and asymptotic normality. Observing that subjects within a group may share identical trend functions, we are motivated to further explore potential homogeneity in these trends. To this end, we propose a homogeneity identification algorithm based on binary segmentation. For the determination of the thresholding parameter in homogeneity identification, we propose a generalized Bayesian information criterion. Furthermore, we introduce a penalized method to discern the constant and linear structures within the nonparametric functions of our model. By leveraging grouped observations, we achieve more efficient estimation and improve the asymptotic properties of the estimators. To demonstrate the finite sample performance of our proposed approach, we conduct simulation studies and apply our methodology to a real-world dataset comprising Air Pollution Data and Integrated Surface Data (APD&ISD). Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Rui Li and Tao Li and Huacheng Su and Jinhong You},
  doi          = {10.1080/10618600.2024.2433672},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1169-1187},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Quantile regression and homogeneity identification of a semiparametric panel data model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural bayes estimators for irregular spatial data using graph neural networks. <em>JCGS</em>, <em>34</em>(3), 1153-1168. (<a href='https://doi.org/10.1080/10618600.2024.2433671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. Although they are appealing to use with spatial models, where estimation is often a computational bottleneck, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be retrained for new datasets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks (GNNs) to tackle the important problem of parameter point estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, the use of GNNs leads to substantial computational benefits, since the estimator can be used with any configuration or number of locations and independent replicates, thus, amortizing the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology on a dataset of global sea-surface temperature, where we estimate the parameters of a Gaussian process model in 2161 spatial regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Matthew Sainsbury-Dale and Andrew Zammit-Mangion and Jordan Richards and Raphaël Huser},
  doi          = {10.1080/10618600.2024.2433671},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1153-1168},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Neural bayes estimators for irregular spatial data using graph neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualization and assessment of copula symmetry. <em>JCGS</em>, <em>34</em>(3), 1140-1152. (<a href='https://doi.org/10.1080/10618600.2024.2432978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization and assessment of copula structures are crucial for accurately understanding and modeling the dependencies in multivariate data analysis. In this article, we introduce an innovative method that employs functional boxplots and rank-based testing procedures to evaluate copula symmetries. This approach is specifically designed to assess key characteristics such as reflection symmetry, radial symmetry, and joint symmetry. We first construct test functions for each specific property and then investigate the asymptotic properties of their empirical estimators. We demonstrate that the functional boxplot of these sample test functions serves as an informative visualization tool of a given copula structure, effectively measuring the departure from zero of the test function. Furthermore, we introduce a nonparametric testing procedure to assess the significance of deviations from symmetry, ensuring the accuracy and reliability of our visualization method. Through extensive simulation studies involving various copula models, we demonstrate the effectiveness of our testing approach. Finally, we apply our visualization and testing techniques to three real-world datasets: a nutritional habits survey with five variables, stock price data for the five top companies in the NASDAQ-100 stock index, and two major stock indices, the US S&P500 and German DAX. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Cristian F. Jiménez-Varón and Hao Lee and Marc G. Genton and Ying Sun},
  doi          = {10.1080/10618600.2024.2432978},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Visualization and assessment of copula symmetry},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable estimation and two-sample testing for large networks via subsampling. <em>JCGS</em>, <em>34</em>(3), 1127-1139. (<a href='https://doi.org/10.1080/10618600.2024.2432974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large networks are routinely used to represent data from many scientific fields. Statistical analysis of these networks, such as estimation and hypothesis testing, has received considerable attention. However, most of the methods proposed in the literature are computationally expensive for large networks. In this article, we propose a subsampling-based method to reduce the computational cost of estimation and two-sample hypothesis testing. The idea is to divide the network into smaller subgraphs with an overlap region, then draw inference based on each subgraph, and finally combine the results together. We first develop the subsampling method for random dot product graph models, and establish theoretical consistency of the proposed method. Then we extend the subsampling method to a more general setup and establish similar theoretical properties. We demonstrate the performance of our methods through simulation experiments and real data analysis. Supplemental materials for the article are available online. The code is available in the following GitHub repository: https://github.com/kchak19/SubsampleTestingNetwork . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Kaustav Chakraborty and Srijan Sengupta and Yuguo Chen},
  doi          = {10.1080/10618600.2024.2432974},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1127-1139},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Scalable estimation and two-sample testing for large networks via subsampling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlexBART: Flexible bayesian regression trees with categorical predictors. <em>JCGS</em>, <em>34</em>(3), 1117-1126. (<a href='https://doi.org/10.1080/10618600.2024.2431072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most implementations of Bayesian additive regression trees (BART) one-hot encode categorical predictors, replacing each one with several binary indicators, one for every level or category. Regression trees built with these indicators partition the discrete set of categorical levels by repeatedly removing one level at a time. Unfortunately, the vast majority of partitions cannot be built with this strategy, severely limiting BART’s ability to partially pool data across groups of levels. Motivated by analyses of baseball data and neighborhood-level crime dynamics, we overcame this limitation by re-implementing BART with regression trees that can assign multiple levels to both branches of a decision tree node. To model spatial data aggregated into small regions, we further proposed a new decision rule prior that creates spatially contiguous regions by deleting a random edge from a random spanning tree of a suitably defined network. Our re-implementation, which is available in the flexBART package, often yields improved out-of-sample predictive performance and scales better to larger datasets than existing implementations of BART. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Sameer K. Deshpande},
  doi          = {10.1080/10618600.2024.2431072},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1117-1126},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {FlexBART: Flexible bayesian regression trees with categorical predictors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotone cubic B-splines with a neural-network generator. <em>JCGS</em>, <em>34</em>(3), 1102-1116. (<a href='https://doi.org/10.1080/10618600.2024.2431070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for fitting monotone curves using cubic B-splines with a monotonicity constraint on the coefficients. We explore different ways of enforcing this constraint and analyze their theoretical and empirical properties. We propose two algorithms for solving the spline fitting problem: one that uses standard optimization techniques and one that trains a Multi-Layer Perceptrons (MLP) generator to approximate the solutions under various settings and perturbations. The generator approach can speed up the fitting process when we need to solve the problem repeatedly, such as when constructing confidence bands using bootstrap. We evaluate our method against several existing methods, some of which do not use the monotonicity constraint, on some monotone curves with varying noise levels. We demonstrate that our method outperforms the other methods, especially in high-noise scenarios. We also apply our method to analyze the polarization-hole phenomenon during star formation in astrophysics. The source code is accessible at https://github.com/szcf-weiya/MonotoneSplines.jl . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lijun Wang and Xiaodan Fan and Huabai Li and Jun S. Liu},
  doi          = {10.1080/10618600.2024.2431070},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1102-1116},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Monotone cubic B-splines with a neural-network generator},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized learning of quantile regression: A smoothing approach. <em>JCGS</em>, <em>34</em>(3), 1091-1101. (<a href='https://doi.org/10.1080/10618600.2024.2431060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed estimation has attracted a significant amount of attention recently due to its advantages in computational efficiency and data privacy preservation. In this article, we focus on quantile regression over a decentralized network. Without a coordinating central node, a decentralized network improves system stability and increases efficiency by communicating with fewer nodes per round. However, existing related works on decentralized quantile regression have slow (sub-linear) convergence speed. We propose a novel method for decentralized quantile regression which is built upon the smoothed quantile loss. However, we argue that the smoothed loss proposed in the existing literature using a single smoothing bandwidth parameter fails to achieve fast convergence and statistical efficiency simultaneously in the decentralized setting. We propose a novel quadratic approximation of the quantile loss using a big bandwidth for the Hessian and a small bandwidth for the gradient. Our method enjoys a linear convergence rate and has optimal statistical efficiency. Numerical experiments and real data analysis are conducted to demonstrate the effectiveness of our method. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jianwei Shi and Yue Wang and Zhongyi Zhu and Heng Lian},
  doi          = {10.1080/10618600.2024.2431060},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1091-1101},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Decentralized learning of quantile regression: A smoothing approach},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local clustering for functional data. <em>JCGS</em>, <em>34</em>(3), 1075-1090. (<a href='https://doi.org/10.1080/10618600.2024.2431057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In functional data analysis, unsupervised clustering has been extensively conducted and has important implications. In most of the existing functional clustering analyses, it is assumed that there is a single clustering structure across the whole domain of measurement (say, time interval). In some data analyses, for example, the analysis of normalized COVID-19 daily confirmed cases for the U.S. states, it is observed that functions can have different clustering patterns in different time subintervals. To tackle the lack of flexibility of the existing functional clustering techniques, we develop a local clustering approach, which can fully data-dependently identify subintervals, where, in different subintervals, functions have different clustering structures. This approach is built on the basis expansion technique and has a novel penalization form. It simultaneously achieves subinterval identification, clustering, and estimation. Its estimation and clustering consistency properties are rigorously established. In simulation, it significantly outperforms multiple competitors. In the analysis of the COVID-19 case trajectory data, it identifies sensible subintervals and clustering structures. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yuanxing Chen and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1080/10618600.2024.2431057},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1075-1090},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Local clustering for functional data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No more, no less than sum of its parts: Groups, monoids, and the algebra of graphics, statistics, and interaction. <em>JCGS</em>, <em>34</em>(3), 1063-1074. (<a href='https://doi.org/10.1080/10618600.2024.2429708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive data visualization has become a staple of modern data presentation. Yet, despite its growing popularity, we still lack a general framework for turning raw data into summary statistics that can be displayed by interactive graphics. This gap may stem from a subtle yet profound issue: while we would often like to treat graphics, statistics, and interaction in our plots as independent, they are in fact deeply connected. This article examines this interdependence in light of two fundamental concepts from category theory: groups and monoids. We argue that the knowledge of these algebraic structures can help us design sensible interactive graphics. Specifically, if we want our graphics to support interactive features which split our data into parts and then combine these parts back together (such as linked selection), then the statistics underlying our plots need to possess certain properties. By grounding our thinking in these algebraic concepts, we may be able to build more flexible and expressive interactive data visualization systems. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Adam Bartonicek and Simon Urbanek and Paul Murrell},
  doi          = {10.1080/10618600.2024.2429708},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1063-1074},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {No more, no less than sum of its parts: Groups, monoids, and the algebra of graphics, statistics, and interaction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional projection K-means. <em>JCGS</em>, <em>34</em>(3), 1051-1062. (<a href='https://doi.org/10.1080/10618600.2024.2429706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new technique for simultaneous clustering and dimensionality reduction of functional data is proposed. The observations are projected into a low-dimensional subspace and clustered by means of a functional K -means. The subspace and the partition are estimated simultaneously by minimizing the within deviance in the reduced space. This allows us to find new dimensions with a very low within deviance, which should correspond to a high level of discriminant power. However, in some cases, the total deviance explained by the new dimensions is so low as to make the subspace, and therefore the partition identified in it, insignificant. To overcome this drawback, we add to the loss a penalty equal to the negative total deviance in the reduced space. In this way, subspaces with a low deviance are avoided. We show how several existing methods are particular cases of our proposal simply by varying the weight of the penalty. The estimation is improved by adding a regularization term to the loss in order to take into account the functional nature of the data by smoothing the centroids. In contrast to existing literature, which largely considers the smoothing as a pre-processing step, in our proposal regularization is integrated with the identification of both subspace and cluster partition. An alternating least squares algorithm is introduced to compute model parameter estimates. The effectiveness of our proposal is demonstrated through its application to both real and simulated data. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Roberto Rocci and Stefano A. Gattone},
  doi          = {10.1080/10618600.2024.2429706},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1051-1062},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Functional projection K-means},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse model-based clustering of three-way data via lasso-type penalties. <em>JCGS</em>, <em>34</em>(3), 1030-1050. (<a href='https://doi.org/10.1080/10618600.2024.2429705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of matrix Gaussian distributions provide a probabilistic framework for clustering continuous matrix-variate data, which are increasingly common in various fields. Despite their widespread use and successful applications, these models suffer from over-parameterization, making them not suitable for even moderately sized matrix-variate data. To address this issue, we introduce a sparse model-based clustering approach for three-way data. Our approach assumes that the matrix mixture parameters are sparse and have different degrees of sparsity across clusters, enabling the induction of parsimony in a flexible manner. Estimation relies on the maximization of a penalized likelihood, with specifically tailored group and graphical lasso penalties. These penalties facilitate the selection of the most informative features for clustering three-way data where variables are recorded over multiple occasions, as well as allowing the identification of cluster-specific association structures. We conduct extensive testing of the proposed methodology on synthetic data and validate its effectiveness through an application to time-dependent crime patterns across multiple U.S. cities. Supplementary files for this article are available online.},
  archive      = {J_JCGS},
  author       = {Andrea Cappozzo and Alessandro Casa and Michael Fop},
  doi          = {10.1080/10618600.2024.2429705},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1030-1050},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sparse model-based clustering of three-way data via lasso-type penalties},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A majorization-minimization gauss-newton method for 1-bit matrix completion. <em>JCGS</em>, <em>34</em>(3), 1017-1029. (<a href='https://doi.org/10.1080/10618600.2024.2428610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called Majorization-Minimization Gauss-Newton ( MMGN ). Our method is based on the majorization-minimization principle, which converts the original optimization problem into a sequence of standard low-rank matrix completion problems. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Using simulations and a real data example, we illustrate that in comparison to existing 1-bit matrix completion methods, MMGN outputs comparable if not more accurate estimates. In addition, it is often significantly faster, and less sensitive to the spikiness of the underlying matrix. In comparison with three standard generic optimization approaches that directly minimize the original objective, MMGN also exhibits a clear computational advantage, especially when the fraction of observed entries is small. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Xiaoqian Liu and Xu Han and Eric C. Chi and Boaz Nadler},
  doi          = {10.1080/10618600.2024.2428610},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1017-1029},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A majorization-minimization gauss-newton method for 1-bit matrix completion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional block diagonal covariance structure detection using singular vectors. <em>JCGS</em>, <em>34</em>(3), 1005-1016. (<a href='https://doi.org/10.1080/10618600.2024.2422985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assumption of independent subvectors arises in many aspects of multivariate analysis. In most real-world applications, however, we lack prior knowledge about the number of subvectors and the specific variables within each subvector. Yet, testing all these combinations is not feasible. For example, for a data matrix containing 15 variables, there are already 1 , 382 , 958 , 545 possible combinations. Given that zero correlation is a necessary condition for independence, independent subvectors exhibit a block diagonal covariance matrix. This article focuses on the detection of such block diagonal covariance structures in high-dimensional data and therefore also identifies uncorrelated subvectors. Our approach exploits the fact that the structure of the covariance matrix is mirrored by the structure of its eigenvectors. However, the true block diagonal structure is masked by noise in the sample case. To address this problem, we propose to use sparse approximations of the sample eigenvectors to reveal the sparse structure of the population eigenvectors. Notably, the right singular vectors of a data matrix with an overall mean of zero are identical to the sample eigenvectors of its covariance matrix. Using sparse approximations of these singular vectors instead of the eigenvectors makes the estimation of the covariance matrix obsolete. We demonstrate the performance of our method through simulations and provide real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jan O. Bauer},
  doi          = {10.1080/10618600.2024.2422985},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1005-1016},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {High-dimensional block diagonal covariance structure detection using singular vectors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subsampling for data streams with measurement constrained categorical responses. <em>JCGS</em>, <em>34</em>(3), 994-1004. (<a href='https://doi.org/10.1080/10618600.2024.2421990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-velocity, large-scale data streams have become pervasive. Frequently, the associated labels for such data prove costly to measure and are not always available upfront. Consequently, the analysis of such data poses a significant challenge. In this article, we develop a method that addresses this challenge by employing an online subsampling procedure and a multinomial logistic model for efficient analysis of high-velocity, large-scale data streams. Our algorithm is designed to sequentially update parameter estimation based on the A-optimality criterion. Moreover, it significantly increases computational efficiency while imposing minimal storage requirements. Theoretical properties are rigorously established to quantify the asymptotic behavior of the estimator. The method’s efficacy is further demonstrated through comprehensive numerical studies on both simulated and real-world datasets. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jun Yu and Zhiqiang Ye and Mingyao Ai and Ping Ma},
  doi          = {10.1080/10618600.2024.2421990},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {994-1004},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Optimal subsampling for data streams with measurement constrained categorical responses},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent markov time-interaction processes. <em>JCGS</em>, <em>34</em>(3), 984-993. (<a href='https://doi.org/10.1080/10618600.2024.2421984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present parametric and semiparametric latent Markov time-interaction processes, that are point processes where the occurrence of an event can increase or reduce the probability of future events. We first present time-interaction processes with parametric and nonparametric baselines, then we let model parameters be modulated by a discrete state continuous time latent Markov process. Posterior inference is based on a novel and efficient data augmentation approach in the Markov chain Monte Carlo framework. We illustrate with a simulation study; and an original application to terrorist attacks in Europe in the period 2001–2017, where we find two distinct latent clusters for the hazard of occurrence of terrorist events, negative association with GDP growth, and self-exciting phenomena. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Rosario Barone and Alessio Farcomeni and Maura Mezzetti},
  doi          = {10.1080/10618600.2024.2421984},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {984-993},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Latent markov time-interaction processes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label random subspace ensemble classification. <em>JCGS</em>, <em>34</em>(3), 971-983. (<a href='https://doi.org/10.1080/10618600.2024.2421248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a new ensemble learning framework, multi-label Random Subspace Ensemble (mRaSE), for multi-label classification. Given a base classifier (e.g., multinomial logistic regression, classification tree, K -nearest neighbors), mRaSE works by first randomly sampling a collection of subspaces, then choosing the best ones that achieve the minimum cross-validation errors and, finally, aggregating the chosen weak learners. In addition to its superior prediction performance, mRaSE also provides a model-free feature ranking depending on the given base classifier. An iterative version of mRaSE is also developed to further improve the performance. A model-free extension is pursued on the iterative version, leading to the so-called Super mRaSE , which accepts a collection of base classifiers as input to the algorithm. We show the proposed algorithms compared favorably with the state-of-the-art classification algorithm including random forest and deep neural network, via extensive simulation studies and two real data applications. The new algorithms are implemented in an updated version of the R package RaSEn .},
  archive      = {J_JCGS},
  author       = {Fan Bi and Jianan Zhu and Yang Feng},
  doi          = {10.1080/10618600.2024.2421248},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {971-983},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multi-label random subspace ensemble classification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning for gaussian graphical regressions with high dimensional covariates. <em>JCGS</em>, <em>34</em>(3), 961-970. (<a href='https://doi.org/10.1080/10618600.2024.2421246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical regression is a powerful approach for regressing the precision matrix of a Gaussian graphical model on covariates, which permits the response variables and covariates to outnumber the sample size. However, traditional approaches of fitting the model via separate node-wise lasso regressions overlook the network-induced structure among these regressions, leading to high error rates, particularly when the number of nodes is large. To address this issue, we propose a multi-task learning estimator for fitting Gaussian graphical regression models, which incorporates a cross-task group sparsity penalty and a within-task element-wise sparsity penalty to govern the sparsity of active covariates and their effects on the graph, respectively. We also develop an efficient augmented Lagrangian algorithm for computation, which solves subproblems with a semi-smooth Newton method. We further prove that our multi-task learning estimator has considerably lower error rates than the separate node-wise regression estimates, as the cross-task penalty enables borrowing information across tasks. We examine the utility of our method through simulations and an application to a gene co-expression network study with brain cancer patients. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jingfei Zhang and Yi Li},
  doi          = {10.1080/10618600.2024.2421246},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {961-970},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multi-task learning for gaussian graphical regressions with high dimensional covariates},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qini curves for multi-armed treatment rules. <em>JCGS</em>, <em>34</em>(3), 948-960. (<a href='https://doi.org/10.1080/10618600.2024.2418820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Qini curves have emerged as an attractive and popular approach for evaluating the benefit of data-driven targeting rules for treatment allocation. We propose a generalization of the Qini curve to multiple costly treatment arms that quantifies the value of optimally selecting among both units and treatment arms at different budget levels. We develop an efficient algorithm for computing these curves and propose bootstrap-based confidence intervals that are exact in large samples for any point on the curve. These confidence intervals can be used to conduct hypothesis tests comparing the value of treatment targeting using an optimal combination of arms with using just a subset of arms, or with a non-targeting assignment rule ignoring covariates, at different budget levels. We demonstrate the statistical performance in a simulation experiment and an application to treatment targeting for election turnout.},
  archive      = {J_JCGS},
  author       = {Erik Sverdrup and Han Wu and Susan Athey and Stefan Wager},
  doi          = {10.1080/10618600.2024.2418820},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {948-960},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Qini curves for multi-armed treatment rules},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling random graphs with specified degree sequences. <em>JCGS</em>, <em>34</em>(3), 934-947. (<a href='https://doi.org/10.1080/10618600.2024.2418817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The configuration model is a standard tool for uniformly generating random graphs with a specified degree sequence, and is often used as a null model to evaluate how much of an observed network’s structure can be explained by its degree structure alone. A Markov chain Monte Carlo (MCMC) algorithm, based on a degree-preserving double-edge swap, provides an asymptotic solution to sample from the configuration model. However, accurately and efficiently detecting when this Markov chain is sufficiently close to its stationary distribution remains an unsolved problem. Here, we provide a solution to sample from the configuration model using this standard MCMC algorithm. We develop an algorithm, based on the assortativity of the sampled graphs, for estimating the gap between effectively independent MCMC states, and a computationally efficient gap-estimation heuristic derived from analyzing a corpus of 509 empirical networks. We provide a convergence detection method based on the Dickey-Fuller Generalized Least Squares test, which we show is more accurate and efficient than three alternative Markov chain convergence tests. Supplementary materials for the proposed methods can be found here.},
  archive      = {J_JCGS},
  author       = {Upasana Dutta and Bailey K. Fosdick and Aaron Clauset},
  doi          = {10.1080/10618600.2024.2418817},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {934-947},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sampling random graphs with specified degree sequences},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sampling from the watson distribution in arbitrary dimensions. <em>JCGS</em>, <em>34</em>(3), 923-933. (<a href='https://doi.org/10.1080/10618600.2024.2416521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present two efficient methods for sampling from the Watson distribution in arbitrary dimensions. The first method adapts the rejection sampling algorithm from Kent, Ganeiber, and Mardia , originally designed for Bingham distributions, using angular central Gaussian envelopes. For the Watson distribution, we derive a closed-form expression for the parameters that maximize sampling efficiency, which is further investigated and bounded by asymptotic results. This approach avoids the curse of dimensionality through a smart matrix inversion, enabling fast runtimes even in high dimensions. The second method, based on Saw , employs adaptive rejection sampling from a projected distribution. This algorithm is also effective in all dimensions and offers rapid sampling capabilities. Finally, our simulation study compares the two main methods, revealing that each excels under different conditions: the first method is more efficient for small samples or large dimensions, while the second performs better with larger samples and more concentrated distributions. Both algorithms are available in the R package watson on CRAN. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lukas Sablica and Kurt Hornik and Josef Leydold},
  doi          = {10.1080/10618600.2024.2416521},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {923-933},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient sampling from the watson distribution in arbitrary dimensions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion corrected kernel density estimator on riemannian manifolds. <em>JCGS</em>, <em>34</em>(3), 906-922. (<a href='https://doi.org/10.1080/10618600.2024.2415543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning obtains a low-dimensional representation of an underlying Riemannian manifold supporting high-dimensional data. Kernel density estimates of the low-dimensional embedding with a fixed bandwidth fail to account for the way manifold learning algorithms distort the geometry of the Riemannian manifold. We propose a novel distortion-corrected kernel density estimator (DC-KDE) for any manifold learning embedding, with a bandwidth that depends on the estimated Riemannian metric at each data point. Exploiting the geometric information of the manifold leads to more accurate density estimation, which subsequently could be used for anomaly detection. To compare our proposed estimator with a fixed-bandwidth kernel density estimator, we run two simulations including one with data lying in a 100 dimensional ambient space. We demonstrate that the proposed DC-KDE improves the density estimates as long as the manifold learning embedding is of sufficient quality, and has higher rank correlations with the true manifold density. Further simulation results are provided via a supplementary R shiny app. The proposed method is applied to density estimation in statistical manifolds of electricity usage with the Irish smart meter data.},
  archive      = {J_JCGS},
  author       = {Fan Cheng and Rob J. Hyndman and Anastasios Panagiotelis},
  doi          = {10.1080/10618600.2024.2415543},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {906-922},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Distortion corrected kernel density estimator on riemannian manifolds},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample efficient nonparametric regression via low-rank regularization. <em>JCGS</em>, <em>34</em>(3), 896-905. (<a href='https://doi.org/10.1080/10618600.2024.2414891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric regression suffers from curse of dimensionality, requiring a relatively large sample size for accurate estimation beyond the univariate case. In this article, we consider a simple method of dimension reduction in nonparametric regression via series estimation, based on the concept of low-rankness which was previously studied in parametric multivariate reduced-rank regression and matrix regression. For d > 2 , the low-rank assumption is realized via tensor regression. We establish a faster convergence rate of the estimator in the (approximate) low-rank case. Limitations of the model are also discussed. Through simulation studies and real data analysis, we compare the estimation accuracy of the proposed method with that of existing approaches. The results demonstrate that the proposed method yields estimates with lower RMSE compared to existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jiakun Jiang and Jiahao Peng and Heng Lian},
  doi          = {10.1080/10618600.2024.2414891},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {896-905},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sample efficient nonparametric regression via low-rank regularization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable clustering: Large scale unsupervised learning of gaussian mixture models with outliers. <em>JCGS</em>, <em>34</em>(3), 884-895. (<a href='https://doi.org/10.1080/10618600.2024.2414889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a widely used technique with a long and rich history in a variety of areas. However, most existing algorithms do not scale well to large datasets, or are missing theoretical guarantees of convergence. This article introduces a provably robust clustering algorithm based on loss minimization that performs well on Gaussian mixture models with outliers. It provides theoretical guarantees that the algorithm obtains high accuracy with high probability under certain assumptions. Moreover, it can also be used as an initialization strategy for k -means clustering. Experiments on real-world large-scale datasets demonstrate the effectiveness of the algorithm when clustering a large number of clusters, and a k -means algorithm initialized by the algorithm outperforms many of the classic clustering methods in both speed and accuracy, while scaling well to large datasets such as ImageNet. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yijia Zhou and Kyle A. Gallivan and Adrian Barbu},
  doi          = {10.1080/10618600.2024.2414889},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {884-895},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Scalable clustering: Large scale unsupervised learning of gaussian mixture models with outliers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous functional regression for subgroup analysis. <em>JCGS</em>, <em>34</em>(3), 872-883. (<a href='https://doi.org/10.1080/10618600.2024.2414113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With ever increasing number of features of modern datasets, data heterogeneity is gradually becoming the norm rather than the exception. Whereas classical regressions usually assume all the samples follow a common model, it becomes imperative to identify the heterogeneous relationship in different subsamples. In this article, we propose a new approach to model heterogeneous functional regression relations. We target at the association between a response and a predictor, whose relationship can vary across underlying subgroups and is modeled as an unknown functional of an auxiliary predictor. We introduce a procedure which performs simultaneous parameter estimation and subgroup identification through a fusion type group-wise penalization. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation. We also establish the oracle property and asymptotic normality of the estimators. We carry out intensive simulations, and illustrate with a new dataset from an Alzheimer’s disease study. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yeqing Zhou and Fei Jiang},
  doi          = {10.1080/10618600.2024.2414113},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {872-883},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Heterogeneous functional regression for subgroup analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AddiVortes: (Bayesian) additive voronoi tessellations. <em>JCGS</em>, <em>34</em>(3), 859-871. (<a href='https://doi.org/10.1080/10618600.2024.2414104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Additive Voronoi Tessellations (AddiVortes) model is a multivariate regression model that uses Voronoi tessellations to partition the covariate space in an additive ensemble model. Unlike other partition methods, such as decision trees, this has the benefit of allowing the boundaries of the partitions to be non-orthogonal and nonparallel to the covariate axes. The AddiVortes model uses a similar sum-of-tessellations approach and a Bayesian backfitting MCMC algorithm to the BART model. We use regularization priors to limit the strength of individual tessellations and accepts new models based on a likelihood. The performance of the AddiVortes model is illustrated through testing on several datasets and comparing the performance to other models along with a simulation study to verify some of the properties of the model. In many cases, the AddiVortes model outperforms random forests, BART and other leading black-box regression models when compared using a range of metrics. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Adam J. Stone and John Paul Gosling},
  doi          = {10.1080/10618600.2024.2414104},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {859-871},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {AddiVortes: (Bayesian) additive voronoi tessellations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private methods for compositional data. <em>JCGS</em>, <em>34</em>(3), 848-858. (<a href='https://doi.org/10.1080/10618600.2024.2412174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidential data, such as electronic health records, activity data from wearable devices, and geolocation data, are becoming increasingly prevalent. Differential privacy provides a framework to conduct statistical analyses while mitigating the risk of leaking private information. Compositional data, which consist of vectors with positive components that add up to a constant, have received little attention in the differential privacy literature. This article proposes differentially private approaches for analyzing compositional data based on the Dirichlet distribution. We explore several methods, including Bayesian and bootstrap procedures. For the Bayesian methods, we consider posterior inference techniques based on Markov chain Monte Carlo, Approximate Bayesian Computation, and asymptotic approximations. We conduct an extensive simulation study to compare these approaches and make evidence-based recommendations. Finally, we apply the methodology to a dataset from the American Time Use Survey.},
  archive      = {J_JCGS},
  author       = {Qi Guo and Andrés F. Barrientos and Víctor Peña},
  doi          = {10.1080/10618600.2024.2412174},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {848-858},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Differentially private methods for compositional data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCMC for bayesian nonparametric mixture modeling under differential privacy. <em>JCGS</em>, <em>34</em>(3), 837-847. (<a href='https://doi.org/10.1080/10618600.2024.2410911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the probability density of a population while preserving the privacy of individuals in that population is an important and challenging problem that has received considerable attention in recent years. While the previous literature focused on frequentist approaches, in this article, we propose a Bayesian nonparametric mixture model under differential privacy (DP) and present two Markov chain Monte Carlo (MCMC) algorithms for posterior inference. One is a marginal approach, resembling Neal’s algorithm 5 with a pseudo-marginal Metropolis-Hastings move, and the other is a conditional approach. Although our focus is primarily on local DP, we show that our MCMC algorithms can be easily extended to deal with global differential privacy mechanisms. Moreover, for some carefully chosen mechanisms and mixture kernels, we show how auxiliary parameters can be analytically marginalized, allowing standard MCMC algorithms (i.e., non-privatized, such as Neal’s Algorithm 2) to be efficiently employed. Our approach is general and applicable to any mixture model and privacy mechanism. In several simulations and a real case study, we discuss the performance of our algorithms and evaluate different privacy mechanisms proposed in the frequentist literature. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Mario Beraha and Stefano Favaro and Vinayak Rao},
  doi          = {10.1080/10618600.2024.2410911},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {837-847},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {MCMC for bayesian nonparametric mixture modeling under differential privacy},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grid point approximation for distributed nonparametric smoothing and prediction. <em>JCGS</em>, <em>34</em>(3), 824-836. (<a href='https://doi.org/10.1080/10618600.2024.2409817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel smoothing is a widely used nonparametric method in modern statistical analysis. The problem of efficiently conducting kernel smoothing for a massive dataset on a distributed system is a problem of great importance. In this work, we find that the popularly used one-shot type estimator is highly inefficient for prediction purposes. To this end, we propose a novel grid point approximation (GPA) method, which has the following advantages. First, the resulting GPA estimator is as statistically efficient as the global estimator under mild conditions. Second, it requires no communication and is extremely efficient in terms of computation for prediction. Third, it is applicable to the case where the data are not randomly distributed across different machines. To select a suitable bandwidth, two novel bandwidth selectors are further developed and theoretically supported. Extensive numerical studies are conducted to corroborate our theoretical findings. Two real data examples are also provided to demonstrate the usefulness of our GPA method. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yuan Gao and Rui Pan and Feng Li and Riquan Zhang and Hansheng Wang},
  doi          = {10.1080/10618600.2024.2409817},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {824-836},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Grid point approximation for distributed nonparametric smoothing and prediction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network embedding-based directed community detection with unknown community number. <em>JCGS</em>, <em>34</em>(3), 812-823. (<a href='https://doi.org/10.1080/10618600.2024.2409789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection of network analysis plays an important role in numerous application areas, in which estimating the number of communities is a fundamental issue. However, many existing methods focus on undirected networks ignoring the directionality of edges or unrealistically assume that the number of communities is known a priori. In this article, we develop a data-dependent community detection method for the directed network to determine the number of communities and recover community structures simultaneously, which absorbs the ideas of network embedding and penalized fusion by embedding the out- and in-nodes into low-dimensional vector space and forcing the embedding vectors toward its center. The asymptotic consistency properties of the proposed method are established in terms of network embedding, directed community detection, and estimation of the number of communities. The proposed method is applied on synthetic networks and real brain functional networks, which demonstrate the superior performance of the proposed method against a number of competitors. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Qingzhao Zhang and Jinlong Zhou and Mingyang Ren},
  doi          = {10.1080/10618600.2024.2409789},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {812-823},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Network embedding-based directed community detection with unknown community number},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of spatial extremes over large geographical domains. <em>JCGS</em>, <em>34</em>(3), 795-811. (<a href='https://doi.org/10.1080/10618600.2024.2409784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various natural phenomena exhibit spatial extremal dependence at short spatial distances. However, existing models proposed in the spatial extremes literature often assume that extremal dependence persists across the entire domain. This is a strong limitation when modeling extremes over large geographical domains, and yet it has been mostly overlooked in the literature. We here develop a more realistic Bayesian framework based on a novel Gaussian scale mixture model, with the Gaussian process component defined though a stochastic partial differential equation yielding a sparse precision matrix, and the random scale component modeled as a low-rank Pareto-tailed or Weibull-tailed spatial process determined by compactly-supported basis functions. We show that our proposed model is approximately tail-stationary and that it can capture a wide range of extremal dependence structures. Its inherently sparse probabilistic structure allows fast Bayesian computations in high spatial dimensions based on a customized Markov chain Monte Carlo algorithm prioritizing calibration in the tail. We fit our model to analyze heavy monsoon rainfall data in Bangladesh. Our study shows that our model outperforms natural competitors and that it fits precipitation extremes well. We finally use the fitted model to draw inference on long-term return levels for marginal precipitation and spatial aggregates. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Arnab Hazra and Raphaël Huser and David Bolin},
  doi          = {10.1080/10618600.2024.2409784},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {795-811},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient modeling of spatial extremes over large geographical domains},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A latent space model for weighted keyword co-occurrence networks with applications in knowledge discovery in statistics. <em>JCGS</em>, <em>34</em>(3), 779-794. (<a href='https://doi.org/10.1080/10618600.2024.2407465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keywords are widely recognized as pivotal in conveying the central idea of academic articles. In this article, we construct a weighted and dynamic keyword co-occurrence network and propose a latent space model for analyzing it. Our model has two special characteristics. First, it is applicable to weighted networks; however, most previous models were primarily designed for unweighted networks. Simply replacing the frequency of keyword co-occurrence with binary values would result in a significant loss of information. Second, our model can handle the situation where network nodes evolve over time, and assess the effect of new nodes on network connectivity. We use the projected gradient descent algorithm to estimate the latent positions and establish the theoretical properties of the estimators. In the real data application, we study the keyword co-occurrence network within the field of statistics. We identify popular keywords over the whole period as well as within each time period. For keyword pairs, our model provides a new way to assess the association between them. Finally, we observe that the interest of statisticians in emerging research areas has gradually grown in recent years. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yan Zhang and Rui Pan and Xuening Zhu and Kuangnan Fang and Hansheng Wang},
  doi          = {10.1080/10618600.2024.2407465},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {779-794},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A latent space model for weighted keyword co-occurrence networks with applications in knowledge discovery in statistics},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
