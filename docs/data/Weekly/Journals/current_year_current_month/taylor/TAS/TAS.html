<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tas">TAS - 14</h2>
<ul>
<li><details>
<summary>
(2025). Learn r: As a language, 2nd ed.. <em>TAS</em>, <em>79</em>(3), 417-419. (<a href='https://doi.org/10.1080/00031305.2025.2490305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Haihan Yu},
  doi          = {10.1080/00031305.2025.2490305},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {417-419},
  shortjournal = {Am. Stat.},
  title        = {Learn r: As a language, 2nd ed.},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric statistical methods using r, 2nd ed.. <em>TAS</em>, <em>79</em>(3), 416. (<a href='https://doi.org/10.1080/00031305.2025.2484865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Bojana Milošević},
  doi          = {10.1080/00031305.2025.2484865},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {416},
  shortjournal = {Am. Stat.},
  title        = {Nonparametric statistical methods using r, 2nd ed.},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science in practice. <em>TAS</em>, <em>79</em>(3), 416-417. (<a href='https://doi.org/10.1080/00031305.2025.2490304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Xiao Hui Tai},
  doi          = {10.1080/00031305.2025.2490304},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {416-417},
  shortjournal = {Am. Stat.},
  title        = {Data science in practice},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connections between statistics and Mathematics/Probability. <em>TAS</em>, <em>79</em>(3), 410-415. (<a href='https://doi.org/10.1080/00031305.2025.2453230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many connections between probability, other mathematics courses, and statistics. Understanding these connections provides insights that might not be fully appreciated when considering each discipline in isolation. While the typical instruction of statistics courses relies on elucidating its foundational principles from mathematical and probability theory, it is generally less appreciated that statistics can in turn provide a deeper understanding of results in mathematics and probability. We offer several examples for which knowledge of statistics can shed new light on probability and other mathematics results. Examples span both undergraduate and graduate level material. In today’s data driven-world, many students are naturally curious about statistics and are exposed to this field early in their undergraduate curriculum. Leveraging connections between statistics and mathematics and probability makes theoretical concepts more intuitive and relevant, fostering a better understanding.},
  archive      = {J_TAS},
  author       = {Michael A. Proschan and Pamela A. Shaw},
  doi          = {10.1080/00031305.2025.2453230},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {410-415},
  shortjournal = {Am. Stat.},
  title        = {Connections between statistics and Mathematics/Probability},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytics, have some humility: A statistical view of fourth-down decision making. <em>TAS</em>, <em>79</em>(3), 393-409. (<a href='https://doi.org/10.1080/00031305.2025.2475801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard mathematical approach to fourth-down decision-making in American football is to make the decision that maximizes estimated win probability. Win probability estimates arise from machine learning models fit from historical data. These models attempt to capture a nuanced relationship between a noisy binary outcome variable and game-state variables replete with interactions and non-linearities from a finite dataset of just a few thousand games. Thus, it is imperative to knit uncertainty quantification into the fourth-down decision procedure; we do so using bootstrapping. We find that uncertainty in the estimated optimal fourth-down decision is far greater than that currently expressed by sports analysts in popular sports media.},
  archive      = {J_TAS},
  author       = {Ryan S. Brill and Ronald Yurko and Abraham J. Wyner},
  doi          = {10.1080/00031305.2025.2475801},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {393-409},
  shortjournal = {Am. Stat.},
  title        = {Analytics, have some humility: A statistical view of fourth-down decision making},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An example to illustrate randomized trial estimands and estimators. <em>TAS</em>, <em>79</em>(3), 383-392. (<a href='https://doi.org/10.1080/00031305.2025.2468399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the International Conference on Harmonisation finalized an estimand framework for randomized trials that was adopted by regulatory bodies worldwide. The framework introduced five strategies for handling post-randomization events; namely the treatment policy, composite variable, while on treatment, hypothetical and principal stratum estimands. We describe an illustrative example to elucidate the difference between these five strategies for handling intercurrent events and provide an estimation technique for each. Specifically, we consider the intercurrent event of treatment discontinuation and introduce potential outcome notation to describe five estimands and corresponding estimators: (1) an intention-to-treat estimator of the total effect of a treatment policy; (2) an intention-to-treat estimator of a composite of the outcome and remaining on treatment; (3) a per-protocol estimator of the outcome in individuals observed to remain on treatment; (4) a g-computation estimator of a hypothetical scenario that all individuals remain on treatment; and (5) a principal stratum estimator of the treatment effect in individuals who would remain on treatment under the experimental condition. Additional insight is provided by defining situations where certain estimands are equal, and by studying the while on treatment strategy under repeated outcome measures. We highlight relevant causal inference literature to enable adoption in practice.},
  archive      = {J_TAS},
  author       = {Linda J. Harrison and Sean S. Brummel},
  doi          = {10.1080/00031305.2025.2468399},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {383-392},
  shortjournal = {Am. Stat.},
  title        = {An example to illustrate randomized trial estimands and estimators},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible distributed lag models for count data using mgcv. <em>TAS</em>, <em>79</em>(3), 371-382. (<a href='https://doi.org/10.1080/00031305.2025.2505514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this tutorial we present the use of R package mgcv to implement Distributed Lag Non-Linear Models (DLNMs) in a flexible way. Interpretation of smoothing splines as random quantities enables approximate Bayesian inference, which in turn allows uncertainty quantification and comprehensive model checking. We illustrate various modeling situations using open-access epidemiological data in conjunction with simulation experiments. We demonstrate the inclusion of temporal structures and the use of mixture distributions to allow for extreme outliers. Moreover, we demonstrate interactions of the temporal lagged structures with other covariates with different lagged periods for different covariates. Spatial structures are also demonstrated, including smooth spatial variability and Markov random fields, in addition to hierarchical formulations to allow for non-structured dependency. Posterior predictive simulation is used to ensure models verify well against the data.},
  archive      = {J_TAS},
  author       = {Theo Economou and Daphne Parliari and Aurelio Tobias and Laura Dawkins and Hamish Steptoe and Christophe Sarran and Oliver Stoner and Rachel Lowe and Jos Lelieveld},
  doi          = {10.1080/00031305.2025.2505514},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {371-382},
  shortjournal = {Am. Stat.},
  title        = {Flexible distributed lag models for count data using mgcv},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing spatial point patterns in digital pathology: Immune cells in high-grade serous ovarian carcinomas. <em>TAS</em>, <em>79</em>(3), 355-370. (<a href='https://doi.org/10.1080/00031305.2025.2459280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplex immunofluorescence (mIF) imaging technology facilitates the study of the tumor microenvironment in cancer patients. Due to the capabilities of this emerging bioimaging technique, it is possible to statistically analyze, for example, the co-varying location and functions of multiple different types of immune cells. Complex spatial relationships between different immune cells have been shown to correlate with patient outcomes and may reveal new pathways for targeted immunotherapy treatments. This tutorial reviews methods and procedures relating to spatial point patterns for complex data analysis. We consider tissue cells as a realization of a spatial point process for each patient. We focus on proper functional descriptors for each observation and techniques that allow us to obtain information about inter-patient variation. Ovarian cancer is the deadliest gynaecological malignancy and can resist chemotherapy treatment effective in cancers. We use a dataset of high-grade serous ovarian cancer samples from 51 patients. We examine the immune cell composition (T cells, B cells, macrophages) within tumors and additional information such as cell classification (tumor or stroma) and other patient clinical characteristics. Our analyses, supported by reproducible software, apply to other digital pathology datasets.},
  archive      = {J_TAS},
  author       = {Jonatan A. González and Julia Wrobel and Simon Vandekar and Paula Moraga},
  doi          = {10.1080/00031305.2025.2459280},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {355-370},
  shortjournal = {Am. Stat.},
  title        = {Analyzing spatial point patterns in digital pathology: Immune cells in high-grade serous ovarian carcinomas},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Play-by-play volleyball win probability model. <em>TAS</em>, <em>79</em>(3), 345-354. (<a href='https://doi.org/10.1080/00031305.2025.2490786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a volleyball point-by-point win probability model that updates the probability of winning a set after each play in the set. The covariate informed product partition model (PPMx) is well suited to flexibly include in-set team performance information when making predictions. However, making predictions in real time would be too expensive computationally as it would require refitting the PPMx after each play. Instead, we develop a predictive procedure based on a single training of the PPMx that predicts in real-time. We deploy this procedure using data from the 2018 Men’s World Volleyball Championship. The procedure first trains a PPMx model using end-of-set team performance statistics from the round robin stage of the tournament. Then based on the PPMx predictive distribution, we predict the win probability after every play of every match in the knockout stages. Finally, we show how the prediction procedure can be enhanced by including pre-set information toward the beginning of the set and set score toward the end.},
  archive      = {J_TAS},
  author       = {Nathan Hawkins and Gilbert W. Fellingham and Garritt L. Page},
  doi          = {10.1080/00031305.2025.2490786},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {345-354},
  shortjournal = {Am. Stat.},
  title        = {Play-by-play volleyball win probability model},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-form power and sample size calculations for bayes factors. <em>TAS</em>, <em>79</em>(3), 330-344. (<a href='https://doi.org/10.1080/00031305.2025.2467919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining an appropriate sample size is a critical element of study design, and the method used to determine it should be consistent with the planned analysis. When the planned analysis involves Bayes factor hypothesis testing, the sample size is usually desired to ensure a sufficiently high probability of obtaining a Bayes factor indicating compelling evidence for a hypothesis, given that the hypothesis is true. In practice, Bayes factor sample size determination is typically performed using computationally intensive Monte Carlo simulation. Here, we summarize alternative approaches that enable sample size determination without simulation. We show how, under approximate normality assumptions, sample sizes can be determined numerically, and provide the R package bfpwr for this purpose. Additionally, we identify conditions under which sample sizes can even be determined in closed-form, resulting in novel, easy-to-use formulas that also help foster intuition, enable asymptotic analysis, and can also be used for hybrid Bayesian/likelihoodist design. Furthermore, we show how power and sample size can be computed without simulation for more complex analysis priors, such as Jeffreys-Zellner-Siow priors or non-local normal moment priors. Case studies from medicine and psychology illustrate how researchers can use our methods to design informative yet cost-efficient studies.},
  archive      = {J_TAS},
  author       = {Samuel Pawel and Leonhard Held},
  doi          = {10.1080/00031305.2025.2467919},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {330-344},
  shortjournal = {Am. Stat.},
  title        = {Closed-form power and sample size calculations for bayes factors},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A class of regression association measures based on concordance. <em>TAS</em>, <em>79</em>(3), 320-329. (<a href='https://doi.org/10.1080/00031305.2024.2448431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of regression association aiming at predictability of a dependent variable Y from an independent variable X have received considerable attention recently. In this article, we provide a unified discussion of some existing measures, including their rationale, properties, and estimation. Motivated by these measures, we consider a general class of regression association measures which views the regression association of Y from X as the association of two independent replications from the conditional distribution of Y given X . We illustrate that the so-called Markov product copulas can be employed as a neat and convenient building block for this class of measures, and the measures so constructed can be expressed as a common form of the proportion of the variance of some function of Y that can be explained by X , rendering the measures a direct interpretation in terms of predictability. Also, the notion of two independent replications from the conditional distribution leads to a simple nonparametric estimation method based on the induced order statistics, hence, no smoothing techniques are required. Under the considered general framework, the performances and utilities of the regression association measures are examined through simulations and real data applications.},
  archive      = {J_TAS},
  author       = {Jia-Han Shih and Yi-Hau Chen},
  doi          = {10.1080/00031305.2024.2448431},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {320-329},
  shortjournal = {Am. Stat.},
  title        = {A class of regression association measures based on concordance},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace’s law of succession estimator and M-statistics. <em>TAS</em>, <em>79</em>(3), 311-319. (<a href='https://doi.org/10.1080/00031305.2024.2448430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic formula for estimating the binomial probability as the proportion of successes contradicts common sense for extreme probabilities when the event never occurs or occurs every time. Laplace’s law of succession estimator, one of the first applications of Bayesian statistics, has been around for over 250 years and resolves the paradoxes, although rarely discussed in modern statistics texts. This work aims to introduce a new theory for exact optimal statistical inference using Laplace’s law of succession estimator as a motivating example. We prove that this estimator may be viewed from a different theoretical perspective as the limit point of the short confidence interval on the double-log scale when the confidence level approaches zero. This motivating example paves the road to the definition of an estimator as the inflection point on the cumulative distribution function as a function of the parameter given the observed statistic. This estimator has the maximum infinitesimal probability of the coverage of the unknown parameter and, therefore, is called the maximum concentration (MC) estimator as a part of a more general M-statistics theory. The new theory is illustrated with exact optimal confidence intervals for the normal standard deviation and the respective MC estimators.},
  archive      = {J_TAS},
  author       = {Eugene Demidenko},
  doi          = {10.1080/00031305.2024.2448430},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {311-319},
  shortjournal = {Am. Stat.},
  title        = {Laplace’s law of succession estimator and M-statistics},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient computation strategy for generalized single-index models and their variants by integrating with GAM. <em>TAS</em>, <em>79</em>(3), 302-310. (<a href='https://doi.org/10.1080/00031305.2025.2464854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various generalizations of single-index models and associated estimation methods have been developed. However, implementing these developed methods requires much effort to program, case by case, due to the lack of a common and flexible vehicle to cover them. We suggest an efficient computation strategy for easily estimating parameters and nonparametric functions in generalized single-index models and generalized partially linear single-index models by integrating with well-developed algorithms and packages for estimating the generalized additive models (Wood; Hastie and Tibshirani, GAM). Such an integration makes estimation in these index-type models much easier, expedient, and flexible and brings a lot of convenience. We briefly introduce the principle and extensively examine numerical performance for various scenarios. Numerical experiments indicate that the proposed strategy works well with finite sample sizes and is especially flexible to model structures. Finally, we analyze two real-data examples as an illustration.},
  archive      = {J_TAS},
  author       = {Ximin Li and Haozhe Liang and Hua Liang},
  doi          = {10.1080/00031305.2025.2464854},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {302-310},
  shortjournal = {Am. Stat.},
  title        = {An efficient computation strategy for generalized single-index models and their variants by integrating with GAM},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiple imputation approach for the cumulative incidence, with implications for variance estimation. <em>TAS</em>, <em>79</em>(3), 291-301. (<a href='https://doi.org/10.1080/00031305.2025.2453674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an alternative approach to estimating the cumulative incidence function that uses nonparametric multiple imputation to reduce the problem to that of estimating a binomial proportion. In the standard competing risks setting, we show mathematically and empirically that our imputation-based estimator is equivalent to the Aalen-Johansen estimator of the cumulative incidence given a sufficient number of imputations. However, our approach allows for the use of a wider variety of methods for the analysis of binary outcomes, including preferred options for uncertainty estimation. While we focus on the cumulative incidence function, the multiple imputation approach likely extends to more complex problems in competing risks.},
  archive      = {J_TAS},
  author       = {Elizabeth C. Chase and Philip S. Boonstra and Jeremy M. G. Taylor},
  doi          = {10.1080/00031305.2025.2453674},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {291-301},
  shortjournal = {Am. Stat.},
  title        = {A multiple imputation approach for the cumulative incidence, with implications for variance estimation},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
