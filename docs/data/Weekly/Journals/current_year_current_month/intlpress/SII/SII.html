<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SII</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sii">SII - 10</h2>
<ul>
<li><details>
<summary>
(2025). Bayesian variable selection for semiparametric zero-inflated longitudinal count data in the presence of non-ignorable missingness. <em>SII</em>, <em>18</em>(4), 569-582. (<a href='https://doi.org/10.4310/SII.250515195908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the issue of zero-inflated longitudinal data in the presence of non-ignorable missingness. To account for the presence of additional zeros, a zero-inflated power series mixed effects model is utilized. This model incorporates spline functions, such as B-spline, to capture the non-linear impact of time on the longitudinal outcome. Also, missing data is a common problem in longitudinal studies. To address this issue, a shared random effects model is utilized. Further, the proposed model addresses a variable selection technique using Dirac spike priors. The MCMC method is used for posterior sampling, which involves the Metropolis-Hastings algorithm within Gibbs sampling. Some simulation studies are performed to investigate the performance of the proposed approach, and it is also applied to analyze a real dataset in a RAND health insurance experiment.},
  archive      = {J_SII},
  author       = {Nawar, Alsalim and Taban, Baghfalaki and Mojtaba, Ganjali and Sultan, Salem},
  doi          = {10.4310/SII.250515195908},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {569-582},
  shortjournal = {Stat. Interface},
  title        = {Bayesian variable selection for semiparametric zero-inflated longitudinal count data in the presence of non-ignorable missingness},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequential estimation with dorfman and halving pooling. <em>SII</em>, <em>18</em>(4), 551-567. (<a href='https://doi.org/10.4310/SII.250522055246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing has recently shown great potential to screen for infectious diseases in a large population with low prevalence, which can considerably reduce costs compared to testing specimens individually. However, a common problem encountered in most existing studies is that it is impractical to obtain additional massive specimens to increase the precision of the estimates. This paper establishes a mixture model while considering continuous biomarker levels (e.g., antibody concentrations) from group samples, which adopts different grouping strategies that incorporate retesting results, Dorfman and Halving, for estimation. We demonstrate the effectiveness of our proposal through several simulation cases, and with application to the plasma glucose data collected from the National Health and Nutrition Examination Survey.},
  archive      = {J_SII},
  author       = {Zhong, Yunning and Wu, Tao and Wei, Hongyu and Chen, Lifei},
  doi          = {10.4310/SII.250522055246},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {551-567},
  shortjournal = {Stat. Interface},
  title        = {A sequential estimation with dorfman and halving pooling},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multivariate robust integrated test for genetic pleiotropy. <em>SII</em>, <em>18</em>(4), 539-550. (<a href='https://doi.org/10.4310/SII.250522054924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic pleiotropy means that a single gene influences more than one trait and the detecting of the genetic pleiotropy can help the biological understanding of a gene among others. Although several multivariate test procedures have been proposed for the problem in the literature, most of them apply only to the situation where the error follows a multivariate normal distribution. In other words, they cannot detect the pleiotropy if the data contain outliers or the error follows a heavy-tailed distribution. To overcome these difficulties, we propose a new multivariate robust integrated test and establish its theoretical properties. An extensive simulation study is conducted to illustrate the efficiency and robustness of the proposled procedure. In particular, it suggests that the method can reach high power and control the asymptotic type I error well.},
  archive      = {J_SII},
  author       = {Huang, Qiyue and Tong, Xingwei and Sun, Jianguo},
  doi          = {10.4310/SII.250522054924},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {539-550},
  shortjournal = {Stat. Interface},
  title        = {A multivariate robust integrated test for genetic pleiotropy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel subsampling strategies for heavily censored reliability data. <em>SII</em>, <em>18</em>(4), 511-538. (<a href='https://doi.org/10.4310/SII.250522054738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational capability often falls short when confronted with massive data, posing a common challenge in establishing a statistical model or statistical inference method dealing with big data. While subsampling techniques have been extensively developed to downsize the data volume, there is a notable gap in addressing the unique challenge of handling extensive reliability data, in which a common situation is that a large proportion of data is censored. In this article, we propose an efficient subsampling method for reliability analysis in the presence of censoring data, intending to estimate the parameters of lifetime distribution. Moreover, a novel subsampling method for subsampling from severely censored data is proposed, i.e., only a tiny proportion of data is complete. The subsamplingbased estimators are given, and their asymptotic properties are derived. The optimal subsampling probabilities are derived through the L-optimality criterion, which minimizes the trace of the product of the asymptotic covariance matrix and a constant matrix. Efficient algorithms are proposed to implement the proposed subsampling methods to address the challenge that optimal subsampling strategy depends on unknown parameter estimation from full data. Real-world hard drive dataset case and simulative empirical studies are employed to demonstrate the superior performance of the proposed methods.},
  archive      = {J_SII},
  author       = {Ruan, Yixiao and Li, Zan and Li, Zhaohui and Lin, Dennis K. J. and Hu, Qingpei and Yu, Dan},
  doi          = {10.4310/SII.250522054738},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {511-538},
  shortjournal = {Stat. Interface},
  title        = {Novel subsampling strategies for heavily censored reliability data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new gamma frailty joint modeling approach for recurrent event and terminal event. <em>SII</em>, <em>18</em>(4), 497-509. (<a href='https://doi.org/10.4310/SII.250522053733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new joint gamma frailty modeling approach for recurrent event data with informative terminal event by adopting a new type of double exponential Cox model to the terminal event. The proposed model overcomes the drawback that the marginal effects of covariates die out over time in gamma frailty Cox models. A sieve maximum likelihood approach is carried out for parameter estimation, and the Bernstein polynomials are employed to approximate the non-decreasing cumulative baseline functions. The EM algorithm is utilized for optimization. Asymptotic properties of the estimators are provided. Simulation studies are conducted to evaluate the finite sample behavior of the proposed estimators. A real dataset of readmissions of patients diagnosed with colorectal cancer is analyzed for illustration.},
  archive      = {J_SII},
  author       = {Zhou, Jie and Xie, Mengqi and Liu, Lei},
  doi          = {10.4310/SII.250522053733},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {497-509},
  shortjournal = {Stat. Interface},
  title        = {A new gamma frailty joint modeling approach for recurrent event and terminal event},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture of partially functional linear regression. <em>SII</em>, <em>18</em>(4), 487-496. (<a href='https://doi.org/10.4310/SII.250522053330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a new mixture of partially functional linear models to characterize the relationship between a scalar response and mixture predictors of functional and covariate vector. The mixing proportions are allowed to change with a covariate, enhancing the flexibility of the proposed model. Utilizing basis function expansion, we develop a modified backfitting EM algorithm to estimate the regression functions. Furthermore, based on Fisher's method, a maximum form test statistics, combining the p -values of each component of mixtures, is proposed for depicting whether the mixing proportions actually depend on the covariate. The asymptotic properties of the resulting estimators are established under mild conditions. To assess the finite sample performance of the estimators, several simulation studies are conducted. Finally, we analyze the motivating datasets to illustrate the powerfulness of the proposed procedure.},
  archive      = {J_SII},
  author       = {Ren, Pengcheng and Yan, Xingyu and Zhao, Peng},
  doi          = {10.4310/SII.250522053330},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {487-496},
  shortjournal = {Stat. Interface},
  title        = {Mixture of partially functional linear regression},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subgroup analysis and homogeneous effect selection in longitudinal data. <em>SII</em>, <em>18</em>(4), 459-486. (<a href='https://doi.org/10.4310/SII.250522013545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a penalized regression approach to simultaneously identify subgroups and homogeneity structures in longitudinal data analysis. Unlike conventional subgroup models, the proposed approach can partition all the effects into homogeneous effects shared by all individuals and heterogeneous effects that have group structures, without requiring prior knowledge to differentiate between heterogeneous and homogeneous effects. Theoretically, we show that the group structure and the homogeneous patterns can be recovered with probability approaching one. An efficient algorithm based on the alternating direction method of multipliers (ADMM) algorithm is developed for computational feasibility. Simulation studies demonstrate the effectiveness of the proposed approach with finite samples, and a real data example is provided for illustration.},
  archive      = {J_SII},
  author       = {Wang, Han and Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.4310/SII.250522013545},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {459-486},
  shortjournal = {Stat. Interface},
  title        = {Subgroup analysis and homogeneous effect selection in longitudinal data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional bayesian mediation analysis with adaptive laplace priors. <em>SII</em>, <em>18</em>(4), 445-457. (<a href='https://doi.org/10.4310/SII.250521220937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mediation analysis method is used to investigate effects of mediators that intervene in the pathways between an exposure variable and an outcome variable. Bayesian methods are naturally used in mediation analysis due to the hierarchical structure of Bayesian models. This paper introduces an innovative adaptive Bayesian mediation analysis method that incorporates adaptive Laplace priors into the predictive model to account for high-dimensional mediators. This approach introduces a penalization function on the estimated direct and indirect effects rather than solely on the coefficients of predictive models. Consequently, estimated effects that lack statistical significance may shrink to zero, facilitating a more robust analysis. We demonstrate the efficacy of our adaptive mediation analysis method on simulations and on a Louisiana triple negative breast cancer (TNBC) dataset to examine racial disparity in diagnosed stage among TNBC patients diagnosed between 2010 and 2017. The dataset is linked to the 2017 hazardous air pollutant emissions burden estimation database using patients' residential address. We effectively explain a portion of the disparity using currently collected variables. The analysis identifies crucial mediators and confounders, highlighting the significance of variables such as age of diagnosis, insurance status, tumor grades, and the concentration of Naphtha in the air.},
  archive      = {J_SII},
  author       = {Yu, Qingzhao and Hagan, Joseph and Wu, Xiaocheng and Richmond-Bryant, Jennifer and Urbanek, Norman and Li, Bin},
  doi          = {10.4310/SII.250521220937},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {445-457},
  shortjournal = {Stat. Interface},
  title        = {High-dimensional bayesian mediation analysis with adaptive laplace priors},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for ratios of proportions in stratified bilateral correlated data. <em>SII</em>, <em>18</em>(4), 411-443. (<a href='https://doi.org/10.4310/SII.250522013240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence interval (CI) methods for stratified bilateral studies use intraclass correlation to avoid misleading results. In this article, we propose four CI methods (sample-size weighted global MLE-based Wald-type CI, complete MLE-based Wald-type CI, profile likelihood CI, and complete MLE-based score CI) to investigate CIs of proportion ratios to clinical trial design with stratified bilateral data under Dallal's intraclass model. Monte Carlo simulations are performed, and the complete MLE-based score confidence interval (CS) method yields a robust outcome. Lastly, two real data examples are conducted to illustrate the proposed four CIs.},
  archive      = {J_SII},
  author       = {Tian, Wanqing and Ma, Chang-Xing},
  doi          = {10.4310/SII.250522013240},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {411-443},
  shortjournal = {Stat. Interface},
  title        = {Confidence intervals for ratios of proportions in stratified bilateral correlated data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic tests for serial correlation and ARCH effect of high-dimensional time series. <em>SII</em>, <em>18</em>(4), 399-410. (<a href='https://doi.org/10.4310/SII.250522012841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a norm-rank-based automatic test for detecting serial correlation and ARCH effect in high-dimensional time series (HDTS). The proposed automatic test is based on the Spearman's rank autocorrelations of the L u -norm of the HDTS up to lag m , where the values of u and m are chosen by a completely data-driven method. The asymptotic null distribution of this automatic test is established without assuming any moment condition of HDTS, so this automatic test has a large application scope covering the commonly observed heavy-tailed data. To account for the possible scaling effect, another standardized norm-rankbased automatic test is further proposed. Simulations and one real example are given to demonstrate the advantage of these two automatic tests over the portmanteau tests, which seek the rejection evidence only from the L 1 -norm of HDTS, perform unstably across the user-chosen value of m , and have unsatisfactory power for small sample sizes.},
  archive      = {J_SII},
  author       = {Zhang, Bingbing and Liu, Mengya and Yan, Ting and Zhu, Ke},
  doi          = {10.4310/SII.250522012841},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {399-410},
  shortjournal = {Stat. Interface},
  title        = {Automatic tests for serial correlation and ARCH effect of high-dimensional time series},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
