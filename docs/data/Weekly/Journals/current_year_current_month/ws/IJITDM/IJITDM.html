<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJITDM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijitdm">IJITDM - 76</h2>
<ul>
<li><details>
<summary>
(2025). A comparison between fuzzy TOPSIS and VIKOR to the selection of aircraft for airspace defense. <em>IJITDM</em>, <em>24</em>(7), 2189-2224. (<a href='https://doi.org/10.1142/S0219622025500348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2013, the Spanish fleet of EF-18 fighters has not received major updates nor any other jet has been proposed for their replacement. Thereby, a decision problem of interest to the Spanish Air Force is addressed in this study. In this regard, a collection of six potential alternatives (Eurofighter Typhoon, F-35 Lightning II, F/A-18 Super Hornet, Dassault Rafale, F-15 EX, and Saab 39 Gripen) and 14 criteria (3 of which being qualitative) are evaluated based on expert consensus by a combination of fuzzy logic and multi-criteria decision making (MCDM) approaches (AHP-VIKOR/AHP-TOPSIS). The weights of the criteria and the valuation of the qualitative ones (via questionnaires) were made possible by the participation of two independent groups of experts, all of them fighter pilots. Once the extraction of knowledge was carried out via a fuzzy version of a modified AHP, it was found that the two most important criteria (“air superiority over neighbor countries” and “tactical capability”) were qualitative. A comparison between the rankings of the alternatives provided by fuzzy VIKOR and fuzzy TOPSIS approaches highlights that the fuzzy VIKOR ranking appears highly influenced by the most important criteria. On the contrary, fuzzy TOPSIS presents a more compensatory behavior than fuzzy VIKOR. Given the differences observed when comparing such rankings of alternatives, a triple sensitivity analysis was conducted to ensure robustness. Our results highlight the Dassault Rafale as the most promising option for replacement. This finding suggests its potential as a benchmark for future fighter selection and next-generation fighter planning.},
  archive      = {J_IJITDM},
  author       = {Juan Miguel Sánchez–Lozano and M. Fernández–Martínez and Mariano Alonso Larraz},
  doi          = {10.1142/S0219622025500348},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2189-2224},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A comparison between fuzzy TOPSIS and VIKOR to the selection of aircraft for airspace defense},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive modeling for identifying undervalued stocks using machine learning. <em>IJITDM</em>, <em>24</em>(7), 2163-2188. (<a href='https://doi.org/10.1142/S0219622025500336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the application of Machine Learning (ML) techniques to identify undervalued stocks, addressing the limitations of traditional investment strategies that rely heavily on fundamental analysis. As financial markets become more complex, characterized by volatility and information asymmetry, conventional valuation methods often struggle to capture these dynamics. In contrast, ML offers the ability to analyze large datasets and uncover intricate patterns, presenting a data-driven alternative for stock selection and portfolio optimization. A comprehensive predictive framework was developed, integrating traditional financial ratios with novel features derived from value investing principles and technical analysis. Several ML models — Random Forest, Long Short-Term Memory (LSTM), and Support Vector Machines — were assessed for their ability to predict high-return stocks. Performance metrics, including accuracy, precision, and recall, were used to evaluate model effectiveness. Among the models tested, the LSTM demonstrated the highest accuracy at 0.81, proving its robustness in identifying undervalued stocks. This research contributes to the growing body of literature on ML in finance by offering a practical framework that bridges theoretical concepts with real-world applications. The study also emphasizes the importance of refining ML algorithms to improve model interpretability and transparency, crucial for fostering trust in these systems. Future research should explore the use of ensemble methods and alternative data sources to further enhance prediction accuracy, while addressing challenges related to accountability in ML-driven investment strategies. This work advances the conversation around algorithmic trading and the future of data-driven finance.},
  archive      = {J_IJITDM},
  author       = {Narongsak Sukma and Chakkrit Snae Namahoot},
  doi          = {10.1142/S0219622025500336},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2163-2188},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Predictive modeling for identifying undervalued stocks using machine learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing interestingness evaluation in ontology-based association rules: A case study on US birth data. <em>IJITDM</em>, <em>24</em>(7), 2139-2162. (<a href='https://doi.org/10.1142/S0219622025500324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of association rule mining, evaluating the interestingness of discovered rules plays a crucial role in extracting meaningful patterns. However, the context of interestingness poses challenges that call for improvements in rule evaluation. This study focuses on addressing this problem by enhancing the evaluation of interestingness in ontology-based association rules. In this study, we present the effective rule evaluation using the ontology (EREO) model, which aims to evaluate the interestingness of ontology-based association rules in the context of US birth data. The EREO model incorporates three levels of rule evaluation: the utilization of proposed effective measures, consultation with domain experts, and the utilization of AI-based methods. To indirectly evaluate the interestingness of ontology-based rules, we propose two effective measures: Ontology-based rule specificity (ORS) and ontology-based rule complexity (ORC). Rule evaluation is further facilitated by domain experts and AI-based methods, employing an interestingness measurement scale (IMS). Furthermore, we compare the average interestingness scores obtained from ORS, ORC, and the EREO model with those derived from traditional interestingness measures. Our findings demonstrate that the proposed interestingness measures consistently outperform the traditional ones, as indicated by higher average scores. Additionally, we observe a positive relationship between the interestingness scores obtained using the three levels of the EREO model. Overall, this study effectively showcases the efficacy of ontology-based association rule evaluation in improving the quality of discovered rules and supporting informed decision-making processes.},
  archive      = {J_IJITDM},
  author       = {C B Abhilash and Kavi Mahesh},
  doi          = {10.1142/S0219622025500324},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2139-2162},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Enhancing interestingness evaluation in ontology-based association rules: A case study on US birth data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A MADRL-based credit allocation approach for interactive multi-agents. <em>IJITDM</em>, <em>24</em>(7), 2117-2137. (<a href='https://doi.org/10.1142/S0219622025500312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-agent systems (MAS), the interactions and credit allocation among agents are essential for achieving efficient cooperation. To enhance the interactivity and efficiency of credit allocation in multi-agent reinforcement learning, we introduce a credit allocation for interactive multi-agents method (CAIM). CAIM not only considers the effects of various actions on other agents but also leverages attention mechanisms to handle the mismatch between observations and actions. With a unique credit allocation strategy, agents can more precisely assess their contributions during collaboration. Experiments in various adversarial scenarios within the SMAC benchmark environment indicate that CAIM markedly outperforms existing multi-agent reinforcement learning approaches. Further ablation studies confirm the effectiveness of each CAIM component. This research presents a new paradigm for enhancing collaboration efficiency and overall performance in MAS.},
  archive      = {J_IJITDM},
  author       = {Ershen Wang and Xiaotong Wu and Chen Hong and Xinna Shang and Peifeng Wu and Chenglong He and Pingping Qu},
  doi          = {10.1142/S0219622025500312},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2117-2137},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A MADRL-based credit allocation approach for interactive multi-agents},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of companies in applied education: Determining the criteria and a case study at an industrial university. <em>IJITDM</em>, <em>24</em>(7), 2079-2115. (<a href='https://doi.org/10.1142/S0219622025500300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universities are transitioning to applied education models within the framework of university–industry cooperation. In this case, both students and universities face the problem of choosing suitable companies. There is no study in the literature that deals with the solution of this problem. Thus, this study discusses companies’ evaluation and selection for the applied education model. A new hybrid methodology is proposed. The methodology consists of the following activities: determining the evaluation criteria with expert opinions, weighting the criteria with AHP, and ranking the companies with TOPSIS and PROMETHEE. The methodology has been tested in a case study with real data from an industrial university. While TOPSIS is effective for certain criteria, the findings suggest that PROMETHEE may provide more effective results specifically for company evaluations within the applied education model, aligning better with the study’s objectives. The applied education model is expected to yield several key outcomes: For company managers, it will lead to increased efficiency by hiring graduates who already have practical experience. From the educators’ perspective, it will result in the training of graduates who are both experienced and highly employable. For students, the model will enhance their employability aligning their skills with their career aspirations and increasing their chances of employment in companies, that match their competencies and preferences after graduation.},
  archive      = {J_IJITDM},
  author       = {Sema Çiftçi and Mehmet Pınarbaşı and HacıMehmet Alakaş},
  doi          = {10.1142/S0219622025500300},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2079-2115},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Evaluation of companies in applied education: Determining the criteria and a case study at an industrial university},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining nonderivable association rules using a genetic algorithm and a tree-based pruning technique. <em>IJITDM</em>, <em>24</em>(7), 2043-2078. (<a href='https://doi.org/10.1142/S0219622025500294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association rule mining is a method for searching databases for patterns or co-occurrences in data. The fundamental step in extracting association rules is mining frequent item sets. However, the itemsets extracted were found to have redundancy in them. This study aims to tackle the redundancy present in the generated association rules and itemsets. One commonly used technique to address redundancy is extracting nonderivable itemsets. The traditional approach in mining such itemsets involved checking for upper and lower bound inequalities by making a breadth-wise search of the dataset. However, this generated many candidate itemsets, increasing the algorithm’s run time and memory consumption. Therefore, it became imperative to develop a technique that addressed these shortcomings. A new approach utilizing a genetic algorithm has been proposed to extract the necessary nonderivable itemsets. To our knowledge, such an approach has not been incorporated so far in extracting nonderivable itemsets. Here operations such as crossover and mutation are applied to generate the required itemsets and this was found to be faster and more efficient in memory in comparison to existing approaches based on distribution and discretization of data. Another problem of pressing concern is the redundancy that prevailed in the association rules that were extracted from the mined itemsets. Traditional approaches involved making repeated scans of the mined collection of association rules to prune out redundant rules. Due to this, the algorithm’s runtime increased. To handle this drawback our work proposes a novel approach based on tree-based pruning to remove redundant association rules. The left and right expansion principles provide the foundation of the tree’s construction. The Minimal Antecedent and Maximal Consequent concepts are used to eliminate redundant rules and produce the necessary set of nonderivable association rules. Based on the findings of this study, it is evident that the suggested method improves run time overall by 48.77% and memory by 35.65%. Concerning metrics such as Precision, Recall, and Accuracy an aggregate improvement of 94.92%, 90.8%, and 91.21% was observed in comparison to existing approaches.},
  archive      = {J_IJITDM},
  author       = {P. P. Jashma Suresh and U. Dinesh Acharya and N. V. Subba Reddy},
  doi          = {10.1142/S0219622025500294},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2043-2078},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Mining nonderivable association rules using a genetic algorithm and a tree-based pruning technique},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation and benchmarking trauma patients in intensive care units using a novel decision-making framework. <em>IJITDM</em>, <em>24</em>(7), 2005-2041. (<a href='https://doi.org/10.1142/S0219622025500282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trauma patients often face complex health consequences and are at a risk of rapid deterioration. Prioritizing trauma patients in intensive care units (ICUs) is essential for timely intervention. Current prioritization approaches are challenging tasks that involve considering multiple evaluation criteria, trade-offs and criteria importance, thus requiring a robust multicriteria decision-making (MCDM) approach. In this study, we propose a novel MCDM framework for the evaluation and benchmarking of trauma patients on the basis of health control (HC) criteria derived from real trauma data. The framework consists of two main phases. In the first phase, a dataset of adult trauma patients referred to a trauma network in the West Midlands region of the United Kingdom is identified and preprocessed. This dataset covers the period from 15 May 2014 to 16 December 2016, providing continuous monitoring of the patients. In the second phase, MCDM methods are employed to develop a dynamic decision matrix (DM) that assesses 35 trauma patients on the basis of 16 HC trauma criteria. Using 2-tuple spherical fuzzy linguistic numbers with fuzzy-weighted zero-inconsistency (2TSFLNs-FWZIC), the framework ensures accurate weighting of the criteria, with C1=PS14 and C5=NISS receiving the highest weight values (0.062606) and C2=CD3+8+(106/L) receiving the lowest weight value (0.062353). This weighting process is guided by the input and expertise of a panel of five emergency medicine specialists who have experience in trauma patient management. The results indicate that CoCoSo effectively ranks patients from the least critical ( k = 1 . 3 4 8 7 2 5 ) to the most critical cases ( k = 1 . 6 2 2 0 5 3 ) for ICU admission. The proposed framework is evaluated via systematic ranking and sensitivity analysis, providing validation measures for its performance, robustness and reliability.},
  archive      = {J_IJITDM},
  author       = {A. S. Albahri and Mohammed S. Al-Samarraay and O. S. Albahri and A. H. Alamoodi and Rula A. Hamid and Raad Z. Homod and Saleh Mahdi Mohammed and Iman Mohamad Sharaf},
  doi          = {10.1142/S0219622025500282},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {2005-2041},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Evaluation and benchmarking trauma patients in intensive care units using a novel decision-making framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault detection and predictive maintenance of photovoltaic facilities. <em>IJITDM</em>, <em>24</em>(7), 1971-2003. (<a href='https://doi.org/10.1142/S0219622025500531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an automatic decision support system for photovoltaic fault detection and predictive maintenance without the use of meteorological variables, which is a remarkable feature due to the lack of weather forecasts and wide variety of cases. This system improves another previous methodology to fault detection and introduces for the first time a module to predictive maintenance with good results. This module is based on fuzzy sets and the modeling of the behavior of the facilities through the computation and selection of a set of decision rules. The new decision support system has been validated and tested in the facilities of the company Grupo Energético de Puerto Real S.A.},
  archive      = {J_IJITDM},
  author       = {Roberto G. Aragón and Fernando Chacón-Gómez and M. Eugenia Cornejo and Jesús Medina and Eloísa Ramírez-Poussa},
  doi          = {10.1142/S0219622025500531},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {1971-2003},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Fault detection and predictive maintenance of photovoltaic facilities},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On COVID-19 epidemic prevention policy based on matrix game and sentiment analysis. <em>IJITDM</em>, <em>24</em>(7), 1941-1969. (<a href='https://doi.org/10.1142/S0219622025500270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively combating the 2019 coronavirus disease (COVID-19), pandemic relies not only on robust governmental policies but also on the active cooperation of the populace. In order to formulate rational and effective preventive policies, this study proposes an interval 2-tuple linguistic matrix game method and defines an interval 2-tuple linguistic scoring function. Building upon this framework, the study employs sentiment analysis based on the BosonNLP sentiment dictionary to extract and calculate textual scores from user comments on COVID-19 pandemic-related topics in Weibo. These two approaches are then integrated and applied to policy decision-making for the COVID-19 situation in the Shanghai region. Finally, the matrix game method is used to find the optimal prevention and control strategy in Shanghai during the epidemic period. The first is “Implement citywide preventive measures”, the second is “Implement traffic control combined with regional control policy”, and the last is “Implement the home quarantine policy”.},
  archive      = {J_IJITDM},
  author       = {Huanyu Wan and Dong Qiu},
  doi          = {10.1142/S0219622025500270},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {1941-1969},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {On COVID-19 epidemic prevention policy based on matrix game and sentiment analysis},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated energy system evaluation based on singular value decomposition and group decision making with incomplete information. <em>IJITDM</em>, <em>24</em>(7), 1911-1939. (<a href='https://doi.org/10.1142/S0219622025500269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformation of energy development is the inevitable choice to achieve the target of carbon neutrality, and the integrated energy system (IES) is conducive to improving the energy efficiency of users and optimizing energy structure, which is an important direction of energy system transformation in the future. Evaluation of IESs is a major part of the implementation of IES projects, while the current evaluations are based on complete information and single decision making. Given this background, this paper studies the IES evaluation with incomplete information and focuses on the group decision. First, the singular value decomposition algorithm is adopted to predict the missing data. Then, when calculating the index weights, a new method based on standard deviation modified unique reference comparison judgment method is proposed, which not only integrates subjective and objective decision information, but also avoids the problem that the combination coefficients of subjective and objective weights cannot be reasonably allocated. Next, on the basis of the comprehensive weights of indexes and normalized evaluation matrix, the weighted evaluation matrix for each expert is constructed, and the expert weights are calculated by genetic algorithm. Finally, the ranking of IES schemes can be realized by integrating expert weights and expert scoring values. Based on the data of IES at a hospital in Henan, the performance of the proposed method is verified experimentally. Experiments results show that the proposed method is reasonable and effective.},
  archive      = {J_IJITDM},
  author       = {Huan Zhang and Ya-Jun Leng and Libo Zhang and Zong-Yu Wu},
  doi          = {10.1142/S0219622025500269},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {1911-1939},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Integrated energy system evaluation based on singular value decomposition and group decision making with incomplete information},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(7), 1905-1910. (<a href='https://doi.org/10.1142/S0219622025030075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030075},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {7},
  pages        = {1905-1910},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of exogenous neuro-structure for dynamical analysis of nonlinear fractional financial crime systems. <em>IJITDM</em>, <em>24</em>(6), 1849-1903. (<a href='https://doi.org/10.1142/S0219622025500257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contributions of AI-based applications in monitoring real-time financial transactions, and detecting fraudulent activity by scrutinizing consumer behavior, transaction patterns, and other relevant measures are worth mentioning for potential threats identification in the fractional financial crime population dynamics. Leveraging these financial crime systems in terms of population dynamics with the exploitation of supervised Nonlinear Autoregressive Exogenous Networks Optimized with the Bayesian Regularization (NARX-BR) procedures for attaining sufficient accuracy and flexibility for the approximate solutions of a fractional variant of stiff Nonlinear Financial Crime Population Dynamics (NFCPDs) differential system. The population dynamics for the financial crime model are classified mainly into susceptible persons, financial criminals, individuals being prosecuted individuals under prosecution, imprisoned persons, and honest individuals by law. The acquisition of synthetic data generated with Grünwald–Letnikov (GL) fractional operator for the multi-layer structure execution of NARX-BR procedure for solving NFCPDs for varying financial crime parameters, such as influence rate, recruitment rate, conversion rate to honest people, freedom rate, financial criminal prosecution rate per capita, percentage of discharge rate from prosecution, transition rate to prison, discharge and acquittal rate from prosecutions. The estimated outcomes of NARX-BR and the calculated numerical solutions of NFCPDs consistently overlap implying that the error between the results is approximately equal to zero. The effectiveness of model performance is assessed through a variety of evaluation metrics, that include minimization of mean square error-based objective function, adaptive regulating parameters of the optimization algorithm, error distribution plots, regression studies, error endogeneity, and cross-correlation analyses. This study contributes to integrating fractional calculus with the knacks of innovative AI and open paths to provide data-driven efficient solution-based policy recommendations in the field of financial crime population dynamics.},
  archive      = {J_IJITDM},
  author       = {Farwah Ali Syed and Kwo-Ting Fang and Adiqa Kausar Kiani and Muhammad Shoaib and Muhammad Asif Zahoor Raja},
  doi          = {10.1142/S0219622025500257},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1849-1903},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Design of exogenous neuro-structure for dynamical analysis of nonlinear fractional financial crime systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consumer decision recognition based on EEG signals for neuromarketing applications. <em>IJITDM</em>, <em>24</em>(6), 1825-1847. (<a href='https://doi.org/10.1142/S0219622025500245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromarketing is a blooming interdisciplinary field that tries to understand the biology of consumer behavior by combining neuroscience with marketing. This technique can be used to grasp consumers’ hidden choices, intentions and decisions by analyzing their physiological and brain signals. Electroencephalography (EEG) is one of the popular neuroimaging techniques to capture and record the neural activity of the brain. Numerous research projections have been made in this field to achieve better results. Earlier approaches did not prioritize effective EEG signal preprocessing and classification methods. This paper presents a model to recognize consumer preferences by analyzing and classifying EEG signals. In this model, EEG signals are decomposed into many subbands using wavelet transform. An enhanced wavelet thresholding method is proposed to eliminate noise from subbands. Several wavelet features are computed from each subband and then fed as input to classifiers. Finally, three different machine learning classifiers are used to classify the signal between like and dislike. The classifiers are K-Nearest Neighbor (KNN), Multilayer Perceptron (MLP) and Support Vector Machine (SVM). EEG signals from 25 people are collected to verify the developed system’s performance. The effectiveness of the developed method with different classifiers is validated by varying brain lobe features and band features. In comparison to other classifiers like KNN and MLP, the designed system with the SVM classifier performs better and achieves an accuracy of 98.21%. The experimental findings for the developed system suggest that research in this area has the potential to alter and enhance marketing tactics for the benefit of both manufacturers and consumers, ultimately leading to a mutually beneficial outcome.},
  archive      = {J_IJITDM},
  author       = {S. Kumar Chandar and J. Vijayadurai and M. Palanivel Rajan},
  doi          = {10.1142/S0219622025500245},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1825-1847},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Consumer decision recognition based on EEG signals for neuromarketing applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the security of E-commerce systems against various types of attacks using deep learning model. <em>IJITDM</em>, <em>24</em>(6), 1801-1824. (<a href='https://doi.org/10.1142/S0219622025500233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, hackers can exploit unprotected e-commerce networks to attack an e-commerce network. As Malware becomes increasingly prevalent, diverse and sophisticated, recent studies highlight the effectiveness of deep Convolutional Neural Networks (CNNs) in detecting malware through attack classification. The background of this is complicated by factors such as data immutability, buyer, fraud and social network attacks. To overcome these issues, our proposed Convolution-based Buffalo Optimization (CbBO) is developed. To increase the performance, traditional buffalo optimization can be combined in a hybrid way with neural networks. The process of buffalo optimization entails identifying the best options depending on herd behavior. It may be necessary to alter how the optimization procedure searches the hyperparameter space to adapt this method to CNNs. Create a fitness feature that assesses CNN’s performance, especially regarding malware detection. Moreover, this research acts as the security classifier for e-commerce systems to identify attack variants and enhance security attack detection. Additionally, a technical model is developed to depict the dynamism of the model’s constituent parts. In the context of e-commerce security, our study will present simulation results to assess the effectiveness of the proposed paradigm. The CbBO model has been implemented using the Python platform and experimental validation demonstrates its capability to achieve 96% accuracy in effectively identifying DDoS attacks.},
  archive      = {J_IJITDM},
  author       = {Pooja Yadav and Ajit Kumar Keshri},
  doi          = {10.1142/S0219622025500233},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1801-1824},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Enhancing the security of E-commerce systems against various types of attacks using deep learning model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRASIAS: A new preference ranking model for comparing organizational performance under disruption. <em>IJITDM</em>, <em>24</em>(6), 1771-1800. (<a href='https://doi.org/10.1142/S0219622025500208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to investigate the impact of COVID-19 on the organizational performance of automotive, fast-moving consumer goods, consumer durables, and healthcare during both pre-and post-pandemic periods, forming a dual objective. To achieve this, the paper uses a set of market price-based ratios including Price-to-Earnings (P/E), Price-to-Book value (P/B), Price-to-Sales (P/S), and Price-to-Cash Flow (P/CF), along with market risk, as indicators for performance comparison. The scope of the assessment is limited to the financial performance analysis. The paper also attempts to relate performance to long-term growth prospects measured by Tobin’s Q model. Second, the paper proposes a Multi-criteria Decision-Making (MCDM) model called Preference Ranking based on Similarity to Ideal Average Solutions (PRASIAS) to compare the organizations. PRASIAS is a novel extension of the recently introduced Preference Ranking on the Basis of Ideal-Average Distance (PROBID) method. PRASIAS uses similarity measures in terms of inclination angle and considers positive and negative ideal solutions, as well as the average solution, to provide a comprehensive decision-making framework. To determine criteria weights, the paper applies a recently developed model called Logarithmic Percentage Change-driven Objective Weighting (LOPCOW). The paper validates the results by comparing them with the outcomes of other MCDM methods, and it also performs sensitivity analysis to examine the stability of the ranking. From the results, it is evident that there are some variations in the comparative positions over the years. Cipla Ltd., Dr. Reddy’s Laboratories Ltd., Bajaj Auto Ltd., and Britannia Industries Ltd. secured the top positions. P/E ratio remains a dominant influencing factor, as indicated by the calculated weights. A positive association is observed between financial stability and the comparative ranking of the organizations, while Tobin’s Q has not proven to be a significant influencer. It is also noteworthy that COVID-19 has impacted the performance of these entities.},
  archive      = {J_IJITDM},
  author       = {Sanjib Biswas and Prasenjit Chatterjee and Edmundas Kazimieras Zavadskas},
  doi          = {10.1142/S0219622025500208},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1771-1800},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {PRASIAS: A new preference ranking model for comparing organizational performance under disruption},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Honey badger optimized meta-material-based 4-port MIMO antenna with WiMAX, WLAN and X-bands for UWB application. <em>IJITDM</em>, <em>24</em>(6), 1741-1769. (<a href='https://doi.org/10.1142/S0219622025500178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multiple input–multiple output (MIMO) antenna is a necessary part of the field of wireless communication systems. Further, the meta-material plays a major part in reducing coupling between antennas. Hence, in this work, a meta-material-based 4-port MIMO antenna is designed for the UWB application using three band operations such as WiMAX (3.2–3.7 GHz), WLAN (4.1–6 GHz) and X-bands (8–12.1 GHz). In this research, the MIMO antenna is designed with complementary-split ring resonators (C-SRR) to improve the bandwidth of antenna elements with defected ground structure (DGS). Further, the metaheuristic algorithm honey badger optimization (HBO) is used for optimizing the antenna dimensions. The proposed design is simulated in the HFSS simulation platform on an FR-4 substrate with dimensions of 3 8 × 3 8 × 1 . 6 mm 3 . The gap between the elements of an antenna is 0 . 1 λ 0 . The radiation efficiency attained by the proposed structure is 93.7% for the overall operating range. Further, the MIMO system is a compact size and achieved a better envelope correlation coefficient ( ECC < 0 . 0 1 ) , total active reflection coefficient ( TARC < − 1 0 dB ) , diversity gain ( DG < 9 . 9 dB ) , channel capacity loss ( CCL < 0 . 4 ) and mean effective gain ( MEG < 3 dB ) at 3.7 GHz. Finally, it is concluded that the proposed meta-material-based MIMO antenna is the appropriate selection for the UWB applications.},
  archive      = {J_IJITDM},
  author       = {Prabhakar S Manage and Udaykumar Naik and Vijay Rayar},
  doi          = {10.1142/S0219622025500178},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1741-1769},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Honey badger optimized meta-material-based 4-port MIMO antenna with WiMAX, WLAN and X-bands for UWB application},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute decision-making: Scaling issues in pairwise comparison approaches based on the analytic hierarchy process. <em>IJITDM</em>, <em>24</em>(6), 1717-1740. (<a href='https://doi.org/10.1142/S021962202550018X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analytic hierarchy process (AHP) has been the subject of intensive discussions over the past decades. The multiplicative AHP (MAHP) was developed to deal with issues regarding the scale and aggregation procedure but has never achieved the same popularity. Both methods represent pairwise comparison processes (PCPs) based on a numerical scale with associated verbal definitions. However, they process the comparisons differently, leading to different results. This paper uses an empirical study to examine whether the original AHP’s results are generally more acceptable than MAHP’s. This leads to a review of principles relating to different scales based on comparative judgments, and finally, a revised scale parameter for MAHP is proposed. The participants in the empirical study are presented with the results of this alternative MAHP (A-MAHP). The results reveal that the A-MAHP — overcoming some theoretical issues discussed in this paper — should be considered for practical use. Thus, an innovative finding is that A-MAHP may serve as a proxy for AHP and accommodate users accustomed to this well-known method. Finally, perspectives are set out for future work.},
  archive      = {J_IJITDM},
  author       = {Steen Leleur and Michael Bruhn Barfod and George Panagakos},
  doi          = {10.1142/S021962202550018X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1717-1740},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Multi-attribute decision-making: Scaling issues in pairwise comparison approaches based on the analytic hierarchy process},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive analytics on time series data to generate a deterministic decision model: A case study on school reopening safely during the pandemic. <em>IJITDM</em>, <em>24</em>(6), 1685-1715. (<a href='https://doi.org/10.1142/S0219622025500191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on developing a new decision-making model to evaluate school reopening strategies during the COVID-19 pandemic. The model integrates deep learning and factor analysis to address the urgent need to restart educational services without worsening the health crisis. It starts by gathering time series data from various districts to apply deep learning for predicting virus dynamics, emphasizing feature extraction and hyperparameter optimization. The subsequent phase involves factor analysis to discover key factors influencing virus spread, using outputs from the deep learning step. Based on these factors, clustering methods then sort districts into controllable or vulnerable groups. The final stage combines these analyzes into a deterministic decision model aiding policymakers in crafting school reopening guidelines. The model identifies three primary controllable factors: infection growth rate, reduction in active cases, and lowered mortality rates. Clustering then reveals that three groups are controllable, enabling specific interventions. This model is noteworthy for considering causal links between pandemic metrics and its adaptability to diverse datasets across districts/subdistricts, offering a scalable solution for decision-makers. The results highlight the importance of local infection trends and tailored data in shaping policies, showing that strong predictive analytics and insight into significant factors are crucial for developing effective, safe school reopening plans.},
  archive      = {J_IJITDM},
  author       = {Feby Artwodini Muqtadiroh and Diana Purwitasari and Muhammad Reza Pahlawan and Riris Diana Rachmayanti and Tsuyoshi Usagawa and Eko Mulyanto Yuniarno and Supeno M. S. Nugroho and Mauridhi Hery Purnomo},
  doi          = {10.1142/S0219622025500191},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1685-1715},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Predictive analytics on time series data to generate a deterministic decision model: A case study on school reopening safely during the pandemic},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel parallel mechanism-based dynamic wearable assistive device for neck rehabilitation. <em>IJITDM</em>, <em>24</em>(6), 1653-1684. (<a href='https://doi.org/10.1142/S0219622025500166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper details a dynamic Wearable Assistive Device (WAD) for individuals suffering from chronic neck pain. Individuals should wear conventional cervical collars, which limit head mobility to a single configuration. A dynamic WAD is designed based on the human anatomical head/neck data and a dimensional synthesis study has been conducted using a loop-closure approach, based on the input parameters like dimensions of the top and base platforms and neck height have been taken into consideration for synthesis. Based on the dimensional synthesis, for the given geometrical parameters the proposed device achieved a maximum angular tilt of 1 9 ∘ for flexion, extension of 1 9 ∘ and lateral bending (right and left) of 3 8 ∘ . However, a major concern in WADs is related to evaluating the performance of the device by assessing the strain on the surface muscles. To address this problem, strain gauges are considered for strain measurement on the surface muscles because they are simple, readily available and low cost when compared to electromyography. Nevertheless, based on the existing survey surface neck muscle strain measurement using a strain gauge has yet to be studied for the controlled head/neck motions. Hence, a NI-9236 C Series Strain Quarter Bridge Input Module DAQ (Data Acquisition system) is used to examine the strain generated on neck surface muscles. Experimental studies were conducted for the test subjects with and without WAD. The various neck surface muscle strain values for head/neck motions were plotted using NI DAQ Express and the results are discussed in detail and simulation studies conducted for various human head/neck dimensions using MATLAB. The obtained strain results for the test subjects were found to be satisfactory as the strain produced on the neck surface muscles using WAD was reduced by approximately 25% than the strain generated without WAD. The findings suggest that by using the proposed WAD, controlled head/neck motions can be achieved and might be used for neck rehabilitation purposes.},
  archive      = {J_IJITDM},
  author       = {B. Jaishankar and Brajesh Kumar Singh and P. Ravi Kumar and Arockya Selvakumar Arockya Doss},
  doi          = {10.1142/S0219622025500166},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1653-1684},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A novel parallel mechanism-based dynamic wearable assistive device for neck rehabilitation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the effects of setting performance and learning goals in a transparent simulation of a dynamically complex task. <em>IJITDM</em>, <em>24</em>(6), 1631-1652. (<a href='https://doi.org/10.1142/S021962202550021X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation-based learning environments are increasingly viewed as promising tools to foster learning in complex domains. However, research has indicated that subjects may nevertheless have persistent cognitive difficulties in comprehending and managing dynamic systems. Previous studies have revealed positive learning effects of using transparent simulations (that is, revealing to users the structure and behavior of the simulator model). This study explores the effects of combining exploratory guidance, learning goals, and performance goals in a transparent simulation of a dynamically complex system. In a simulation experiment, participants interacted with a system dynamics model representing the growth of a business venture. Participants who had previously worked higher learning goals under exploratory guidance and were then given higher performance goals achieved higher performance and demonstrated better comprehension of the model dynamics. However, participants who were only subjected to more specific, high performance goals did not improve their outcomes and revealed larger differences within the treatment group.},
  archive      = {J_IJITDM},
  author       = {Carlos Capelo and Renato Pereira},
  doi          = {10.1142/S021962202550021X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1631-1652},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Exploring the effects of setting performance and learning goals in a transparent simulation of a dynamically complex task},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart tournament scheduling using a POX-heuristic genetic algorithm. <em>IJITDM</em>, <em>24</em>(6), 1613-1629. (<a href='https://doi.org/10.1142/S0219622025500221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an optimization algorithm that utilizes the precedence preserving order-based crossover (POX), heuristic techniques, and genetic algorithms (GAs) to address the task of arranging tournament scheduling for events of varying sizes. The study begins by conducting an extensive literature review on the subject of solving tournament scheduling problems. The GA approach is enhanced through the incorporation of POX for crossover operations and the inclusion of a heuristic algorithm for mutation. Subsequently, the POX-heuristic GA is developed and its performance is evaluated using data from 10 distinct tournaments. A comparison between the outcomes obtained from the proposed method and the results generated by the LeagueLobster software demonstrates that the proposed approach achieves a higher level of efficiency, with improvements ranging from 10.87% to 335.03% over the original actual schedule. The contributions stem from the successful integration of POX, heuristic techniques, and GAs to address the issue of poorly performing genes within chromosomes, ultimately leading to more effective optimization of the chromosomes.},
  archive      = {J_IJITDM},
  author       = {Mu-Chun Su and Jieh-Haur Chen and Achmad Muhyidin Arifai and Che-Hsuan Chang and Hsi-Hsien Wei},
  doi          = {10.1142/S0219622025500221},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1613-1629},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Smart tournament scheduling using a POX-heuristic genetic algorithm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(6), 1607-1612. (<a href='https://doi.org/10.1142/S0219622025030063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030063},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {6},
  pages        = {1607-1612},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to scenario assessment and selection. <em>IJITDM</em>, <em>24</em>(5), 1567-1605. (<a href='https://doi.org/10.1142/S0219622025500154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scenario planning is an effective and common tool that provides organizations with future insights and supports their strategic planning process by identifying alternative scenarios. Developing scenarios involves combining various future events, which require assessing and selecting a manageable number of distinct scenarios based on different criteria. Studies in the literature typically carry out this process relying on only a few criteria. Moreover, none of them considers multiple criteria simultaneously and they completely disregard the interaction among criteria. To address these issues, a novel approach that assesses scenarios through the simultaneous consideration of multiple criteria and their interactions is proposed. For the aggregation of the assessments, Choquet integral is used. Also, a new binary integer programming model that simultaneously accounts for multiple constraints is developed to select a manageable number of scenarios covering a wide range of distinct futures. The model allows complete flexibility in choosing the weights in the objective function. The approach is demonstrated for a manufacturing company to support its strategies for warehouse operations. A comparative analysis and the findings of the application indicate that the proposed approach effectively reduces possible loss of information and improves the comprehensiveness and accuracy of the scenario assessment and selection process.},
  archive      = {J_IJITDM},
  author       = {Ozgur Yanmaz and Umut Asan},
  doi          = {10.1142/S0219622025500154},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1567-1605},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A novel approach to scenario assessment and selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manipulation of individual judgments in the quantitative pairwise comparisons method. <em>IJITDM</em>, <em>24</em>(5), 1539-1566. (<a href='https://doi.org/10.1142/S0219622025500142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making methods very often use the technique of comparing alternatives in pairs. In this approach, experts are asked to compare different options, and then a quantitative ranking is created from the results obtained. It is commonly believed that experts (decision-makers) are honest in their judgments. In our work, we consider a scenario in which experts are vulnerable to bribery. For this purpose, we define a framework that allows us to determine the intended manipulation and present three algorithms for achieving the intended goal. Analyzing these algorithms may provide clues to help defend against such attacks.},
  archive      = {J_IJITDM},
  author       = {Michał Strada and Konrad Kułakowski},
  doi          = {10.1142/S0219622025500142},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1539-1566},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Manipulation of individual judgments in the quantitative pairwise comparisons method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The automated arbitrage strategy of cross-cryptocurrency exchanges. <em>IJITDM</em>, <em>24</em>(5), 1521-1537. (<a href='https://doi.org/10.1142/S0219622025500130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of decentralized finance (DeFi) allows arbitrageurs to obtain risk-free income from price gaps of cryptocurrency tokens in many global markets. Several automated arbitrage techniques have been invented to profit from single or multiple platforms, including Centralized and Decentralized Exchange (CEX and DEX), triangular, and DEX-Fait. This paper proposes the arbitrage strategy of cross-cryptocurrency exchanges (ASCEX), a novel automated arbitrage strategy for CEX-DEX platforms, to maximize profit and loss (PNL) using a token route searching algorithm. Based on feature comparison, ASCEX outperforms the existing trading strategies available. Our actual trade experiment shows that ASCEX can generate up to 0.95% monthly risk-free profit compared to 0.34% trading on DEX alone.},
  archive      = {J_IJITDM},
  author       = {Warodom Werapun and Naratorn Boonpeam and Esther Sangiamkul and Jakapan Suaboot},
  doi          = {10.1142/S0219622025500130},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1521-1537},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {The automated arbitrage strategy of cross-cryptocurrency exchanges},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abstractive text summarization for legal documents using optimization-based bi-LSTM with Encoder–Decoder model. <em>IJITDM</em>, <em>24</em>(5), 1493-1519. (<a href='https://doi.org/10.1142/S0219622025500129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the amount of textual data has grown quickly, creating a useful resource for information extraction and analysis. Due to the high complexity and unstructured nature of legal documents, automated text summarizing (ATS) is a necessary but difficult process. ATS is a technique that uses computer power to summarize lengthy paragraphs quickly. Summarizing the extensive materials manually is a highly difficult and time-consuming operation for people. As a result, an optimization-based deep learning (DL) model for abstract summarization is presented in the paper (AS). The proposed working procedure is broken down into three stages. They are text pre-processing, feature extraction, and abstractive summary categorization. The dataset is first placed through a pre-processing process, including stop word removal, tokenization, lemmatization, and stemming. The words and phrases are then represented in vector format throughout the feature extraction phase, which is handled by the NumberBatch (a combination of Enhanced GloVe Model (EGM), Enhanced FastText (EFT), and word2vector). To produce the abstractive text summary, the features obtained from the Numberbatch are fed into the DL model Bi-LSTM-based encoder–decoder with attention model. The metaheuristic optimization Honey Badger algorithm (HBA) is employed to optimize the network weights. This would increase the effectiveness of creating summaries based on ROUGE scores, and the proposed Bi-LSTM-HBA model performs better than currently used methods.},
  archive      = {J_IJITDM},
  author       = {Deepti Aggarwal and Arun Sharma},
  doi          = {10.1142/S0219622025500129},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1493-1519},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Abstractive text summarization for legal documents using optimization-based bi-LSTM with Encoder–Decoder model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary and target value-based normalization procedures for MCDM methods: An application on workers’ physical workload evaluation by using CRITIC-PSI-CoCoSo. <em>IJITDM</em>, <em>24</em>(5), 1459-1491. (<a href='https://doi.org/10.1142/S0219622025500117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to perform to find the most appropriate normalization procedure in MCDM methods. Normalization procedures proposed in the literature have tended to focus on cost and benefit type criteria. Some of the criteria discussed in MCDM problems may be required not to exceed a certain lower bound or upper bound, while others may be required to meet a certain target value. However, some criteria require target and boundary values-based normalizations, and these types of procedures receive much less attention despite their importance in many decision-making problems such as workers’ physical workload evaluation. Criteria affecting physical workload require target and boundary values-based normalizations because these normalization procedures best match the properties of human physiological capacities. In this context, this study proposes new normalization procedures based on target and boundary values. These new normalizations are applied in Criteria Importance through Inter-criteria Correlation (CRITIC), Preference Selection Index (PSI) to obtain criteria weights and Combined Compromise Solution (CoCoSo) method to prioritize workers for their physical workload levels. Additionally, a sensitivity analysis was performed to interpret the ranking differences for CRITIC-CoCoSo and PSI-CoCoSo integrations based on proposed normalization procedures.},
  archive      = {J_IJITDM},
  author       = {Pelin Toktaş and Gülin Feryal Can and Elif Kılıç Delice},
  doi          = {10.1142/S0219622025500117},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1459-1491},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Boundary and target value-based normalization procedures for MCDM methods: An application on workers’ physical workload evaluation by using CRITIC-PSI-CoCoSo},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing a grey wolf optimization-based gray box model for cash flow forecasting: A study on tehran stock exchange companies. <em>IJITDM</em>, <em>24</em>(5), 1435-1458. (<a href='https://doi.org/10.1142/S0219622025500087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cash flow forecasting is a critical aspect of financial planning and has long been of interest to investors and creditors. The Gray Box (GB) method is a widely used tool for forecasting, but a significant challenge in developing a grey box model is parameter estimation. In this study, we introduce a novel approach to cash flow forecasting called the Grey Wolf Optimization-based Grey Box model (GWOGB). The GWOGB employs a GWO algorithm as a global search method to determine the parameters of the GB model. To enhance the accuracy of future cash flow forecasting, we incorporate firm-level control variables as well as market and financial control variables into the classical model. To evaluate the performance of the proposed model, we use a sample of 250 firms listed on the Tehran Stock Exchange. Our empirical findings indicate that the GWOGB outperforms the generalized method of moments (GMM). Additionally, we employ sensitivity analysis to discern the source of forecast error and find that the inclusion of the nonlinear effect of sales growth rate on working capital accruals and future cash flow significantly reduces forecast error. The results show that using a nonlinear form of the GWOGB model is a promising approach for modeling the complex relationships between cash flow and working capital accruals.},
  archive      = {J_IJITDM},
  author       = {Ahmad Ahmadi and Farzaneh Nassirzadeh and Esmaeil Hadavandi and Mohammad Chavosh Nejad and Arash Ghorbani},
  doi          = {10.1142/S0219622025500087},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1435-1458},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Developing a grey wolf optimization-based gray box model for cash flow forecasting: A study on tehran stock exchange companies},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent fusion of heuristically optimized 1DCNN with weighted optimized DNN for thyroid disorder prediction framework. <em>IJITDM</em>, <em>24</em>(5), 1397-1433. (<a href='https://doi.org/10.1142/S0219622025500105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early stage of thyroid disease prediction is useful to decrease the mortality and morbidity rate and also to increase the diagnosis efficiency of the patient-specific treatments. Existing thyroid prediction approaches suffer from several limitations because of unreliable human false-positive predictive outcomes. The deep learning-based diagnosis methodology provides higher prediction accuracy and earlier detection of thyroid disorders from the collected data set. However, the researchers face several challenges in the prediction of thyroid nodules from large dimensional datasets with higher prediction accuracy. Hence, this research aims to implement an efficient and Hybrid Deep Learning (HDL) for thyroid prediction to provide better treatment for thyroid disorder. Initially, the experimental data are obtained from standard datasets. The collected data are normalized using the data normalization technique. The normalized data are further utilized in the optimal feature selection, which is carried out using the Hybrid Artificial Gorilla Troops Sandpiper Optimization (HAGTSO) to get the optimally required features for thyroid prediction. The thyroid disorder can be identified using the HDL with One-Dimensional Convolutional Neural Network Model (1DCNN) and Deep Neural Network (DNN), where parameters in 1DCNN and weight in DNN get optimized using the developed HAGTSO. The experimental results demonstrate that higher performance is provided by the newly developed thyroid predictive model when compared to other comparative algorithms while considering the negative and positive metrics. The numerical analysis of the offered model shows 97% and 96% in terms of accuracy and specificity measures. Here, the designed model proved that it shows better performance than the existing methods.},
  archive      = {J_IJITDM},
  author       = {K. Hema Priya and K. Valarmathi},
  doi          = {10.1142/S0219622025500105},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1397-1433},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Intelligent fusion of heuristically optimized 1DCNN with weighted optimized DNN for thyroid disorder prediction framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminant decision making of cardiovascular diseases based on cloud-based convolutional attention network. <em>IJITDM</em>, <em>24</em>(5), 1361-1396. (<a href='https://doi.org/10.1142/S0219622024500032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) have become the number one killer affecting human health. In order to reduce the burden of medical workers, facilitate government screening of the population and enable patients to conduct their own health status checks, there is an urgent need for a complementary diagnostic system to predict the occurrence of CVD. In this study, a new cloud-based convolutional attention network (C-CAN) model is proposed for the discriminant decision making of CVD. In this model, the indicator data for discriminant decision making of CVD are trained using an improved one-dimensional convolutional neural network (1D CNN) model structure based on the correlation of factors influencing CVD given by decision-making trial and evaluation laboratory (DEMATEL) and cloud models. This 1D CNN model consists of a convolutional pooling module, an attention module and a fully connected module. The cloud model is used to process the original data based on the discriminating opinion of experts, so as to select the important factors that affect CVD. The attention mechanism is effective in augmenting attention to the essential elements of the data and reducing attention to the less important features. Both have similarities in that they are effective in augmenting the important features in the data and combine with each other to achieve better results. Moreover, the C-CAN is compared with decision tree (DT), K -nearest neighbors (KNN), random forests (RF) and normal CNN according to the CVD dataset from the Kaggle platform. The results show that the classification accuracy, precision, recall and F1 value of C-CAN are all higher than that of all compared models. Further, the proposed model is further externally validated using other imbalanced datasets, and the results indicate that C-CAN has good resilience for imbalanced data. Our findings suggest that C-CAN represents a promising new approach that may somehow address the challenges associated with deep learning (DL) in the medical field.},
  archive      = {J_IJITDM},
  author       = {Wei Liu and Congjun Rao},
  doi          = {10.1142/S0219622024500032},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1361-1396},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Discriminant decision making of cardiovascular diseases based on cloud-based convolutional attention network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability evaluation of binary group decision-making mechanism. <em>IJITDM</em>, <em>24</em>(5), 1329-1360. (<a href='https://doi.org/10.1142/S021962202450007X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is an important management activity. This study evaluates the reliability of group decision-making (GDM) and multi-attribute GDM (MAGDM) mechanisms for a class of 0–1 binary decision-making problem. We define the reliability of GDM and MAGDM, use the weighted voting system to model the GDM and MAGDM mechanisms, and propose two algorithms to evaluate the reliability of GDM and MAGDM considering the participation of general or professional decision makers. Additionally, the influence of some system parameters, such as the number of decision makers or attributes, cognitive accuracy of decision makers, and threshold of weighted majority voting rule, on the reliability of GDM and MAGDM was analyzed using random simulation experiments. The results of the random experiment show that: increasing the number of decision makers or attributes could improve the decision accuracy; the reduction in the individual subjective accuracy reduces the overall decision accuracy, which was difficult to compensate for by increasing the number of DMs; guiding DMs to reach consensus through group discussion decreased the decision accuracy of GDM and MAGDM.},
  archive      = {J_IJITDM},
  author       = {Qiang Liu and Xinyu Peng and Qingmiao Liu and Qiao Li},
  doi          = {10.1142/S021962202450007X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1329-1360},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Reliability evaluation of binary group decision-making mechanism},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the critical success factors of marketing automation through cross-case analysis and DEMATEL. <em>IJITDM</em>, <em>24</em>(5), 1283-1327. (<a href='https://doi.org/10.1142/S0219622024500081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the critical success factors (CSFs) of marketing automation (MA) for business-to-business (B2B) IT companies. The research employs a distinctive approach combining qualitative and quantitative methods to study the cause–effect phenomena and ascertain the rank of the CSFs to address the lack of comprehensive research in the existing literature. Utilizing the technology acceptance model (TAM) and DeLone & McLean’s information system success model (D&M ISSM) as a theoretical framework, the study underscores the underexplored domain of MA in the B2B IT sector. Expert interviews and the decision-making trial and evaluation laboratory method (DEMATEL) are employed to understand and rank CSFs comprehensively. Methodological triangulation is applied to the findings of expert interviews and DEMATEL analysis to confirm the CSFs of MA. The study finds that “system integration,” “flexibility and adaptation,” “personalized information,” and “general satisfaction” are the highest-ranked CSFs for adopting MA among B2B IT firms. This study provides valuable insights for managers in B2B IT companies on the CSFs driving MA adoption and effectiveness, enabling them to make informed decisions and optimize their MA strategies.},
  archive      = {J_IJITDM},
  author       = {Akshay Mathur and Saumya Singh},
  doi          = {10.1142/S0219622024500081},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1283-1327},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Evaluating the critical success factors of marketing automation through cross-case analysis and DEMATEL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(5), 1277-1281. (<a href='https://doi.org/10.1142/S0219622025030051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030051},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {5},
  pages        = {1277-1281},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-criteria strategic evaluation model to determine the suitability of newly rising engineering departments in turkish universities based on the data from the year 2009 to 2020 using the econophysics perspective. <em>IJITDM</em>, <em>24</em>(4), 1247-1276. (<a href='https://doi.org/10.1142/S0219622024500044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent decade, engineering and technology have been developed rapidly, and new requirements are rising for engineering education. So, new and more focused departments are rising in the engineering faculty. The Turkish economy has been developed, and it is necessary to develop new technologies in industry based on the new investments. The scientific models are required to decide which engineering departments are necessary based on the socio-economic development of the economy. For this aim, we present a strategic analysis of which department can be established to meet the requirements in Turkey in light of the latest developments in the world. In the analysis stage, we listed engineering departments in the world universities and compared them with Turkish universities. We determined the selection criteria for the alternative departments, following these analyses and their related data collected from the Turkish Statistical Institute’s website. We used the new impulse and momentum principle-based weight assignment procedure integrated Technique for Order Preferences by Similarity to the Ideal Solution (IMP-TOPSIS) method to rank alternative departments using different scenarios. We concluded that Artificial Intelligence Engineering is the most suitable alternative. In addition, Aerospace Engineering has the second importance, and Materials Science and Nanotechnology Engineering have the third importance, according to the obtained results.},
  archive      = {J_IJITDM},
  author       = {Yusuf Tansel İç},
  doi          = {10.1142/S0219622024500044},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1247-1276},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A multi-criteria strategic evaluation model to determine the suitability of newly rising engineering departments in turkish universities based on the data from the year 2009 to 2020 using the econophysics perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hedging salmon price risk based on fuzzy copula-GMM model. <em>IJITDM</em>, <em>24</em>(4), 1221-1246. (<a href='https://doi.org/10.1142/S0219622023500682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copula method can explain the dependent function or connection function which connects the joint distribution and the univariate marginal distribution. Therefore, copula has recently become a most significant important tool in the financial field of risk management, portfolio allocation, and derivative asset pricing. However, it leads to a possibilistic uncertainty in estimating the parameters of copulas because of insufficient historical data, imprecise parameter estimation, and uncertain knowledge of future prices. This paper proposes a fuzzy copula model via Kullback–Leibler (KL) divergence to model the fuzzy relations, and further to investigate the hedging issues of salmon futures. We use a new framework of hedging under fuzzy circumstances, consisting of innovative marginal distributions and fuzzy intervals. By synergizing fuzzy copula and simulations, we use the fuzzy copula-GMM to obtain the hedge ratios of salmon futures. The empirical results show that, compared with traditional probabilistic methods, the fuzzy copula-GMM hedges the salmon spot risk measured by variance more successfully.},
  archive      = {J_IJITDM},
  author       = {Xing Yu and Chenya Liu and Weiguo Zhang},
  doi          = {10.1142/S0219622023500682},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1221-1246},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Hedging salmon price risk based on fuzzy copula-GMM model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling photovoltaic facilities via fuzzy sets and linguistic petri nets. <em>IJITDM</em>, <em>24</em>(4), 1191-1219. (<a href='https://doi.org/10.1142/S0219622024500068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a formal procedure based on linguistic Petri nets and fuzzy sets in order to model photovoltaic facilities and provide different alerts with respect to a correct performance. Furthermore, this procedure has been illustrated in a real case. Specifically, it is tested on six solar photovoltaic facilities (stations) owned by the company Grupo Energético de Puerto Real (GEN). The energy generated by each station has been compared through Dynamic Time Warping (DTW) and analyzed by a designed Petri net, which considers fuzzy sets and Natural Language for a better interpretation and understanding for the (nonexpert) user. The results show that the proposed procedure offers excellent results, bigger profits for the company and that it can be scalable and exportable to other facilities.},
  archive      = {J_IJITDM},
  author       = {Jesús Medina and Juan Moreno-García},
  doi          = {10.1142/S0219622024500068},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1191-1219},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Modeling photovoltaic facilities via fuzzy sets and linguistic petri nets},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A personalized individual semantic extraction model based on criterion for adaptive consensus reaching process under improved basic uncertain linguistic environment. <em>IJITDM</em>, <em>24</em>(4), 1155-1190. (<a href='https://doi.org/10.1142/S0219622023500591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized individual semantics (PIS) is an important factor reflecting the personal habits of decision makers (DMs) and has been widely studied by scholars. Using criteria as a non-negligible information source in multi-criteria group decision making (MCGDM), how to extract PIS from it is a research gap to be solved. In addition, existing measurements of consensus are insufficiently sensitive to differences between individuals, while the current direction rules use a matrix as the unit of measurement, which is not detailed and precise enough. Therefore, this paper first constructs a PIS extraction model according to the principle that similar criteria have similar descriptions and mutually exclusive criteria have dissimilar descriptions. Secondly, the preference information of PIS is mingled with uncertainty and reliability of improved basic uncertain linguistic information (IBULI) as the data of the consensus reaching algorithm. The proposed consensus algorithm not only fully considers the dispersion of DMs in the consensus measurement stage, but also improves the objectivity of the consensus process through an adaptive feedback stage. Finally, the validity of the proposed model is verified by an example and comparative analysis of the selection of sustainable building materials.},
  archive      = {J_IJITDM},
  author       = {Mengmeng Zhu and Junjun Mao and Wei Xu},
  doi          = {10.1142/S0219622023500591},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1155-1190},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A personalized individual semantic extraction model based on criterion for adaptive consensus reaching process under improved basic uncertain linguistic environment},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance assessment of business process optimization algorithms using a prototype dataset generator. <em>IJITDM</em>, <em>24</em>(4), 1125-1154. (<a href='https://doi.org/10.1142/S0219622024500056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern approaches of business administration consider processes as the fundamental element for the structure of a company that pursues high competitiveness and profitability. Hence, the implementation of effective processes has become a major concern, in an attempt to develop methodologies for their continuous improvement with respect to predefined evaluation criteria. Considering that an optimization method needs to be evaluated systematically in terms of performance and accuracy, the aim of this paper is the development of a generator that can construct synthetic test problems of diverse size and difficulty, employing well-defined complexity parameters derived from the Graph Theory and where a business process design is represented as a two-terminal-directed acyclic graph. However, a plethora of different formulations regarding the process optimization problem that have been presented in the relevant literature motivated our research to consider the target model of the generator as an ideal case, which can be derived by the aforementioned formulations after reducing some of their constraints. The experimental evaluation demonstrates how different value settings of the complexity parameters affect the difficulty of the generated instances, proving the capability of the proposed generator to create test sets of varying complexity.},
  archive      = {J_IJITDM},
  author       = {Athanasios Dragoslis and Ilias Sakellariou and Kostas Vergidis},
  doi          = {10.1142/S0219622024500056},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1125-1154},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Performance assessment of business process optimization algorithms using a prototype dataset generator},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Basic statistical methods in determining criteria weights. <em>IJITDM</em>, <em>24</em>(4), 1103-1124. (<a href='https://doi.org/10.1142/S0219622024500093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of technology has facilitated data accessibility, leading to an expansion in the range of criteria employed in decision problem design. This situation offers an advantage for making precise and rational decisions, but when it comes to managing spending, it becomes a disadvantage. Specifically, the expense of acquiring expert views utilized in the computation of criteria weights by subjective approaches experiences a substantial rise. Hence, decision-makers may employ objective methodologies to determine criterion weights. Nevertheless, objective methods provide a more limited range of choices compared to subjective methods. The study aims to utilize two widely recognized fundamental statistical approaches in order to enhance the capabilities of objective methods. One of the suggested approaches is the dissimilarity-based weighting method, which calculates the differentiation of values within the criteria. Another approach is the weighting method, which relies on the interquartile range. The methods were adapted as means of weighting criteria. Explanatory examples were provided, simulation-based comparisons were conducted, and ultimately applied to an actual data set. The data from each scenario were compared using the factorial analysis of variance method. The findings produced demonstrate that the proposed methods align with other objective methodologies. Furthermore, the proposed approaches were observed to take more time to finish the procedure compared to the Entropy and Standard Deviation methods, but less time compared to the Critic and Merec methods. Consequently, the suggested techniques are introduced as alternative approaches derived from established fundamental statistical procedures, which are straightforward to comprehend and valuable for professionals.},
  archive      = {J_IJITDM},
  author       = {Üzeyir Fidan},
  doi          = {10.1142/S0219622024500093},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1103-1124},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Basic statistical methods in determining criteria weights},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-CVaR portfolio optimization models based on chance theory. <em>IJITDM</em>, <em>24</em>(4), 1067-1101. (<a href='https://doi.org/10.1142/S021962202350058X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The indeterminacy of financial markets leads investors to face different types of security returns. Usually, security returns are assumed to be random variables when sufficient transaction data are available. If data are missing, they can be regarded as uncertain variables. However, uncertainty and randomness coexist. In this situation, chance theory is the main tool to deal with this complex phenomenon. This paper investigates the conditional value at risk (CVaR) of uncertain random variables and its application to portfolio selection. First, we define the CVaR of uncertain random variables and discuss some of its mathematical properties. Then, we propose an uncertain random simulation to approximate the CVaR. Next, we define the inverse function of the CVaR of uncertain random variables, as well as a computational procedure. As an application in finance, we establish uncertain random mean-CVaR portfolio selection models. We also perform a numerical example to illustrate the applicability of the proposed models. Finally, we numerically compare the mean-CVaR models with the mean-variance models with respect to the optimal investment strategy.},
  archive      = {J_IJITDM},
  author       = {Souad Chennaf and Jaleleddine Ben Amor},
  doi          = {10.1142/S021962202350058X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1067-1101},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Mean-CVaR portfolio optimization models based on chance theory},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated interval type-2 fuzzy PROMETHEE-II decision model for the selection of medical waste treatment techniques. <em>IJITDM</em>, <em>24</em>(4), 1035-1065. (<a href='https://doi.org/10.1142/S0219622023500554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of medical waste treatment techniques has become a serious health and safety issue since the amount and variety of medical waste are rapidly increasing during the Corona Virus Disease 2019 (COVID-19) pandemic in the whole world. This paper aims to propose an integrated interval type-2 fuzzy Preference Ranking Organization Method for Enrichment Evaluations-II (PROMETHEE-II) decision model for medical waste treatment techniques selection problem considering interactive relationships among criteria under a high uncertain environment. First, interval type-2 fuzzy sets (IT2FSs) are introduced to express imprecision information within the context of high uncertainty. Second, a type-2 Prioritized Aggregation (PA) operator is constructed to aggregate the evaluation information considering the priority of experts. Third, the Decision Making Trial and Evaluation Laboratory (DEMATEL) method is combined with the IT2FS to calculate the weights of criteria considering the interaction relationships among them. Then, an extended interval type-2 fuzzy PROMETHEE-II method is proposed to rank each alternative, in which a distance measure-based preference function is adopted. After that, a case study of medical waste treatment selection in Jiangsu province is used to illustrate the effectiveness of the proposed approach. The results demonstrate that incineration is the most appropriate approach in handling medical waste. Finally, sensitivity analysis and comparative analysis are implemented to further test the advantages of the proposed approach.},
  archive      = {J_IJITDM},
  author       = {Jing Tang and Xinwang Liu and Weizhong Wang},
  doi          = {10.1142/S0219622023500554},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {1035-1065},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {An integrated interval type-2 fuzzy PROMETHEE-II decision model for the selection of medical waste treatment techniques},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Factors affecting the adoption of cloud for software development: A case from turkey. <em>IJITDM</em>, <em>24</em>(4), 997-1033. (<a href='https://doi.org/10.1142/S0219622023500517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based solutions for software development activities have been emerging in the last decade. This study aims to develop a hybrid technology adoption model for cloud use in software development activities. It is based on Technology Acceptance Model (TAM), Technology–Organization–Environment (TOE) framework, and the proposed extension Personal–Organization–Project (POP) structure. The methodology selected is a questionnaire-based survey and data are collected through personally administered questionnaire sessions with developers and managers, resulting in 268 responses regarding 84 software development projects from 30 organizations in Turkey, selected by considering company and project sizes and geographical proximity to allow face-to-face response collection. Structural Equation Modeling (SEM) is used for statistical evaluation and hypothesis testing. The final model was reached upon modifications and it was found to explain the intention to adopt and use the cloud for software development meaningfully. To the best of our knowledge, this is the first study to identify and understand factors that affect the intention of developing software on the cloud. The developed hybrid model was validated to be used in further technology adoption studies. Upon modifying the conceptual model and discovering new relations, a novel model is proposed to draw the relationships between the identified factors and the actual use, intention to use and perceived suitability. Practical and social implications are drawn from the results to help organizations and individuals make decisions on cloud adoption for software development.},
  archive      = {J_IJITDM},
  author       = {Erhan Pisirir and Oumout Chouseinoglou and Cüneyt Sevgi and Erkan Uçar},
  doi          = {10.1142/S0219622023500517},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {997-1033},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Factors affecting the adoption of cloud for software development: A case from turkey},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An OWA analysis of the VSTOXX volatility index. <em>IJITDM</em>, <em>24</em>(4), 963-995. (<a href='https://doi.org/10.1142/S0219622025500099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyze the information value of the VSTOXX (volatility) index as a measure of risk for the European stock market. Based upon daily data from 2007 to 2023, the properties of the VSTOXX index are inspected and contrasted under various market conditions and in high- and low-volatility periods. Moreover, to evaluate the contribution of each country-specific index to the VSTOXX index, we employ the Ordered Weighted Averaging (OWA) operator as an analysis tool. We obtain a number of useful insights. Only for France and Germany the correlation between the country-specific volatility index and the VSTOXX index is high during the entire period. In addition, the VSTOXX index acts more like a maximum than as a minimum of volatility for the European stock markets and acts as an average only during periods of extreme volatility. Our findings offer important implications for both investors and policymakers.},
  archive      = {J_IJITDM},
  author       = {Luca Gambarelli and Silvia Muzzioli and Bernard De Baets},
  doi          = {10.1142/S0219622025500099},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {963-995},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {An OWA analysis of the VSTOXX volatility index},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(4), 957-961. (<a href='https://doi.org/10.1142/S021962202503004X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S021962202503004X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {4},
  pages        = {957-961},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A PAE model based on synthetic control method and policy instruments for the carbon emission reduction effect of green finance pilot zones. <em>IJITDM</em>, <em>24</em>(3), 929-956. (<a href='https://doi.org/10.1142/S0219622025410056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green finance directs social capital to support projects that benefit the environment and sustainable development. This paper constructs a policy analysis and evaluation (PAE) model based on a synthetic control method and policy instrument. Through the quantitative analysis of the actual effect of policy impacts and the analysis of policy instruments, the PAE model jointly analyses and evaluates the policy effects from both causes and consequences. This paper takes the first batch of green finance pilot zones in China as an example. The model results show that Guizhou has the best emission constraint effect. The carbon reduction effect in Jiangxi and Zhejiang is not significant. The growth of carbon emissions in Guangdong and Xinjiang accelerated. Furthermore, the PAE model analyses the mechanism of policy impacts. The difference in policy focus is the cause of the difference in carbon emission control effects. This result strengthens the reliability of the evaluation conclusions from the perspective of policy supply and provides valuable policy experience.},
  archive      = {J_IJITDM},
  author       = {Che Han and Aihua Li and Yuxue Chi and Bing Ma},
  doi          = {10.1142/S0219622025410056},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {929-956},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A PAE model based on synthetic control method and policy instruments for the carbon emission reduction effect of green finance pilot zones},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the opportunities to implement industry 4.0 through absorptive capacity: A multi mediator model evaluation in the manufacturing industry of mexico. <em>IJITDM</em>, <em>24</em>(3), 891-928. (<a href='https://doi.org/10.1142/S0219622025410044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge absorptive capacity (ACAP) has been recognized as a driving force for ICT-driven digital transformation; however, questions remain about how to maximize opportunities for implementing Industry 4.0 (I4.0) while considering the role of ACAP. Our primary objectives are to analyze how ACAP influences I4.0 opportunities (strategic, operational, and environmental/social) and to examine the mediating role of these opportunities in the connection between ACAP and I4.0 implementation. This empirical study, employing a predictive type, analyzed data from 200 manufacturing SMEs in Guanajuato, Mexico, using Partial Least Squares Structural Equation Modeling (PLS-SEM) via SmartPLS4, with a hierarchical component model Type I. The findings underscore that ACAP significantly enhances the potential for successful I4.0 implementation. Furthermore, the study highlights the critical importance of a strategic approach to leveraging I4.0 opportunities. Interestingly, the environmental and social opportunities showed limited influence on I4.0 adoption, with no evidence of their mediating role. This research contributes to illuminating the complex interplay between ACAP, I4.0 opportunities, and their collective contribution to the successful implementation of Industry 4.0 in Mexican SMEs. Finally, the study offers actionable insights for industrial practitioners and contributes to methodological advancements in PLS-SEM analysis.},
  archive      = {J_IJITDM},
  author       = {Héctor Cuevas-Vargas and Rudy Fernández-Escobedo},
  doi          = {10.1142/S0219622025410044},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {891-928},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Improving the opportunities to implement industry 4.0 through absorptive capacity: A multi mediator model evaluation in the manufacturing industry of mexico},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective route optimization for long-distance cold chain transportation considering time window. <em>IJITDM</em>, <em>24</em>(3), 865-889. (<a href='https://doi.org/10.1142/S0219622025410032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the long-distance cold chain transportation from production to distribution, besides the high transportation cost, the product spoilage and carbon emission from refrigerant and fuel usage during transportation also merit attention. At the same time, the time window for transportation indirectly affects customer satisfaction, so this paper shows how to develop a suitable transportation route with multiple optimization objectives mentioned above. First, the optimization objectives of long-distance cold chain transport are specified as transport cost, product spoilage and carbon emission, and the time window of delivery is taken into account to establish a multi-objective optimization model. Second, an improved ant colony algorithm is employed to resolve the Pareto optimal solution of the model. Finally, the effectiveness of the model is confirmed by a calculation example and compared with other methods.},
  archive      = {J_IJITDM},
  author       = {Congmiao Fang and Xiaoyan Gu and Shuangshuang Cheng and Dengsheng Wu},
  doi          = {10.1142/S0219622025410032},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {865-889},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Multi-objective route optimization for long-distance cold chain transportation considering time window},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting crude oil prices using a convolutional neural network with time-delay embedding. <em>IJITDM</em>, <em>24</em>(3), 843-863. (<a href='https://doi.org/10.1142/S0219622025410020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel hybrid forecasting model, TDE-CNN, to model the complex dynamics of crude oil price movements. The model integrates Time-Delay Embedding (TDE) Method with a Convolutional Neural Network (CNN) to leverage both spatial and temporal information. The TDE-CNN model uses the TDE method to transform raw crude oil data into higher-dimensional space to reveal underlying spatio-temporal patterns, while the CNN effectively models these patterns for improved predictive accuracy. The TDE-CNN model is applied to forecast major crude oil spot price movements, and its forecasting performance has been comprehensively and rigorously evaluated. Empirical results demonstrate that the TDE-CNN model achieves lower forecasting errors compared to benchmark models, as measured by Mean Squared Error (MSE). Additionally, the Diebold-Mariano test confirms that the improvement in forecasting accuracy is statistically significant.},
  archive      = {J_IJITDM},
  author       = {Kaijian He and Lean Yu and Jia Liu and Yingchao Zou},
  doi          = {10.1142/S0219622025410020},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {843-863},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Forecasting crude oil prices using a convolutional neural network with time-delay embedding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-based spillover network analysis of financial institutions in chinese mainland and hong kong. <em>IJITDM</em>, <em>24</em>(3), 817-842. (<a href='https://doi.org/10.1142/S0219622025410019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong connection among financial institutions provides more possible channels for information spillovers under different market conditions. Therefore, this paper attempts to comprehensively capture the spillover effects among financial institutions in Chinese mainland and Hong Kong by constructing the spillover networks based on the Quantile Vector AutoRegression (QVAR) model. By taking the stock prices as the representatives of financial institutions, we find that the spillover effects under extremely positive and negative conditions are larger than those under normal conditions. The magnitudes of spillovers are dynamic and show a dramatically upward trend when the market encounters extreme shocks, such as the Sino-US trade friction and the COVID-19 pandemic. The directional spillovers of the financial sector and its subsectors from Chinese mainland to Hong Kong are greater than those from Hong Kong to Chinese mainland, and banking contributes more to the spillovers. In addition, most institutions in Chinese mainland are spillover transmitters, and whether the institutions are net receivers or not varies over shock sizes. These results are essential for risk management in financial institutions in the opening financial markets.},
  archive      = {J_IJITDM},
  author       = {Yinhong Yao and Zhensong Chen and Wei Chen and Xueyong Liu},
  doi          = {10.1142/S0219622025410019},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {817-842},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Quantile-based spillover network analysis of financial institutions in chinese mainland and hong kong},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grain temperature prediction based on GRU deep fusion model. <em>IJITDM</em>, <em>24</em>(3), 797-815. (<a href='https://doi.org/10.1142/S0219622023410031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temperature is an essential quality index in storage. Prediction of temperature can help the grain storage industry to apply the appropriate operations such as ventilation or drying to improve the quality of grain and extend the suitable storage time. Traditional machine learning methods usually cannot accurately predict the temperature data of the grain considering the complexity of environmental factors and grain warehouse conditions. To make better use of the temporal data such as temperature/humidity information of grain itself and its environment, this paper proposes a gated recurrent unit (GRU)-based algorithm to predict the change of the data. The grain warehouse environmental data are collected by multi-functional sensors inside a grain depot, including temperature, humidity, wind speed, air pressure, etc. Some of these data features such as rain or snow days are sparse data features. Excessive sparse features can affect the training accuracy of the model. At the same time, due to sensor aging or extreme weather conditions, the data collected may not be accurate, and the data contain noise, which also has a significant impact on the training of the model. To improve the performance of the proposed GRU framework, multivariate linear regression is used for feature generation to optimize the volatility of weather data, strengthen and construct the characteristics of datasets, and wavelet filtering is used to denoise the corresponding features. This paper focuses on the data sparse and noise problem and applies the MLR and wavelet filtering to improve the GRU prediction framework for grain warehouse temporal data. According to our experiment, the temperature prediction results based on the GRU deep fusion model have better improvement in prediction accuracy and time than the existing neural network algorithms such as long–short-term memory (LSTM), GRU, and transformer.},
  archive      = {J_IJITDM},
  author       = {Bo Mao and Shancheng Tao and Bingchan Li},
  doi          = {10.1142/S0219622023410031},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {797-815},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Grain temperature prediction based on GRU deep fusion model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecological, social and governance impact on the company’s performance: Information technology sector insight. <em>IJITDM</em>, <em>24</em>(3), 765-795. (<a href='https://doi.org/10.1142/S0219622023410043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable topics have become increasingly important in recent years, as the world faces growing environmental and social challenges. Environmental, social, and governance (ESG) ratings are tools used to assess the sustainability practices of companies. This study focuses on the impact of environmental, social and governance components on the financial performance of IT companies. The panel data were collected for 43 IT companies operating in the time period from 2004 to 2020. Data include ESG ratings, their components and financial performance indicators of IT companies. The method of OLS regression, a model with fixed individual effects or a model with random individual effects, was used. It was found that the increase in the environmental and social scores has a significant impact on the financial performance of IT companies. This paper is an extended version of our work published in the ITQM 2022.},
  archive      = {J_IJITDM},
  author       = {Alexander M. Karminsky and Alexandra A. Egorova and Daria A. Chigireva},
  doi          = {10.1142/S0219622023410043},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {765-795},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Ecological, social and governance impact on the company’s performance: Information technology sector insight},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering high-risk bank risk factors based on risk matrix. <em>IJITDM</em>, <em>24</em>(3), 743-764. (<a href='https://doi.org/10.1142/S021962202341002X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bank risk management is a crucial issue in the stability of the financial system. How to select high-risk factors that make banks in trouble and how these factors affect bank risks have always been a core problem. Previous studies comprehensively identified bank risk factors from textual risk disclosures and used the disclosure frequency of risk factors to determine important factors to which banks should pay more attention. This paper creatively constructs the textual risk matrix with frequency and sentiment of risk factors to divide bank risk factors into the high-risk category, mid-risk category, and low-risk category. Then we explore the impact of different categories of risk factors on bank risk and the risk perception of investors. Based on 457,383 sentences of 2,735 Form 10-K reports of 240 American commercial banks from 2006 to 2020, 33 bank risk factors were identified. Three risk factors belong to in high-risk category, including loan loss risk, regulation risk, and interest rate risk. Three factors are classified in the mid-risk category and 27 risk factors are low-risk factors. The regression results show that compared with individual bankruptcy risk, risk factors have better prediction and interpretive ability on the systemic risk. The disclosure of bank risk factors will affect the investors’ risk perception, especially the worse risk situation of the high-risk factors will increase the risk perceived by investors.},
  archive      = {J_IJITDM},
  author       = {Lu Wei and Xiyuan Miao and Haozhe Jing and Guowen Li},
  doi          = {10.1142/S021962202341002X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {743-764},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Discovering high-risk bank risk factors based on risk matrix},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Operational risk measurement: An optimal timescale selection method based on information entropy. <em>IJITDM</em>, <em>24</em>(3), 713-741. (<a href='https://doi.org/10.1142/S0219622023410055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational risk capital is typically allocated on an annual timescale in extant research, while ignoring that the dependence between operational risk events may differ on different timescales, such as semi-annual and quarterly. This study examines whether operational risk dependence structures differ on multiple timescales and proposes an optimal timescale selection method based on information entropy theory to obtain more reasonable operational risk measurement results. Different dependence modeling methods, including linear correlation and copula functions, are employed to analyze the dependence structures on multiple timescales. The information changes between different timescales are calculated to select the optimal timescale. The empirical analysis is based on 2,024 operational risk events from 1994 to 2017 in the Chinese Operational Loss Database (COLD), which is one of the largest external operational risk datasets for the Chinese banking industry. The results reveal significant differences in operational risk dependence at different timescales which affect the operational risk measurement results. Crucially, the quarterly timescale should be considered for more reasonable measurements. The findings provide important insights for considering a more reasonable allocation of operational risk capital under multiple timescale dependence.},
  archive      = {J_IJITDM},
  author       = {Yanpeng Chang and Yinghui Wang and Jianping Li and Xiaoqian Zhu},
  doi          = {10.1142/S0219622023410055},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {713-741},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Operational risk measurement: An optimal timescale selection method based on information entropy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(3), 707-711. (<a href='https://doi.org/10.1142/S0219622025030038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030038},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {3},
  pages        = {707-711},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A health evaluation method of complex systems based on evidential reasoning rule considering perturbations. <em>IJITDM</em>, <em>24</em>(2), 679-706. (<a href='https://doi.org/10.1142/S0219622022500419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex system has been widely applied in aerospace, chemical industry, manufacturing, military equipment and other fields. To ensure its safe and reliable operation, it is necessary for technicians to grasp the health state of the system effectively. However, if there are perturbations in the observation data, the evaluation result may be inaccurate. Besides, the effectiveness of evaluation result can be affected by perturbation, which means that the threshold of perturbation intensity should be determined. To solve these problems, in this paper, a new health evaluation method of complex systems is proposed based on the evidential reasoning (ER) rule by considering perturbations. First, the structure of the evaluation model is constructed. Second, an ER rule-based health evaluation model is proposed, and the idea of perturbation analysis is introduced for the robustness analysis of the model. Third, a parameter optimization model is established, and the upper bound of perturbation intensity is estimated accurately. The threshold can measure the effectiveness of the evaluation result and the anti-perturbation ability of the system in engineering practice. Finally, a case study of the WD615 model diesel engine is conducted to validate the effectiveness of the proposed method.},
  archive      = {J_IJITDM},
  author       = {Shuaiwen Tang and Zhijie Zhou and You Cao and Pengyun Ning and Peng Zhang and Dao Zhao and Leiyu Chen},
  doi          = {10.1142/S0219622022500419},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {679-706},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A health evaluation method of complex systems based on evidential reasoning rule considering perturbations},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of financial systemic crisis on a causal and reliable perspective. <em>IJITDM</em>, <em>24</em>(2), 651-677. (<a href='https://doi.org/10.1142/S0219622023500475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial network models, to assess systemic risk, a general understanding and prediction of precisely how a single financial institution is associated with systemic risk from the network perspective remains lacking. This paper proposes a framework for predicting and assessing system crises through inferring the cause–effect relationships between financial institutions and system state, which is structured in three steps: the assessment stage for system state based on the mean-variance framework, the prediction stage based on a Bayesian network and the reliability stage based on the Markov process. By applying them to monthly returns of financial institutions, it implies the need to pay attention to insurance and Broker sectors while regulating the banking system on the Bayesian network theory. Moreover, we find that the measure contains predictive power both during tranquil periods and during financial crisis periods. The results can be applied to derive interventions in financial crisis management with regard to systemic risk prediction and system state reliability.},
  archive      = {J_IJITDM},
  author       = {Yafei Wang and Zhengming Zhou and Xiao Cao},
  doi          = {10.1142/S0219622023500475},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {651-677},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Assessment of financial systemic crisis on a causal and reliable perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAMOM: Multicriteria attribution model for online marketing. <em>IJITDM</em>, <em>24</em>(2), 627-649. (<a href='https://doi.org/10.1142/S021962202250081X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the problems facing online marketing in omni-channel environments is efficient budget allocation to each channel. The complexity of this problem is greater in omni-channel environments due to the greater number of channels and the need to analyze data sources containing user interaction information. To solve this problem, different attribution models have been proposed to assign the weight that each channel has in the acquisition of a product, also known as conversion. Each of these attribution models adopts a strategy to define the weighting of channels. The decision-making strategy is established using the company’s expert knowledge, which can contain different criteria depending on the department to which the expert belongs. The aim of this research is to present a new multicriteria attribution model for online marketing (MAMOM) based on Analytical Hierarchical Process, that resolves this type of problem. MAMOM is a meta-model that takes as input information related to channel features, user interactions and even decisions from other strategies. This information is integrated to enable the integration of interdepartmental strategies as well as to obtain a dynamic attribution model based on expert interviews. The expert assessment procedure is carried out subjectively and simply with a pairwise assessment of the criteria, finally MAMOM obtains a final formulation to calculate the investment to be made in each channel if the experts’ opinions are considered. Results show that first five channels selected by the MAMOM are nearly identical to those that would be obtained with the traditional models but with a different sequence caused by experts’ knowledge. This result shows the capacity of MAMOM to factor in expert opinion and experience for making the investment outcome more aligned with the tactical and strategic objectives of a given online marketing campaign.},
  archive      = {J_IJITDM},
  author       = {Miguel A. Patricio and Antonio Berlanga and David Palomero and José M. Molina},
  doi          = {10.1142/S021962202250081X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {627-649},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {MAMOM: Multicriteria attribution model for online marketing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPEEDSER: A possibilistic system for query disambiguation, expansion and translation. <em>IJITDM</em>, <em>24</em>(2), 583-625. (<a href='https://doi.org/10.1142/S0219622023500499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design, implement and assess in this paper a new architecture of a possibilistic mono- and cross-language information retrieval (IR/CLIR) system. The latter is useful to experiment query disambiguation, expansion and translation processes in both IR and CLIR frameworks. We take advantage of possibility theory to overcome the problems of query disambiguation and expansion in an uncertain and imprecise IR/CLIR context. We investigate the impact of combining possibilistic query disambiguation with expansion on IR/CLIR efficiency. A co-occurrence graph representation is exploited to quantify the similarity between query terms and their semantically close words (expansion task) or between query terms and their possible meanings (disambiguation task). We extend the possibilistic mono-language query disambiguation approach to a cross-language framework. We conduct a set of experiments using ROMANSEVAL data collection, the French–English parallel text corpus Europarl and the CLEF-2003 French–English IR/CLIR test collection. Results highlight some statistically significant improvements of our possibilistic approaches when compared to some state-of-the-art IR/CLIR works.},
  archive      = {J_IJITDM},
  author       = {Bilel Elayeb and Oussama Ben Khiroun},
  doi          = {10.1142/S0219622023500499},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {583-625},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {SPEEDSER: A possibilistic system for query disambiguation, expansion and translation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable supplier evaluation in an automotive company using fuzzy multi-criteria decision-making methods. <em>IJITDM</em>, <em>24</em>(2), 535-581. (<a href='https://doi.org/10.1142/S0219622022500833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable supplier management literature is mainly on sustainable supplier selection but sustainable supplier performance monitoring & evaluation studies are scarce. Furthermore, the studies do not differentiate the sustainability evaluation criteria between the supplier selection and monitoring & evaluation stages. To bridge this gap, this study aimed to monitor & evaluate sustainability performance of the suppliers of a focal manufacturing company in automotive sector with the TBL approach. Toward that end, we questioned and tried to identify first the sustainable supplier selection and monitoring & evaluation differentiating criteria, and then, how we can rank and rate the sustainable suppliers for developmental purposes in the monitoring & evaluation phase. 21 criteria were determined for sustainable supplier performance in monitoring & evaluation by a detailed literature review and taking the opinions of the focal company. Then, decision makers assessed sustainability dimensions and determined criteria related to these dimensions. The weights of dimensions and criteria were calculated by Interval-Valued Intuitionistic Fuzzy AHP method. Then, sustainability performances of selected suppliers from the focal company’s portfolio were ranked by using Fuzzy EDAS, Fuzzy CODAS and Fuzzy MOORA methods. The results show that sustainable supplier monitoring & evaluation criteria involve a mix of external criteria (rules and regulations) and internal criteria (suppliers’ values). This finding helps us understand how we can have a more relaxed criteria set involving basic external criteria while selecting suppliers to have access to a more innovative and diverse supplier space and then have more challenging internal and external criteria to monitor & evaluate those suppliers toward true sustainability.},
  archive      = {J_IJITDM},
  author       = {Tuğçe Şişman and Sinem Büyüksaatçi Kiriş and Dilek Yilmaz},
  doi          = {10.1142/S0219622022500833},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {535-581},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Sustainable supplier evaluation in an automotive company using fuzzy multi-criteria decision-making methods},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex pythagorean fuzzy decision-based approach for developing english translation of the scripture-based multiclustering algorithms. <em>IJITDM</em>, <em>24</em>(2), 473-533. (<a href='https://doi.org/10.1142/S0219622025500075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The English Translation of the Quran Tafsir (ETQT) is essential to understanding and interpreting Allah’s words. Clustering is a common text mining technique for eliciting meaningful knowledge from a text collection. It is commonly used when the selected datasets lack typical ground truths. To the best of our knowledge, no study has evaluated and benchmarked ETQTs to select the most comprehensive and appropriate one. The process of evaluating and benchmarking ETQTs falls under the multicriteria decision-making (MCDM) problem because of different issues, namely, multiple internal clustering validation criteria, data variation, and trade-offs between different criteria. The fuzzy decision by opinion score method (FDOSM) is one of the most recommended MCDM ranking methods in the literature to address the said issues. FDOSM has been extended under different fuzzy set (FS) environments to address issues of uncertainty and vagueness caused by expert feedback subjectivity. Although prior versions of FDOSM improved the uncertainty and vagueness issues, they remain open issues. Therefore, this paper extended FDOSM into the complex Pythagorean fuzzy decision by opinion score method (CPFDOSM) to evaluate and benchmark ETQTs. The proposed method consists of two main phases. The first phase formulates decision-matrix-based cluster algorithms and internal cluster validation criteria. The second phase (CPFDOSM development) prioritizes ETQTs and selects the optimum one. Data generation is performed on five different cluster algorithms and six internal cluster validation criteria using 16 ETQTs based on three decision-makers (DMs). Results show the following: (1) 6.25% of the individual decision-making results are identical among the three DMs, whereas 93.75% ( n = 1 5 / 1 6 ) are different when δ = 0 . 5 and δ = 2 . When δ = 0 . 5 . In addition, T7 has consistent ranks (Rank = 16) across all DMs, whereas T14 has consistent ranks (Rank = 1) across all DMs when δ = 1 . (2) The results of the group decision-making reveal T14 is the most comprehensive and appropriate ETQT across all δ values. Objective validation and comparison analysis show the proposed method is ranked systematically.},
  archive      = {J_IJITDM},
  author       = {Mohammed A. Ahmed and A. A. Zaidan and Sarah Qahtan and Hassan A. Alsattar and B. B. Zaidan and Nahia Mourad and Hanif Baharin and Gang Kou and Khaironi Yatim},
  doi          = {10.1142/S0219622025500075},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {473-533},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Complex pythagorean fuzzy decision-based approach for developing english translation of the scripture-based multiclustering algorithms},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance dependence multi-criteria decision-making: A preference model based on an improved choquet integral. <em>IJITDM</em>, <em>24</em>(2), 443-472. (<a href='https://doi.org/10.1142/S021962202550004X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-criteria decision-making, the Choquet integral can handle the interaction between criteria; however, the interaction between criteria is not only related to the criteria themselves but may also depend on the performance values of criteria. We present a further extension of the Choquet integral considering this fact. A classic example is used to illustrate the limitations of the Choquet integral, and a performance-dependent assumption is proposed that describes the interactions influenced by the performance of alternatives on criteria. Based on this assumption, we propose an alternative capacity and construct an improved Choquet integral to obtain alternative’s global utility. A preference model based on the improved Choquet integral with a wider range of applicability and more realistic decision results is proposed. Finally, a numerical example is provided to illustrate the feasibility and validity of our reference model.},
  archive      = {J_IJITDM},
  author       = {Yu Gao and Mei Cai and Jingmei Xiao and Guang Yang},
  doi          = {10.1142/S021962202550004X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {443-472},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Performance dependence multi-criteria decision-making: A preference model based on an improved choquet integral},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring corporate reputation factors through cluster and sentiment analysis of CEO letters. <em>IJITDM</em>, <em>24</em>(2), 419-442. (<a href='https://doi.org/10.1142/S0219622025500051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate reputation is one of the most valuable assets a company must maintain to remain in business, and it becomes even more critical during crises such as COVID-19, which pose a severe threat not only to employees, customers, and the general public but also to the company’s fundamental survival. The purpose of this study is to identify themes affecting corporate reputation and assess how leading companies responded to the COVID-19 crisis. This study targeted the top 100 (RQ score higher than 50) companies according to the 23rd Annual Reputation Quotient from Axios-Harries Poll 100 RQ report. Employing cluster and sentiment analysis, the study explores CEO letters to understand how they emphasize reputation factors and employ impression management tactics. Findings of our study illustrate nuanced reactions and attitudes of CEOs toward issues presented by COVID-19, shed light on the intricacies of reputation management during times of crisis.},
  archive      = {J_IJITDM},
  author       = {Jue Wang and Hak-Seon Kim},
  doi          = {10.1142/S0219622025500051},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {419-442},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Exploring corporate reputation factors through cluster and sentiment analysis of CEO letters},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontology-aware model-driven approach for service-oriented application development: A stepwise refinement manner. <em>IJITDM</em>, <em>24</em>(2), 395-418. (<a href='https://doi.org/10.1142/S0219622024500123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies are attracting increasing attention in software engineering research due to their ability to precisely model the semantic aspects of systems. Enriching software system models using ontology principles, especially in the process of developing interactive systems like Service-Oriented Architecture (SOA), can lead to the automated production of high-quality codes. Additionally, ontology-aware specification of the service from the abstract to the concrete level leads to early precise extraction of the service required by the users. This paper introduces a model-driven ontology-aware service development process to reduce the burden of code generation. This integrated approach utilizes a stepwise refinement methodology facilitated by a novel refinement algorithm to automate SOA development. The effectiveness of the approach is evaluated using three proposed parameters which examine the characteristics of the refined model in each refining step through some practical SOA case studies. Finally, we calculate the recall, precision, F-measure, accuracy, and also time analysis of discovered querying services for various scenarios in AWS and Netflix before and after applying ontology. The results show an average of 17% improvement in these metrics after applying ontology.},
  archive      = {J_IJITDM},
  author       = {Abdolghader Pourali and Maryam Nooraei Abadeh},
  doi          = {10.1142/S0219622024500123},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {395-418},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {An ontology-aware model-driven approach for service-oriented application development: A stepwise refinement manner},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to select algorithms for predictive maintenance: An economic decision model and real-world instantiation. <em>IJITDM</em>, <em>24</em>(2), 361-394. (<a href='https://doi.org/10.1142/S0219622024500147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive maintenance represents a promising application of Artificial Intelligence in the industrial context. The evaluation and selection of predictive maintenance algorithms primarily rely on statistical measures such as absolute and relative prediction errors. However, a purely statistical approach to algorithm selection may not necessarily lead to the optimal economic outcome, as the two types of prediction errors are negatively correlated, thus, cannot be jointly optimized, and are associated with different costs. As the current literature lacks corresponding guidance, we developed a decision model for industrial full-service providers, applying an economic perspective to selecting predictive maintenance algorithms. The decision model was instantiated and evaluated in a real-world setting with a European machinery company providing full-service solutions in the field of car wash systems. Building on sensor data from 4.9 million car wash cycles, the instantiation demonstrates the applicability and effectiveness of the decision model with fidelity to a real-world phenomenon. In sum, the decision model provides economic insights into the trade-off between the algorithms’ error types and enables users to focus on economic concerns in algorithm selection. Our work contributes to the prescriptive knowledge of algorithm selection and predictive maintenance in line with the consideration of different types of cost.},
  archive      = {J_IJITDM},
  author       = {Lukas Fabri and Björn Häckel and Robert Keller and Anna Maria Oberländer},
  doi          = {10.1142/S0219622024500147},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {361-394},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {How to select algorithms for predictive maintenance: An economic decision model and real-world instantiation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(2), 355-360. (<a href='https://doi.org/10.1142/S0219622025030026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030026},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {2},
  pages        = {355-360},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reject inference using discriminative dual stack sparse auto-encoders for consumer credit risk evaluation. <em>IJITDM</em>, <em>24</em>(1), 327-353. (<a href='https://doi.org/10.1142/S0219622025500038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk evaluation has gained substantial attention within financial institutions, serving as a pivotal tool to predict borrower repayment behavior and provide precise credit risk estimations. Traditional credit risk approaches discarded rejected applicants and were built only on accepted applicants, which posed sample selection bias issue. Previous reject inference methods solved the bias issue by incorporating information of rejected applicants. However, these methods assumed that the accepted and rejected samples had identical dimensions. In practical financial scenarios, financial institutions often encounter situations where the dimensions of accepted samples were larger than those of the rejected samples. Therefore, the additional features in accepted samples might not be fully utilized in the previous reject inference. In this study, we proposed a discriminative dual stack sparse auto-encoder (DD-SSAE) reject inference method that was suitable for the real scenarios. The proposed DD-SSAE has the following characteristics: (1) rejected samples were filtered based on our selection mechanism; (2) a stack sparse auto-encoder (SSAE), within a self-taught learning framework, was carried out to incorporate information of the selected rejected samples into the common features of accepted samples; and (3) a data fusion module, consisting of another SSAE network and a data fusion layer, was introduced to combine extra features with common features for accepted samples. The proposed method was verified on a Chinese consumer dataset and the findings illustrated its superiority over four conventional credit scoring models and five previous reject inference models.},
  archive      = {J_IJITDM},
  author       = {Gang Kou and Siqi Weng and Feng Shen and Fahd Saleh S. Alotaibi},
  doi          = {10.1142/S0219622025500038},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {327-353},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Reject inference using discriminative dual stack sparse auto-encoders for consumer credit risk evaluation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple risks and uncertain portfolio management. <em>IJITDM</em>, <em>24</em>(1), 297-325. (<a href='https://doi.org/10.1142/S0219622023500190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies comparative static effects under uncertainty when investors face a portfolio decision problem with both an endogenous risk and a background risk. Since the security market is complex, there exists situation where security return and background asset return are given by experts’ estimates when they cannot be reflected by historical data. Focusing on such a situation, an uncertain mean-chance model with background risk for optimal portfolio selection is developed, in which the use of chance of portfolio return failing to reach the threshold can help investors easily determine their tolerance toward risk and thus facilitate a decision making. Then we analyze the solution of the programming problem under different threshold return level, i.e., how different degrees of threshold return will affect allocation between risky asset and risk-free asset. Furthermore, we discuss the effects of changes in mean and standard deviation of risky asset and background asset on investment decisions when security return and background asset return follow normal uncertainty distributions. Finally, a real portfolio selection example is given as illustration.},
  archive      = {J_IJITDM},
  author       = {Guowei Jiang and Xiaoxia Huang and Tingting Yang},
  doi          = {10.1142/S0219622023500190},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {297-325},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Multiple risks and uncertain portfolio management},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Megale: A metadata-driven graph-based system for data lake exploration. <em>IJITDM</em>, <em>24</em>(1), 259-295. (<a href='https://doi.org/10.1142/S0219622024500135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data lakes are storage repositories that contain large amounts of data (big data) in its native format; encompassing structured, semi-structured or unstructured. Data lakes are open to a wide range of use cases, such as carrying out advanced analytics and extracting knowledge patterns. However, the sheer dumping of data into a data lake would only lead to a data swamp. To prevent such a situation, enterprises can adopt best practices, among which to manage data lake metadata. A growing body of research has focused on proposing metadata systems and models for data lakes with a special interest on model genericness. However, existing models fail to cover all aspects of a data lake, due to their static modeling approach. Besides, they do not fully cover essential features for an effective metadata management, namely governance, visibility and uniform treatment of data lake concepts. In this paper, we propose a dynamic modeling approach to meet these features, based on two main constructs: data lake concept and data lake relationship . We showcase our approach by Megale, a graph-based metadata system for NoSQL data lake exploration. We present a proof-of-concept implementation of Megale and we show its effectiveness and efficiency in exploring the data lake.},
  archive      = {J_IJITDM},
  author       = {Doulkifli Boukraa and Meriem Bouraoui and Chaima Grine and Racha Ouahab},
  doi          = {10.1142/S0219622024500135},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {259-295},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Megale: A metadata-driven graph-based system for data lake exploration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of trade credit on financing strategy in a dual capital-constrained supply chain. <em>IJITDM</em>, <em>24</em>(1), 223-257. (<a href='https://doi.org/10.1142/S0219622022500687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study analyzes the financing strategy of a two-echelon supply chain, consisting of a manufacturer and a retailer, both subject to capital constraints. Specifically, the bank provides loans to the manufacturer, who then grants trade credit to the retailer. Based on the three-party game analysis framework of the bank, manufacturer, and retailer, this paper constructs a supply chain financing model under the information symmetry and information asymmetry structures, respectively; measures the maximum financing ability of the manufacturer; and discusses the influence of trade credit, moral hazard, and information structure on the manufacturer’s and bank’s strategies. The results show that under the trade credit situation, it is critical for the bank to provide loan to manufacturer who does not have moral hazard. The maximum financing capacity of the manufacturer is affected by the rate of return on moral hazard and the intensity of trade credit default. The increase of trade credit default intensity and risk exposure will lead to the increase of the interest rate of bank loan, and in the case of information asymmetry, the bank will often ask for a higher interest rate to deal with the information disadvantage. The strategy for the bank to make the credit line is more complex, and the degree of information asymmetry plays a positive moderating effect on the influence of trade credit on the credit line. Our findings provide implications for participants who implement financing actions to improve their financial performance and control the moral hazard and default risk along a supply chain.},
  archive      = {J_IJITDM},
  author       = {Xiaofeng Xie and Yang Yang and Xingyang Lyu and Fengying Zhang and Xiuying Hu and Zongfang Zhou},
  doi          = {10.1142/S0219622022500687},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {223-257},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {The impact of trade credit on financing strategy in a dual capital-constrained supply chain},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dwarf mongoose chimp optimization enabled RMDL for sentiment categorization using cell phone data. <em>IJITDM</em>, <em>24</em>(1), 197-222. (<a href='https://doi.org/10.1142/S0219622025500026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is the process of looking through digital text to determine if the emotional tone of a text is positive, negative, or neutral. It helps companies improve their product, but a serious problem arises in classifying the polarity of certain texts with information, sentences or features to forecast their opinion. Therefore, sentiment classification should be done using new technology that classifies reviews as positive or negative so that users can make effective decisions. This research paper develops an effective model to classify sentiment using cell phone data. Initially, the Amazon phone document is passed to the BERT tokenization stage to split the acquired reviews. Then, the Aspect Term Extraction (ATE) is applied and the Term Frequency-Inverse Document Frequency (TF-IDF) is extracted as the first output. Afterward, Wordnet ontology features are extricated as the second output. Moreover, features like statistical, sarcasm linguistic, and N -gram features are extracted from BERT tokenization and considered as the third output. Finally, the sentiment is classified by subjecting the obtained three outputs to Random Multimodal Deep Learning (RMDL), which is tuned by Dwarf Mongoose Chimp Optimization (DMCO). DMCO is created by the combination of the Dwarf Mongoose Optimization (DMO) and the Chimp Optimization Algorithm (ChOA). The developed DMCO-RMDL approach attained high accuracy, True Positive Rate (TPR), True Negative Rate (TNR), precision, recall, and F 1-score values of 93%, 92.8%, 92.2%, 91.5%, 94.1%, and 94.8%, respectively.},
  archive      = {J_IJITDM},
  author       = {Minu P. Abraham and K. R. Udaya Kumar Reddy},
  doi          = {10.1142/S0219622025500026},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {197-222},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Dwarf mongoose chimp optimization enabled RMDL for sentiment categorization using cell phone data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The best evaluation sequence method and application based on quantum cognitive theory. <em>IJITDM</em>, <em>24</em>(1), 169-196. (<a href='https://doi.org/10.1142/S0219622025500014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the credit risk evaluation process, considering that the decision maker’s irrational behavior may cause the interference effect between evaluation information, and further decrease the reliability of evaluation results. To solve this problem, the best evaluation sequence method and its application in credit risk evaluation based on quantum cognitive theory is proposed in this paper. First, the quantum cognition theory and the evaluation information given by the decision maker are used to get the interference degree between evaluation information. Second, the interference degree between evaluation information is aggregated to obtain the comprehensive interference degree of each alternative. Third, according to the idea that the greater the comprehensive interference degree of the alternative, the more backward the evaluation sequence of the alternative is, we determine the best evaluation sequence of alternatives. Finally, our proposed method is applied to obtain the best evaluation sequence of commercial bank credit risk.},
  archive      = {J_IJITDM},
  author       = {Wangwang Yu and Xinwang Liu and Yingping Zi},
  doi          = {10.1142/S0219622025500014},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {169-196},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {The best evaluation sequence method and application based on quantum cognitive theory},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A decision modeling approach for data acquisition systems of the vehicle industry based on interval-valued linear diophantine fuzzy set. <em>IJITDM</em>, <em>24</em>(1), 89-168. (<a href='https://doi.org/10.1142/S0219622023500487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling data acquisition systems (DASs) can support the vehicle industry in the development and design of sophisticated driver assistance systems. Modeling DASs on the basis of multiple criteria is considered as a multicriteria decision-making (MCDM) problem. Although literature reviews have provided models for DASs, the issue of imprecise, unclear, and ambiguous information remains unresolved. Compared with existing MCDM methods, the robustness of the fuzzy decision by opinion score method II (FDOSM II) and fuzzy weighted with zero inconsistency II (FWZIC II) is demonstrated for modeling the DASs. However, these methods are implemented in an intuitionistic fuzzy set environment that restricts the ability of experts to provide membership and nonmembership degrees freely, simulate real-world ambiguity efficiently, utilize a narrow fuzzy number space, and deal with interval data. Thus, this study used a more efficient fuzzy environment interval-valued linear Diophantine fuzzy set (IVLDF) with FWZIC II for criterion weighting and IVLDF with FDOSM for DAS modeling to address the issues and support industrial community characteristics in the design and implementation of advanced driver assistance systems in vehicles. The proposed methodology comprises two consecutive phases. The first phase involves adapting a decision matrix that intersects DAS alternatives and criteria. The second phase (development phase) proposes a decision modeling approach based on formulation of IVLD-FWZIC II and IVLD-FDOSM II to model DASs. A total of 14 DASs were modeled on the basis of 15 DAS criteria, including seven subcriteria for “comprehensive complexity assessment” and eight subcriteria for “design and implementation,” which had a remarkable effect on the DAS design when implemented by industrial communities. Systematic ranking, sensitivity analysis, and modeling checklists were conducted to demonstrate that the modeling results were subject to systematic ranking, as indicated by the high correlations across all described scenarios of changing criterion weight values, supporting the most important research points, and proposing a value-adding process in modeling the most desirable DAS.},
  archive      = {J_IJITDM},
  author       = {M. J. Baqer and H. A. AlSattar and Sarah Qahtan and A. A. Zaidan and Mohd Azri Mohd Izhar and Iraq T. Abbas},
  doi          = {10.1142/S0219622023500487},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {89-168},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {A decision modeling approach for data acquisition systems of the vehicle industry based on interval-valued linear diophantine fuzzy set},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multidimensional analysis of investment priorities for circular economy with quantum spherical fuzzy hybrid modeling. <em>IJITDM</em>, <em>24</em>(1), 61-87. (<a href='https://doi.org/10.1142/S021962202350075X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular economy aims recycling in the production process instead of destroying the products. With the help of this situation, waste can be considered in the remanufacturing process so that the rate of consumption of natural resources can be decreased. It is necessary to focus on certain investment issues to achieve a circular economy, but all investments have some risks. Hence, the economies should make priority analysis to take efficient actions. Investment priorities are identified to have circular economy. A novel fuzzy decision-making model has been created for this purpose. In the first stage, balanced scorecard criteria are evaluated with the help of multi stepwise weight assessment ratio analysis (M-SWARA). Later, the multidimensional investment priorities of circular economy are ranked. In this context, elimination and choice translating reality (ELECTRE) approach is taken into consideration. The main contribution of the paper is that a new methodology is created by the name of M-SWARA. Owing to these new improvements, cause and effect relationship among the items can be analyzed. It is identified that financial issues play the most crucial role for investments to improve circular economy. On the other side, it is also concluded that remanufacturing is the most significant investment alternative to develop circular economy. For the sustainability of the investment to improve circular economy, necessary financial analysis should be performed. With the help of this situation, these substances can be reintroduced into the production process in the form of raw materials. With the increase of remanufacturing, it will be possible to reduce waste and save scarce material resources.},
  archive      = {J_IJITDM},
  author       = {Hasan Dinçer and Serhat Yüksel and Umit Hacıoglu and Babek Erdebilli},
  doi          = {10.1142/S021962202350075X},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {61-87},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Multidimensional analysis of investment priorities for circular economy with quantum spherical fuzzy hybrid modeling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage integrated dynamic assessment method for urban resilience based on multisource data. <em>IJITDM</em>, <em>24</em>(1), 29-59. (<a href='https://doi.org/10.1142/S0219622024410013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban resilience assessment (URA) is challenging because of urban system complexity and dynamic resilience variability. This paper develops a URA method with comprehensive feature consideration and integrated data use and then constructs a hierarchical URA criteria system. Subsequently, a two-stage integrated dynamic assessment method based on multisource data is presented, wherein the subjective–objective combination weights are determined, and the dimensional and overall urban resilience (UR) indexes are constructed. The applicability and superiority of the proposed method to existing methods are verified using a case study in Beijing. The results showed that UR in Beijing has improved substantially in 2016–2020; social and ecological dimensions are important for UR improvement; and synergies exist between different UR dimensions, which are crucial for resilient urban development. This study provides a systematic solution for URA that features a dynamic perspective, multisource data utilization, and subjective–objective combination weights, enhancing URA comprehensiveness and accuracy.},
  archive      = {J_IJITDM},
  author       = {Lulu Shen and Jianping Li and Xiaolei Sun and Weilan Suo},
  doi          = {10.1142/S0219622024410013},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {29-59},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Two-stage integrated dynamic assessment method for urban resilience based on multisource data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear step-adjusting programming in factor space. <em>IJITDM</em>, <em>24</em>(1), 7-28. (<a href='https://doi.org/10.1142/S0219622023410018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent behavior that appears in a decision process can be treated as a point y , the dynamic state observed and controlled by the agent, moving in a factor space impelled by the goal factor and blocked by the constraint factors. Suppose that the feasible region is cut by a group of hyperplanes, when point y reaches the region’s wall, a hyperplane will block the moving, and the agent needs to adjust the moving direction such that the target is pursued as faithfully as possible. Since the wall is not able to be represented by a differentiable function, the gradient method cannot be applied to describe the adjusting process. We, therefore, suggest a new model, named linear step-adjusting programming (LSP) in this paper. LSP is similar to a kind of relaxed linear programming (LP). The difference between LP and LSP is that the former aims to find the ultimate optimal point, while the latter just does a direct action in a short period. Where will a blocker encounter? How do you adjust the moving direction? Where further blockers may be encountered next, and how should the direction be adjusted again? … If the ultimate best is found, that’s a blessing; if not, that’s fine. We request at least an adjustment should be got at the first time. However, the former is idealism, and the latter is realism. In place of a gradient vector, the projection of goal direction g in a subspace plays a core role in LSP. If a hyperplane block y goes ahead along with the direction d , then we must adjust the new direction d ′ as the projection of g in the blocking plane. Suppose there is only one blocker at a time. In that case, it is straightforward to calculate the projection, but how to calculate the projection when more than one blocker is encountered simultaneously? It is still an open problem for LP researchers. We suggest a projection calculation using the Hat matrix in the paper. LSP will attract interest in economic restructuring, financial prediction, and reinforcement learning.},
  archive      = {J_IJITDM},
  author       = {Jing He and Hui Zheng and Rozbeh Zarei and Ho-Chung Lui and Qi-Wei Kong and Yi-Mu Ji and Xingsen Li and Hailong Yang and Baorui Du and Yong Shi and Pingjiang Wang and Andre van Zundert},
  doi          = {10.1142/S0219622023410018},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {7-28},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Linear step-adjusting programming in factor space},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s introduction. <em>IJITDM</em>, <em>24</em>(1), 1-5. (<a href='https://doi.org/10.1142/S0219622025030014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJITDM},
  author       = {Yong Shi},
  doi          = {10.1142/S0219622025030014},
  journal      = {International Journal of Information Technology & Decision Making},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Int. J. Inf. Tech. Decis.},
  title        = {Editor’s introduction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
