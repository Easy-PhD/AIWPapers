<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJSEKE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijseke">IJSEKE - 60</h2>
<ul>
<li><details>
<summary>
(2025). BAQoS: A burst I/O aware quality of service optimization for cloud storage service. <em>IJSEKE</em>, <em>35</em>(10), 1505-1527. (<a href='https://doi.org/10.1142/S0218194025500500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud storage services, burst I/O workloads from data analytics and artificial intelligence/machine learning (AI/ML) applications present significant challenges to Quality of Service (QoS) management. Existing scheduling models like dmClock ensure fair and stable I/O bandwidth allocation in typical scenarios. However, they falter under frequent burst traffic, leading to lower resource utilization and higher task latency. To address this, we propose BAQoS, a Burst I/O Aware Quality of Service Optimization for Cloud Storage Service. BAQoS employs refined request classification, a burst-aware hierarchical scheduling algorithm, and a high-performance scheduler architecture (HPSA). These features enable dynamic resource allocation and efficient scheduling for both burst and regular requests. Experiments show that BAQoS markedly enhances performance under burst workloads, accelerating burst request processing by up to 7.09 and improving overall system performance by 48.86%. Furthermore, BAQoS ensures superior performance for non-burst users, achieving a 51.15% performance boost for high-priority users, a 5.34% increase in system throughput, and an over 50% reduction in IOPS standard deviation among same-priority users.},
  archive      = {J_IJSEKE},
  author       = {Jingzhe Zhao and Yu Li and Hongzhang Yang and Guangping Xu and Ping Wang and Shang Yang},
  doi          = {10.1142/S0218194025500500},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1505-1527},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {BAQoS: A burst I/O aware quality of service optimization for cloud storage service},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-aware next event prediction based on temporal influence. <em>IJSEKE</em>, <em>35</em>(10), 1485-1503. (<a href='https://doi.org/10.1142/S0218194025500494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate modeling of influence relationships between events is critical to achieving effective sequential recommendation. Representing event sequences as a Hawkes process is a widely adopted approach; however, it is suboptimal in practical applications because it overlooks the contextual information of events and assumes that all historical events exert a positive influence on current events. To address these limitations, we propose a category-aware next event prediction method. This method computes both the event-level and category-level influences of all historical events associated with the target user on candidate events and recommends the Top-k ones with the highest predicted event intensities. Specifically, it captures the temporal influence between events from the perspectives of event features and the features of the categories to which the events belong. Using embedding learning, the method derives feature vectors for both events and their corresponding categories, as well as their mutual influence relationships, thereby allowing more accurate prediction of the next event. Extensive experiments on real-world datasets demonstrate that the proposed method achieves superior performance across multiple evaluation metrics compared to conventional methods.},
  archive      = {J_IJSEKE},
  author       = {Peiji Yu and Tianxing Wu},
  doi          = {10.1142/S0218194025500494},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1485-1503},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Category-aware next event prediction based on temporal influence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Datetime feature recommendation by word embedding methods using data column names. <em>IJSEKE</em>, <em>35</em>(10), 1465-1484. (<a href='https://doi.org/10.1142/S0218194025500469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of large volumes of data to derive new insights is commonly referred to as data science, and its widespread adoption is increasingly necessary. One critical step in data science workflows is feature extraction from data, known as feature engineering. This process heavily depends on expert experience, which has led to current research efforts aimed at its automation. In this paper, we introduce a novel approach to automate feature extraction from the textual information found in data column names. Specifically, we employ natural language processing and source code analysis techniques on existing source code and column names, with a particular focus on datetime features, to construct a knowledge database. Utilizing this knowledge database, we propose a system that recommends datetime features based on newly provided textual information. Departing from conventional methods such as one-hot encoding and Word2Vec word embeddings, our approach is informed by prior research and shifts toward Doc2Vec, which vectorizes at the document level. In our experiments, we validate the classification accuracy of the knowledge database and demonstrate its application in predictive tasks, such as housing price prediction, where it shows improved prediction accuracy. Our proposed approach, which utilizes a Doc2Vec model pre-trained on Wikipedia, results in enhanced prediction accuracy during vectorization.},
  archive      = {J_IJSEKE},
  author       = {Satoshi Masuda and Tomohiro Takeda},
  doi          = {10.1142/S0218194025500469},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1465-1484},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Datetime feature recommendation by word embedding methods using data column names},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning-based approach for determining infeasible paths of programs. <em>IJSEKE</em>, <em>35</em>(10), 1435-1464. (<a href='https://doi.org/10.1142/S0218194025500457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program path analysis is an essential component of software defect detection and quality assurance. Accurately identifying infeasible paths can prevent false positives caused by invalid paths, enabling developers to pinpoint actual defects more efficiently and enhancing overall software quality and reliability. This paper proposes an integrated approach for determining infeasible paths based on program path features and constraint-based reinforcement learning. First, a loop-structure path search and reduction algorithm is proposed to systematically simplify path explosion induced by loops. Then, a global subgraph-based path reduction algorithm is introduced to effectively remove redundant and irrelevant paths. Subsequently, we propose a path set generation algorithm guided by control and implication relationships to construct an optimized path set. Path constraints and symbolic path constraints are used to enhance semantic representation. Finally, a reinforcement learning-based model utilizing reachability rewards and exploration rewards to dynamically determine path reachability. Experimental results show that our proposed approach significantly reduces path explosion, accurately identifies infeasible paths and outperforms existing methods in terms of accuracy and computational efficiency.},
  archive      = {J_IJSEKE},
  author       = {Peng Dai and Tang He and Zebo Peng and Chen Zhao and Yunzhan Gong},
  doi          = {10.1142/S0218194025500457},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1435-1464},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A reinforcement learning-based approach for determining infeasible paths of programs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An iterative group-based MOPSO with isomap-guided leaders and DQN-adaptive parameters for automated path coverage test case generation. <em>IJSEKE</em>, <em>35</em>(10), 1399-1434. (<a href='https://doi.org/10.1142/S0218194025500445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Test Case Generation for Path Coverage (ATCG-PC) is a critical yet challenging task in software testing, especially for large-scale programs where metaheuristic algorithms often suffer from premature convergence and inefficient exploration. This paper proposes a novel algorithm, Isomap-DQN-MOPSO (IDMOPSO), which significantly enhances the Multi-Objective Particle Swarm Optimization (MOPSO) framework. Our approach introduces an iterative, prefix-based path grouping strategy to manage complexity. Crucially, it integrates two machine learning-based enhancements: an Isomap manifold learning strategy for more effective leader selection to guide the swarm and escape local optima, and a hybrid Deep Q-Network (DQN) for dynamically adapting learning factors to balance exploration and exploitation. Comprehensive experiments on a diverse set of 18 programs demonstrate that IDMOPSO achieves superior performance, particularly on large-scale programs where it attains significantly higher path coverage rates than state-of-the-art methods. Ablation studies confirm the synergistic effect of combining Isomap and DQN, validating our approach as a robust and scalable solution for complex ATCG-PC problems.},
  archive      = {J_IJSEKE},
  author       = {Yuchen Fang and Zhitao He},
  doi          = {10.1142/S0218194025500445},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1399-1434},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An iterative group-based MOPSO with isomap-guided leaders and DQN-adaptive parameters for automated path coverage test case generation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paraphrase-augmented evaluation for dialogue models: A unified framework with AMR and GPT-based rewriting. <em>IJSEKE</em>, <em>35</em>(10), 1383-1398. (<a href='https://doi.org/10.1142/S0218194025500433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large language models (LLMs) have achieved remarkable progress in dialogue generation. However, existing automatic evaluation methods still face challenges in diverse scenarios, particularly in terms of limited generalization and low alignment with human judgment. To address these issues, we propose a paraphrase-based evaluation framework that integrates Abstract Meaning Representation (AMR) with general-purpose language models (GPT). This approach generates diverse paraphrases across lexical, syntactic and stylistic dimensions to enhance the coverage of traditional evaluation metrics. Experimental results show that incorporating paraphrase augmentation significantly improves the correlation between automatic metrics and human evaluation on multiple datasets. Additionally, extensive experiments on six mainstream LLMs demonstrate the effectiveness and generalizability of the proposed method. This study offers new insights into improving the human alignment of automatic evaluation and lays a foundation for the application and optimization of LLMs in open-domain dialogue systems.},
  archive      = {J_IJSEKE},
  author       = {Xiao Liu and Zhenping Xie and Juncheng Zhou and Senlin Jiang},
  doi          = {10.1142/S0218194025500433},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1383-1398},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Paraphrase-augmented evaluation for dialogue models: A unified framework with AMR and GPT-based rewriting},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APT: A simple adapter for reusing RGB video transformers in compressed video action recognition. <em>IJSEKE</em>, <em>35</em>(9), 1369-1382. (<a href='https://doi.org/10.1142/S021819402550041X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing adoption of compressed video across diverse applications underscores the demand for efficient action recognition methods. Traditional RGB-based methods face limitations, especially because they depend heavily on computationally intensive optical flow for temporal analysis. We propose a novel method for compressed video action recognition that aims to effectively bridge the gap between compressed-domain and RGB-domain. Specifically, we design a plug-and-play multi-modal feature adapter that enables pretrained RGB-based Transformer models to be directly applied to compressed videos. Our method offers a low-cost cross-domain transfer solution by efficiently fusing I-frame and P-frame within compressed videos, facilitating deep feature alignment and modeling in the compressed domain. Extensive experiments on the UCF101 (93.8%) and HMDB51 (70.7%) datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches for compressed video action recognition.},
  archive      = {J_IJSEKE},
  author       = {Jiyuan Wang and Huilan Luo},
  doi          = {10.1142/S021819402550041X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1369-1382},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {APT: A simple adapter for reusing RGB video transformers in compressed video action recognition},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VulEPEDE: A function-level vulnerability detection method via enhanced positional encoding and dependency embedding. <em>IJSEKE</em>, <em>35</em>(9), 1341-1368. (<a href='https://doi.org/10.1142/S0218194025500408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As software complexity increases, integrating vulnerability detection becomes essential to ensure the security and integrity of modern systems. Traditional static and dynamic analysis methods face limitations in efficiency and accuracy, particularly for large-scale vulnerability detection, while existing deep learning methods struggle to fully capture structural information and dependencies in code, leading to incomplete identification of vulnerabilities. In this paper, we propose VulEPEDE, an innovative function-level vulnerability detection method. VulEPEDE leverages Program Dependency Graphs (PDG) to represent function code and constructs a Vulnerability Semantic Dependency Graph (VSDG) using slicing techniques, introducing function parameter nodes as slicing candidates to capture more comprehensive vulnerability trigger chains. It integrates two core modules: the Enhanced Positional Encoding (EPE) module and the Dependency Embedding (DE) module. The EPE module combines node attribute encoding with positional encoding using a Transformer and multi-head attention mechanism to capture complex features and contextual semantics of code, while the DE module learns dependency embeddings between code nodes through convolutional neural networks. We evaluate VulEPEDE on three widely used datasets, comparing its performance against state-of-the-art deep learning-based methods. Experimental results demonstrate that VulEPEDE outperforms the best baseline methods by 1.66%, 17.54%, and 28.96% in F1-score across the three datasets, with considerable computational efficiency.},
  archive      = {J_IJSEKE},
  author       = {Shuailin Yang and Jiadong Ren and Jiazheng Li and Bing Zhang and Ke Xu},
  doi          = {10.1142/S0218194025500408},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1341-1368},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {VulEPEDE: A function-level vulnerability detection method via enhanced positional encoding and dependency embedding},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DB-DSCN: Image tampering detection using dual branch deep stacked convolutional network. <em>IJSEKE</em>, <em>35</em>(9), 1323-1340. (<a href='https://doi.org/10.1142/S0218194025500366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the new era of technology with unusual image forging equipment and procedures, digital imaging has become basic. Digital images cannot even be used as evidence anywhere, since it is widely recognized that they may be faked. In order to assist in relieving such derelictions, the problem is examined in an incomprehensible manner. In the digital era, copy-move and splicing of images to produce a fabricated one are commonplace. While the latter entails combining two images to drastically alter the original and produce a new, forged image, copy-move entails copying a portion of the image and pasting it onto another portion of the image. Therefore, to address the limitation of the existing model, a novel hybrid Deep Learning (DL) model is developed. To enhance image details before detecting a tampered image, high pass filtering is employed. The high pass filter might reveal information that has emerged due to the tampered image. To extract the visual semantic features and statistical features, Attention-based Residual Network (AttResNet) and Grey Level Co-occurrence Matrix (GLCM) are utilized. An Attention-based Feature Fusion (AttFF) module is used to fuse the extracted features. A Dual Branch Deep Stacked Convolutional Network (DB-DSCN) is employed to classify the tampered image. The experimental results of the proposed model achieved an accuracy of 98.26%, a precision of 96.28%, a recall of 95.98% and an F 1-score of 96%. The proposed model seems to be a superior model for detecting tampered images compared to existing models. It acquired higher accuracy than other existing models. The performance of the proposed model is evaluated in terms of accuracy, precision, recall and F 1-score, respectively.},
  archive      = {J_IJSEKE},
  author       = {V. M. Ponney and B. L. Velammal and K. Kulothungan},
  doi          = {10.1142/S0218194025500366},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1323-1340},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DB-DSCN: Image tampering detection using dual branch deep stacked convolutional network},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-level formal specifications for deep neural networks. <em>IJSEKE</em>, <em>35</em>(9), 1289-1321. (<a href='https://doi.org/10.1142/S0218194025500342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining sufficient high-quality labeled data remains a critical challenge for training deep neural networks (DNNs). Recently, a specification-based method has been proposed to systematically define object characteristics for automated data generation. However, this approach typically relies on abstract descriptions, resulting in a gap between specifications and executable data generation. To address this issue, this paper proposes a two-level formal specification approach. Specifically, we apply the first-level specification to describe object characteristics and the second-level specification to define parameters and values suitable for data generation. This paper focuses on discussing both levels of specifications to facilitate human comprehension and machine handling to reduce the gap mentioned above. The performance of this approach is demonstrated through a case study on traffic sign recognition.},
  archive      = {J_IJSEKE},
  author       = {Yanzhao Xia and Shaoying Liu},
  doi          = {10.1142/S0218194025500342},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1289-1321},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Two-level formal specifications for deep neural networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the combination of classical knowledge engineering tools and LLMs to build automated planning models. <em>IJSEKE</em>, <em>35</em>(9), 1267-1287. (<a href='https://doi.org/10.1142/S0218194025430028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Planning (AP) is a problem-solving technique applicable to a wide range of scenarios and goals. It typically requires a complete and accurate description of the planning task expressed in a formal language to generate a solution plan that achieves the goals. However, creating these descriptions can be time-consuming and error-prone, often resulting in unsolvable planning tasks. Planning systems often lack the ability to explain why a task is deemed unsolvable. In this work, we present an integrated knowledge engineering system that allows users to graphically depict AP use cases using transition diagrams, which are automatically converted into a formal language. To facilitate the debugging process, we propose connecting the system with large language models (LLMs) to explore their capabilities in assisting with flawed planning tasks, fixing the model and making the tasks solvable.},
  archive      = {J_IJSEKE},
  author       = {Alba Gragera and Ángel García-Olaya and Fernando Fernández},
  doi          = {10.1142/S0218194025430028},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1267-1287},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {On the combination of classical knowledge engineering tools and LLMs to build automated planning models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying scrum to knowledge transfer among software developers. <em>IJSEKE</em>, <em>35</em>(9), 1239-1265. (<a href='https://doi.org/10.1142/S0218194023430015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the teaching-learning processes, research and continuous innovation are encouraged. Now, when talking about innovation, the concept of adapting methodologies that have been successfully applied to speed up and increase the quality of software projects begins to emerge, is the case of agile software development methodologies. Scrum is one of the most widely used methodologies in the software development process, it increases productivity and deliverable quality of software development teams, but there is not much evidence of how to use it to structure learning content and its use to transfer new knowledge in training or software development contexts. To fill this gap, this research analyzes the applicability of Scrum for transferring new knowledge among software developers is developed. In this paper, we describe the details of how to start transferring knowledge using Scrum, guiding the reader through its standards, processes, phases, and objectives. We also report an experiment for improving students’ performance to support the approach’s benefits in the academy context and a case study performed inside the company context.},
  archive      = {J_IJSEKE},
  author       = {Fernando Ibarra-Torres and Matias Urbieta and Nuria Medina-Medina},
  doi          = {10.1142/S0218194023430015},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1239-1265},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Applying scrum to knowledge transfer among software developers},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure and efficient authentication for smart grid communication using ECC and PUF. <em>IJSEKE</em>, <em>35</em>(8), 1219-1238. (<a href='https://doi.org/10.1142/S0218194025500391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advanced metering infrastructure AMI is an essential component of the smart grid (SG), providing essential support for two-way communication and real-time data exchange between users and utility providers. However, due to the potential untrustworthiness of the open channel, protecting the confidentiality and integrity of communication data is challenging. Previously, a number of privacy-preserving authentication and key agreement (AKA) schemes have been proposed for securing SGs. However, these solutions either have some security and privacy vulnerabilities or require expensive computational and communication costs that cannot be applied to resource-constrained smart meters. In this work, we propose a novel, lightweight and privacy-preserving AKA scheme for SGs based on the elliptic curve cryptography and physically uncloneable function. The security analysis shows that our construction is secure against several security attacks. In addition, performance comparison with several related works shows the practicality of our design.},
  archive      = {J_IJSEKE},
  author       = {Rixuan Qiu and Zhiyuan Luo and Qun He and Yu Zhou and Shuiping Kang and Chaoping Wei},
  doi          = {10.1142/S0218194025500391},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1219-1238},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Secure and efficient authentication for smart grid communication using ECC and PUF},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-energy complementary scheduling based on interval many-objective optimization of power grid for sustainable cloud computing center. <em>IJSEKE</em>, <em>35</em>(8), 1201-1217. (<a href='https://doi.org/10.1142/S021819402550038X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of renewable energy sources (RES) into new power system (NPS) represents a crucial approach for reducing carbon emissions (CEs) and mitigating resource consumption in cloud computing centers. However, the inherent uncertainties of RES generation, such as randomness, volatility, and intermittency, pose significant challenges to the scheduling of NPS. A data-driven multi-energy complementary uncertainty scheduling method is proposed to address these issues and ensure a stable power supply from NPS to cloud computing centers. First, we constructed a stochastic differential equation (SDE) to represent the uncertainty characteristics of renewable energy generation, and proposed a SDE deep network based on LSTM (LSTM-SDE Net) to capture and learn the aleatoric uncertainty and epistemic uncertainty of units; subsequently, we established the multi-energy complementary uncertain many-objective scheduling model (MuC-UMaOSM) by utilizing the interval parametric estimation, which aims to optimize grid total cost (TC), RES consumption rate (CR), CEs, grid economic benefits (EB) and load balancing (LB); finally, in order to obtain trade-off solutions with improved convergence, diversity, and uncertainty, we designed a two-population cooperative interval many-objective evolutionary algorithm (T-PIMaOEA) to solve the proposed model. Experimental results demonstrate the excellent performance of our method in the multi-energy complementary scheduling (MECS).},
  archive      = {J_IJSEKE},
  author       = {Jingbo Zhang and Jie Wen and Xingjuan Cai and Wuzhao Li},
  doi          = {10.1142/S021819402550038X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1201-1217},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Multi-energy complementary scheduling based on interval many-objective optimization of power grid for sustainable cloud computing center},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Credit card fraud detection algorithm based on a stacked ensemble model. <em>IJSEKE</em>, <em>35</em>(8), 1177-1200. (<a href='https://doi.org/10.1142/S0218194025500378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread global adoption of credit card payments has significantly increased convenience for consumers. However, this shift has also heightened the demand for robust fraud detection systems across various financial institutions. While numerous fraud detection algorithms have been proposed in both academia and industry, significant challenges remain in the accurate identification of minority class samples and the effective capture of time series information during feature extraction. To address these challenges, this paper proposes a long short-term memory-logistic regression stacked ensemble model (LLSE) that integrates long short-term memory (LSTM) and logistic regression (LR). The first layer of the model extracts time series features by combining three independently trained LSTM base learners whose outputs are concatenated and passed to the second layer, an LR meta-learner, to generate the final predictions. This dual-layer decision-making mechanism effectively captures spatiotemporal correlation features in fraudulent transactions, thereby enhancing the overall performance of the model. The model’s performance is evaluated via two publicly available credit card datasets. This study innovatively combines the TS-N2VA feature extraction model with the LLSE classifier to comprehensively enhance credit card fraud detection performance through both hidden feature extraction and classifier improvement. Experiments on two public credit card datasets demonstrate that the LLSE classifier achieves significantly better performance in identifying minority class samples while maintaining overall recognition accuracy, outperforming other fraud detection models in the experiments.},
  archive      = {J_IJSEKE},
  author       = {Tinggui Chen and Xiaqin Yang and Limin Ni},
  doi          = {10.1142/S0218194025500378},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1177-1200},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Credit card fraud detection algorithm based on a stacked ensemble model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robot control knowledge recommendation model PKGAT based on multimodal knowledge graph. <em>IJSEKE</em>, <em>35</em>(8), 1149-1175. (<a href='https://doi.org/10.1142/S0218194025500354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of robotic control, the interdisciplinary and complex nature of knowledge leads to issues such as knowledge fragmentation and a steep learning curve, posing significant challenges to mastering domain expertise. Recommendation systems, as typical information-filtering tools, are often employed to facilitate knowledge retrieval. Nevertheless, they inherently suffer from cold-start and data sparsity problems, which compromise recommendation accuracy. To overcome these limitations, this study first combines the recommendation system with the knowledge graph and improves upon the traditional BERT-BiLSTM-CRF entity recognition model by incorporating an attention mechanism to enhance entity extraction performance. Verified on the public dataset, the accuracy rate, recall rate and F1 value of the improved model were 91.6%, 94.5% and 93.1%, respectively, which increased by 1.4%, 2.9% and 2.3%, respectively, compared with the BERT-BiLSTM-CRF model. Subsequently, by using the YOLOv5 object detection network to extract image features from the self-built robot image dataset, a multimodal robot control knowledge graph was constructed. Finally, to address the limitations of conventional KGAT, a Personalized Knowledge Graph Attention Network (PKGAT) model is proposed by integrating the recommendation system with the constructed knowledge graph and incorporating a cold-start mitigation module. This results in a knowledge graph-based recommendation system tailored for robotic control domain knowledge. The results from the experiments show that the AUC value and F1 value of the PKGAT model in the MovieLens dataset are 94.56% and 94.20% respectively, which have increased by 2.35% and 2.07%, respectively, compared with the KGAT model. The AUC value and F1 value in the Book-Crossing dataset were 75.30% and 73.60%, respectively, which increased by 2.02% and 1.48%, respectively, compared with the KGAT model.},
  archive      = {J_IJSEKE},
  author       = {Xingda Hu and Zhongchen Yuan and Zongmin Ma},
  doi          = {10.1142/S0218194025500354},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1149-1175},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A robot control knowledge recommendation model PKGAT based on multimodal knowledge graph},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAPRSUM: Retrieval-augmented code summarization with advanced prototype refinement. <em>IJSEKE</em>, <em>35</em>(8), 1121-1147. (<a href='https://doi.org/10.1142/S0218194025500317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code summarization aims to automatically generate natural language descriptions for code snippets, thereby enhancing programmers’ comprehension of the code and significantly improving code maintainability and development efficiency. Code reuse is a prevalent practice in software development, and information retrieval approaches, which leverage similar summaries from a corpus, have shown promising results. However, these approaches often heavily rely on the quality of the corpus, and discrepancies between retrieved and target summaries may result in inaccuracies or incomplete descriptions. In this paper, we propose RAPRSUM, a retrieval-augmented code summarization framework that integrates information retrieval and deep learning techniques. Specifically, RAPRSUM refines retrieved summary prototypes using a deep learning model to preserve essential information while eliminating redundancy. A pretrained large language model is then employed to generate the final summary by jointly considering the input code and the refined prototype. We evaluated RAPRSUM on three public datasets, demonstrating its significant improvement over existing baseline methods. Additional ablation studies confirm the effectiveness of the proposed modules. Finally, through case analysis and robustness verification, we highlight RAPRSUM’s superior performance in generating high-quality code summaries.},
  archive      = {J_IJSEKE},
  author       = {Yangbo Lin and Xingqi Wang and Shanggui Zhan and Dan Wei and Bin Chen},
  doi          = {10.1142/S0218194025500317},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1121-1147},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {RAPRSUM: Retrieval-augmented code summarization with advanced prototype refinement},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modernizing 90s era software to a new language and environment using LLMs — An empirical investigation. <em>IJSEKE</em>, <em>35</em>(8), 1099-1119. (<a href='https://doi.org/10.1142/S021819402550024X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legacy software, particularly from the 1990s, often becomes obsolete due to aging hardware and outdated software environments. Traditionally, software modernization required extensive manual effort, involving reverse engineering, code rewriting, and re-architecting. However, advancements in large language models (LLMs) have introduced new possibilities for automating software translation and modernization. This paper explores the feasibility of using LLMs for modernizing 90s-era Windows applications, specifically migrating legacy C and C + + code to Python. Our methodology includes decompilation, source code analysis, automated translation using ChatGPT, and user interface reconstruction. We empirically evaluate three software projects by analyzing LLM-based translation accuracy across different code structures, including algorithmic logic, file handling, and graphical interfaces. Results indicate that while LLMs achieve high translation accuracy ( − 8 8 % ) for structured code, challenges persist in handling decompiled code and user interface generation. The study provides insights into the effectiveness and limitations of LLMs in real-world software renovation, offering guidelines for leveraging machine learning in legacy system modernization. These findings contribute to both academic research and practical applications, suggesting a pathway for cost-effective and scalable legacy software migration.},
  archive      = {J_IJSEKE},
  author       = {Dragan Bojić and Dražen Drašković},
  doi          = {10.1142/S021819402550024X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1099-1119},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Modernizing 90s era software to a new language and environment using LLMs — An empirical investigation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generation of tutorial systems on PCs for smartphone apps. <em>IJSEKE</em>, <em>35</em>(7), 1071-1098. (<a href='https://doi.org/10.1142/S0218194025500330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The total number and diversity of smartphone users are increasing. Additionally, their usage varies such as communication, document processing and online shopping. Applications realize different usages to meet users’ requirements. However, some users are unfamiliar with smartphones. They have difficulty learning the operations and need support. A tutorial system, which provides operations along with users’ status, is effective to resolve these issues. However, showing both the target application’s screen and the tutorial on a smartphone display is challenging. Herein we propose a method to generate a system that shows tutorials on a Personal Computer (PC). The target application on a smartphone and the tutorial system on a PC are connected. The user’s status is sent to the tutorial system to realize a customized tutorial. To generate the tutorial system, a log obtaining function is added to the target application without modifying the source program. The log data of user’s operations are recorded. Operation flows in the target application are constructed and frequent operations are weighted. Then tutorials are generated and shown based on the weighting. Our method allows a user to view appropriate tutorials for the current status of the operation on a large display of a PC.},
  archive      = {J_IJSEKE},
  author       = {Sixin Hua and Junko Shirogane and Yoshiaki Fukazawa},
  doi          = {10.1142/S0218194025500330},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1071-1098},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Generation of tutorial systems on PCs for smartphone apps},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of MBFL on novice programs across different programming languages. <em>IJSEKE</em>, <em>35</em>(7), 1037-1070. (<a href='https://doi.org/10.1142/S0218194025500329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming education in computer science is growing rapidly, and debugging is a key challenge for novice programmers due to their limited experience. Mutation-Based Fault Localization (MBFL) is widely used in industry, but its effectiveness and challenges in novice programs need further study. While Python is a popular language in machine learning and data science, there is little research comparing fault localization in Python and Java for novice programmers. To bridge this gap, we conduct an empirical study to evaluate MBFL’s accuracy and execution overhead in common novice programming errors across different languages. We analyze how program features like code coverage and mutation score affect MBFL’s performance and whether these effects differ between languages. We also examine how MBFL’s effectiveness changes when suspiciousness scores are the same and how mutant noise and coincidental correct test cases vary across languages. Additionally, we propose a mutation confidence formula based on repair potential and behavioral difference to assess the usefulness of mutants in MBFL. Our study demonstrates that MBFL works well for novice fault localization in both Java and Python, with Python performing better. MBFL correctly identifies 45, 70, and 92 faults within the TOP-N (N = 1, 3, 5), proving its strong performance. However, tie problems, mutant noise, and coincidental correct test cases weaken MBFL, especially in Java. Results in both languages show a strong positive correlation between mutant confidence and fault localization accuracy, confirming the formula’s effectiveness across languages.},
  archive      = {J_IJSEKE},
  author       = {Yating Yang and Ao Mei and Chao Yang},
  doi          = {10.1142/S0218194025500329},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1037-1070},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study of MBFL on novice programs across different programming languages},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MalRGBDet: Windows malware detection method based on RGB image representation and heterogeneous neural network. <em>IJSEKE</em>, <em>35</em>(7), 1009-1036. (<a href='https://doi.org/10.1142/S0218194025500305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world’s most widely used operating system, Windows has long been a primary target for malware attacks, causing severe economic losses and threats to data security for users and enterprises. Existing detection methods often struggle with low accuracy when dealing with complex malware, suffering from high false-negative and false-positive rates. Additionally, malware detection in Windows faces challenges such as limited datasets, a lack of benign sample contrast and insufficient original feature information. To address these issues, we propose a malware detection method based on RGB image representation and heterogeneous neural network (MalRGBDet). First, we collected malware samples from the GitHub and VirusShare platforms, along with benign software from Windows systems, to build a dataset named MalDet. This data set contains unprocessed malicious and benign samples, providing original feature information and addressing the lack of benign samples in existing data sets. Next, we extracted three key features from the malware samples: code sections, data sections and API call sequences. These features closely relate to the behavior of malware and accurately describe its operations. We then transformed these features into uniformly sized RGB images, which helped reveal hidden patterns. Finally, we employ a heterogeneous neural network that integrates ResNet and AlexNet for classification. ResNet, with its deep architecture and residual learning mechanism, significantly enhances the model’s representation capability and classification performance, thereby improving detection accuracy. Meanwhile, AlexNet’s Dropout regularization strategy effectively boosts the model’s generalization ability. In our data set of 1952 Windows software samples, MalRGBDet achieved more than 95% in accuracy, precision, recall and F1-score, improving these metrics by up to 4% compared to the latest methods. Furthermore, false-negative and false-positive rates were kept below 5%.},
  archive      = {J_IJSEKE},
  author       = {Rong Ren and Hongchang Zhang and Bing Zhang and Haitao He and Guoyan Huang and Qian Wang},
  doi          = {10.1142/S0218194025500305},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1009-1036},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {MalRGBDet: Windows malware detection method based on RGB image representation and heterogeneous neural network},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced fault detection using coupling and cohesion metrics with deep CNN modeling. <em>IJSEKE</em>, <em>35</em>(7), 987-1007. (<a href='https://doi.org/10.1142/S0218194025500299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting faults in software modules is important for reducing system failures and improving software quality. Traditional fault prediction methods often rely on failure history or statistical models, which may not work well when structural complexities exist within the code. This work introduces a new model called Enhanced Coupling and Cohesion Metrics-based Fault Detection (ECCMFD). It uses deep structural properties of code, such as Conceptual Lack of Cohesion in Methods (C-LCOM) and Conceptual Coupling Between Object Classes (CCBO), to capture how components interact and how focused each class remains on its purpose. These metrics are passed into a Deep Convolutional Neural Network (Deep CNN) that learns patterns in software design and predicts fault-prone modules. The model is evaluated on standard NASA MDP datasets including KC1, CM1 and PC3. It outperforms widely used models like Baseline CNN, Random Forest (RF) and XGBoost in all key evaluation metrics. ECCMFD achieved a 5% improvement in precision, a reduction in Root Mean Square Error (RMSE) by 0.3, and better performance in F1 score and accuracy. This improvement is due to the combination of well-defined structural metrics and the deeper feature learning capability of the deep CNN architecture.},
  archive      = {J_IJSEKE},
  author       = {Ravi Kumar Tirandasu and Prasanth Yalla and Sridevi Tumula and Premkumar Chithaluru and Manoj Kumar and Anuradha Dhull},
  doi          = {10.1142/S0218194025500299},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {987-1007},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhanced fault detection using coupling and cohesion metrics with deep CNN modeling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing python code smell detection with heterogeneous ensembles. <em>IJSEKE</em>, <em>35</em>(7), 963-986. (<a href='https://doi.org/10.1142/S0218194025500287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code smells indicate potential issues in Software design that can impact maintainability, testing and overall quality. Detecting them early is crucial for improving system reliability. While machine learning has been used for code smell detection, most studies focused on Java, with limited research on other languages. In this study, we empirically investigated the effectiveness of both deep learning and heterogeneous ensemble models in detecting multiple Python code smells, including Large Class, Long Method, Long Scope Chaining, Long Parameter List and Long Base Class List. We evaluated three heterogeneous ensemble models: Stacking, Hard Voting and Soft Voting ensembles, alongside three deep learning models: Convolutional Neural Networks, Long Short-Term Memory and Gated Recurrent Units. Each ensemble was built using eight base models, and the Wilcoxon test was used to assess performance differences. Results indicated that Stacking consistently outperformed other models with superior stability and detection performance. Convolutional Neural Networks performed well in some smells but struggled with complex nested structures, where ensemble models offered more stability. Hard and Soft Voting ensembles were competitive but less stable than Stacking. These findings highlight the potential of ensemble and deep learning models in enhancing Python code smell detection.},
  archive      = {J_IJSEKE},
  author       = {Rana Sandouka and Hamoud Aljamaan},
  doi          = {10.1142/S0218194025500287},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {963-986},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing python code smell detection with heterogeneous ensembles},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances and challenges in multimodal entity linking: A comprehensive survey. <em>IJSEKE</em>, <em>35</em>(7), 943-961. (<a href='https://doi.org/10.1142/S0218194025300039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study offers a comprehensive examination of the recent advances and challenges in multimodal entity linking (MEL). The paper begins by outlining the background of the growth of multimodal data and the significance of knowledge bases. It provides a detailed description of the definition and architecture of the MEL task. Through an extensive analysis of existing methods, datasets, and evaluation metrics, this paper highlights the potential of MEL to enhance information retrieval accuracy and facilitate knowledge graph construction. The study indicates that, despite significant progress in the MEL field, challenges remain, including issues related to data quality, model generalization, model interpretability and multimodal fusion technologies. Finally, the paper proposes future research directions, such as the application of large language model and the development of scientific theories related to multimodal information fusion and disambiguation, thereby offering clear guidance for subsequent research.},
  archive      = {J_IJSEKE},
  author       = {Huayu Li and Xiaotong He and Yang Yue and Cuicui Wang},
  doi          = {10.1142/S0218194025300039},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {943-961},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Advances and challenges in multimodal entity linking: A comprehensive survey},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative API recommendation based on global semantics and local context. <em>IJSEKE</em>, <em>35</em>(6), 917-941. (<a href='https://doi.org/10.1142/S0218194025500275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During software development, developers often need appropriate but unfamiliar APIs to implement a specific functionality. Under such circumstances, developers tend to leverage search tools to seek for the relevant APIs. However, there are always semantic gaps between query words and APIs, which negatively affects the performance of these tools. In this study, we introduce Glo-APIRec, a method that combines global semantics with local context to estimate the semantic relevance between query words and APIs to recommend APIs. In this method, the Transformer model is employed to obtain global semantics, while the Word2Vec model is utilized to capture local context using a fixed-size window. First, Glo-APIRec collects millions of Java projects from GitHub to construct the corpus. Afterward, a set of tuples consisting of words and APIs is built by extracting comments and API sequences from the source code files. Finally, Transformer is employed to capture long distance semantics about API sequences and code comments. Meanwhile, Word2Vec is used to generate word vectors to capture the local context by introducing the random shuffling strategy to break the positions of words and APIs in the tuples. We evaluate the performance of Glo-APIRec with 30 sentence-level queries. Experimental results show that Glo-APIRec can achieve 0.600 in terms of SuccessRate for top-1 recommendation and 0.900 for top-10 recommendation. When recommending 10 APIs, Glo-APIRec can achieve 0.480, 0.703 and 0.717 in terms of precision, Mean Reciprocal Rank ( M R R M R R MRR ) and Normalized Discounted Cumulative Gain ( N D C G N D C G NDCG ), and outperforms the state-of-the-art method by 26.2%, 31.7% and 27.9%, respectively.},
  archive      = {J_IJSEKE},
  author       = {Shuoming Li and Dongjin Yu and Xin Chen and Xulin Fan and Dengfa Luo and Tong Wu and Wangliang Yan},
  doi          = {10.1142/S0218194025500275},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {917-941},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Generative API recommendation based on global semantics and local context},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCATCom: Code comment generation by fusing multi-information. <em>IJSEKE</em>, <em>35</em>(6), 887-916. (<a href='https://doi.org/10.1142/S0218194025500263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several code comment generation approaches based on sequence-to-sequence (Seq2Seq) models have been proposed. Such approaches often extract structure information from abstract syntax trees (ASTs) using a certain serialization method. However, some structural information is inevitably lost while serializing ASTs. Furthermore, existing serialization methods only consider the “type” attribute of the nodes, neglecting the “value” attribute of the nodes. To further improve the performance of code comment generation, we propose a code comment generation approach, called SCATCom, which integrates a more comprehensive set of information from source code and ASTs, encompassing semantic, sequential, syntactic, and hierarchical structure information for code comment generation. Meanwhile, an AST traversal method, called V-POT, is presented, which considers both the “type” and the “value” attributes of the nodes. Experiments were designed and conducted on two commonly used datasets to validate the performance of our approach and the impact of five different serialization ways of ASTs on two code comment generation methods. The BLEU, METEOR, and ROUGE scores for our approach reach 52.6, 34.16, and 63.26 with an improvement of 4 . 1 7 ∼ 1 7 . 3 9 , 5 . 1 ∼ 2 1 . 5 5 , and 6 . 3 6 ∼ 2 2 . 1 6 compared to the baselines. It is evident that V-POT, which retains both the “type” and the “value” attributes, is superior to other methods that use only the “type” attribute.},
  archive      = {J_IJSEKE},
  author       = {Rongcun Wang and Jie Zhou and Xiang Chen and Zhanqi Cui and Shujuan Jiang},
  doi          = {10.1142/S0218194025500263},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {887-916},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {SCATCom: Code comment generation by fusing multi-information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying android malware using fine-grained path information from HIN. <em>IJSEKE</em>, <em>35</em>(6), 861-886. (<a href='https://doi.org/10.1142/S0218194025500251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the rapid advances of mobile internet and Internet of Things (IoT), Android has become one of the most widely used operating systems in mobile terminals and IoT devices. However, the massive growth of Android malware poses challenging security problems to these terminals and devices. In this paper, we propose a novel heterogeneous information network (HIN)-based method, called FDroid, for fast and accurate detection of Android malware. Specifically, we first design a fine-grained HIN to model the relationship between APKs and APIs and then extract finer-grained path information, including code blocks, packages and call patterns compared to traditional HIN-based methods, which can effectively improve the detection accuracy without increasing the number of paths. Second, we devise a TF-IWF-based contribution calculation algorithm to select a small number of sensitive APIs calls with high representativeness, which can effectively save the detection time and storage space. Third, we develop an expanded matrix-assisted support vector machine (SVM) classifier for Android malware detection. Experimental results show that the FDroid can achieve 97.43% detection accuracy. Meanwhile, compared with the other related HIN-based malware detection methods, the training time of FDroid is less than 0.1% of them, and the detection time is less than 10% of them.},
  archive      = {J_IJSEKE},
  author       = {Erfan Zhao and Weina Niu and Cheng Huang and Xixuan Ren and Jiacheng Gong and Anran Hou},
  doi          = {10.1142/S0218194025500251},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {861-886},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying android malware using fine-grained path information from HIN},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RESEARCH NOTES: Multiclass classification for self-admitted technical debt via large pre-trained language model. <em>IJSEKE</em>, <em>35</em>(6), 835-860. (<a href='https://doi.org/10.1142/S0218194025500238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical debt refers to suboptimal solutions adopted for short-term goals. Self-admitted technical debt (SATD) is the debt that is explicitly marked through comments or documentation, making it traceable. Multi-classification of SATD helps developers understand different debt types and improve efficiency. This paper proposes a SATD multi-classification method based on Fine-Tuning the GPT-3.5-turbo model for SATD prediction. This study uses a public dataset containing 10 projects with code comments. We classify design debt, requirement debt, and defect debt and evaluate our method’s performance. The experimental results show that compared to the best baseline model, our method achieves average improvements of 11.41%, 1.72% and 3.72% in MacroF, MacroP and MacroR metrics, respectively, in the MTO scenario. In the OTO scenario, improvements are 2.33%, 3.70% and 2.18%, respectively. These results indicate that our method has a strong generalization ability in SATD multi-classification and offers a new approach to managing technical debt.},
  archive      = {J_IJSEKE},
  author       = {Yiyang Du and Xingguang Yang and Zhenyu Shu and Zijie Huang and Gang Wang and Libo Xu},
  doi          = {10.1142/S0218194025500238},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {835-860},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {RESEARCH NOTES: Multiclass classification for self-admitted technical debt via large pre-trained language model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RR-GCN: Exploring untrained random embeddings for relational graphs. <em>IJSEKE</em>, <em>35</em>(6), 809-834. (<a href='https://doi.org/10.1142/S0218194025500184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inception of the Relational Graph Convolutional Network (R-GCN) marked a milestone in the Semantic Web domain as a widely cited method that generalizes end-to-end hierarchical representation learning to Knowledge Graphs (KGs). R-GCNs generate representations for nodes of interest by repeatedly aggregating parametrized, relation-specific transformations of their neighbors. However, in this work, it is posited that the R-GCN’s main contribution lies in this “message passing” paradigm , rather than the learned weights. To prove this, the “Random Relational Graph Convolutional Network” (RR-GCN) is introduced, which leaves all parameters untrained and thus constructs node embeddings by aggregating randomly transformed random representations from neighbors. Additionally, the advantage offered by learnable parameters for RR-GCN without completely losing the advantages of random transformations is explored. It is empirically shown that RR-GCNs can compete with fully trained R-GCNs in node classification.},
  archive      = {J_IJSEKE},
  author       = {Sandeep Ramachandra and Vic Degraeve and Gilles Vandewiele and Bram Steenwinckel and Sofie Van Hoecke and Femke Ongenae},
  doi          = {10.1142/S0218194025500184},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {809-834},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {RR-GCN: Exploring untrained random embeddings for relational graphs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in call graph methodologies for enhanced program comprehension. <em>IJSEKE</em>, <em>35</em>(6), 793-808. (<a href='https://doi.org/10.1142/S0218194025300015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of software development, where maintaining and understanding complex systems is increasingly challenging, call graph techniques play a critical role in enhancing software comprehension by providing a visual and structural representation of function calls within a system. This paper explores the role of call graphs in simplifying software maintenance and debugging. It highlights how call graphs significantly improve developers’ understanding of system architectures and function interactions, reducing the time spent on manual code exploration. Furthermore, the paper explores recent advancements in call graph techniques, particularly the integration of machine learning and deep learning models with traditional call graph approaches. This hybrid methodology demonstrates enhanced accuracy and relevance in tasks such as program comprehension and code refactoring, making it a valuable tool for modern software engineering practices.},
  archive      = {J_IJSEKE},
  author       = {Rakan Alanazi},
  doi          = {10.1142/S0218194025300015},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {793-808},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Advancements in call graph methodologies for enhanced program comprehension},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class imbalance-oriented online feature selection method for just-in-time software defect prediction. <em>IJSEKE</em>, <em>35</em>(5), 767-791. (<a href='https://doi.org/10.1142/S0218194025500226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just-in-time software defect prediction (JIT-SDP) is a defect prediction technique that targets changes in software code, offering significant advantages in quickly identifying potential defects and improving development efficiency. However, most existing methods assume that the importance of features remains stable over time, overlooking the dynamic changes in feature distributions and the evolution of class imbalance in real-world development environments. This limitation eventually degrades the predictive performance. To address this issue, this paper proposes an Imbalance-oriented Online Feature Selection (IOFS) method, which dynamically adjusts the feature importance and uncertainty parameters to adapt in real time to concept drift and class imbalance in data streams, thereby enhancing model performance and generalization. The experimental validation on 14 open-source project datasets demonstrates that IOFS significantly improves the values of G − G − G− Mean on 11 datasets and effectively reduces the average of the absolute differences between recalls for each time step, exhibiting robustness to dynamic feature changes and sensitivity to development-phase feature differences. This study provides an effective solution for online JIT-SDP.},
  archive      = {J_IJSEKE},
  author       = {Qiao Yu and Siyu Ren and Yi Zhu and Jiaxuan Jiang and Shutao Zhang},
  doi          = {10.1142/S0218194025500226},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {767-791},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Class imbalance-oriented online feature selection method for just-in-time software defect prediction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CodeQG: Automated multiple question generation for source code comprehension. <em>IJSEKE</em>, <em>35</em>(5), 737-765. (<a href='https://doi.org/10.1142/S0218194025500214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During software maintenance and evolution, developers spend more than half of their time on code comprehension activities. In order to understand an unfamiliar code base, they would naturally ask different types of questions related to code snippets and try to find the answers. In this paper, we conduct an initial work to explore the possibility of automatic question generation for program comprehension. We construct a large-scale data set containing pairs of source code and questions that are automatically transformed from inline comments based on dependency analysis and semantic role labeling. We also build a comprehensive taxonomy of question types so as to generate questions concerning different aspects of code snippets, such as purpose, implementation details and so on. Then, we propose a deep learning-based prototype CodeQG to automatically generates multiple types of questions for code snippets. We evaluate CodeQG by using both typical performance metrics and manual evaluation. The results show that (1) we can achieve a value of 42.02 on BLEU4 and 60.81 on ROUGE-L for the generated questions; (2) overall, the questions are very correct in grammatical, semantic and format; (3) the questions are related to the corresponding code snippet and are helpful for developers in source code comprehension activities. Our work gives insights into automatically generating multiple types of questions for code comprehension. We expect this exploration will improve the applicability and generality of machine code comprehension.},
  archive      = {J_IJSEKE},
  author       = {Xiaowei Zhang and Lin Chen and Kaiyuan Qi and Weiqin Zou and Liye Pang and Lianfa Zhang and Peng Zhang and Guanqun Xu and Dong Zhang},
  doi          = {10.1142/S0218194025500214},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {737-765},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {CodeQG: Automated multiple question generation for source code comprehension},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTCEA: Guiding multi-modal entity alignment via entity-type information. <em>IJSEKE</em>, <em>35</em>(5), 707-735. (<a href='https://doi.org/10.1142/S0218194025500202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal entity alignment aims to identify equivalent entities across diverse knowledge graphs by leveraging multiple modalities of entity information. This process is crucial for the fusion of multi-modal knowledge graphs. While current research primarily investigates how to utilize side information from entity visuals, relations, and attributes, it often overlooks the significant role of entity-type information. Furthermore, multi-modal data embedding encounters noise that negatively impacts the performance of the entity alignment task. To address these gaps, this paper introduces MTCEA, a multi-modal entity alignment method guided by entity-type information. The proposed method captures the constraints associated with entities based on the entity-type information obtained from knowledge graph ontology; then, it utilizes two embedding strategies for type constraints to enhance the model’s performance in knowledge representation. This allows effective modal fusion that integrates more fine-grained semantic constraints related to types, which improves the alignment accuracy across various cross-lingual knowledge graphs. MTCEA is validated on three subsets of DBP15K. Experimental results demonstrate that our model achieves good results overall on the Hits@1, Hits@10, and MRR metrics. In an experimental setting without using entity name, MTCEA outperforms state-of-the-art baselines.},
  archive      = {J_IJSEKE},
  author       = {Xiaoming Zhang and Ziyi Zheng and Huiyong Wang and Mehdi Naseriparsa},
  doi          = {10.1142/S0218194025500202},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {707-735},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {MTCEA: Guiding multi-modal entity alignment via entity-type information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VulnTrace: Tracking and detecting code vulnerabilities with historical commits and semantic embeddings. <em>IJSEKE</em>, <em>35</em>(5), 679-706. (<a href='https://doi.org/10.1142/S0218194025500196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open source software has evolved into a fundamental element of the contemporary information sector; however, security threats within its supply chain are persistently rising. Within the collaborative development framework of open source, the introduction of malicious code can lead to significant security vulnerabilities. Conventional methods for detecting these vulnerabilities, which rely on machine learning, face challenges such as a lack of sufficient datasets, inadequate deep semantic understanding, and limitations to single-vulnerability detection. To address these challenges, we introduce a novel approach named VulnTrace, which analyzes historical records of submissions in open source projects to construct a high-quality dataset of vulnerabilities with accurate labels. VulnTrace employs Word2Vec alongside Abstract Syntax Tree (AST) technologies to capture both the semantic and structural details of code segments and utilizes a Transformer model for precise vulnerability identification, thereby enhancing accuracy and interpretability in detection. Experimental results indicate that VulnTrace achieves approximately 93% accuracy, 95% precision, 83% recall and an F1 score of 88% in vulnerability detection tasks, significantly reducing false positives and demonstrating remarkable robustness.},
  archive      = {J_IJSEKE},
  author       = {Qijie Song and Jiaobo Jin and Tiantian Zhu and Tieming Chen and Mingqi Lv and Licheng Pan and Jian-Ping Mei and Xiang Pan},
  doi          = {10.1142/S0218194025500196},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {679-706},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {VulnTrace: Tracking and detecting code vulnerabilities with historical commits and semantic embeddings},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting the interpretability of GraphCodeBERT for code similarity tasks. <em>IJSEKE</em>, <em>35</em>(5), 657-678. (<a href='https://doi.org/10.1142/S0218194025500160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the degree of similarity of code fragments is crucial for ensuring software quality, but it remains challenging due to the need to capture the deeper semantic aspects of code. Traditional syntactic methods often fail to identify these connections. Recent advancements have addressed this challenge, though they frequently sacrifice interpretability. To improve this, we present an approach aiming to augment the transparency of the similarity assessment by using GraphCodeBERT, which enables the identification of semantic relationships between code fragments. This approach identifies similar code fragments and clarifies the reasons behind that identification, helping developers better understand and trust the results. The source code for our implementation is available at https://www.github.com/jorge-martinez-gil/graphcodebert-interpretability .},
  archive      = {J_IJSEKE},
  author       = {Jorge Martinez-Gil},
  doi          = {10.1142/S0218194025500160},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {657-678},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Augmenting the interpretability of GraphCodeBERT for code similarity tasks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review of software vulnerability mining approaches based on symbolic execution. <em>IJSEKE</em>, <em>35</em>(5), 609-656. (<a href='https://doi.org/10.1142/S0218194025300027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the software industry, the escalating issue of software vulnerabilities has posed significant risks to users. Symbolic execution as a vulnerability mining technology offers a high-test coverage. The existing reviews of symbolic execution methods focus on summarizing various techniques and tools. While some studies have analyzed the technical challenges, classification frameworks and development trends of these methods, they lack a comprehensive and systematic review. This study aims to address the gap in existing reviews by providing a comprehensive, systematic analysis of symbolic execution techniques for vulnerability mining. We conducted a detailed review of 60 peer-reviewed papers published between 2005 and 2024, focusing on symbolic execution techniques for vulnerability mining. First, we reviewed the main techniques used in the symbolic execution process, including program instrumentation, path selection strategy and constraint-solving techniques. Second, we extracted the main information from the selected papers, and the detailed information on the symbolic execution tools is in the form of a table. Compared and analyzed the research object, the execution process at the same time, the software architecture and the application on different system platforms. Finally, we present a comprehensive and systematic summary of current challenges and corresponding solutions in the field. This study provides an in-depth analysis of vulnerability detection technologies based on symbolic execution, serving as a valuable guide for researchers in this domain.},
  archive      = {J_IJSEKE},
  author       = {Lining Li and Rong Ren and Jun Dong and Bing Zhang and Xu Kang},
  doi          = {10.1142/S0218194025300027},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {609-656},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A systematic literature review of software vulnerability mining approaches based on symbolic execution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PEAPOD: A heterogeneous defect prediction approach based on deep density sampling and deep domain adaptation. <em>IJSEKE</em>, <em>35</em>(4), 569-607. (<a href='https://doi.org/10.1142/S0218194025500172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Defect Prediction (HDP) refers to using labeled instance data from source projects to predict potential defect instances for the target project. This approach addresses the feature heterogeneity problem between different datasets, aiming to improve software quality actually. Notably, existing research on HDP addresses imbalance issues by focusing solely on minority classes and neglecting the influence of majority classes. Moreover, these studies fail to regard issues of feature consistency and differences in data distribution in depth while addressing heterogeneity in the feature space. Consequently, this paper proposes a 2-stage HDP approach, PEAPOD, based on dee P d E nsity s A mple and dee P d O main a D aptation. First, PEAPOD creates a balanced dataset by transforming data into a latent space, selecting high-quality samples by density and enhancing the defective class with synthetic samples through feature-level oversampling. Next, PEAPOD aligns matching features utilizing KS-test and the Hungarian algorithm and subsequently reduces data distribution differences via a Domain Adversarial Neural Network (DANN). Extensive and rigorous experimental results on 23 projects from 5 public datasets demonstrate PEAPOD outperforms HDP baseline methods.},
  archive      = {J_IJSEKE},
  author       = {Yifan Zou and Huiqiang Wang and Hongwu Lv and Shuai Zhao},
  doi          = {10.1142/S0218194025500172},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {569-607},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {PEAPOD: A heterogeneous defect prediction approach based on deep density sampling and deep domain adaptation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep neural networks for traffic flow estimation for Spatial–Temporal DOMAIN. <em>IJSEKE</em>, <em>35</em>(4), 547-567. (<a href='https://doi.org/10.1142/S0218194025500159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow estimation is critical for road traffic management. However, the traditional systems measuring procedures demand a significant amount of time and money to continually collect the essential data and keep the associated hardware, like cameras and sensors. On the other hand, deep learning approaches give a scientifically solid framework for modeling the ambiguity and complicated relationships between multiple variables. Accordingly, this study utilized deep learning networks and built a model to estimate traffic flows using anticipated journey times. Stacked Autoencoders (SAEs), Gated Recurrent Units (GRUs) and Long Short-Time Memory units (LSTMs) techniques were specifically used to train the model. A number of experiments were carried out using various time sequence values and compared the estimated traffic flows produced by the suggested model and the actual ones collected from real sensors. The results demonstrate that the suggested model is capable of capturing the general trend of actual traffic flows. Our research proposes building a model to estimate traffic flows using anticipated journey times to capture the general trend of actual traffic loads using a deep learning model. Different experiments are completed using various time sequence values and comparing the estimated traffic flows produced by the suggested models and real data sensors. This work provides a solution using a deep learning model to estimate traffic flow that provides a consummate for e-businesses to use this approach to reduce their transportation cost and increase their customer satisfaction.},
  archive      = {J_IJSEKE},
  author       = {Ahmet E. Topcu and Yehia Ibrahim Alzoubi and Ersin Elbasi and Erdem Ozdemir},
  doi          = {10.1142/S0218194025500159},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {547-567},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Deep neural networks for traffic flow estimation for Spatial–Temporal DOMAIN},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate query for industrial fault knowledge graph based on vector index. <em>IJSEKE</em>, <em>35</em>(4), 525-545. (<a href='https://doi.org/10.1142/S0218194025500147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the industrial sector, index-based fault knowledge graph query techniques are essential for accelerating fault information retrieval and improving the accuracy and efficiency of diagnosing equipment issues. Using knowledge graph embedding, these systems transform entities and their relationships into dense vectors, making it easier for machine learning algorithms to process knowledge graph queries effectively. However, existing models often focus on boosting search accuracy at the cost of time efficiency, particularly when dealing with large fault knowledge graphs. To address this, we propose an optimized query method for fault knowledge graphs using vector indexing. The process starts by converting the entities and relationships in the knowledge graph into a vector space, generating a concise vector representation. Advanced vector database technology is then employed to build a specialized vector index library designed for fault knowledge graphs. This includes dividing the search space through clustering algorithms and employing approximate matching techniques to enhance query speed. By utilizing the indexed fault knowledge graph, we can conduct similarity searches to facilitate approximate querying. Evaluations show that our approach significantly reduces search times and outperforms traditional methods in terms of accuracy, demonstrating the value of vector index libraries in boosting the overall query efficiency of knowledge graphs, while keeping high accuracy levels.},
  archive      = {J_IJSEKE},
  author       = {Shichen Zhai and Hao Ji and Kun Zhang and Yongcheng Wu and Zongmin Ma},
  doi          = {10.1142/S0218194025500147},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {525-545},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Approximate query for industrial fault knowledge graph based on vector index},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experimental examination of the effect of programming languages and frameworks on mobile application development processes. <em>IJSEKE</em>, <em>35</em>(4), 503-523. (<a href='https://doi.org/10.1142/S0218194025500135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile technologies have become an integral part of our daily lives, making it increasingly important for developers to create mobile applications quickly and efficiently. There are many programming languages and frameworks available for mobile application development. This study discusses frameworks like Flutter and React Native, as well as the languages Kotlin and Java used in native application development. It also investigates which application development technique provides better services for developers. As part of the research, similar applications were developed in each language and subjected to various performance tests. These tests measured parameters such as central processing unit (CPU) usage, Random Access Memory (RAM) consumption, code length, and application size. In the study, three applications with different levels of complexity were developed separately using Flutter, React Native, and Kotlin. The applications included a to-do list with 250, 500, and 1000 items, a simple calculator, and a tic-tac-toe game. Performance tests were then conducted. The results showed that Kotlin stands out as the most efficient platform, with low CPU and memory usage, small file size, and high performance. Flutter is notable for its balanced resource usage and cross-platform support. Although React Native offers a fast development process, it should be carefully considered for large projects where performance is crucial due to its high resource usage. The suitability of these three mobile application development techniques for a given project can be determined using the results obtained from this study.},
  archive      = {J_IJSEKE},
  author       = {Durmuş Özkan Şahin and Zeynep Altun and Öykü Kaya and Batuhan Semiz},
  doi          = {10.1142/S0218194025500135},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {503-523},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Experimental examination of the effect of programming languages and frameworks on mobile application development processes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A static analysis framework for investigating tainted data sources in software systems. <em>IJSEKE</em>, <em>35</em>(4), 469-501. (<a href='https://doi.org/10.1142/S0218194025500123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most effective methods for detecting software security vulnerabilities is taint analysis. Some software defects originate from certain external input data. Analyzing the taint sources and the data flow propagation from these sources to defect points through static analysis can help us understand the causes of software defects and reduce the difficulty of debugging them. This paper combines intraprocedural and interprocedural analysis methods to obtain global taint source information. A novel propagation path calculation algorithm is proposed, incorporating predecessor node computation and alias analysis, effectively reducing the negative impact of irrelevant code on the performance of taint analysis. This method not only helps detect errors that lead to vulnerabilities but also analyzes the impact of vulnerable input data on the system. Based on the global taint source analysis algorithm, we developed a static taint source analysis prototype tool for C programs, called AWsTS. Experiments conducted on five open-source projects show that AWsTS improves the accuracy of analysis results without increasing the required analysis time. The average precision for intra-procedural taint source analysis is 93.4%, and the average recall is 90.2%. Similarly, for interprocedural taint source analysis, the average precision is 87.6%, and the average recall is 84.9%. Additionally, AWsTS can output taint propagation paths, providing valuable support for further taint analysis.},
  archive      = {J_IJSEKE},
  author       = {Peng Dai and Xiaoqin Ma and Zebo Peng and Chen Zhao and Qianjin Zhang},
  doi          = {10.1142/S0218194025500123},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {469-501},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A static analysis framework for investigating tainted data sources in software systems},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying epistemic network analysis to open source software projects. <em>IJSEKE</em>, <em>35</em>(4), 441-467. (<a href='https://doi.org/10.1142/S0218194025500093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive emergence of Open Source Software (OSS) projects led to the rapid increment of data available to software engineers and those who would like to contribute to an OSS project. There is an arising need for a way to analyze and draw conclusions about the value of an OSS project, from an epistemic point of view. OSS projects have been used for a long time as vehicles for learning programming and software engineering. However, there is a need for more evidence about the scientific value conveyed by OSS projects. We posit that if we deeply study the dialogues of software engineers involved in the communities of the various projects and draw conclusions from them, this can be an additional way of assessing OSS projects. Learning analytics, data mining and data science provide diverse statistical and computational approaches to analyzing data. In this work, we describe Epistemic Network Analysis (ENA), a network analysis technique used by a growing community of researchers to support thick descriptions based on large volumes of data. ENA is based on the theory of epistemic frames. The theory of epistemic frames models the ways of being, thinking and acting inside some community of practice. In this research, we focus on the epistemic frame of software engineering and discuss the elements of this frame: the knowledge, the set of skills and values and the set of processes for making and justifying decisions. We coded the discourses of two well-known open source projects, LibreOffice and OpenOffice, and we used the online tool ENA WebKit to analyze the coded rows of the discourses, but also to visualize and compare the networks of different units of data. We conducted three types of experiments and comparisons on the mean networks of the two projects, on the networks of the different bugs, as well as on the networks of some software engineers who participated in the discussions, and we came to conclusions about how epistemic these dialogues were and therefore assessing their educational value. This is an introductory work utilizing the ENA method for OSS discourse analysis. We believe that the rational presented in this paper can lead to a set of processes on evaluating OSS projects based on their epistemology and at the same time, become an educational tool for software engineering courses and research groups.},
  archive      = {J_IJSEKE},
  author       = {Apostolos Kritikos and Konstantina Papadopoulou and Ioannis Stamelos},
  doi          = {10.1142/S0218194025500093},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {441-467},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Applying epistemic network analysis to open source software projects},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable instance matching using lucene and mongodb. <em>IJSEKE</em>, <em>35</em>(3), 421-440. (<a href='https://doi.org/10.1142/S0218194025500111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of the semantic web and Linked Open Data (LOD) cloud has led to the creation and integration of various knowledge bases defined by ontologies. A significant challenge within the LOD paradigm is identifying resources that refer to the same real-world object to enable large-scale data integration and sharing. In this context, instance matching has emerged as a key solution, linking co-referent instances from heterogeneous data sources using owl:sameAs links. Traditional approaches focus on schema-level matching but often fail to address property-level heterogeneity. Moreover, given the large scale of instances, examining all possible instance pairs is impractical. This paper proposes a scalable and efficient instance-matching approach using MongoDb (Humongous database) and Lucene. MongoDb stores instances at any scale and Lucene uses inverted indexes to identify matching candidates. Experiments on the instance matching track from the Ontology Alignment Evaluation Initiative (OAEI’2022) show that our approach matches the F -measure score of RE-Miner, the top performer in OAEI’2020, while surpassing all other participants in OAEI’2020, 2021 and 2022. Additionally, it operates 17 times faster than RE-Miner, four times faster than Lily and 15 times faster than LogMap, the fastest in OAEI’2020, 2021 and 2022, respectively. Moreover, we evaluate our approach on other knowledge bases from OAEI’2010. Once again, our approach gets highly competitive resuts compared to state-of-the-art approaches.},
  archive      = {J_IJSEKE},
  author       = {Siham Amrouch and Ryma Guefrouchi and Nawel Zemmal and Sadok Ben Yahia},
  doi          = {10.1142/S0218194025500111},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {421-440},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Scalable instance matching using lucene and mongodb},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OscSe: A practical security assessment model for general open source components. <em>IJSEKE</em>, <em>35</em>(3), 397-419. (<a href='https://doi.org/10.1142/S021819402550010X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open source components (OSCs) have become a vital part for developing modern applications. The security of these components could affect the overall security of the software depends on them. Thus, the security of an OSC should be evaluated first before integrating to the software. However, the existing models lack generality, and cannot be easily automatic applied to OSCs developed in different programming language. To this end, we propose a security assessment model for OSCs, called the CRAM, which features generality and automation. The proposed model is constructed under the hypothesis that OSC with a larger and more active community is more likely to disclose more vulnerabilities. And it evaluates the security of OSC from its performance in size as well as activities of open source community and vulnerability disclosures. In the experiment section, we present validation and application experiments. In the validation experiment, we find that the basic hypothesis of the proposed model is valid, and there is a positive correlation between the community size as well as activities and vulnerability risk of OSCs. In the application experiment, we further evaluate our approach with large-scale open source components. Our hypothesis is further validated. The most of OSCs in the ecosystem are in line with the hypothesis. Finally, we successfully build the security baseline according to the hypothesis, and 5 vulnerable OSCs classified as vulnerable by our model are analyzed. The result proves the effectiveness of our model to identify a vulnerable open source ecosystem around the ecosystem.},
  archive      = {J_IJSEKE},
  author       = {Ziyan Wang and Cheng Huang and Yang You},
  doi          = {10.1142/S021819402550010X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {397-419},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {OscSe: A practical security assessment model for general open source components},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A change-level defect prediction approach based on Teacher–Student network. <em>IJSEKE</em>, <em>35</em>(3), 375-396. (<a href='https://doi.org/10.1142/S0218194025500081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-level defect prediction, also known as just-in-time (JIT) defect prediction, concentrates on predicting if a specific commit is likely to introduce defects. It effectively alleviates the limitations of traditional file-level defect prediction techniques, such as coarse-grained, hard to trace and poor timeliness. Currently, most change-level defect prediction techniques construct defect prediction models by using either expert features or semantic features. Recent studies have shown that the defect prediction performance can be enhanced by integrating these two types of features. However, obtaining expert features is not an easy task, due to missing historical data in real projects. To address the aforementioned problem, this paper proposes TS-SDP ( T eacher– S tudent based S oftware D efect P rediction) based on teacher–student network. First, the source code is analyzed to extract expert features and semantic features. Then, a teacher–student network framework is constructed. In this framework, both features are used as inputs to the teacher network and only semantic features are used as inputs to the student network. The student network is enabled to learn about the expert features from the teacher network through the loss function. Finally, the student network is used to differentiate commits that are defect-inducing and those that are not, in the presence of only semantic features. The results of the experiments carried out on a dataset containing 21 different projects show that, when only semantic features are available, the cross-network knowledge dissemination between the teacher and student network makes it possible to predict defects. When compared to the state-of-the-art change-level defect prediction method, JIT-Fine, TS-SDP is 0.130, 0.114, 0.123 and 0.016 greater in P r e c i s i o n , R e c a l l , F - measure and A c c u r a c y , respectively.},
  archive      = {J_IJSEKE},
  author       = {Xinhong Duan and Xiguo Gu and Jiale Zhang and Zhanqi Cui},
  doi          = {10.1142/S0218194025500081},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {375-396},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A change-level defect prediction approach based on Teacher–Student network},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting interaction related bugs in a multi-tenant setting using kubernetes. <em>IJSEKE</em>, <em>35</em>(3), 351-374. (<a href='https://doi.org/10.1142/S021819402550007X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tenant architectures allow multiple users to run similar applications on shared infrastructure, reducing the cost of resources. Developers of such applications must ensure tenant isolation to prevent users from interfering with one another. Concurrency bugs may arise only when a specific set of users is logged in at the same time. This paper introduces a framework for testing user interactions with a server, aimed at detecting timing-related bugs. The framework uses a scheduler to control when users log in and out of the system. If a particular set of users is logged in simultaneously, the server will simulate a potential bug. Users also keep a copy of the server’s data, and if the server’s data differs from their copy, a bug has occurred, which the user will report. The framework is designed to be flexible, supporting the addition of custom schedulers that can use either static scheduling algorithms or advanced Artificial Intelligence techniques to speed up bug detection more efficiently than traditional fuzz testing methods.},
  archive      = {J_IJSEKE},
  author       = {Bogdan Ghimis},
  doi          = {10.1142/S021819402550007X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {351-374},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Detecting interaction related bugs in a multi-tenant setting using kubernetes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RESEARCH NOTES — GMRepair: Graph mining template-based automated software repair. <em>IJSEKE</em>, <em>35</em>(3), 327-349. (<a href='https://doi.org/10.1142/S0218194025500068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing scale and complexity of software recently, automated software bug repair has grown in importance. However, the current automated software bug repair process suffers from issues such as coarse-grained repair granularity and poor patch quality. To address these problems, we propose a graph mining template-based automatic software repair (GMRepair) to improve the performance of automated software bug repair. First, this approach adopts the Ochiai fault localization technique to locate and generate a list of suspicious defect statements. We utilize the GumTree tool to parse the bug and repair program files, generating edit scripts. These edit scripts are then transformed into a graphical representation. Second, we utilize a frequent graph miner to obtain graph mining templates by matching the context of the suspicious statements with the context of the graph mining templates, generating an initial population for them. The buggy program is evolved using genetic programming through mutation and crossover operations, generating new individuals. Finally, we sequentially pass the candidate patches (CPs) through corresponding test cases and prioritize the test cases using priority sorting techniques. Patches that fail to pass the test cases are filtered out, and the patches that pass the test cases are output. We conducted the experiments using two datasets, QuixBugs and Defects4J. In Defects4J, the GMRepair successfully repaired 41 defects, while in QuixBugs, it successfully repaired 15 defects. Compared to the existing methods, GMRepair offers a higher success rate and efficiency in defect repair.},
  archive      = {J_IJSEKE},
  author       = {Heling Cao and Yanlong Guo and Yun Wang and Fangchao Tian and Zhaolong Wang and Yonghe Chu and Miaolei Deng and Panpan Wang and Zhenghao He and Shuting Wei},
  doi          = {10.1142/S0218194025500068},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {327-349},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {RESEARCH NOTES — GMRepair: Graph mining template-based automated software repair},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPERT: Reinforcement learning-enhanced transformer model for agile story point estimation. <em>IJSEKE</em>, <em>35</em>(3), 293-325. (<a href='https://doi.org/10.1142/S0218194025500044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Story point estimation is a key practice in Agile project management that assigns effort values to user stories, helping teams manage workloads effectively. Inaccurate story point estimation can lead to project delays, resource misallocation and budget overruns. This study introduces Story Point Estimation using Reinforced Transformers (SPERT), a novel model that integrates transformer-based embeddings with reinforcement learning (RL) to improve the accuracy of story point estimation. SPERT utilizes Bidirectional Encoder Representations from Transformers (BERT) embeddings, which capture the deep semantic relationships within user stories, while the RL component refines predictions dynamically based on project feedback. We evaluate SPERT across multiple Agile projects and benchmark its performance against state-of-the-art models, including SBERT-XG, LHC-SE, Deep-SE and TF-IDF-SE. Results demonstrate that SPERT outperforms these models in terms of Mean Absolute Error (MAE), Median Absolute Error (MdAE) and Standardized Accuracy (SA). Statistical analysis using Wilcoxon tests and A12 effect size confirms the significance of SPERT’s performance, highlighting its ability to generalize across diverse projects and improve estimation accuracy in Agile environments.},
  archive      = {J_IJSEKE},
  author       = {Waleed Younas and Rui Chen and Jing Zhao and Tahreem Iqbal and Mohamed Sharaf and Azhar Imran},
  doi          = {10.1142/S0218194025500044},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {293-325},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {SPERT: Reinforcement learning-enhanced transformer model for agile story point estimation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCIA: Hierarchical change impact analysis based on hierarchy program slices. <em>IJSEKE</em>, <em>35</em>(2), 263-292. (<a href='https://doi.org/10.1142/S0218194025500056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change impact analysis (CIA) is an essential method in software maintenance and evolution. Its accuracy and usability play a crucial role in its application. However, most CIAs are coarse-grained and limited to class and method levels. Despite the fine-grained CIAs’ success in giving the statement-level impact set, they are still limited without the sub-statement level dependency analysis, leading to low precision. Additionally, their unstructured impact sets make it challenging for users to comprehend the impact content. This paper proposes Hierarchical Change Impact Analysis (HCIA), a Hierarchical CIA technique based on the sub-statement level dependence graph. HCIA can perform a forward hierarchy program slicing on the change set from five levels: sub-statement, statement, method, class, and package. Based on the program slices, HCIA calculates the impact factor of the impact sets at the five levels to generate the final impact set. In the experiment, we evaluate the relationship between the impact factor and the actual affected codes and assess the most appropriate size of HCIA impact sets. Furthermore, we evaluate HCIA on 10 open-source projects by comparing our approach with popular CIAs at the five levels. The experimental result shows that HCIA is more accurate than the popular CIAs.},
  archive      = {J_IJSEKE},
  author       = {Jianming Chang and Lulu Wang and Zaixing Zhang},
  doi          = {10.1142/S0218194025500056},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {263-292},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {HCIA: Hierarchical change impact analysis based on hierarchy program slices},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-based evaluation metric for question answering systems. <em>IJSEKE</em>, <em>35</em>(2), 243-262. (<a href='https://doi.org/10.1142/S0218194025500032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper addresses the limitations of traditional evaluation metrics for Question Answering (QA) systems that primarily focus on syntax and n-gram similarity. We propose a novel model-based evaluation metric, MQA-metric, and create a human-judgment-based dataset, squad-qametric and marco-qametric, to validate our approach. The research aims to solve several key problems: the objectivity in dataset labeling, the effectiveness of metrics when there is no syntax similarity, the impact of answer length on metric performance, and the influence of real answer quality on metric results. To tackle these challenges, we designed an interface for dataset labeling and conducted extensive experiments with human reviewers. Our analysis shows that the MQA-metric outperforms traditional metrics like BLEU, ROUGE and METEOR. Unlike existing metrics, MQA-metric leverages semantic comprehension through large language models (LLMs), enabling it to capture contextual nuances and synonymous expressions more effectively. This approach sets a standard for evaluating QA systems by prioritizing semantic accuracy over surface-level similarities. The proposed metric correlates better with human judgment, making it a more reliable tool for evaluating QA systems. Our contributions include the development of a robust evaluation workflow, creation of high-quality datasets, and an extensive comparison with existing evaluation methods. The results indicate that our model-based approach provides a significant improvement in assessing the quality of QA systems, which is crucial for their practical application and trustworthiness.},
  archive      = {J_IJSEKE},
  author       = {Dilan Bakır and Mehmet S. Aktas and Beytullah Yıldız},
  doi          = {10.1142/S0218194025500032},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {243-262},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A model-based evaluation metric for question answering systems},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method to evaluate the credibility of domain knowledge network using validated expert knowledge. <em>IJSEKE</em>, <em>35</em>(2), 217-241. (<a href='https://doi.org/10.1142/S0218194025500020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are living in an era of knowledge explosion, where all kinds of knowledge are emerging and becoming more and more complicated with the development of new techniques and new ideas. When we study knowledge and apply them to understand and solve problems, the credibility of knowledge is becoming our main concerns. Usually, high credible domain knowledge can guide us correctly understand all concepts and the relationships between them in this domain. Due to its good layer structure and scalability, domain knowledge network is widely used to represent knowledge in knowledge engineering, artificial intelligence and others in recent years. How to ensure the credibility of domain knowledge network? This is an important and interesting topic. In this paper, we propose a method to evaluate the knowledge credibility for domain knowledge network, which means that we can start from the layer structure of domain knowledge network, and evaluate the credibility of knowledge layer by layer using validated expert knowledge such as domain dictionary, domain ontology and domain expert experience. We conduct experiments with six domain knowledge network constructed based on network data and six domain knowledge network constructed manually based on published books or domain dictionaries, which describe the same domain knowledge in pairs. Experimental results show that the knowledge credibility of domain knowledge network constructed from validated expert knowledge is significantly higher than the knowledge credibility of domain knowledge network constructed directly from network data, which satisfy our expectation and also prove the effectiveness of our credibility evaluation method.},
  archive      = {J_IJSEKE},
  author       = {Yin Li and Ying Zhou and Li Liao and Bixin Li},
  doi          = {10.1142/S0218194025500020},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {217-241},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A method to evaluate the credibility of domain knowledge network using validated expert knowledge},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RESEARCH NOTES: Design of a distributed and highly scalable fog architecture for heterogeneous IoT infrastructures. <em>IJSEKE</em>, <em>35</em>(2), 195-215. (<a href='https://doi.org/10.1142/S0218194025430016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing can provide an effective solution to the challenges presented by today’s ever-emerging Internet of Things (IoT) infrastructures. As the number of interconnected devices progressively increases, these infrastructures require better solutions to ensure high scalability and processing capacity, along with an efficient use of available resources. This is why this paper presents a distributed Fog architecture, specifically designed to address the challenges and difficulties presented by heterogeneous IoT environments. This Fog architecture is used as an intermediate layer between the IoT devices and the final layer, it has been designed after the previous analysis of the requirements to be met for the solution, then the modularization of the architecture has been carried out so that it can be easily distributed, and finally, an implementation has been generated on a real environment as a validation case of the proposal.},
  archive      = {J_IJSEKE},
  author       = {Lucía Arnau Muñoz and José Vicente Berná Martínez and Carlos Calatayud Asensi and David Saavedra Pastor},
  doi          = {10.1142/S0218194025430016},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {195-215},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {RESEARCH NOTES: Design of a distributed and highly scalable fog architecture for heterogeneous IoT infrastructures},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward a pattern-based comprehensive framework using process mining for RBAC conformance checks. <em>IJSEKE</em>, <em>35</em>(2), 157-194. (<a href='https://doi.org/10.1142/S0218194025500019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event logs often record the execution of business process instances. Detecting traces in the event logs that do not comply with access control policies, such as role-based access control (RBAC) policies, is essential to ensuring system security. Moreover, process mining has been extensively utilized for security analysis in recent years. However, pattern-based approaches for designing and analyzing RBAC policies in the context of business processes through process mining are notably absent. In this paper, we present a systematic framework for checking the conformance of RBAC implemented in the event logs of business processes with the RBAC policies specified in domain knowledge. To facilitate the representation of the RBAC policies derived from the domain knowledge, we employ an RBAC domain-specific language (DSL) combined with our RBAC-driven object constraint language (OCL) invariant patterns built from the various types of RBAC constraints. The implemented RBAC in an event log is represented as snapshots within our framework. Then, we validate the snapshots with the RBAC policies to be able to detect RBAC conformance issues. The proposed framework is experimented with and evaluated on two business process logs, one simulated log and one real-world event log named “BPI Challenge 2017”.},
  archive      = {J_IJSEKE},
  author       = {Duc-Hieu Nguyen and Yuichi Sei and Yasuyuki Tahara and Akihiko Ohsuga},
  doi          = {10.1142/S0218194025500019},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {157-194},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Toward a pattern-based comprehensive framework using process mining for RBAC conformance checks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software quality assessment model: A new approach for software testing tools. <em>IJSEKE</em>, <em>35</em>(2), 139-155. (<a href='https://doi.org/10.1142/S0218194024500517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of software testing is a crucial phase in determining the quality of software, and this phase requires significant costs and a considerable amount of time for testers. This paper discusses the development of a framework for software quality assessment, involving flexible choices of software testing methods and variables in the form of an application. The method used is experimental, developing a new framework based on previous research, where previous research was limited to specific methods and testing variables. The result of this research is the creation of a new framework for software quality assessment. It is hoped that this framework can serve as a reference for software companies in evaluating software quality. In terms of complexity, this framework has the advantage of allowing a tester to choose methods with more flexible or unlimited testing variables. Regarding the estimated time and costs, with PF = 4 , 5 and 1 0 , the practical application complexity of the developed framework is estimated to have the best costs, time and human resources at IDR 254,240,000, with an estimated time of 3,178 work hours and 6,356 work hours with a team of 3 people.},
  archive      = {J_IJSEKE},
  author       = {Zulkifli Zulkifli and Mardiana Mardiana and Dikpride Despa},
  doi          = {10.1142/S0218194024500517},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {139-155},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software quality assessment model: A new approach for software testing tools},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining fine-grained code change patterns using multiple feature analysis. <em>IJSEKE</em>, <em>35</em>(1), 111-138. (<a href='https://doi.org/10.1142/S0218194024500505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining high code quality is a crucial concern in software development. Existing studies demonstrated that developers frequently face recurrent bugs and adopt similar fix measures, known as code change patterns. As an essential static analysis technique, code pattern mining supports various tasks, including code refactoring, automated program repair, and defect prediction, thus significantly improving software development processes. A prevalent approach to identifying code patterns involves translating code changes to edit actions into a Bag-of-Words (BoW) model. However, when applied to open-source projects, this method exhibits several limitations. For instance, it overlooks function call information and disregards feature word order. This study introduces MIFA, a novel technique for mining code change patterns using multiple feature analysis. MIFA extends existing BoW methods by incorporating analysis of function calls and overall changes in the Abstract Syntax Tree (AST) structure. We selected 20 popular Python projects and evaluated MIFA in both intra-project and cross-project scenarios. The experimental results indicate that: (1) MIFA achieved higher silhouette coefficients and F1 scores compared to other state-of-the-art methods, demonstrating a superior accuracy; (2) MIFA can assist developers in detecting unique change patterns more earlier, with an efficiency improvement of over 40% compared to random sampling. Additionally, we discussed critical parameters for measuring the similarity of code changes, guiding users to apply our method effectively.},
  archive      = {J_IJSEKE},
  author       = {Di Liu and Yang Feng},
  doi          = {10.1142/S0218194024500505},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {111-138},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Mining fine-grained code change patterns using multiple feature analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Code recommendation for schema evolution of mimic storage systems. <em>IJSEKE</em>, <em>35</em>(1), 89-110. (<a href='https://doi.org/10.1142/S0218194024500499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schema evolution of mimic storage systems is a time-consuming and error-prone task due to the redundant development of heterogeneous executors. The ORM-based proxy requires an entire class to represent the structure of a data table. There lacks domain-specific code recommendation techniques to boost storage development. To address this issue, we design a novel type of code context, i.e. schema context, that combines features of code text, syntax and structure. Regarding the requirements of class-level granularity, we focus on behavior and attribute in code syntax, and use element position and structural metrics to mine the hidden relationships. Based on schema context and an existing inference mode, we propose SchemaRec to recommend ORM-related class for the database executors once one of them has been changed. We conduct experiments with 110 open-source projects, and the results show that SchemaRec obtains more accurate results than Lucene, DeepCS, QobCS and SEA in terms of Top-1, Top-10 and MRR accuracy due to the better ability of context representation. We also find that code syntax is the most important information because it involves behavior and attribute information of ORM-related classes.},
  archive      = {J_IJSEKE},
  author       = {Xianglong Kong and Zhuo Lv and Cen Chen and Hao Chang and Nuannuan Li and Fan Zhang},
  doi          = {10.1142/S0218194024500499},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {89-110},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Code recommendation for schema evolution of mimic storage systems},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The trustworthiness metric model of interface based on defects. <em>IJSEKE</em>, <em>35</em>(1), 59-88. (<a href='https://doi.org/10.1142/S0218194024500487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interface is a crucial element in component-based software, enabling the linkage of distinct components to facilitate interaction. Defects within the interface can significantly impact the overall trustworthiness of the system. Therefore, it is essential to assess the interface trustworthiness based on a defect-centric approach. This paper introduces a novel model for evaluating interface trustworthiness, anchored in defect analysis. First, the defect types are formalized based on interface specifications. Then, the comprehensive weight allocation method is established to characterize the importance degree of each interface defect type by combining the G1 and CRITIC methods. Subsequently, the attributes of the interface are evaluated by defect value analysis, and the trustworthiness measurement model of the interface is proposed based on these attributes. Furthermore, to evaluate the trustworthiness of the whole system, the trustworthiness measure models under different combination structure of components are established. Finally, the model’s’ applicability is demonstrated through an illustrative example. This trustworthiness evaluation from the interface view can guide interface designers to obtain high-quality interfaces and improve the trustworthiness of the entire software.},
  archive      = {J_IJSEKE},
  author       = {Yanfang Ma and Xiaotong Gao},
  doi          = {10.1142/S0218194024500487},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {59-88},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The trustworthiness metric model of interface based on defects},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing translation validation of compiler transformations with large language models. <em>IJSEKE</em>, <em>35</em>(1), 45-57. (<a href='https://doi.org/10.1142/S0218194024500475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a framework that integrates Large Language Models (LLMs) into translation validation, targeting LLVM compiler transformations where formal verification tools fall short. Our framework utilizes the existing tools, like Alive2, to perform initial validation. For transformations deemed unsolvable by traditional methods, our approach leverages fine-tuned LLMs to predict soundness or unsoundness, with subsequent fuzzing applied to identify counterexamples for unsound transformations. Our approach has proven effective in complex scenarios, such as deep-learning accelerator designs, enhancing the reliability of compiler transformations.},
  archive      = {J_IJSEKE},
  author       = {Yanzhao Wang and Fei Xie},
  doi          = {10.1142/S0218194024500475},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {45-57},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enhancing translation validation of compiler transformations with large language models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of fault localization on novice programs and addressing the tie problem. <em>IJSEKE</em>, <em>35</em>(1), 19-44. (<a href='https://doi.org/10.1142/S0218194024500426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming education is becoming increasingly popular in universities. However, due to a lack of debugging experience, novices often encounter numerous difficulties in the programming process. Automatic fault localization techniques have emerged as a promising solution to address this issue. Among these techniques, Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) have been widely used in industrial programs. However, there is a significant difference between industrial and novice programs and the performance of these methods on novice programs has not been extensively studied. To fill this gap, we conducted an empirical study to evaluate the fault localization performance and execution overhead of SBFL and MBFL in a typical novice programming environment. Our study specifically examined how different program characteristics, including code coverage and mutation score, affect the accuracy of these localization methods. Additionally, during the study, we identified the tie problem in both methods and further investigated its impact on fault localization techniques in novice programs. To remove the impact of the tie problem, we proposed using PageRank scores as weights for the suspiciousness, sorting, and locating faults based on the weighted suspiciousness. The PageRank algorithm is based on statement coverage information and constructs a directed graph. From the directed graph, a transition matrix generates the weight scores (PageRank scores) for each statement. Our research demonstrates that both SBFL and MBFL are effective for fault localization in novice programs, with MBFL showing significantly better performance in our tests. In TOP- N ( N = 1 , 3 , 5 ) , MBFL accurately locates 67, 96 and 114 faults, respectively, indicating superior performance. Additionally, calculating weighted suspiciousness significantly alleviates the tie problem.},
  archive      = {J_IJSEKE},
  author       = {Yuxing Liu and Jiaxin Zhong and Qihua Hei and Xuchuan Zhou and Jingzhong Xiao},
  doi          = {10.1142/S0218194024500426},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {19-44},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An empirical study of fault localization on novice programs and addressing the tie problem},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using peer assessment leveraging large language models in software engineering education. <em>IJSEKE</em>, <em>35</em>(1), 1-18. (<a href='https://doi.org/10.1142/S0218194024500359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the integration of generative AI and large language models into the realm of software engineering education and training, with a specific focus on the transformation of traditional peer assessment methodologies. The motivation stems from the growing demand for innovative educational techniques that can effectively engage and empower learners in mastering Software Engineering principles. The proposed approach involves presenting students with modeling exercises solved by ChatGPT, prompting them to critically evaluate and provide constructive feedback on the generated solutions. By engaging students in a dialogue with the AI model, we aim to foster a dynamic learning environment where learners can articulate their considerations and insights, thereby enhancing their comprehension of software engineering principles, critical thinking and self evaluation skills. Preliminary results from pilot implementations indicate promising outcomes, suggesting that this approach not only enhances the quality of peer feedback but also contributes to a more interactive and engaging educational experience.},
  archive      = {J_IJSEKE},
  author       = {Marco Fiore and Marina Mongiello},
  doi          = {10.1142/S0218194024500359},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Using peer assessment leveraging large language models in software engineering education},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
