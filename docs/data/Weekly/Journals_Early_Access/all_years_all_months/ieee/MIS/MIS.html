<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MIS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mis">MIS - 26</h2>
<ul>
<li><details>
<summary>
(2025). A survey on continuous unlearning in generative AI: Approaches and trade-offs. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3616192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Artificial Intelligence (GenAI) models have innovated content creation but raise concerns about privacy, security, and regulatory compliance such as GDPR. In response, unlearning techniques have emerged to selectively remove data while preserving the utility of the model. This paper reviews unlearning methods in centralized and decentralized settings. These strategies mitigate risks such as data leakage, membership inference, and bias amplification. By integrating unlearning with continuous or lifelong learning paradigms, GenAI models can adapt dynamically while honoring the ‘right to be forgotten.’ In existing unlearning methods, we explore key trade-offs involving computational overhead, accuracy retention, generative quality, and thorough data deletion. Our review covers technical and ethical considerations and future directions, highlighting a balanced path toward responsible GenAI systems.},
  archive      = {J_MIS},
  author       = {Yang Zhao and Hongyang Du and Yijing Lin and Keyi Xiang and Dusit Niyato and H. Vincent Poor},
  doi          = {10.1109/MIS.2025.3616192},
  journal      = {IEEE Intelligent Systems},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {A survey on continuous unlearning in generative AI: Approaches and trade-offs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From observation to action: A novel IoT architecture for climate data integrity. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3615299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-evolving field of climate data management, the pressing need for accurate, real-time data to mitigate the effects of severe weather phenomena has never been more critical. This study presents the development of an advanced Internet-of-Things (IoT) sensor architecture aimed at enhancing climate data management; a real-life industrial scenario in the Dominican Republic was targeted, specifically, a region frequently impacted by severe atmospheric phenomena. Specifically, our solution integrates (a) machine learning for data validation and (b) blockchain-oriented computing for ensuring data integrity; the resulting system is designed to provide reliable, real-time climate data crucial for disaster risk reduction, emergency planning, and climate change policies. Our field evaluation suggests that the potential to transform climate data management practices in our scenario is significant, offering a model that could be replicated in similar contexts globally.},
  archive      = {J_MIS},
  author       = {Stefano Radocchia and Fabiano Izzo and Damian Andrew Tamburri},
  doi          = {10.1109/MIS.2025.3615299},
  journal      = {IEEE Intelligent Systems},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {From observation to action: A novel IoT architecture for climate data integrity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aura: An automated system for the real-time evaluation of flight maneuver performance. <em>MIS</em>, 1-11. (<a href='https://doi.org/10.1109/MIS.2025.3607236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated evaluation of flight maneuver performance in civilian and military aircraft enhances pilot proficiency and aircraft safety. In this article, we present Aura, an inaircraft flight maneuver training system that provides real-time performance feedback to pilots. Aura consists of sequential flight data capture, flight data analysis, and flight data visualization modules. The flight data capture module uses a pipeline of computer vision techniques and object detection algorithms to collect flight data optically. The flight data analysis module applies a transformer-based classifier network, trained on a custom dataset, to identify flight maneuvers in real-time. The flight data visualization module displays processed data to pilots on a hardware display in a task-specific layout. Aura does not need a physical interface with aircraft avionics, thereby circumventing data access issues in military aircraft. Aura is ground tested using UH-60M Black Hawk helicopter flight simulator recordings and is flight tested for several hours in a Cessna 172S G1000 to validate effectiveness.},
  archive      = {J_MIS},
  author       = {Mahdi Al-Husseini and Joshua Barnett and Joseph Divyan Thomas and Tony G. Chen},
  doi          = {10.1109/MIS.2025.3607236},
  journal      = {IEEE Intelligent Systems},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Aura: An automated system for the real-time evaluation of flight maneuver performance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting AI for attacks: On the interplay between adversarial AI and offensive AI. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3610364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Artificial Intelligence (AI) continues to evolve, it has transitioned from a research-focused discipline to a widely adopted technology, enabling intelligent solutions across various sectors. In security, AI’s role in strengthening organizational resilience has been studied for over two decades. While much attention has focused on AI’s constructive applications, the increasing maturity and integration of AI have also exposed its darker potentials. This article explores two emerging AI-related threats and the interplay between them: AI as a target of attacks (‘Adversarial AI’) and AI as a means to launch attacks on any target (‘Offensive AI’) – potentially even on another AI. By cutting through the confusion and explaining these threats in plain terms, we introduce the complex and often misunderstood interplay between Adversarial AI and Offensive AI, offering a clear and accessible introduction to the challenges posed by these threats.},
  archive      = {J_MIS},
  author       = {Saskia Laura Schröer and Luca Pajola and Alberto Castagnaro and Giovanni Apruzzese and Mauro Conti},
  doi          = {10.1109/MIS.2025.3610364},
  journal      = {IEEE Intelligent Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Exploiting AI for attacks: On the interplay between adversarial AI and offensive AI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable sentiment analysis with DeepSeek-r1: Performance, efficiency, and few-shot learning. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3614967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1—an open-source reasoning model—against OpenAI’s GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative.},
  archive      = {J_MIS},
  author       = {Donghao Huang and Zhaoxia Wang},
  doi          = {10.1109/MIS.2025.3614967},
  journal      = {IEEE Intelligent Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Explainable sentiment analysis with DeepSeek-r1: Performance, efficiency, and few-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based women hate speech detection system using YouTube URLs for effective content moderation. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3594849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Countering online hate speech is essential for creating a safer digital space where positive interactions can thrive. As central hubs of global communication, platforms like YouTube require effective moderation through explainable and affective computing approaches. This study introduces a novel AI-driven system for detecting women-centric hate speech. We collected 11,245 YouTube video URLs using specific keywords, then extracted audio to create Urdu transcripts and transliterated them into Roman Urdu, resulting in two distinct datasets. Various feature sets were explored using classic machine learning and deep learning algorithms. Results showed that classical models achieved 0.90 accuracy on the Urdu dataset, while deep learning models reached 0.96 accuracy on Roman Urdu. The corpus is publicly available to promote transparency and further research. Comparative evaluations against existing English hate speech datasets demonstrate the effectiveness of the proposed approach. This work lays the foundation for more ethical and transparent content moderation systems.},
  archive      = {J_MIS},
  author       = {Zohaib Ahmad Khan and Yuanqing Xia and Fiza Khaliq and Weiwei Jiang and Muhammad Shahid Anwar},
  doi          = {10.1109/MIS.2025.3594849},
  journal      = {IEEE Intelligent Systems},
  month        = {8},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {AI-based women hate speech detection system using YouTube URLs for effective content moderation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging ChatGPT-based augmentation and contrastive learning for chinese MOOC sentiment analysis. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3596178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the unique challenges of sentiment analysis in Chinese massive open online course (MOOC) reviews, where pedagogically embedded language, intra-sentence sentiment shifts, and class imbalance complicate classification tasks. To tackle these domain-specific issues, we integrated ChatGPT-based data augmentation with contrastive learning within a BERT-Chinese framework. We evaluated ChatGPT-based augmentation (GPTaug), similar word replacement, and random word deletion under a dual-loss setup combining supervised cross-entropy and InfoNCE contrastive learning, focusing on how they enhance model performance across sentiment categories. Results revealed that the integration of contrastive learning with data augmentation strategies substantially improved sentiment classification in Chinese MOOC reviews. Especially, GPTaug demonstrated robust and balanced performance across polarity categories, particularly enhancing the detection of underrepresented neutral sentiments. These findings suggest that generative augmentation, when aligned with contrastive objectives, mitigates data sparsity and semantic ambiguity in educational sentiment analysis, providing actionable insights for optimizing sentiment analysis in low-resource, domain-specific contexts.},
  archive      = {J_MIS},
  author       = {Xieling Chen and Haoran Xie and S. Joe Qin and Lingling Xu and Xiaohui Tao and Fu Lee Wang},
  doi          = {10.1109/MIS.2025.3596178},
  journal      = {IEEE Intelligent Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Leveraging ChatGPT-based augmentation and contrastive learning for chinese MOOC sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-speed dorsal hand vein recognition using siamese lightweight neural network. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3600417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dorsal hand vein recognition possesses unique advantages such as liveness detection and high stability in identity recognition. However, high computational demands and the need to retrain the model as the number of registered users increases, make it challenging to fully apply current deep learning research results to most edge devices. To address this limitation, MSGNet (Modified Siamese GhostNet) in this paper utilizes siamese GhostNet backbones to reduce computational demands and enhance adaptability to dynamic datasets. Additionally, optimizations such as the introduction of a multi-scale convolution module improve accuracy and robustness. A dataset comprising 118 subjects was constructed for this paper. Experimental results showed that the model achieved an average matching time of 25.61 ms, a recognition rate of 98.82%, and an equal error rate of 1.60%. These results outperforms existing lightweight dorsal hand vein recognition algorithms, establishing MSGNet as a state-of-the-art solution for practical deployment.},
  archive      = {J_MIS},
  author       = {Yinfei Zheng and Zeyi Luo and Qiongwen Zhang and Zhifei Li and Qianguan Fu and Huilong Duan and Gaokai Liu and Yonghua Chu and Gang Yu},
  doi          = {10.1109/MIS.2025.3600417},
  journal      = {IEEE Intelligent Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {High-speed dorsal hand vein recognition using siamese lightweight neural network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Considering sentiment causes in in-context learning for aspect-based sentiment analysis. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3584862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) aims to identify aspect terms in texts and determine their sentiment polarities. The in-context learning (ICL) paradigm, powered by large language models (LLMs), has proven effective in low-resource scenarios, where the retrieval of effective demonstration examples is crucial. Existing retrieval methods prioritize semantic and syntactic similarities, overlooking the fact that sentiment is often driven by its underlying causes. Recognizing that similar causes tend to yield similar sentiments, we propose the Semantic-Causal Contextual Demonstration Retrieval (SCCDR), a demonstration retriever that integrates semantic and syntactic information while explicitly modeling sentiment causes. SCCDR was trained using contrastive learning based on rich contextual signals, including semantics, aspect-sentiment relationships, syntactic structures, and sentiment causes. Experiments on four datasets show that SCCDR outperforms other retrieval methods, thereby effectively improving ABSA performance under the ICL paradigm.},
  archive      = {J_MIS},
  author       = {Mengtian Shi and Rui Fan and Tingting He and Guanyi Chen},
  doi          = {10.1109/MIS.2025.3584862},
  journal      = {IEEE Intelligent Systems},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Considering sentiment causes in in-context learning for aspect-based sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finetuning large language models with behavioral alignment for depression detection. <em>MIS</em>, 1-8. (<a href='https://doi.org/10.1109/MIS.2025.3584888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a prevalent mental health issue, and early detection is crucial for effective intervention. Social media platforms offer a wealth of data that can be leveraged for depression detection. In this paper, we propose the depression large language model (DLLM), a novel two-stage finetuning framework designed to enhance the accuracy and robustness of depression detection using multimodal data from social media. In the first stage, we design specific prompts to incorporate various types of multimodal data and collect diverse instruction data. The DLLM is then fine-tuned for depression detection based on this data. In the second stage, we enhance the model’s robustness and generalization by performing behavioral alignment. This involves a deep understanding of user actions to improve behavior perception, enabling the policy model to distinguish between positive and negative behaviors for individual users. Experiments on the WU3D dataset show that DLLM outperforms state-of-the-art baselines (e.g., +6.2% accuracy over ALBERT, +2.4% F1 over EKG-MDDM) and demonstrates strong generalization in ablation studies.},
  archive      = {J_MIS},
  author       = {Xifeng Ning and Hailu Sun and Dejun Yu and Chao Yang and Ruonan Fang and Lin Fan and Qika Lin and Yifan Zhu},
  doi          = {10.1109/MIS.2025.3584888},
  journal      = {IEEE Intelligent Systems},
  month        = {7},
  pages        = {1-8},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Finetuning large language models with behavioral alignment for depression detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARISE: Explainable multi-modal aggressive driving detection via driver state and environment perception. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3586243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting aggressive driving is challenging but crucial for public safety. Existing methods rely on time-series data of drivers’ physiology, behavior, and vehicle movement but overlook driver’s emotion and environmental influences. We propose ARISE, a multi-source aggregation model integrating physiological, behavioral, and emotional data, vehicle sensor inputs, and environmental conditions. ARISE employs multi-source feature extraction, multimodal fusion, and a classifier to detect aggressive driving. Unlike graph-based methods that fails to detect gradual aggression shifts or transformer-based methods prone to delays, ARISE explicitly models vehicle state continuity and the aggressive driving environment. Motion similarity descriptor tracks state transitions, while aggression descriptor quantifies environmental aggression. Additionally, driving performance descriptor assesses driving workload and stability. Experiments show that ARISE significantly outperforms state-of-theart methods in aggressive driving detection.},
  archive      = {J_MIS},
  author       = {Sainan Zhang and Jun Zhang and Weiguo Song and Tan Yue and Luyao Zhu},
  doi          = {10.1109/MIS.2025.3586243},
  journal      = {IEEE Intelligent Systems},
  month        = {7},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {ARISE: Explainable multi-modal aggressive driving detection via driver state and environment perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight attribute localizing models for pedestrian attribute recognition. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3588759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian Attribute Recognition (PAR) focuses on identifying various attributes in pedestrian images, with key applications in person retrieval, suspect re-identification, and soft biometrics. However, Deep Neural Networks (DNNs) for PAR often suffer from over-parameterization and high computational complexity, making them unsuitable for resource-constrained devices. Traditional tensor-based compression methods typically factorize layers without adequately preserving the gradient direction during compression, leading to inefficient compression and a significant accuracy loss. In this work, we propose a novel approach for determining the optimal ranks of low-rank layers, ensuring that the gradient direction of the compressed model closely aligns with that of the original model. This means that the compressed model effectively preserves the update direction of the full model, enabling more efficient compression for PAR tasks. The proposed procedure optimizes the compression ranks for each layer within the ALM model, followed by compression using CPD-EPC or truncated SVD. This results in a reduction in model complexity while maintaining high performance.},
  archive      = {J_MIS},
  author       = {Ashish Jha and Dimitrii Ermilov and Anh Huy Phan and Konstantin Sobolev and Salman Ahmadi-Asl and Naveed Ahmed and Imran Junejo and Zaher AL Aghbari and Thar Baker and Ahmed Mohamed Khedr and Andrzej Cichocki},
  doi          = {10.1109/MIS.2025.3588759},
  journal      = {IEEE Intelligent Systems},
  month        = {7},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Lightweight attribute localizing models for pedestrian attribute recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOWOv3: An efficient and generalized framework for spatio temporal action detection. <em>MIS</em>, 1-11. (<a href='https://doi.org/10.1109/MIS.2025.3581100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a significant enhancement of YOWOv2, resulting in a lightweight and efficient action detection and recognition model referred to as YOWOv3. This model utilizes recently proposed architectures to reduce computational costs while integrating an enhanced label assignment mechanism to preserve model performance, thereby compensating for the decreased complexity of the architecture. Experiments were conducted on two popular datasets, UCF101-24 and AVAv2.2, to evaluate the proposed YOWOv3. YOWOv3 illustrates competitive results, obtains an mAP of 20.93% on AVAv2.2, and 88.64% on UCF101-24, by utilizing just 54.7M parameters and 37.2 GFLOPs compared to benchmark YOWOv2 with 109.7M parameters and 53.6 GFLOPs, as well as an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively. The code is publicly available at: https://github.com/AakiraOtok/YOWOv3.},
  archive      = {J_MIS},
  author       = {Dang Duc Manh Nguyen and Bui Duc Nhan and Jia Ching Wang and Viet-Hang Duong},
  doi          = {10.1109/MIS.2025.3581100},
  journal      = {IEEE Intelligent Systems},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Intell. Syst.},
  title        = {YOWOv3: An efficient and generalized framework for spatio temporal action detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FairPreprocessor: Better fairness via addressing imbalanced data through synthetic data generation and mitigating biased labels. <em>MIS</em>, 1-15. (<a href='https://doi.org/10.1109/MIS.2025.3580459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine learning(ML) model acquires logic from the training dataset, and any bias within it impacts the model’s decision. Previous studies have revealed that ‘biased labels’ and ‘imbalanced data’ are significant causes of bias in the training dataset. This study proposes a preprocessing approach, FairPreprocessor, that addresses ’imbalanced data’ through rebalancing the internal data distribution by employing synthetic data techniques grounded in differential evolution. It also selects the most suitable crossover rate in synthetic data generation to achieve better fairness. Additionally, it identifies and removes biased labels through situation testing, thereby mitigating their effects and developing fairer ML software. To facilitate an open science, this study’s source code and datasets are available online at https://github.com/sendgmale/FairPreprocessor.},
  archive      = {J_MIS},
  author       = {Hem Chandra Joshi and Sandeep Kumar},
  doi          = {10.1109/MIS.2025.3580459},
  journal      = {IEEE Intelligent Systems},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Intell. Syst.},
  title        = {FairPreprocessor: Better fairness via addressing imbalanced data through synthetic data generation and mitigating biased labels},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning motion-scene disentanglement for trajectory prediction from videos. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2025.3573854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent trajectory prediction plays a significant role in various intelligent systems, as it entails the accurate anticipation of the future trajectory based on the historical data. Conventional approaches often rely on ready-made trajectory coordinates as inputs, which remain inapplicable in video-based scenarios. While two-stage trajectory prediction methods based on tracking-prediction paradigms have made progress in predicting trajectories, they still suffer from the information degradation and error accumulation due to the independence stages. In this paper, we propose an end-to-end model (MSDN) to directly predict future trajectories from videos. We design novel disentanglement structures to explicitly learn the motion-aware and scene-aware representations from videos to avoid information degradation. To alleviate error accumulation, we propose the temporal consistency learning structure. Extensive experiments on the ETH-UCY dataset and the Stanford Drone Dataset demonstrate that the proposed model not only outperforms the previous approaches, but also significantly enhances the inference speed.},
  archive      = {J_MIS},
  author       = {Haowen Tang and Ping Wei and Ziyang Ren and Huan Li},
  doi          = {10.1109/MIS.2025.3573854},
  journal      = {IEEE Intelligent Systems},
  month        = {5},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Learning motion-scene disentanglement for trajectory prediction from videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical relation reasoning for 3-D object detection. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3564347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-D object detection is an important problem in many intelligent system applications. Based on powerful spatial information provided by point clouds, existing methods focus primarily on the intrinsic geometric properties of objects while neglecting the physical relationships and interactions among the objects. This may lead to physically unreasonable predictions, such as floating objects or object volume overlaps. In this paper, we propose a novel 3-D object detection method from the perspective of physical relation reasoning. Specifically, we introduce two aspects of physical relations, including stability and volume exclusion. In addition, we introduce room layouts to assist in 3-D object detection and formulate two physical constraints on the basic of volume exclusion and stability to ensure that all objects conform to real-world physics constraints. We validate our proposed model on ScanNetV2 and SUN RGB-D datasets, and the results demonstrate the effectiveness.},
  archive      = {J_MIS},
  author       = {Jialu Qin and Ping Wei},
  doi          = {10.1109/MIS.2025.3564347},
  journal      = {IEEE Intelligent Systems},
  month        = {4},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Physical relation reasoning for 3-D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRLNet: Cascaded resolution learning network for natural scenes segmentation. <em>MIS</em>, 1-8. (<a href='https://doi.org/10.1109/MIS.2025.3549432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The natural environment presents a multitude of scenes with diverse content, posing challenges for satisfactory segmentation results using existing segmentation networks. In response, we propose a Cascaded Resolution Learning Network (CRLNet) to enhance segmentation performance through global textual embedding and multi-resolution feature learning. The CRLNet constructs a multi-path segmentation system that integrates multi-resolution feature data from different paths, thereby progressively enhancing local feature learning. Two key modules, the Partition-Fusion Channel Attention Module (PFCAM) and the Features Learning Module (FLM), are pivotal components of CRLNet. The PFCAM serves as a computationally efficient channel attention module to mitigate segmentation confusion stemming from similar objects. Meanwhile, the FLM is tailored to learn resolution feature maps from different paths, facilitating the refinement of object representation and enhancing segmentation performance. Extensive experiments conducted on real natural scene datasets demonstrate the superiority of the proposed CRLNet over existing efficient segmentation methods in terms of accuracy.},
  archive      = {J_MIS},
  author       = {Wei Li and Shishun Tian and Guoguang Hua and Muxin Liao and Yuhang Zhang and Wenbin Zou},
  doi          = {10.1109/MIS.2025.3549432},
  journal      = {IEEE Intelligent Systems},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Intell. Syst.},
  title        = {CRLNet: Cascaded resolution learning network for natural scenes segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated circuit image synthesis for unsupervised circuit annotation via shape consistent image translation. <em>MIS</em>, 1-9. (<a href='https://doi.org/10.1109/MIS.2025.3550208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques achieve promising performance on the circuit annotation task for the hardware assurance of integrated circuits, but are reliant on large amounts of costly, labeled training data. Recently, image synthesis techniques have been explored to mitigate this reliance. However, exiting methods often lack precise pixel-level correspondence between the input image and synthetic image. When applied to IC images, this could lead to inconsistencies in the circuit structures, which then cause changes in circuit connectivity. In this paper, we propose shape consistent image translation (SCIT) to synthesize IC images that have a high pixel-level correspondence with the input images. Our experiments show that a segmentation model trained SCIT-generated images reduces the number of circuit connection errors by 56.26% compared to the second best reported technique. Our proposed SCIT also produced synthetic IC images of the highest image quality when evaluated across three IC image datasets.},
  archive      = {J_MIS},
  author       = {Yee-Yang Tee and Xuenong Hong and Deruo Cheng and Tong Lin and Yiqiong Shi and Bah-Hwee Gwee},
  doi          = {10.1109/MIS.2025.3550208},
  journal      = {IEEE Intelligent Systems},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Integrated circuit image synthesis for unsupervised circuit annotation via shape consistent image translation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated online learning with filters for lung nodule segmentation in low-dose computed tomography images. <em>MIS</em>, 1-8. (<a href='https://doi.org/10.1109/MIS.2025.3554552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung nodules are critical indicators of pulmonary lesions closely associated with lung cancer progression. Low-dose computed tomography (LDCT) is widely employed to identify lung nodules. However, the large volume of imaging data it produces creates significant processing challenges for healthcare professionals, leading to diagnostic delays, uncertainty, and increased labor costs. To address these challenges, we propose a federated online learning framework that enables collaboration among multiple hospitals while complying with regulations. Our system includes three filters for lung nodule segmentation: a relabeling data balance filter, a two-stage network filter for segmentation and false positive reduction, and an interactive labeling correction filter. The system improves model robustness and mitigates biases during collaborative training. Tested in three hospitals, our framework preserved data privacy and achieved an accuracy of approximately 85%, outperforming models trained on local data.},
  archive      = {J_MIS},
  author       = {Chen-Shun Lee and Yau-Lin Tseng and Chao-Chun Chang and Chia-Ying Lin and Yi-Ting Yen and Ching-Ting Tu and Shu-Mei Guo and Jenn-Jier James Lien},
  doi          = {10.1109/MIS.2025.3554552},
  journal      = {IEEE Intelligent Systems},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Federated online learning with filters for lung nodule segmentation in low-dose computed tomography images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attention-driven and autoencoder-based bidirectional LSTM for long interval gap-filling of a water treatment process data set. <em>MIS</em>, 1-10. (<a href='https://doi.org/10.1109/MIS.2024.3513159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water quality monitoring with distributed water quality probes is crucial for environmental protection and public health. But missing data in time series poses a significant challenge in environmental management. This paper introduces a novel long interval gap-filling method using a Bidirectional Long Short-Term Memory Autoencoder with multi-head Attention Mechanism (BiLSTM-AAM). The multi-head attention mechanism allows the model to focus on informative regions within the data, even with extensive gaps while the autoencoder module aids in dimensionality reduction. The efficacy of BiLSTM-AAM was evaluated by a big real-world water quality dataset and further tested in other environmental contexts, demonstrating superior performance compared to three other similar models. The results highlight the model’s ability to accurately reconstruct missing values, capture complex dependencies, and restore data integrity even with high percentages of missing data. Our findings indicate significant implications for pattern analysis and machine intelligence, ultimately contributing to better monitoring for water process engineering and beyond.},
  archive      = {J_MIS},
  author       = {Rohan Gudla and Jinxiang Cheng and Ni-Bin Chang},
  doi          = {10.1109/MIS.2024.3513159},
  journal      = {IEEE Intelligent Systems},
  month        = {12},
  pages        = {1-10},
  shortjournal = {IEEE Intell. Syst.},
  title        = {An attention-driven and autoencoder-based bidirectional LSTM for long interval gap-filling of a water treatment process data set},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ordinal prompt-regularized graph optimal transport for image ordinal estimation. <em>MIS</em>, 1-11. (<a href='https://doi.org/10.1109/MIS.2024.3519586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel approach for image ordinal estimation, leveraging the power of Optimal Transport (OT) and prompt learning. Traditional ordinal regression methods primarily focus on learning a model to predict numerical scores, which may not directly reflect the intrinsic order. To address this limitation, we introduce a framework, termed Ordinal Prompt-regularized Graph Optimal Transport (OPGOT), which utilizes OT to align the distribution of images and that of ordinal labels. First, we incorporate prompt learning with pre-trained text encoders to construct ordinal prompts through a token-wise distance-based weighting scheme, enabling the model to capture the semantic relationships between ordinal categories. Second, the OPGOT matches the graphs of image features and prompt embeddings via optimizing the OT with language-image cost. Hence, the learned transport plan reflects the intrinsic ordinal relationships. We conduct extensive evaluations on four benchmark datasets of different scenarios, demonstrating that OPGOT achieves significant improvements against existing methods.},
  archive      = {J_MIS},
  author       = {Xiangkai Wang and Kai Zhang and Xiaoxu Liu and Jia Jia and Maozhi Zhang},
  doi          = {10.1109/MIS.2024.3519586},
  journal      = {IEEE Intelligent Systems},
  month        = {12},
  pages        = {1-11},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Ordinal prompt-regularized graph optimal transport for image ordinal estimation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton-based traffic gesture recognition via a motion-guided 2S-GCN in autonomous driving: A review of challenges and opportunities. <em>MIS</em>, 1-8. (<a href='https://doi.org/10.1109/MIS.2024.3490753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the safety of autonomous driving systems, recognizing the gestures of cyclists and controllers on traffic roads is crucial. However, general action recognition methods are unsuitable for traffic gestures. This is because, compared to general actions, traffic gestures exhibit distinct characteristics, namely: (1) short action duration; (2) atypical action posture; and (3) small dataset sizes . We propose a novel two-stream graph convolutional network (2S-GCN) to address these challenges. Firstly, we introduced a motion-guided module connecting the motion and joint stream. By leveraging the motion stream’s spatial expression capability, we guided the joint stream’s learning in spatial dimensions to enhance the network’s performance to short action durations. Secondly, we utilized a simplified skeleton topology that exclusively captures the upper body during data preprocessing, thereby preserving essential information while eliminating redundancy. This approach enhances the network’s adaptability to atypical action postures. Thirdly, we use a GCN embedded with a multi-channel attention module as the backbone, which is particularly suitable for small datasets, and conduct extensive experiments on two datasets. The experimental results demonstrate that our network can accurately recognize traffic gestures and has significant advantages over state-of-theart methods.},
  archive      = {J_MIS},
  author       = {Xiaofeng Guo and Yang Mo and Yaonan Wang and Qing Zhu},
  doi          = {10.1109/MIS.2024.3490753},
  journal      = {IEEE Intelligent Systems},
  month        = {11},
  pages        = {1-8},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Skeleton-based traffic gesture recognition via a motion-guided 2S-GCN in autonomous driving: A review of challenges and opportunities},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot object detection based on self-knowledge distillation. <em>MIS</em>, 1-8. (<a href='https://doi.org/10.1109/MIS.2022.3205686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields, due to the lack of large-scale training data, the traditional object detection methods cannot complete the actual work well. The main reason is the overfitting problem and lack of the generalization ability. In this work, we propose a general method to alleviate the overfitting problem in the few-shot object detection. Our work extends Faster R-CNN with self-knowledge distillation algorithm and designs the loss function with attention mechanism, which can improve true detection in the foreground. In this way, object detector can learn an approximate mapping relationship from few samples, which makes the network possess a stronger generalization ability when tackling few images. Through numerous comparative experiments, we demonstrate that our method is general and feasible on VOC and COCO benchmarks datasets with different settings. We provide a new idea for solving the problem of few-shot object detection, and produce an excellent performance of recall rate on few-shot object detection.},
  archive      = {J_MIS},
  author       = {Yang Li and Yicheng Gong and Zhuo Zhang},
  doi          = {10.1109/MIS.2022.3205686},
  journal      = {IEEE Intelligent Systems},
  month        = {9},
  pages        = {1-8},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Few-shot object detection based on self-knowledge distillation},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). GLARE-SSCPM: An intelligent systemto support the treatment of comorbid patients. <em>MIS</em>, 1. (<a href='https://doi.org/10.1109/MIS.2018.111144734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of software tools supporting physicians in the treatment of comorbid patients is a challenging goal and a hot topic in Medical Informatics and Artificial Intelligence. Computer Interpretable Guidelines (CIGs) are consolidated tools to support physicians with evidence-based recommendations in the treatment of patients affected by a specific disease. However, the applications of two or more CIGs on comorbid patients is critical, since dangerous interactions between (the effects of) actions from different CIGs may arise. GLARE-SSCPM is the first tool supporting, in an integrated way, (i) the knowledge-based detection of interactions, (ii) the management of the interactions, and (iii) the final merge of (part of) the CIGs operating on the patient. GLARE-SSCPM is characterized by being very supportive to physicians, providing them support for focusing, interaction detection, and for an hypothesize and test approach to manage the detected interactions. To achieve such goals, it provides advanced Artificial Intelligence techniques. Preliminary tests in the educational context, within the RoPHS project, have provided encouraging results.},
  archive      = {J_MIS},
  author       = {Luca Piovesan and Paolo Terenziani and Gianpaolo Molino},
  doi          = {10.1109/MIS.2018.111144734},
  journal      = {IEEE Intelligent Systems},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Intell. Syst.},
  title        = {GLARE-SSCPM: An intelligent systemto support the treatment of comorbid patients},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2014). Exploring alterations of brain networks of AD patients using WTC method. <em>MIS</em>, 1. (<a href='https://doi.org/10.1109/MIS.2014.37'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: To explore the influences of different frequency bands on preprocessing of resting-state fMRI datasets used by the Wavelet Transform Coherence (WTC) method, and to study changes in the functional brain networks of AD patients. Method: Resting-state fMRI datasets of 10 AD patients and 11 healthy controls were collected in this study and time series of 90 brain regions defined by AAL (Automated Anatomical Labeling) were exacted after preprocessing. Wavelet transformation was performed for each time series, and a functional brain network were established in different frequencies (0.125Hz, 0.0625Hz) using the WTC (Wavelet Transform Coherence) method. The topology parameters of networks, containing global efficiency, clustering coefficient, average short paths length and small world property were calculated and averaged within each group. Result: The results imply that there are significant differences of topology parameters in networks of different frequencies. Likewise, statistical analysis of topology parameters of AD and HC (Healthy Controls) show that global efficiency, clustering coefficient and small world properties of AD all decreased by varying degrees, while the short path length of AD remained longer. Conclusion: Our research provides a theoretical basis for the choice of filter bands for data preprocessing in functional magnetic resonance imaging. The findings may serve as indicators for early diagnosis of AD patients.},
  archive      = {J_MIS},
  author       = {Li Yapeng and Yuanyuan Qin and Xi Chen and Wei Li},
  doi          = {10.1109/MIS.2014.37},
  journal      = {IEEE Intelligent Systems},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Exploring alterations of brain networks of AD patients using WTC method},
  year         = {2014},
}
</textarea>
</details></li>
<li><details>
<summary>
(2012). Towards incremental development of human-agent-robot applications using mixed-reality testbeds. <em>MIS</em>, 1. (<a href='https://doi.org/10.1109/MIS.2012.7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing is essential part of the development of human-agent-robot team (HART) applications. Individual algorithms in such applications cannot be tested in isolation as their performance depends significantly on complex interactions among distributed software code, humans, hardware and the target environment. Any testing involving robots and human actors is, however, time-consuming and costly. We therefore propose an incremental development framework employing mixed-reality testbeds, which can reduce testing cost by replacing parts of the application and surrounding reality with synthetic computational models. The proposed framework introduces the concept of testbed fidelity and proposes how test reliability and cost should be managed to maximize the effectiveness of the development process. The framework is illustrated on two example applications in the domain of multi-UAV tracking and anti-maritime piracy operations.},
  archive      = {J_MIS},
  author       = {Michal Jakob and Michal Pechoucek and Michal Cap and Ondrej Vanek and Peter Novak},
  doi          = {10.1109/MIS.2012.7},
  journal      = {IEEE Intelligent Systems},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Intell. Syst.},
  title        = {Towards incremental development of human-agent-robot applications using mixed-reality testbeds},
  year         = {2012},
}
</textarea>
</details></li>
</ul>

</body>
</html>
