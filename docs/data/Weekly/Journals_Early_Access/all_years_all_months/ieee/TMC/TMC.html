<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc">TMC - 301</h2>
<ul>
<li><details>
<summary>
(2025). Aerial shepherds: Enabling hierarchical localization in heterogeneous MAV swarms. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3616380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heterogeneous micro aerial vehicles (MAV) swarm consists of resource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but cost-effective basic MAVs (BMAVs), offering opportunities in diverse fields. Accurate and real-time localization is crucial for MAV swarms, but current practices lack a low-cost, high-precision, and real-time solution, especially for lightweight BMAVs. We find an opportunity to accomplish the task by transforming AMAVs into mobile localization infrastructures for BMAVs. However, translating this insight into a practical system is challenging due to issues in estimating locations with diverse and unknown localization errors of BMAVs, and allocating resources of AMAVs considering interconnected influential factors. This work introduces TransformLoc, a new framework that transforms AMAVs into mobile localization infrastructures, specifically designed for low-cost and resource-constrained BMAVs. We design an error-aware joint location estimation model to perform intermittent joint estimation for BMAVs and introduce a similarity-instructed adaptive grouping-scheduling strategy to allocate resources of AMAVs dynamically. TransformLoc achieves a collaborative, adaptive, and cost-effective localization system suitable for large-scale heterogeneous MAV swarms. We implement and validate TransformLoc on industrial drones. Results show it outperforms all baselines by up to 68% in localization performance, improving navigation success rates by 60%. Extensive robustness and ablation experiments further highlight superiority of its design.},
  archive      = {J_TMC},
  author       = {Haoyang Wang and Jingao Xu and Chenyu Zhao and Yuhan Cheng and Xuecheng Chen and Chaopeng Hong and Xiao-Ping Zhang and Yunhao Liu and Xinlei Chen},
  doi          = {10.1109/TMC.2025.3616380},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Aerial shepherds: Enabling hierarchical localization in heterogeneous MAV swarms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MagPrint++: Continuous user fingerprinting on mobile devices using electromagnetic signals. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3616308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the nature of user-device interactions (e.g., who is using the device and what he/she is doing with it) is critical for many applications including time management, user profiles, and privacy protection. However, in scenarios where mobile devices are shared among family members or multiple employees in a company, conventional account-based statistics are not meaningful. This poses an even bigger problem when dealing with sensitive data. Moreover, fingerprint readers and front-facing cameras were not designed to continuously identify users. In this study, we developed MagPrint++ , a novel approach to fingerprint users based on unique patterns in the electromagnetic (EM) signals associated with the specific use patterns of users. Initial experiments showed that time-varying EM patterns are unique to individual users. They are also temporally and spatially consistent, which makes them suitable for fingerprinting. MagPrint++ has a number of advantages over existing schemes: i) Non-intrusive fingerprinting, ii) implementation both on COTS mobile phones and a small and easy-to-deploy device, and iii) high accuracy thanks to the proposed classification algorithm. In experiments involving 30 users, MagPrint++ achieves $94.3\%$ accuracy in classifying users from these traces, which represents a $10.9\%$ improvement over the state-of-the-art classification method.},
  archive      = {J_TMC},
  author       = {Lanqing Yang and Xinqi Chen and Hao Pan and Yi-Chao Chen and Guangtao Xue and Zechen Li and Yiheng Bian and Dian Ding and Linghe Kong and Jiadi Yu and Feng Lyu and Minglu Li and Ziyu Shen and Bo Zhang},
  doi          = {10.1109/TMC.2025.3616308},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MagPrint++: Continuous user fingerprinting on mobile devices using electromagnetic signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target wake time scheduling for time-sensitive and energy-efficient wi-fi networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3617324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Sensitive Networking (TSN) is fundamental for the reliable, low-latency networks that will enable the Industrial Internet of Things (IIoT). Wi-Fi has historically been considered unfit for TSN, as channel contention and collisions prevent deterministic transmission delays. However, this issue can be overcome by using Target Wake Time (TWT), which enables the access point to instruct Wi-Fi stations to wake up and transmit in non-overlapping TWT Service Periods (SPs), and sleep in the remaining time. In this paper, we first formulate the TWT Acceptance and Scheduling Problem (TASP), with the objective to schedule TWT SPs that maximize traffic throughput and energy efficiency while respecting Age of Information (AoI) constraints. Then, due to TASP being NP-hard, we propose the TASP Efficient Resolver (TASPER), a heuristic strategy to find near-optimal solutions efficiently. Using a TWT simulator based on ns-3, we compare TASPER to several baselines, including HSA, a state-of-the-art solution originally designed for WirelessHART networks. We demonstrate that TASPER obtains up to 24.97% lower mean transmission rejection cost and saves up to 14.86% more energy compared to the leading baseline, ShortestFirst, in a challenging, large-scale scenario. Additionally, when compared to HSA, TASPER also reduces the energy consumption by 34% and reduces the mean rejection cost by 26%. Furthermore, we validate TASPER on our IIoT testbed, which comprises 10 commercial TWT-compatible stations, observing that our solution admits more transmissions than the best baseline strategy, without violating any AoI deadline.},
  archive      = {J_TMC},
  author       = {Fabio Busacca and Corrado Puligheddu and Francesco Raviglione and Riccardo Rusca and Claudio Casetti and Carla Fabiana Chiasserini and Sergio Palazzo},
  doi          = {10.1109/TMC.2025.3617324},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Target wake time scheduling for time-sensitive and energy-efficient wi-fi networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced velocity-adaptive scheme: Joint fair access and age of information optimization in vehicular networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3617145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for driving assistance. Nevertheless, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwell time and communication duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI.},
  archive      = {J_TMC},
  author       = {Xiao Xu and Qiong Wu and Pingyi Fan and Kezhi Wang and Nan Cheng and Wen Chen and Khaled B. Letaief},
  doi          = {10.1109/TMC.2025.3617145},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhanced velocity-adaptive scheme: Joint fair access and age of information optimization in vehicular networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secrecy energy efficiency of hybrid wireless body area networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3618098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid Wireless Body Area Networks (HyWBANs) are revolutionizing healthcare by integrating joint sensing and communication capabilities. However, this advancement introduces critical security challenges, as attackers can exploit sensing channels to intercept sensitive medical data. This paper introduces Secrecy Energy Efficiency (SEE) as a new performance metric for hybrid radio-optical wireless networks, enabling a quantitative assessment of secure communication under power-constrained conditions. We formulate and solve optimization problems to maximize the optical secrecy rate and SEE. We extend this analysis to a joint allocation framework for Ultra Wideband (UWB) and Near-Infrared (NIR) channels. Our approach leverages Sequential Fractional Programming (SFP), which enables to tackle the non-convex SEE maximization problem by a sequence of convex problems, addressing secure transmissions' inherent non-convexity and fractional nature with intentional jamming. Based on lab-based in-body measurements through porcine tissue and on radio and optical average synthetic phantoms, numerical evaluations demonstrate that the NIR link can achieve approximately 3 bit/Hz/Joule in SEE. Further, we show that optimal power allocation significantly outperforms random allocation methods, highlighting the potential of this approach for mission-critical healthcare applications. These findings provide a robust foundation for designing next-generation, low-power medical communication systems that balance security requirements with stringent energy constraints.},
  archive      = {J_TMC},
  author       = {Simone Soderi and Alessio Zappone},
  doi          = {10.1109/TMC.2025.3618098},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secrecy energy efficiency of hybrid wireless body area networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving rényi layer-wise budget allocation against gradient leakage for federated learning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3618185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is vulnerable to gradient-based privacy attacks, where malicious attackers reconstruct training data from exchanged gradients. While existing differential privacy (DP) defenses mitigate this, they often cause excessive additive noise due to the inequality scaling in the theoretical analyses, which degrades the model's utility or fail under adaptive attacks. To address this issue, we propose FedMSBA, a layer-wise privacy-preservation method that adaptively allocates privacy budgets via Rényi DP (RDP) and modified sensitivity. FedMSBA dynamically scales noise to model intricacies and adaptively choose the better applied DP mechanisms, which provides a tighter mathematical bound and finally prevents non-convergence while resisting reconstruction attacks. Experiments demonstrate superior privacy-utility trade-offs compared to state-of-the-art defenses. FedMSBA achieves an approximately 2% improvement in accuracy and a 5% enhancement in privacy preservation. Furthermore, FedMSBA's performance remains nearly unaffected by variations in the privacy budget $\epsilon$ and failure rate $\delta$.},
  archive      = {J_TMC},
  author       = {Leyu Shi and Ying Gao and Chong Chen and Siquan Huang and Jiafeng Zhao and Xiping Hu},
  doi          = {10.1109/TMC.2025.3618185},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving rényi layer-wise budget allocation against gradient leakage for federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing the AoI for pull-based target-level data collection in energy-harvesting IoTs. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3618092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collection is a crucial task of IoTs. According to the data collection scheme and the required data granularity, data collection in IoTs can be classified into pull-based/push-based data collection as well as node-level/target-level data collection. Thus, there are four scenarios for data collection: Push-based node-level data collection (Push-Node), Push-based target-level data collection (Push-Target), Pull-based node-level data collection (Pull-Node), and Pull-based target-level data collection (Pull-Target). Energy-Harvesting IoT (EH-IoT) is an important component of IoTs and the Age of Information (AoI) minimization problem has been studied extensively for data collection in EH-IoTs. However, existing works only studied the problem under the scenario of Push-Node, Push-Target and Pull-Node. Therefore, this paper investigates the AoI minimization problem for Pull-based Target-level data collection in EH-IoTs (AoI-Pull-Target) for the first time. AoI-Pull-Target is formally defined and proved to be NP-hard. A two-stage dynamic programming-based node scheduling algorithm and a real-time schedule adjustment scheme are proposed to solve the problem. The proposed algorithm is analyzed theoretically. Extensive simulations and real-world testbed experiments verify the high performance of our algorithm.},
  archive      = {J_TMC},
  author       = {Bingkun Yao and Hong Gao and Yang Zhang and Dongjing Miao and Quan Chen and Jianzhong Li},
  doi          = {10.1109/TMC.2025.3618092},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Minimizing the AoI for pull-based target-level data collection in energy-harvesting IoTs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User-centric communication service provision for edge-assisted mobile augmented reality. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3618147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future 6G networks are envisioned to facilitate edge-assisted mobile augmented reality (MAR) via strengthening the collaboration between MAR devices and edge servers. In order to provide immersive user experiences, MAR devices must timely upload camera frames to an edge server for simultaneous localization and mapping (SLAM)-based device pose tracking. In this paper, to cope with user-specific and non-stationary uplink data traffic, we develop a digital twin (DT)-based approach for user-centric communication service provision for MAR. Specifically, to establish DTs for individual MAR devices, we first construct a data model customized for MAR that captures the intricate impact of the SLAM-based frame uploading mechanism on the user-specific data traffic pattern. We then define two DT operation functions that cooperatively enable adaptive switching between different data-driven models for capturing non-stationary data traffic. Leveraging the user-oriented data management introduced by DTs, we propose an algorithm for network resource management that ensures the timeliness of frame uploading and the robustness against inherent inaccuracies in data traffic modeling for individual MAR devices. Trace-driven simulation results demonstrate that the user-centric communication service provision achieves a 14.2% increase in meeting the camera frame uploading delay requirement in comparison with the slicing-based communication service provision widely used for 5G.},
  archive      = {J_TMC},
  author       = {Conghao Zhou and Jie Gao and Shisheng Hu and Nan Cheng and Weihua Zhuang and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3618147},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User-centric communication service provision for edge-assisted mobile augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service migration strategies based on partially observable and multi-objective optimization. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3618278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing (MEC) extends cloud computing to the network edge, supporting resource-intensive mobile applications. Service migration ensures seamless continuity and high-quality service (QoS) when users move between MEC servers. In the Internet of Vehicles (IoV), the high mobility of vehicles causes network instability, complicating the collection of system information. In addition, vehicles impose strict latency and green energy requirements, and the search for the Pareto front among conflicting objectives increases the complexity of migration. Existing service migration methods rely on centralized decision making using complete system information, which is not suitable for the user-centric IoV environment. Current multi-objective reinforcement learning approaches lack sufficient exploration randomness, leading to suboptimal performance. We propose the Adversarial Variational State Inference with Maximum Entropy Multi-Objective Policy Optimization (AVSIMEMPO) algorithm to address the partially observable and multi-objective optimization problem, optimizing migration node selection. The service migration problem is modeled as a partially observable Markov decision process (POMDP). To solve this, we design an encoding network, AVSI, integrating Long ShortTerm Memory (LSTM), Variational Autoencoders (VAE), and adversarial learning to extract hidden state. We also introduce the Maximum Entropy Multi-Objective Policy Optimization (MEMPO) algorithm, which enhances exploration randomness through maximum entropy and dynamic weight design. Extensive experiments based on real mobility trajectories show that our method outperforms baseline algorithms and achieves nearoptimal results in various MEC scenarios.},
  archive      = {J_TMC},
  author       = {Yingzhen Hou and Lei Yang and Yu Dai},
  doi          = {10.1109/TMC.2025.3618278},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service migration strategies based on partially observable and multi-objective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning via gradient-fusion and gradient-decoupling for heterogeneous data. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3618262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables devices to collaboratively train a global model without centralizing raw data. However, heterogeneous data distributions often hinder a single global model from performing optimally across diverse clients. In this paper, we introduce two personalized federated learning methods to address the aforementioned challenge. First, we introduce a gradient-fusion approach that merges global gradients aggregated via a learnable collaboration matrix with local gradients tailored to each client's data. This fusion harmonizes collective intelligence with device-specific knowledge, thereby improving personalization under data heterogeneity. Second, we propose a gradient-decoupling method that mitigates the limitations of relying solely on a collaboration matrix for balancing knowledge sharing and personalization. By decoupling the tasks and regularizing local updates through the incorporation of extracted collective intelligence, we introduce an inductive bias that significantly improves generalization. Extensive evaluations on four computed tomography and pathology imaging datasets covering a wide range of heterogeneity show that our frameworks substantially outperform existing baselines. We also provide theoretical analyses that establish performance bounds and convergence guarantees under mild assumptions.},
  archive      = {J_TMC},
  author       = {Zaobo He and Yusen Li and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3618262},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized federated learning via gradient-fusion and gradient-decoupling for heterogeneous data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent omni-surface-aided multi-objective ISAC: A meta hybrid deep reinforcement learning approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3618256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an intelligent omni-surface (IOS)-aided integrated sensing and communication (ISAC) system, where a base station (BS) provides simultaneous target sensing and communication services with an IOS under outdated and imperfect channel state information (CSI). Both the communication sum-rate and sensing signal-to-noise ratio are maximized through joint optimization of BS beamforming and IOS configuration. To address this problem, we propose an intelligent joint optimization scheme called meta multi-objective hybrid deep reinforcement learning (meta-MHDRL). Specifically, the meta-MHDRL framework first introduces a hybrid deep reinforcement learning (DRL) approach that integrates double-critic-based deep deterministic policy gradient with deep double Q-network algorithms, enabling parallel optimization of both continuous-domain variables (i.e., BS beamforming, IOS reflecting phase shift, and IOS reflecting/refracting amplitudes) and the discrete-domain variable (i.e., IOS refracting phase shift). Thereafter, an objective-preference weight is incorporated into the hybrid DRL framework, such that meta-MHDRL can capture the trade-off between communication and sensing performance. To address the complex coupling relationships among different optimization variables, we further put forth a synchronized experience replay mechanism for meta-MHDRL, which maintains training synchronization among different neural networks. In addition, a meta-learning approach is developed to enhance the generalization ability of meta-MHDRL across different objective-preference weights. Simulation results show that meta-MHDRL attains more Pareto-efficient solutions than other schemes under outdated and imperfect CSI while maintaining stronger robustness across various simulation setups. Besides, we demonstrate the generalization ability of meta-MHDRL for unseen tasks},
  archive      = {J_TMC},
  author       = {Xiaowen Ye and Xianxin Song and Yi Wu and Liqun Fu},
  doi          = {10.1109/TMC.2025.3618256},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligent omni-surface-aided multi-objective ISAC: A meta hybrid deep reinforcement learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAAP: A graph-based VNF deployment framework for embedding bidirectional SFC in mobile edge networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3617016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Mobile Edge Computing (MEC) networks, mobile interactive applications, such as multiplayer online games, federated learning, and interactive multi-view video, are becoming increasingly popular. The embedding of Bidirectional Service Function Chains (BSFCs) for these applications has been studied. However, existing BSFC embedding research only considers static users, and the prevailing Virtual Network Functions (VNFs) placement methods do not account for the impact of individual node resources on the path, thus failing to maximize node resource utilization at the network level. Considering the aforementioned issues, we introduce a framework for BSFC embedding tailored for mobile users, named Path as a Point (PAAP). This framework integrates the resources of the paths and then globally considers the impact of the computing resources of edge nodes on the paths connecting them, maximizing edge node resource utilization across the entire network. We propose a three-phase algorithm within the PAAP framework. In the first phase, we introduce a singleuser algorithm for VNFs deployment during BSFC embedding. In the second phase, this optimization is extended to multiple users by establishing inter-user association rules. In the third phase, candidate application placement positions are evaluated across the entire network, culminating in the determination of optimal placement strategies. Empirical evaluations confirm the effectiveness of the proposed algorithm, demonstrating significant performance improvements over baseline methods.},
  archive      = {J_TMC},
  author       = {Yuhao Xie and Dehui Ou and Zhen Zhang and Yuhui Deng and Geyong Min and Lin Cui and Zhijie Wang},
  doi          = {10.1109/TMC.2025.3617016},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PAAP: A graph-based VNF deployment framework for embedding bidirectional SFC in mobile edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated continual learning with bounded forgetting via diffusion-based generative replay in edge computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3618275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated continual learning (FCL) aims to train a shared model incrementally on evolving tasks while respecting data privacy across distributed edge devices. In edge settings, historical task data for replay is often unavailable; moreover, as the model is continuously updated, features of newly introduced classes encroach on the feature space of older classes, distorting decision boundaries and exacerbating catastrophic forgetting. We propose DGR-FCL, an FCL framework that bounds forgetting via server-side, diffusion-based generative replay. The generator is trained with margin constraints and batch-normalization alignment to faithfully recover previously learned distributions. On each client, we introduce supervised contrastive learning with synthetic negatives to separate old and new classes, and an importance-weighted feature-distillation strategy to constrain representation drift along feature directions critical for old tasks. Theoretically, an integral probability metric (IPM) analysis shows that margin preservation on replay data ensures bounded forgetting. Empirically, DGR-FCL achieves consistent 4–5% accuracy gains over state-of-the-art baselines on both independent and identically distributed (IID) and non-IID benchmarks, offering a robust solution to the stability–plasticity trade-off in decentralized, continuously evolving environments.},
  archive      = {J_TMC},
  author       = {Zaobo He and Yunkun Wang and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3618275},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated continual learning with bounded forgetting via diffusion-based generative replay in edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TruChord: A secure communication framework for hybrid SDIoT architecture based on chord overlay. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3618876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a highly integrated and comprehensive application of the new generation of information technology, IoT inevitably comes with security issues. The communication security of IoT devices is the foundation for their large-scale development. To address the problems of missing cross-domain identity authentication, low routing efficiency, and frequent malicious attacks in hybrid SDIoT, we propose a secure and trusted communication framework for IoT based on overlay networks, namely TruChord. This framework introduces DICE technology to generate device firmware identifiers, constructs a trust chain, and realizes hardware binding of device identities and firmware integrity measurement. Meanwhile, it designs a domain-specific identifier mapping mechanism, which integrates SDN domain DPID, IP domain prefix, and firmware identifiers to make physically adjacent nodes logically adjacent on the ring. Finally, it proposes a hierarchical trust aggregation protocol, which ensures secure communication between nodes through intra-domain local verification and cross-domain global aggregation with the help of a trust mechanism. Through formal proof, TruChord can resist Sybil attacks and collusion attacks. Simulation experiments show that TruChord outperforms the comparison frameworks in core indicators such as end-to-end delay, network traffic, and convergence time, providing an efficient solution for cross-domain secure communication of IoT devices.},
  archive      = {J_TMC},
  author       = {Yu Zhang and Bei Gong and Zahid Halim and Hisham Alasmary and Muhammad Waqas and Iftekhar Ahmad},
  doi          = {10.1109/TMC.2025.3618876},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TruChord: A secure communication framework for hybrid SDIoT architecture based on chord overlay},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age-optimal rate control transport protocol for cohesive clustered satellite systems. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3618276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohesive clustered satellite (CCS) system integrates diverse payloads from multiple formation-flying low Earth orbit (LEO) satellites to provide pervasive intelligent services to ground user equipment (UE), which is regarded as the critical component of satellite-integrated Internet. Considering the CCS system aggregates information at a backbone satellite and connect to the ground station via the satellite-integrated Internet, achieving timely transmission within this bottleneck link becomes crucial. In this paper, we propose an age-optimal rate control (ARC) protocol in transport layer, where each satellite in the CCS system independently and iteratively adjusts its transmit rate (TR) according to the feedback queuing delay at the backbone satellite to approach its age-optimal rate (AR), aimed at minimizing the average age of information (AAoI) of CCS system. Specifically, we derive the upper bound AAoI by modeling an $M/M/1$ queue at backbone satellite, and formulate an AAoI minimization problem by utilizing the upper bound AAoI and solve it to obtain the AR. Then, each satellite can calculate its expected rate (ER) by calculating the queuing delay via the timestamp of Acknowledgements (ACK) from backbone satellite, which is significantly larger than AR. Further, we design an Exponential then Additive Increase Additive Decrease (EAIAD) algorithm to control the convergence position and oscillation of TR, thereby avoiding the significantly increase in AAoI resulting from TR exceeds AR. Finally, we conduct extensive simulations to demonstrate that our ARC protocol achieves minimum AAoI compared to the state-of-the-art transport protocols.},
  archive      = {J_TMC},
  author       = {Hao Liu and Jian Jiao and Jianhao Huang and Weizhi Wang and Ye Wang and Qinyu Zhang},
  doi          = {10.1109/TMC.2025.3618276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age-optimal rate control transport protocol for cohesive clustered satellite systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating catastrophic forgetting in personalized federated learning for edge devices using state-space models. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3618886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing involves distributed devices operating in dynamic environments with diverse resource constraints and highly heterogeneous data distributions. In these settings, personalized federated learning (PFL) provides a collaborative learning framework that preserves the unique data characteristics of each device. However, PFL models often encounter bidirectional catastrophic forgetting. During consecutive training rounds, the personalized characteristics learned by local models are readily overwritten by updates from the global model, while the global model's shared representations are degraded by distribution shifts arising from heterogeneous local data. This challenge is further amplified in edge computing settings, where devices must adapt to fast-changing heterogeneous data. To address this issue, we propose FedSSM, a novel framework that leverages state-space models to mitigate forgetting in PFL. By capturing the temporal evolution of local model parameters through hidden states, the framework enhances the retention of critical knowledge throughout training rounds. Extensive experiments on multiple benchmark datasets demonstrate that FedSSM outperforms various state-of-the-art PFL algorithms, particularly with high data heterogeneity. The code can be found at: https://github.com/wei-d-zhang/FedSSM.},
  archive      = {J_TMC},
  author       = {Weidong Zhang and Dongshang Deng and Xuangou Wu and Tao Zhang and Xiao Zheng and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3618886},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating catastrophic forgetting in personalized federated learning for edge devices using state-space models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-quantum secure authenticated key agreement scheme for vehicular digital twin. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3618752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of digital twins, which leverage the server's robust data-processing and storage capacities to manage vehicular tasks, effectively addresses the limitations of traditional Internet of Vehicles (IoV). However, the communication link between a vehicle and its digital twin remains open and insecure, necessitating the development of an Authenticated Key Agreement (AKA) scheme to secure this channel. Existing AKA schemes for Vehicular Digital Twin (VDT) fail to achieve post-quantum security, and the progression of quantum computing has intensified the urgency to establish post-quantum secure solutions. To address these shortcomings, we propose a lightweight, quantum-resistant three-party AKA scheme for VDT networks based on the Ring Learning With Errors (RLWE) computational problem. Our scheme enables one-round interaction with privacy preservation while ensuring the efficiency of the AKA protocol through its RLWE-based design. Additionally, our approach mitigates the risk of temporary key reuse during Gaussian sampling, thereby resisting newly discovered key reuse attacks targeting lattice-based key agreement schemes. Security proofs and analyses prove that the proposed scheme meets the fundamental security and privacy requirements of IoV. Furthermore, performance evaluations indicate that our scheme achieves post-quantum security with low computational overhead and energy consumption, making it compatible with quantum-resistant VDT communications.},
  archive      = {J_TMC},
  author       = {Jie Cui and Jiyu Liu and Lu Wei and Irina Bolodurina and Jiaxin Li and Hong Zhong},
  doi          = {10.1109/TMC.2025.3618752},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Post-quantum secure authenticated key agreement scheme for vehicular digital twin},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pao-ding: Accelerating cross-edge video analytics via automated CNN model partitioning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3618296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-edge video analytics is a technology that involves collaborative processing of video data among multiple edge devices. To handle increasingly complex video analytics tasks on edge devices with higher frame rates or improved accuracy, cross-edge video analytics leverages model partitioning techniques to divide Convolutional Neural Network (CNN) models into multiple sub-models deployed across different computing nodes. This approach enables the efficient utilization of heterogeneous edge computing resources for compute-intensive CNN inference. Nonetheless, several challenges persist in cross-edge video analytics. These include the partitionability of CNN models requiring a lot of manual work, excessive transmission of intermediate features leading to performance degradation, and high computational complexity in partitioning decision-making. To tackle these challenges, this paper proposes Pao-Ding, a framework to accelerate cross-edge video analytics. Pao-Ding employs gradient information to automatically parse the structure of CNN models, thereby enabling partitionability for chain, simple Directed Acyclic Graph (DAG), and complex DAG models. By incorporating a skip-layer design, Pao-Ding is able to compress redundant intermediate features more accurately. Additionally, a layer-pruning-based algorithm is proposed in Pao-Ding which effectively reduces the computational overhead associated with partitioning decision-making. Experimental results on real hardware show that Pao-Ding supports automated model partitionability for 67 CNN models with a model coverage rate of 94.37%. Compared to state-of-the-art, Pao-Ding reduces total prediction error by 47.43%, lowers the partitioning decision time by 37.5%–58.35%, and decreases the average video frame processing time by 5.75%–22.18%. Furthermore, a cross-edge video analytics simulation platform developed using Docker and Pumba verifies the robustness of Pao-Ding under various conditions involving different number of nodes, computing capacities, and network environments.},
  archive      = {J_TMC},
  author       = {Guanping Liang and Biao Han and Ruidong Li and Xueqiang Han and Zhigang Sun},
  doi          = {10.1109/TMC.2025.3618296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pao-ding: Accelerating cross-edge video analytics via automated CNN model partitioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint task offloading and resource allocation in ultra-dense multi-access edge computing: A mean field learning approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3619077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ultra-dense multi-access edge computing (MEC), efficient task offloading and resource allocation are critical to meeting stringent delay and energy constraints. However, the rapid growth in terminal devices poses a significant challenge to traditional deep reinforcement learning (DRL)-based methods, which struggle to maintain efficient task offloading and resource allocation due to increased computational complexity and decision latency. To address this challenge, in this paper, we propose a mean field theory (MFT)-guided DRL approach which leverages the statistical characteristics of a large population of terminal devices to reduce the computational complexity of decision-making. Firstly, we formulate the joint task offloading and resource allocation problem as a mean field Markov decision process (MFMDP) with the objective of minimizing the overall system energy consumption while satisfying the task delay requirements and resource constraints. Secondly, by leveraging the inherent structure of the MFMDP, which represents the system dynamics using state and action distributions rather than joint action spaces, we achieve significant dimensionality reduction and improved scalability. Thirdly, we prove that the Q-function of MFMDP satisfies the fixed point theory, and considering the continuity of task offloading and resource allocation variables, we develop an MFT-guided deep deterministic policy gradient (MFT-DDPG) algorithm to solve the proposed problem. Experimental results show that MFT-DDPG significantly outperforms conventional DRL baselines in convergence speed and scalability. Specifically, for 50 terminal devices, the training time of proposed MFT-DDPG is reduced by up to 86.1%, 92.1%, and 94.6% compared to those of MADDPG, QMIX, and DDPG, respectively. In comparison with the Edge Server Computing Only (ECO) scheme and Device and Edge Server Collaborative Computing (DECC) scheme, the proposed method consistently yields lower energy consumption and delay across varying numbers of terminal devices, transmission power, and edge server computing resources compared to benchmark solutions, demonstrating its robustness and effectiveness in ultra-dense MEC scenarios.},
  archive      = {J_TMC},
  author       = {Huixian Gu and Liqiang Zhao and Zhu Han and Xiaoli Chu and Gan Zheng and Jiaxin Liu and Guorong Zhou},
  doi          = {10.1109/TMC.2025.3619077},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint task offloading and resource allocation in ultra-dense multi-access edge computing: A mean field learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IAB vs. RIS: Performance-cost tradeoffs in 5G/6G systems with multicast and unicast traffic in roadside deployments. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3619418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of millimeter wave (mmWave) and sub-terahertz (sub-THz) frequency bands in 5G and future 6G networks promises an unprecedented capacity enhancement at the air interface driven by highly directional transmissions. While this facilitates interference suppression and increased deployment density, it also presents challenges in multicast service delivery, particularly due to the use of directional antennas and environmental factors that can cause signal blockage. Technologies such as Integrated Access and Backhaul (IAB) and Reconfigurable Intelligent Surfaces (RISs) have emerged to address these challenges and reduce capital expenditure. This study comprehensively compares IAB- and RIS-based designs for cost-efficient densification in mmWave and sub-THz 5G/6G systems, focusing on both unicast and multicast traffic in roadside-type deployments. Two deployment scenarios with tunable parameters are analyzed, and optimization frameworks are formulated for each approach, accounting for propagation characteristics, radio properties, and antenna directionality. The evaluation metrics not only assess the performance of each technology (IAB and RIS) but also consider the deployment costs required to achieve equivalent performance levels. Numerical results show that both RIS- and IAB-based deployments can effectively support multicast and unicast traffic, with IAB systems demonstrating superior performance in terms of overall resource utilization up to 50% in sparse deployment scenarios. Instead, in highly dense deployment scenarios, RISs exhibit superior scalability and resource efficiency compared to IABs, achieving up to a 3 times improvement factor. Furthermore, unlike IAB deployments, the performance of RIS-based systems continuously improves as the number of RIS nodes increases. From a capital expenditure perspective, RIS deployments prove to be more cost-efficient than IAB systems, provided that the unit cost of RIS is lower.},
  archive      = {J_TMC},
  author       = {Olga Chukhno and Dmitri Moltchanov and Gianluca Brancati and Sara Pizzi and Antonella Molinaro and Giuseppe Araniti},
  doi          = {10.1109/TMC.2025.3619418},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IAB vs. RIS: Performance-cost tradeoffs in 5G/6G systems with multicast and unicast traffic in roadside deployments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconfigurable intelligent surface aided mobile fog computing: A space aggregation-based lyapunov driven reinforcement learning approach. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3619505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of mobile devices within Internet of Things (IoT) has substantially heightened the demand for mobile edge computing (MEC). Fog computing (FC) is a more advanced form of edge computing that allows computing nodes to cooperate with each other. Reconfigurable intelligent surfaces (RIS) have emerged as a critical technology for optimizing wireless communication environments, attracting considerable attention. In this paper, we develop an online optimization problem for RIS-aided mobile FC deployed across wireless networks with computing nodes at the base stations (BS). We propose a Lyapunov-drift-plus-penalty-based, space aggregation-assisted proximal policy optimization (LSAPPO) algorithm to tackle the challenges in online optimization problem in RIS-aided mobile FC system. Our technique integrates a reinforcement learning (RL) algorithm employing the proximal policy optimization (PPO) agent, further enhanced by Lyapunov drift-plus-penalty optimization. The space aggregation technique effectively consolidates excessive decision variables and channel state information (CSI) into a manageable set of parameters to streamline the computing framework. Numerical simulation result shows that our proposed algorithm surpasses the benchmarks, underscoring the effectiveness in complicated wireless networks. Furthermore, we introduce the multi-agent LSAPPO algorithm to address the distributed demands of practical scenarios. The multi-agent LSAPPO algorithm enhances convergence speed and performs better in large-scale problems.},
  archive      = {J_TMC},
  author       = {Wenhan Xu and Cunhua Pan and Yulan Yuan and Yuan Wu and Danny H.K. Tsang},
  doi          = {10.1109/TMC.2025.3619505},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reconfigurable intelligent surface aided mobile fog computing: A space aggregation-based lyapunov driven reinforcement learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving machine learning dependability through model switching and compression. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3619560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) can be often distributed, owing to the need to harness more resources and/or to preserve privacy. Accordingly, distributed learning has received significant attention from the literature; however, most works focus on the expected learning quality (e.g., loss) attained and do not consider the distribution thereof. It follows that ML models are not dependable, and may fall short of the required performance in many real-world cases. In this work, we tackle this challenge and propose DepL, a framework attaining dependable learning orchestration. DepL efficiently makes joint, near-optimal decisions concerning (i) which data to use for learning, (ii) the ML models to use – chosen within a set of full-size models and compressed versions thereof – and when to switch from one model to another, and (iii) the clusters of physical nodes to use for the learning. DepL improves over previous works by guaranteeing that the learning quality target (e.g., a minimum loss) is achieved with a target probability, while minimizing the learning (e.g., energy) cost. DepL has provably low polynomial computational complexity and a constant competitive ratio. Further, experimental results using the CIFAR-10 and GTSRB datasets show that it consistently matches the optimum and outperforms state-of-theart approaches (30% faster learning and 40–80% lower cost).},
  archive      = {J_TMC},
  author       = {Francesco Malandrino and Giuseppe Di Giacomo and Marco Levorato and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3619560},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Achieving machine learning dependability through model switching and compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VMR-STAG based online SFC orchestration in space-terrestrial integrated networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3619832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-Terrestrial Integrated Networks (STIN) have an important influence on Service Function Chains (SFCs), broadening their service scope and enhancing application performance. However, STIN features a dynamic terrestrial access layer and an inter-satellite layer; it's complex to orchestrate the SFC in such a dynamic network. In this paper, we provide the Virtual Multi-Resource Storage Time Aggregated Graph (VMR-STAG) model and SFC orchestration algorithms for SFC orchestration in STIN. Inspired by the Virtual Topology (VT) method, VMR-STAG aggregates the dual-layer dynamic topology and time-varying resources into a single virtual graph, thus reducing the complexity of the STIN model. Based on VMR-STAG, we propose the Offline Request Orchestration (ORO) and Online Request Orchestration (OLRO) algorithms, designed to minimize SFC migration frequency, service delay, service jitter, and enhance load balancing. Leveraging resource distribution across multiple time slots, these algorithms schedule the long-lasting, high-performance SFC within STIN. Evaluation and simulation results demonstrate that our proposed model and algorithms significantly outperform conventional solutions, achieving significant improvements in model complexity, SFC migration frequency, workload balancing, service delay, and jitter.},
  archive      = {J_TMC},
  author       = {Yepeng Liu and Ran Zhang and Jiang Liu and Ninghan Sun and Xinyuan Zhang},
  doi          = {10.1109/TMC.2025.3619832},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VMR-STAG based online SFC orchestration in space-terrestrial integrated networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust DNN partitioning and resource allocation under uncertain inference time. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3619509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge intelligence systems, deep neural network (DNN) partitioning and data offloading can provide real-time task inference for resource-constrained mobile devices. However, the inference time of DNNs is typically uncertain and cannot be precisely determined in advance, presenting significant challenges in ensuring timely task processing within deadlines. To address the uncertain inference time, we propose a robust optimization scheme to minimize the total energy consumption of mobile devices while meeting task probabilistic deadlines. The scheme only requires the mean and variance information of the inference time, without any prediction methods or distribution functions. The problem is formulated as a mixed-integer nonlinear programming (MINLP) that involves jointly optimizing the DNN model partitioning and the allocation of local CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first decompose the original problem into two subproblems: resource allocation and DNN model partitioning. Subsequently, the two subproblems with probability constraints are equivalently transformed into deterministic optimization problems using the chance-constrained programming (CCP) method. Finally, the convex optimization technique and the penalty convex-concave procedure (PCCP) technique are employed to obtain the optimal solution of the resource allocation subproblem and a stationary point of the DNN model partitioning subproblem, respectively. The proposed algorithm leverages real-world data from popular hardware platforms and is evaluated on widely used DNN models. Extensive simulations show that our proposed algorithm effectively addresses the inference time uncertainty with probabilistic deadline guarantees while minimizing the energy consumption of mobile devices.},
  archive      = {J_TMC},
  author       = {Zhaojun Nan and Yunchu Han and Sheng Zhou and Zhisheng Niu},
  doi          = {10.1109/TMC.2025.3619509},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust DNN partitioning and resource allocation under uncertain inference time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPNet: Multi-stage progressive convolutional neural networks for trajectory prediction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3619573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction is a continuing concern within autonomous vehicles. Psychological research shows that pedestrian traveling is a cyclic alternation. Pedestrians constantly interact with their surroundings, including social agents and physical environments, and plan paths to achieve goals. Nevertheless, most existing trajectory prediction methods are based on a single-stage design, which runs counter to traffic psychology principles. In this work, we present MPNet, a novel multi-stage progressive convolutional neural network that decomposes complicated trajectory prediction into multiple manageable components, where lightweight sub-networks handle each stage with a divide-and-conquer methodology. Specifically, our sub-networks are based on the encoder-decoder architecture, in which we capture interactive information and estimate goals to achieve trajectory prediction. The communication among sub-networks depends on a novel cross-stage fusion design. We introduce a feedback channel mutual attention mechanism and a cross-sub-network fusion unit to enable efficient information sharing across different stages. At each stage, we develop a symmetric gated supervision module to supervise future trajectory generation from coarse to fine. Extensive experiments demonstrate that MPNet achieves state-of-the-art performance, reducing Average Displacement Error (ADE) and Final Displacement Error (FDE) by 5.6%/7.4% on the ETH and UCY Datasets, 7.5%/7.7% on the Stanford Drone Dataset, and 7.1%/22.5% on the Intersection Drone Dataset, while maintaining comparable computational complexity. Additionally, our MPNet-Tiny variant reduces parameters by 90.5% and inference time by 1.83 seconds with competitive accuracy.},
  archive      = {J_TMC},
  author       = {Huihui Pan and Changzhi Yang and Jue Wang and Yuanduo Hong},
  doi          = {10.1109/TMC.2025.3619573},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MPNet: Multi-stage progressive convolutional neural networks for trajectory prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2FDP-BRL: A new framework of distributed task offloading for IoAV in extreme weather scenarios. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3604461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Autonomous Vehicles (IoAV), task offloading is crucial for managing tasks that require extensive computing power to guarantee vehicle safety under different weather scenarios. However, extreme weather events can lead to infrastructure damage and network disruptions, significantly increasing the computational demands of autonomous vehicles. These vehicles require additional computing resources to navigate complex road conditions and risks, all while facing a high degree of uncertainty, such as fluctuations in vehicle resource utilization and task workloads. To address these challenges, a new and lightweight task offloading decision framework, named 2FDP-BRL, has been first proposed in this paper. This framework not only considers the fast response time required for autonomous driving, but also considers the resource shortage and offloading uncertainty caused by extreme weather. Therefore, we introduce the dynamic pricing idea and the Interval Type-2 Fuzzy Inference System (IT2FIS) utilizing broad reinforcement learning to deal with various dynamic uncertainties in the IoAV under extreme weather. For the authenticity of experimental results, we utilize the VISSIM platform to collect experimental data and conduct simulations. Moreover, to accurately simulate extreme weather scenarios, we also account for the variability of infrastructure and road elements, including reduced transmission rates and decreased efficiency in executing tasks. Furthermore, to enhance the realism of the simulation, we incorporate historical weather data from NOAA for Shenyang in 2024 to model dynamic uncertainties under extreme weather conditions and conduct comparative experimental analyses focusing on task completion rates. Finally, the proposed framework was implemented on both a local setup and the Huawei Atlas 200I DK A2 device, illustrating its efficacy design.},
  archive      = {J_TMC},
  author       = {Xiting Peng and Shun Song and Xiaoyu Zhang and Mianxiong Dong and Kaoru Ota and Lexi Xu},
  doi          = {10.1109/TMC.2025.3604461},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {2FDP-BRL: A new framework of distributed task offloading for IoAV in extreme weather scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sniffing the application usage information with the leakage current of laptops. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart devices are proliferating in every aspect of our lives, providing convenience but also exposing us to the risk of information leakage at any moment. Attackers can monitor the user and infer private information such as personality and preferences by stealing the behavioral information. In this paper, we investigated the potential threat of information stealing via the leakage current of laptops and electrodes in wearable devices (e.g., smart watches and bracelets). Specifically, the leakage current in the laptop adapter can flow from the metal casing into the human body and be collected by electrodes in wearable devices when the user is using a laptop with a metal casing (e.g., MacBook). We verified the correlation between leakage current and the working states of the laptop, where different operations corresponding to different CPU instructions can generate different leakage currents. Based on this, we propose LeakThief, a system that consists of three components: leakage current detection, application operation detection, and application recognition. The experiments in a real-world environment demonstrated that the proposed system can recognize 25 common applications with high accuracy, including launching-based (96.4%) and in-application operation-based recognition (81.2%).},
  archive      = {J_TMC},
  author       = {Dian Ding and Yijie Li and Yongzhao Zhang and Yi-Chao Chen and Xiaoyu Ji and Guangtao Xue},
  doi          = {10.1109/TMC.2025.3599338},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sniffing the application usage information with the leakage current of laptops},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rising from pieces: Effective inference at the edge via robust split ML. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3605382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing processing demands of today's mobile deep learning applications impose stringent requirements on edge devices. Offloading these tasks to the cloud, while being a potential solution, often results in significant data transfer overhead, as well as privacy and connectivity concerns. To address these challenges, split machine learning (split ML) has emerged as an innovative paradigm, enabling task distribution among edge devices themselves. However, split ML systems inherently exhibit instability due to the hardware and communication limitations of mobile devices, which frequently result in failures and malfunctions of client nodes. In light of these challenges, we present Axolotl, a fault-tolerant edge split ML inference system for addressing node failure with minimal performance impact. Specifically, we first design a novel curriculum dropout mechanism to enhance the model's resilience by gradually exposing it to potential server node failures. We then design inverse-proximal weight consolidation to mitigate catastrophic forgetting caused by curriculum dropout. To further tackle potential node failures, we innovate in a resource-aware substitution module that offload the functions of a failed node to neighboring ones, ensuring efficient information flow. Extensive experiments demonstrate the effectiveness and robustness of Axolotl in various deep learning networks and tasks in edge environments.},
  archive      = {J_TMC},
  author       = {Yuxuan Weng and Tianyue Zheng and Zhe Chen and Menglan Hu and Jun Luo},
  doi          = {10.1109/TMC.2025.3605382},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Rising from pieces: Effective inference at the edge via robust split ML},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uplink resource allocation for RSMA-aided digital twin-assisted user-centric cell-free massive MIMO systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3604722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates uplink radio resource optimization of a user-centric (UC) cell-free (CF) massive multiple-input multiple-output (mMIMO) system aided by the rate splitting multiple access (RSMA) technique subject to pilot contamination. We formulate problem to maximize the minimum spectral efficiency (SE) problem by jointly addressing decoding order selection, power allocation, and access point (AP) - user equipment (UE) association assignment. The envisioned optimization exhibits two challenges. First, it requires global channel state information (CSI) for near-optimal performance, which incurs substantial overhead and data collection costs in large-scale CF networks. Second, the optimization is intractable due to its NP-hard and discrete non-linear programming nature. To address the CSI acquisition issue, we utilize a digital twin (DT) of the CF mMIMO system, leveraging its context-awareness to acquire global CSI with reduced overhead. To address computational intractiablity of the optimization problem, we decompose it into three sub-problems. The power allocation sub-problem is transformed into a second-order cone programming problem and solved by the bisection method. Additionally, we propose a computationally efficient heuristic approach for power allocation. Next, we propose an analytical method for the decoding order selection by ranking the channels in descending order of strength. Simulation results validate the ability of the proposed approach to attain the near-optimal performance. Subsequently, the AP-UE association assignment problem is solved by a heuristic approach to further improve the SE performance. Finally, we solve the original NP-hard problem in a unified manner via the block-coordinate descent algorithm. Simulation results underscore a substantial 61% improvement in the SE performance when integrating the RSMA technique into a UC CF mMIMO system.},
  archive      = {J_TMC},
  author       = {Manobendu Sarker and Md. Zoheb Hassan and Georges Kaddoum and Abraham O. Fapojuwo},
  doi          = {10.1109/TMC.2025.3604722},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Uplink resource allocation for RSMA-aided digital twin-assisted user-centric cell-free massive MIMO systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure charging scheduling in wireless rechargeable sensor networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Rechargeable Sensor Networks (WRSNs) promise to address the limited energy resource issue for sensor nodes through wireless power transfer technology. However, WRSNs are vulnerable to various security threats, such as compromised node attack and malicious mobile charger (MC) attack, which can disrupt the charging process and degrade charging efficiency. In this work, we investigate the eneRgy conversion Efficiency maximization problem unDer chargIng attackS (REDIS). We propose a blockchain-based framework that employs a lightweight multi-layer storage approach tailored for resource-constrained sensor nodes and features consensus algorithms that validate charging transactions. Furthermore, we introduce a validation node selection strategy that integrates consensus execution with charging scheduling, reducing energy consumption, and improving energy efficiency. Extensive simulations and experiments validate the effectiveness of our framework, improving energy efficiency by 30% and as much as 5 times in networks without attacks and those under full attacks, respectively.},
  archive      = {J_TMC},
  author       = {Wei Yang and Chi Lin and Jing Deng and Haipeng Dai and Liming Chen and Xinxin Fan and Li Zhang},
  doi          = {10.1109/TMC.2025.3605390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure charging scheduling in wireless rechargeable sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully anonymous broadcast signcryption for secure health data transmission in WBANs. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support comprehensive and personalized healthcare delivery, sensitive personal health data must often be transmitted from wearable devices to multiple designated specialists via wireless body area networks. Although existing identity-based broadcast signcryption schemes provide secure one-to-many transmission, they typically fail to preserve recipient anonymity. In particular, each recipient is usually required to know the identities of all other recipients to unsigncrypt the message, which undermines privacy and enables inference attacks, especially problematic in telemedicine applications. In this paper, we propose a novel broadcast signcryption scheme that simultaneously achieves recipient anonymity, sender anonymity, and resilience to physical attacks, which are not realized in prior work. Our key innovation lies in an anonymity-preserving unsigncryption mechanism based on inner product, which enables each recipient to independently verify and decrypt the broadcast message without knowledge of others' identities. In addition, we incorporate a physical unclonable function with a fuzzy extractor to resist physical attacks in the presence of noise. Furthermore, a comprehensive security analysis demonstrates that our scheme satisfies essential security properties, including authentication, integrity, confidentiality, and full anonymity. It is resilient against common attacks such as impersonation, modification, replay, and man-in-the-middle attacks. Finally, performance evaluations confirm the practical feasibility of our scheme.},
  archive      = {J_TMC},
  author       = {Yangfan Liang and Gao Liu and Xianchao Zhang and Yiming Chen and Jingxue Chen and Yuanjun Xia and Yining Liu},
  doi          = {10.1109/TMC.2025.3605205},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fully anonymous broadcast signcryption for secure health data transmission in WBANs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIGC-enhanced federated learning: Addressing data scarcity in preference-based scenarios. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3605397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a decentralized machine learning paradigm that enables collaborative model training while preserving data privacy by avoiding sensitive data exfiltration from local storage. Despite its success in various domains, current FL frameworks lack mechanisms to accommodate personalized training objectives, particularly in optimizing performance for specific data classes. Recent advancements in Artificial Intelligence Generated Content (AIGC) present opportunities to address these limitations by supplementing training data for user-preferred classes using generative models. Nonetheless, incorporating AIGC into FL introduces significant challenges, including non-compliant data quality, disorganized data distributions, limited computational resources, and slow data generation speed on edge devices. To address these challenges, we propose AIGC-enhanced Federated Preference Learning (FPL), a novel framework designed to enhance FL performance for user-specified preference classes (PCs). Our approach employs pre-training and fine-tuning of generative models across diverse datasets to improve the quality of synthetic data. Additionally, we optimize FPL efficiency through a client selection strategy that matches tasks involving generated data with suitable clients and a data distribution mechanism that allocates synthetic data to where it is most needed. To further accelerate data supplementation, data augmentation is utilized on local clients. We provide theoretical convergence guarantees for AIGC-enhanced FPL and demonstrate its effectiveness through comprehensive experiments on MNIST and CIFAR-10 datasets, including ablation studies on various data supplementation techniques.},
  archive      = {J_TMC},
  author       = {Chenyu Wang and Zhi Zhou and Zixin Xu and Zhijie Wang and Shaoquan Wang and Xu Chen},
  doi          = {10.1109/TMC.2025.3605397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIGC-enhanced federated learning: Addressing data scarcity in preference-based scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-assisted message reporting scheme with weighted threshold signature for vehicular ad-hoc networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicular ad-hoc networks (VANETs), message reporting is an effective method for improving traffic safety and efficiency. Most existing VANET message reporting schemes rely on the trust value of a single vehicle to determine message authenticity, which leads to unreliable message sources. Even multi vehicle-assisted reporting schemes are limited by the assumption that all vehicles have the same credibility, which does not reflect the actual dynamic VANET environment in which vehicles have different credibilities. To address this issue, we propose a blockchain-assisted VANET message reporting scheme with weighted threshold signatures. Through the design of weights, the credibility of different vehicles is quantified, and the impact of vehicles on the signing process is differentiated. Threshold signature generation relies on the weight sum of all signatories reaching a predetermined threshold, to enable flexible and reliable message reporting. Security analysis shows that our proposed scheme combined with blockchain can satisfy the security and privacy requirements of VANET message reporting. Performance analysis indicates that our proposed scheme outperforms the most advanced VANET message reporting schemes in terms of transmission and computation performance.},
  archive      = {J_TMC},
  author       = {Ru Li and Jie Cui and Jing Zhang and Lu Wei and Hong Zhong and Debiao He},
  doi          = {10.1109/TMC.2025.3605858},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-assisted message reporting scheme with weighted threshold signature for vehicular ad-hoc networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AppGen: Mobility-aware app usage behavior generation for mobile users. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile app usage behavior reveals human patterns and is crucial for stakeholders, but data collection is costly and raises privacy issues. Data synthesis can address this by generating artificial datasets that mirror real-world data. In this paper, we propose AppGen, an autoregressive generative model designed to generate app usage behavior based on users' mobility trajectories, improving dataset accessibility and quality. Specifically, AppGen employs a probabilistic diffusion model to simulate the stochastic nature of app usage behavior. By utilizing an autoregressive structure, AppGen effectively captures the intricate sequential relationships between different app usage events. Additionally, AppGen leverages latent encoding to extract semantic features from spatio-temporal points, guiding behavior generation. These key designs ensure the generated behaviors are contextually relevant and faithfully represent users' environments and past interactions. Experiments with two real-world datasets show that AppGen outperforms state-of-the-art baselines by over 12% in critical metrics and accurately reflects real-world spatio-temporal patterns. We also test the generated datasets in applications, demonstrating their suitability for downstream tasks by maintaining algorithm accuracy and order.},
  archive      = {J_TMC},
  author       = {Zihan Huang and Tong Li and Yong Li},
  doi          = {10.1109/TMC.2025.3605939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AppGen: Mobility-aware app usage behavior generation for mobile users},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDGE360: Edge-enabled multi-agent DRL for region-aware rate adaptation solution to enhance quality of 360° video streaming. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal tile-based bitrate allocation improves the Quality of Experience (QoE) for adaptive 360° video streaming across multiple clients in heterogeneous network environments; however, it is challenging as it implies accurate viewport prediction, finest tile-based bitrate reservation, and maintaining QoE fairness, particularly under constrained network conditions. This paper proposes a strategy named EDGE360, that employs an edge-driven Multi-Agent Deep Reinforcement Learning (MADRL) solution for rate adaptation to improve the joint QoE in DASH-based rich media content delivery based on adaptive viewport prediction and Video Multi-method Assessment Fusion (VMAF) corresponding tiling granularity selection. Cooperative strategies among agents in the central critic network are crucial for addressing the complexity of network instances at the edge and optimizing media streaming bitrate assignment in multiple-client scenarios. Therefore, EDGE360 aims to implement the Counterfactual Multi-Agent Policy Gradients (COMA) based on 5G network traces to train agents in policies that optimize individual client QoE and fairness among clients, resulting in an improved rich streaming experience. At the edge, a tile-based quality monitor evaluates viewport trajectories, buffer status, and network throughput, employing deep learning to forecast optimal tile bitrate allocation, which is formulated as an MDP and solved with MADRL. Based on extensive experimentation, EDGE360 surpasses state-of-the-art adaptive bitrate algorithms by achieving the highest average reward, outperforming RAPT360, 360SRL, and BOLA360 by 8.12%, 11.86%, and 18.00%, respectively, demonstrating superior convergence and refinement.},
  archive      = {J_TMC},
  author       = {Fazal E Subhan and Abid Yaqoob and Cristina Hava Muntean and Gabriel-Miro Muntean},
  doi          = {10.1109/TMC.2025.3605849},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EDGE360: Edge-enabled multi-agent DRL for region-aware rate adaptation solution to enhance quality of 360° video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EOC-tracking: An environmental obstacles constrained adaptive wi-fi tracking framework. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi device-free tracking enables the inference of user behaviors without physical contact, which is crucial for intelligent indoor location-based services. Nevertheless, the practical implementation of current tracking systems is constrained by several critical limitations: 1) The low-quality sensing signals in complex scenarios lead to increased tracking errors; 2) Existing methods inadequately adjust to dynamic environments, necessitating additional data collection or retraining processes. To address these challenges, this paper introduces EOC-Tracking, a device-free Wi-Fi tracking system that dynamically incorporates environmental information. Our key innovation involves leveraging obstacles to correct illogical users' trajectories and facilitate adjustment to varying environments. This significantly improves the accuracy of the follow-up in complex and changing environments. The EOC-Tracking system is built upon three fundamental design principles: 1) A lightweight dual-branch neural network architecture that effectively fuses environmental data with Wi-Fi signal characteristics; 2) An autonomous map updating mechanism that facilitates real-time adaptation to environmental layout modifications without human intervention; 3) A sophisticated data-driven, phased training paradigm that optimizes the model's ability to learn and apply obstacle constraints. We implement EOC-Tracking using commercial Wi-Fi devices and deploy it on low-power embedded systems such as the MCU. Experimental results demonstrate that EOC-Tracking can reduce tracking errors by at most 49.48% compared to datadriven methods and 62.21% compared to model-based methods in various complex scenarios.},
  archive      = {J_TMC},
  author       = {Jinwei Gao and Qixuan Cai and Mengjie Yu and Xinyu Tong and Tony Xiao Han and Xiulong Liu and Xin Xie and Wenyu Qu},
  doi          = {10.1109/TMC.2025.3605936},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EOC-tracking: An environmental obstacles constrained adaptive wi-fi tracking framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service-oriented segmented trajectory design for low-altitude UAV-assisted MEC networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3605865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the integration of Unmanned Aerial Vehicles (UAV) with Internet of Things (IoT) infrastructure to enhance Mobile Edge Computing capabilities in urban environments. While UAVs offer promising solutions for mobile edge computing, their deployment in high-rise urban areas presents significant challenges, particularly in computational resource balancing, energy-efficient trajectory planning, and dynamic IoT service provisioning. We propose a comprehensive low-altitude UAV-assisted mobile edge computing framework that jointly optimizes UAV trajectory planning, the assignment of offloaded tasks to specific UAVs, and the strategic deployment and energy management of the UAV fleet to maximize system utility. We first formulate this as a multi-objective optimization problem and prove its NP-hardness due to its non-convex and integer linear programming nature. To tackle this challenge, we develop a decomposition-based approach that systematically addresses the coupled variables. We then propose a novel Variable Strategy Reinforcement Learning-based Lin-Kernighan-Helsgaun algorithm that synergistically combines Q-learning, Sarsa, and Monte Carlo methods with the LKH algorithm. The proposed solution is further enhanced by incorporating two refined trajectory optimization mechanisms, the Trajectory Refining Algorithm and the Service-Oriented Segmented Trajectory Refining Algorithm, specifically designed to improve the robustness and reliability in solving the Computation Offloading Trajectory Optimization Problem. Extensive simulation results demonstrate that our proposed algorithms consistently outperform state-of-the-art approaches, achieving faster convergence, higher energy efficiency for UAVs, and lower computational latency for IoT devices.},
  archive      = {J_TMC},
  author       = {Pengfei Wu and Fu Xiao and Chao Sha and Haiping Huang},
  doi          = {10.1109/TMC.2025.3605865},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service-oriented segmented trajectory design for low-altitude UAV-assisted MEC networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P2TS: A preemptive approach for priority-aware task scheduling in computing power networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3606454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging computing paradigm, Computing Power Networks (CPNs) are dedicated to coordinating and managing network resources and computing resources to achieve interconnectivity in computing power perception. Efficient collaborative computing of massive data can be achieved through the scheduling function of CPNs. However, existing scheduling research mainly focuses on selecting network links and computing nodes, lacking consideration for task execution after scheduling, which may degrade the Quality of Service (QoS), leading to widespread failures and significant losses. To address this issue, we design a priority-aware preemptive task scheduling (P2TS) strategy for CPNs to jointly optimize task scheduling and execution in terms of success rate, average processing delay, and load balancing. Specifically, at the execution level, we propose a priority-aware preemptive mechanism (P2M) to optimize post-scheduling task execution. Then, at the scheduling level, we apply deep reinforcement learning (DRL) to optimize the scheduling process supporting the P2M in CPNs. A series of simulations are conducted to demonstrate the superiority of our strategy.},
  archive      = {J_TMC},
  author       = {Tao Huang and Haoxiang Qiu and Qinqin Tang and Li Feng and Renchao Xie and Tianjiao Chen and Zehui Xiong},
  doi          = {10.1109/TMC.2025.3606454},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {P2TS: A preemptive approach for priority-aware task scheduling in computing power networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative highly-maneuvering target tracking using multi-AUV networks: A bearing-only approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3606690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target tracking is a fundamental technology for marine development, providing real-time position estimates of the interested targets. However, due to the harsh underwater environment and the noncooperativity of targets, improving tracking accuracy remains a challenge, especially for highly-maneuvering targets. To address this problem, based on multi-autonomous underwater vehicle (multi-AUV) networks, this paper extends the idea of interacting multiple models (IMM) and designs a bearing-only cooperative tracking algorithm in the consideration of the harsh underwater acoustic channels. Specifically, in position prediction, the combination of historical information and the concept of IMM reduces the severe time-lagged effect in traditional prediction methods and the model reliance in standard IMM filters. Then, during position update, a rigidity-assisted relative position representation is designed based solely on bearing measurements, which alleviates the impact of information loss due to communication interruptions, significantly enhancing the continuity of target tracking. Moreover, the algorithm design also considers various uncertainties that may concurrently occur underwater (e.g., error accumulation and model mismatches), and robust optimization strategies with the principle of maximum entropy are designed to enhance the environmental adaptability. Through various simulations and field experiments, the advantages of the proposed method have been validated.},
  archive      = {J_TMC},
  author       = {Yichen Li and Yang Yang and Wenbin Yu and Xinping Guan},
  doi          = {10.1109/TMC.2025.3606690},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cooperative highly-maneuvering target tracking using multi-AUV networks: A bearing-only approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic multi-modal UAV control for optimized coverage and backhaul connectivity in spatially unstructured and dispersed user environments. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3606778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have emerged as a promising solution for establishing wireless communications in regions lacking terrestrial network infrastructure, such as remote or emergency areas. Deploying UAV networks effectively in these scenarios poses significant challenges due to the unknown and potentially complex locations of users. In scenarios where users are dispersed in intricate spatial patterns, achieving high coverage and resilient network connectivity among the UAV networks is challenging. The irregular and arbitrary distribution of users can lead to gaps in coverage, as traditional UAV placement optimization approaches are often unable to adapt to such dynamic environments. This complexity necessitates advanced strategies to ensure reliable and continuous network service to users. In this paper, we propose a distributed approach that leverages flocking dynamics and distributed consensus algorithms for dynamic UAV positioning. By enabling a multi-modal UAV operation policy, we develop a framework which enables the network to dynamically respond to complex user locations and establish backhaul connectivity between dispersed user clusters. Simulation results demonstrate that our approach successfully establishes a robust and adaptable UAV network capable of providing seamless coverage for complex user configurations and also ensuring comprehensive inter-cluster connectivity among dispersed user clusters. Additionally, the network exhibits strong resilience against random failures, swiftly recovering from disruptions to ensure stable and reliable communication even when UAVs are compromised.},
  archive      = {J_TMC},
  author       = {Yuhui Wang and Junaid Farooq and Juntao Chen},
  doi          = {10.1109/TMC.2025.3606778},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic multi-modal UAV control for optimized coverage and backhaul connectivity in spatially unstructured and dispersed user environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaestroBot: Generalized gesture-driven hierarchical coordination for robotic formations. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3606847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic swarm coordination holds transformative potential for applications such as warehouse automation, search & rescue, and entertainment. However, approaches relying on wearable devices or vision-based systems are often constrained by hardware-intensive, high computational requirements, reliance on line-of-sight, and privacy concerns. Wireless sensing, particularly using Channel State Information (CSI), offers a promising alternative by translating environmental perturbations into CSI variation data. Nevertheless, existing CSI-based systems face significant challenges in domain adaptation, resource limitation, and scalability issues. This paper introduces MaestroBot, a hierarchical motion coordination system that combines distributed CSI-based wireless sensing with domain-adaptive learning to address these limitations. For leader robots, the system features a lightweight hand gesture recognition model, built on a “Hybrid-Single” knowledge distillation framework, achieving up to 95.87% accuracy while maintaining adaptability across diverse domains. For follower robots, the hierarchical motion propagation model leverages localized CSI analysis and dual-layer error correction mechanisms to deliver 97.2% accuracy with a low latency of 0.085 seconds, even in multi-row formations. Additionally, its cost-effective hardware design ensures practical scalability and real-world deployability. These results position MaestroBot as an efficient, robust, and privacy-preserving solution for large-scale robotic swarm coordination in dynamic environments.},
  archive      = {J_TMC},
  author       = {Yutong Liu and Zhiye Wang and Yuhan Xu and Yang Yue and Haiming Jin and Linghe Kong and Rui Li and Xi Chen and Qiao Xiang and Jun Zhang and Guihai Chen},
  doi          = {10.1109/TMC.2025.3606847},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MaestroBot: Generalized gesture-driven hierarchical coordination for robotic formations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility resilient vehicular federated learning: Enhancing training efficiency in dynamic environments. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicular environment presents unique challenges, including massive data generation, stringent latency requirements for safety-critical applications, bandwidth limitations, and intermittent connectivity, which make centralized learning approaches impractical. Vehicular Federated Learning (VFL) enables distributed model training by leveraging local data from connected vehicles, while preserving data privacy and reducing network overhead. However, the dynamic nature of VFL presents several additional challenges. High vehicle mobility and unstable channels lead to inconsistent client participation, while heterogeneous vehicle capabilities result in unbalanced training workloads and competitive resource allocation. These challenges significantly degrade VFL model performance and prolong training periods. In this paper, we propose a Mobility Resilient Vehicular Federated Learning (MR-VFL) scheme, which comprises two key components: an amplification-based adaptive vehicular FL (AVFL) training scheme and a dual-timescale FL scheduler. Specifically, AVFL adapts local training epochs to vehicle capabilities to improve scheduling flexibility and alleviate the impact of insufficient local epochs on model updates, which enhances training efficiency and reduces communication competition. The dual-timescale FL scheduler includes a macro scheduling strategy that optimizes long-term VFL performance based on the correlation between convergence speed and model accuracy, and a Mamba-based real-time scheduler that enhances training efficiency and reduces decision latency in massive vehicles scenarios. Extensive simulations show that MR-VFL effectively mitigates performance degradation due to complex vehicle mobility and heterogeneity, and improves training efficiency.},
  archive      = {J_TMC},
  author       = {Tianao Xiang and Yuanguo Bi and Lin Cai and Mingjian Zhi},
  doi          = {10.1109/TMC.2025.3607138},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility resilient vehicular federated learning: Enhancing training efficiency in dynamic environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilient topological control for dynamic underwater optical wireless networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Wireless Optical Networks (UWONs) play a critical role in tasks such as ocean monitoring and resource exploration, which require high connectivity and reliability. However, water turbidity, ocean currents, and ambient light noise significantly affect communication stability, creating serious deployment challenges. To address this, we propose a network topology optimization method based on resilience evaluation. First, a resilience evaluation method is designed to quantify the network's ability to adapt to disturbances. Then, an improved predecessor-based evolutionary algorithm (Pred-EA) is used for resilience-guided topology optimization. To improve algorithmic efficiency and search quality, the prim algorithm is introduced to ensure chromosome feasibility, enhancing both the diversity of the initial population and computational efficiency. Experimental results show that our method achieves better recovery performance than three comparison methods in all scenarios. The average number of recovered edges improves by up to 18.30% over the second-best method. Under non-recoverable conditions, resilience improves by up to 20.52%. These results confirm the strong topological robustness and practical value of the proposed method in dynamic underwater environments.},
  archive      = {J_TMC},
  author       = {Qing Zhang and Youling Huang and Lin Lin and Chi Lin},
  doi          = {10.1109/TMC.2025.3607035},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resilient topological control for dynamic underwater optical wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quick-pass continuous authentication with real-time biometrics extraction on COTS earphones using out-ear microphones. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3606846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous authentication is increasingly critical for cyber security. However, existing approaches are time-consuming due to their simplistic signal modulation and low efficiency in feature extraction. In this paper, we propose a continuous authentication technique, OnePiece. OnePiece is free from the requirement of in-ear microphones, which are necessary for existing earphone authentication systems. It exploits out-ear microphones for biometrics extraction, which are ubiquitous on off-the-shelf earphones. We analyze the acoustic response model of ears towards out-ear microphones via the air, which is different from that towards in-ear microphones. A frequency-varying ultrasonic modulation scheme is proposed to characterize in-depth ear biometrics in user-friendly, error-free, and time-efficient ways. Therefore, OnePiece enables quick-pass authentication once users wear the earphones, followed by continuous authentication covering the whole course. Moreover, we propose a wake-up mechanism to reduce the consumed power, which addresses the key power consumption issue in ultrasonic sensing techniques. Particularly, OnePiece can be smoothly deployed on off-the-shelf wired and wireless earphones. It performs good cross-device performance in which users just register only once. Extensive evaluations are conducted to validate its effectiveness under real-world scenarios.},
  archive      = {J_TMC},
  author       = {Ming Gao and Jiatong Chen and Xin Tong and Ruitong Ye and Yike Chen and Fu Xiao and Jinsong Han},
  doi          = {10.1109/TMC.2025.3606846},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quick-pass continuous authentication with real-time biometrics extraction on COTS earphones using out-ear microphones},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient iTreeKEM-based group key agreement protocol for flying ad-hoc networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Flying Ad-hoc Network (FANET) evolves toward larger scales and higher levels of autonomy, the importance of secure and efficient group communication continues to grow. However, resource-constrained unmanned aerial vehicles (UAVs) face dual challenges: limited computational power struggles to meet the high demands of complex cryptographic algorithms, while bandwidth constraints exacerbate communication overhead caused by multi-round interaction mechanisms. Moreover, existing solutions find it hard to support dynamic group environments and are prone to single point of failure (SPoF) in centralized architectures, which significantly compromises system reliability and scalability. To address these issues, this paper proposes a novel key agreement protocol for FANET. The protocol employs an improved tree-based key encapsulation mechanism (iTreeKEM) to support rapid key updates in highly dynamic environments. It reduces the computational cost for each group member by 90.08% even when the group size reaches 128. To further enhance system robustness, the protocol introduces a smart contract-based distributed leader election mechanism, effectively eliminating SPoF. The security of the proposed protocol is guaranteed by the CDH problem under the generalized selective decryption (GSD) model. Finally, we implement the protocol in NS-3 simulations, and the results demonstrate its effective applicability to FANET.},
  archive      = {J_TMC},
  author       = {Tianqi Zhou and Shijia Hong and Jian Shen and Md Zakirul Alam Bhuiyan and Pandi Vijayakumar and Debiao He},
  doi          = {10.1109/TMC.2025.3607316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient iTreeKEM-based group key agreement protocol for flying ad-hoc networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Microservice deployment in space computing power networks via robust reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for Earth observation, it is important to provide reliable real-time remote sensing inference services to meet the low-latency requirements. The Space Computing Power Network (Space-CPN) offers a promising solution by providing onboard computing and extensive coverage capabilities for real-time inference. This paper presents a remote sensing artificial intelligence applications deployment framework designed for Low Earth Orbit satellite constellations to achieve real-time inference performance. The framework employs the microservice architecture, decomposing monolithic inference tasks into reusable, independent modules to address high latency and resource heterogeneity. This distributed approach enables optimized microservice deployment, minimizing resource utilization while meeting quality of service and functional requirements. We introduce Robust Optimization to the deployment problem to address data uncertainty. Additionally, we model the Robust Optimization problem as a Partially Observable Markov Decision Process and propose a robust reinforcement learning algorithm to handle the semi-infinite Quality of Service constraints. Our approach yields sub-optimal solutions that minimize accuracy loss while maintaining acceptable computational costs. Simulation results demonstrate the effectiveness of our framework.},
  archive      = {J_TMC},
  author       = {Zhiyong Yu and Yuning Jiang and Xin Liu and Yuanming Shi and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3607488},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Microservice deployment in space computing power networks via robust reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CI-HDA: Heterogeneous deniable authentication based on CLC and IBC for location privacy in edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing improves the performance of Internet of Things (IoT) devices by moving cloud services closer to where these devices operate, thereby reducing delays and saving bandwidth. However, the IoT devices communicate wirelessly with edge servers, which creates a significant challenge in keeping the devices' locations private, especially when they use different methods. To address this, researchers have proposed various methods. However, these methods require a lot of computational power and storage, which makes them unsuitable for edge computing environments. To address these challenges, we propose a certificateless cryptography (CLC) and identity-based cryptography (IBC) based heterogeneous deniable authentication (CI-HDA) scheme. It enables an IoT device operating CLC to securely communicate with an edge server using IBC. The server verifies the devices authenticity without proving its participation to third parties, which ensures location privacy in edge computing environments. Additionally, our scheme supports batch verification, which enables the server to efficiently validate multiple authenticators simultaneously. The security of CI-HDA scheme is formally proven in the random oracle model, and performance evaluations demonstrate reduced computational and communication/storage overheads compared to existing methods. We also explored its application in military surveillance, which highlights its practicality in privacy-sensitive edge computing environments.},
  archive      = {J_TMC},
  author       = {Ikram Ali and Jianqiang Li and Jie Chen and Yong Chen and Shamsher Ullah and Abdul Wakeel},
  doi          = {10.1109/TMC.2025.3607414},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CI-HDA: Heterogeneous deniable authentication based on CLC and IBC for location privacy in edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Through-wall cross-domain user identification via lip movement micro-doppler and MIMO radar: An unsupervised domain adaptation approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip movement-based user identification holds significant promise for public security and intelligent surveillance due to its dynamic patterns, forgery resistance, and individual distinctiveness. Recently, millimeter-wave radar has been employed for contactless identification, offering advantages such as light insensitivity, privacy preservation, and sensitivity to fine motion. However, its limited wall penetration and vulnerability to occlusion present ongoing challenges. Moreover, existing recognition approaches rely heavily on supervised learning, demanding large labeled datasets and exhibiting poor generalization across domains. To overcome these limitations, we propose Lip-TWCDID, a lip movement-based cross-domain user identification system using 1–2 GHz MIMO radar. The use of low-frequency signals enhances penetration, while the MIMO architecture improves spatial resolution, enabling stable detection of fine-grained micro-Doppler signatures of lip movements through a 22 cm brick wall. To reduce dependence on labeled data and improve domain generalization, we introduce a novel unsupervised domain adaptation (UDA) framework, consistency-adversarial-contrastive learning (CACL), which integrates pseudo-label consistency learning, domain adversarial training, and pseudo-supervised contrastive learning. Specifically, pseudo-label consistency enforces prediction consistency under input perturbations, improving robustness; domain adversarial training introduces a domain discriminator to encourage domaininvariant feature learning and align feature distributions; pseudosupervised contrastive learning leverages high-confidence pseudolabels to perform contrastive learning in the feature space, enhancing inter-class separability and intra-class compactness. By jointly optimizing these components, CACL effectively adapts to unlabeled target domains while minimizing annotation costs. Extensive experiments demonstrate that CACL outperforms state-of-the-art UDA methods and significantly improves the generalization and robustness of through-wall user identification.},
  archive      = {J_TMC},
  author       = {Kai Yang and Dongsheng Zhu and Yujie Xu and Chong Han and Jian Guo and Lijuan Sun},
  doi          = {10.1109/TMC.2025.3607413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Through-wall cross-domain user identification via lip movement micro-doppler and MIMO radar: An unsupervised domain adaptation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware multi-task decentralized federated learning for vehicular networks: Modeling, analysis, and optimization. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising paradigm that can enable collaborative model training between vehicles while protecting data privacy, thereby significantly improving the performance of intelligent transportation systems (ITSs). In vehicular networks, due to mobility, resource constraints, and the concurrent execution of multiple training tasks, how to allocate limited resources effectively to achieve optimal model training of multiple tasks is an extremely challenging issue. In this paper, we propose a mobility-aware multi-task decentralized federated learning (MMFL) framework for vehicular networks. By this framework, we address task scheduling, subcarrier allocation, and leader selection, as a joint optimization problem, termed TSLP. For the case with a single FL task, we derive the convergence bound of model training. For general cases, we first model TSLP as a resource allocation game, and prove the existence of a Nash equilibrium (NE). Then, based on this proof, we reformulate the game as a decentralized partially observable Markov decision process (DEC-POMDP), and develop an algorithm based on heterogeneous-agent proximal policy optimization (HAPPO) to solve DEC-POMDP. Finally, numerical results are used to demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TMC},
  author       = {Dongyu Chen and Tao Deng and He Huang and Juncheng Jia and Mianxiong Dong and Di Yuan and Keqin Li},
  doi          = {10.1109/TMC.2025.3607496},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware multi-task decentralized federated learning for vehicular networks: Modeling, analysis, and optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-layer design for dynamic routing and MAC protocols in terahertz nanonetworks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3607434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of nanotechnology has enabled the deployment of medical nanonetworks within the human body, further accelerated by progress in terahertz communication technologies and nanonetwork routing protocols. However, nanonodes may move in dynamic environments due to environmental influences or self-propulsion mechanisms, posing significant challenges to routing protocol design. To address this, we propose a dynamic routing protocol for nanonetworks that integrates real-time velocity vectors to account for time-varying node positions. During relay node selection, key factors such as candidate nodes' velocity vectors are comprehensively evaluated to determine the optimal relay for next-hop transmission. To ensure efficient data exchange among multiple nodes, we design a time-division multiple access (TDMA)-based media access control (MAC) protocol to prevent packet collisions and losses. The protocol assigns distinct transmission and reception time slots to different node types and implements a countdown mechanism to manage channel access and eliminate conflicts. Numerical and simulation results demonstrate that the proposed protocol significantly outperforms benchmark protocols, achieving notable improvements in both time and energy efficiency.},
  archive      = {J_TMC},
  author       = {Duyu Dai and Yu Huang and Mingyue Cheng and Miaowen Wen and Nan Yang and Chan-Byoung Chae},
  doi          = {10.1109/TMC.2025.3607434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cross-layer design for dynamic routing and MAC protocols in terahertz nanonetworks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sacrifice for QoS-aware routing: A graph reinforcement learning approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet today hosts a multitude of communication sessions from diverse vertical industries, each with distinct and increasingly stringent quality-of-service (QoS) requirements across multiple performance metrics. However, QoS-aware routing remains a significant challenge in traffic engineering, as existing solutions struggle to adapt to dynamic network conditions and meet these rigorous QoS demands. To address this issue, this paper proposes a multi-agent graph reinforcement learning-based routing algorithm that provides differentiated treatment for multiple services. First, we explore both nodebased and link-based graph reinforcement learning paradigms for performance comparison. Second, two key mechanisms, i.e., a packet sacrifice mechanism and a Tchebycheff-based reward function, are designed to realize adaptive sacrifice behavior patterns, aiming to optimize the lower bound of service satisfaction rates and enhance fairness across services. Furthermore, to ensure practical applicability, we devise a distributed computing architecture featuring neighborhood-restricted data acquisition and asynchronous historical information retrieval. Extensive simulation results demonstrate that our proposed algorithms significantly outperform benchmark methods regarding the minimum service satisfaction rate, even under unseen networks. Besides, the distributed computing architecture is proven to incur no performance penalty, which can be generalized to other resource-constrained applications.},
  archive      = {J_TMC},
  author       = {Yuqian Song and Jingli Zhou and Shudan Yu and Jun Liu},
  doi          = {10.1109/TMC.2025.3607388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive sacrifice for QoS-aware routing: A graph reinforcement learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrivGuardInfer: Channel-level end-edge collaborative inference strategy protecting original inputs and sensitive attributes. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-edge collaborative inference improves computational efficiency by dividing a deep neural network into two parts, executed across the end device and the edge node in parallel. However, adversaries like malicious edge nodes can exploit transmitted data to reconstruct original inputs or infer sensitive attributes. Existing collaborative inference strategies upload the majority of input features to the edge node, significantly increasing the risk of privacy leakage, even without input reconstruction. Therefore, we propose PrivGuardInfer, a channel-level DNN end-edge collaborative inference strategy that optimizes intra-layer partition to simultaneously protect original inputs and sensitive attributes while ensuring latency constraints, supported by three key designs. First, the privacy measurements oriented both layer depth and channel count, jointly quantify the difficulty of reconstructing original inputs using varying numbers of feature maps across different layers. After assessing each channel's contribution, the information offset further measures the difficulty of inferring sensitive attributes. Finally, PrivGuardInfer models the privacy-optimal intra-layer partition under latency constraints as a grouped knapsack problem, mapping attack difficulty to item values and inference latency to item weights. Experimental results reveal that PrivGuardInfer achieves an average improvement of 80.54% in defending against model inversion attacks and 63.34% against attribute inference attacks compared to existing end-edge partition strategies. Moreover, it outperforms current privacy protection methods by an average of 69.37% and 49.75% in mitigating these two types of attacks.},
  archive      = {J_TMC},
  author       = {Yunhao Yao and Zhiqiang Wang and Puhan Luo and Yihang Cheng and Jiahui Hou and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3607483},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PrivGuardInfer: Channel-level end-edge collaborative inference strategy protecting original inputs and sensitive attributes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is FISHER all you need in the multi-AUV underwater target tracking task?. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is significant to employ multiple autonomous underwater vehicles (AUVs) to execute the underwater target tracking task collaboratively. However, it's pretty challenging to meet various prerequisites utilizing traditional control methods. Therefore, we propose an effective two-stage learning from demonstrations training framework, FISHER, to highlight the adaptability of reinforcement learning (RL) methods in the multi-AUV underwater target tracking task, while addressing its limitations such as extensive requirements for environmental interactions and the challenges in designing reward functions. The first stage utilizes imitation learning (IL) to realize policy improvement and generate offline datasets. To be specific, we introduce multi-agent discriminator-actor-critic based on improvements of the generative adversarial IL algorithm and multi-agent IL optimization objective derived from the Nash equilibrium condition. Then in the second stage, we develop multi-agent independent generalized decision transformer, which analyzes the latent representation to match the future states of high-quality samples rather than reward function, attaining further enhanced policies capable of handling various scenarios. Besides, we propose a simulation to simulation demonstration generation procedure to facilitate the generation of expert demonstrations in underwater environments, which capitalizes on traditional control methods and can easily accomplish the domain transfer to obtain demonstrations. Extensive simulation experiments from multiple scenarios showcase that FISHER possesses strong stability, multi-task performance and capability of generalization.},
  archive      = {J_TMC},
  author       = {Guanwen Xie and Jingzehua Xu and Ziqi Zhang and Xiangwang Hou and Dongfang Ma and Shuai Zhang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3607882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Is FISHER all you need in the multi-AUV underwater target tracking task?},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The survey hole inpainting problem: A machine learning approach. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3607935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the inpainting of missing data in an indoor field, such as geomagnetism and WiFi fingerprints. As opposed to typical image/video inpainting problems, this problem poses several new challenges. First, unlike images with rectangular shapes and fixed RGB channels, indoor geographic data are multi-channeled and highly influenced by building structures. Second, unlike natural objects with fixed shapes, each geographic field is distinct and geographic data are environmentsensitive, following complex physical laws. Consequently, learning from data in other fields is difficult. Third, such data may be obtained from manual surveys and crowdsourcing, which often results in weakly-labeled and noisy datasets. We model our field data as (i) manually surveyed labeled data with holes and (ii) crowdsourced weakly-labeled data without holes. We propose a two-level adversarial regularization inpainting model to conquer these challenges and validate our results with real field data.},
  archive      = {J_TMC},
  author       = {Wei-Zhi Lin and Jen-Jee Chen and Yu-Chee Tseng},
  doi          = {10.1109/TMC.2025.3607935},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The survey hole inpainting problem: A machine learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group-based federated learning with cost-efficient sampling mechanism in mobile edge computing networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) that preserves privacy has appeared as a prospective paradigm in mobile edge computing networks. However, due to the system and data heterogeneity of mobile clients (MCs), group-based FL with a sampling mechanism is crucial for minimizing model training costs. To address these challenges, we investigate and formulate the problem of group-based FL with a sampling mechanism for reducing model training cost (i.e., latency and energy consumption), and propose a group-based FL with a cost-efficient sampling mechanism (GFLCSM) framework to address it. More precisely, before training, each MC locally pre-trains a model, estimates its data distribution from the classifier's gradient norms, and uploads it to the central server (CS) instead of raw data to preserve privacy. Using this information, the CS transforms vanilla FL into a group-based FL. During training, GFLCSM replaces the random sampling mechanism with a cost-efficient one. Moreover, to enhance robustness against network dynamics, we extend GFLCSM with a backup resampling mechanism, termed GFLCSM-E. Experimental results indicate that GFLCSM surpasses the baseline frameworks, reducing latency by 24.63% and energy consumption by 11.47% on average across two datasets, while GFLCSM-E maintains high performance even under client dropout. The source code address is https://github.com/kt4ngw/GFLCSM.},
  archive      = {J_TMC},
  author       = {Jian Tang and Xiuhua Li and Guozeng Xu and Penghua Li and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3608024},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Group-based federated learning with cost-efficient sampling mechanism in mobile edge computing networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VR-PCT: Enhanced VR semantic performance via edge-client collaborative multi-modal point cloud transformers. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic recognition is crucial for virtual reality (VR) applications, but the efficient fusion of multi-modal data poses significant challenges under resource-constrained VR scenarios. While integrating millimeter-wave (mmWave) radar point clouds with vision data offers a promising solution, existing methods often suffer from excessive data overhead and degraded accuracy due to redundant and noisy information. To address this limitation, this paper presents VR-PCT, a multi-modal transformer for edge-client collaborative VR semantic recognition that fuses mmWave radar point cloud and vision data for VR applications. VR-PCT introduces a novel collaborative design where VR clients perform lightweight semantic region detection while VR edge processes multi-modal VR semantic recognition. Through efficient edge-client collaboration, VR-PCT optimizes the transmission of mmWave point cloud and vision data by transmitting only the VR semantic region of vision data instead of the entire video. Additionally, it incorporates adaptive cross-modal data selection and fusion strategies to achieve real-time semantic recognition while significantly reducing data redundancy. Across 22 participants engaged in four experimental scenes utilizing VR devices from three different manufacturers, our evaluation demonstrates that VR-PCT achieves 97.6% recognition accuracy while reducing transmission overhead by 81.5% compared to existing approaches. These results highlight the effectiveness of VR-PCT in enabling efficient and accurate multi-modal VR semantic recognition for VR applications. The code and data of VR-PCT are released on https://github.com/luoyumei1-a/VR-PCT.},
  archive      = {J_TMC},
  author       = {Luoyu Mei and Shuai Wang and Ruofeng Liu and Yun Cheng and Shuai Wang and Wenchao Jiang and Zhimeng Yin and Tian He},
  doi          = {10.1109/TMC.2025.3607791},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VR-PCT: Enhanced VR semantic performance via edge-client collaborative multi-modal point cloud transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic communication based on large language model for underwater image transmission. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater communication is essential for environmental monitoring, marine biology research, and underwater exploration. Traditional underwater communication faces limitations like low bandwidth, high latency, and susceptibility to noise, while semantic communication (SC) offers a promising solution by focusing on the exchange of semantics rather than symbols or bits. However, SC encounters challenges in underwater environments, including semantic information mismatch and difficulties in accurately identifying and transmitting critical information that aligns with the diverse requirements of underwater applications. To address these challenges, we propose a novel SC framework based on Large Language Models (LLMs). Our framework leverages visual LLMs to perform semantic compression and prioritization of underwater image data according to the query from users. By identifying and encoding key semantic elements within the images, the system selectively transmits high-priority information while applying higher compression rates to less critical regions. On the receiver side, an LLM-based recovery mechanism, along with Global Vision ControlNet and Key Region ControlNet networks, aids in reconstructing the images, thereby enhancing communication efficiency and robustness. Our framework reduces the overall data size to 0.8% of the original. Experimental results demonstrate that our method significantly outperforms existing approaches, ensuring high-quality, semantically accurate image reconstruction.},
  archive      = {J_TMC},
  author       = {Weilong Chen and Wenxuan Xu and Haoran Chen and Xinran Zhang and Zhijin Qin and Yanru Zhang and Zhu Han},
  doi          = {10.1109/TMC.2025.3607717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semantic communication based on large language model for underwater image transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable intelligent reflecting surface-assisted mobile edge computing systems: A physical layer security and encryption design. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has emerged as a promising technology to extend the functionality of end-users' wireless devices while prolonging their battery life by offloading computationally intensive tasks to remote edge servers. However, the inherent broadcast nature of wireless transmission during offloading introduces notable security challenges. To address this issue, we propose leveraging intelligent reflecting surface (IRS) technology to enhance physical layer security (PLS). Nevertheless, attaining high PLS for all users in dense networks with multiple malicious terminals is challenging. In this paper, we investigate the physical layer encryption (PLE) to complement the PLS in enabling secure wireless transmission. Since such encryption and decryption processes require computation resources, we aim to optimize the encryption decision, offloading decision, as well as wireless and computing resource allocations. Our objective is to minimize the maximum weighted energy consumption while satisfying practical constraints, including limited computing and wireless resources, fulfilling minimum user rate requirements, and complying with IRS conditions. To tackle the non-convex objective and constraints, we explore the utilization of bisection search and successive convex approximation (SCA) methods. Our numerical results confirm the efficiency of the proposed design in terms of energy consumption and network capacity within a secure MEC network.},
  archive      = {J_TMC},
  author       = {Ti Ti Nguyen and Vu Nguyen Ha and Thanh-Dung Le and Duc-Dung Tran and Symeon Chatzinotas and Kim-Khoa Nguyen},
  doi          = {10.1109/TMC.2025.3607599},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliable intelligent reflecting surface-assisted mobile edge computing systems: A physical layer security and encryption design},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cellular user re-identification risks with networking behaviors analysis and modeling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile network operators (e.g., China Mobile, Verizon) are significant for providing communication services and collecting massive amounts of data. However, operators are increasingly concerned about customer data breaches involving third-party application providers (e.g., Tencent, Apple, Netflix). This concern is particularly aggravated when anonymous datasets shared with third-party providers or publicly released can be linked to user data compromised in breaches, leading to severe re-identification attacks and privacy threats. However, comprehensive methods for identifying such privacy risks on a large scale are lacking due to limited networking behavioral data. To address this, we aim to measure the re-identification privacy risk associated with sharing or releasing cellular traces amidst data breaches. Based on the analysis of key privacyimpacting features in traffic usage and base station association data, we propose a novel re-identification method, SURE, which learns similarities between cellular traces to classify if traces belong to the same user. Extensive experiments on a largescale dataset of 10,000 users over four months demonstrate SURE's superior performance, with AUC scores exceeding 0.9. Our findings reveal significant re-identification risks in data sharing/release, influenced by data scale and user attributes, corroborated by a public dataset.},
  archive      = {J_TMC},
  author       = {Sijing Duan and Feng Lyu and Shanshan Wang and Yi Ding and Xiaohao He and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3607772},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring cellular user re-identification risks with networking behaviors analysis and modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sleep scheduling algorithm for the $k$-coverage problem in 3D heterogenous BF-WSNs. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Battery-free Wireless Sensor Networks (BF-WSNs) have emerged as an essential part of Internet of Things (IoT) systems. Although two-dimensional (2D) BF-WSNs have been researched, three-dimensional (3D) ones are more indicative of real-world applications. Achieving $k$-coverage in this scenario is a greater challenge and has yet to be investigated. This paper presents an optimization problem in heterogeneous 3D BF-WSNs to maximize $k$-coverage quality while considering the recharging and sampling rates. We prove the problem is NP-Hard and decouple it into two subproblems. The first optimizes $k$-coverage quality for each time slot without energy constraints. The second ensures that nodes scheduled for operation in each time slot can be immediately replenished with sufficient energy. We prove the near-optimal and optimal solutions to the two subproblems composes the near-optimal solution to the original problem. Following the development of Distributed Iterative Grouping (DIG) algorithm and Adaptive Sampling (AS) method to address two subproblems each, we propose a sleep scheduling algorithm to integrate them and solve the original problem. Simulation results verify the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_TMC},
  author       = {Yanlei Chen and Haoyang Zhou and Jingjing Li and Na Tang and Ping Li},
  doi          = {10.1109/TMC.2025.3607841},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sleep scheduling algorithm for the $k$-coverage problem in 3D heterogenous BF-WSNs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A superposition code-based semantic communication approach with quantifiable and controllable security. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of achieving security in semantic communication (SemCom) over a wiretap channel, where a legitimate receiver coexists with an eavesdropper experiencing a poorer channel condition. Despite previous efforts to secure SemCom against eavesdroppers, guarantee of approximately zero information leakage remains an open issue. In this work, we propose a secure SemCom approach based on superposition code, aiming to provide quantifiable and controllable security for digital SemCom systems. The proposed method employs a double-layered constellation map, where semantic information is associated with satellite constellation points and cloud center constellation points are randomly selected. By carefully allocating power between these two layers of constellation, we ensure that the symbol error probability (SEP) of the eavesdropper when decoding satellite constellation points is nearly equivalent to random guessing, while maintaining a low SEP for the legitimate receiver to successfully decode the semantic information. Simulation results demonstrate that the peak signal-to-noise ratio (PSNR) and mean squared error (MSE) of the eavesdropper's reconstructed data, under the proposed method, can range from decoding Gaussian-distributed random noise to approaching the variance of the data. This validates the effectiveness of our method in nearly achieving the experimental upper bound of security for digital SemCom systems when both eavesdroppers and legitimate users utilize identical decoding schemes. Furthermore, the proposed method consistently outperforms benchmark techniques, showcasing superior data security and robustness against eavesdropping. The implementation code is publicly available at: https://github.com/1weixuanchen/A-Superposition-Code-Based-Semantic-Communication.},
  archive      = {J_TMC},
  author       = {Weixuan Chen and Shuo Shao and Qianqian Yang and Zhaoyang Zhang and Ping Zhang},
  doi          = {10.1109/TMC.2025.3608054},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A superposition code-based semantic communication approach with quantifiable and controllable security},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online location planning for AI-defined vehicles: Optimizing joint tasks of order serving and spatio-temporal heterogeneous model fine-tuning. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living. Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model finetuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning. To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.},
  archive      = {J_TMC},
  author       = {Bokeng Zheng and Bo Rao and Tianxiang Zhu and Chee Wei Tan and Jingpu Duan and Zhi Zhou and Xu Chen and Xiaoxi Zhang},
  doi          = {10.1109/TMC.2025.3608179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online location planning for AI-defined vehicles: Optimizing joint tasks of order serving and spatio-temporal heterogeneous model fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WarmGait: Thermal array-based gait recognition for privacy-preserving person re-ID. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) can recognize users based on their clothing, body shape, and other information without the need for clear facial images, and is widely applied in the field of intelligent security. Traditional Re-ID systems mainly rely on high-definition RGB cameras, but the deployment of large-scale high-definition RGB cameras indoors has caused serious privacy and ethical concerns. Recently, wireless-based Re-ID systems (Wi-Fi, RFID, millimeter-wave radar, etc.) have shown promising prospects, but the limited sensing resolution hinders their practical deployment. In this paper, we propose WarmGait, a Re-ID system based on thermal array sensors, which can achieve high-precision Re-ID at low cost and minimize the invasion of user privacy. However, using thermal arrays for Re-ID still faces two major challenges. The first is the low and unclear texture resolution of images caused by low-cost infrared devices. The second is that existing gait recognition methods require maintaining the sequential constraint of gait images, which reduces the flexibility of gait recognition or Re-ID. To address these two challenges, we first designed an edge module inspired by Taylor Finite Difference (TFD) to aggregate image edge information to help improve the resolution of infrared devices. Then, we considered gait as a collection of gait profiles and extracted features from the frame level and collection level for recognition, breaking through the limitations of the number and order of input images. After extensive experimental evaluation, our model can achieve an average recognition accuracy of 87.3% in various scenarios, demonstrating the potential of WarmGait in Re-ID.},
  archive      = {J_TMC},
  author       = {Hongbo Jiang and Lei Ye and Jingyang Hu and Xiaotian Chen and Siyu Chen and Wei Zhang and Kehua Yang},
  doi          = {10.1109/TMC.2025.3608447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WarmGait: Thermal array-based gait recognition for privacy-preserving person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint inference offloading and model caching for small and large language model collaboration. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), with advanced content creation and inference capabilities, can provide immersive intelligent services to users in mobile edge networks. However, the increasing demand for real-time artificial intelligence (AI) applications aggravates the limitations of cloud-based LLMs due to the long response time. Meanwhile, Small Language Models (SLMs), which are cost-effective and locally deployable for terminal devices, can serve as an efficient supplement to LLMs for performing latency-sensitive tasks with lower generalization capability. Due to the resource constraints of edge networks and the diverse requirements of user tasks, it is critical to design an inference framework that effectively coordinates the deployment and collaboration of LLMs and SLMs. In this paper, we propose an LLM-SLM collaborative inference (LSCI) scheme under a mobile edge computing (MEC) architecture, which jointly decides where to cache models and how to offload inference tasks to balance latency, accuracy, and resource costs. To optimize inference performance subject to resource constraints, we jointly solve the inference task offloading and model caching problem in LSCI scheme. Specifically, we employ deep reinforcement learning (DRL) to select highly popular SLMs to be cached on the edge server, and distributed belief propagation technique to solve the associated inference task offloading issue. Numerical results show that the proposed LSCI scheme can achieve significant performance gain in terms of inference performance when compared with a number of baseline solutions.},
  archive      = {J_TMC},
  author       = {Xinyi Xu and Gang Feng and Yijing Liu and Shuang Qin and Jian Wang and Yunxiang Wang},
  doi          = {10.1109/TMC.2025.3608303},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint inference offloading and model caching for small and large language model collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising and adaptive online vertical federated learning for sequential multi-sensor data in IIoT. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of computational capabilities in edge devices such as intelligent sensors in the Industrial Internet of Things (IIoT), these sensors evolving beyond simple data collection to support complex computational tasks. This advancement provides new opportunities for adopting distributed learning approaches in IIoT. In this study, we focus on enhancing learning performance in an industrial assembly line scenario where multiple distributed sensors sequentially collect real-time data with distinct feature spaces. However, existing research lacks an online distributed learning framework tailored for such IIoT settings. To address this gap, we propose the Denoising and Adaptive Online Vertical Federated Learning (DAO-VFL) algorithm, a novel algorithm that leverages the computing potential of edge sensors while addressing key challenges such as communication overhead and data privacy. DAO-VFL effectively manages continuous data streams and adapts to shifting learning objectives. Furthermore, it can address critical challenges prevalent in industrial environment, such as communication noise and heterogeneity of sensor capabilities. To support the proposed algorithm, we provide a comprehensive theoretical analysis, highlighting the effects of noise reduction and adaptive local iteration decisions on the regret bound. Experimental results on two real-world datasets further demonstrate the superior performance of DAO-VFL compared to benchmarks.},
  archive      = {J_TMC},
  author       = {Heqiang Wang and Xiaoxiong Zhong and Kang Liu and Fangming Liu and Weizhe Zhang},
  doi          = {10.1109/TMC.2025.3608244},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Denoising and adaptive online vertical federated learning for sequential multi-sensor data in IIoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical MAFDRL-based resource allocation and incentive mechanism for TN-NTN in 6G networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of existing wireless networks for demanding applications like brain-computer interfaces and intelligent transportation systems, we propose an advanced framework for joint resource allocation and task offloading across integrated terrestrial and non-terrestrial networks (TN-NTN). This framework utilizes multiple layers, including ground users, UAVs, HAPs, and satellites, to improve service quality and immersive experiences, particularly in scenarios like Metaverse applications. Ground users request resources, while UAVs and HAPs serve as resource providers, and satellites ensure reliable communication during emergencies. A double auction-based incentive scheme is employed in which operators control UAV and HAP resources to maximize utility, and users aim to minimize computation costs and protect data privacy. To handle the complexity of the operator-user interaction, which results in an NP-hard optimization problem, we applied a hierarchical multi-agent federated deep reinforcement learning (FeDRL) approach. Our simulation results demonstrate that the FeDRL algorithm significantly improves social welfare by 6.38%, 17.43%, and 28.73% over modified MADDPG, FRL, and DDPG algorithms, respectively.},
  archive      = {J_TMC},
  author       = {Abegaz Mohammed Seid and Aiman Erbad and Hayla Nahom Abishu and Gordon Owusu Boateng and Latif U. Khan and Carla Fabiana Chiasserini and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3608291},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A hierarchical MAFDRL-based resource allocation and incentive mechanism for TN-NTN in 6G networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data divergence-aware client selection via knowledge graph for federated LLM fine-tuning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of edge devices and growing awareness of privacy protection, recent developers have transformed to fine-tune LLMs via federated learning instead of centralized training. Federated fine-tuning can leverage distributed data sources and computation power, but it also suffers from system and statistical heterogeneity. Client selection is an effective tool to solve the system and statistical heterogeneity in FL, but existing client selection schemes that involve online measurement will not be as effective in LLM fine-tuning as in conventional FL due to the huge LLM size and fewer fine-tuning rounds. In this paper, to the best of our knowledge, we are the first to consider both system and statistical heterogeneity in federated LLM fine-tuning, and we formulate a new latency minimization problem. We propose to measure client data overlap via knowledge graph offline to assist client selection in federated LLM fine-tuning. Our client selection scheme excels in both model accuracy and fine-tuning latency. We evaluate our scheme via two LLMs and two applications via four datasets. The experiment results illustrate that our scheme achieves the highest accuracy while 2.05x faster than the baselines.},
  archive      = {J_TMC},
  author       = {Bihai Zhang and Dan Wang and Yifei Zhu and Zhu Han},
  doi          = {10.1109/TMC.2025.3608172},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Data divergence-aware client selection via knowledge graph for federated LLM fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreeEnv: Enabling zero-effort RF-based micro-environment changes monitoring. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, a major issue of WiFi-based sensing technologies is how to adapt to changes in the surrounding environment. The extreme sensitivity of Channel State Information (CSI) makes many WiFi sensing arts frustrated when applied to the complex and unknown real world. To solve this problem, in this paper, we propose freeEnv designed to automatically identify the micro-environmental changes (even tiny movements of the laptop) using WiFi devices, which can coexist with other WiFi sensing tasks with zero effort. To achieve automatic identification of micro-environmental changes, we quantify micro-environmental changes based on the physical propagation laws of WiFi signals and the main factors that affect CSI measurements. Then, we design a micro-environmental changes identification method, which determines whether the environment has changed by calculating the Earth Mover's Distance (EMD) of the Probability Density Function (PDF) of continuous CSI, without requiring training data. To remove the influence of dynamic human behaviors, we design a human dynamic detection scheme, which is achieved by obtaining the average inter-cluster distance of performing Gaussian Mixture Model (GMM) clustering on CSI. We evaluate freeEnv in real-world scenarios with six different hardware, four different scenarios, and twenty-four ways of micro-environmental changes. The results show that our method is robust to different devices and scenarios, and can achieve the average precision of 96.1% and 93.2% for micro-environmental changes identification and human dynamic behavior detection. By testing on a case study of threshold-based human presence detection, freeEnv can effectively improve the detection performance.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Feiyu Han and Mingzhu Yang and Shanyue Wang and Panlong Yang and Yubo Yan},
  doi          = {10.1109/TMC.2025.3608245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreeEnv: Enabling zero-effort RF-based micro-environment changes monitoring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiered spatio-temporal difficulty: Curriculum scheduler for multi-sensor traffic flow prediction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of the Internet of Things (IoT) has enhanced smart city services for traffic monitoring, leading to numerous schemes for accurate flow prediction based on traffic sensors. However, existing approaches primarily capture spatio-temporal (ST) dependencies from traffic graphs and train their models using randomly ordered data. This overlooks the fact that the modeling difficulty of each sensor/node in the ST traffic graph can vary significantly due to its spatial dependencies and temporal trends, resulting in unreliable and unstable predictions in IoT scenarios. In this context, we argue that a well-designed curriculum with an easy-to-difficult order can improve the training of ST models. Therefore, this paper introduces an ST difficulty measurer to score the node-level difficulty of traffic graph from both spatial and temporal aspects, and then implements a curriculum in the ST model training process. More specifically, based on the tiered ST difficulty score, the ST model training begins with a subgraph consisting of “easy” nodes characterized by relatively consistent spatial relationships and regular temporal patterns. Gradually, more difficult nodes are incorporated into the subgraph and participate in subsequent training stages. Comprehensive experiments and analysis on two real-world traffic flow datasets confirm the effectiveness of our proposed approach.},
  archive      = {J_TMC},
  author       = {Zhiwen Zhang and Hongjun Wang and Zipei Fan and Renhe Jiang and Wei Yuan and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TMC.2025.3608620},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Tiered spatio-temporal difficulty: Curriculum scheduler for multi-sensor traffic flow prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deterministic backoff approach for wi-fi and NR-U coexistence in shared bands. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unlicensed (shared) bands, wireless technologies typically operate without central coordination, which can lead to unwanted transmission interruptions, collisions, and resource wastage. We focus on Wi-Fi and NR-U coexistence in shared bands and solve the aforementioned problem with a deterministic backoff approach. The proposed scheme allows active transmitters to learn the number of nearby interferers in a distributed manner and, as a result, implement an ordered round-robin transmission schedule to minimize collisions. We show analytically that the scheme converges not only in equilibrium (when collisions are negligible), but also in a general case (when collisions are frequent at the beginning or in the middle of the proposed scheme's operation). Furthermore, extensive simulations prove that the proposed scheme guarantees fairness between contenting cells and technologies (both in terms of throughput and delay) as well as optimizes channel efficiency. We also study the impact of imperfect readings of the number of contending nodes (which may be the result of, e.g., asymmetric channel conditions) on the performance of the proposed scheme. In most cases, the deterministic backoff approach outperforms legacy channel access schemes. Additionally, our proposal does not add additional signaling overhead and is backward compatible with standard Wi-Fi and NR-U operation.},
  archive      = {J_TMC},
  author       = {Ilenia Tinnirello and Menzo Wentink and Alice Lo Valvo and Szymon Szott and Katarzyna Kosek-Szott},
  doi          = {10.1109/TMC.2025.3608270},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A deterministic backoff approach for wi-fi and NR-U coexistence in shared bands},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwinULoc: Pre-trained swin transformer U-net with ToF offset correction for resource-efficient WiFi indoor localization. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquity of WiFi infrastructure has motivated significant research into WiFi-based indoor positioning systems as practical alternatives to GNSS. While deep learning approaches show promise, existing models face three critical limitations: (1) inadequate modeling of long-range feature dependencies, (2) difficulty in correcting time-of-flight (ToF) offsets induced by device clock asynchrony, and (3) prohibitive computational costs for environmental adaptation through model retraining. This paper introduces SwinULoc, a novel U-shaped indoor positioning framework that synergizes Swin Transformer blocks with 2D CSI heatmap processing. Our architecture uniquely addresses these challenges through three key innovations: First, the integration of shifted window attention mechanisms enables effective learning of long-range signal correlations. Second, a multi-access-point fusion strategy enhanced with skip connections achieves precise ToF offset correction through cross-device pattern analysis and multi-scale feature integration. Third, a transfer learning paradigm reduces retraining costs by 75% compared to conventional approaches. Extensive evaluations demonstrate SwinULoc's superiority, achieving 70% higher positioning accuracy than state-of-the-art baselines while requiring only 1/4 of the training resources for new environments.},
  archive      = {J_TMC},
  author       = {Wenhao Zhang and Xingfa Shen and Sicong Xia and Zhibo Wang},
  doi          = {10.1109/TMC.2025.3608157},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SwinULoc: Pre-trained swin transformer U-net with ToF offset correction for resource-efficient WiFi indoor localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semi-supervised indoor localization algorithm based on probabilistic distribution modeling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in wireless technology have spurred the growth of indoor localization applications based on wireless signals. Most existing methods model the relationship between input data and the target's location using the $l_{2}$ loss function, which assumes that the residual between the predicted location and the ground truth follows a Gaussian distribution with a fixed standard deviation. Nevertheless, this approach may not conform to the actual distribution and lacks confidence measures for individual predictions, potentially compromising accuracy. Furthermore, in the realm of regression, Semi-Supervised Learning (SSL) remains relatively unexplored due to the absence of a reliable method to quantify prediction uncertainty, especially when labeled data is scarce. To address these challenges, we developed a novel indoor localization algorithm that employs probabilistic distribution modeling. Our approach focuses on indoor localization with Channel Impulse Response (CIR) as the input. Crucially, it leverages Maximum Likelihood Estimation (MLE) to model the localization error as a Gaussian distribution, and utilizes Residual Log-likelihood Estimation (RLE) to capture arbitrary error distributions. This enables us to extract the confidence of each prediction and utilize the most reliable ones as pseudo-labels for the unlabeled data. By employing probabilistic distribution modeling, we observed a significant improvement in localization accuracy over the $l_{2}$ loss function. Additionally, by integrating pseudo-labeled data for model retraining, our algorithm achieves superior performance compared to existing state-of-the-art machine learning-based and SSL-based methods. This is demonstrated by our evaluation on two public datasets, showcasing the efficiency of the proposed method.},
  archive      = {J_TMC},
  author       = {Ruofei Gao and Xiaotao Li and Wai Chen},
  doi          = {10.1109/TMC.2025.3608276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A semi-supervised indoor localization algorithm based on probabilistic distribution modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEDS: An affinity-driven efficient DRL-based task scheduling framework for edge computing. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a promising paradigm that deploys computing resources at the network edge to provide services. Many existing solutions leverage deep reinforcement learning (DRL) to optimize task scheduling, yet they often rely on global scheduling approaches. However, such solutions result in an excessively large decision search space, reducing task scheduling efficiency in complex environments. Additionally, the cold start problem impedes the generation of optimal scheduling strategies. To address these challenges, we propose AEDS, a DRL-based task scheduling framework designed to enhance scheduling efficiency. AEDS optimizes the decision-making process from three aspects: (1) Decision Space Reduction. AEDS incorporates a novel affinity matching mechanism that identifies the most suitable edge cluster based on task characteristics, thereby significantly narrowing the decision search space. (2) Decision Process Optimization. AEDS adopts a hybrid strategy combining offline pre-training and online fine-tuning to address the cold start problem. Offline pre-training with historical task data ensures effective initial scheduling, while online fine-tuning periodically updates the DRL model to enhance long-term adaptability to dynamic system changes. (3) Decision Strategy Calibration. AEDS proposes a task migration solution to adapt to real-time workload variations dynamically. It utilizes triple queues to assess server overload and dynamically calibrates the scheduling strategy through task migration within interconnected clusters. Comprehensive experimental results validate the efficacy of AEDS. Compared with existing frameworks, AEDS reduces task latency by $28.23\%$ and enhances task completion rate by $10.28\%$. Furthermore, by effectively narrowing the decision scope, AEDS accelerates the decision-making process by a remarkable $88.06\%$},
  archive      = {J_TMC},
  author       = {Xu Liu and Zhaolong Jian and Xueshuo Xie and Qiankun Dong and Mulin Li and Xiaoyu Zhang and Tao Li},
  doi          = {10.1109/TMC.2025.3608263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AEDS: An affinity-driven efficient DRL-based task scheduling framework for edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal online federated learning with modality missing in internet of things. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3608269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) ecosystem generates vast amounts of multimodal data from heterogeneous sources such as sensors, cameras, and microphones. As edge intelligence continues to evolve, IoT devices have progressed from simple data collection units to nodes capable of executing complex computational tasks. This evolution necessitates the adoption of distributed learning strategies to effectively handle multimodal data in an IoT environment. Furthermore, the real-time nature of data collection and limited local storage on edge devices in IoT call for an online learning paradigm. To address these challenges, we introduce the concept of Multimodal Online Federated Learning (MMO-FL), a novel framework designed for dynamic and decentralized multimodal learning in IoT environments. Building on this framework, we further account for the inherent instability of edge devices, which frequently results in missing modalities during the learning process. We conduct a comprehensive theoretical analysis under both complete and missing modality scenarios, providing insights into the performance degradation caused by missing modalities. To mitigate the impact of modality missing, we propose the Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype learning to effectively compensate for missing modalities. Experimental results on two multimodal datasets further demonstrate the superior performance of PMM compared to benchmarks.},
  archive      = {J_TMC},
  author       = {Heqiang Wang and Xiang Liu and Xiaoxiong Zhong and Lixing Chen and Fangming Liu and Weizhe Zhang},
  doi          = {10.1109/TMC.2025.3608269},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multimodal online federated learning with modality missing in internet of things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sym-FEC: Enhancing error correction in LoRa PHY with a symbol-level FEC decoder. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, a leading wireless technology for Low Power Wide Area Networks (LPWAN), is well-known for its long transmission range and low power consumption. The extended range is primarily attributed to the Chirp Spread Spectrum technique. However, the LoRa physical layer (LoRa PHY) contributes only marginally to this advantage, as it employs an inefficient Forward Error Correction (FEC) strategy for error recovery. In this paper, we introduce Sym-FEC, a symbol-level FEC decoder designed to link the received signals' spectrum with the coding correlations inherent in LoRa PHY, thereby enhancing error recovery. The key enabler of Sym-FEC is signal copy retrieval. We begin by facilitating signal copy conversion between two symbols and extend this to the general case, where signal copy conversions can be performed between any symbols in a coding block. Approaches are also introduced to assess the validity of the block-wide decoding results. Extensive hardware evaluations demonstrate that Sym-FEC provides Signal-to-Noise-Ratio (SNR) improvement of 2.3dB to 3dB compared to the traditional decoder in LoRa PHY. Sym-FEC requires no modifications at the transmitter while incurs low storage and computational complexity at the gateway, thus can be easily integrated into gateway nodes.},
  archive      = {J_TMC},
  author       = {Weiwei Chen and Xianjin Xia and Shuai Wang and Xianjun Deng and Jiehong Wu and Caishi Huang},
  doi          = {10.1109/TMC.2025.3608893},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sym-FEC: Enhancing error correction in LoRa PHY with a symbol-level FEC decoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A privacy-preserving auction for task offloading and resource allocation in UAV-assisted MEC. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a complementary solution for Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAVs) can temporarily provide reliable and flexible offloading services when edge servers are damaged or unavailable. However, existing UAV-assisted MEC systems suffer from issues such as uneven resource allocation, low utilization efficiency, load imbalance, and poor dynamic adaptability, affecting service quality. Moreover, sensitive user equipment (UE) information faces leakage during the computational process of UAVs. How to jointly optimize the scheduling of servers and UAVs for task offloading and resource allocation without compromising UEs' privacy remains a significant challenge. Thus, this paper proposed a privacy-preserving auction framework (namely Prizty) by considering the trajectory of UAVs, their constrained energy and computational capabilities, and the variability in UE distribution. Prizty employs a combinatorial obfuscation method to protect UEs' privacy and links bidding prices to computational resources and energy characteristics. It calls the sub-algorithm WPA to determine the winners by balancing social costs and utility. Theoretical analysis demonstrates that Prizty satisfies truthfulness and individual rationality while maintaining scalability for large-scale resource allocation problems. Extensive experiments on real-world datasets validate Prizty's effectiveness in critical metrics, including offload rate, average service latency, energy consumption, and social cost.},
  archive      = {J_TMC},
  author       = {Jiajie Xu and Xiaolong Xu and Guangming Cui and Muhammad Bilal and Rong Gu and Wanchun Dou and Arumugam Nallanathan},
  doi          = {10.1109/TMC.2025.3609202},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A privacy-preserving auction for task offloading and resource allocation in UAV-assisted MEC},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the timeliness of radio channel access: Random access or scheduled access?. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the role of channel access schemes in enhancing the timeliness of status updates in sensor networks. Specifically, we model the large-scale sensor network as a Poisson cellular network and derive the network average age of information (AoI) under five different channel access schemes: slotted ALOHA, frame slotted ALOHA, random scheduling, round robin, and channel-aware. These schemes are categorized based on random vs. scheduled access and non-channel-aware vs. channel-aware. Our goal is to investigate when the additional overhead and complexity introduced by scheduling and channel state information (CSI) are beneficial, enabling better decisions in network design. Our findings reveal that the effectiveness of these schemes is influenced by the signal-to-interference ratio (SIR) decoding threshold, which often reflects the length of communication data. For short-packet communications, the performance differences among various channel access strategies are minimal, and the gains from scheduling are limited. Additionally, the inclusion of extra CSI does not yield performance improvements; in fact, some simple scheduling strategies, along with channelaware strategy that leverage CSI, may not outperform basic random access methods. Among the protocols we examined, the round robin scheme achieves the best performance. In contrast, scheduled access schemes exhibit a clear performance advantage in long-packet communications. Furthermore, the channel-aware scheme significantly enhances the network AoI performance, particularly in networks with higher transmitter competition.},
  archive      = {J_TMC},
  author       = {Zhiling Yue and Yuting Tang and Nikolaos Pappas and Yaru Fu and Tony Q. S. Quek and Howard H. Yang},
  doi          = {10.1109/TMC.2025.3609285},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On the timeliness of radio channel access: Random access or scheduled access?},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative access with waiting window: Enhancing age-of-information in CSMA networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3609393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next generation of industrial Internet-of-Things systems demands timely delivery of fresh data, quantified by the Age-of-Information (AoI) metric, from a vast number of sensors. In such systems, sensors usually adopt random access where the back-off mechanism is mostly crucial. This paper studies the back-off mechanism for Carrier Sense Multiple Access (CSMA) systems in collision-prone scenarios. Considering the fact that transmitting immediately after a prior successful transmission yields limited AoI reduction, we propose an enhanced waiting CSMA scheme by introducing a waiting window to the existing contention window. In the proposed scheme, each source waits after each successful transmission. It enables a collaborative channel access strategy, such that sources have recently transmitted grant greater opportunities for others to access the channel. To optimize the window sizes, we analyze the Normalized Average Peak AoI (NPAoI) and Normalized Average AoI (NAoI), which are normalized by the packet transmission duration. Our findings indicate that minimizing these metrics is feasible through maintaining a nearly constant collision probability. Specifically, NPAoI reaches its minimum by adjusting either the contention or the waiting window, while NAoI requires a larger waiting window combined with a smaller contention window. Given the network scale, we derive optimal window sizes in closed form. When the network scale is unknown or dynamic, we propose a data-driven approach to adjust window sizes based solely on the real-time measured collision probability. Simulation results confirm that the proposed waiting CSMA scheme significantly outperforms existing methods for both NPAoI and NAoI, particularly in large-scale networks.},
  archive      = {J_TMC},
  author       = {Yifan Gu and Suzhi Bi and Zhaoxu Wang and Zhi Quan},
  doi          = {10.1109/TMC.2025.3609393},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative access with waiting window: Enhancing age-of-information in CSMA networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow prioritization in asynchronous TSN with multiple ATS instances. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3609519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-sensitive networking (TSN) is an essential technology for the development of deterministic networks as it can offer deterministic quality of service (QoS) in terms of transmission delay. In addition, it is more scalable, more affordable, and simplifies the management of current industrial networks. This paper focuses on asynchronous traffic shaping (ATS) in TSN and proposes a novel solution to prioritize the flows being transmitted in a TSN with multiple ATS instances to meet their delay requirements. To this end, we formally formulate the flow prioritization assignment problem in an ATS-TSN network, demonstrate the correctness of the proposed algorithm, and study the solution's scalability. The results show that our solution is optimal, obtaining a shorter execution time than an exhaustive search with the same prioritization result. Furthermore, our solution scales correctly as a function of the number of flows with a considerably low execution time. Compared to another ATS prioritization method, our solution finds feasible solutions four orders of magnitude larger with reduced execution time. Moreover, the results show that per-flow prioritization has higher utilization than per-Priority Code Point (PCP) prioritization. Finally, an increase in the percentage of traffic with strict delay requirements harms the maximum achievable utilization.},
  archive      = {J_TMC},
  author       = {Julia Caleya-Sanchez and Jonathan Prados-Garzon and Pablo Muñoz and Juan M. Lopez-Soler and Pablo Ameigeiras},
  doi          = {10.1109/TMC.2025.3609519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Flow prioritization in asynchronous TSN with multiple ATS instances},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E2E hybrid computation offloading for complex MEC system. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) places computational resources at the network edge, thereby enabling compute-intensive applications through task offloading. However, in dynamic multi-user, multi-server environments, user mobility induces time-varying channel conditions, and the spatiotemporal heterogeneity of server loads further complicates system behavior. Consequently, the system must jointly optimize discrete offloading decisions and continuous resource-allocation parameters, forming a hybrid action space whose integrated decision-making mechanism is central to breaking the long-standing trade-off between latency and energy consumption. Traditional deep reinforcement learning (DRL) approaches that rely on a single policy network often suffer from strong strategy coupling and Q-value estimation bias, leading to policy oscillations and the curse of dimensionality in highly dynamic scenarios and thus impeding stable convergence. To address this problem, this paper proposes an innovative End-to-End Hybrid Computation Offloading (E2EHCO) framework based on an enhanced Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. By employing dual critic networks together with a delayed-update mechanism, the method effectively suppresses Q-value overestimation, while the integration of Softmax and Tanh activations in the actor network allows simultaneous handling of discrete and continuous actions, thereby achieving efficient and robust joint decision optimization under dynamically changing conditions. Experiments on real-world mobility traces show that, relative to benchmark methods, E2EHCO reduces total latency by at least 20% and energy consumption by approximately 16% in high-density user scenarios, providing an adaptive offloading solution with real-time responsiveness for large-scale, dynamic MEC systems.},
  archive      = {J_TMC},
  author       = {Jingjing Zhang and Xiaoheng Deng and Jian Yin and Xianjun Deng and Xuechen Chen and Jinsong Gui and Shichao Zhang},
  doi          = {10.1109/TMC.2025.3609546},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {E2E hybrid computation offloading for complex MEC system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving inter-logical channel interference for large-scale LoRa deployments. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3609316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWANs are envisioned to connect billions of IoT devices through thousands of physically overlapping yet logically orthogonal channels (termed logical channels). These logical channels hold significant potential for enabling highly concurrent scalable IoT connectivity. Large-scale deployments however face strong interference between logical channels. This practical issue has been largely overlooked by existing works but becomes increasingly prominent as LoRaWAN scales up. To address this issue, we introduce Canas, an innovative gateway design that is poised to orthogonalize the logical channels by eliminating mutual interference. To this end, Canas develops a series of novel solutions to accurately extract the meta-information of individual ultra-weak LoRa signals from the received overlapping channels. The meta-information is then leveraged to accurately reconstruct and subtract the LoRa signals over thousands of logical channels iteratively. Real-world evaluations demonstrate that Canas can enhance concurrent transmissions across overlapping logical channels by 2.3× compared to the best known related works.},
  archive      = {J_TMC},
  author       = {Shiming Yu and Ziyue Zhang and Xianjin Xia and Yuanqing Zheng and Jiliang Wang},
  doi          = {10.1109/TMC.2025.3609316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resolving inter-logical channel interference for large-scale LoRa deployments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing learning to communicate with reward-shaped curriculum and network awareness. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3608813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication enhances collaboration among artificial intelligence agents, for example, by sharing observations that contribute to safer driving. Given the conflicts between limited communication resources and communication needs, learning effective communication strategies is essential. We observe that incorporating learning to communicate can complicate mastering primary tasks, like vehicle control, the original focus in autonomous driving. This is due to the uncertainty in information acquisition during the learning process, which can lead to an unstable environment for primary tasks. In this paper, we introduce ReSCOM, an efficient joint learning framework that combines learning-to-communicate with primary tasks. ReSCOM progressively adjusts the learning emphasis through rewardshaped curriculum, allowing agents to shift their focus from primary tasks and basic communication tasks (e.g., how to encode) to advanced communication strategies (e.g., determining when it is worthwhile to communicate). This approach minimizes the impact on the learning efficiency of primary tasks while simultaneously facilitating communication learning. Besides, we explore the extent to which communication channel states (i.e., delays and packet loss) and protocols impact agent cooperation and learning. We evaluate ReSCOM against state-of-the-art methods in various tasks, demonstrating its strong performance. Furthermore, we verify that current modern wireless channels, includingWi-Fi, 4G, and 5G, provide low enough delays that their impact can be ignored. When packet loss occurs, we find that the UDP protocol performs better than TCP because, for agent cooperation, timely information is more valuable than reliability.},
  archive      = {J_TMC},
  author       = {Xinghai Wei and Jie Yuan and Tingting Yuan and Xiang Liu and Xiaoming Fu},
  doi          = {10.1109/TMC.2025.3608813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing learning to communicate with reward-shaped curriculum and network awareness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and security analysis of SDN-based IoT-oriented blockchain protected E-voting system. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3609480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electoral system is one of the fundamental pillars of democracy, but the traditional voting system suffers from several limitations such as fraud voting, vote tampering, impersonation, and inefficiencies. To overcome these limitations, several research works have been initiated to design a blockchain-based e-voting system. These designs addressed the loopholes of the existing ones to a limited extent. Here, a novel multi-level blockchain-secured SDN-based IoT enabled e-voting system has been proposed. The proposed system consists of booth, district, state, and country level systems. Here, a voter needs to be authenticated at the booth-level and then this valid vote data can be propagated to the upper hierarchical levels and stored there after signing and encrypting it using ECDSA and ECC respectively. Man-in-the-middle attacks, DoS/DDoS, unauthorized access, and impersonation attacks are avoided using flow rules in SDN controllers and firewalls installed in the servers. Furthermore, blockchain technology provides security for voting data stored at all levels. The security strengths were tested at different levels (e.g., programming, operating system, and network level) using open-source tools (i.e. scyther, nmap, metasploit, etc.). The performance of the proposed architecture was evaluated satisfactorily in a testbed. It also performed satisfactorily under both normal and stressed conditions in a scaled-up environment.},
  archive      = {J_TMC},
  author       = {Ngangbam Indrason and Kalyan Baital and Goutam Saha},
  doi          = {10.1109/TMC.2025.3609480},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and security analysis of SDN-based IoT-oriented blockchain protected E-voting system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient and scalable asynchronous federated learning via stragglers version control. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous Federated Learning (AFL) has emerged as a promising paradigm to address the challenges posed by heterogeneous device environments in federated learning systems. However, the problem of low accuracy and slow convergence due to inconsistent updates from stragglers and normal clients remains severe in AFL. Previous works either discard or penalize the updates from stragglers, which can lead to the loss of valuable data or introduce bias into the model. Furthermore, existing AFL frameworks integrating synchronous optimization algorithms face the challenges of weak compatibility and scalability, limiting large-scale training. In this paper, we propose DVAFL1 an efficient and scalable AFL framework that significantly improves the performance of AFL in terms of model accuracy and convergence speed by effectively utilizing and compensating for the updates from stragglers, while naturally integrating synchronous optimization algorithms. Specifically, DVAFL introduces a dynamic window protocol for adaptive aggregation to balance the contribution of stragglers and ensure faster and more stable convergence. Further, the version control mechanism corrects stale gradients by compensating for the missed model updates of stragglers, thereby improving model performance. Extensive experiments on three public datasets demonstrate that DVAFL achieves an average convergence speed 2.3× faster and an accuracy improvement of 5.5% compared to state-of-the-art AFL methods.},
  archive      = {J_TMC},
  author       = {Chuyi Chen and Yanchao Zhao and Zhe Zhang and Wenzhong Li and Jie Wu},
  doi          = {10.1109/TMC.2025.3609568},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards efficient and scalable asynchronous federated learning via stragglers version control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation resource management in mobile edge computing for healthcare using lyapunov-deep deterministic policy gradient. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mobile edge computing healthcare (MECH) system, the integration of MECH servers and wearable medical sensors can achieve real-time monitoring and analysis of user health. However, the system still faces key challenges such as network security risks and high energy consumption. To address these issues, this paper proposes a dual pronged solution. First, a new mechanism integrating smart contracts and asymmetric encryption is designed to achieve secure and efficient user authentication. Then, a method called Lyapunov Deep Deterministic Policy Gradient (L-DDPG) has been proposed to solve the resource optimization of the system. L-DDPG utilizes the Lyapunov optimization framework to transform the original long-term average constraint optimization problem into an instant optimization problem for each time slot, and solves the system optimization variables using the Deep Deterministic Policy Gradient algorithm. Through this design, L-DDPG effectively combines the advantages of Lyapunov optimization in ensuring system stability, as well as the decision modeling ability of deep reinforcement learning in complex state spaces, thereby simultaneously improving the resource utilization efficiency and safety of the system. The experimental results show that compared with existing baseline methods, L-DDPG significantly reduces the average energy consumption of equipment while effectively reducing task response delay, demonstrating better overall performance.},
  archive      = {J_TMC},
  author       = {Qiang He and Yang Xia and Zheng Feng and Lianbo Ma and Yingjie Lv and Keping Yu and Ammar Hawbani and Kaifa Zheng and Li Xu},
  doi          = {10.1109/TMC.2025.3608771},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computation resource management in mobile edge computing for healthcare using lyapunov-deep deterministic policy gradient},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-layer position-pose fusion framework for joint magnetoquasistatic field and IMU positioning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3608822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetoquasistatic (MQS) field positioning has demonstrated significant potential for emergency rescue applications due to its strong penetration and non-reliance on pre-deployment. However, its accuracy is notably impaired by metal interference and distance attenuation. Inertial Measurement Units (IMUs) can reliably provide motion data even in environments affected by metal and electromagnetic interference, but they suffer from cumulative drift over time. Effectively, combining MQS field and IMU positioning to harness their respective advantages presents a crucial challenge. To address this, we propose a Multi-Layer Position-Pose Fusion (MP2F) framework that integrates MQS field with IMU data to enhance position and pose estimation. The MP2F framework comprises three layers: a Quaternion-based Pose Fusion Layer (QPFL), a Kalman Filter-based Position Fusion Layer (KFFL), and a Global Position-Pose Fusion Layer (GP2FL). Specifically, QPFL utilizes the Extended Kalman Filter (EKF) to effectively mitigate magnetic field distortion and IMU drift, thereby significantly enhancing pose estimation precision. Next, KFFL incorporates the fused pose estimation from QPFL into an inertial navigation motion model, and leverages MQS field observations to further improve positional accuracy. Finally, GP2FL formulates a nonlinear least squares optimization problem by marginalizing prior factors, inertial sensor factors, and Kalman fusion outputs, enabling globally optimized state estimation. Comprehensive simulation results and analyses prove that the proposed MP2F framework achieves high-precision position and pose estimation in complex emergency scenarios, with strong robustness. Experimental results in real-world environments show that the proposed MP2F achieves improvements in positioning accuracy of 61.1%, 58.7%, 48.4%, and 50.2% over EKF, iMag+, GWO-PF, and MagLoc, respectively.},
  archive      = {J_TMC},
  author       = {Bocheng Qian and Lei Huang and Xiansheng Guo and Gordon Owusu Boateng and Rui Ma and Nirwan Ansari},
  doi          = {10.1109/TMC.2025.3608822},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A multi-layer position-pose fusion framework for joint magnetoquasistatic field and IMU positioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing dynamic task assignment in spatial crowdsourcing: Bilateral preference-aware approaches. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3603833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task assignment is a crucial challenge in spatial crowdsourcing (SC). Most existing studies have two limitations: Firstly, only one-sided preferences of workers or tasks are taken into account, and the satisfaction of workers or tasks could be improved; Secondly, tasks are always assigned based on the current locations of workers, which is not suitable for many real-life applications, such as carpooling, where the trajectories of workers require to be taken into account. To this end, we investigate a new problem of Bilateral Preference-aware Dynamic Task Assignment (BDTA), which is proven to be NP-hard, to maximize overall satisfaction by incorporating worker-task bilateral preferences and assigns tasks using the trajectories of workers. For the BDTA problem, we first propose a hybrid batch processing framework to address uneven data distribution. After that, a task-initiated bidirectional select algorithm is proposed to mitigates the impact of task order on the matching results. Furthermore, we propose an $\alpha$-approximate task-initiated generalized deferred-acceptance algorithm and a reverse generalized deferred-acceptance algorithm to enhance the stability and overall satisfaction of task assignment results. Extensive experiments are conducted on both real and synthetic datasets to validate the effectiveness and efficiency of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Yang Huang and Yumeng Liu and Xu Zhou and Tianyue Ren and Zhibang Yang and Keqin Li and Kenli Li},
  doi          = {10.1109/TMC.2025.3603833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing dynamic task assignment in spatial crowdsourcing: Bilateral preference-aware approaches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-efficient identifying key tag distribution in large-scale RFID systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of RFID-enabled applications, large-scale RFID systems often require multiple readers to ensure full coverage of numerous tags. In such systems, we sometimes pay more attention to a subset of tags instead of all, which are called key tags. This paper studies an under-investigated problem key tag distribution identification, which aims to identify which key tags are beneath which readers. This is crucial for efficiently managing specific items of interest, which can quickly pinpoint key tags and help RFID readers covering these tags collaborate to improve the tag inventory efficiency. We propose a protocol called Kadept that identifies the key tag distribution by designing a sophisticated Cuckoo filter that teases out key tags as well as assigns each of them a singleton slot for response. With this design, a great number of trivial (non-key) tags will keep silent and free up bandwidth resources for key tags, and each key tag is sorted in a collision-free way and can be identified with only 1-bit response, which significantly improves the time efficiency. To enhance the scalability and efficiency of Kadept for high key tag proportions, we propose E-Kadept protocol, which accelerates the identification process by designing an incremental Cuckoo filter that reduces false positives and improves space efficiency. We theoretically analyze how to optimize protocol parameters of Kadept and E-Kadept, and conduct extensive simulations under different tag distribution scenarios. Compared with the state-of-the-art, E-Kadept can improve the time efficiency by a factor of 1.75×, when the ratio of key tags to all tags is 0.3.},
  archive      = {J_TMC},
  author       = {Yanyan Wang and Jia Liu and Zhihao Qu and Shen-Huan Lyu and Bin Tang and Baoliu Ye},
  doi          = {10.1109/TMC.2025.3609967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Time-efficient identifying key tag distribution in large-scale RFID systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE maximization for laser-powered multi-UAV communication networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3610026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) communication is expected to play an important role in many applications, including emergency services, remote surveillance and even daily logistics, thanks to its great advantages such as flexible deployment and mobility support. In this paper, we propose a multi-UAV-aided communication scheme supported by laser power transfer (LPT), where the quality-of-experience (QoE) requirements of user equipment (UE) is considered. Specifically, to improve the sum QoE, we first maximize the sum of average data rates (ADRs) of all UEs through an alternating optimization of UAVs' positions, UE-network association and LPT station (LPTS)-network association. Then, we devise an advanced Gale-Shapley rematching (GSRM) scheme to address the intractable 0-1 programming problem in UE/LPTS-network association. Moreover, an L2-norm polynomial (L2NP) programming method is designed to transform the L2NP of the LPT-based UAV positioning problem into a convex form. Finally, a redundant resource reallocation (RRR) algorithm is designed to recycle and reallocate the excessive transmit power and backhaul capacity of both the base station (BS) and the UAVs, to further maximize the number of UEs satisfying the QoE requirements. Simulation results show that the proposed UAV-aided communication scheme can help more UEs achieve their QoE requirements at a reduced power consumption compared with existing solutions.},
  archive      = {J_TMC},
  author       = {Jianchao Chen and Ming Jiang},
  doi          = {10.1109/TMC.2025.3610026},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE maximization for laser-powered multi-UAV communication networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards the age of semantic information: A deep learning-enabled generalized deduplication-based semantic transmission mechanism. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3609792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the upcoming global-coverage 6G networks, high packet loss and long latency in long-distance transmissions exacerbate the trade-off between data timeliness and integrity, particularly in time-sensitive applications involving time-series data with stringent integrity requirements. This challenge exposes the limitations of existing transmission systems, such as source-channel coding and semantic communication, which fail to jointly address both dimensions. In this paper, we propose a deep learning (DL)-enabled generalized deduplication (GD)-based semantic transmission (DLGD-ST) mechanism for time-series data. By leveraging GD to address the impact of semantic ambiguity on data integrity, DLGD-ST exploits the semantic recovery and temporal discreteness of the data to effectively mitigate the conflict between integrity and timeliness. In particular, a well-designed long-short-term memory (LSTM)-based GD algorithm is developed to separate shallow semantic components and supplementary components, ensuring the integrity of semantic transmission. A deep semantic encoding process is then performed using a double-layer progressive dimension reduction (DPDR) and adaptive quantization (AQ) scheme, which capitalizes on the channel robustness of semantics to reduce transmission rounds and improve timeliness. Furthermore, an incremental dimension hybrid automatic repeat request (ID-HARQ) mechanism is introduced to improve semantic reliability by retransmitting high-dimensional semantics, thereby further minimizing end-to-end transmission rounds. To accurately evaluate performance, we introduce the Age of Semantic Information (AoSI), which incorporates integrity constraints into the generalized Age of Information (AoI) to jointly assess integrity and timeliness. Simulation results demonstrate that the proposed DLGD-ST mechanism, enabled by accurate data recovery and reduced transmission rounds, achieves better AoSI performance compared to existing communication systems under both high and low signal-to-noise ratio (SNR) conditions.},
  archive      = {J_TMC},
  author       = {Yunlai Xu and Ronghao Gao and Qinyu Zhang and Zhihua Yang},
  doi          = {10.1109/TMC.2025.3609792},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards the age of semantic information: A deep learning-enabled generalized deduplication-based semantic transmission mechanism},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase-proof: Robust mobile two-factor authentication via phase fingerprinting. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3610154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-factor authentication (2FA) has been widely applied with the proliferation of mobile devices. Currently, many existing 2FA solutions propose to use acoustic fingerprints as the second factor. However, these schemes mainly consider using the magnitude or non-linearity information of the acoustic signal for authentication and ignore the fingerprint variations caused by the change of transmission distance between devices, which could threaten the accuracy of the system. To address these vulnerabilities, we propose Phase-Proof, a secure and robust 2FA that utilizes the phase distortions incurred by the acoustic elements of mobile devices as the second proof. Given the received acoustic signal, our system first extracts phase information from the signal and designs a new distance effect elimination scheme to remove the impact of transmission distances for robust fingerprint extraction. Moreover, our device authentication component then adopts a transfer learning-based method to capture the subtle differences in devices' fingerprints for accurate device authentication. Moreover, to further withstand the co-located attacks, our proximity detection component collects certain environmental randomness from the enrolled phone, and then matches it with the environmental randomness collected from the login device to verify the proximity of two devices. Our experimental results show that our system is accurate in providing 2FA and robust to various attacks across different scenarios.},
  archive      = {J_TMC},
  author       = {Tingyuan Yang and Shuyu Liu and Yanzhi Ren and Haitao Jia and Ziyu Shao and Hongbo Liu and Jiadi Yu and Hongwei Li},
  doi          = {10.1109/TMC.2025.3610154},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Phase-proof: Robust mobile two-factor authentication via phase fingerprinting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed rate limiting under decentralized cloud networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of cloud applications has led to unprecedented increases in network traffic volume, diversity, and complexity. As Cloud Service Providers (CSPs) adopt decentralized, geographically distributed data centers, effective traffic management across these environments has become critical. Distributed Rate Limiting (DRL) has emerged as an essential tool to manage the complex traffic dynamics of decentralized networks, yet traditional centralized rate limiting methods fall short, facing limitations in scalability, adaptability to bursty traffic, and efficiency. This paper presents C3PDAR (Cloud Control with Constant Probabilities and Dynamic Adjustment Range), a novel DRL algorithm tailored for decentralized cloud infrastructures. C3PDAR introduces three key innovations: (1) CPS-BPS DualPoint Rate Limiting and Parent-Child Token Bucket mechanisms, which effectively mitigate burst traffic and short-lived connections while improving bandwidth fairness and inter-tenant isolation; (2) A vSwitch-CGW Cascade Rate Limiting architecture, which reduces CPU overhead in CGW clusters and accelerates convergence by 42%–78%; (3) Virtual Extensible Local Area Network (VXLAN) Padding scheme, which embeds rate-limiting information in existing traffic instead of transmitting new data packets, reducing the communication overhead of the C3PDAR algorithm by over 40%. By integrating these advancements, C3PDAR delivers a scalable, robust solution that outperforms traditional DRL approaches in performance, fault tolerance, and resource efficiency. C3PDAR uniquely empowers CSPs to manage complex, high-volume traffic dynamics in decentralized cloud environments, offering both theoretical insights and practical optimizations for next-generation network control.},
  archive      = {J_TMC},
  author       = {Xiang Hu and Tianyu Xu and Lilong Chen and Xiaochong Jiang and Ye Yang and Liming Ye and Xu Wang and Yilong Lv and Chenhao Jia and Yongwang Wu and Zhigang Zong and Xing Li and Bingqian Lu and Shunmin Zhu and Chengkun Wei and Wenzhi Chen},
  doi          = {10.1109/TMC.2025.3610314},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed rate limiting under decentralized cloud networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentivizing throughput enhancement in blockchain-based energy trading system. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based energy trading (BBET) systems depend on prosumers to allocate energy between trading activities and blockchain mining operations. However, inadequate incentive structures lead prosumers to under-contribute to mining, creating throughput bottlenecks and system performance degradation. This paper introduces the Fee and Two-Piece Compensation (FTPC) mechanism to optimize energy allocation and enhance system throughput. We formulate the interaction between the system designer and prosumers as a three-stage Stackelberg game where the system designer establishes the incentive framework in Stage I, while prosumers determine energy allocation in Stage II and set transaction fees in Stage III. Our analysis demonstrates that prosumers' failure to internalize mining's positive externality results in suboptimal throughput investment. Counterintuitively, we show that impatient prosumers may exploit others' mining contributions as free riders. The FTPC mechanism resolves these issues by jointly optimizing transaction fees and compensation structures to align individual incentives with social welfare. We prove that FTPC achieves socially optimal outcomes through fully decentralized decision-making. Numerical evaluation shows FTPC improves social welfare and prosumer payoffs by 88.1% and 87.8%, respectively. Ethereum testbed implementation validates equilibrium convergence through iterative best-response dynamics.},
  archive      = {J_TMC},
  author       = {Yunshu Liu and Man Hon Cheung and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3610648},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentivizing throughput enhancement in blockchain-based energy trading system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic searchable symmetric encryption with efficient and complete access control for multi-user cloud computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable symmetric encryption (SSE) enables the storage and retrieval of encrypted data on untrusted cloud servers, while dynamic searchable symmetric encryption (DSSE) further supports updating encrypted data. To date, in multi-user environments, most DSSE schemes cannot achieve simultaneous access control for both keyword retrieval and data updates. To address this issue, we propose a new DSSE scheme with efficient and complete (keyword retrieval and update) access control for multi-user environments, named EFCAM. Our work has simultaneously achieved efficient, flexible, and fine-grained access control for keyword retrieval and updating, this is extremely rare in existing research. For update operations, we combine file index encoding and homomorphic encryption (HE) technology, so that EFCAM optimizes the calculation; to achieve flexible access control, we adopt an equality test scheme that can supports three types of update authorization. For retrieval operations, users do not need to share keys. By executing a single query, the users can effectively retrieve all the data that they have permission to access. To enhance system security and operational efficiency, we have extended EFCAM with a dynamic policy update mechanism for flexible and real-time adjustment of access control policies. We formally analyze the security of EFCAM to prove that our scheme has forward security (FS) and backward security (BS). Experimental results show that, EFCAM maintains outstanding efficiency in encrypted data retrieval and update operations within multi-user environments, while also exhibiting strong scalability.},
  archive      = {J_TMC},
  author       = {Liqun Yang and Yuze Yang and Dusit Niyato and Zhoujun Li and Wanxu Xia and Liang Sun},
  doi          = {10.1109/TMC.2025.3609829},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic searchable symmetric encryption with efficient and complete access control for multi-user cloud computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Octopus: Optimizing interactive video QoE via loosely coupled codec-transport adaptation. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the quality of experience (QoE) in interactive video streaming (IVS) remains a persistent challenge due to the need for ultra-low latency and rising bandwidth demands. Conventional algorithms, whether rule-based or learning-based, are obsessed with achieving tight coupling between encoding and sending bitrate adaptations for low-latency guarantee. However, our measurement studies reveal alarming harms of tight coupling in suppressing throughput, encoding bitrates and smoothness, as application- and transport-layer bitrate adaptations inherently have different mechanisms and goals. To tackle this problem, we propose Octopus, the first loosely coupled cross-layer bitrate adaptation algorithm for IVS to maximize QoE. Instead of blind synchronization, Octopus promotes mutual cooperation and independence between encoding and sending bitrate adaptations by integrating a multi-head network with shortcut connections and auto-regressive action modules. Additionally, based on meta-imitation reinforcement learning, we design a network condition-aware online adaptation scheme that enables the loosely coupled policy to swiftly adapt to diverse and dynamic wireless networks. We implement Octopus on a testbed, a microcosm of real-world deployment, with transceiver pairs running WebRTC on the WeChat for Business dataset. Results show that Octopus outperforms state-of-the-art algorithms, either improving bitrates by 37.1%, or optimizing stalling rate and smoothness by 54.1% and 9.2%, or achieving all-around improvements.},
  archive      = {J_TMC},
  author       = {Xuedou Xiao and Mingxuan Yan and Yingying Zuo and Boxi Liu and Paul Ruan and Yang Cao and Yue Cao and Wei Wang},
  doi          = {10.1109/TMC.2025.3610501},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Octopus: Optimizing interactive video QoE via loosely coupled codec-transport adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERA: A QoE-aware collaborative inference algorithm for NOMA-based edge intelligence. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although AI has been extensively adopted and has profoundly transformed our lives, it is not feasible to directly deploy large AI models on edge devices with limited resources. To enhance the performance of Edge Intelligence (EI), model split inference has been proposed. In this approach, an AI model is segmented into sub-models, with the most resource-intensive parts offloaded wirelessly to the edge server. This reduces the resource demands and inference latency on the device. However, previous studies have primarily focused on enhancing and optimizing system Quality of Service (QoS), often overlooking Quality of Experience (QoE), which is another crucial aspect for users. Even though QoE has been extensively studied in Edge Computing (EC), the distinct differences between task offloading in EC and split inference in EI, along with specific QoE issues that remain unaddressed in both fields, render these algorithms ineffective for edge split inference scenarios. Therefore, this paper introduces an effective resource allocation algorithm, dubbed ERA, which aims to: 1) expedite split inference in EI, and 2) balance inference delay, QoE, and resource consumption. ERA incorporates resource consumption, QoE, and inference latency to determine the most optimal model split and resource allocation strategies. Given that it is impossible to simultaneously minimize inference delay and resource consumption while maximizing QoE, we employ a gradient descent-based algorithm to find the best possible compromise. Furthermore, to address the complexity arising from parameter discretization in the gradient descent algorithm, we have developed a pipeline gradient descent approach, known as PipGD. We have also examined the properties of the proposed algorithms, including their convergence, complexity, and approximation error. The experimental results clearly show that ERA outperforms previous studies significantly in terms of performance.},
  archive      = {J_TMC},
  author       = {Xin Yuan and Ning Li and Quan Chen and Wenchao Xu and Song Guo},
  doi          = {10.1109/TMC.2025.3610699},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ERA: A QoE-aware collaborative inference algorithm for NOMA-based edge intelligence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient privacy-preserving federated learning via homomorphic encryption-enabled over-the-air computation. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables collaborative model training across devices, but data exchanges pose privacy risks. Homomorphic Encryption (HE) is widely used to enhances privacy in FL but incurs significant communication and computation latency. Prior work reduced this latency using compressions, but sacrificed learning accuracy and overlooked the impact of the number of participating devices on latency. Over-the-air computation (AirComp) leverages wireless channels' superposition property to achieve high spectral efficiency and efficient aggregation irrespective of device number. In this paper, we propose HEAirFed, integrating AirComp with the state-ofthe-art HE scheme CKKS for efficient privacy-preserving FL. In HEAirFed, we develop a ciphertext-oriented wireless communication module to ensure homomorphic operations leverage AirComp's superposition property, enabling correct decryption. We further build a rigorous error analysis model, derive the worst-case upper bound of approximation error, and characterize this bound's impact on the convergence guarantee of HEAirFed, measured by the optimality gap with bounded approximation error. Then, we minimize this gap and derive a near-optimal solution in semi-closed form. Extensive experimental results on real-world datasets validate the ciphertext-oriented design's necessity, the error analysis's correctness, and demonstrate that HEAirFed achieves a substantial reduction in communication and aggregation latency compared to baseline, with minimal learning accuracy loss.},
  archive      = {J_TMC},
  author       = {Yehui Wang and Baoxian Zhang and Jinkai Zhang and Cheng Li},
  doi          = {10.1109/TMC.2025.3610887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient privacy-preserving federated learning via homomorphic encryption-enabled over-the-air computation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Singular value decomposition based indoor localization using small scale crowd sensing data. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional crowd sensing based indoor localization methods rely on large scale pre-collected fingerprint data to construct a radio map with cumbersome prior preparation. However, when they lack floor plan information or only have a little of data is willing to share, the tracking accuracy degrades significantly. In this paper, we propose a singular value decomposition (SVD) track matching scheme to obtain an effective radio map based on small scale crowd sensing data, which is a non-learning based system (SVD-CSP). SVD-CSP fuses received signal strength indicator (RSSI), inertial measurement unit (IMU), and magnetic field strength to label surrounding WiFi access points as marker points. The proposed scheme uses SVD method to directly compute the rotation matrix and displacement vector among the crowd sensing trajectories and attain the reliable tracks. The radio map is constructed and users are tracked according to our developed bidirectional Bayesian filter, which contains forward filter and reverse filter. The density-based spatial clustering of applications with noise (DBSCAN) is embedded within the forward filter to improve the radio map quality. Meanwhile, the reverse filter fuses pedestrian dead reckoning (PDR) and radio map-based localization to track users. Experimental results demonstrate that SVD-CSP can achieve robust localization using extremely sparse crowd trajectories (e.g., 4 trajectories in a 648 m2 scenario, 30 trajectories in a 2856 m2 scenario) without deep learning training or infrastructure knowledge.},
  archive      = {J_TMC},
  author       = {Xiaohao Liu and Yubin Zhao and Xiaofan Li and Huaming Wu and Cheng-Zhong Xu},
  doi          = {10.1109/TMC.2025.3610882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Singular value decomposition based indoor localization using small scale crowd sensing data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating stable matching between workers and spatial-temporal tasks for dynamic MCS: A stagewise service trading approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing effective incentive mechanisms in mobile crowdsensing (MCS) networks is crucial for engaging distributed mobile users (workers) to contribute heterogeneous data for various applications (tasks). In this paper, we propose a novel stagewise trading framework to achieve efficient and stable task-worker matching, explicitly accounting for task diversity (e.g., spatio-temporal limitations) and network dynamics inherent in MCS environments. This framework integrates both futures and spot trading stages. In the former, we introduce the futures trading-driven stable matching and pre-path-planning mechanism (FT-SMP3), which enables long-term taskworker assignment and pre-planning of workers' trajectories based on historical statistics and risk-aware analysis. In the latter, we develop the spot trading-driven DQN-based path planning and onsite worker recruitment mechanism (ST-DP2WR), which dynamically improves the practical utilities of tasks and workers by supporting real-time recruitment and path adjustment. We rigorously prove that the proposed mechanisms satisfy key economic and algorithmic properties, including stability, individual rationality, competitive equilibrium, and weak Pareto optimality. Extensive experiements further validate the effectiveness of our framework in realistic network settings, demonstrating superior performance in terms of service quality, computational efficiency, and decision-making overhead.},
  archive      = {J_TMC},
  author       = {Houyi Qi and Minghui Liwang and Xianbin Wang and Liqun Fu and Yiguang Hong and Li Li and Zhipeng Cheng},
  doi          = {10.1109/TMC.2025.3610915},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accelerating stable matching between workers and spatial-temporal tasks for dynamic MCS: A stagewise service trading approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive decentralized federated learning in energy and latency constrained wireless networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3611075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derived convergence bound reveals the impact of the rounds of local training on the model performance. Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained. Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different graph-based aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios. Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed. Simulation results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes.},
  archive      = {J_TMC},
  author       = {Zhigang Yan and Dong Li and Qiang Sun and Dusit Niyato and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3611075},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive decentralized federated learning in energy and latency constrained wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating interference for automotive millimeter-wave radar perception in dense traffic scenarios. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3610963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive Millimeter-wave (mmWave) radar is becoming an essential modality for autonomous vehicles to enable all-weather perception, especially when LiDAR and camera fail in foggy, rainy, or snowy conditions. It is expected that the mutual interference among multiple radars becomes a critical issue in dense traffic scenarios, which can severely degrade the radar performance and lead to accidents. Despite extensive interference mitigation techniques, none can meet the less valid signal distortion while high robustness requirements for automotive radar perception in dense traffic scenarios. To overcome this predicament, we propose mmMic, a novel multiple mutual interference mitigation system that can accurately separate interference and recover valid signals to maintain the reliability of the radar measurements. The key insight is to design an interference estimator that can accurately localize the interference signal according to its linear frequency modulation features in the time-frequency (TF) domain. In addition, mmMic also fully exploits undisturbed valid signal information within an extended time-frequency domain to reconstruct the damaged signal. Our experiments on a real testbed show that mmMic can improve SINR to interference-free levels from multiple radars, achieving an average SINR improvement of 17% compared to the best-performing baseline.},
  archive      = {J_TMC},
  author       = {Wei Wang and Chunshen Li and Bixin Zeng and Lieke Chen and Liang Sun and Kai Luo and Da Chen},
  doi          = {10.1109/TMC.2025.3610963},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating interference for automotive millimeter-wave radar perception in dense traffic scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedBRB: A solution to the small-to-large scenario in device-heterogeneity federated learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3610985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the success of large models has demonstrated the importance of scaling up model sizes. However, it is difficult to directly train large models locally on multiple mobile devices due to their intrinsic computational constraints. To address this challenge, it becomes a crucial need to train larger global models by training small local models on devices. As a distributed learning approach, federated learning (FL) allows multiple devices to train models locally and aggregate them to form the global model by sharing the updated parameters with the server, thus enabling the co-training of models. This promising feature has spurred an increasing interest in exploring the collaborative training of large models. Despite the advent of existing device-heterogeneity FL approaches, they still have limitations in fully covering the parameter space of the global model. To fill this gap, we propose a novel approach called FedBRB (Block- wise Rolling and weighted Broadcast). The core idea of FedBRB is to utilize local models of small devices to train all modules of a large global model and broadcast the trained parameters to the entire space, thereby enabling faster information sharing. This approach not only improves training efficiency but also fully utilizes limited computational resources. Experiments demonstrate that FedBRB can produce significant performance gains, achieving state-of-the-art results. Additionally, this paper provides theoretical and experimental analyses of FedBRB convergence, thereby paving a theoretical ground and providing practical guidance for further research and application of the FedBRB method.},
  archive      = {J_TMC},
  author       = {Tianchi Liao and Ziyue Xu and Qing Hu and Hong-Ning Dai and Huaiwei Huang and Zibin Zheng and Chuan Chen},
  doi          = {10.1109/TMC.2025.3610985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedBRB: A solution to the small-to-large scenario in device-heterogeneity federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid model with bayesian nonparametric inference for RF fingerprint identification. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency fingerprint identification (RFFI) aims to identify subtle impairments in hardware devices, which play an important role in the mobile environment security community. To identify various mobile devices in the complex electromagnetic environment, deep learning methods such as convolutional neural networks (CNN) and recurrent neural networks (RNN) have been adopted to extract device hardware-related features. However, the single network structure has difficulty in comprehensive feature extraction, as many factors can introduce hardware impairments. In this paper, we propose a hybrid model termed switching dynamical deep network (SDDN) for RFFI tasks, which can jointly extract both coarse-grained radio frequency fingerprints (RFFs) and fine-grained RFFs. Additionally, the proposed hybrid model consists of a probabilistic part and a deterministic part. Specifically, in the probabilistic part, the switching linear dynamical systems (SLDS) are incorporated to establish the correspondence between the signal slice and the feature extraction network (FEN). In the deterministic part, multiple independent FENs are established to extract the RFFs. Moreover, to automatically determine the suitable number of FENs, a Bayesian nonparametric prior distribution is placed over the probabilistic part. Finally, an end-to-end parameter optimization method that is based on variational inference and stochastic gradient descent is proposed. Experiments on a real-life Wi-Fi dataset demonstrate the superiority of the proposed method over existing methods.},
  archive      = {J_TMC},
  author       = {Jian Yang and Jiadi Bao and Luyao Zhang and Yatong Wang and Fang Yang and Shafei Wang},
  doi          = {10.1109/TMC.2025.3611135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A hybrid model with bayesian nonparametric inference for RF fingerprint identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple CPUs cooperation for CF massive MIMO with MmWave fronthaul and backhaul. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive multiple-input multiple-output (CF massive MIMO) is regarded as a promising technology for next-generation wireless communication systems. However, relying on a single central processing unit (CPU) in CF massive MIMO systems is not scalable in practical networks, requiring the introduction of multiple CPUs for more efficient and feasible transmission. In this paper, we investigate a CF massive MIMO system with multiple CPUs. To obtain flexible and cost-efficient deployment, we propose to use wireless x-haul links instead of wired ones. More specifically, we assume that both the fronthaul links from the APs to the corresponding CPU and the backhaul links between CPUs operate under millimeter wave (mmWave) networks. Taking into account a tradeoff between the degree of centralized coordination and the signal overhead on the backhaul links, we consider four levels of multiple CPUs cooperation schemes from fully centralized to fully distributed. In addition, we propose a binary search method to allocate the backhaul capacities for maximizing the sum spectral efficiency (SE). Simulation results show that mmWave backhaul amplifies the compression noise introduced by mmWave fronthaul, leading to a more pronounced impact on the SE of systems. In this case, the centralized processing scheme can generate more compression noise due to the larger data overhead on the backhaul link, making the distributed processing scheme a superior processing scheme, especially when dealing with a large number of APs or significant distances between CPUs.},
  archive      = {J_TMC},
  author       = {Feiyang Li and Qiang Sun and Jiayi Zhang and Cunhua Pan and Kai-Kit Wong},
  doi          = {10.1109/TMC.2025.3611133},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multiple CPUs cooperation for CF massive MIMO with MmWave fronthaul and backhaul},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active data routing based on reward backpropagation-enabled multi-agent Q-learning towards SDN-enabled wireless buoy networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3611873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in Wireless Buoy Network (WBN) have significantly accelerated the development of marine exploitation and monitoring, acting as a relay between underwater and surface networks in emerging 6G scenarios. Due to unstable maritime communication environment, it is a challenging issue to deploy the optimal data routing or collection strategies to ensure the collected data to be delivered to the target point. By employing the Software-Defined Networking (SDN) technology, this paper proposes the paradigm of Software-Defined WBN (SDWBN) to improve the network management efficiency and provide a platform to embed the Multi-Agent Reinforcement Learning (MARL) framework (for data routing intelligence), respectively. On account of the proposed SDWBN, this paper proposes a Reward Backpropagation-enabled Multi-Agent Deep Q-learning algorithm (RBMADQ)-based active routing scheme, which aims to assist buoys in making routing decisions and navigating the challenges posed by the dynamic and unstable communication environment. Further, this paper proposes a dual replay buffer-based training method, to enhance the convergence speed of the proposed RBMADQ-based routing scheme. Evaluation results demonstrate that the proposed routing scheme performs better compared with recent research products, with a higher packet delivery rate, lower network latency, and simultaneously, less communication overhead, etc.},
  archive      = {J_TMC},
  author       = {Yuan Liu and Guangjie Han and Chuan Lin and Shengchao Zhu},
  doi          = {10.1109/TMC.2025.3611873},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Active data routing based on reward backpropagation-enabled multi-agent Q-learning towards SDN-enabled wireless buoy networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint UAVs deployment and resource allocation for AoI-aware RIS-assisted UAV-USV MEC network. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age of information (AoI)-sensitive bidirectional computation tasks quality of service (QoS) for unmanned surface vehicles (USVs) is a critical issue in realizing ship-shore cooperative systems. In this paper, a reconfigurable intelligent surface (RIS)-assisted unmanned aerial vehicle (UAV)-USV mobile edge computing (MEC) network architecture is proposed, where one RIS-carried tethered UAV (TUAV) and rotary-wing UAVs (RUAVs) are cooperatively dispatched to serve USVs bidirectional data computation with average AoI (AAoI) constraint. The minimization of weighted sum USVs AAoI and RUAVs flight energy is formulated by jointly considering RUAVs service duration indicators, TUAV-mounted RIS phase shift, TUAV hovering altitude, and RUAVs' trajectories. A heursitic solution is proposed to address this minimized issue. In particular, a novel mixed linear quadratic Lyapunov framework is utilized to transform the original long-term stochastic problem into a list of deterministic single-slot problems. Then, each single-slot problem is divided into two subproblems. First, the subproblem of RUAVs' trajectories is tackled by an enhanced whale optimization algorithm. Second, the subproblem of RUAVs service duration indicators, TUAV-mounted RIS phase shift and TUAV hovering altitude is addressed by an enhanced alternating optimization algorithm. The results demonstrate that the proposed heuristic solution reduces long-term RUAVs flight energy consumption by approximately 50% while maintaining satisfactory USVs AAoI.},
  archive      = {J_TMC},
  author       = {Yangzhe Liao and Yuanyan Song and Dan Song},
  doi          = {10.1109/TMC.2025.3611808},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint UAVs deployment and resource allocation for AoI-aware RIS-assisted UAV-USV MEC network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-CrowdCache: A decentralized game-theoretic model for edge content sharing over time-varying communication networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3611963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a promising solution for enhancing user experience, minimizing content delivery expenses, and reducing backhaul traffic. This paper presents a game-theoretic framework to address the edge resource crowdsourcing problem, where mobile edge devices (MEDs) provide idle storage for content caching in exchange for rewards from a content provider (CP). We model the interaction between the CP and MEDs as a Stackelberg game, with the CP as the leader setting the reward structure and the MEDs as followers competing in a non-cooperative game for these rewards. We propose a novel privacy-preserving method to derive the Stackelberg equilibrium of the game. Notably, our algorithm is designed to operate effectively in time-varying communication networks, addressing the high mobility inherent in MEC environments. This contrasts with state-of-the-art algorithms, which assume a static communication network among MEDs–an impractical condition that does not account for the mobility of MEDs during algorithm execution. Specifically, our approach employs consensus-based algorithms to compute the Nash equilibrium (NE) for MEDs, with MEDs exchanging NE profile estimates with neighbors via row-stochastic mixing matrices and performing gradient steps to optimize their utility in a fully decentralized manner. Based on the computed NE strategies, we propose a zeroth-order reward search algorithm for the CP to determine the optimal strategy for profit maximization. Our comprehensive analysis details the properties of the equilibrium and establishes the geometric convergence of the proposed algorithms to the NE. We also derive explicit bounds for the stepsizes based on the game's properties and the graphs' connectivity structure. Extensive numerical results validate the efficacy of our proposed approach.},
  archive      = {J_TMC},
  author       = {Duong Thuy Anh Nguyen and Jiaming Cheng and Ni Trieu and Duong Tung Nguyen and Angelia Nedic},
  doi          = {10.1109/TMC.2025.3611963},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Bi-CrowdCache: A decentralized game-theoretic model for edge content sharing over time-varying communication networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient multi-UAV navigation for cooperative data sensing and transmission. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3612221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) hold significant potential for sensing services in a large scope of area, thanks to their wide coverage and adaptable deployment. Considering the complex environment dynamics and limited sensing range, navigating multiple UAVs in a distributed way becomes challenging to implement cooperative data sensing and transmission tasks. In this paper, we optimize the trajectory design of UAVs by jointly considering the collected data volume, geographical fairness and limited energy reserve during their service period. To achieve the long-term serving objective, a memory augmented multi-agent deep reinforcement learning approach is presented to ensure energy-efficient distributed trajectory design with partial observations. Specifically, the intrinsic criterion is developed to enhance UAV spatial exploration when reaching the boundary of explored regions. Then, to address the information loss caused by incomplete observations, the spatial-temporal memory augmented actor-critic architecture is designed to extract historical contextual features for multi-UAV cooperative navigation. Furthermore, the prioritized experience replay mechanism is incorporated to enhance important experience exploitation for UAV collaboration. Extensive simulations using two real-world datasets in Shenzhen and Beijing demonstrate that the proposed method outperforms the state-of-the-art methods in terms of data collection ratio, geographical fairness, and energy consumption ratio.},
  archive      = {J_TMC},
  author       = {Hu He and Jun Peng and Lin Cai and Weirong Liu and Chenglong Wang and Xin Gu and Zhiwu Huang},
  doi          = {10.1109/TMC.2025.3612221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient multi-UAV navigation for cooperative data sensing and transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossSense: Enabling cross-technology sensing between WiFi and LoRa. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3612289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive increase in wireless devices, enabling sensing between incompatible radios has become critically beneficial. Integrating diverse IoT devices enhances sensing accuracy by providing richer data, while utilizing the diverse characteristics of heterogeneous signals meets sensing needs in complex environments. However, most existing wireless sensing methods primarily focus on homogeneous signals, while research on sensing with heterogeneous signals is still in its infancy. In this paper, we propose CrossSense, a novel Cross-Technology Sensing (CTS) framework that enables sensing between incompatible WiFi and LoRa device. CrossSense recovers the fine-grained trajectory of a WiFi transmitter based on its emulated LoRa signals. To decompose the motion feature components of WiFi transmitter, we develop a chirp difference vector model that utilizes the energy peak within each chirp window for sensing. We model the relationship between sampling frequency offsets and oscillation frequency offsets among heterogeneous devices to guide the extraction of motion features from the emulated signal. We also propose a greedy-based peak enhancement method to calculate the optimized LoRa phases, minimizing the impact of phase discontinuity caused by cyclic prefix (CP) errors. We implement a prototype of CrossSense on the USRP platform. The extensive experiments demonstrate that CrossSense can achieve an efficient Cross-Technology Sensing with $2.92cm$ distance accuracy and $0.26cm/s$ speed accuracy over a $120m$ sensing range.},
  archive      = {J_TMC},
  author       = {Dan Xia and Guanghui Chen and Fu Yu and Yuxuan Wang and Xiaolong Zheng and Liang Liu and Shanguo Huang and Huadong Ma},
  doi          = {10.1109/TMC.2025.3612289},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrossSense: Enabling cross-technology sensing between WiFi and LoRa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unity is power: Semi-asynchronous collaborative training of large-scale models with structured pruning in resource-limited clients. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3612427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study to release the potential of massive heterogeneous weak computing power to collaboratively train large-scale models on dispersed datasets. In order to improve both efficiency and accuracy in resource-adaptive collaborative learning, we take the first step to consider the unstructured pruning, varying submodel architectures, knowledge loss, and straggler challenges simultaneously. We propose a novel semiasynchronous collaborative training framework, namely Co-S2P, with data distribution-aware structured pruning and cross-block knowledge transfer mechanism to address the above concerns. Furthermore, we provide theoretical proof that Co-S2P can achieve asymptotic optimal convergence rate of O(1/√ N∗EQ). Finally, we conduct extensive experiments on two types of tasks with a real-world hardware testbed including diverse IoT devices. The experimental results demonstrate that Co-S2P improves accuracy by up to 8.8% and resource utilization by up to 1.2× compared to state-of-the-art methods, while reducing memory consumption by approximately 22% and training time by about 24% on all resource-limited devices.},
  archive      = {J_TMC},
  author       = {Yan Li and Xiao Zhang and Mingyi Li and Guangwei Xu and Feng Chen and Yuan Yuan and Yifei Zou and Mengying Zhao and Jianbo Lu and Dongxiao Yu},
  doi          = {10.1109/TMC.2025.3612427},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unity is power: Semi-asynchronous collaborative training of large-scale models with structured pruning in resource-limited clients},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient detection framework adaptation for edge computing: A plug-and-play neural network toolbox enabling edge deployment. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3612469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, edge computing has emerged as a prevailing paradigm in applying deep learning-based object detection models, offering a promising solution for time-sensitive tasks. However, existing edge object detection faces several challenges: 1) These methods struggle to balance detection precision and model lightweightness. 2) Existing generalized edge-deployment designs offer limited adaptability for object detection. 3) Current works lack real-world evaluation and validation. To address these challenges, we propose the Edge Detection Toolbox (ED-TOOLBOX), which leverages generalizable plug-and-play components to enable edge-site adaptation of object detection models. Specifically, we propose a lightweight Reparameterized Dynamic Convolutional Network (Rep-DConvNet) that employs a weighted multi-shape convolutional branch structure to enhance detection performance. Furthermore, ED-TOOLBOX includes a Sparse Cross-Attention (SC-A) network that adopts a localized-mapping-assisted self-attention mechanism to facilitate a well-crafted Joint Module in adaptively transferring features for further performance improvement. Moreover, we propose an Efficient Head for the classification and location modules to achieve more efficient prediction. Additionally, in practical industrial scenarios, we identify that helmet detection-one of the most representative edge object detection tasks-overlooks band fastening, which introduces potential safety hazards. To address this, we build a Helmet Band Detection Dataset (HBDD) and apply an edge object detection model optimized by the ED-TOOLBOX to tackle this real-world task. Extensive experiments validate the effectiveness of components in ED-TOOLBOX. In visual surveillance simulations, ED-TOOLBOX-assisted edge detection models outperform six state-of-the-art methods, enabling real-time and accurate detection. These results demonstrate that our approach offers a superior solution for edge object detection.},
  archive      = {J_TMC},
  author       = {Jiaqi Wu and Shihao Zhang and Simin Chen and Lixu Wang and Zehua Wang and Wei Chen and Fangyuan He and Zijian Tian and F. Richard Yu and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3612469},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient detection framework adaptation for edge computing: A plug-and-play neural network toolbox enabling edge deployment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of sensing and data offloading in digital twin-assisted internet of vehicles. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3613397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Twin (DT) technology can build consistent virtual replicas of physical infrastructures by collecting sensed environmental information, and further offers a powerful paradigm for real-time monitoring, simulation, and optimization of vehicular networks. However, there exists a natural trade-off between information sensing and DT reconstruction. Specifically, insufficient sensing or unreliable sensing results may increase DT inconsistency, while exhaustive sensing can incur substantial energy overhead, especially for fast-changing, resource-constrained, and dynamic Internet of Vehicles (IoV) systems. To address this challenge, we investigate a multi-layer DT-IoV framework based on cloud-edge-vehicle collaborations. Through the joint design of task sensing, vehicle selection, and data offloading strategies, we maximize the consistency of DT reconstruction under limited resources. Furthermore, we develop a spatio-temporal similarity-based hierarchical clustering reinforcement learning (HC-RL) algorithm in a mesh-free environment to minimize the selection of invalid vehicles. Simulation results demonstrate the effectiveness of our proposed framework and method in preserving DT model fidelity and minimizing energy consumption and latency.},
  archive      = {J_TMC},
  author       = {Mingan Luan and Jinyang Wu and Zheng Chang and Yuan Gao and Shahid Mumtaz},
  doi          = {10.1109/TMC.2025.3613397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of sensing and data offloading in digital twin-assisted internet of vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards generalization fairness in federated learning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3613253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training in mobile computing, where fairness holds paramount importance. Traditional efforts have largely concentrated on ensuring fairness across clients with different data distributions (i.e., domain- wise generalization fairness). However, the aspect of fairness within classes (class- wise generalization fairness) remains largely unexplored. Thus, an important question arises: is it possible to simultaneously achieve domain- wise and class- wise generalization fairness? Moreover, current approaches often improve model performance on weaker distributions at the cost of compromising performance on stronger ones, introducing another dimension of unfairness. So, can we boost performance on weaker distributions without compromising that on stronger ones? To this end, we introduce a global classifier with super logits distillation strategy to achieve comprehensive generalization fairness. The basic idea is to guide local updating by selecting reliable global supervision via a unified global classifier across multiple clients. First, we use these super logits to improve underperforming distributions while preserving the performance of well-performing distributions, promoting domain- wise generalization fairness. Second, we dynamically allocate local distillation loss weights according to the intra-client and inter-client class fairness, accelerating the training of underperforming classes and enhancing class- wise generalization fairness. Comprehensive experiments demonstrate the enhanced fairness and superior performance of our method, highlighting the significance of fairness in mobile computing environments.},
  archive      = {J_TMC},
  author       = {Mang Ye and Yuhang Chen and Wenke Huang and Hui Cai and Laizhong Cui},
  doi          = {10.1109/TMC.2025.3613253},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards generalization fairness in federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer scheduling in gig platforms using a generative diffusion model with duality guidance. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3613450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, gig platforms have emerged as a new paradigm, seamlessly connecting workers and tasks while leveraging workers' collective intelligence, participation, and shared resources. Traditionally, platforms have operated under the assumption of worker homogeneity, where service capabilities and associated service costs are similar. However, in mobile computing scenarios, such as mobile crowdsensing, the diversity in worker capabilities and costs renders the supply and demand matching into a complex problem characterized by multiple layers of workers possessing distinct attributes. The dynamic nature of incoming task requests requires the continual reallocation of these workers, thereby introducing a time-dependent overhead. In this paper, we introduce a framework, called the Generative Diffusion Model with Duality Guidance, termed Guid, to address the intricate multi-layer scheduling problem. We formalize a time-slotted long-term optimization problem that captures the spatiotemporal dynamics of task requests and worker services, as well as the intricate time-coupled overhead. Our framework employs a generative diffusion model to explore the complex solution space of the problem and generate superior solutions. To effectively manage time coupling, we utilize dual optimization theory to generate time slot-aware information, guiding the generative diffusion model towards solutions that assure long-term performance. We provide a rigorous theoretical analysis demonstrating that our guidance solution ensures a parameterized competitive ratio guarantee relative to the theoretically optimal solution. Our comprehensive experiments further illustrate that the proposed method outperforms benchmark techniques, achieving reduced overhead compared to seven baseline methods.},
  archive      = {J_TMC},
  author       = {Xinyu Lu and Zhanbo Feng and Jiong Lou and Chentao Wu and Guangtao Xue and Wei Zhao and Jie Li},
  doi          = {10.1109/TMC.2025.3613450},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-layer scheduling in gig platforms using a generative diffusion model with duality guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Courier working time aware vehicle scheduling for efficient urban logistics. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3613677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Order Transfer from the transfer center to delivery stations is an essential and expensive part of urban logistics. Vehicles are scheduled to send transferred orders to multiple delivery stations sequentially in one transfer trip. A key problem is to generate the vehicle's route for efficient order transfer, i.e., minimizing average order transfer time. In this paper, we explore fine-grained delivery station features, i.e., downstream couriers' remaining working times in last-mile delivery trips and the transferred order distribution to design a Prediction-and-Scheduling framework for efficient Order Transfer called PSOT+, including three main components: i) a Courier's Remaining Working Time Prediction component to predict each courier's working time for conducting heterogeneous tasks with attention-based route generation and multi-task-based working time and workload coprediction; ii) a Single Vehicle Scheduling component to generate a vehicle's route to delivery stations with an order-transfertime-aware heuristic algorithm; and iii) a Capacity-Constrained Vehicle Scheduling component to generate multiple vehicles' cooperative order transfer routes with a capacity-constrained inter-exchange algorithm. The evaluation results with real-world data from one of the largest logistics companies in China show PSOT+ improves the courier's remaining working time prediction effectively and reduces the average order transfer time by up to 59% compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Wenjun Lyu and Haotian Wang and Yiwei Song and Shuai Wang and Yunhuai Liu and Tian He and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3613677},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Courier working time aware vehicle scheduling for efficient urban logistics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end orchestration of NextG media services over the distributed compute continuum. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3613949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NextG (5G and beyond) networks, through the increasing integration of cloud/edge computing technologies, are becoming highly distributed compute platforms ideally suited to host emerging resource-intensive and latency-sensitive applications (e.g., industrial automation, extended reality, distributed AI). The end-to-end orchestration of such demanding applications, which involves function/data placement, flow routing, and joint communication/computation/storage resource allocation, requires new models and algorithms able to capture: (i) their disaggregated microservice-based architecture, (ii) their complex processing graph structures, including multiple-input multiple-output processing stages, and (iii) the opportunities to efficiently share and replicate real-time data streams that may be useful for multiple functions and/or end users. To this end, we first identify the technical gaps in existing literature that prevent efficiently addressing the optimal orchestration of emerging applications described by information-aware directed acyclic graphs (DAGs). We then leverage the recently proposed Cloud Network Flow optimization framework and a novel functionally-equivalent DAG-to-Forest graph transformation procedure to design IDAGO (Information-Aware DAG Orchestration), a polynomial-time multi-criteria approximation algorithm for the optimal orchestration of NextG media services over NextG compute-integrated networks. Results show that IDAGO's multiplicative cost reductions over leading baselines scale linearly with aggregate service load, reaching up to 3X gains in scenarios based on AWS and Unreal Engine data under moderate service loads.},
  archive      = {J_TMC},
  author       = {Alessandro Mauro and Antonia M. Tulino and Jaime Llorca},
  doi          = {10.1109/TMC.2025.3613949},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-to-end orchestration of NextG media services over the distributed compute continuum},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreeBeacon: Efficient communication and data aggregation in battery-free IoT. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3614227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve sustainability, Internet-of-Things (IoT) is increasingly adopting battery-free devices powered by ambient energy scavenged from the environment. The unpredictable availability of ambient energy leads to device intermittency, bringing critical challenges to device communication and related fundamental operations like data aggregation. We propose FreeBeacon, a novel scheme for efficient communication and data aggregation in battery-free IoT. We argue that the communication challenge between battery-free devices originates from the complete uncertainty of the environment. FreeBeacon is built on the insight that by introducing just a small degree of certainty into the system, the communication problem can be largely simplified. To this end, FreeBeacon first introduces a small number of battery-powered devices as beacons for battery-free devices. Then, FreeBeacon features protocols for battery-free devices to achieve interaction with the beacon and to perform communication efficiently following customized schedules that implement different data aggregation schemes while achieving resilience. We evaluate FreeBeacon with extensive prototype-based experiments and simulation studies. Results show that FreeBeacon can consistently achieve an order of magnitude data aggregation efficiency when compared with the state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Gaosheng Liu and Kasım Sinan Yıldırım and Lin Wang},
  doi          = {10.1109/TMC.2025.3614227},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreeBeacon: Efficient communication and data aggregation in battery-free IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-based radio scheduler for RAN slicing: Learning to deal with different network scenarios. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3614453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future mobile network schedulers have the complex mission of distributing radio resources among various applications with different requirements. The radio access network (RAN) slicing enables the creation of different logical networks by using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources among the slices to fulfill their requirements. Several recent studies have proposed advances in machine learning-based RRS. However, these works often evaluate their models under limited scenarios and with minimal slice diversity, raising concerns about their real-world applicability. The generalization capabilities of these models remain uncertain without rigorous testing across diverse network conditions and slice configurations, which may hinder their effectiveness upon deployment in operational networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a RAN slicing context. The proposed method protects high-priority slices when the available radio resources are insufficient, using transfer learning to reduce the number of required training steps. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines.},
  archive      = {J_TMC},
  author       = {Cleverson V. Nahum and Salvatore D'Oro and Pedro Batista and Cristiano B. Both and Kleber V. Cardoso and Aldebaro Klautau and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3614453},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intent-based radio scheduler for RAN slicing: Learning to deal with different network scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced security for NextG mobile networks: A hybrid fuzzing approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3614127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents HyFuzz, a hybrid intelligent fuzz testing platform designed to enhance the security validation of next generation (NextG) mobile networks. HyFuzz integrates symbolic formal analysis with adaptive fuzzing to enable the discovery of vulnerabilities that emerge from subtle state inconsistencies and session level command manipulations. Specifically, HyFuzz demonstrates support for multi step intra session fuzzing, where carefully crafted command sequences cause persistent state desynchronization between User Equipment (UE) and the network. Complementing this, HyFuzz employs formal guided deep fuzzing, directing fuzzing efforts to high risk protocol states identified by symbolic analysis. Through a dual mode architecture supporting both virtual (ZMQ) and over the air (OTA) fuzzing, HyFuzz provides an extensible testbed for low level and behavioral vulnerability discovery. Experimental results across 1,281 test cases reveal 1,105 failure instances, including stealthy failures that manifest only under extended interaction contexts. Our findings suggest HyFuzz provides a foundational capability toward more realistic and semantically rich vulnerability detection in modern mobile infrastructure.},
  archive      = {J_TMC},
  author       = {Jingda Yang and Paul Ratazzi and Ying Wang},
  doi          = {10.1109/TMC.2025.3614127},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Advanced security for NextG mobile networks: A hybrid fuzzing approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmWave radar-based unsupervised gesture recognition via image-aligned heterogeneous domain transfer. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3614353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Gesture Recognition (HGR) using mmWave radar has become increasingly promising due to its exceptional contactless perception sensitivity. Conventional approaches predominantly rely on supervised models to learn radar signals, thus incurring substantial costs associated with annotation. To address this limitation, certain works embrace transfer learning to effectively transfer knowledge from labeled source domain to unlabeled target domain, achieving unsupervised recognition in the target domain. However, existing transfer-based methods still necessitate large-scale labeled source domain radar data, thereby constraining their practical applicability. To this end, we propose a novel unsupervised solution for mmWave-based HGR by transferring public image gestures to radar data, eliminating the need for acquiring labeled radar data in source domain. We aim to establish heterogeneous alignment between images and radar signals, facilitating cross-domain transfer. Initially, we mitigate the negative impact of data heterogeneity by employing sophisticated signal processing techniques to convert raw radar signals into gesture trajectories. Subsequently, we introduce an Adversarial-Contrastive Domain Transfer Model (ACDTM) to achieve fine-grained alignment. ACDTM not only confuses the source and target domains by adversarial learning, enabling the acquisition of domain-invariant features, but also designs a robust similarity matrix to facilitate intra-class alignment through contrastive learning. Additionally, ACDTM conducts adversarial self-training on target domain with pseudo-labeled distribution. Our experimental findings substantiate that the unsupervised accuracy achieves about 80$\sim$92% on different mmWave gesture datasets, outperforming existing unsupervised HGR schemes by large margins. Code is available at https://github.com/onlinehuazai/mmGesture.},
  archive      = {J_TMC},
  author       = {Qihua Feng and Kunpeng Cheng and Chunhui Duan},
  doi          = {10.1109/TMC.2025.3614353},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmWave radar-based unsupervised gesture recognition via image-aligned heterogeneous domain transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-level-attention-based continuous trajectory design and computation offloading for multi-UAV cooperative target search. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3614596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are widely used for region monitoring and target search. However, existing UAV swarm search methods usually suffer from UAV motion models' low accuracy and inflexibility, resulting in low search efficiency, and there is a gap between the UAV motion model design and reality. Moreover, current methods ignore the impact of search task offloading locations and computational resource allocation on search efficiency. Finally, existing UAV swarm search methods lack scalability and generalization as the number of UAVs increases. This paper explores the cooperative search problem of UAV swarms in three-dimensional continuous space. We comprehensively consider UAV partial observability, search area delay constraint, collision avoidance condition, energy collection and consumption, etc. We jointly optimize the UAV flight trajectory, charging decision, search task offloading, and computing resource allocation to reduce the uncertainty of the search area and improve target detection rates. Specifically, we propose a heuristic embedded Multi-Agent Proximal Policy Optimization (MAPPO) algorithm. This method significantly reduces the action space dimension by using a heuristic energy-efficient task offloading and computational resource allocation strategy, and uses MAPPO with a two-level attention mechanism, a Beta distribution, and a safe flying control strategy to efficiently obtain the optimal flight trajectory and charging strategy. Two-level attention can handle a dynamically sized agent network, significantly improving algorithm performance and scalability. Furthermore, through the Curriculum Learning (CL) mechanism, we extend our algorithm to a large-scale UAV swarm cooperative search scenario. Simulation results show that compared with other benchmarks, our method can significantly reduce the search area's uncertainty and improve the target detection rate.},
  archive      = {J_TMC},
  author       = {Haowen Zhu and Junpeng Hui and Zehua Guo},
  doi          = {10.1109/TMC.2025.3614596},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two-level-attention-based continuous trajectory design and computation offloading for multi-UAV cooperative target search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling and fusion for multimodal federated learning in energy-constrained wireless networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3615129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of privacy-preserving applications, such as medical diagnostics and the Metaverse, highlights the importance of federated learning (FL) for distributed model training at the wireless edge. These applications often rely on multimodal data (e.g., text, images, audio), necessitating advances in multimodal federated learning (MMFL). However, MMFL faces challenges like energy efficiency, multimodal fusion, and heterogeneity. To address these, a scheduling and fusion-based MMFL framework (SFMMFL) is proposed that focuses on improving both the scheduling mechanism and aggregation strategy. To improve the training performance under energy constraint, a Lyapunov-based scheduling algorithm is proposed, in which long-term optimization is transformed into immediate optimization. After that, to tackle the issue of model separation caused by multimodal datasets, a multimodal model aggregation strategy based on Knowledge Distillation (KD) is introduced for multimodal fusion. Convergence analysis proves its feasibility, and simulation results demonstrate that it can achieve faster and more stable convergence performance while improving model training accuracy. Specifically, our proposed SFMMFL can lower the energy consumption of the system by about $20\%$ for computing and $16.67\%$ for transmission.},
  archive      = {J_TMC},
  author       = {Jianing Zheng and Jiadong Yu and Xiaolan Liu},
  doi          = {10.1109/TMC.2025.3615129},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Scheduling and fusion for multimodal federated learning in energy-constrained wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel OTFS-based massive random access scheme in cell-free massive MIMO systems for high-speed mobility. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3615587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the research of next-generation wireless communication technologies, orthogonal time frequency space (OTFS) modulation is emerging as a promising technique for highspeed mobile environments due to its superior efficiency and robustness in doubly-selective channels. Additionally, the cellfree architecture, which eliminates the issues associated with cell boundaries, offers broader coverage for radio access networks. Integrating cell-free architecture with OTFS modulation enables support for massive random access in high-mobility machinetype communication scenarios. This paper explores a massive random access scheme based on OTFS modulation within a cellfree architecture. A transceiver model for uplink OTFS signals involving multiple access points (APs) is developed, where channel estimation with fractional channel parameters is approximated as a two-dimensional block sparse matrix recovery problem. Building on existing superimposed and embedded preamble schemes, a hybrid preamble strategy intended for massive random access is proposed. This scheme leverages superimposed and embedded preambles to respectively achieve rough and accurate active user equipment (UE) detection (AUD), as well as precise channel estimation. Moreover, this study introduces a generalized approximate message passing and pattern-coupled sparse Bayesian learning with Laplacian prior (GAMP-PCSBL-La) algorithm, which effectively captures block sparse features after discrete cosine transform (DCT), delivering precise estimation results with reduced computational complexity. Simulation results demonstrate that the proposed scheme is effective and provides superior performance compared to other existing schemes.},
  archive      = {J_TMC},
  author       = {Yanfeng Hu and Dongming Wang and Xinjiang Xia and Jiamin Li and Pengcheng Zhu and Xiaohu You},
  doi          = {10.1109/TMC.2025.3615587},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel OTFS-based massive random access scheme in cell-free massive MIMO systems for high-speed mobility},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-markov options enabled DDPG method for autonomous vehicle overtaking with LiDAR and RADAR fusion. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3615629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Vehicle (AV) navigation in dynamic environments is highly complex, with overtaking maneuvers adding complexity due to multiple sub-maneuvers and limited environmental perception. Recent advancements in Deep Reinforcement Learning (DRL) and sensors address a few challenges. Yet, the high dimensional sensor data and complex maneuvers slow RL agents in effectively perceiving the environment and performing the overtaking. Furthermore, the dynamic conditions and individual sensor data limit the RL agent's ability to localize and execute the precise sub-maneuvers. Thus, in this work, we propose TDRLO, a Deep Deterministic Policy Gradient (DDPG) based semi-Markov options acquired hierarchical reinforcement learning framework for AVs overtaking with LiDAR-RADAR fusion to tackle this issue. Our approach uses the option policy to control the sub-maneuvers execution and preprocesses sensor data for extracting crucial overtaking checkpoints and low dimensional environmental perception. Temporal Difference (TD) updates in the DDPG algorithm enhance episodic learning and incremental updates, while the option policy reduces the overtaking complexity. Moreover, the preprocessing of raw data and sensor fusion provides a better environmental perception and efficient overtaking in the CARLA simulator. We evaluated our approach using the National Highway Traffic Safety Administration (NHTSA) inspired overtaking pre-crash scenarios in CARLA. The result shows an average 20-30% improvement in average peak reward with stable critic values, alongside 100% completion rate, 10-30% least collision rate, and 20-40% more average speed compared to the baseline sate-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Shikhar Singh Lodhi and Neetesh Kumar and Pradumn Kumar Pandey},
  doi          = {10.1109/TMC.2025.3615629},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semi-markov options enabled DDPG method for autonomous vehicle overtaking with LiDAR and RADAR fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online task planning with collision avoidance for heterogeneous mobile robots. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3615707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, heterogeneous mobile robots (HMRs) have been widely implemented in warehouses or factories with limited space to improve efficiency. HMRs have different capabilities and sizes, which leads to two bottlenecks in the task planning for HMRs in an online environment. One is that it is hard to efficiently find high-quality task assignment results between heterogeneous tasks and HMRs because heterogeneous tasks need to be completed by HMRs of different capabilities. The other is that finding collision-free paths for HMRs of different sizes to complete assigned tasks is hard to satisfy the real-time demand. Existing works mainly consider solving these two bottlenecks separately while ignoring the mutual influence between the results of task assignment and collision-free path finding. To solve these issues, in this paper, we formally define the Online Task Planning with Collision Avoidance for HMRs (OTCH) problem, in which the platform aims to find the best task assignment and collision-free paths for HMRs to complete as many heterogeneous tasks as possible while minimizing the total cost containing the path cost and the delay cost. To solve the OTCH problem, we propose a novel task planning framework that considers task assignment and collision-free path finding simultaneously. Within this framework, we first present an aggregation algorithm to accelerate calculations. Then, we propose a network-flow-based task assignment algorithm to find a high-quality task assignment between heterogeneous tasks and HMRs in each batch. Finally, we propose a novel and efficient path finding algorithm to find collision-free paths for HMRs to complete assigned tasks while satisfying real-time demand. In addition, we not only analyze the complexity of the OTCH problem in detail but also give theoretical analysis for our proposed algorithms. Extensive experiments in a real-world dataset and two simulated datasets show that our proposed algorithms outperform the state-of-the-art while having the best scalability},
  archive      = {J_TMC},
  author       = {Yifei Li and Hejiao Huang},
  doi          = {10.1109/TMC.2025.3615707},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online task planning with collision avoidance for heterogeneous mobile robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficiency analysis and optimization for cell-free mMIMO networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3615728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive multiple-input multiple-output (CF mMIMO) networks, in which multiple antennas simultaneously serve multiple user equipments (UEs), offer significant spectral efficiency (SE) gains. However, their energy efficiency (EE) performance still requires further investigation. Traditional approaches to radio resource allocation in CF mMIMO systems focus on solving the two-dimensional UE-antenna precoding problem. In this work, we propose a novel resource allocation framework that addresses the four-dimensional UE-antennafrequency-time precoding allocation for EE maximization. Considering the frequency-domain fast fading variations and timedomain traffic dynamics, we develop algorithms for EE maximization. Our heuristic delayed scheduling algorithm enhances EE by up to 10% compared to the algorithm designed for sum SE maximization. Furthermore, we demonstrate that EE performance is highly sensitive to system load, achieving a 7.4% higher EE at 69% system load compared to full load under the simulation settings. Finally, we analyze the impact of UE load on IP packet delay, establishing a relationship between the maximum UE load and packet delay budget.},
  archive      = {J_TMC},
  author       = {Adam Girycki and Md Arifur Rahman and Sofie Pollin},
  doi          = {10.1109/TMC.2025.3615728},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy efficiency analysis and optimization for cell-free mMIMO networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic optimization of edge aggregation structures and update frequencies for efficient distributed hierarchical model training. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3615667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing enables distributed machine learning models to be deployed and trained near the user space. However, the intricate nature of edge computing raises several challenges to distributed machine learning frameworks: 1) inferior convergence arising from non-independent and identically distributed (non-IID) edge data; 2) inefficient structural adaptation, where device dynamism complicates the adjustment of aggregation structure; and 3) reduced training efficiency, as resource heterogeneity and fluctuations create systemic stragglers. To address these issues, a distributed hierarchical model training framework has been proposed by considering the dynamic aggregation structure and frequency in this paper. This framework designs an Edge Aggregation Structure and Frequency method, namely EASF, for distributed model training in heterogeneous edge computing environments. First, a dynamic distributed aggregation structure method is formulated to consider various data distribution patterns. This method constructs and modifies the aggregation structure in a distributed manner to adapt to variations in working edge devices. Second, a self-adapted aggregation frequency method and a timeout abandonment mechanism are proposed to allow each node to update its aggregation frequency adaptively. Lastly, a theoretical analysis demonstrates the convergence property of the EASF method in dynamic environments. Extensive experiments have been conducted on a set of open testbeds. Results show that the EASF significantly improves the efficiency and accuracy of hierarchical model training in heterogeneous edge computing.},
  archive      = {J_TMC},
  author       = {Xiaolong Xu and Jiayang Sun and Guangming Cui and Lianyong Qi and Muhammad Bilal and Wanchun Dou and Zhipeng Cai and Jon Crowcroft},
  doi          = {10.1109/TMC.2025.3615667},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic optimization of edge aggregation structures and update frequencies for efficient distributed hierarchical model training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph reinforcement learning for dependency-aware multi-task allocation in spatial crowdsourcing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3616027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial Crowdsourcing (SC) has emerged as a significant paradigm for executing complex real-world projects and tasks, which are often decomposed into interdependent subtasks requiring diverse worker skills. A key challenge lies in the efficient allocation of multiple tasks to workers while respecting intricate subtask dependencies and skill requirements within a limited timeframe. This paper formally defines and addresses this Dependency-aware Multi-task Allocation (DMA) problem. We propose a novel framework, termed Heterogeneous Graph Reinforcement Learning for Task Allocation (HGRL-TA), to derive optimal allocation policies. Central to our framework is a multi-relation graph model that uniformly represents the complex problem state, and a Compound-path-based Heterogeneous Graph Attention Network (CHANet) designed to generate comprehensive state embeddings by capturing the intricate relationships among tasks and workers. The allocation decisions are made sequentially by a policy network, which is trained jointly with CHANet using the Proximal Policy Optimization (PPO) algorithm. Extensive experiments on both real-world and synthetic datasets demonstrate that the proposed HGRL-TA framework significantly outperforms seven representative baselines, showcasing its effectiveness and generalizability for the DMA problem.},
  archive      = {J_TMC},
  author       = {Yong Zhao and Zhengqiu Zhu and Chen Gao and En Wang and Jincai Huang and Fei-Yue Wang},
  doi          = {10.1109/TMC.2025.3616027},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Heterogeneous graph reinforcement learning for dependency-aware multi-task allocation in spatial crowdsourcing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoBit: A cooperative bit-based layer-4 load balancer for mobile edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3616200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offloading computation tasks to Mobile Edge Computing (MEC) is a common practice for service providers, which requires Load Balancers (LB) to route packets to different devices. Current load balancers face significant scalability challenges due to increasing traffic loads in MEC environments, as well as limited and costly resources. In this paper, we propose CoBit(Cooperative Bit-based), a scalable layer-4 load balancer that reuses existing mid or low-end switches to achieve higher performance cooperatively. CoBit leverages a bit-based cooperative scheme to achieve routing with checking different partial bits of five-tuples in packet headers. CoBit can dramatically reduce the load on any single device and improve robustness against rapid changes in rules. For balanced load distribution with CoBit, we formulate the bit allocation across switches. We then propose dynamic programming-based algorithms for two MEC network scenarios: a linear topology and a multi-edge topology. We also address the challenge of limited SRAM in switches by developing a heuristic algorithm that optimally partitions bits of rules while ensuring that storage constraints are met. Finally, we conduct extensive evaluations with real-world MEC topologies and traffic datasets, demonstrating that CoBit performs much better than the previous rule-based LB scheme.},
  archive      = {J_TMC},
  author       = {Shu Yang and Xinze Wu and Yaodong Huang and Laizhong Cui},
  doi          = {10.1109/TMC.2025.3616200},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CoBit: A cooperative bit-based layer-4 load balancer for mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards trusted 6G mobile edge computing: A secure batch large language models deployment framework. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3616137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) sparked massive applications in 6G. However, the emerging 6G Mobile Edge Computing (MEC) based on LLM caching leaves model protection unconsidered. To protect the LLM assets under the extended Dolev-Yao (DY) threat model, a secure batch LLMs deployment framework is proposed for 6G MEC, which securely delivers the sanitized crafted (san-crafted) LLM and the crafted-random-values (CR-values) from 6G edge to the Rich Execution Environment (REE) and Trusted Execution Environment (TEE) of mobile devices, respectively. Firstly, the 6G MEC cached LLM is san-crafted by an efficiency-improved sanitizable signature to protect the integrity of the LLM during the entire deployment process. Then, a lightweight batch authentication protocol is proposed to improve the efficiency of verifying ultra-massive model requests. Finally, to be compatible with the state-of-the-art secure inference (i.e., Magnitude), the san-crafted LLM delivered into mobile device's REE is verified by sanitizable signatures, and then conducts secure inference with the corresponding CR-values provisioned into the TEE. Rigorous security proofs confirm that our framework meets the security requirements of LLM deployment. Compared to the benchmark, the proposed framework significantly improves both computational and communication efficiency, reducing computational overhead by 89.92% and communication overhead by 47.08%. This framework facilitates the TEE-based LLMs secure inference for ultra-massive mobile devices in 6G MEC.},
  archive      = {J_TMC},
  author       = {Yu Sun and Jianhua Liu and Gaojian Xiong and Qinglin Song and Jianwei Liu and Gang Wang and Rui Wang},
  doi          = {10.1109/TMC.2025.3616137},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards trusted 6G mobile edge computing: A secure batch large language models deployment framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two time-scale DRL for service caching and task offloading in cross-domain marine networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3594602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing computational demands and limited network resources in marine environments, efficient service caching and task offloading have become critical. In such environments, Autonomous Underwater Vehicles (AUVs) rely on Unmanned Surface Vehicles (USVs) as relays, forming a cross-domain network comprising underwater acoustic and above-water RF links. However, the heterogeneity in bandwidth, latency, and bit error rates introduces challenges for reachability analysis and delay estimation. This paper addresses the joint optimization of caching, task offloading, and resource allocation in a cross-domain marine network composed of offshore base stations, USVs, and AUVs. To tackle the inherent heterogeneity in network links and decision timescales, we formulate the problem as a two-time-scale Hierarchical Markov Decision Process (H-MDP) and propose a Two Time-Scale Deep Reinforcement Learning (T2S-DRL) approach that integrates a hybrid policy network and a lightweight structure-aware action masking mechanism. The large time-scale agent optimizes caching decisions, while the short time-scale agent focuses on offloading and resource allocation. Extensive simulations show that our approach significantly reduces task execution delay and energy consumption, validating its effectiveness.},
  archive      = {J_TMC},
  author       = {Zhaoxiang Huang and Zhiwen Yu and Liang Wang and Yingnan Zhao and Huan Zhou and and Bin Guo},
  doi          = {10.1109/TMC.2025.3594602},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two time-scale DRL for service caching and task offloading in cross-domain marine networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeurIT: Pushing the limit of neural inertial tracking for indoor robotic IoT. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3594717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost inertial measurement units and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor have they maximized the potential of deep learning to achieve the desired accuracy. To address these limitations, we introduce NeurIT, which elevates tracking accuracy to a new level. NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining both RNN and Transformer to learn representative features in both time and frequency domains. To fully utilize IMU information, we strategically employ body-frame differentiation of magnetometers, considerably reducing the tracking error. We implement NeurIT on a customized robotic platform and conduct evaluation in various indoor environments. Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance. Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data. Moreover, NeurIT demonstrates robustness in large urban complexes and performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions while surpassing it in feature-sparse settings. We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things. NeurIT is open-sourced here: https://github.com/aiot-lab/NeurIT.},
  archive      = {J_TMC},
  author       = {Xinzhe Zheng and Sijie Ji and Yipeng Pan and Kaiwen Zhang and Chenshu Wu},
  doi          = {10.1109/TMC.2025.3594717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NeurIT: Pushing the limit of neural inertial tracking for indoor robotic IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cer-FeaUn: Certified feature unlearning in vertical federated learning. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3594851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature unlearning, forgetting sensitive features while maintaining the accuracy of models, is a pressing issue against Feature Inference Attacks (FIA) in Vertical Federated Learning (VFL). This issue is addressed by retraining the model from scratch on a dataset without the sensitive features from scratch or Federated Unlearning (FU) for all samples. However, they either introduce high overheads due to retraining or reduce the accuracy of the unlearned model. In this paper, we proposed Cer-FeaUn, a certified feature unlearning, trading off between the overheads and accuracy. Specifically, a novel feature perturbation strategy is first proposed to construct a perturbed dataset, where the sensitive features are perturbed with noises. Then, the effect is defined as the parameters difference between models trained with the original and perturbed dataset. Finally, an unlearned model is trained in first-order, where the effect is removed from the original model in one epoch. Furthermore, Cer-FeaUn performs certified removal for server-side models with strongly convex loss functions. That is, the distribution of the unlearned model is statistically indistinguishable from that of the retrained model. For the scenario with a few sensitive features, simulation results show that the accuracy of the unlearned model is up to 84.79%, and the runtime of Cer-FeaUn is 15 times faster than that of the retrained model.},
  archive      = {J_TMC},
  author       = {Yilei Wang and Zhaobo Lu and Zhiquan Liu and Tao Li and Zhenhua Chen and Willy Susilo},
  doi          = {10.1109/TMC.2025.3594851},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cer-FeaUn: Certified feature unlearning in vertical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable heading tracking for pedestrian road crossing prediction using commodity devices. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3594740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian heading tracking enables applications in pedestrian navigation, traffic safety, and accessibility. Previous works, using inertial sensor fusion or machine learning, are limited in that they assume the phone is fixed in specific orientations, hindering their generalizability. We propose a new heading tracking algorithm, the Orientation-Heading Alignment (OHA), which leverages a key insight: people tend to carry smartphones in certain ways due to habits, such as swinging them while walking. For each smartphone attitude during this motion, OHA maps the smartphone orientation to the pedestrian heading and learns such mappings efficiently from coarse headings and smartphone orientations. To anchor our algorithm in a practical scenario, we apply OHA to a challenging task: predicting when pedestrians are about to cross the road to improve road user safety. In particular, using 755 hours of walking data collected since 2020 from 60 individuals, we develop a lightweight model that operates in real-time on commodity devices to predict road crossings. Our evaluation shows that OHA achieves 3.4 times smaller heading errors across nine scenarios than existing methods. Furthermore, OHA enables the early and accurate detection of pedestrian crossing behavior, issuing crossing alerts 0.35 seconds, on average, before pedestrians enter the road range.},
  archive      = {J_TMC},
  author       = {Yucheng Yang and Jingjie Li and Kassem Fawaz},
  doi          = {10.1109/TMC.2025.3594740},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliable heading tracking for pedestrian road crossing prediction using commodity devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALCS: An adaptive latency compensation scheduler for multipath TCP in satellite-terrestrial integrated networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end transmission by simultaneously utilizing terrestrial and satellite networks, offering significant benefits in scenarios like emergency response and cross-continental communication. Low Earth Orbit (LEO) satellite networks offer reduced Round Trip Time (RTT) for long-distance data transmission and serve as a crucial backup during terrestrial network failures. Meanwhile, terrestrial networks are characterized by ample bandwidth resources and generally more stable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN is vital for optimizing resource utilization and ensuring efficient data transfer by exploiting the complementary strengths of both networks. However, the inherent challenges of STIN, such as heterogeneity, instability, and handovers, pose difficulties for traditional multipath schedulers, which are typically designed for terrestrial networks. We propose a novel multipath data scheduling approach for STIN, the Adaptive Latency Compensation Scheduler (ALCS), to address these issues. ALCS refines transmission latency estimates by incorporating RTT, congestion window size, inflight and queuing packets, and satellite trajectory information. It further employs adaptive mechanisms for latency compensation and proactive handover management. Implemented in the MPTCP Linux Kernel and evaluated in a simulated STIN testbed, ALCS outperforms existing multipath schedulers, delivering faster data transmission and achieving throughput gains of 9.8% to 44.0% compared to benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Lin Wang and Ze Wang and Zeyi Deng and Jingjing Zhang and Yue Gao},
  doi          = {10.1109/TMC.2025.3594896},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ALCS: An adaptive latency compensation scheduler for multipath TCP in satellite-terrestrial integrated networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced dual-currency VCG auction mechanism for resource allocation in IoV: A value of information perspective. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6 G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the “value of information” as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6 G networks.},
  archive      = {J_TMC},
  author       = {Wei Wang and Nan Cheng and Conghao Zhou and Haixia Peng and Haibo Zhou and Zhou Su and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3594358},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An enhanced dual-currency VCG auction mechanism for resource allocation in IoV: A value of information perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VitalEar: An earable heartbeat and respiratory rate monitoring system under aerobic exercises. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart rate (HR) and respiratory rate (RR) are essential physiological indicators of people's physical function and exercise performance. Advancement in sensor technology has rendered earable devices with in-ear microphones feasible for vital sign monitoring. However, it is rather challenging to monitor heart rate and respiration simultaneously with a single earable device especially when a person is doing exercises. This is because intense physical activities can lead to significant noise interference which can easily obscure physiological signals. To address this challenge, this paper presents VitalEar, an exercise physiological monitoring system based on in-ear microphones, designed to estimate HR and RR while addressing complex motion interference and variability in users and activities. VitalEar employs Empirical Wavelet Transform (EWT) to decompose heartbeats into periodic and harmonic coefficients, enhancing noise reduction in the ECG spectrogram reconstruction model. Additionally, VitalEar incorporates a DCN-LSTM-based breathing curve reconstruction model to mitigate background noise and variability in user and activity. The experiments show that VitalEar achieves an average MAE of 5.61 BPM and 2.31 RPM, MAPE of 4.16% and 10.58% for HR and RR estimation, respectively. Compared to related work, our approach offers significant advantages in robustness against intense physical activities},
  archive      = {J_TMC},
  author       = {Yuzheng Zhu and Zhangxin Liang and Jie Zheng and Yongpan Zou and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3595178},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VitalEar: An earable heartbeat and respiratory rate monitoring system under aerobic exercises},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential privacy space decomposition algorithm based on hierarchical model. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3595426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing an appropriate division method is crucial for partitioning two-dimensional spatial data under the constraints of differential privacy. The current mainstream partitioning methods include grid-based partitioning and hierarchical partitioning. In order to optimize query accuracy while satisfying differential privacy conditions, it remains challenge to achieve the sum minimization of noise error and uniformity assumption error. To address this issue, we propose the HOLG (Hierarchical Optimization of Logical Grids) algorithm, employing a ”divide-merge-divide” approach. It begins with fine-grained grid partitioning of the data domain, followed by heuristic merging of grids with similar data distributions. After determining the scale of the query domain, the merged regions are further subdivided into smaller regions with similar query probabilities, constructing a hierarchical structure to reduce uniformity assumption errors. Additionally, we design a novel noise injection method and introduce consistency constraints to further minimize noise errors. To reduce the time complexity of the HOLG partitioning method, Huffman trees is employed to optimize the processing of the hierarchical tree set generated by HOLG, ensuring query utility while effectively reducing the query response time for the partitioning algorithm. Experimental results on large-scale spatial datasets demonstrate that HOLG outperforms similar algorithms in query accuracy. Furthermore, when combined with the Huffman tree optimization, it effectively reduces query response time.},
  archive      = {J_TMC},
  author       = {Haiping Huang and Chaorun Sun and Zhenqi Shi and Wei Zhang and Jiyun Cang and Fu Xiao},
  doi          = {10.1109/TMC.2025.3595426},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Differential privacy space decomposition algorithm based on hierarchical model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDT-SaFL: Semi-asynchronous federated learning for edge digital twin in industrial internet-of-things. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through conducting equivalent model training within the paradigm of edge intelligence, the Digital Twin Edge Networks (DITEN) have been widely employed in the Industrial Internet-of-Things (IIoT) to facilitate the cost-effective execution without the operational disruption. However, due to the insufficient consideration of heterogeneity in computing and communication capabilities of distinct industrial terminals in the Digital Twin (DT) model training, the existing approaches of DT construction/update have unbalanced model training cost and loss in the whole life cycle of DT model, hindering the abilities of quick responding to complex and dynamic productions and ensuring the data consistency of virtual-real space. To address this issue, we define a global loss minimization problem with constraint, and propose an original approach of semi-asynchronous federated learning, named EDT-SaFL, as a promising solution. Considering the collaborative utilization of heterogeneous resources, and the contribution of local data quantity and quality to the global model update, the EDT-SaFL consists of three important operations, Terminal Selection for Model Training, Self-Adaptation of Local Training Iterations, and Semi-asynchronous Global Aggregation. With the analysis of convergence, complexity and communication overhead, the experiments have evidently demonstrated the superiority of EDT-SaFL on the datasets of CIFAR-10 and Industrial-Equipment.},
  archive      = {J_TMC},
  author       = {Ming Tao and Lingling Liao and Yin Zhang and Lei Liu and Geyong Min and Dusit Niyato and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3595117},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EDT-SaFL: Semi-asynchronous federated learning for edge digital twin in industrial internet-of-things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control plane initial synchronization optimization for ultra-dense cell-free massive MIMO. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3595656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-dense cell-free Massive MIMO (CF-MM) is a promising technology for future 6 G networks. Unlike previous research, we focus on the challenges of CF-MM in the control plane. Specifically, we investigate the initial synchronization processing for CF-MM, which is fundamental for user equipment (UE) to access the wireless network. Regardless of whether LTE- or New Radio (NR)-based synchronization signals are used, the existing initial access procedures are incompatible with CF-MM, leading to severe interference on the broadcast channel and thus a high initial access failure rate. To address this issue, we design a new synchronization scheme for CF-MM based on the latest NR standard. With this new scheme, the initial synchronization interference problem is then transformed into a transmission and reception points (TRP) ID partition problem. To solve problem, we then propose an approach that combines the latest multiple operator heuristic (MOH) method with a parameterized local search(PLS) method. Simulation results show that the proposed method can achieve the optimal solution for scenarios with a low number of TRPs. Furthermore, the gap between the proposed method and the optimal solution is less than 2% for high number TRPs cases, demonstrating its effectiveness.},
  archive      = {J_TMC},
  author       = {Miaona Huang and Jun Chen},
  doi          = {10.1109/TMC.2025.3595656},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Control plane initial synchronization optimization for ultra-dense cell-free massive MIMO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ground-assisted LEO satellite federated learning: Dynamic, efficient, distributed learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3593252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of Low Earth Orbit (LEO) satellites, they generate a vast amount of data. This data has been instrumental in supporting machine learning (ML) in various terrestrial services to address global challenges such as monitoring climate change and natural disasters. However, many national regulations restrict the direct transmission of satellite data to ground stations (GSs). Therefore, ground-assisted satellite federated learning (FL) has emerged as a paradigm to safeguard data privacy by transferring model parameters instead of raw data for collaborative training. At present, the existing groundassisted satellite FL methods encounter practical challenges: 1) The dynamic environment of LEO satellites results in continuous changes in the types of data collected by satellites, making it difficult for traditional FL models to adapt to these changes. This can lead to a deterioration in model accuracy over extended periods of model training. 2) Communication between satellites and GS is affected by atmospheric interference and weather factors, resulting in increased transmission delays and affecting the realtime efficiency of the FL system. In response to these challenges, we propose a dynamic, efficient, and distributed ground-assisted LEO satellite federated learning (DEDFL) framework to improve model accuracy and reduce satellite communication delays. In DEDFL, we design a Balanced Class Memory Extraction and an information playback strategy that enables the onboard FL model to adapt to changing satellite data types, thus achieving a performance balance across different classes. Additionally, we propose an adaptive fine coding method for parameter adoption prior to satellite transmission, effectively reducing the delay caused by satellites and ground-specific environmental variations. Experimental results demonstrate that the DEDFL method offers better accuracy and communication efficiency than other baseline algorithms.},
  archive      = {J_TMC},
  author       = {Fuyao Zhang and Dan Wang and Jiawen Kang and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3593252},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ground-assisted LEO satellite federated learning: Dynamic, efficient, distributed learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAP-UAV-assisted maritime IoT communication network. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3596169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wireless networks has spurred an increasing demand for high-quality maritime communication services. This study presents an innovative unicast-multicast access and backhaul maritime communication network (UMABMCN), in which a high-altitude platform (HAP) provides HAP-to-vessel (H2V) unicast services to vessels and backhaul support to unmanned aerial vehicles (UAVs) through HAP-to-UAV (H2U) links. Additionally, multiple UAVs are deployed to deliver UAV-to-vessel (U2V) multicast transmission services to vessels. Specifically, we formulate a HAP-UAV-assisted unicast-multicast cooperation multi-objective optimization problem (UMCMOP) aimed at maximizing the sum achievable rate of base stations (BS)-to-vessel (B2V), maximizing the sum backhaul rate of H2U, and minimizing the energy consumption of UAVs via jointly optimizing communication connection between BSs and vessels, power allocations of UAVs, along with the placement of UAVs. The formulated UMCMOP is a mixed integer non-linear programming (MINLP) problem. To address this, we propose an enhanced multi-objective multi-verse optimization (EMOMVO-CGD) algorithm, which integrates a chaos probability operator, gray wolf exploitation operator, and discrete update operator. To further validate the performance of EMOMVO-CGD, a joint communication connection, power allocation and placement optimization (JCCPAPO) method is proposed. Simulation results demonstrate that the two proposed algorithms outperform benchmark strategies in optimizing the aforementioned objectives.},
  archive      = {J_TMC},
  author       = {Lingling Liu and Chong Shen and Feng Shu and Feng Wang and Shujing Li and Tony Q.S. Quek},
  doi          = {10.1109/TMC.2025.3596169},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HAP-UAV-assisted maritime IoT communication network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability evaluation for WSNs based on deep reinforcement learning and graph neural networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Network (WSN) reliability evaluation is essential for ensuring the stable operation of network. Traditional methods usually focus on the network topology structure, and calculate the normal operation probability of WSNs. However, these methods usually ignore the energy consumption and network lifetime. In this paper, a novel reliability evaluation algorithm TLR is proposed, which calculates the network lifetime under dynamic network environment according to the pre-set network topology structure reliability threshold, and realizes the comprehensive reliability analysis of network topology and lifetime. In addition, as the basis for reliability evaluation, this paper proposes a new deep reinforcement learning network framework GNN-AC combining graph neural network and actor-critic network, which solves the challenge of constructing Virtual Backbone Network (VBN) in dynamically operating networks. Based on the self-defined fitness matrix and fitness value, the objective function is set to optimize the VBN construction scheme to accurately calculate the network lifetime, and the relationship between the reliability of network topology and network lifetime is discussed. Simulations are carried out for various sizes of WSNs to show the advantages and effectiveness of the proposed approach in estimating network lifetime and reliability evaluation.},
  archive      = {J_TMC},
  author       = {Ziheng Xiao and Shenghao Liu and Hongwei Lu and Lingzhi Yi and Hanjun Gao and Xianjun Deng and Heng Wang and Jong Hyuk Park},
  doi          = {10.1109/TMC.2025.3595199},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliability evaluation for WSNs based on deep reinforcement learning and graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollaboRadio: A hybrid device-edge-cloud collaboration paradigm for fine-grained radio map construction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3596312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio map presents communication parameters of interest, e.g., received signal strength, across a geographical region in a specific frequency band. It can be leveraged to improve the efficiency of spectrum utilization. With the rapid development of radio technology and the proliferation of radioenabled devices, there is an increasing demand for finer granularity (higher resolution) in radio maps. However, the problem of fine-grained radio map construction is uniquely challenging, as it requires to utilize an extremely small number of radio samples collected by sparsely distributed sensor devices to infer the map. To address the challenge, we propose a hybrid device-edge-cloud collaboration paradigm called CollaboRadio. CollaboRadio first groups sensor devices into multiple clusters, with cluster locations optimized based on principles of radio propagation. Next, it leverages a small AI model on each edge server to generate a local radio map for the respective cluster region from radio samples collected by intra-cluster sensors. Finally, it employs a large AI model in the cloud to construct a global radio map for the entire region from the local maps produced by edge servers of different clusters. For the implementation of CollaboRadio, we develop an UNet-based small model for the edge server and a Transformerbased large model for the cloud. Extensive simulations show that CollaboRadio is capable of constructing fine-grained radio maps from an ultra-low sampling rate of 0.1%, and significantly outperforms state-of-the-art.},
  archive      = {J_TMC},
  author       = {Shuai Shao and Lu Cheng and Ke Chen and Qingyu Liu and Shuhang Zhang and Hongliang Zhang and Lingyang Song},
  doi          = {10.1109/TMC.2025.3596312},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CollaboRadio: A hybrid device-edge-cloud collaboration paradigm for fine-grained radio map construction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service satisfaction-aware adaptive service migration and resource allocation in vehicular edge computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3596342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of vehicle-to-everything (V2X) technology, service migration has become an important approach to provide low-latency computing services and ensure service continuity for high-speed moving vehicles in vehicular edge computing (VEC), which enables VEC to efficiently support advanced transportation services. However, optimizing service satisfaction for service migration in multi-vehicle heterogeneous VEC networks is challenging, since the complex, multifactorial, and nonlinear dependencies between service satisfaction and quality of service (QoS) metrics is intractable, and the rapidly changing computational loads in edge server results in inefficient utilization of edge resources. In this paper, we propose a service Satisfaction-based Adaptive service Migration and resource Allocation joint Optimization scheme (SAMAO) to improve service migration efficiency and edge resource utilization in VEC. Firstly, we develop an adaptive computation resource allocation algorithm that can adjust resource allocation strategy according to load status of edge servers to improve vehicle service satisfaction. Then, to minimize energy consumption and ensure service satisfaction for vehicles, we propose a utility maximization algorithm to formulate migration decisions based on pre-allocated computation resources on servers. Finally, numerous simulations based on Shanghai Telecom real-world dataset show that SAMAO can achieve significant advantages in terms of average service satisfaction and computation cost.},
  archive      = {J_TMC},
  author       = {Yufei Liu and Yuanguo Bi and Yuheng Liu and Dusit Niyato and Kaiqi Yang and Liang Zhao and Ammar Hawbani},
  doi          = {10.1109/TMC.2025.3596342},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service satisfaction-aware adaptive service migration and resource allocation in vehicular edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient and real-time sensing for federated continual learning via sample-driven control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3597713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent Real-Time Sensing (RTS) system must continuously acquire, update, integrate, and apply knowledge to adapt to real-world dynamics. Managing distributed intelligence in this context requires Federated Continual Learning (FCL). However, effectively capturing the diverse characteristics of RTS data in FCL systems poses significant challenges, including severely impacting computational and communication resources, escalating energy costs, and ultimately degrading overall system performance. To overcome these challenges, we investigate how the data distribution shift from ideal to practical RTS scenarios affects Artificial Intelligence (AI) model performance by leveraging the generalization gap concept. In this way, we can analyze how sampling time in RTS correlates with the decline in AI performance, computation cost, and communication efficiency. Based on this observation, we develop a novel Sample-driven Control for Federated Continual Learning (SCFL) technique, specifically designed for mobile edge networks with RTS capabilities. In particular, SCFL is an optimization problem that harnesses the sampling process to concurrently minimize the generalization gap and improve overall accuracy while upholding the energy efficiency of the FCL framework. To solve the highly complex and time-varying optimization problem, we introduce a new soft actor-critic algorithm with explicit and implicit constraints (A2CEI). Our empirical experiments reveal that we can achieve higher efficiency compared to other DRL baselines. Notably, SCFL can significantly reduce energy consumption up to 85% while maintaining FL convergence and timely data transmission.},
  archive      = {J_TMC},
  author       = {Minh Ngoc Luu and Minh-Duong Nguyen and Ebrahim Bedeer and Van Duc Nguyen and Dinh Thai Hoang and Diep N. Nguyen and Quoc-Viet Pham},
  doi          = {10.1109/TMC.2025.3597713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient and real-time sensing for federated continual learning via sample-driven control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPACE: Speaker adaptation for acoustic eavesdropping using mmWave radio signals. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3598703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of voice-related interaction and communication has raised concerns about privacy leakage and security. For example, millimeter-wave (mmWave) radio signals have been exploited as a potential attacker for acoustic eavesdropping. However, speaker variability and low-quality input pose significant challenges for the practical deployment of mmWave-based eavesdropping. In this paper, we propose SPACE, an acoustic eavesdropping system to recover intelligible speech from low-quality mmWave signals, which can adapt to numerous different speakers and unseen ones. SPACE is a two-stage system that first reconstructs the spectrogram using a novel Radio TransUNet and then synthesizes the waveform through a neural vocoder. Specifically, to alleviate the negative effect of speaker variability, we introduce a speaker encoder to capture speaker features and a fusion network to condition the spectrogram reconstruction based on the extracted speaker characteristics. Further, to facilitate intelligible speech recovery from low-quality input, we design a Frequency Transformation Layer to exploit the correlation among all frequency harmonics and incorporate the neural vocoder to synthesize the speech waveform from the reconstructed spectrogram without using the contaminated phase. The experimental results show that SPACE outperforms existing mmWave-based approaches in scenarios with numerous different speakers and unseen speakers.},
  archive      = {J_TMC},
  author       = {Running Zhao and Luca Jiang-Tao Yu and Tingle Li and Zhihan Jiang and Chenwei Zhang and Chenshu Wu and Hang Zhao and Edith C.H. Ngai},
  doi          = {10.1109/TMC.2025.3598703},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SPACE: Speaker adaptation for acoustic eavesdropping using mmWave radio signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-resolution massive MIMO channel estimation with LSTM attention-based CBDNet. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel estimation of a massive multi-input multi-output (MIMO) system that utilizes a one-bit analog-to-digital converter (ADC) is a foremost challenge. Traditional deep learning (DL) approaches have been recently employed to circumvent this problem; however, they are limited to noise levels. Unlike the existing works, we use a DL-based denoise architecture for channel estimation from one-bit received signals, improving the estimation performance as the signal-to-noise ratio (SNR) increases. The model leverages a dual-branch architecture to estimate and remove noise from input data. We propose a DL model: a long short-term memory (LSTM) attention-based convolutional blind denoising network (LA-CBDNet) comprising the noise estimation subnetwork and the non-blind denoising subnetwork. The noise estimation subnetwork comprises convolutional layers estimating the noise map, followed by an LSTM to converge the noise estimation. The non-blind subnetwork comprises accompanying attention and LSTM layers estimating the noise matrix. The numerical results demonstrate that our model performs better than benchmark approaches for varying SNRs and base station (BS) antennas. In addition, it outperforms the comparative methods for different pilot lengths and number of users.},
  archive      = {J_TMC},
  author       = {Islam Helmy and Wooyeol Choi},
  doi          = {10.1109/TMC.2025.3599399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Low-resolution massive MIMO channel estimation with LSTM attention-based CBDNet},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-tier submodel partition framework for enhancing UAV swarm robustness in forest fire detection. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of Unmanned Aerial Vehicle (UAV) swarm for Forest Fire Detection (FFD) missions presents unique challenges, e.g., the early forest fires are difficult to identify due to environment diversity and feature complexity, especially when some UAVs could be destroyed in harsh environments. To address these challenges, UAV swarm-based FFD missions can leverage advanced deep learning techniques, where online model updates, robustness, and communication overhead control become crucial for ensuring the effectiveness and adaptability of these missions. In this paper, we propose a Two-tier Submodel Partition Framework (TSPF) to enhance the robustness of UAV swarm conducting FFD missions. TSPF utilizes online model updates to adapt to diverse mission environments, thus strengthening the generalization capability of the model. In addition, a graph coloring method, an intragroup backup mechanism, and a Dynamic Server Selection (DSS) mechanism for the grouping are employed to enhance the robustness of FFD missions when some UAVs are destroyed, hence maintaining the high performance of FFD missions in harsh environments. Moreover, TSPF enables submodel updates by aggregating the parameters of selected layers within/between UAV groups, thereby effectively reducing the model parameter uploads (communication overhead) in model training. Experimental evaluations demonstrate that our proposed TSPF significantly improves the detection accuracy of forest fires, enhances the robustness of FFD missions against the destruction of some UAVs, and reduces the communication overhead in FFD missions.},
  archive      = {J_TMC},
  author       = {Xingyu Li and Wenzhe Zhang and Linfeng Liu and Ping Wang},
  doi          = {10.1109/TMC.2025.3599384},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two-tier submodel partition framework for enhancing UAV swarm robustness in forest fire detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From preparation to execution: Security protocol for third-party MES-enabled 5 g support handover authentication and key evolution. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) platforms offered by third-party providers increasingly complement fifth-generation (5 G) mobile networks. This business model separates the Mobile Network Operator (MNO)-managed gNodeB (gNB), from the Mobile Edge Host (MEH) belonging to a different management domain. Such separation exposes existing schemes that ignore key evolutions in the MEC domain to User Equipment (UE) privacy leakage. This paper proposes a secure two-phase handover scheme. The preparation phase builds a dedicated MEH-to-MEH channel to conveys MNO-independent context and pre-authentication data that enables UE to verify the target pair before the handover decision. The execution phase performs dual-track authentication and key evolution between the target gNB-MEH pair belonging to different registration domains and UE, establishing two isolated sessions to protect UE privacy in both domains. The proposed scheme seamlessly integrates with existing standards while overcoming the security threats inherent in the access layer, such as fake base station attacks. We show that our scheme satisfies key forward/backward secrecy, anonymity, and unlinkability using various formal and informal analysis methods. The prototype implemented on NS-3 5 G mmWave has only an 8.344 ms latency increase over the Conventional-5 G protocol stack, and this latency increment is minimally reduced by 33.98% compared to the state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Ye Bi and Chunfu Jia},
  doi          = {10.1109/TMC.2025.3599376},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From preparation to execution: Security protocol for third-party MES-enabled 5 g support handover authentication and key evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-heterogeneous federated learning with bidirectional knowledge distillation. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) faces significant challenges in addressing the system heterogeneity of edge devices, which often exhibit diverse computational and resource constraints. To overcome these limitations, we propose FedBid, a novel model-heterogeneous FL framework based on bidirectional, data-free knowledge distillation. By synthesizing data on the server and integrating forward-backward knowledge distillation with group aggregation, FedBid allows devices to adopt different models tailored to their resources and enabling efficient knowledge sharing across diverse model architectures without relying on public datasets. In addition, we further introduce progressive learning and generator perturbation strategies to enhance the efficiency of bidirectional knowledge distillation. Extensive experiments on benchmark datasets (Fashion-MNIST, CIFAR-10) and a real-world dataset (FLAIR) under varied data distributions demonstrate that FedBid achieves superior model performance than the state-of-the-art methods while reducing computational overhead on resource-constrained clients.},
  archive      = {J_TMC},
  author       = {Hao Zhang and Yaolin Zhu and Tingting Wu and Siyao Cheng and Jie Liu},
  doi          = {10.1109/TMC.2025.3599315},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Model-heterogeneous federated learning with bidirectional knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal pyramid-based multi-scale data completion in sparse crowdsensing. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Crowdsensing has emerged as a crucial and flexible method for collecting spatio-temporal data in various applications, such as traffic management, environmental monitoring, and disaster response. By recruiting users and utilizing their diverse mobile devices, this approach often results in data that is both sparse and multi-scale, complicating the data completion process. Although numerous data completion algorithms have been developed to address data sparsity, most assume that the collected data is of the same or similar scale, rendering them ineffective for multi-scale data. To overcome this limitation, in this paper, we propose a spatio-temporal pyramid-based multi-scale data completion framework in Sparse Crowdsensing. The basic idea is to leverage a pyramid structure to efficiently capture the complex interrelations between different scales. We first develop a Spatial-Temporal Pyramid Construction Module (ST-PC) to handle multi-scale inputs, and then propose a Spatial-Temporal Pyramid Attention Mechanism (ST-PAM) to capture multi-scale correlations while reducing computational complexity. Furthermore, our method incorporates cross-scale constraints to optimize completion performance. Extensive experiments on four real-world spatio-temporal datasets demonstrate the effectiveness of our framework in multi-scale data completion.},
  archive      = {J_TMC},
  author       = {Wenbin Liu and Hao Du and En Wang and Jiajian Lv and Weiting Liu and Bo Yang and Jie Wu},
  doi          = {10.1109/TMC.2025.3599322},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatio-temporal pyramid-based multi-scale data completion in sparse crowdsensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WaveRoRA: Wavelet rotary route attention for multivariate time series forecasting. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various sensors of Internet of Things (IoT) generate massive amounts of mobile traffic data, forming multivariate time series (MTS). Accurate forecasting of MTS facilitates the enhancement of proactive autoscaling and resource allocation in edge networks. While recent Transformer-based models (Transformers) have achieved significant success in MTS forecasting (MTSF), they tend to rely solely on either time-domain or frequency-domain features, which captures inadequate trends and periodic characteristics. To this end, we propose a wavelet learning framework that seamlessly integrates wavelet transforms with Transformers to benefit from time and frequency characteristics. We design a mixing-splitting architecture to model multi-scale wavelet coefficients and utilizes the attention mechanism to capture inter-series dependencies in the wavelet domain. However, the vanilla softmax self-attention (SA) is high-computational-cost and its smoothing effect diminishes the contrast between strong and weak variable correlations. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). RoRA incorporates rotary positional embeddings to enhance feature diversity and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix. Such design strengthens interactions among strongly correlated variables while mitigating the impact of weakly correlated noise. We further propose WaveRoRA, a unified model that leverages RoRA capturing inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs. Our code is available at https://github.com/Leopold2333/WaveRoRA.},
  archive      = {J_TMC},
  author       = {Aobo Liang and Yan Sun and Nadra Guizani},
  doi          = {10.1109/TMC.2025.3599406},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WaveRoRA: Wavelet rotary route attention for multivariate time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-resistant cross-domain authentication scheme based on certificate conversion in V2G. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-grid (V2G) is an important technology that utilizes mobile computing technology and mobile networks to enable two-way energy and information exchange between electric vehicles and the grid. Due to the openness and instability of mobile networks, V2G system faces many security challenges. Therefore, it is necessary to authenticate cross-domain vehicles to ensure the security and stability of grid services. However, existing schemes face various challenges in terms of security and efficiency. Notably, most of the existing V2G certificate-based cross-domain authentication schemes are based on traditional classical cryptography, which are unable to resist quantum attacks. Additionally, in these schemes, the grid server has to generate certificates not only for vehicles in its own domain but also for vehicles from other domains that travel to its domain. This not only increases the computational burden on the grid server but also reduces the overall authentication efficiency. To address these challenges, this paper proposes a lattice-based cross-domain authentication scheme for V2G networks, which integrates a novel certificate conversion mechanism. The key innovation lies in enabling vehicles to achieve seamless crossdomain transitions through converted certificates, allowing for multiple re-authentications without repeated certificate issuance. This approach significantly reduces computational overhead for both electric vehicles and grid servers while maintaining robust security guarantees. Specifically, we introduce the proxy resignature technology to generate the converted certificates, which combines the Chinese Remainder Theorem and secure multiparty computation to generate proxy rekeys. Crucially, to make our scheme quantum-resistant, we employ lattice-based cryptography. Finally, compared to similar competing schemes, our scheme improves computation efficiency and communication efficiency, and reduces storage cost while maintaining security.},
  archive      = {J_TMC},
  author       = {Jing-yi Yang and Run-hua Shi and Shao-fu Zhang},
  doi          = {10.1109/TMC.2025.3599433},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum-resistant cross-domain authentication scheme based on certificate conversion in V2G},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRDST: Low-latency DAG consensus through robust dynamic sharding and tree-broadcasting for IoV. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) is emerging as a pivotal technology for enhancing traffic management and safety. Its rapid development demands solutions for enhanced communication efficiency and reduced latency. However, traditional centralized networks struggle to meet these demands, prompting the exploration of decentralized solutions such as blockchain. Addressing blockchain's scalability challenges posed by the growing number of nodes and transactions calls for innovative solutions, among which sharding stands out as a pivotal approach to significantly enhance blockchain throughput. However, existing schemes still face challenges related to a) the impact of vehicle mobility on blockchain consensus, especially for cross-shard transaction; and b) the strict requirements of low latency consensus in a highly dynamic network. In this paper, we propose a DAG (Directed Acyclic Graph) consensus leveraging Robust Dynamic Sharding and Tree-broadcasting (DRDST) to address these challenges. Specifically, we first develop a standard for evaluating the network stability of nodes, combined with the nodes' trust values, to propose a novel robust sharding model that is solved through the design of the Genetic Sharding Algorithm (GSA). Then, we optimize the broadcast latency of the whole sharded network by improving the tree-broadcasting to minimize the maximum broadcast latency within each shard. On this basis, we also design a DAG consensus scheme based on an improved hashgraph protocol, which can efficiently handle crossshard transactions. Finally, the simulation proves the proposed scheme is superior to the comparison schemes in latency, throughput, consensus success rate, and node traffic load.},
  archive      = {J_TMC},
  author       = {Runhua Chen and Haoxiang Luo and Gang Sun and Hongfang Yu and Dusit Niyato and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3599385},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRDST: Low-latency DAG consensus through robust dynamic sharding and tree-broadcasting for IoV},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework of arithmetic-level variable precision computing for in-memory architecture: Case study in MIMO signal processing. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational complexity poses a significant challenge in wireless communication. Most existing attempts aim to reduce it through algorithm-specific approaches. However, the precision of computing, which directly relates to both computing performance and computational complexity, is a dimension that is fundamental but rarely explored in the literature. With the emerging architecture of in-memory computing, variable precision computing (VPC) is enabled, allowing each arithmetic operation to be processed with a distinct and specifically optimized computing precision. In this paper, we establish a unified framework of arithmetic-level variable precision computing (ALVPC), which aims to determine the optimized computing precision for each arithmetic operation. We first develop an arithmetic propagation error model exploiting stochastic analysis, and then formulate a mathematical optimization problem to strike balance between computing performance and computational complexity. Two algorithms, namely, offline VPC and online VPC, are proposed to solve the problem considering various practical concerns. Particularly, in a case study on zero-forcing (ZF) precoding, we reveal the Pareto boundary between computing performance and complexity, which exhibits up to a 60% sumrate enhancement or equivalently up to a 30% complexity reduction compared to the traditional fixed-length methods.},
  archive      = {J_TMC},
  author       = {Kaixuan Bao and Wei Xu and Xiaohu You and Derrick Wing Kwan Ng},
  doi          = {10.1109/TMC.2025.3599529},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A framework of arithmetic-level variable precision computing for in-memory architecture: Case study in MIMO signal processing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-enhanced spatial range query over large-scale encrypted mobile cloud datasets. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving spatial range query allows users to obtain valid data based on specific spatial attributes or geographical location while ensuring privacy. However, many existing Privacy-Preserving Spatial Range Query (PSRQ) schemes generally face the problems of low query efficiency and insufficient security when dealing with large-scale mobile cloud data sets, and it is difficult to resist Indistinguishability under Chosen-Plaintext Attack (IND-CPA). To solve these challenges, we first propose an Efficient and Secure Spatial Range Query scheme (ESSRQ), which is based on a dual mobile cloud architecture by integrating Geohash algorithm, Circular Shift Coalesce Zero-Sum Garbled Bloom Filter (CSC-ZGBF) and Symmetric Homomorphic Encryption (SHE), achieving a constant search complexity. However, ESSRQ cannot protect the access patterns, where the cloud server still has the potential to infer attacks based on the index position and even obtain plaintext queries. On this basis, we further propose an extended scheme ESSRQ-PIR, which introduces Private Information Retrieval (PIR) into single mobile cloud-based architecture, effectively prevents the leakage of access patterns, enhances the security of ESSRQ and can also realize efficient query on large-scale cloud datasets. Formal security analysis proves that our proposed schemes are secure against IND-CPA, and extensive experiments demonstrate that our schemes improve the query efficiency by up to nearly 20 times when compared with previous solutions. These features make the proposed schemes particularly suitable for privacy-preserving spatial queries in mobile cloud computing environments.},
  archive      = {J_TMC},
  author       = {Yinbin Miao and Jiaqi Yu and Jiliang Li and Xinghua Li and Jun Feng and Zhiquan Liu and Robert H. Deng},
  doi          = {10.1109/TMC.2025.3599519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-enhanced spatial range query over large-scale encrypted mobile cloud datasets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards accurate training time estimation for on-device heterogeneous federated learning. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimation of on-device model training time is increasingly required for emerging learning paradigms on mobile edge devices, such as heterogeneous federated learning (HFL). HFL usually customizes the model architecture according to the different capabilities of mobile edge devices to ensure efficient use of local data from all devices for training. However, due to oversimplification of training time modeling, existing methods rely on a single coefficient to represent computational heterogeneity, resulting in sub-optimal HFL efficiency. We find that existing methods ignore the important impact of runtime optimization of deep learning frameworks, which we call development-chain diversity. Specifically, layers of a model may have different algorithm implementations, and deep learning frameworks often have different strategies for selecting the algorithm they believe is the best based on a range of runtime factors, resulting in different training latencies and invalid predictions from existing methods. In this paper, in addition to considering this diversity to ensure synchronized completion time of model training, we also study how to select the best algorithm each time to reduce the latency of the per-round training, thereby further improving the overall efficiency of federated training. To this end, we propose LATTE, which consists of a novel data-driven selector that identifies the best algorithm at runtime based on relative runtime factors. By further integrating it into our training latency model, LATTE provides accurate training time estimation, significantly outperforming traditional heuristic approaches. To further improve the robustness of LATTE, we proposed dynamic device adapter to cope with the dynamic joining and exiting of the clients. We develop LATTE as middleware, compatible with different deep learning frameworks. Extensive results show significantly improved training convergence speed and model accuracy compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Kun Wang and Zimu Zhou and Zhenjiang Li},
  doi          = {10.1109/TMC.2025.3599524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards accurate training time estimation for on-device heterogeneous federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing secrecy performance of full-duplex relaying systems using IRS and RSMA. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a secure wireless system that integrates rate-splitting multiple access (RSMA), intelligent reflecting surfaces (IRS), and full-duplex relaying (FDR) to enhance secrecy performance against multiple colluding eavesdroppers. Closed-form expressions for the secrecy outage probabilities (SOPs) and average secrecy capacities (ASCs) of both common and private messages are derived. Comparative analysis with a baseline RSMA-FDR system (without IRS) demonstrates the benefits of IRS in improving secrecy. Numerical results reveal that RSMA-IRS-FDR achieves superior secrecy performance, with SOPs and ASCs strongly influenced by transmission power, particularly differing between message types. Moreover, the adverse effect of residual self-interference (RSI) is substantially mitigated in the IRS-aided system. The secrecy performance is further enhanced by increasing the number of IRS elements. The study also investigates the impact of key parameters such as power allocation, target secrecy rate, fading order, Wi-Fi frequency, and number of eavesdroppers, offering practical insights for secure RSMA-IRS-FDR system design.},
  archive      = {J_TMC},
  author       = {Phuong T. Tran and Thong-Nhat Tran and Ba Cao Nguyen and Tran Manh Hoang and Le The Dung and Taejoon Kim},
  doi          = {10.1109/TMC.2025.3599513},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing secrecy performance of full-duplex relaying systems using IRS and RSMA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-USV coverage path planning using spatial graph multi-actor-attention-critic reinforcement learning framework with operator pooling. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-unmanned surface vehicle coverage path planning (MCPP) presents significant challenges in large-scale aquatic environments due to dynamic ocean currents, non-Euclidean spatial structures, and task load imbalance. To address these challenges, we propose the spatial graph multi-agent actorattention- critic (SGMAAC) framework, which integrates a novel spatial graph attention network (SpGAT) for adaptive non- Euclidean feature extraction and operator pooling for dynamic path optimization and task load balance. Specifically, SpGAT captures global-local topological dependencies to enhance decisionmaking under irregular topography, while operator pooling employs multi-step grow, deduplicate, and exchange operations to eliminate redundant paths and balance task loads across USVs. Additionally, SGMAAC introduces a multi-objective reward function that jointly optimizes coverage efficiency, collision avoidance, and energy consumption, enabling coordinated path planning in large-scale aquatic environments. Experimental results demonstrate that SGMAAC outperforms baseline methods across diverse aquatic scenarios, achieving improvements in convergence speed, makespan, task load balance, and path costs.},
  archive      = {J_TMC},
  author       = {Yuanbo Zhu and Guangjie Han and Chuan Lin and Fan Zhang and Yun Hou},
  doi          = {10.1109/TMC.2025.3599616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-USV coverage path planning using spatial graph multi-actor-attention-critic reinforcement learning framework with operator pooling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge data auditing method supporting multi-keyword validation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3599724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of latency-sensitive applications like VR/AR-based immersive gaming, fueled by 5G and edge computing, demands ultra-low latency. While deploying data replicas on edge servers addresses latency, the highly distributed and dynamic edge environment makes these replicas vulnerable to corruption. Furthermore, the constrained resources of edge servers, compared to cloud infrastructure, render traditional data integrity auditing schemes inefficient or impractical. Crucially, existing solutions lack the ability to perform targeted integrity checks on specific data segments (e.g., files containing sensitive user information), leaving critical vulnerabilities undetected. To overcome these limitations, this paper introduces EDI-K, a novel batch auditing scheme featuring multi-keyword authentication. EDI-K's key contributions are: (1) Enabling efficient, targeted integrity verification for data containing specific keywords; (2) Guaranteeing keyword privacy during the auditing process; and (3) Incorporating a novel data structure that facilitates highly efficient dynamic operations (updates, inserts, deletes) on the audited data. Security analysis confirms EDI-K's robustness, while comprehensive performance evaluations demonstrate its significant efficiency advantages over existing approaches, making it particularly suitable for the resource-scarce edge computing landscape.},
  archive      = {J_TMC},
  author       = {Jun Ye and Yu Jiang},
  doi          = {10.1109/TMC.2025.3599724},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge data auditing method supporting multi-keyword validation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum multi-agent reinforcement learning for cooperative mobile access in space-air-ground integrated networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving global space-air-ground integrated network (SAGIN) access only with CubeSats presents significant challenges such as the access sustainability limitations in specific regions (e.g., polar regions) and the energy efficiency limitations in CubeSats. To tackle these problems, high-altitude long-endurance unmanned aerial vehicles (HALE-UAVs) can complement these CubeSat shortcomings for providing cooperatively global access sustainability and energy efficiency. However, as the number of CubeSats and HALE-UAVs, increases, the scheduling dimension of each ground station (GS) increases. As a result, each GS can fall into the curse of dimensionality, and this challenge becomes one major hurdle for efficient global access. Therefore, this paper provides a quantum multi-agent reinforcement Learning (QMARL)-based method for scheduling between GSs and CubeSats/HALE-UAVs in order to improve global access availability and energy efficiency. The main reason why the QMARL-based scheduler can be beneficial is that the algorithm facilitates a logarithmic-scale reduction in scheduling action dimensions, which is one critical feature as the number of CubeSats and HALE-UAVs expands. Additionally, individual GSs have different traffic demands depending on their locations and characteristics, thus it is essential to provide differentiated access services. The superiority of the proposed scheduler is validated through data-intensive experiments in realistic CubeSat/HALE-UAV settings.},
  archive      = {J_TMC},
  author       = {Gyu Seon Kim and Yeryeong Cho and Jaehyun Chung and Soohyun Park and Soyi Jung and Zhu Han and Joongheon Kim},
  doi          = {10.1109/TMC.2025.3599683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum multi-agent reinforcement learning for cooperative mobile access in space-air-ground integrated networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMIT: Reflection-based charging jamming attack. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Wireless Rechargeable Sensor Networks (WRSNs) based platforms have become promising for broad applications. However, if an adversary disrupts the wireless charging process in WRSNs, sensors may die due to lack of timely energy supply, compromising the reliability and availability of systems relying on sensing tasks. In this paper, we develop a zero-cost power jamming attack in WRSNs, termed rEflection-based jaMmIng aTtack (EMIT), which introduces an off-the-shelf and inconspicuous reflector such as a Coca-Cola can that intentionally reflects the wave from the charger to destructively interfere with the charging wave at the target sensor. Our approach lifts the limitations of traditional charging attacks, including high cost, complex implementation and ease of detection. We conduct extensive field experiments to evaluate EMIT attack in different types of WRSNs. The results show that on average, the success rate of EMIT attack is 90% in WRSNs with fixed charging locations, and 75% in WRSNs with dynamic charging locations. Finally, we build a real-world WRSN on university campus to study the effectiveness of EMIT attack in complex scenarios. In total, EMIT attack causes 134 sensor deaths over 66 days.},
  archive      = {J_TMC},
  author       = {Tang Liu and Jing Gao and Yuan Yin and Die Wu and Jian Peng and Wenzheng Xu and Baijun Wu and Yazhou Tu},
  doi          = {10.1109/TMC.2025.3600093},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EMIT: Reflection-based charging jamming attack},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel secure split federated semantic learning framework and its optimization for digital twin network evolution. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel secure split federated semantic learning (SFsL) framework to facilitate the maintenance and evolution of digital twin networks (DTNs). Efficiently updating and evolving DTNs generally involves several critical processes: semantic extraction and transmission for physical-to-virtual synchronization, virtual model transformation and verification, and ensuring the security and privacy of physical entity data. While conventional semantic communication frameworks can effectively address semantic extraction and transmission, the complexities of virtual model transformation, verification, and data security demand a more comprehensive approach. To address these challenges, the proposed SFsL framework integrates split federated learning with task-oriented secure semantic communication schemes. In addition, it incorporates a token-based semantic defence method to distinguish between adversarial and authentic semantic data and an asynchronous secure model aggregation mechanism to enhance data-sharing efficiency. The system reliability is then formulated as a stochastic optimization problem, aiming to minimize cost complexity while maintaining high accuracy during periodic model aggregation. Evaluation results, obtained using performance metrics such as privacy loss, experienced loss, accuracy, cost and reliability, demonstrate that the SFsL framework outperforms other commonly adopted security and privacy schemes, offering improved efficiency towards the maintenance and evolution of such dynamic systems. This highlights the capability of SFsL to enable adaptive, efficient and reliable network evolutions when deployed in practical DTNs with dynamic resource constraints.},
  archive      = {J_TMC},
  author       = {Samuel D. Okegbile and Haoran Gao and Jun Cai},
  doi          = {10.1109/TMC.2025.3599838},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel secure split federated semantic learning framework and its optimization for digital twin network evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SC-GIR: Goal-oriented semantic communication via invariant representation learning for image transmission. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-oriented semantic communication (SC) aims to revolutionize communication systems by transmitting only task-essential information. However, current approaches face challenges such as joint training at transceivers, leading to redundant data exchange and reliance on labeled datasets, which limits their task-agnostic utility. To address these challenges, we propose a novel framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for image transmission. Our framework leverages self-supervised learning to extract an invariant representation that encapsulates crucial information from the source data, independent of the specific downstream task. This compressed representation facilitates efficient communication while retaining key features for successful downstream task execution. Focusing on machine-to-machine tasks, we utilize covariance-based contrastive learning techniques to obtain a latent representation that is both meaningful and semantically dense. To evaluate the effectiveness of the proposed scheme on downstream tasks, we apply it to various image datasets for lossy compression. The compressed representations are then used in a goal-oriented AI task. Extensive experiments on several datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%, and achieves over 85% classification accuracy for compressed data under different SNR conditions. These results underscore the effectiveness of the proposed framework in learning compact and informative latent representations.},
  archive      = {J_TMC},
  author       = {Senura Hansaja Wanasekara and Van-Dinh Nguyen and Kok-Seng Wong and M.-Duong Nguyen and Symeon Chatzinotas and Octavia A. Dobre},
  doi          = {10.1109/TMC.2025.3600434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SC-GIR: Goal-oriented semantic communication via invariant representation learning for image transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RipeTrack: Assessing fruit ripeness and remaining lifetime using smartphones. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have shown that a significant fraction of fresh fruits is discarded at the retail and consumer levels, wasting precious resources, polluting the environment, and contributing to increased food prices. An important factor contributing to this problem is the lack of scalable solutions for determining fruit ripeness and remaining lifetime. We propose a cost-effective solution that leverages the sensing capabilities of phones and machine learning models to analyze the optical properties of fruits at various ripening stages. The proposed solution is non-invasive, works for different fruits, and produces intuitive outputs, e.g. Unripe/Ripe/Expired and the percentage of remaining lifetime, enabling retailers and consumers to minimize food waste. We implement a proof-of-concept mobile application, RipeTrack, and demonstrate the accuracy and robustness of the proposed approach using an extensive empirical study with multiple fruits, including avocados, pears, bananas, nectarines, and mangoes. Our results show, for example, that RipeTrack can identify the ripeness level of avocados and pears with an accuracy of 95% and 98%, respectively, and it can predict their remaining lifetimes with an accuracy of 93% and 97%. Our results also show that RipeTrack can easily be extended to new fruits using transfer learning, and it functions in realistic environments, e.g. homes and grocery stores, that have diverse illuminations.},
  archive      = {J_TMC},
  author       = {Muhammad Shahzaib Waseem and Neha Sharma and Mohamed Hefeeda},
  doi          = {10.1109/TMC.2025.3599917},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RipeTrack: Assessing fruit ripeness and remaining lifetime using smartphones},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference service fidelity maximization in DT-assisted edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin (DT) technology enables smooth integrations of cyber and physical worlds in alignment with the Industry 4.0 initiative. DTs are virtual presentations of physical objects. Through synchronizations with physical objects in real-time, DTs can reflect the states of their objects with high fidelity. Orthogonal to the DT technology, mobile edge computing (MEC) is a promising computing paradigm that shifts computing power to the edge network, which is appropriate for delay-sensitive intelligent services. In this paper, we study fidelity-aware inference services in a DT-assisted MEC environment, where machine learning-based inference models must be continuously retrained using updated DT data in order to provide high-fidelity services for consumers. To this end, we first formulate two novel optimization problems: the initial DT and model placement problem with the aim of minimizing the total cost of various resources consumed, and the cumulative fidelity maximization problem to maximize the long-term cumulative fidelity of service models while minimizing the cost of resource consumption on service model fidelity enhancements over a given time horizon, through jointly scheduling mobile devices to upload their update data to synchronize with their DTs and determining whether DTs and/or models to be migrated at each time slot. We then develop an efficient algorithm for the initial DT and model placement problem, through a reduction to a series of minimum-cost maximum matching problems in auxiliary graphs. We also devise an online algorithm with a provable competitive ratio for the cumulative fidelity maximization problem, by designing an elegant service request admission strategy. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms are promising, and outperform their baselines by no less than 28%.},
  archive      = {J_TMC},
  author       = {Jing Li and Jianping Wang and Weifa Liang and Xiaohua Jia and Albert Y. Zomaya},
  doi          = {10.1109/TMC.2025.3600390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Inference service fidelity maximization in DT-assisted edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed resource allocation and coordinated scheduling for end-edge-cloud collaborative computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tier computation offloading is crucial to address capacity constraints and improve flexibility for mobile devices. However, existing research on multi-layer computing offloading faces challenges like inefficient resource utilization and poor scalability, particularly in handling diverse computational tasks. To address these challenges, this paper proposes a distributed resource allocation and mixed task offloading framework for end-edge-cloud collaborative systems that support partial and full task offloading modes. First, we propose a three-tier network computing architecture and formulate a task-offloading utility maximization problem by jointly optimizing mixed task-offloading and resource allocation. The proposed problem is a mixed integer nonlinear program (MINLP), which we solve by decomposing it into two subproblems resource allocation and task offloading. Edge computing resources and bandwidth allocation can be independently optimized at each edge node with a fixed task offloading strategy. Cloud computing resource allocation, while convex, involves a global constraint, which we solve in a decentralized manner using a multi-agent optimization approach. Then, we propose a joint task offloading and resource allocation optimization algorithm, CNO-TORA, to obtain the solution to the formulated problem. The algorithm is supported by strong theoretical guarantees and is almost surely convergent to a globally optimal solution. Experimental results on a real dataset demonstrate that our algorithm is scalable to large-scale networks and outperforms baselines, achieving improvements in average system utility ranging from 4.01%-28.15%.},
  archive      = {J_TMC},
  author       = {Changqing Long and Wenchao Meng and Shizhong Li and Shibo He and Chaojie Gu and Lin Cai},
  doi          = {10.1109/TMC.2025.3599885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed resource allocation and coordinated scheduling for end-edge-cloud collaborative computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifying AI for networking and networking for AI: The self-evolving edge learning. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3600270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Learning environments, characterized by limited wireless resources, encounter significant bottlenecks in network performance, particularly in Federated Learning (FL) tasks. Current resource allocation strategies are primarily classified into “AI for Networking” and “Networking for AI”. However, both approaches fail to adequately address the interaction between network states and AI task requirements, thereby limiting their overall effectiveness. To address this, we propose a novel bidirectional dynamic collaborative optimization mechanism that enables real-time interaction between AI task performance and network states. This mechanism adjusts both AI task resource requirements and network configurations based on performance feedback, breaking away from traditional unidirectional optimization approaches. We introduce the AI-network unified algorithm, which incorporates data-driven dynamic sensing and enhances system adaptability and robustness, achieving self-optimization in edge learning. Theoretical analysis and simulation results demonstrate the significant advantages of our approach in simultaneously improving network resource utilization and AI task performance, providing an effective solution for the future wireless network.},
  archive      = {J_TMC},
  author       = {Jie Tang and Yeguang Qin and Fengxiao Tang and Ming Zhao and Nei Kato},
  doi          = {10.1109/TMC.2025.3600270},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unifying AI for networking and networking for AI: The self-evolving edge learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G-TPS: A two-phase real-time scheduling and adaptation framework for 5G radio access networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the many industrial wireless solution candidates, 5G New Radio (NR) has drawn significant attention in recent years due to its capabilities to support ultra-high-speed communication, wide coverage, ultra-low latency, and massive connectivity. Despite its great potential, 5G NR also brings significant complexity in scheduling data flows to meet their hard real-time requirements in industrial applications. In this paper, we first leverage a 5G RAN testbed to benchmark the downlink throughput and explore the impact of modulation and coding scheme (MCS) selection on the network performance. We then formulate a real-time flow scheduling problem in industrial 5G NR, which features per-flow real-time schedulability guarantee through time-frequency resource allocation. We propose a novel two-phase scheduling framework, named 5G-TPS, to construct a schedule that meets the deadlines of all the flows. To adapt to dynamic channel conditions, 5G-TPS enables online schedule adjustment for affected flows to meet their timing requirements. For large-scale multi-cell 5G industrial systems with cloud radio access network (C-RAN) architecture, we further introduce a user association algorithm respecting the real-time requirements of individual user equipment (UEs). Extensive experimental studies show that 5G-TPS can achieve schedulability ratios comparable to the Satisfiability Modulo Theory (SMT)-based exact solution and outperform many other state-of-the-art scheduling approaches, including the built-in 5G NR schedulers.},
  archive      = {J_TMC},
  author       = {Tianyu Zhang and Jiachen Wang and X. Sharon Hu and Song Han},
  doi          = {10.1109/TMC.2025.3599880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {5G-TPS: A two-phase real-time scheduling and adaptation framework for 5G radio access networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An offline multi-agent reinforcement learning framework for radio resource management. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a $15\%$ improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.},
  archive      = {J_TMC},
  author       = {Eslam Eldeeb and Hirley Alves},
  doi          = {10.1109/TMC.2025.3599918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An offline multi-agent reinforcement learning framework for radio resource management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigFormer: A transformer for signaling data augmentation via location reconstruction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile signaling data has been used to discover comprehensive and fine-grained patterns of individual travel activities. However, mobile signaling data often exhibits notable data quality issues caused by systematic factors (e.g., weather, construction, congestion) or individual factors (e.g., low battery, poor reception). Improving the quality of signaling data is a crucial yet challenging task due to irregular spatiotemporal intervals and complex inter-correlations among base stations. In this paper, we formalize the augmentation of signaling data as a base station location reconstruction task. We propose a location-aware transformer structure named SigFormer, which employs self-supervised learning to reconstruct the location information of missing base stations based on the sampled signaling sequence. Specifically, we first design a Continuous Signaling Encoder to model irregularly sampled signaling sequences and encode interval information generated by base station transitions. Then, we learn the Base Station Embedding to describe implicit features of base stations and design a Neighbor Region Encoder to incorporate geographic information as an auxiliary for base station representations. Finally, through an attention-based encoder-decoder framework, we aggregate location information and base station features, employing sequence learning to capture spatiotemporal dependencies and reconstruct base station locations. Experiment results on real-world datasets indicate that our method outperforms existing approaches for location reconstruction. The proposed method originates from practical demands in telecom systems, addressing the challenge of missing and abnormal base station locations by modeling spatiotemporal transition patterns in signaling data. Our approach supports various tasks in practical applications, including imputing missing base station locations and correcting anomalous locations, effectively improving the quality of signaling data.},
  archive      = {J_TMC},
  author       = {Mingzhe Liu and Haiyang Jiang and Tongyu Zhu and Leilei Sun and Jibin Wang and Hao Sheng},
  doi          = {10.1109/TMC.2025.3599925},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SigFormer: A transformer for signaling data augmentation via location reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FullPerception: Network-level collaborative perception for eliminating vehicular blind spots. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3600060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative perception can significantly enhance the perceptual capabilities of autonomous vehicles by sharing sensing information through vehicular communications. However, large-scale sharing of sensing information often results in unsustainable network loads, making it challenging to maximize perception performance with limited communication resources in complex environments. To address this challenge, we propose FullPerception, an innovative cooperative perception framework that jointly orchestrates sensing information sharing and communication resource allocation at the network level. FullPerception advocates for the sharing of semantic information (neural network features) within critical areas, i.e., blind spots. With limited communication resources, FullPerception strategically eliminates these blind spots to maximize the accumulated perception performance. We formulate this strategy as a weighted optimization problem and prove its NP-hardness. We propose a simple yet effective algorithm, Proactive Conflict-free Scheduling (PCS), which guarantees a good performance ratio by considering broader contexts. PCS is meticulously combined with recursive structure, accounting for both the overall and future contexts to determine link scheduling and resource allocation. We demonstrate that FullPerception improves perception accuracy by 20% relative to single-vehicle systems and by 10% compared to existing scheduling methods through large-scale comprehensive joint simulation experiments.},
  archive      = {J_TMC},
  author       = {Lin Liang and Guiyang Luo and Yijing Lin and Lei Deng and Nan Cheng and Quan Yuan and Jinglin Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3600060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FullPerception: Network-level collaborative perception for eliminating vehicular blind spots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-sustainable multi-functional RIS-enabled integrated sensing and communication systems. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surface (RIS)-enabled integrated sensing and communication (ISAC) systems enhance spectrum efficiency and sensing accuracy. Building on this, we propose a novel self-sustainable multi-functional RIS (S-MFRIS) concept that supports multiple functionalities: reflection, refraction, amplification, energy harvesting, and target sensing. By harvesting energy from incident signals, the S-MFRIS can reflect, refract, and amplify signals without needing an external power supply, effectively overcoming double-fading attenuation. Furthermore, by deploying low-cost sensor elements, the S-MFRIS can capture echo signals from multiple targets, mitigating the signal attenuation commonly associated with multi-hop links. Then, we establish an S-MFRIS-enabled ISAC system and formulate an optimization problem to maximize the signal-to-interference-plus-noise ratio (SINR) of the sensing targets, subject to constraints on communication rate, power budget, and reflection coefficients. To solve this non-convex problem, we decompose it into three sub-problems, which are efficiently addressed using an iterative algorithm. Simulation and numerical results demonstrate the following key findings: (1) The proposed algorithm achieves better convergence and performance than the semidefinite relaxation-based and random-based algorithms. (2) The performance of the MFRIS-aided system varies under different operating protocols, with the self-sustainable MFRIS outperforming other schemes, particularly when the power budget is sufficient. (3) The proposed S-MFRIS achieves $30\%-46\%$ sensing SINR gains at most for the same total power budget or element configuration. (4) The number of sensing elements improves sensing performance up to a certain point, after which further increases in the number of elements yield diminishing returns.},
  archive      = {J_TMC},
  author       = {Xueyan Cao and Shubin Wang and Yuzheng Ren},
  doi          = {10.1109/TMC.2025.3599887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Self-sustainable multi-functional RIS-enabled integrated sensing and communication systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\pi$-eLight: Programmatic interpretable reinforcement learning for effective traffic signal control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in Deep Reinforcement Learning (DRL) have significantly improved the performance of adaptive Traffic Signal Control (TSC). However, DRL policies are typically represented by over-parameterized neural networks, which function as black-box models. Consequently, the learned policies often lack interpretability and are challenging to deploy on resource-constrained edge hardware. Moreover, the DRL methods frequently exhibit poor generalization, struggling to transfer the learned policies across different geographical regions. These limitations hinder the real-world applicability of learning-based approaches. To address these issues, we suggest the use of an inherently interpretable program for representing the control policy. We present Programmatic Interpretable reinforcement learning for effective traffic signal control ($\pi$-eLight), a new approach designed to autonomously discover non-differentiable programs. Specifically, we first define an effective program framework as the control policy, where certain components remain learnable. Next, we introduce a Domain Specific Language (DSL) for constructing interpretable programs and transformation rules for generating programs with hierarchical structures. Last, we utilize Monte Carlo Tree Search (MCTS) to find the optimal program in a discrete space. Extensive experiments demonstrate that $\pi$-eLight consistently outperforms DRL-based baselines while exhibiting superior generalization across intersections in different cities. Moreover, the learned programmatic policies can be directly deployed on edge devices with minimal computational resources, further enhancing real-world applicability.},
  archive      = {J_TMC},
  author       = {Yin Gu and Kai Zhang and Qi Liu and Haojie Yuan and Runlong Yu},
  doi          = {10.1109/TMC.2025.3600533},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {$\pi$-eLight: Programmatic interpretable reinforcement learning for effective traffic signal control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPdetector: A multi-party collaborative federated transfer learning approach for IoT intrusion detection. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3600306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive adoption of the Internet of Things (IoT) is accompanied by numerous network security threats, making the timely detection of anomalies in traffic data through intrusion detection increasingly critical. The existing intrusion detection methods based on federated learning can achieve good results under the condition of sufficient labeled data. However, the traffic data of participants in real IoT environments have various characteristics and there are a large amount of unlabeled data, which easily leads to the performance degradation of the intrusion detection model. To address this challenge, this paper proposes a novel federated transfer learning approach for IoT intrusion detection based on multi-party collaboration called MPdetector. MPdetector uses an encoder to extract the feature representation of diverse traffic data from heterogeneous clients, and maps the feature representation of each client to a unified feature space. In addition, a label transfer strategy is introduced to make full use of unlabeled data, and a new mapping function is used to reconstruct the traffic data of each client to expand client's local data set, which can further improve the detection performance of the model in varied and complex IoT environments. Theoretical analysis proves that the entire transfer learning process of MPdetector is conducted within a secure context. Experiments on four widely used intrusion detection datasets show that MPdetector can detect known and unknown abnormal traffic more accurately than the existing three classical intrusion detection algorithms, and has strong generalization. Meanwhile, the detection effect of MPdetector will be further improved with the increase of the volume of labeled traffic.},
  archive      = {J_TMC},
  author       = {Li Lin and ZhenKun Chen},
  doi          = {10.1109/TMC.2025.3600306},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MPdetector: A multi-party collaborative federated transfer learning approach for IoT intrusion detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFF-SLAM: Dynamic feature filtering-based simultaneous localization and mapping for UAV positioning in IoT-enabled complex environments. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the 5G RedCap, the upcoming 6G and the proliferation of the Internet of Things (IoT) have catalyzed the rapid advancement of unmanned aerial vehicle (UAV) technology while also promoting UAVs' widespread application. In IoT-enabled environments where the global positioning system (GPS) signals are compromised, visual simultaneous localization and mapping (V-SLAM) technology has emerged as an effective positioning solution, valued for its reliability. However, the presence of dynamic elements in complex environments, such as pedestrians and vehicles, poses challenges to the positioning accuracy of UAVs employing V-SLAM for navigation. This paper proposes a dynamic feature filtering-based SLAM (DFF-SLAM) approach to eliminate the impact of dynamic factors in dynamic environments, thereby enhancing the positioning accuracy of UAVs in IoT-enabled complex environments. Firstly, a semantic detection thread is designed to identify semantic information in the scene and acquire prior dynamic targets, facilitating the filtering of prior dynamic feature points. Secondly, optical flow tracking conducted at each level of the image pyramid facilitates feature point matching across consecutive images. Finally, the epipolar geometry constraint is utilized to determine the motion status of remaining feature points, further filtering out dynamic feature points. Simulation results demonstrate that compared to traditional visual SLAM systems, the UAV equipped with the DFF-SLAM system achieves more accurate positioning and meets real-time positioning requirements when navigating through IoT enabled complex environments},
  archive      = {J_TMC},
  author       = {Jinglei Li and Yiming Jia and Meng Qin and Qinghai Yang and Tony Q. S. Quek and Wen Gao and Kyung Sup Kwak},
  doi          = {10.1109/TMC.2025.3600661},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DFF-SLAM: Dynamic feature filtering-based simultaneous localization and mapping for UAV positioning in IoT-enabled complex environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of UAV-carried IRS for urban low altitude mmWave communications with deep reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3600682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging technologies in sixth generation (6G) of wireless communications, such as terahertz communication and ultra-massive multiple-input multiple-output, present promising prospects. Despite the high data rate potential of millimeter wave communications, millimeter wave (mmWave) communications in urban low altitude economy (LAE) environments are constrained by challenges such as signal attenuation and multipath interference. Specially, in urban environments, mmWave communication experiences significant attenuation due to buildings, owing to its short wavelength, which necessitates developing innovative approaches to improve the robustness of such communications in LAE networking. In this paper, we explore the use of an unmanned aerial vehicle (UAV)-carried intelligent reflecting surface (IRS) to support low altitude mmWave communication. Specifically, we consider a typical urban low altitude communication scenario where a UAV-carried IRS establishes a line-of-sight (LoS) channel between the mobile users and a source user (SU) despite the presence of obstacles. Subsequently, we formulate an optimization problem aimed at maximizing the transmission rates and minimizing the energy consumption of the UAV by jointly optimizing phase shifts of the IRS and UAV trajectory. Given the non-convex nature of the problem and its high dynamics, we propose a deep reinforcement learning-based approach incorporating neural episodic control, long short-term memory, and an IRS phase shift control method to enhance the stability and accelerate the convergence. Simulation results show that the proposed algorithm effectively resolves the problem and surpasses other benchmark algorithms in various performances.},
  archive      = {J_TMC},
  author       = {Wenwen Xie and Geng Sun and Bei Liu and Jiahui Li and Jiacheng Wang and Hongyang Du and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3600682},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of UAV-carried IRS for urban low altitude mmWave communications with deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing network reliability in UASNs: A collision-aware critical node identification algorithm. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical node identification is essential for Underwater Acoustic Sensor Networks (UASNs) to ensure network connectivity and reliability. Existing methods identify critical nodes by evaluating their contributions to network connectivity and node communication count. However, these methods identify critical nodes inaccurately due to neglecting the influence of packet collisions, leading to unreliable network. Packet collisions disrupt connected links and cause communication failures, resulting in unreliable network connectivity and improper communication count. To this end, we propose the Collision-Aware Critical Node Identification Algorithm (CCNIA), which accounts for the impact of packet collisions to improve the accuracy of critical node identification and enhance network reliability. CCNIA identifies critical nodes with high connectivity, large collision probability, and heavy network load, through building the three following interdependent models. Specifically, Topological Connectivity Model (TCM) evaluates link reachability by analyzing connectivity and density within a node's local network. Based on TCM, Collision Probability Model (CPM) further ensures packet reliability by quantifying the impact of packet collisions on critical node identification. Through CPM's reliable packet transmissions, Network Load Model (NLM) assesses network efficiency by analyzing node occurrence count within global end-to-end communication paths. Experiments show that CCNIA outperforms existing methods across diverse network configurations, enhancing network reliability in terms of packet delivery ratio, delay, and energy efficiency.},
  archive      = {J_TMC},
  author       = {Shanshan Song and Xiujuan Wu and Cangzhu Xu and Miao Pan and Guangjie Han},
  doi          = {10.1109/TMC.2025.3600460},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing network reliability in UASNs: A collision-aware critical node identification algorithm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning on heterogeneous and long-tailed data via disentangled representation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a popular distributed machine learning method that enables the development of a robust global model through decentralized computation and periodic model aggregation, without requiring direct access to clients' data. However, data heterogeneity poses a significant challenge in FL, and the global long-tail distribution exacerbates this issue. While substantial research has focused on mitigating performance degradation caused by long-tailed distributions, existing methods typically concentrate on addressing discrepancies between local and global class distributions, often overlooking the fact that these discrepancies stem from variations in the data itself. To address this, we propose a novel approach, Federated Context Optimization and Feature Information Decoupling (FedDR), which generates partition strategies for each sample to extract and leverage long-tail, global, personalized, and label-text information within its features to enhance the representational distinction of tail classes. Specifically, we first design a Feature Information Decoupling module that separates global, personalized, and long-tail information within the features and incorporates this information into the loss function to strengthen the global model's focus on personalized information in tail samples. Furthermore, to exploit the textual label information embedded in the samples, we integrate a cross-modal model, CoOp, which utilizes open-vocabulary prior knowledge, and implement dynamic knowledge distillation between the client model and CoOp to enhance the client model's feature representation capability. Extensive experimental results on multiple benchmarks demonstrate that the proposed FedDR outperforms state-of-the-art methods in the federated long-tailed learning setting.},
  archive      = {J_TMC},
  author       = {Yizhi Zhou and Junxiao Wang and Yuchen Qin and Xin Xie and Zhipeng Song and Heng Qi},
  doi          = {10.1109/TMC.2025.3600767},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated learning on heterogeneous and long-tailed data via disentangled representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BOTH: Efficient coordination of mobile agents with graph-enhanced bayesian online learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3600920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative agents, consisting of at least one human and one mobile robot agent working toward a common objective, are increasingly prevalent and effective in both social and industrial spheres, such as manufacturing. The inherent heterogeneity of these agents requires efficient and scalable Task Scheduling and Allocation (TSA) schemes that match individuals to tasks based on their abilities and meet specific temporal constraints, maximizing performance in less time. Existing works face challenges as exact methods rely on assumptions and deterministic models, which struggle to scale and infer time-varying, stochastic human task performance. While offline reinforcement learning shows promise, it is time-consuming and heavily dependent on training data that is often scarce in practical factory settings. To address these challenges, we formulate the TSA problem in mobile multi-agent teams as a temporal-constrained contextual decision-making process and propose the Bayesian Optimization-augmented Team coordination among Heterogeneous agents (BOTH), a novel scalable and training-free scheduling approach. The core idea is to use Gaussian Processes (GP) to iteratively infer agent dynamics in real-time, enabling the automatic derivation of a robust TSA solution that requires no prior data and adapts to varying problem sizes. We start by employing a heterogeneous graph-based encoder to extract representative context from the individual differences among team agents and tasks, considering strict temporal constraints. Following this, we propose a GP-driven Bayesian optimizer to intelligently explore and exploit optimal task assignments for each context, without making assumptions about the system. Experiments on synthetic and real datasets demonstrate that BOTH boosts accuracy and time efficiency compared to competing baselines, even within a few iterations.},
  archive      = {J_TMC},
  author       = {Hui Wang and Zhiwen Yu and Yao Zhang and Jiaqi Liu and Liekang Zeng and Huan Zhou and Bin Guo and Guoliang Xing},
  doi          = {10.1109/TMC.2025.3600920},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BOTH: Efficient coordination of mobile agents with graph-enhanced bayesian online learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO-split: A semi-supervised split learning framework over LEO satellite networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3601371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the increasing deployment of LEO satellite systems has enabled various space analytics (e.g., crop and climate monitoring), which heavily relies on the advancements in deep learning (DL). However, the intermittent connectivity between LEO satellites and ground station (GS) significantly hinders the timely transmission of raw data to GS for centralized learning, while the scaled-up DL models hamper distributed learning on resource-constrained LEO satellites. Though split learning (SL) can be a potential solution to these problems by partitioning a model and offloading primary training workload to GS, the labor-intensive labeling process remains an obstacle, with intermittent connectivity and data heterogeneity being other challenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL design tailored for satellite networks to combat these challenges. Leveraging SS learning to handle (labeled) data scarcity, we construct an auxiliary model to tackle the training failure of the satellite-GS non-contact time. Moreover, we propose a pseudo-labeling algorithm to rectify data imbalances across satellites. Lastly, an adaptive activation interpolation scheme is devised to prevent the overfitting of server-side sub-model training at GS. Extensive experiments with real-world LEO satellite traces (e.g., Starlink) demonstrate that our LEO-Split framework achieves superior performance compared to state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Yuxin Zhang and Zhe Chen and Zihan Fang and Cong Wu and Xianhao Chen and Yue Gao and Jun Luo},
  doi          = {10.1109/TMC.2025.3601371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LEO-split: A semi-supervised split learning framework over LEO satellite networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiFuse: A high-precision on-device depth perception system with multi-data fusion. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3601357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MobiFuse, a high-precision depth perception system on mobile devices that combines dual RGB and Time-of-Flight (ToF) cameras. To achieve this, we leverage physical principles from various environmental factors to propose the Depth Error Indication (DEI) modality, characterizing the depth error of ToF and stereo-matching. Furthermore, we employ a progressive fusion strategy, merging geometric features from ToF and stereo depth maps with depth error features from the DEI modality to create precise depth maps. Additionally, we create a new ToF-Stereo depth dataset, RealToF, to train and validate our model. Our experiments demonstrate that MobiFuse excels over baselines by significantly reducing depth measurement errors by up to 77.7%. It also showcases strong generalization across diverse datasets and proves effectiveness in two downstream tasks: 3D reconstruction and 3D segmentation. The demo video of MobiFuse in real-life scenarios is available at the de-identified YouTube link.},
  archive      = {J_TMC},
  author       = {Jinrui Zhang and Deyu Zhang and Tingting Long and Wenxin Chen and Ju Ren and Yunxin Liu and Yudong Zhao and Yaoxue Zhang and Youngki Lee},
  doi          = {10.1109/TMC.2025.3601357},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiFuse: A high-precision on-device depth perception system with multi-data fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-aware perturbation optimization for robust deep reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3601531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and statespecific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks},
  archive      = {J_TMC},
  author       = {Zongyuan Zhang and Tianyang Duan and Zheng Lin and Dong Huang and Zihan Fang and Zekai Sun and Ling Xiong and Hongbin Liang and Heming Cui and Yong Cui},
  doi          = {10.1109/TMC.2025.3601531},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {State-aware perturbation optimization for robust deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Provably secure and reliable privacy-preserving authentication scheme for drone-to-drone communications in internet of autonomous things. <em>TMC</em>, 1-10. (<a href='https://doi.org/10.1109/TMC.2025.3601369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancements in wireless communication technologies, Unmanned Aerial Vehicles (UAVs), also known as Small Unmanned Aerial Vehicles (SUAVs) or drones, have been increasingly used in various applications, including the civilian sector. As a result, the security of SUAVs has garnered significant attention from the research community. Furthermore, drones are resource-constrained in nature and can be vulnerable to various known cybersecurity attacks over wireless communication. In light of these considerations, we propose a Provably Secure and Reliable Privacy-Preserving Authentication Scheme for Drone-to-Drone Communications in Internet of Autonomous Things (PSRS-D2D). The proposed scheme employs a secure one-way cryptographic hash and Elliptic Curve Cryptography (ECC) to accomplish a certain level of security. We provide security and privacy analysis, comparing it with competing UAV authentication schemes. This ensures that the PSRS-D2D scheme can withstand various prominent security properties, including mutual authentication and strong anonymity, and is secure against several attacks, such as replay, impersonation, and Man-In-The-Middle (MITM) attacks. We evaluated the performance of the proposed scheme in terms of computational and communicational costs. Furthermore, we conducted a formal security analysis using the Real-Or-Random (ROR) model and the Scyther simulation tools, which demonstrate that our scheme offers significant advantages in terms of security and performance.},
  archive      = {J_TMC},
  author       = {Mohd Shariq and Norziana Jamil and Gopal Singh Rawat and Shehzad Ashraf Chaudhry and Mehedi Masud and Ashok Kumar Das},
  doi          = {10.1109/TMC.2025.3601369},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Provably secure and reliable privacy-preserving authentication scheme for drone-to-drone communications in internet of autonomous things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto actor-critic for communication and computation co-optimization in non-cooperative federated learning services. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3601833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations – PAC-MCoFL achieves approximately $5.8\%$ and $4.2\%$ improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.},
  archive      = {J_TMC},
  author       = {Renxuan Tan and Rongpeng Li and Xiaoxue Yu and Xianfu Chen and Xing Xu and Zhifeng Zhao},
  doi          = {10.1109/TMC.2025.3601833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pareto actor-critic for communication and computation co-optimization in non-cooperative federated learning services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal flight speed scheduling and battery swapping in UAV-enabled mobile edge computing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3601743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-distance and long-duration flight missions of unmanned aerial vehicles (UAVs), optimal scheduling of flight speed and energy replenishment is crucial to ensure flight efficiency and safety. This paper focuses on a UAV-based patrol inspection system, where a UAV is scheduled to visit multiple task nodes that are geographically distributed in the communication coverage of a base station (BS). The UAV hovers at each task node, performing data collection and data processing. The BS is equipped with a mobile edge computing (MEC) server and a battery swapping station, offering computation and energy support to the UAV. A decision-making model customized for the UAV is proposed, jointly optimizing flight speed selection, battery swapping, and task offloading to minimize the UAV's total operational cost in its flight. By introducing virtual nodes in the flight network, we construct a unidirectional extended graph, based on which the original nonconvex cost minimization problem is reformulated to a tractable mixed-integer convex problem. Further, a fast heuristic based on analytical target cascading (ATC) is developed to obtain suboptimal solutions to large-scale problems. Results demonstrate that the proposed model can lower the UAV's total operational cost by providing greater flexibility in terms of speed selection and battery swapping, and the proposed heuristic shows high computational efficiency for large-scale network scenarios.},
  archive      = {J_TMC},
  author       = {Dongmei Ye and Zhengqing Sun and Weifeng Zhong and Jiawen Kang and Xumin Huang and Dong In Kim and Shengli Xie and Chau Yuen},
  doi          = {10.1109/TMC.2025.3601743},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimal flight speed scheduling and battery swapping in UAV-enabled mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARWIN: Digital twin assisted robot navigation and WIreless network management. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3602213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated warehouses involve robots that move across the floor, avoiding obstacles while remaining connected via an access point (AP) to a central controller that instructs the robots. The complex propagation environment and presence of metallic surfaces results in spotty coverage, which changes over time as the location of stored products and machinery changes. Thus, maintaining an assured connectivity to APs while performing navigation is a challenge, although it is needed to relay local sensor data from the robots to the controller and receive directions from the latter. $\rm{DARWIN}$, involves creating a digital twin of the warehouse for training the robots by jointly optimizing the navigation and avoiding wireless dead-spots. $\rm{DARWIN}$ has three key capabilities: First, it captures the features of both physical and RF environments in the digital world. Second, it allows real-time updating of the digital twin if significant disparity is detected compared to the physical environment. Finally, it includes a reinforcement learning algorithm that jointly optimizes navigation and network resource management, while accounting for handover and outage. We validate $\rm{DARWIN}$ on an emulation environment consisting of Robot Operating System and Gazebo platforms along with real-world RF measurements. Results reveal that $\rm{DARWIN}$ reduces the number of steps by 43% compared to choosing the closest AP, while detecting environmental changes with maximum 96% accuracy to maintain a high-fidelity digital twin.},
  archive      = {J_TMC},
  author       = {Batool Salehi and Debashri Roy and Mark Eisen and Amit Baxi and Dave Cavalcanti and Kaushik Chowdhury},
  doi          = {10.1109/TMC.2025.3602213},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DARWIN: Digital twin assisted robot navigation and WIreless network management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlueKey: Exploiting bluetooth low energy for enhanced physical-layer key generation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3602221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) is a prevalent technology in various applications due to its low power consumption and wide device compatibility. Despite its numerous advantages, the encryption methods of BLE often expose devices to potential attacks. To fortify security, we investigate the application of Physical-layer Key Generation (PKG), a promising technology that enables devices to generate a shared secret key from their shared physical environment. Although extensively investigated, PKG is generally discussed in the context of Wi-Fi, and existing solutions for BLE demonstrate significantly lower performance. To bridge this gap, we propose a distinctive approach that capitalizes on the inherent characteristics of BLE to facilitate efficient PKG. We utilize the constant tone extension within BLE protocols to extract comprehensive physical layer information and introduce an innovative method that employs Legendre polynomial quantization for PKG. This method facilitates the exchange of secret keys with a high key matching rate and a high key generation rate. The efficacy of our approach is validated through extensive experiments on a software-defined radio platform, underscoring its potential to enhance security in the rapidly expanding field of BLE applications. A pilot study on commercial off-the-shelf BLE devices further validates the system's practicality, revealing important trade-offs between performance and hardware constraints in real-world deployments.},
  archive      = {J_TMC},
  author       = {Yawen Zheng and Fan Dang and Zihao Yang and Jinyan Jiang and Xu Wang and Lin Wang and Kebin Liu and Xinlei Chen and Yunhao Liu},
  doi          = {10.1109/TMC.2025.3602221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BlueKey: Exploiting bluetooth low energy for enhanced physical-layer key generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leverage the duty ratio of frequency-shift wave to design a novel amplitude modulation for backscatter communications. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3602718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for ultra-low-power wireless connectivity motivates the study of backscatter communication technology. There are already some related products on the market based on excitation signals from commercial radios. However, their modulation techniques, which are the key to backscatter communications, mainly focus on the phase or frequency domain while amplitude modulation is largely ignored. Most research works either deploy one finely tuned RF impedance port for every needed reflection state or connect a nonlinear device to antenna and tune the reflection amplitude by adjusting its biasing voltage, where the former is too complex and the latter is unstable under variable incident signal power, limiting the backscatter applications. For this reason, we introduce AMscatter to leverage the duty ratio of the frequency-shift wave (FS-wave) to design a novel amplitude modulation. Both theoretical analysis and experimental results show that the reflection amplitude approximates a sinusoidal function of the duty ratio. This method requires only two fixed RF impedances, making AMscatter simple and stable. Moreover, we show how to use AMscatter to design quadrature amplitude modulation (QAM) and pulse shaping to improve backscatter communication performance. Extensive experimental results show that the throughput can be as high as 3.9 Mbps, the supported operational range can reach 20 m with 16-QAM modulation, and the out-of-band interference can be suppressed by 15 dB without negatively affecting the communication performance through our pulse shaping.},
  archive      = {J_TMC},
  author       = {Longzhi Yuan and Hangcheng Cao and Yanan Ma and Wei Gong and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3602718},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Leverage the duty ratio of frequency-shift wave to design a novel amplitude modulation for backscatter communications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uplink and downlink subband resource allocation for subband full-duplex enabled industrial intelligent manufacturing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3602872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of industrial intelligent manufacturing necessitates wireless communication systems capable of replacing conventional wired infrastructures, offering superior flexibility, scalability, and reduced maintenance overhead. While 5 G New Radio (NR) Ultra-Reliable Low-Latency Communication (uRLLC) standards (Release 15-17) have shown promise for mission-critical applications, current implementations remain constrained by their unidirectional optimization paradigm, unable to simultaneously satisfy the dual imperatives of sub-millisecond latency ($\lt 1$ ms) and 99.9999% reliability demanded by industrial control systems. To address these challenges, we present a transformative subband full-duplex (SBFD) network architecture that ensures persistent time-domain spectral availability for concurrent uplink/downlink operations, thereby eliminating direction-switching latency. Our solution introduces three key innovations: (1) an interference-aware SBFD resource allocation framework that strategically isolates UL/DL subbands to minimize cross-link interference (CLI), (2) a dual-optimization algorithm that jointly maximizes spectral efficiency while guaranteeing channel-adaptive reliability thresholds, and (3) a practical implementation scheme compatible with existing 5G NR physical layer specifications. Extensive simulations under realistic factory channel models demonstrate 58.3% reduction in aggregate CLI and 41.2% improvement in control command decoding accuracy compared to legacy half-duplex systems. This research establishes a new paradigm for wireless industrial networks, effectively closing the performance gap between 5G URLLC specifications and the exacting demands of Industry 4.0 applications.},
  archive      = {J_TMC},
  author       = {Zheng Jiang and Dingyou Ma and Bowen Wang and Ningyan Guo and Kan Yu and Qixun Zhang},
  doi          = {10.1109/TMC.2025.3602872},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Uplink and downlink subband resource allocation for subband full-duplex enabled industrial intelligent manufacturing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated distributionally robust optimization with non-convex objectives: Algorithm and analysis. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3602796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. Nevertheless, prevailing DRO techniques encounter three primary challenges in distributed environments: 1) addressing asynchronous updating efficiently; 2) leveraging the prior distribution effectively; 3) appropriately adjusting the degree of robustness based on varying scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. In addition, a new uncertainty set, i.e., constrained $D$-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. We further enhance the proposed framework by integrating various uncertainty sets and conducting a comprehensive theoretical analysis of the computational complexity associated with each uncertainty set. To expedite convergence speed, we also introduce ASPIRE-ADP, a method that can dynamically adjust the number of active workers. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity and communication complexity are also analyzed. Extensive empirical studies on real-world datasets validate that the proposed method excels not only in achieving fast convergence and robustness against data heterogeneity and malicious attacks but also in effectively managing the trade-off between robustness and performance.},
  archive      = {J_TMC},
  author       = {Yang Jiao and Kai Yang and Dongjin Song},
  doi          = {10.1109/TMC.2025.3602796},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated distributionally robust optimization with non-convex objectives: Algorithm and analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPBR: Privacy-preserving and byzantine-robust edge-assisted hierarchical federated learning in mobile networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3602911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-assisted Hierarchical Federated Learning (EHFL) accelerates global model training across mobile devices by hierarchically aggregating models. However, EHFL encounters critical challenges such as privacy risks for local and edge-level models, vulnerability to collusive Byzantine attacks, and issues with model diversity and heterogeneity due to Non-Independent and Identically Distributed (Non-IID) data. In this paper, we propose PPBR, a novel hybrid scheme that subtly integrates Condensed Local Differential Privacy (CLDP) and Packed Linearly Homomorphic Encryption (PLHE) to achieve strong privacy protection and resilience against various Byzantine attacks in Non-IID data scenarios. Specifically, PPBR clusters the sign statistics of local models and clips the norms of edge-level momenta to filter anomalous models and mitigate Byzantine faults while retaining diverse models coming from Non-IID data. To enhance privacy protection with acceptable accuracy loss, the sign tuples of local models are perturbed with CLDP guarantees, and the momenta of edge-level models are encrypted under PLHE. Meanwhile, PPBR enhances privacy in single-edge-server and single-cloud-server aggregations by using random perturbations, secret sharing, and PLHE. In addition to safeguarding privacy with accommodating abrupt dropouts of mobile devices and edge servers, the aggregations effectively mitigate the adverse effects of Non-IID data under advanced Byzantine attacks. Theoretical analysis and comprehensive experiments validate PPBR's strong privacy guarantees and resilience to various Byzantine attacks under Non-IID data.},
  archive      = {J_TMC},
  author       = {Yuanyuan He and Jie Zhang and Peng Yang and Zhe Sun and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3602911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PPBR: Privacy-preserving and byzantine-robust edge-assisted hierarchical federated learning in mobile networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multi-class novelty detection in structural vibrations with modified contrastive loss. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3603092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a framework for multi-class novelty detection using structural vibration signals. Structural vibration-based person identification is a promising soft-biometric approach with potential applications in elderly care and access control. However, current research faces two key challenges. The first challenge is the lack of large-scale datasets necessary for thorough evaluation in structural vibration gait recognition. To address this, we created a new dataset with recordings from fifty individuals. The second challenge lies in the limited exploration of deep learning methods for large-scale multi-class novelty detection in structural vibration data. To fill this gap, we propose the energy-shifted contrastive loss function, specifically designed for this task. Our results demonstrate that the proposed framework achieves 96.57% accuracy in multi-class classification. For novelty detection, it achieves an Receiver Operating Characteristic-Area Under the Curve (ROC-AUC) score of 89.15% for single footsteps, which improves to 93.83% with five consecutive footsteps.},
  archive      = {J_TMC},
  author       = {Mainak Chakraborty and Chandan and Bodhibrata Mukhopadhyay and Subrat Kar},
  doi          = {10.1109/TMC.2025.3603092},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep multi-class novelty detection in structural vibrations with modified contrastive loss},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the optimal path and DNN partition for collaborative edge inference. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3602966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Neural Networks (DNNs) have enabled a wide range of intelligent mobile applications, but their computational demands pose challenges for resource-constrained devices. Collaborative edge inference addresses this by partitioning a DNN inference task into several subtasks across multiple network nodes. However, most existing methods assume known network parameters or fixed processing paths. In this paper, we consider a more complex setting where network parameters are unknown and multiple paths are available. The goal is to learn both the optimal path and the DNN layer assignment along it, while accounting for security threats and path-switching costs. We first derive structural insights under full information to reduce the decision space. We then formulate the learning problem as an adversarial group linear bandit with switching costs, where rewards follow a hybrid stochastic-adversarial process. To solve this, we propose B-EXPUCB, a new algorithm that combines ideas from blocked EXP3 and LinUCB, and show that it achieves sublinear regret. Extensive simulations demonstrate that B-EXPUCB outperforms existing methods in collaborative edge inference.},
  archive      = {J_TMC},
  author       = {Yin Huang and Letian Zhang and Jie Xu},
  doi          = {10.1109/TMC.2025.3602966},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Learning the optimal path and DNN partition for collaborative edge inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed optimization of task offloading and resource allocation for mobile edge computing with multifactorial uncertainty. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3603064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for computation-intensive and lowlatency services grows, mobile edge computing (MEC) has been widely applied in smart devices to provide efficient and real-time assistance. However, most existing studies impose fixed assumptions and lack consideration for the uncertainty within MEC. This makes it difficult for these studies to reasonably offload tasks in complex and highly volatile scenarios. Therefore, we construct an MEC task offloading system considering multifactorial uncertainty (MECTOS-MU), which involves multiple devices and MEC servers (MSs). In MECTOS-MU, task offloading and resource allocation are jointly optimized while complying with the constraint on latency to minimize the energy consumption of all devices, which is an NP-hard problem. To address this issue, we propose a novel algorithm called distributed game offloading based on load balancing (DGOLB). This method integrates task offloading prioritization, static game theory, and load balancing to formulate efficient task offloading decisions and resource allocation schemes. Extensive simulation results demonstrate that DGOLB outperforms other baseline algorithms in terms of energy consumption, ratio of dropped tasks, and average task response time, especially in scenarios with a large number of devices.},
  archive      = {J_TMC},
  author       = {Bin Xu and Honggen Bian and Qiulan Cui and Xiaohui Yu and Jin Qi and Yimu Ji},
  doi          = {10.1109/TMC.2025.3603064},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed optimization of task offloading and resource allocation for mobile edge computing with multifactorial uncertainty},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AoI-aware incentive mechanism for UAV-assisted mobile crowdsensing: A contract-theoretic approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3604073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of mobile devices, mobile crowdsensing (MCS) has become a paradigm with broad application prospects. However, traditional MCS face numerous challenges, such as surges in network traffic and infrastructure failures. To address these issues, we leverage flexible and low-cost Unmanned Aerial Vehicles (UAVs) in the MCS framework. UAV-assisted crowdsensing (UCS) provides an innovative approach to data collection that effectively addresses problems such as insufficient network coverage and congestion. In the UCS framework, UAVs can serve not only as temporary base stations (BSs) but also participate in collecting data and processing tasks. Nevertheless, the lack of adequate incentive mechanisms may lead both UAVs and mobile users to be reluctant to participate in sensing tasks. Therefore, this paper aims to investigate hierarchical incentive mechanisms for UCS. Considering the freshness of the collected data and the benefits of the platform, we adopt the Age of Information (AoI) metric to measure the quality of data. To ensure AoI of data, we model the incentive mechanisms from both the UAV and user perspectives, and we formulate them as single-dimensional and multi-dimensional contract-based incentives under scenarios of information asymmetry. Furthermore, we derive the optimal contract scheme under the constraints of individual rationality and incentive compatibility. Finally, experimental results confirm the effectiveness of the proposed contract design and maximize the utility of the model owner.},
  archive      = {J_TMC},
  author       = {Yuran Guo and Ying Chen and Hongtao Li and Yuan Wu and Jiwei Huang},
  doi          = {10.1109/TMC.2025.3604073},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AoI-aware incentive mechanism for UAV-assisted mobile crowdsensing: A contract-theoretic approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning assisted detection of anomalous events with insufficient primary attribute data samples in MEC networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3604253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays IoT devices in Mobile Edge Computing (MEC) networks have been deployed in large-scale quantities to guarantee sensing data collection for anomalous event detection as full as possible even if some devices are in fault. Some techniques, such as clustering and dimensionality reduction, are adopted to eliminate redundant sensing data collection in this large-scale deployment. However, they not only have high computational complexity and easily cause the loss of information on the primary sensing attributes for detection, but also bring certain errors to the detection because of their low sensitivity to data processed. In addition, insufficient collection of primary attribute data samples often results from physical or human factors, and blind imputation of large-scale data gaps without basis may lead to greater irreparable losses. To address the above challenges, we first complete the selection of optimal primary attribute device collection and aggregation (PADCA) path based on minimum spanning tree, reducing data communication cost for redundant primary attributes collection. Then, we propose an anomalous impact correlation search strategy to quickly locate all MEC servers whose management regions have cascading anomalous event and help determine the transferable source MEC servers. Leveraging this, we use transfer learning to help detect anomalous events in the management regions of the MEC servers with insufficient primary attribute data samples, where a particle swarm optimization based back-propagation (PSO-BP) neural network model is used to infer the fusion weight of each primary attribute. Experimental results show that our method achieves higher detection performance in terms of detection time, energy consumption, accuracy, and receiver operating characteristic (ROC) curve compared to the benchmarks by at least 24%, 34%, 0.5 and 0.05.},
  archive      = {J_TMC},
  author       = {Jine Tang and Xiaotong Ma and Song Yang and Yong Xiang and Zhangbing Zhou},
  doi          = {10.1109/TMC.2025.3604253},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Transfer learning assisted detection of anomalous events with insufficient primary attribute data samples in MEC networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BatTera: Non-destructive lithium-ion battery coating measurement with terahertz. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3604081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrode coating measurement is a crucial task in practical lithium-ion battery systems, where the thickness and refractive index of the electrode coating directly reflect the battery's quality, energy density, capacity, and lifespan. In this paper, we propose the design, implementation, and evaluation of BatTera, the practical system for an accurate, high-resolution, non-destructive, and safe electrode coating measurement, with the ability to simultaneously measure coating thickness and refractive index. BatTera's contributions are twofold. Firstly, we build a comprehensive mathematical model that characterizes the arrival time of echo signals from both sides of the electrode coating by thoroughly analyzing the electrode structure based on “coating-foil-coating”. This model serves as a theoretical foundation guiding the measurement of coating thickness and refractive index. Secondly, we propose a series of effective signal-processing algorithms to address the practical challenges of double-side coating misalignment and deformation interference, thus adaptive improving the signal-to-noise ratio of Terahertz signals and pushing BatTera one big step closer to real adoptions. We implement BatTera based on the commercial Terahertz device QT-TO1000 and conduct extensive experiments using five types of cathode electrode samples in three different sizes, collected from one of the world's largest new energy battery manufacturers. The results show that BatTera achieves high measurement accuracy with a mean average error of 6.106$\upmu$m for thickness and 0.230 for refractive index.},
  archive      = {J_TMC},
  author       = {Long Tan and Xu Chen and Xiuzhen Guo and Xinghua Guo and Yuanchao Shu and Jiming Chen},
  doi          = {10.1109/TMC.2025.3604081},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BatTera: Non-destructive lithium-ion battery coating measurement with terahertz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling reliable and anonymous data collection for fog-assisted mobile crowdsensing with malicious user detection. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3602659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid developments of mobile devices and fog computing have facilitated the data collection paradigm of fog-assisted mobile crowdsensing, providing great convenience for individuals with limited resources to collect large-scale data. However, the openness of crowdsensing network and the untrusted behaviors of some task participants raise concerns regarding participants' privacy and data reliability. Previous works mostly focus on preserving the privacy of task participants and often overlook the issue of data reliability in the presence of dishonest participants. In this paper, we propose a new data collection scheme tailored for fog-assisted mobile crowdsensing. It enables the cloud to detect invalid sensing data in the ciphertext domain, simultaneously ensuring data confidentiality and reliability. Additionally, our scheme is designed to protect the anonymity of honest task participants while guaranteeing the traceability of dishonest participant once invalid data are detected. Formal analysis is provided to prove the correctness and security of our scheme. Furthermore, we implement our scheme to evaluate its performance, and the experimental results demonstrate that it can achieve the aforementioned security properties with modest performance overhead.},
  archive      = {J_TMC},
  author       = {Mingyang Song and Zhongyun Hua and Yifeng Zheng and Rushi Lan and Qing Liao and Guoai Xu},
  doi          = {10.1109/TMC.2025.3602659},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling reliable and anonymous data collection for fog-assisted mobile crowdsensing with malicious user detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient sensor fusion at the edge via system-wide dynamic gated neural networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation mobile systems will support multiple AI-based applications, each leveraging heterogeneous sensors and data sources through deep neural network (DNN) architectures collaboratively executed within the network. In this context, to minimize the cost of the AI inference task subject to requirements on latency, quality, and – crucially – reliability of the inference process, it is vital to optimize (i) the set of sensors/data sources and (ii) the DNN architecture, (iii) the network nodes executing sections of the DNN, and (iv) the resources to use. To achieve these goals, we leverage dynamic gated neural networks with branches, and propose a novel algorithmic strategy called Quantile-constrained Inference (QIC), based upon quantile-Constrained policy optimization. QIC makes joint, high-quality, swift decisions on all the above aspects of the system, with the aim to minimize inference energy cost. We remark that this is the first contribution connecting gated dynamic DNNs with infrastructure-level decision making. We evaluate QIC using a dynamic gated DNN with stems and branches for optimal sensor fusion and inference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera data, and real-world wireless measurements. Our results confirm that QIC closely matches the optimum and outperforms existing approaches in reducing energy consumption (compute, communication, and total) and application requirements failure by over 70%.},
  archive      = {J_TMC},
  author       = {Chetna Singhal and Yashuo Wu and Francesco Malandrino and Sharon G. L. Contreras and Marco Levorato and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3586882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource-efficient sensor fusion at the edge via system-wide dynamic gated neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and trustworthy block propagation for blockchain-enabled mobile embodied AI networks: A graph resfusion approach. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3587006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By synergistically integrating mobile networks and embodied artificial intelligence (AI), mobile embodied AI networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of messages or vulnerability to malicious tampering, potentially causing severe accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the validator trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.},
  archive      = {J_TMC},
  author       = {Jiawen Kang and Jiana Liao and Runquan Gao and Jinbo Wen and Huawei Huang and Maomao Zhang and Changyan Yi and Tao Zhang and Dusit Niyato and Zibin Zheng},
  doi          = {10.1109/TMC.2025.3587006},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and trustworthy block propagation for blockchain-enabled mobile embodied AI networks: A graph resfusion approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MultiScanner: Enabling simultaneous detection of multiple liquids with mmWave radar based on a composite reflection model. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional liquid detection approaches are often time-intensive and invasive, typically requiring the opening of containers for examination. While recent initiatives have proposed several innovative solutions, including camera-based and vibration sensor-based techniques, these approaches still face limitations in terms of convenience. The development of radio frequency (RF) technology, particularly millimeter-wave (mmWave) radar, offers a promising solution for non-invasive and contactless liquid detection. In particular, during the past few years, a number of radar-based sensing systems have been developed to detect or identify liquids. However, little work has been done on the simultaneous detection of multiple liquids. To fill this gap, we design a novel composite reflection model, which overcomes the detection challenges due to composite interference and environmental reflections, by utilizing the consistency and uniqueness of the reflection signals from multiple liquid targets. Based on the proposed model, we develop a system named MultiScanner, which is able to detect different types of liquids in multi-target scenarios, exhibiting high location independence without the need for extensive data training. Extensive experiments validate the effectiveness of MultiScanner, achieving up to 95.91% accuracy in detecting 10 hazardous-normal liquid combinations in 2-target scenarios. Moreover, even in more complex 5-target scenarios, an detection accuracy of 86.49% can be obtained. To the best of our knowledge, this is the first study that uses RF signals for multi-liquid detection.},
  archive      = {J_TMC},
  author       = {Yifan Guo and Zhu Wang and Zhihui Ren and Wei Xu and Yangqian Lei and Qian Qin and Zhuo Sun and Chao Chen and Bin Guo and Zhiwen Yu and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3587079},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MultiScanner: Enabling simultaneous detection of multiple liquids with mmWave radar based on a composite reflection model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EagleEye: Balancing latency, accuracy, and power on edge-assisted UAVs for urban crowd surveillance. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) equipped with cameras provide a promising way for large-scale urban crowd surveillance due to their convenient deployment and flexible mobility. However, UAVs are constrained by limited power and computing resources, hindering existing work in achieving efficient UAV-based crowd surveillance, i.e., long flight time, high accuracy, and low latency. To this end, we propose EagleEye, a low-power, high-precision, and low-latency crowd surveillance system empowered by edge-assisted UAVs. It leverages lightweight devices on UAV-side to compress video information edge-independently, then transmits key video information instead of raw high-definition videos. Furthermore, we propose a novel spatio-temporal Compressive-Sensing-based video feature compression algorithm to achieve efficient, low-latency video compression. It can reduce video volumes greatly while minimizing the loss of crowd-surveillance-related information. Specifically, inspired by the Compressive Sensing theory, we compress the video content from both the temporal and spatial perspectives by accounting for inter-frame redundancy and intra-frame information saliency, respectively. Finally, we implement a prototype system and conduct extensive experiments based on four large-scale datasets with over 20,000 frames. The experimental results demonstrate that EagleEye can reduce transmission latency by 31.4% only with no more than 4% of accuracy loss in urban crowd detection.},
  archive      = {J_TMC},
  author       = {Chaocan Xiang and Zhenghan Li and Qianyuan Zhang and Xuangou Wu and Xiao Zheng and Yulan Guo},
  doi          = {10.1109/TMC.2025.3586860},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EagleEye: Balancing latency, accuracy, and power on edge-assisted UAVs for urban crowd surveillance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRaMirror: Illuminating blind spots in urban LPWAN with reflective smart surfaces. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3587057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of low-power wide-area networks (LPWAN) in urban environments faces a critical challenge with signal blockage caused by dense obstacles like buildings, resulting in blind spots where end nodes have difficulty reaching the gateway. This paper proposes LoRaMirror, a reflective smart surface design, to essentially eliminate these blind spots and improve overall communication in urban LoRaWAN. LoRaMirror is different from existing smart surface designs, as it addresses unprecedented challenges posed by LPWAN's unique application scenario of extremely long communication distances, extremely low data transmission rates, and extremely wide coverage. LoRaMirror is prototyped with a 16-antenna multi-layer array and the experimental results show significant performance gains in real world practice.},
  archive      = {J_TMC},
  author       = {Songfan Li and Yanbo Zhang and Jansen Christian Liando and Li Lu and Mo Li},
  doi          = {10.1109/TMC.2025.3587057},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LoRaMirror: Illuminating blind spots in urban LPWAN with reflective smart surfaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When good becomes evil: Exploring crosstalk attack surfaces on multi-port USB chargers. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-port chargers, designed to simultaneously charge multiple mobile devices such as smartphones, have gained significant popularity, with millions of units sold in recent years. However, this multi-device charging feature introduces security and privacy risks. If not properly designed and implemented, these chargers can enable communication between connected devices because they are inherently interconnected, which leads to crosstalk voltage leakages. Despite their widespread use, these risks have not been thoroughly investigated. We have identified novel attack surfaces in the circuit design of multi-port chargers that allow an adversary who shares the multi-port charger with the target victim in close proximity to exploit one port to (i) recognize fine-grained user activities of other devices being charged, (ii) eavesdrop on secret audio transmission from USB-C audio pins, and (iii) inject malicious audio commands into built-in voice assistants of charging devices (e.g., Siri, Google Assistant). In this paper, we design and implement XPORTHEFT, a novel system to analyze and demonstrate the uncovered security and privacy threats in multi-port chargers. Specifically, it leverages changes in voltage signals in one neighbor port to monitor voltage changes in the charging port induced by user activities in various user interfaces, such as recognizing running apps and detecting keystrokes. Moreover, XPORTHEFT can also achieve audio transmission eavesdropping and launch inaudible audio injection attacks from the neighbor port to the charging mobile device via the USB-C interface. We extensively evaluate the effectiveness of XPORTHEFT using five commercial multi-port chargers and five mobile devices. The evaluation results show its high effectiveness in recognizing the launch of 20 mobile apps (88.7%) and revealing unlocking passcodes (98.8%), as well as eavesdropping on the audios of numeric digits (97.1%) and alphabetic characters (98.0%). Furthermore, XPORTHEFT achieves 100% success rates in inaudible audio injection attacks on three commercial voice assistants. In addition, our study also shows that XPORTHEFT is resilient to various impact factors and presents the potential to attack multiple victims.},
  archive      = {J_TMC},
  author       = {Tao Ni and Zehua Sun and Yongliang Chen and Yihe Zhou and Jiayimei Wang and Weitao Xu and Qingchuan Zhao and Cong Wang},
  doi          = {10.1109/TMC.2025.3587292},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {When good becomes evil: Exploring crosstalk attack surfaces on multi-port USB chargers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative diffusion model-assisted efficient fingerprinting for in-orchard localization. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise robot localization at the tree level is essential for smart agriculture applications such as precision disease management and targeted nutrient distribution. Existing methods fail to achieve the required accuracy. We propose OrchLoc, a fingerprinting-based localization solution that achieves treelevel precision using a single Long Range (LoRa) gateway. Our approach utilizes channel state information (CSI) across eight channels as a localization fingerprint. To minimize labor-intensive site surveys for fingerprint database construction and maintenance, we develop a CSI generative model (CGM) that learns the relationship between CSI vectors and their corresponding locations. The CGM is fine-tuned using CSI data from static agricultural LoRa sensor nodes, enabling continuous fingerprint database updates. Extensive experiments in two orchards demonstrate that OrchLoc effectively achieves accurate tree-level localization with minimal overhead, improving robot navigation},
  archive      = {J_TMC},
  author       = {Kang Yang and Yuning Chen and Wan Du},
  doi          = {10.1109/TMC.2025.3587206},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Generative diffusion model-assisted efficient fingerprinting for in-orchard localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-rate uncoordinated concurrent random access in underwater acoustic networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncoordinated random-access protocols are well-suited for underwater acoustic (UWA) networks due to their simplicity and low overhead. However, their performance is hindered by severe collisions and the challenging characteristics of UWA channels such as rich multipath and Doppler effect. Existing UWA physical layer waveforms struggle to resolve collisions while maintaining high data rates. This paper introduces ZCMod, a high-rate waveform allowing uncoordinated concurrent random access in UWA networks. ZCMod employs a Zadoff–Chu (ZC) sequence-based modulation that assigns unique ZC sequences to users to minimize inter-user interference and encodes multiple bits through cyclic shifts of the sequences to improve data rates. ZCMod further addresses the unique challenges of UWA channels via two new designs: 1) a shape-based demodulation approach that estimates the data-induced shift of channel response shape between the preamble and data symbols to handle rich multipath, and 2) an auxiliary modulation approach that modulates each data symbol with two ZC sequences, one for extracting current channel response shape and the other for data modulation, to handle the fast time-varying channel. Experimental results in a lake and a swimming pool and extensive simulation results show that a) ZCMod achieves around 100% higher throughput compared with the state-of-the-art (SOTA) approaches in quasi-static channels, and b) ZCMod maintains comparable throughput in fast time-varying channels as in quasi-static conditions, where the SOTA approaches experience significant degradation.},
  archive      = {J_TMC},
  author       = {Enqi Zhang and Lei Liang and Lizhao You and Zhaorui Wang and Deqing Wang and Liqun Fu},
  doi          = {10.1109/TMC.2025.3586911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {High-rate uncoordinated concurrent random access in underwater acoustic networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous federated learning driven by multi-knowledge distillation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a fully heterogeneous federated learning environment, the client has significant differences in model structure and local data distribution (Non-IID), and the joint learning of the client model is blocked due to the limited communication content available for interaction in a fully heterogeneous scenario. In this context, the global knowledge constructed by the server through the simple aggregation of the client logits is essentially a fuzzy representation containing a lot of noise and information loss, which is difficult to effectively guide the client model update. To solve these problems, this paper proposes a heterogeneous federated learning framework (FedMkd) based on multi-knowledge distillation fusion to cope with multiple challenges in heterogeneous environments. The FedMkd framework uses a class-grained logits interaction architecture (CLIA) and introduces an efficient knowledge sharing mechanism. It innovatively integrates two knowledge distillation methods: 1) Temperature-Adaptive Knowledge Distillation (TAKD), which provides differentiated temperatures for teacher and student models by adaptively adjusting the distillation temperature, maximizing knowledge transfer between them; 2) Class-related Knowledge Distillation (CRKD), which introduces batch-level sample correlation loss to reduce over-reliance on specific samples or classes and improve the model's understanding of overall data features. We conducted a large number of experiments on four public data sets. The results show that in a variety of data and model heterogeneous scenarios, FedMkd still performs better than the comparison method when the communication overhead is reduced by more than one order of magnitude.},
  archive      = {J_TMC},
  author       = {Bin Xu and Longgang Cheng and Qing Wen and Zhensheng Zou and Xiaoxuan Hu and Zhenjiang Dong and Jin Qi},
  doi          = {10.1109/TMC.2025.3586921},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Heterogeneous federated learning driven by multi-knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-timescale service caching, computing, and communication optimization for low-latency high-reliability edge-cloud networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the mixed-timescale joint optimization of service caching, computing, and communication for edge-cloud networks. By jointly considering the caching update, user association, task offloading, and computing and communication resource allocation in different timescales with queueing dynamics, we first formulate a long-term optimization problem whose goal is to minimize the network energy consumption while stabilizing the edge-cloud network. By exploiting the Lyapunov optimization techniques, we decompose the overall problem into large-, medium-, and small-timescale problems, and their individual solution approaches are provided. To further improve the latency and reliability, an end-to-end latency measure is proposed along with the use of extreme value theory to derive an enhancement approach. Performance analysis of our approaches is provided and computer simulations are conducted to evaluate the proposed approaches. Results show that our approaches have better latency performance as compared to reference schemes. Furthermore, results also show that our proposed enhancement approach can effectively improve the latency and reliability.},
  archive      = {J_TMC},
  author       = {Chang-Lin Ye and Ming-Chun Lee and Chen-Yuan Wu},
  doi          = {10.1109/TMC.2025.3587713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixed-timescale service caching, computing, and communication optimization for low-latency high-reliability edge-cloud networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-inspired decentralized satellite learning in space computing power networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3587796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring. However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data. To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on-board data processing. Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks. To tackle this issue, we propose to employ spiking neural networks (SNNs) for on-board data processing, which is supported by the neuromorphic computing architecture. The extreme sparsity in its computation enables a high energy efficiency. Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum. We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed. We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance. Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks.},
  archive      = {J_TMC},
  author       = {Peng Yang and Ting Wang and Haibin Cai and Yuanming Shi and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3587796},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Brain-inspired decentralized satellite learning in space computing power networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalizable prompt-based prototypical framework for CSI-based few-shot and cross-domain activity recognition. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3587702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensing systems for human activity recognition (HAR) have made great strides in recent years. However, current HAR models face challenges in generalization due to a limited number of training samples (i.e., few-shot problem) and pattern differences of the collected channel state information (CSI) data across diverse domains (i.e., cross-domain problem). To address the above problems, in this paper, we propose a prompt-based prototypical framework called Wi-Prompt. Our Wi-Prompt framework consists of the following three modules: Prompt generation module, prototypical representation module, and activity recognition module, which works as follows. Prompt generation module is developed to extract prior knowledge from samples in the source domains, providing insightful guidance for establishing class prototypes in the target domains. Prototypical representation module effectively captures the most representative prototype vector of each class with a temporal convolutional network (TCN)-attention model. Activity recognition module determines the activity class of new sample by comparing the Euclidean distance between its corresponding prototype vector and the prototype vector of each class. The greatest advantage of Wi-Prompt is its utilization of prompt-based prototype representation, which eliminates the need for prior domain-specific knowledge about the original CSI samples, making it highly adaptable to a wide variety of CSI datasets collected across different domains. Extensive experiment results based on real-world traces show that our proposed Wi-Prompt outperforms state-of-the-art models under various cross-domain scenarios.},
  archive      = {J_TMC},
  author       = {Yunming Zhao and Wei Gong and Minghui Liwang and Li Li and Baoxian Zhang and Cheng Li},
  doi          = {10.1109/TMC.2025.3587702},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A generalizable prompt-based prototypical framework for CSI-based few-shot and cross-domain activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing QoE in collaborative edge systems with feedback diffusion generative scheduling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3587744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing is a promising approach for delivering low-delay services to computation-intensive Internet of Things applications. Deep Reinforcement Learning (DRL) has become an effective way to solve task scheduling decisions in edge systems due to its adaptive learning ability to interact with the environment. However, current DRL-based task scheduling methods still face several challenges, such as limited exploration, sample inefficiency, and performance instability, which can lead to degraded user Quality of Experience (QoE). To address these challenges, we observe that diffusion models, famous for their performance in image generation, exhibit strong exploration, data efficiency, and performance stability. This inspires us to propose FDEdge, a novel feedback diffusion generative scheduling method for enhancing user QoE in collaborative edge systems. We first design an innovative Feedback Diffusion (FDN) model by leveraging historical action probability information during the denoising process. We then incorporate the FDN model into DRL, forming an effective and efficient framework for task scheduling in collaborative edge systems. We also present a probability derivation to ensure the FDEdge's rationality. Extensive experimental results demonstrate that our FDEdge method significantly reduces service delays by $45.42\%$ to $87.57\%$ and speeds up training episode durations by $2.5\times$ times for a higher QoE than state-of-the-art methods. We release our open-source code at https://github.com/ChangfuXu/FDEdge.},
  archive      = {J_TMC},
  author       = {Changfu Xu and Jianxiong Guo and Yuzhu Liang and Haodong Zou and Jiandian Zeng and Haipeng Dai and Weijia Jia and Jiannong Cao and Tian Wang},
  doi          = {10.1109/TMC.2025.3587744},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing QoE in collaborative edge systems with feedback diffusion generative scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GATO: Global transmission optimization for SAGIN-assisted IoRT data collection. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 6G networks, the increasing demand for global coverage has catalyzed a significant expansion in the deployment of the Internet of Remote Things (IoRT). Supported by Space-Air-Ground Integrated Networks (SAGIN), IoRT can efficiently transmit data from areas lacking infrastructure. In this paper, we explore a key yet challenging issue faced by SAGIN: can we exploit the mobility and cooperation of SAGIN heterogeneous nodes to enhance overall transmission performance in all-weather? We approach this issue in three steps. Firstly, to ensure the all-weather transmission, we propose a layered SAGIN transmission design that combines the advantages of radio-frequency (RF) and free-space-optics (FSO) transmissions. Secondly, we formulate an overall transmission performance optimization problem. We aim to maximize the total upload data amount of SAGIN by jointly optimizing the satellite selection, the trajectories of high-altitude-platforms (HAPs) and unmanned aerial vehicles (UAVs), and the UAVs' transmit power. However, the formulated optimization problem falls into a mixed-integer nonlinear programming (MINLP), which is challenging to address. Thirdly, to solve this problem, we propose a global transmission optimization (GATO) strategy by employing block coordinate descent and successive convex approximation technologies. Finally, extensive experimental results demonstrate that the proposed strategy can significantly improve SAGIN's overall transmission performance.},
  archive      = {J_TMC},
  author       = {Yanbo Fan and Yuanguo Bi and Yufei Liu and Dusit Niyato and Liang Zhao and Qiang He and Ammar Hawbani},
  doi          = {10.1109/TMC.2025.3587634},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GATO: Global transmission optimization for SAGIN-assisted IoRT data collection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient cooperative mechanism for distributed multi-agent traffic signal control. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3587257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control (TSC) provides a cost-effective approach to alleviating urban traffic congestion without requiring modifications to physical road infrastructure. As a classic multi-agent coordination problem, TSC can be optimized with multi-agent reinforcement learning (MARL). However, understanding the cooperative strategies among traffic signals is challenging due to the spatiotemporal dynamics inherent in distributed traffic signal controllers. Previous MARL approaches for TSC often rely on imprecise and redundant cooperation mechanisms, leading to excessive communication costs and inefficient training. In this study, we propose a novel dynamic and controllable cooperation MARL algorithm (DCoo) to optimize traffic signals coordination for alleviating traffic congestion. DCoo introduces a collaboration scheduling module (CSM) to obtain spatiotemporal collaborative information and incorporates a cooperative hyper-decision network (HDN) to optimize decisions by leveraging the processed information. In particular, the CSM first captures crucial temporal information for cooperation by analyzing the relationships between historical and current observations of intersections. CSM then frames the selection of spatial information as a binary classification problem to effectively expand the scope of collaboration among intersections. Additionally, HDN integrates an attention mechanism and permutation invariance to enhance the scalability of DCoo. We conduct comparative experiments on three real-world traffic datasets, and the results demonstrate that DCoo outperforms comparable algorithms in both training and testing phases.},
  archive      = {J_TMC},
  author       = {Lulu Li and Yafei Li and Shaohui Zhang and Yuanyuan Jin and Shuo He and Ke Wang and Mingliang Xu},
  doi          = {10.1109/TMC.2025.3587257},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient cooperative mechanism for distributed multi-agent traffic signal control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FREAUTH$^{+}$: A robust frequency feature-based device authentication mechanism for magnetic wireless power transfer system. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3587937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device authentication is critical for preventing unauthorized access and ensuring streamlined operation in magnetic wireless power transfer (WPT) systems. However, existing authentication techniques often suffer from security vulnerabilities and lack compatibility with low-cost receiver devices, limiting their practical applications. In this paper, we introduce FREAUTH$^{+}$ (FREquency feature-based AUTHentication), a novel authentication mechanism tailored specifically for magnetic-based WPT systems. Our approach involves acquiring impedance solely at the transmitter side without requiring active cooperation from the receiver. Using a dual-frequency interleaved subtraction technique, we eliminate ideal receiver impedance to reveal subtle frequency characteristics. These frequency features are then normalized to mitigate the effects of device positioning, and a temperature compensation method is applied to account for thermal variations. This process enables the generation of a robust hardware fingerprint based on frequency characteristics. For authentication, we employ a discrete Fréchet distance-based algorithm for effective fingerprint matching. We also develop a prototype of FREAUTH$^{+}$ and conduct extensive experiments. The results demonstrate its reliability, achieving 96.23% authentication performance across more than 60 devices with an average response time of 1.6 seconds. Furthermore, FREAUTH$^{+}$ exhibits excellent stability, effectively mitigating interference caused by device payload, device positioning, and temperature variations.},
  archive      = {J_TMC},
  author       = {Shenyao Jiang and Hao Zhou and Wangqiu Zhou and Xinyu Wang and Zhenjiang Li and Yusheng Ji},
  doi          = {10.1109/TMC.2025.3587937},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FREAUTH$^{+}$: A robust frequency feature-based device authentication mechanism for magnetic wireless power transfer system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating tail latency for on-device inference with load-balanced heterogeneous models. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3588430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serving machine learning models on edge, mobile, and embedded devices places stringent requirements on inference latency. From operating a real enterprise service, we observed that even a fully optimized model could lead to severe violations of latency objectives when the load surges. A straightforward and mature approach is to auto-scale multiple models to balance the load. However, unlike cloud clusters, edge or mobile devices usually cannot afford to deploy multiple model replicas. Therefore, in this paper, we explore a new idea: in addition to the original model, we deploy one (or more) heterogeneous model(s) with much smaller resource overhead on the device, and perform load balancing among all models. We overcame the technical challenges posed by performance dynamics and developed InferRouter based on queuing theory. We implement and evaluate InferRouter on three real on-device inference systems, covering mobile sensing, video analytics, and natural language processing applications. Experimental results show that compared with strong baselines, InferRouter can decrease 85.2% P99 latency (5.8x faster) and improve 5.9% accuracy on the mobile workload. For a traffic video analytics task, InferRouter achieves 55.1% higher accuracy with zero deadline misses. InferRouter also shows its advantages in saving resources compared with auto-scaling and offloading approaches.},
  archive      = {J_TMC},
  author       = {Mu Yuan and Lan Zhang and Di Duan and Liekang Zeng and Miao-Hui Song and Zichong Li and Guoliang Xing and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3588430},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating tail latency for on-device inference with load-balanced heterogeneous models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized DP-DFL: Towards differentially private decentralized federated learning via randomized model interaction. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional federated learning (FL) frameworks rely on a central server for model coordination among distributed mobile terminals (MTs). The centralization faces two critical challenges, i.e., single point of failure and potential privacy leakage. Differentially private decentralized FL (DP-DFL) has been proposed to address these challenges, wherein the MTs exchange models in a decentralized manner and maintain the differential privacy (DP) guarantee by adding noise to local models before model interaction. However, existing DP-DFL frameworks confront difficulty in achieving the expected privacy and convergence performance, simultaneously. To address this issue, we propose a novel DP-DFL framework (called randomized DP-DFL) that employs a randomized model interaction scheme to lower the model exposure frequency and hence reduce privacy budget consumption. Specifically, the scheme includes two sequential steps, i.e., randomized terminal assignment and randomized model transmission. In Step 1), the model interaction phase of DFL is further divided into several sequential substages. MTs are randomly assigned to each sub-stage. In Step 2), each MT sequentially transmits either a model previously received from its neighbors or its own local model according to the assigned sub-stage order. The proposed scheme enhances the MTs' privacy of DFL since the exposure probabilities of the MTs' local models are significantly reduced via these two randomized steps. Besides, we theoretically analyze the convergence and privacy performance of randomized DP-DFL. In particular, properly tuning the number of sub-stages in randomized DP-DFL can achieve an optimal balance between privacy and convergence. Experimental results show that randomized DP-DFL consistently outperforms traditional frameworks. Compared with baselines, randomized DP-DFL reduces 40.9% privacy loss under the same target accuracy while improving 9.5% learning accuracy under the same privacy loss on EMNIST and CIFAR-10, respectively},
  archive      = {J_TMC},
  author       = {Weihao Zhu and Long Shi and Kang Wei and Yipeng Zhou and Zhe Wang and Zehui Xiong and Jun Li},
  doi          = {10.1109/TMC.2025.3588537},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Randomized DP-DFL: Towards differentially private decentralized federated learning via randomized model interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localization in 5G and beyond: A multi-objective approach for accuracy, latency, and resilience. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3588712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of localization capabilities within the cellular architecture through dedicated 5G network functions has notably enhanced cellular positioning accuracy and enabled new location-based services. However, this architectural shift requires placing measurement acquisition and computation at the network edge and core, resulting in distributed computational resources and increased latency and security risks. As a result, minimizing latency and ensuring resilience against security threats, in addition to achieving high accuracy, become critical performance indicators in location-based services. This paper examines both 3GPP-standardized and O-RAN-based 5G architectures, detailing the key functions, interfaces, and parameters influencing the localization process, from measurement acquisition to position estimation. We define performance indicators for evaluating localization services and develop a system model that quantifies costs related to latency, accuracy, computation, and resilience against security threats. By jointly considering these factors, we formulate a multi-objective optimization problem that guides the selection of an optimal system configuration to simultaneously satisfy multiple localization requirements. We validate our approach through a case study of an end-to-end 5G system using both simulations and experimental data. Specifically, we evaluate various algorithms and implementations across standardized channels and scenarios. Furthermore, we conduct experimental measurements using Software-Defined Radios (SDRs) and open-source 5G platforms to assess operational latency with commercial-off-the-shelf (COTS) devices.},
  archive      = {J_TMC},
  author       = {Luca Petrucci and Samuele Zanini and Ivan Palamà and Nicola Blefari Melazzi and Stefania Bartoletti},
  doi          = {10.1109/TMC.2025.3588712},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Localization in 5G and beyond: A multi-objective approach for accuracy, latency, and resilience},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring upper-6GHz and mmWave in urban 5G networks: A direct on-field comparison. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3587904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for mobile bandwidth is driving 5G networks toward the use of high-frequency spectrum, particularly the upper-6 GHz and mmWave bands. While these bands offer vast bandwidth potential, their propagation characteristics raise critical deployment challenges. This paper presents the first direct, on-field comparative evaluation of 5G standalone (SA) macro-cell deployments operating in these two bands, conducted in Milan, Italy. We show that the upper-6 GHz band can deliver wide-area urban coverage (up to 600 meters) with stable gigabit-level downlink throughput, even in (NLoS) scenarios. mmWave, traditionally deemed unsuitable for NLoS, exhibits strong performance via urban reflections, achieving up to 1.3 Gbps in downlink and 250 Mbps in uplink. Furthermore, outdoor-to-indoor connectivity at mmWave frequencies proves viable through glass facades, challenging pessimistic assumptions about penetration losses. These findings, derived from synchronized deployments and extensive measurements, provide new insights into the complementary roles of these bands and the practical feasibility of their integration into future 5G networks.},
  archive      = {J_TMC},
  author       = {Marcello Morini and Eugenio Moro and Chiara Rubaltelli and Ilario Filippini and Antonio Capone and Danilo De Donno},
  doi          = {10.1109/TMC.2025.3587904},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring upper-6GHz and mmWave in urban 5G networks: A direct on-field comparison},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrowdNet: Adaptive collaborative inference for dynamic mobile intelligent service. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) such as convolutional networks and Transformers are increasingly deployed to provide intelligent service. However, enabling large-scale DNNs in an infrastructure-less mobile crowd encounters the challenges of high resource consumption, poor performance, and low service availability. This paper proposes CrowdNet, a novel device-todevice (D2D) collaborative inference framework for dynamic mobile environments. CrowdNet introduces a CellNet architecture as its core component, designed as lightweight DNNs ideal for deployment and operation on resource-constrained mobile devices. The CellNets can perform inference tasks either independently or collaboratively to ensure robustness against network disruptions. A topology expansion method is utilized to create an inference flow from the physical communication topology, enabling the distributed operation of inference tasks. To handle the dynamic participation of mobile devices, CrowdNet employs fine-tuning adaptation for flexible assembly and collaborative inference. A reinforcement learning (RL)-based approach is introduced to optimize inference topology. Trained with a multiobjective optimization strategy, CrowdNet can enhance overall performance while maintaining individual CellNet functionality. Extensive experiments based on mobile network testbed and realworld datasets validate the effectiveness of CrowdNet on various intelligent tasks, exhibiting remarkable performance gains and robustness compared to state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Gong Chen and Wenzhong Li and Yuchu Fang and Yi Zhang and Sanglu Lu},
  doi          = {10.1109/TMC.2025.3588100},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrowdNet: Adaptive collaborative inference for dynamic mobile intelligent service},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling real-time video detection with adaptive and distributed scheduling in mobile edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time video detection is essential for many mobile visual applications, which brings the heavy computational burden of deep neural networks. Mobile edge computing offers a promising solution by deploying computational resources near mobile devices. However, achieving efficient video detection on mobile devices requires addressing challenges such as different performance requirements, diverse computing and network conditions, and system dynamics. We propose a realtime video detection framework in mobile edge computing, where multiple video streams from mobile devices are processed while balancing key performance metrics with consideration of grouping. A joint optimization problem of task scheduling, model selection, and resource provisioning is formulated for the system, where decisions are made on two timescales. To this end, we propose a window controller to unify decision-making at the time-slot level. We design an online scheduling algorithm based on multi-agent deep reinforcement learning to enable adaptive and distributed scheduling, while a masking-enhanced attention mechanism enables efficient explicit information exchange between mobile devices. Experimental evaluations across different numbers of mobile devices demonstrate that, in terms of average reward, the proposed algorithm outperforms local processing by 14.600%, fixed offloading by 10.007%, and four learning-based scheduling baselines by an average of 2.267%.},
  archive      = {J_TMC},
  author       = {Zhicheng Liu and Yilan Wang and Yunfeng Zhao and Chao Qiu and Cheng Zhang and Xiaofei Wang and Mianxiong Dong},
  doi          = {10.1109/TMC.2025.3588142},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling real-time video detection with adaptive and distributed scheduling in mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel spatial-temporal learning method for enhancing generalization in adaptive video streaming. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive video streaming has become a fundamental technology for video delivery. With the rise of deep reinforcement learning (DRL), streaming vendors are increasingly adopting DRL-driven adaptive bitrate (ABR) algorithms. In real-world deployments, most ABR approaches are developed with the aim of maintaining good performance across a wide variety of network environments. However, contrary to this expectation, our empirical findings show that even when trained on extensive real-world network trace data, these DRL-based ABR algorithms achieve only 43.1% to 48.9% of Quality-of-Experience (QoE) under highly diverse network conditions, which falls significantly short of the 100% optimum. We termed this problem as “ABR Under-Generalization”. To overcome this problem, we introduce BETA – a novel DRL-based ABR framework that incorporates both spatial and temporal learning mechanisms: 1) Spatially, BETA features a detector that flags the network conditions likely to cause poor performance, then trains specialized ABR models tailored for those conditions; 2) Temporally, BETA enhances its learning by incorporating multi-step decision experiences at each training epoch, enabling the trained model to account for long-term environmental dynamics. Comprehensive evaluations show that BETA outperforms state-of-the-art ABR algorithms, yielding average QoE gains of 19.4% to 50.9%, and achieving improvements of up to 244.1% under severely fluctuating network conditions.},
  archive      = {J_TMC},
  author       = {Guanghui Zhang and Ziming Wang and Huaren Wei and Mengbai Xiao and Hui Yuan and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2025.3588135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel spatial-temporal learning method for enhancing generalization in adaptive video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeVA: An edge-assisted video analytics framework for depth estimation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3588864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-assisted video analytics frameworks, which offload vision-based tasks to edge servers, offer a promising approach to enhance accuracy while minimizing network resource overhead. However, these frameworks often overlook depth estimation, a critical task for applications like augmented reality and intelligent surveillance. Depth estimation, which calculates the distance between objects and the camera, generates depth images with unique characteristics, making existing approaches impractical or inefficient for video analytics in this context. In this work, we present DeVA, an edge-assisted video analytics framework for depth estimation that ensures accuracy with minimal network resource overhead. We examine the impact of various video analytics configurations, including resolution and quantization parameter (QP), on accuracy. Additionally, we analyze the region of interest (RoI) for depth estimation and propose methods for tracking RoI areas locally on the device. DeVA features an adaptive video encoding mechanism that dynamically adjusts the resolution for offloaded video and optimizes QPs for RoI and non-RoI areas. We implement DeVA and evaluate its performance using public video datasets. The results show that DeVA reduces 57.12% of the bandwidth overhead while keeping depth estimation errors within acceptable limits, demonstrating a great balance between accuracy and network resource usage.},
  archive      = {J_TMC},
  author       = {Shutong Chen and Jingwen Yin and Ruichao Zhong and Fangming Liu},
  doi          = {10.1109/TMC.2025.3588864},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeVA: An edge-assisted video analytics framework for depth estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On joint covert and secure communications in D2D-enabled cellular systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3589011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the joint covert and secure communications in a device-to-device (D2D)-enabled cellular system (DCS) consisting of a base station BS, an eavesdropper Eve, and two user equipments UE and UR. To conduct secure communications with UE against Eve, BS works either under the cellular mode using direct transmission or under the D2D mode replying through UR, while UR is greedy since it opportunistically transmits its own covert message to UE against the detection from BS. To understand the fundamental performance of secrecy rate and covert rate in DCS, we first develop theoretical models to depict the detection probability/secrecy rate of BS and covert rate of UR under different modes (i.e., underlay, overlay, or cellular). Based on these models, we further explore the secrecy rate maximization (SRM) for BS subject to the constraints of detection probability at BS and transmit power at both BS and UR, as well as the covert rate maximization (CRM) for UR subject to the constraints of covertness requirement and covert transmit power. Finally, we employ the Newton-based searching method to solve the SRM/CRM problems and illustrate via numerical results the achievable secrecy rate and covert rate of BS and UR under various DCS scenarios.},
  archive      = {J_TMC},
  author       = {Ranran Sun and Bin Yang and Yulong Shen and Xiaohong Jiang and Tarik Taleb},
  doi          = {10.1109/TMC.2025.3589011},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On joint covert and secure communications in D2D-enabled cellular systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link performance for mobile LoRa networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, as a typical representative of Low Power Wide Area Networks (LPWAN), has been widely used to connect massive IoT devices. However, in mobile applications, there is significant packet loss in LoRa transmission due to link performance degradation. Existing studies take little account of end-devices' movement, particularly when the movement pattern is unknown. We propose LMLoRa to enhance the Link Performance for Mobile LoRa networks in general scenarios for both single-gateway and multi-gateway applications. The key observation is that, due to LoRa's unique feature, repeating the original packet content enables the use of smaller, more energy-saving transmission parameters, which not only enhances link performance but also reduces energy consumption. Technically, we propose a link performance estimation model based on packet content repetition for both single-gateway and multi-gateway mobile networks. Then, we propose the corresponding channel frequency selection model to avoid transmission collisions. Finally, we design low-overhead communication mechanisms to operate the system. To evaluate the performance of LMLoRa in various scenarios, we design and implement real-world testbeds and a simulation platform for both single-gateway and multi-gateway scenarios. Extensive results show that LMLoRa improves packet delivery ratio by an average of 33.4% to 69.2% compared with the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Ciyuan Chen and Zhuqing Xu and Runqun Xiong and Dian Shen and Weizheng Wang and Junzhou Luo and Xiaohua Jia},
  doi          = {10.1109/TMC.2025.3588923},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing link performance for mobile LoRa networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing task migration for public and private services in vehicular edge networks: A dual-layer graph neural network approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3589245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the vehicular edge networks (VEN), task migration is complicated by issues like vehicle movement, diverse resource allocation, and integrating sensing with communication technologies. This paper presents a task migration strategy to optimize task flow under limited resources in PMN-assisted VEN. Vehicles can send public and private tasks to roadside units (RSUs), constrained by bandwidth, computational power, and storage space. Public tasks aim at data collection for road transportation management, while private tasks cover a spectrum of services from work to entertainment. To address the limitations imposed by resource scarcity and meet the demands of task migration, we have developed a dual-layer graph neural network (GNN) that leverages vehicle mobility patterns. In particular, the first layer of GNN acquires vehicle information and the latest surrounding information, and sends it to the nearby RSU. Considering the variety of tasks and multi-dimensional resource constraints, the second GNN layer forecasts RSU resource availability and vehicular trajectories. Subsequently, a task-based maximum flow algorithm (T-MFA) is proposed to refine task migration paths and resource allocation strategies to maximize task flow. Simulation experiments validate the efficacy of the proposed algorithm, demonstrating its capability to achieve optimal task migration by accommodating differences in tasks, resources, and capacities.},
  archive      = {J_TMC},
  author       = {Xiaowen Huang and Tao Huang and Peng Cheng and Jinhong Yuan and Shuguang Zhao and Guanglin Zhang},
  doi          = {10.1109/TMC.2025.3589245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing task migration for public and private services in vehicular edge networks: A dual-layer graph neural network approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EASTER: Embedding aggregation-based heterogeneous models training in vertical federated learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3589426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) allows collaborative machine learning without sharing local data. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization of participants' local knowledge aggregation. To address this challenge, this paper proposes a novel approach called Embedding Aggregation-based Heterogeneous Models Training in Vertical Federated Learning (EASTER). EASTER focuses on aggregating the local embeddings of each participant's knowledge during forward propagation. We propose an embedding protection method based on lightweight blinding factors, which injects the blinding factors into the local embedding of the passive party. However, the passive party does not own the sample labels, so the local model's gradient cannot be calculated locally. To overcome this limitation, we propose a new method in which the active party assists the passive party in computing its local heterogeneous model gradients. Theoretical analysis and extensive experiments demonstrate that EASTER can simultaneously train multiple heterogeneous models and outperform some recent methods in model performance. For example, compared with the state-of-the-art method, the model accuracy of EASTER was improved by 7.22% under the CIFAR-10 dataset.},
  archive      = {J_TMC},
  author       = {Shuo Wang and Keke Gai and Jing Yu and Liehuang Zhu and Weizhi Meng and Bin Xiao},
  doi          = {10.1109/TMC.2025.3589426},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EASTER: Embedding aggregation-based heterogeneous models training in vertical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-CU: Intelligent cache replacement and content update for data freshness in cloud-edge networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3589609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand on time-sensitive contents increases, data freshness recently becomes an important performance metric in the cache-enabled networks. Therefore, in this paper, we design the joint cache replacement and content update algorithm in the cloud-edge networks considering the data freshness at both the cloud server and the edge server. We define a fresh content acquisition with cache hit (FACH) ratio as a performance metric, which shows the portion of users obtaining the requesting content from the edge server while satisfying the freshness constraint. To maximize the FACH ratio, we propose the reinforcement learning (RL)-based algorithm, named the intelligent Cache replacement and content Update (i-CU) algorithm. In the proposed algorithm, we newly suggest the score-based action decision to reduce the action space while guaranteeing the constraints of the problem. In the simulation results, we develop and evaluate the i-CU algorithm for various datasets, which verifies that the i-CU algorithm can achieve the higher FACH ratio compared to the existing baselines under the various network parameters.},
  archive      = {J_TMC},
  author       = {Sinwoong Yun and Dongsun Kim and Sungjin Lee and Jemin Lee},
  doi          = {10.1109/TMC.2025.3589609},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {I-CU: Intelligent cache replacement and content update for data freshness in cloud-edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFFGCN: Multimodal feature fusion graph convolution network for radio map estimation with uneven spatial sampling. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3590335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio map estimation (RME) is a crucial method for analyzing spectrum space utilization and network coverage, serving as an essential tool for the mobile communication. However, physical constraints, security, privacy, and other issues often render some areas inaccessible, resulting in extremely sparse and unevenly distributed measurement data. To address these challenges, we propose a multimodal feature fusion graph convolution network (MFFGCN). The model incorporates a dual-encoder architecture with an adaptive multi-feature fusion module to exploit environmental information and learn the shadowing effects of radio-signal propagation. We then convert the coarse estimation into regional feature patches and construct a graph over these patches. A graph neural network aggregates contextual information among them, thereby alleviating the impact of uneven spatial sampling. Extensive experiments on open datasets demonstrate that our method achieves state-of-the-art performance, effectively reducing the effects of uneven sampling.},
  archive      = {J_TMC},
  author       = {Han Zhang and Yu Han and Lingxin Meng and Guan Gui and Wei Xiang and Yun Lin},
  doi          = {10.1109/TMC.2025.3590335},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MFFGCN: Multimodal feature fusion graph convolution network for radio map estimation with uneven spatial sampling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-aware neural adaptive scaling for vRAN resource allocation. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3590472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although virtualized radio access networks (vRANs) offer flexibility and scalability, current scaling methods in vRANs tend to overlook the costs associated with energy consumption and service interruptions during the reconfiguration of containerized network functions (cNFs), leading to inefficient resource utilization. Moreover, traditional vertical and horizontal scaling approaches exacerbate these issues by requiring cNFs to be stopped and restarted or by deploying cNFs with fixed resource sizes. To address these challenges, we propose a cost-aware neural adaptive scaling (CNAS) framework, which adjusts cNF resource allocation dynamically, minimizing service disruptions and avoiding overprovisioning. The cost model incorporates energy consumption during the activation, allocation, and termination of cNFs and servers, as well as the operational cost of maintaining active cNFs and servers. We then formulate the integer linear programming (ILP) problem to minimize total costs. Due to the NP-hard complexity of this problem, two heuristic algorithms are applied: one utilizes dynamic programming to establish the cost-aware resource allocation, while the other uses a greedy approach to handle cNFs and servers following the determined resource allocation. Trace-driven simulation results demonstrate that CNAS can reduce the scaling costs by 53.4% and the total costs by 21.3% compared to the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Taeyun Kim and Daeyoung Jung and Yujin Kim and Sangheon Pack},
  doi          = {10.1109/TMC.2025.3590472},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-aware neural adaptive scaling for vRAN resource allocation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the generalization ability of spatiotemporal model in urban scenario. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal neural networks have shown great promise in urban scenarios by effectively capturing temporal and spatial correlations. However, urban environments are constantly evolving, and current model evaluations are often limited to traffic scenarios and use data mainly collected only a few weeks after training period to evaluate model performance. The generalization ability of these models remains largely unexplored. To address this, we propose a Spatiotemporal Out-of-Distribution (ST-OOD) benchmark, which comprises six urban scenario: bike-sharing, 311 services, pedestrian counts, traffic speed, traffic flow, ride-hailing demand, and bike-sharing, each with in-distribution (same year) and out-of-distribution (next years) settings. We extensively evaluate state-of-the-art spatiotemporal models and find that their performance degrades significantly in out-of-distribution settings, with most models performing even worse than a simple Multi-Layer Perceptron (MLP). Our findings suggest that current leading methods tend to over-rely on parameters to overfit training data, which may lead to good performance on in-distribution data but often results in poor generalization. We also investigated whether dropout could mitigate the negative effects of overfitting. Our results showed that a slight dropout rate could significantly improve generalization performance on most datasets, with minimal impact on in-distribution performance. However, balancing in-distribution and out-of-distribution performance remains a challenging problem. We hope that the proposed benchmark will encourage further research on this critical issue.},
  archive      = {J_TMC},
  author       = {Hongjun Wang and Jiyuan Chen and Tong Pan and Zheng Dong and Renhe Jiang and Xuan Song},
  doi          = {10.1109/TMC.2025.3590606},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Evaluating the generalization ability of spatiotemporal model in urban scenario},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contributed perception-based dynamic evolution method for autonomous vehicle groups in open scenes. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately handling dynamic evolution events is a significant challenge for autonomous vehicle groups (AVGs) in open scenes, which can be affected by complex road conditions and various interference factors. Existing work on the dynamic evolution of AVGs in open scenes concentrates on semi-centralized groups, assessing communication links as the sole criterion. However, there lack the mathematical analysis of and methods for the dynamic evolution of events in distributed AVGs with cooperative perception. To address this issue, we propose a contributed perception-based dynamic evolution method designed for distributed AVGs. This method ensures that group members can continuously and timely exchange valid perceptual information. Firstly, we investigate the impact of external interference on the contributed perception of vehicle groups to understand the drivers behind their dynamic evolution. Secondly, we define a range of vehicle group evolution behaviors and corresponding handling methods in response to external interference. Lastly, we introduce group states and perceptibility to delineate the evolution dynamics. Simulation results demonstrate the superiority of our proposed method over existing ones in terms of average group contribution, accessibility, persistence, timeliness, and perceptibility.},
  archive      = {J_TMC},
  author       = {Qichao Mao and Jiujun Cheng and MengChu Zhou and Zhangkai Ni and Guiyuan Yuan and Shangce Gao and Chuanhuang Li},
  doi          = {10.1109/TMC.2025.3590653},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Contributed perception-based dynamic evolution method for autonomous vehicle groups in open scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge large AI model agent-empowered cognitive multimodal semantic communication. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communications (SemCom) provide efficient transmission for mobile edge computing (MEC) services by extracting critical semantics from raw information. Although widely adopted in various scenarios, existing single-modal Sem Com systems struggle to efficiently support edge multimodal data transmission. Additionally, mobile end users have varying communication requirements across different modalities. However, existing work lacks the ability to generate personalized communication policies tailored to diverse intents (Typically, communication policies include bandwidth allocation and modulation and coding schemes, etc.). In this paper, we propose an edge Cognitive SemCom Agent (CSCA) to facilitate edge multimodal SemCom. Specifically, CSCA leverages an edge Large AI Model (LAM) to realize modality alignment and natural language intent understanding. Moreover, we develop a communication planning module to realize the planning capability, which generates personalized wireless communication policies based on LAM's environment and intent cognition. Particularly, to assess the efficiency of communication policies in multimodal SemCom and capture intent competition, we present a novel indicator named cognitive SemCom quality indicator (CSCQI). Then, we use the denoising diffusion probabilistic model to optimize the generation policy. Extensive experimental results demonstrate that CSCA achieves an average improvement in intent satisfaction rate and semantic accuracy by 42.19% and 29.75% respectively, while reducing communication delay by 33.40%.},
  archive      = {J_TMC},
  author       = {Yan Sun and Yinqiu Liu and Shaoyong Guo and Xuesong Qiu and Jiewei Chen and Jiakai Hao and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3590723},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge large AI model agent-empowered cognitive multimodal semantic communication},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scenario robust stochastic optimization based approach for scheduling of mobile charging stations. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, mobile charging stations (MCSs) have emerged as a complement of fixed charging stations. An idle MCS could receive few charging requests due to the lack of neighboring electric vehicles to be charged (EVCs), and thus its charging profit could be affected. To decrease the scheduling cost and increase the charging profit of MCSs, idle MCSs should proactively move to the regions with high charging demand and with low charging supply. However, the future charging demand of EVCs is uncertain and related to some external factors. To this end, we introduce the robust stochastic optimization (RSO) model to formulate the future charging demand as an uncertain variable, and then a scheduling approach based on the multi-scenario robust stochastic optimization (SA-MSRSO) is proposed. In SAMSRSO, the probability distributions of the uncertain variable across a range of potential scenarios are approximatively obtained by analyzing the historical charging demand, and a multivariate regression tree (MRT) is applied for solving the model. Extensive simulations and comparisons demonstrate the performance superiority of our proposed SA-MSRSO, i.e., the charging profit of MCSs can be significantly increased, and the proportion of successfully charged EVCs can be effectively enhanced.},
  archive      = {J_TMC},
  author       = {Linfeng Liu and Youheng Zheng and Yu Tang and Jia Xu},
  doi          = {10.1109/TMC.2025.3590688},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-scenario robust stochastic optimization based approach for scheduling of mobile charging stations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust model for underwater wireless sensor networks based on variational autoencoders. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With sensitive applications in marine exploration, disaster warning, and oil monitoring, ensuring the data integrity of Underwater Wireless Sensor Networks (UWSNs) has become an increasingly critical challenge. Existing trust models primarily rely on Euclidean distance to evaluate the trustworthiness of nodes. However, the lack of a probabilistic framework for representing trust evidence undermines the accuracy and reliability of these models. In this paper, we propose a novel trust model based on a Variational Autoencoder (VAE) that maps trust evidence into a latent space to model the normal data distribution, thus enabling the identification of malicious nodes. Recognizing the limitations of Euclidean latent spaces in capturing the non-linear characteristics of underwater acoustic signals, we extend this framework by introducing a Riemannian manifold as the latent space. The proposed method enhances scalability by reconstructing implicit features, thereby offering the potential to uncover previously unrecognized threats and improving the accuracy of malicious node identification. Simulation results demonstrate that the proposed trust model achieves a detection accuracy of 94%. Compared to existing trust models, this approach exhibits superior verification accuracy and robustness, making it a promising solution for ensuring the reliability of UWSNs in underwater environments characterized by high noise levels and dynamic topology changes.},
  archive      = {J_TMC},
  author       = {Na Xia and Sizhou Wei and Meng Li and Yin Wang and Jiashan Wan},
  doi          = {10.1109/TMC.2025.3590741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trust model for underwater wireless sensor networks based on variational autoencoders},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated MADDPG-based collaborative scheduling strategy in vehicular edge computing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3590747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Vehicular Edge Computing (VEC), vehicles of fload computational tasks to Roadside Units (RSUs) equipped with edge servers to achieve efficient processing. Considering that vehicles switch connections between RSUs during highspeed movement, obtaining the state information of other RSUs is crucial for achieving global collaborative decision-making. However, frequent sharing of RSUs' state data during the training of scheduling models may result in privacy leakage risks. To address this issue, we federally train a joint scheduling model for task offloading and resource allocation without the need for state sharing among RSUs. We prove that the proposed task offloading problem influenced by resource allocation is a strict multi-node non-cooperative potential game problem, and use the potential function as the reward function for MultiAgent Deep Deterministic Policy Gradient (MADDPG). Finally, we propose the Fed-MADDPG algorithm to find the equilibrium point of task offloading and apply the gradient descent method and the Lagrange multiplier method to maximize the average task completion rate among RSUs under constraints, ensuring the framework has optimal computational and transmission performance. We conduct simulation experiments using realworld datasets, and the results show that this method has superior performance compared to previous approaches.},
  archive      = {J_TMC},
  author       = {Songxin Lei and Huijun Tang and Chuangyi Li and Xueying Zhang and Chenli Xu and Huaming Wu},
  doi          = {10.1109/TMC.2025.3590747},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated MADDPG-based collaborative scheduling strategy in vehicular edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An underwater secure localization scheme based on physical layer cryptographic learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open underwater environments, ensuring accurate positions of sensors while protecting private information of localization systems presents a significant challenge. The physical channel differences between terrestrial and underwater networks render most existing privacy protection schemes designed for terrestrial networks inapplicable underwater. Moreover, limited research on underwater privacy protection has led to high implementation complexity and communication expenses. In this paper, to reduce the complexity of privacy protection, a secure mobile localization scheme using autonomous underwater vehicles (AUVs) as anchors is proposed for underwater sensor networks, based on adversarial neural cryptography utilizing acoustic channel features. Depending on whether eavesdroppers show interest in keys, two adversarial cryptography models are proposed to protect transmission of legitimate localization information and to actively counter eavesdroppers with learning capabilities in real time. Furthermore, to obtain effective keys and minimize unnecessary key transmission, random physical layer channel features are dynamically utilized as real-time keys for the cryptography system, and a synchronous channel probing protocol is designed for key generation. Simulation and experimental results demonstrate that, compared to other approaches, the proposed secure localization scheme effectively prevents the leakage of position information and maintains localization accuracy while operating at lower implementation complexity and communication expenses.},
  archive      = {J_TMC},
  author       = {Rong Fan and Azzedine Boukerche and Pan Pan and Zhigang Jin and Yishan Su},
  doi          = {10.1109/TMC.2025.3591016},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An underwater secure localization scheme based on physical layer cryptographic learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private and effective range counting query over evolving data in internet of things. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range counting query is the fundamental task for data analysis and data mining in Internet of Things (IoT). However, it poses a threat to the data privacy of data contributors, which is exacerbated by evolving data in particular. Several studies focus on range counting query on a timestamp over the finite evolving data, and thus are not applicable to the longitudinal range counting query on the infinite evolving data. To this end, we propose the Private and Effective Range counting query over Evolving data (PERE) that supports both the finite evolving data and the infinite evolving data, and is applicable for both the range counting query on a specific timestamp and the longitudinal range counting query. Specifically, we first design a Private Infinite Update Framework for IoT evolving data while providing meaningful privacy protection. The framework is coupled with a general and practical data evolution paradigm. Then, we propose a Optimized Frequency Perturbation consisting of an enhanced frequency oracle protocol and random sampling attribute. On this basis, we further propose a novel Adaptive Interval Merging mechanism that dynamically considers all potential interval consolidation possibilities and the reasonable selection of intervals for merging, to balance non-uniform error and noise error. Thereafter, we further reduce the estimated error in query results by Frequency Adjustment that consists of Norm-Sub and weighted average process. Last, we theoretically prove that the proposed PERE satisfies Local Differential Privacy (LDP), that the query results of PERE are unbiased, and that the variance of the query results is desirable. Furthermore, the extensive experiments on multiple real-world and synthetic datasets validate the effectiveness of PERE, as well as its advantages over the state-of-the-art works in answering range counting queries of evolving data.},
  archive      = {J_TMC},
  author       = {Ping Zhao and Dazhi Hu},
  doi          = {10.1109/TMC.2025.3590918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Private and effective range counting query over evolving data in internet of things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-of-service aware LLM routing for edge computing with multiple experts. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.},
  archive      = {J_TMC},
  author       = {Jin Yang and Qiong Wu and Zhiying Feng and Zhi Zhou and Deke Guo and Xu Chen},
  doi          = {10.1109/TMC.2025.3590969},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quality-of-service aware LLM routing for edge computing with multiple experts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient and secure federated learning for edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the collaborative machine learning nature of Federated Learning (FL), it enables the training of machine learning models on large-scale distributed datasets in edge computing environments. Nevertheless, the application of FL in edge computing still faces three crucial challenges: resource constraint, privacy leakage, and Byzantine failures. Unfortunately, current approaches lack the ability to effectively balance these three challenges. In this paper, we propose FedEdge, a cost-efficient and secure FL for edge computing. FedEdge contains two main mechanisms: adaptive compression perturbation and dynamic update filtering. The adaptive compression perturbation mechanism reduces the communication overhead, provides different levels of privacy protection for edge nodes, and prevents Byzantine attacks. The dynamic update filtering mechanism is used to further filter Byzantine attacks and limit the impact of adaptive compression perturbation on the global model performance. The experimental results on the MNIST, CIFAR-10, CIFAR-100, and CelebA datasets demonstrate the effectiveness of FedEdge against free-riders, label-flipping, and sign-flipping attacks. Theoretical analysis also demonstrate that FedEdge can still converge even when the majority of edge nodes are malicious.},
  archive      = {J_TMC},
  author       = {Zhuangzhuang Zhang and Libing Wu and Zhibo Wang and Jiahui Hu and Chao Ma and Qin Liu},
  doi          = {10.1109/TMC.2025.3590799},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-efficient and secure federated learning for edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniTrans: A unified vertical federated knowledge transfer framework for enhancing edge healthcare collaboration. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-hospital collaboration has the potential to mitigate disparities in medical resources across different regions. However, strict privacy regulations prohibit the direct sharing of sensitive patient information between hospitals. Vertical Federated Learning (VFL) provides a novel privacy-preserving machine learning paradigm designed to maximizes data utility across multiple hospitals. Nevertheless, traditional VFL methods primarily benefit patients with overlapping data, leaving non-overlapping patients without guaranteed improvements in distributed healthcare prediction services. While some existing knowledge transfer techniques attempt to improve prediction performance for non-overlapping patients, they fail to adequately address scenarios where overlapping and non-overlapping patients originate from different domains, resulting in challenges such as feature and label heterogeneity. To address these issues, we propose UniTrans, a unified vertical federated knowledge transfer framework for edge healthcare collaboration. Our framework consists of three key steps. First, we extract the federated representation of overlapping patients by employing an effective vertical federated representation learning method to model multi-party joint features online. Next, each hospital learns a local knowledge transfer module offline, enabling the domain-adaptive transfer of knowledge from the federated representation of overlapping patients to the enriched representation of local non-overlapping patients. Finally, hospitals utilize these enriched local representations to enhance performance across various downstream medical prediction tasks. Extensive experiments on real-world medical datasets demonstrate the effectiveness and scalability of UniTrans in both intra-domain and cross-domain knowledge transfer. The code of UniTrans is available at https://github.com/Chung-ju/Unitrans.},
  archive      = {J_TMC},
  author       = {Chung-ju Huang and Yuanpeng He and Xiao Han and Wenpin Jiao and Zhi Jin and Leye Wang},
  doi          = {10.1109/TMC.2025.3590813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UniTrans: A unified vertical federated knowledge transfer framework for enhancing edge healthcare collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADGTrace: Achieving adaptive trajectory synthesis with generated data. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3590770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User trajectory publication has promoted various location-based applications like user travel recommendation. However, possible privacy leakages have hindered more inclusive trajectory data analysis and utilization. Privacy-preserving trajectory synthesis is a popular approach to address the above privacy issues. Existing methods unavoidably produce low trajectory utility since they usually apply perturbed versions of human moving patterns. Worse still, they cannot adaptively adjust this synthesis according to the varying granularity demands of different users. This paper proposes a novel adaptive trajectory synthesis framework with generated data, namely ADGTrace. Our model achieves privacy preservation without introducing additional noise while maintaining high adaptation. ADGTrace directly synthesizes artificial trajectories that share the similar patterns with real ones through a generative and selective optimization process. Additionally, we present a grid granularity alignment strategy to achieve adaptive trajectory synthesis, satisfying varying user demands. Extensive experiments on real-world datasets demonstrate the superiority of ADGTrace over the state-of-the art methods under various utility metrics, maintaining strong attack resilience.},
  archive      = {J_TMC},
  author       = {Hui Cai and Chen Lan and Biyun Sheng and Jian Zhou and Yuanyuan Yang and Yanmin Zhu and Fu Xiao},
  doi          = {10.1109/TMC.2025.3590770},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ADGTrace: Achieving adaptive trajectory synthesis with generated data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing non-intrusive load monitoring privacy invasion: A precise adversarial attack scheme for networked smart meters. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3590765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grid, through networked smart meters employing the non-intrusive load monitoring (NILM) technique, can considerably discern the usage patterns of residential appliances. However, this technique also incurs privacy leakage. To address this issue, we propose an innovative scheme based on adversarial attack in this paper. The scheme effectively prevents NILM models from violating appliance-level privacy, while also ensuring accurate billing calculation for users. To achieve this objective, we overcome two primary challenges. First, as NILM models fall under the category of time-series regression models, direct application of traditional adversarial attacks designed for classification tasks is not feasible. To tackle this issue, we formulate a novel adversarial attack problem tailored specifically for NILM and providing a theoretical foundation for utilizing the Jacobian of the NILM model to generate imperceptible perturbations. Leveraging the Jacobian, our scheme can produce perturbations, which effectively misleads the signal prediction of NILM models to safeguard users' appliance-level privacy. The second challenge pertains to fundamental utility requirements, where existing adversarial attack schemes struggle to achieve accurate billing calculation for users. To handle this problem, we introduce an additional constraint, mandating that the sum of added perturbations within a billing period must be precisely zero. Experimental validation on real-world power datasets REDD and U.K.-DALE demonstrates the efficacy of our proposed solutions, which can significantly amplify the discrepancy between the output of the targeted NILM model and the actual power signal of appliances, and enable accurate billing at the same time. Additionally, our solutions exhibit transferability, making the generated perturbation signal from one target model applicable to other diverse NILM models.},
  archive      = {J_TMC},
  author       = {Jialing He and Jiacheng Wang and Ning Wang and Shangwei Guo and Liehuang Zhu and Dusit Niyato and Tao Xiang},
  doi          = {10.1109/TMC.2025.3590765},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Preventing non-intrusive load monitoring privacy invasion: A precise adversarial attack scheme for networked smart meters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal based 3D localization via the channel adjustment LED-tag. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of intelligent systems like assisted driving and robotics, all-weather target identification and 3D localization systems have become crucial for reliable obstacle avoidance and navigation. However, vision-based methods struggle to provide accurate target locations under low light or bad weather. Radar-based solutions like mmWave radar and LiDAR are robust but hindered by high costs and challenges in recognizing target identities at scale. In this paper, we propose a low-cost, all-weather target identification and 3D localization system based on LED-tags, which system can address the needs of intelligent systems for obstacle avoidance in complex environments. We explore the backscatter communication of LED devices and design a dual-modal LED-Tag, which includes two features: a backscatter RF signal detectable by RF devices and visual light spot information detectable by cameras, both sharing the same ID. To enhance the limited backscatter capability, we propose a multi-branch parallel model that enhances the signal strength using beamforming synthesis and a channel adjustment mechanism to improve robustness in complex environments, ensuring accurate 3D localization. For multi-target identification, we design an LED-tag encoding system, assigning each tag a unique encoding sequence. Each target's identity can be recognized with our customized ID decoding method, which leverages prior information and time-domain sampling characteristics. Extensive experimental results show that the backscatter communication and target detection range of LED-tags can reach 15m. Moreover, the system achieves an average localization error of 7.3cm within a 5m range, demonstrating the system's excellent performance in terms of practicality and accuracy.},
  archive      = {J_TMC},
  author       = {Shiyuan Ma and Lei Xie and Chuyu Wang and Yanling Bu and Long Fan and Jingyi Ning and Qing Guo and Baoliu Ye and Sanglu Lu},
  doi          = {10.1109/TMC.2025.3590801},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-modal based 3D localization via the channel adjustment LED-tag},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCFI-net: Global-local cross-spatial-channel feature interaction network for point cloud geometry compression. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently compressing large-scale point cloud data under limited bandwidth and computing resource conditions has become a critical issue to be addressed in mobile computing platforms. Although the octree structure can efficiently represent large-scale and complex point clouds, existing octree-based Point Cloud Geometry Compression (PCGC) approaches typically focus on exploiting either spatial or channel features individually, neglecting the interaction across spatial-channel dimensions. In addition, current approaches are also limited to small-scale point clouds due to reliance on global Transformer or local convolutional neural network (CNN). To solve these issues, we introduce GCFI-Net, a global-local cross-spatial-channel feature interaction network for predicting the occupancy probability distribution of each octree node in this paper. In the GCFI-Net, we propose a Multiscale Convolutional Fusion-based Spatial Interaction (MCFSI) module to capture global context and model spatial interactions, and a Global-Local Cross-Channel Interaction (GLCCI) module with dual pathways to integrate global and local cross-channel information. Additionally, we propose a Multiscale-enhanced Spatial and Channel Interaction (MSCI) module to aggregate features from ancestor and sibling nodes, which further enhances the octree node representation ability. Extensive experiments on large-scale sparse LiDAR and dense human body point clouds demonstrate that the proposed GCFI-Net achieves superior compression performance with fewer parameters compared to state-of-the-art PCGC methods.},
  archive      = {J_TMC},
  author       = {Xinjie Wang and Yifan Zhang and Xinpu Liu and Ke Xu and Jianwei Wan and Yulan Guo and Hanyun Wang},
  doi          = {10.1109/TMC.2025.3590775},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GCFI-net: Global-local cross-spatial-channel feature interaction network for point cloud geometry compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive interference alignment for underwater optical wireless sensor networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3590884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Optical Wireless Sensor Networks (UOWSNs) have emerged as a promising solution for high-speed underwater communication. However, these networks face a critical challenge of mutual interference among optical nodes, which occurs when the directional optical beams intersect or coverage areas overlap due to node mobility in dynamic underwater environments. Existing interference management approaches demonstrate limited effectiveness due to their reliance on simplified channel models and inability to handle rapid topology changes, resulting in significant network performance degradation. This paper presents a novel framework that systematically addresses interference management in UOWSNs through two key innovations. First, we propose a Sparse Bayesian Learning-based Interference Detection (SBL-ID) algorithm that enables real-time identification and characterization of interference patterns under complex underwater channel conditions. Second, we develop an Adaptive Interference Alignment and Delay Compensation (AIADC) algorithm that projects interference signals into a reduced-dimensional subspace, thereby enhancing the signal-to-interference ratio and facilitating accurate detection of desired signals amid interference. Our framework transforms the NP-hard interference management problem into tractable optimizations, achieving near-optimal solutions with polynomial time complexity. Extensive simulations demonstrate that our approach reduces BER by 95% and improves network throughput by 67% compared to state-of-the-art techniques. Testbed experiments conducted in both pool and lake further validate our framework's effectiveness, maintaining consistent performance improvements under diverse underwater conditions.},
  archive      = {J_TMC},
  author       = {Yang Chi and Chi Lin and Fengqi Li and Feng Ding and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TMC.2025.3590884},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive interference alignment for underwater optical wireless sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asynchronous consensus method with low communication traffic and high efficiency for distributed multi-agent scheduling. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Internet of Things (AIoT) is growing into a new frontier field with broad development prospects, which essence is the collaborative enhancement of networked heterogeneous agent swarms. The market-based approach is an effective way for the cooperative scheduling of agent swarm, where networked agents need to distributedly select and arrange tasks meeting the spatio-temporal constraints. This paper proposes a new asynchronous consensus method aimed at substantially mitigating the communication traffic and decreasing the message transmission requirements associated with the market-based approach, ultimately leading to a reduction in scheduling time. Firstly, the method innovatively introduces timestamps of agent information updates, which are more informative, thereby reducing inter-agent communication volume to $ n/m$ of that in the original protocol (where $ n$ represents the number of agents and $ m$ denotes the number of tasks, with $ m\gt n$). Secondly, agent-centric asynchronous consensus protocols are designed based on the new timestamps, which can resolve inter-agent task conflicts more rapidly and efficiently. Additionally, a mechanism for avoiding message flooding is proposed to prevent endless broadcasts caused by communication issues such as packet loss, link disruptions, and node withdrawals. Finally, through a self-developed ad-hoc network simulation system, the swarm scheduling under real networking conditions is simulated. The validation results demonstrate that the algorithm can significantly reduce communication traffic and scheduling time.},
  archive      = {J_TMC},
  author       = {Runfeng Chen and Jie Li and Yiting Chen and Yuchong Huang and Xiangke Wang and Lincheng Shen},
  doi          = {10.1109/TMC.2025.3591038},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An asynchronous consensus method with low communication traffic and high efficiency for distributed multi-agent scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DepGuard: Depression recognition and episode monitoring system with a ubiquitous wrist-worn device. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3591096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression significantly impacts mental health, severely disrupting patients' daily lives. During depressive episodes, individuals may experience symptoms such as excessive guilt, self-harm, and suicidal ideation. Compared to proprietary devices like brain electrode caps, wearable technologies for depression detection have gained attention due to their affordability and portability—enabling real-time monitoring of depressive states. However, challenges such as low-quality data from ubiquitous devices, individual variability, and the complexity of multimodal physiological signal analysis limit model generalizability. To address these issues, we present DepGuard, a novel ubiquitous wearable system for depression assessment based on multimodal physiological signals. DepGuard performs a two-stage detection process: depression recognition and real-time episode monitoring. For depression recognition, we propose an unsupervised domain adaptation method to reduce the domain gap between source and target subjects. For episode monitoring, we employ a few-shot learning strategy to enable personalized modeling. Both approaches enhance cross-subject generalization. Our system achieves 90.75% accuracy in cross-subject depression recognition using 30 unlabeled samples per target subject, and 93.52% accuracy in episode monitoring using 15 labeled samples per class.},
  archive      = {J_TMC},
  author       = {Yufei Zhang and Shuo Jin and Wenting Kuang and Yuda Zheng and Qifeng Song and Changhe Fan and Yongpan Zou and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3591096},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DepGuard: Depression recognition and episode monitoring system with a ubiquitous wrist-worn device},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EP-GSPR: An efficient privacy-preserving graph shortest path retrieval scheme. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous development of mobile terminal applications, online maps, and other navigation services have become widely used, simultaneously giving rise to significant security risks. To address the issues of privacy leakage and low efficiency in traditional graph shortest path retrieval schemes, an efficient privacy-preserving graph shortest path retrieval scheme is proposed, called EP-GSPR. Specifically, this scheme addresses the privacy security problems in the existing graph shortest path retrieval solutions by ensuring the bilateral privacy protection of the user's query location and the database privacy of the cloud server. Throughout the retrieval process, the cloud server cannot obtain the user's location information, and the user cannot access any database information other than the retrieval results. To overcome the performance bottlenecks in existing schemes, a progressive iterative retrieval framework is designed as the fundamental modular, called Pirf, achieving sub-linear retrieval costs and low storage overhead on the cloud server side. Finally, the security analyses demonstrate the EP-GSPR scheme achieves the bilateral privacy-preserving in terms of user and server sides. The comprehensive experiment evaluations also state the efficiency and practicality of the proposed scheme},
  archive      = {J_TMC},
  author       = {Chenbin Zhao and Ruifeng Zhu and Jing Chen and Ruiying Du and Kun He and Jianting Ning and Yang Xiang},
  doi          = {10.1109/TMC.2025.3591097},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EP-GSPR: An efficient privacy-preserving graph shortest path retrieval scheme},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal diffusion model for cellular traffic generation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, the increasing demand for network traffic necessitates strategic network infrastructure planning. Accurate modeling of traffic demand through cellular traffic generation is crucial for optimizing base station deployment, enhancing network efficiency, and fostering technological innovation. In this paper, we introduce STOUTER, a spatio-temporal diffusion model for cellular traffic generation. STOUTER incorporates noise into traffic data through a forward diffusion process, followed by a reverse reconstruction process to generate realistic cellular traffic. To effectively capture the spatio-temporal patterns inherent in cellular traffic, we pre-train a temporal graph and a base station graph, and design the Spatio-Temporal Feature Fusion Module (STFFM). Leveraging STFFM, we develop STUnet, which estimates noise levels during the reverse denoising process, successfully simulating the spatio-temporal patterns and uncertainty variations in cellular traffic. Extensive experiments conducted on five cellular traffic datasets across two regions demonstrate that STOUTER improves cellular traffic generation by 52.77% in terms of the Jensen-Shannon Divergence (JSD) metric compared to existing models. These results indicate that STOUTER can generate cellular traffic distributions that closely resemble real-world data, providing valuable support for downstream applications.},
  archive      = {J_TMC},
  author       = {Xiaosi Liu and Xiaowen Xu and Zhidan Liu and Zhenjiang Li and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3591183},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatio-temporal diffusion model for cellular traffic generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel integrated sensing and communication scheme in UAVs-enabled vehicular networks with MARL-driven adaptive control. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3591259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel integrated sensing and communication (ISAC) scheme tailored for UAVs-enabled vehicular networks, which leverages the information coverage capabilities of multiple UAVs and addresses critical challenges posed by multiple moving users. Unlike many traditional scheme, our scheme efficiently leverages ISAC signal echoes and real-time data uploads to provide communication services while achieving accurate sensing, thereby overcoming issues of resource waste and low operational efficiency. In the scheme, we aim to optimize both communication and sensing indicators, taking into account practical issues such as energy saving and collision avoidance for UAVs. However, the inherent complexity of multi-objective stochastic optimization in dynamic environments and limited communication resources render centralized UAV control inconvenient. To address the above challenges, we propose a novel multi-agent reinforcement learning (MARL) algorithm based on local information to realize the distributed adaptive control of motion decision, power selection, and channel allocation for UAVs. The algorithm combines random network distillation (RND) and dynamic data augmentation with multi-agent deep deterministic policy gradient (MADDPG) to encourage agents to explore effectively under sparse rewards and improve MADDPG's policy learning ability in finite data, thus approaching the global optimal solution. Experimental results demonstrate that the proposed algorithm can improve communication and sensing performance by more than 16.71% and 68.26% compared with other baselines and satisfy the set constraints. Furthermore, by adjusting hyperparameters, we can optimize the ISAC performance while achieving different energy savings levels for UAVs, proving that the designed scheme can reduce the waste of resources and improve the ISAC operation efficiency.},
  archive      = {J_TMC},
  author       = {Ziyuan Wang and Xiao-Ping Zhang and Wenbo Ding and Yuhan Dong and Xinlei Chen},
  doi          = {10.1109/TMC.2025.3591259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel integrated sensing and communication scheme in UAVs-enabled vehicular networks with MARL-driven adaptive control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeOAR: Real-time online action recognition on edge devices. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenges of Online Action Recognition (OAR), a framework that involves instantaneous analysis and classification of behaviors in video streams. OAR must operate under stringent latency constraints, making it an indispensable component for real-time feedback for edge computing. Existing methods, which typically rely on the processing of entire video clips, fall short in scenarios requiring immediate recognition. To address this, we designed EdgeOAR, a novel framework specifically designed for OAR on edge devices. EdgeOAR includes the Early Exit-oriented Task-specific Feature Enhancement Module (TFEM), which comprises lightweight submodules to optimize features in both temporal and spatial dimensions. We design an iterative training method to enable TFEM learning features from the beginning of the video. Additionally, EdgeOAR includes an Inverse Information Entropy (IIE) and Modality Consistency (MC)-driven fusion module to fuse features and make better exit decisions. This design overcomes the two main challenges: robust modeling of spatio-temporal action representations with limited initial frames in online video streams and balancing accuracy and efficiency on resource-constrained edge devices. Experiments show that on the UCF-101 dataset, our method EdgeOAR reduces latency by 99.23% and energy consumption by 99.28% compared to state-of-the-art (SOTA) method. And achieves an adequate accuracy on edge devices.},
  archive      = {J_TMC},
  author       = {Wei Luo and Deyu Zhang and Yin Tang and Fan Wu and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3591188},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeOAR: Real-time online action recognition on edge devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chirp-level information-based collaborative key generation for LoRa networks via perturbed compressed sensing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical-layer key generation holds significant potential in establishing cryptographic key pairs for emerging LoRa networks. Nevertheless, current key generation solutions may underperform due to critically impaired channel reciprocity, attributed to the low data rate and long range inherent in LoRa networks. In this study, we present ChirpKey, a novel key generation scheme for LoRa networks. We pinpoint the key hurdles as the coarse-grained channel measurement, inefficient quantization methods, and out-of-range device constraints. To capture fine-grained channel information, we introduce a unique, LoRa-specific channel measurement method that focuses on analyzing chirp-level variations in LoRa packets. We also propose a LoRa channel state estimation algorithm to neutralize asynchronous channel sampling. Instead of the traditional quantization approach, we propose an innovative key delivery method based on perturbed compressed sensing, offering enhanced robustness and security. For LoRa devices beyond each other's communication reach, we integrate relay nodes to ensure reliable key generation. To foster secure group communication, we formulate two protocols that facilitate collaborative key generation across both star and chain configurations. Evaluation across diverse real-world scenarios reveals that ChirpKey enhances the key matching rate by 11.03–26.58% and increases the key generation rate by 27–49× in comparison to existing leading systems. Our security analysis shows that ChirpKey can effectively withstand a variety of prevalent attacks. Furthermore, we implement a ChirpKey prototype, demonstrating its capability to operate within 0.2 s.},
  archive      = {J_TMC},
  author       = {Huanqi Yang and Zehua Sun and Hongbo Liu and Xianjin Xia and Yu Zhang and Tao Gu and Gerhard Hancke and Weitao Xu},
  doi          = {10.1109/TMC.2025.3591298},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Chirp-level information-based collaborative key generation for LoRa networks via perturbed compressed sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end coordinated spatio-temporal redundancy elimination for fast video analytics. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3591307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge video analytics typically rely on conventional encoding standards to transmit device visual data for server-side inference. Unfortunately, general-purpose compression solutions retain unnecessary visual data that does not contribute to accuracy, resulting in significant latency throughout Video Analytics Pipeline (VAP). While previous approaches have made partial progress, they cannot systematically eliminate VAP redundancy due to uncoordinated subsystem-level optimization. Achieving complete redundancy elimination presents a major challenge, as a lack of spatio-temporal coordination risks offsetting latency gains with computational overhead (associated with redundancy elimination). Crucio overcomes these limitations with an end-to-edge framework that integrates temporally adaptive frame filtering and coordinated video compression. It leverages redesigned asymmetric autoencoders to synchronize inter-frame temporal compression with intra-frame spatial feature extraction. Additionally, Crucio employs a one-pass decoding mechanism for encoded critical frames and dynamically adjusts batching scales to minimize latency. Empirical results demonstrate Crucio's superiority, outperforming existing solutions (e.g., DDS, Reducto, and STAC) by over a 31% reduction in end-to-end latency at 0.9 accuracy thresholds.},
  archive      = {J_TMC},
  author       = {Andong Zhu and Sheng Zhang and Lingkun Meng and Xiaohang Shi and Xiangyu Li and Dongxu Wang and Ke Cheng and Hesheng Sun and Sanglu Lu and Jie Wu and Yu Liang},
  doi          = {10.1109/TMC.2025.3591307},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-to-end coordinated spatio-temporal redundancy elimination for fast video analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified diffusion framework for traffic imputation and prediction with physical priors. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread occurrence of missing data in traffic sensor networks critically undermines the performance of intelligent transportation systems. Existing data-driven models often fail to accurately capture dynamic spatiotemporal dependencies, particularly in highly heterogeneous road environments. To address this issue, this paper presents a diffusion-based framework with physically consistent priors for modeling spatiotemporal traffic data (DCDM), which integrates physical constraints to improve the robustness and interpretability of the imputation and forecasting processes. Specifically, we develop a spatio-temporal feature extractor grounded in the infiltration principle of Fick's law to model directional flow dynamics between adjacent road nodes. To mitigate the impact of missing data at critical nodes, we introduce a global information compensation mechanism that enhances the denoising process by capturing long range spatiotemporal dependencies. The proposed model jointly optimizes data imputation and prediction tasks within a unified diffusion framework. Extensive experiments on three real-world datasets demonstrate that DCDM consistently outperforms state-of-theart methods under various missing data conditions, achieving superior reconstruction accuracy and predictive stability. Moreover, statistical tests and visualization results further confirm the model's robustness, interpretability, and strong generalization ability in generating high-quality data that closely matches the true distribution.},
  archive      = {J_TMC},
  author       = {Peng Liu and Yaodong Zhu and Yang Yang and Caixia Wang and Jingfeng Jie},
  doi          = {10.1109/TMC.2025.3591423},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A unified diffusion framework for traffic imputation and prediction with physical priors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedGraft: Memory-aware heterogeneous federated learning via model grafting. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3591537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Federated Learning (FL) is good at collaborative learning among devices without compromising their data privacy, it suffers from the problem of large-scale deployment in Mobile Edge Computing (MEC) applications. This is mainly because the varying memory sizes of edge devices inevitably result in limited sizes of their hosting models. According to the Cannikin Law, when dealing with heterogeneous devices with different memory sizes, the learning capability of existing homogeneous FL schemes is greatly restricted by the weakest device. Worse still, although existing heterogeneous FL methods enable a MEC application to involve numerous devices equipped with heterogeneous models, their knowledge aggregation processes require either extra training data or architecture similarity of models. To address the above issues, this paper presents a novel FL method named FedGraft that enables effective knowledge sharing among heterogeneous device models of different sizes without imposing unrealistic assumptions. In FedGraft, all the device models are grafted to a common rootstock based on our proposed model partitioning and grafting mechanism, facilitating knowledge sharing among heterogeneous models on top of a tree-like global model. Meanwhile, using our proposed device selection strategy, the reassembled submodels extracted from the global model can be reasonably dispatched to corresponding devices with sufficient memory, thus enhancing the overall FL performance. Comprehensive experimental results show that, compared with state-of-the-art heterogeneous FL methods, FedGraft can improve inference accuracy by up to 17% in various memory-constrained scenarios.},
  archive      = {J_TMC},
  author       = {Ruixuan Liu and Ming Hu and Zeke Xia and Xiaofei Xie and Jun Xia and Pengyu Zhang and Yihao Huang and Mingsong Chen},
  doi          = {10.1109/TMC.2025.3591537},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedGraft: Memory-aware heterogeneous federated learning via model grafting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing joint speed and altitude schedule for UAV data collection in low-altitude airspace. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-altitude airspace in major cities across the world is increasingly congested with unmanned aerial vehicles (UAVs) and other aircraft. Emerging technologies, innovative business models, and supportive government policies are driving the growth of the low-altitude economy, where UAVs play a crucial role. Given the limited on-board energy of UAVs, this paper investigates the Joint UAV Speed and Altitude Scheduling (JUSAS) problem for data collection from sensors deployed along power transmission lines, bridges, highways, railways, water/gas/oil pipelines, or rivers/coasts. Distinct from existing work, the paper focuses on jointly optimizing UAV speed and altitude scheduling while determining the wireless sensor collection order. It accounts for the altitude-specific sensor transmission range model and the complexities of overlapping range relationships. We first propose the Slowest Segment First (SSF) policy to obtain an optimal UAV speed scheduling for fixed-altitude scenarios. Building upon this, we then reformulate JUSAS as a shortest-path-type problem using our novel flight scheduling graph, solved efficiently through the SSF-based Ant Colony Optimization (SSF-ACO) algorithm. To handle practical scenarios without prior sensor information along the path, we develop SSF-ACO-Online for real-time scheduling. Extensive simulations demonstrate that SSF-ACO significantly outperforms four other algorithms (i.e., SSF-Only, SSF-GA, SSF-PSO, and SSF-SA) in energy efficiency, and reduces 13.11% energy consumption on average. SSF-ACO-Online achieves comparable performance with energy consumption 1.24% higher than offline counterpart in average.},
  archive      = {J_TMC},
  author       = {Yiqian Wang and Jianping Huang and Feng Shan and Yuming Gao and Runqun Xiong and Junzhou Luo},
  doi          = {10.1109/TMC.2025.3591698},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing joint speed and altitude schedule for UAV data collection in low-altitude airspace},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overlapping coalition game approach for collaborative block mining and edge task offloading in MEC-assisted blockchain networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a promising technology that enhances the efficiency of mobile blockchain networks, by enabling miners, often acted by mobile users (MUs) with limited computing resources, to offload resource-intensive mining tasks to nearby edge computing servers. Collaborative block mining can further boost mining efficiency by allowing multiple miners to form coalitions, pooling their computing resources and transaction data together to mine new blocks collaboratively. Therefore, an MEC-assisted collaborative blockchain network can leverage the strengths of both technologies, offering improved efficiency, security, and scalability for blockchain systems. While existing research in this area has mainly focused on the singlecoalition collaboration mode, where each miner can only join one coalition, this work explores a more comprehensive multicoalition collaboration mode, which allows each miner to join multiple coalitions. To analyze the behavior of miners and the edge computing service provider (ECP) in this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the ECP, as the leader, determines the prices of computing resources for all MUs. In Stage II, each MU decides the coalitions to join, resulting in an overlapping coalition formation (OCF) game; Subsequently, each coalition decides how many edge computing resources to purchase from the ECP, leading to an edge resource competition (ERC) game. We derive the closed-form Nash equilibrium for the ERC game, based on which we further propose an OCFbased alternating algorithm to achieve a stable coalition structure for the OCF game and develop a near-optimal pricing strategy for the ECP's resource pricing problem. Simulation results show that the proposed multi-coalition collaboration mode can improve the system efficiency by 12.64% ∼ 17.63%, compared to the traditional single-coalition collaboration mode.},
  archive      = {J_TMC},
  author       = {Licheng Ye and Zehui Xiong and Lin Gao and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3591822},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An overlapping coalition game approach for collaborative block mining and edge task offloading in MEC-assisted blockchain networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated knowledge distillation using hierarchical reinforcement learning in resource-constrained IoT edge-cloud computing environments. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Federated Learning (FL) in IoT Edge-Cloud Computing environments, mobile terminals are able to cooperate without the leakage on raw data. However, factors including the terminals' high mobility and the network fluctuations make the cooperator selection during FL training extremely complex. Under the distributed cooperation, traditional FL strategies show certain limitations and cannot always select the available nodes when training, leading to the difficulties in energy and latency optimization. In this paper, we propose a Hierarchical Reinforcement Learning (HRL)-based federated knowledge distillation (HRL-FedKD) framework in which both high-level and low-level controllers utilize the Double Deep Q-Network (DDQN) algorithm. The high-level controller selects the nodes participating in FL training, while the low-level controller determines the number of local training epochs for each node. After training, the global model will be compressed into a lightweight model by knowledge distillation (KD) in deployment while preserving the personalization of local models. The experiments were conducted using Chest X-Ray and Brain Tumor MRI datasets to validate the proposed FL strategy. The results demonstrate that the HRL-FedKD framework can effectively optimize latency and energy consumption in complex state spaces.},
  archive      = {J_TMC},
  author       = {Yishan Chen and Zhiqiang Wang and Huashuai Cai and Zhen Qin and Shuiguang Deng},
  doi          = {10.1109/TMC.2025.3591610},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated knowledge distillation using hierarchical reinforcement learning in resource-constrained IoT edge-cloud computing environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-trust based robust federated learning against betrayal behaviors. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its advantage of protecting data privacy and reducing communication overhead, Federated Learning (FL) is becoming a promising machine learning paradigm. However, resource limitations and unstable communication connections on the participating client end can lead to unintentional failures that degrade FL performance. Moreover, as FL systems scale and interconnect increasingly, they face growing exposure to intentional network risks. Furthermore, the assumption of continued trust in historically benign clients introduces vulnerabilities to potential internal betrayal within FL systems. In this paper, we enhance the robustness of FL by incorporating the zero-trust principle, which eliminates implicit trust in clients and mitigates unintentional failures, intentional attacks, and strategic betrayal risks. The framework incorporates dynamic client selection and aggregation weight allocation through trustworthiness evaluation and sustained skepticism toward each potential betrayal behavior. Specifically, a Dirichlet-based trust evaluation technique is presented to update clients' trustworthiness with evolving observations. Then, to reduce potential betrayal loss, we formulate a min-max optimization problem that minimizes the worst-case betrayal loss. Next, we transform the formulation into a convex programming problem for solution. Extensive simulations are conducted to demonstrate the efficacy of the zero-trust based FL in the accurate trust assessment and the system's betrayal-aware robustness enhancement.},
  archive      = {J_TMC},
  author       = {Xinran Zhang and Dan Wang and Yifei Zhu and Weilong Chen and Zheng Chang and Zhu Han},
  doi          = {10.1109/TMC.2025.3591632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Zero-trust based robust federated learning against betrayal behaviors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust lyapunov optimization for LEO satellite networks routing control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellite networks are emerging as crucial components of space-air-ground integrated networks (SAGINs), extending beyond terrestrial capabilities to provide global data transmission services for the Internet of Things (IoT) and mobile devices. The proliferation of connected devices has led to increased data volumes and highly variable, bursty traffic patterns, thus posing significant challenges for network stability and necessitating effective routing control mechanisms. Traditional Lyapunov optimization methods have been fundamental in network optimization, offering stability guarantees under the assumption that traffic flows remain strictly within the network's capacity region. However, this assumption is often violated in LEO satellite networks due to their dynamic and bursty nature, thereby rendering conventional approaches inadequate for ensuring stability. To address this challenge, we propose a robust Lyapunov optimization framework tailored for LEO satellite networks. Our method relaxes the strict requirements of traditional Lyapunov optimization by allowing the network to tolerate finite violations of the capacity region while still ensuring overall system stability. This approach demonstrates that, for a stabilizable network system, it is not necessary for traffic to remain within the capacity region at every time slot. We validate the effectiveness of the proposed robust Lyapunov optimization through extensive simulations under various traffic conditions and LEO satellite network configurations. The results confirm that LEO satellite networks can maintain stability despite finite violations of the capacity region, ensuring reliable performance amid dynamic and bursty traffic demands.},
  archive      = {J_TMC},
  author       = {Zhemin Huang and Zhong-Ping Jiang and Zhu Han and Yong Liu},
  doi          = {10.1109/TMC.2025.3591766},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust lyapunov optimization for LEO satellite networks routing control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated split learning with improved communication and storage efficiency. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is one of the popular distributed machine learning (ML) solutions but incurs significant communication and computation costs at edge devices. Federated split learning (FSL) can train sub-models in parallel and reduce the computational burden of edge devices by splitting the model architecture. However, it still requires a high communication overhead due to transmitting the smashed data and gradients between clients and the server in every global round. Furthermore, the server must maintain separate partial models for every client, leading to a significant storage requirement. To address these challenges, this paper proposes a novel communication and storage efficient federated split learning method, termed CSE-FSL, which utilizes an auxiliary network to locally update the weights of the clients while keeping a single model at the server, hence avoiding frequent transmissions of gradients from the server and greatly reducing the storage requirement of the server. Additionally, a new model update method of transmitting the smashed data in selected epochs can reduce the amount of smashed data sent from the clients. We provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its convergence under non-convex loss functions. The extensive experimental results further indicate that CSE-FSL achieves a significant communication reduction over existing FSL solutions using real-world FL tasks.},
  archive      = {J_TMC},
  author       = {Yujia Mu and Cong Shen},
  doi          = {10.1109/TMC.2025.3591744},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated split learning with improved communication and storage efficiency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS prediction for component services in 5 g via graph-based deep reinforcement learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of Quality of Service (QoS) in terms of response time, packet loss rate, latency and throughput for component services is essential for 5 G to fulfill specific Service Level Agreements (SLAs). However, most current efforts failed to fully exploit the time-varying mobility features of users and parallel iteration multi-rules to perform QoS prediction for component services, incurring poor prediction accuracy. Therefore, in this paper, we are devoted to accurate QoS Prediction of Component Services (QPCS) for 5 G via Graph-based Deep Reinforcement Learning (GDRL) to tackle this issue. Towards this end, the QoS prediction is modeled as GDRL-based QoS tensor factorization by designing a Spatio-Temporal-Recurrent-based Graph Attention Network (STR-GAT) and introducing it into Deep Deterministic Policy Gradient (DDPG) to factorize QoS tensor with multiple available rules in parallel. Specifically, a low-rank QoS tensor and an adjacency tensor are established, which include partial QoS observations of component services in each Base Station (BS), along with some missing elements, and evolving spatial information of users across these BSs, respectively. Then, the novel STR-GAT is designed by introducing spatio-temporal relations into conventional GAT to fully derive the mobility features of users to explore potential actions, while the derivative DDPG is adopted to perform tensor factorization with multiple available rules in parallel. Furthermore, the action smoothing and hierarchical-based replay buffer with priority-based and random sampling are designed and introduced into DDPG to stabilize training process and accelerate model convergence. Experimental simulation results on real-world datasets validate the superiorities of QPCS compared with the state-of-the-art approaches in predicting the QoS of component services in 5 G.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Qifan Wang and Geyong Min and Miao Wang and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2025.3591783},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS prediction for component services in 5 g via graph-based deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAFE: Towards practical WiFi localization via continuous angle focusing effect. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based indoor localization serves as a critical foundation for numerous real-world applications and has attracted widespread attentions over the past decade. Recent advancements have demonstrated the feasibility of achieving decimeter-level accuracy by leveraging Angle of Arrival (AoA) information. However, existing commercial WiFi Access Points (APs) suffer from phase offset across different antennas, which significantly degrade the performance of AoA-based methods. Previous works either relied on labor-intensive manual calibration or involved inaccurate and non-robust automatic calibration, which hinders their widespread use in large-scale deployments. Moreover, to expand the signal coverage and enhance communication performance, the inter-antenna spacing in existing commercial APs typically exceeds the standard half-wavelength. The resulting angle ambiguity problem can mislead target detection results, which has not been well resolved in existing works. To address the above two practical challenges, in this paper, we propose CAFE, a practical WiFi indoor localization system based on the Continuous Angle Focusing Effect. The key insight lies on the fact that the angle information of multiple APs originates from the same client, and thus exhibits highly convergent properties in both the temporal and spatial dimensions. By further exploring the binary nature of phase offset and the periodicity of grating lobes, our approach can efficiently resolve the above two practical challenges. Extensive experiments are provided to demonstrate the effectiveness of the proposed CAFE system, which outperforms state-of-the-art methods by $22.1\%$ in median localization error for simple scenarios and by $37.1\%$ for complex multipath scenarios.},
  archive      = {J_TMC},
  author       = {Shuai Yang and Pengfei Yin and Guanzhong Wang and Tianyu Zhang and Dongheng Zhang and Yan Chen},
  doi          = {10.1109/TMC.2025.3591919},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAFE: Towards practical WiFi localization via continuous angle focusing effect},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing operator intent for haptic teleoperation under high latencies. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic teleoperation is a promising technology with applications in telemaintenance and disaster management. However, it faces significant challenges when the application is subjected to a high network latency and environments with moving objects. This work aims to extend Model Mediated Teleoperation (MMT) to overcome challenges in supporting dynamic environments. Instead of striving for perfect model alignment, we acknowledge the inevitable mismatch between the remote environment and its model at the operator. We propose a set of design principles and an accompanying framework for designing MMT solutions that prioritize operator intent. Our approach is exemplified through an application where an operator, located 8000 km away (The Netherlands – India) and subjected to an average of 179 ms end-to-end latency, guides a robot arm to draw on a whiteboard whose position is actively altered. We evaluate the effectiveness of our approach through a user study. We show a 3-point improvement on a 7-point Likert scale when users utilize our approach to teleoperate over significant network latency of up to 1 second.},
  archive      = {J_TMC},
  author       = {H.J.C. Kroep and P. Makridis and J. Huidobro and K. Wösten and D. Choudhary and N. Gnani and T.V. Prabhakar and S. Coppens and K. Van Berlo and R.Venkatesha Prasad},
  doi          = {10.1109/TMC.2025.3591197},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Utilizing operator intent for haptic teleoperation under high latencies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RFAR: Action recognition based on single tag. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition in classrooms has recently become a research hotspot. Traditional solutions usually rely on sensors or computer vision methods. However, these methods have some disadvantages, such as difficulty in deployment, susceptibility to ambient light, and privacy and security issues. This paper proposes RFAR, a contactless method for classroom action recognition. This method utilizes an RFID tag placed on the desktop to capture various actions and subsequently evaluate the student's learning status. To enhance the reliability of singletag identification, fused data consisting of two or three types of data sequences (RSSI, phase, and Doppler shift) are incorporated. Furthermore, a dynamic antenna system is utilized to identify the optimal angle for tag-antenna alignment. Notably, the single-tagper-person design eliminates severe interference among multiple tags and simplifies device deployment in multi-person scenarios. This method is proposed based on COTS RFID devices and shows high robustness across different environments and equipment. Experimental results show a recognition accuracy of 93.9% in single-person scenarios and 81.5% in five-person scenarios.},
  archive      = {J_TMC},
  author       = {Zhanjun Hao and Jiang Zhang and Xiang Li and Yuejiao Wang and Guowei Wang and Fenfang Li and Hao Liu and Chengrui Tao},
  doi          = {10.1109/TMC.2025.3592191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RFAR: Action recognition based on single tag},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic routing mechanism for load distribution in UAV swarm networks with edge caching. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3589569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of the UAV swarm network has made its widespread application across a multitude of domains. However, the inherently dynamic nature of the network often gives rise to intermittent connectivity issues, leading to a significant reduction in the data transmission capacity. To address this challenge, this study explores the integration of Information-centric Network (ICN) with the delay-tolerant network (DTN). This design aims to enhance message delivery rates by caching content data packets in UAV nodes. Building upon this architecture, we study the congestion control and load balancing problem. We design an on-demand collaborative communication routing algorithm. In our design, we first propose a routing decision model that incorporates multiple routing metrics to capture the dynamic evolution patterns of network nodes, effectively controlling local congestion issues. Subsequently, we employ Lyapunov optimization techniques to achieve a network load balancing. By integrating the Lyapunov drift function, we ensure the stability of a feasible solution space within the model. Additionally, considering the high communication overhead caused by the sparse communication characteristics of DTN, we deploy a Multi-Agent Incentivized Communication (MAIC) algorithm to optimize routing scheduling strategies. Within the MAIC framework, each agent develops unique models for its teammates to generate customized information and minimize network information redundancy. Simulation results demonstrate that this algorithm effectively ensures a congestion control and a load balancing within the UAV swarm network while maintaining communication overhead in routing computations at a minimal level.},
  archive      = {J_TMC},
  author       = {Qun Li and Zunliang Wang and Haipeng Yao and Tianle Mai and Zhipei Li and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3589569},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic routing mechanism for load distribution in UAV swarm networks with edge caching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeFL: Model-agnostic federated learning with generative models. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3592483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed training paradigm that enables collaborative learning across clients without sharing local data, thereby preserving privacy. However, the increasing scale and complexity of modern deep models often exceed the computational or memory capabilities of edge devices. Furthermore, clients may be constrained to use heterogeneous model architectures due to hardware variability (e.g., ASICs, FPGAs) or proprietary requirements that prevent the disclosure or modification of local model structures. These practical considerations motivate the need for model-heterogeneous FL, where clients participate using distinct model architectures. In this work, we propose Generative Model-Aided Federated Learning (GeFL), a framework that enables cross-client knowledge sharing via a generative model trained in a federated manner. This generative model captures global data semantics and facilitates local training without requiring model homogeneity across clients. While GeFL achieves strong performance, empirical analysis reveals limitations in scalability and potential privacy leakage due to generative sample memorization. To address these concerns, we propose GeFL-F, which utilizes feature-level generative modeling. This approach enhances scalability to large client populations and mitigates privacy risks. Extensive experiments across image classification tasks demonstrate that both GeFL and GeFL-F offer competitive performance in heterogeneous settings. Code is available at [1].},
  archive      = {J_TMC},
  author       = {Honggu Kang and Seohyeon Cha and Joonhyuk Kang},
  doi          = {10.1109/TMC.2025.3592483},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GeFL: Model-agnostic federated learning with generative models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divide and conquer: Advancing large-scale multi-agent pathfinding with hierarchical reinforcement learning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3592410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-robot systems face the intricate multi-agent pathfinding (MAPF) challenge as a pivotal hurdle. It has been uncovered through recent research that tackling MAPF issues can be effectively approached through reinforcement learning, offering a fully decentralized solution. Nonetheless, the escalation in the scale of the multi-robot system introduces sample inefficiency, posing a significant barrier for learning-based methods. We introduce a novel hierarchical reinforcement learning architecture aimed at addressing large-scale MAPF by leveraging spatial and temporal abstraction. This approach enhances exploration efficiency by recognizing intermediate rewards. The framework employs an upper-tier controller that segments the map into linked regions, thereby streamlining the optimization of agents' paths on a regional basis to foster improved global outcomes. To tackle each segmented problem, a subordinate-level controller is designed, which integrates heuristic directions and an inter-agent communication strategy. The merit of our methodology is confirmed by empirical experiments, showcasing advancements over prevailing methods in success rates and reduction in completion time across test scenarios of various magnitudes.},
  archive      = {J_TMC},
  author       = {Bing Li and Kaixin Chen and Zhaoyi Song and Rongqing Zhang and Xiang Cheng},
  doi          = {10.1109/TMC.2025.3592410},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Divide and conquer: Advancing large-scale multi-agent pathfinding with hierarchical reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incentive mechanism for federated learning with time-varying client availability. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3592202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), distributed users collaboratively train a neural network model under the coordination of a central server. However, time-varying client availability, coupled with non-independent and non-identically distributed (non-IID) datasets, leads to a biased convergence. In this work, we prove the convergence of FL under time-varying client availability. The theoretical result shows that biased convergence occurs when available client distribution does not algin with the client population distribution. To address this challenge, we propose a pricing-based incentive mechanism to encourage clients to adjust their availability. First, we model the strategic interactions among clients as a non-cooperative game under an arbitrary pricing scheme. We prove that this game is a potential game and its equilibrium can be found through optimization. Second, we derive an optimal pricing scheme for large client populations and propose a bi-level optimization algorithm using Particle Swarm Optimization (PSO) for general scenarios. Through analysis of client availability evolution, we prove the effectiveness of our scheme in mitigating biased convergence. Experimental results using real-world client availability dataset show that our approach addresses time-varying client availability issue, achieving up to 99.5% improvement over benchmarks and enhancing FL convergence rates by up to 2.49 times.},
  archive      = {J_TMC},
  author       = {Shuo Wang and Bing Luo and Ming Tang},
  doi          = {10.1109/TMC.2025.3592202},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An incentive mechanism for federated learning with time-varying client availability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FluidEdge: Expediting serverless machine learning inference via bottleneck-aware auto-scaling on edge SoCs. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3592334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile applications based on machine learning (ML) are increasingly relying on offloading to the edge devices for low-latency, resource-efficient computation. Applying serverless computing for these ML applications on the edge offers a promising solution for handling dynamic workloads while meeting user-specified latency service-level objectives (SLOs). However, existing serverless frameworks, with their coarse-grained data parallelism and rigid model partitioning, are inadequate for ML inference on widely adopted edge System-on-Chip (SoC) devices. This paper presents FluidEdge, an edge-native serverless inference framework. FluidEdge identifies bottleneck operators in ML models and addresses them through a novel fine-grained intra-function latency-sensitive auto-scaling approach that dynamically scales inference bottlenecks during online serving. Additionally, it employs inter-function scaling to further prevent latency SLO violations and leverages the unified memory of edge SoCs for efficient data sharing during inference. Experimental results demonstrate that FluidEdge achieves a 37.4% latency improvement and 67.3%-87.6% SLO violation reduction compared to best-performed state-of-the-art serverless inference frameworks.},
  archive      = {J_TMC},
  author       = {Borui Li and Tiange Xia and Weilong Wang and Jingyuan Zhang and Shuai Wang and Chenhong Cao and Zheng Dong and Shuai Wang},
  doi          = {10.1109/TMC.2025.3592334},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FluidEdge: Expediting serverless machine learning inference via bottleneck-aware auto-scaling on edge SoCs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Throughput-aware cooperative task offloading in dynamic mobile edge computing systems. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3592450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the commercialization of fifth-generation (5G) mobile communication technology and the rapid proliferation of mobile devices (MDs), demand for data computation is surging. This growth increases the reliance of MDs on low latency and high throughput. For this purpose, Mobile Edge Computing (MEC) enhances the user's data processing capability by offloading computation tasks to servers at the network edge. However, achieving high efficiency in task offloading is challenging due to factors such as decision complexity, network dynamics, and user data privacy protection. Additionally, energy causal constraints and the coupling between offloading proportions and resource distribution cannot be ignored. In this paper, we first establish a dynamic task offloading problem to optimize the long-term throughput of the system. Using perturbed Lyapunov optimization, we transform MD delay and energy threshold constraints into the stability control of corresponding virtual queues. Then, we propose the Lyapunov-guided federated deep reinforcement learning (DRL) online task offloading algorithm called LyFOTO, which combines a federated learning (FL) framework and an Actor-Critic (AC) model. Under favorable communication conditions, the LyFOTO algorithm adaptively boosts system throughput; under poorer conditions, it properly delays task offloading, without violating queue backlog constraints. Through mathematical analysis, we discuss the performance of the LyFOTO algorithm. Simulation experiments validate that LyFOTO effectively balances system throughput and device battery energy. Finally, Comparative results show that LyFOTO outperforms other benchmark algorithms in maximizing system throughput while ensuring task backlog and energy threshold constraints.},
  archive      = {J_TMC},
  author       = {Longbao Dai and Fanzi Zeng and Haoran Kong and Jianghao Cai and Hongbo Jiang and Keqin Li},
  doi          = {10.1109/TMC.2025.3592450},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Throughput-aware cooperative task offloading in dynamic mobile edge computing systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Squeezer: Efficient multi-DNN inference for edge video analytics via cross-model scheduling. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analytics at the edge is becoming increasingly prevalent in many scenarios, such as smart campuses and intelligent factories. These applications often consist of multiple subtasks, which necessitates the optimization for multi-DNN (Deep Neural Network) inference. Due to limited consideration over cross-model scheduling, current practices cannot fully leverage available computing resources, leading to suboptimal performance. To address this, we propose Squeezer, a multiDNN serving framework that holistically schedules multiple DNN models on an edge server with a single GPU. Squeezer decouples the cross-model scheduling into a two-layered approach, which involves (1) balanced operator grouping which partitions operators of multiple DNN models into groups, significantly reducing the scheduling complexity and (2) kernel scheduler which orchestrates parallel execution within each group by considering the interplay among kernels running in parallel, thereby enabling cross-model optimizations in multi-DNN inference. Performance evaluation results demonstrate that Squeezer outperforms state-of-the-art baselines, achieving up to 1.91× improvement in system throughput.},
  archive      = {J_TMC},
  author       = {Xiang Wang and Lingxiao Ma and Ziyan Fu and Xiangyu Li and Yuanchun Li and Ju Ren and Yaoxue Zhang and Yunxin Liu},
  doi          = {10.1109/TMC.2025.3592647},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Squeezer: Efficient multi-DNN inference for edge video analytics via cross-model scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing communication and device clustering for clustered federated learning with differential privacy. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.},
  archive      = {J_TMC},
  author       = {Dongyu Wei and Xiaoren Xu and Shiwen Mao and Mingzhe Chen},
  doi          = {10.1109/TMC.2025.3592885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing communication and device clustering for clustered federated learning with differential privacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing age of semantic information for analytics-oriented video streaming systems. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video streaming systems are critical for intelligent applications to transmit video data from end devices to servers for real-time analysis. In contrast to traditional humancentric streaming systems, which prioritize user-perceived metrics, machine-centric streaming systems are designed to continuously provide fresh and accurate information for analytics purposes. Although numerous studies have investigated policies to optimize streaming performance, most of them employ the segment-by-segment streaming framework from human-centric systems. Through comprehensive theoretical analysis and experimentation, we uncover that the segmented streaming approach is sub-optimal for machine-centric streaming systems compared to the straightforward frame-by-frame streaming approach. Furthermore, instead of relying on conventional frame-level metrics, we introduce a novel metric called the Age of Semantic Information (AoSI) to evaluate the performance of analyticsoriented streaming systems. This metric balances the quantity and timeliness of the semantic information. Consequently, we propose a compression ratio adaption method tailored to optimize AoSI performance for frame-by-frame streaming systems. This method leverages a deep learning (DL)-based predictor to discover the dynamic, latent relationships between compression and inference accuracy. Evaluated on actual streaming prototypes and real-world datasets, our method significantly surpasses both segmented and frame-by-frame baseline methods in terms of worst-case and average AoSI performance.},
  archive      = {J_TMC},
  author       = {Ziyao Huang and Weiwei Wu and Kui Wu and Guanyu Gao and Jianping Wang},
  doi          = {10.1109/TMC.2025.3588474},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Minimizing age of semantic information for analytics-oriented video streaming systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain mmWave gesture recognition via parameter-free attention under human activity interference. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3592959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition provides an effective human-computer interaction that makes device control more intuitive and convenient. Although the research on mmWave radar-based gesture recognition has demonstrated promising results, existing studies have exclusively addressed the cross-domain challenge or the human activity interference problem, and no attention has been paid to the cross-domain problem in the presence of human activity interference. To address these issues, we propose a novel mmWave radar-based gesture recognition system, named GestSAM, which leverages a parameter-free attention mechanism to effectively extract gesture features that are less affected by environmental noise. By integrating this mechanism with deep learning techniques, GestSAM significantly reduces the impact of human activity interference while maintaining robust cross-domain gesture recognition performance. This approach ensures robust, high-accuracy recognition of gestures. In order to evaluate the performance of our system, we construct a dataset containing six different gesture types performed by fifteen volunteers in seven different scenarios and simulate three interference conditions. The experimental results show that under human activity interference, the model achieves average recognition accuracies of 92.79% and 94.62% in cross-user and cross-scenario, respectively.},
  archive      = {J_TMC},
  author       = {Yunyi Li and Yang Yang and Lian Xiao and Shuai Wang and Linqing Gui and Fu Xiao},
  doi          = {10.1109/TMC.2025.3592959},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cross-domain mmWave gesture recognition via parameter-free attention under human activity interference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced semi-supervised learning for WiFi gesture recognition via dynamic threshold-based spatio-temporal attention networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3592965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi sensing advancements facilitate the capture of human gestures from wireless signals, ensuring both privacy preservation and robustness under low-light conditions. Deep learning-based WiFi Human Gesture Recognition (HGR) demonstrates remarkable performance in handling complex gestures. To reduce labeling efforts, recent years have seen the emergence of semi-supervised WiFi HGR, leveraging massive amounts of unlabeled data. However, existing semi-supervised schemes often assume a balanced class distribution and utilize a fixed threshold for selecting pseudo-labels of unlabeled samples, leading to low performance for minority classes and decreased model generalization on real-world imbalanced datasets. To address this issue, we propose a novel semi-supervised WiFi HGR approach with dynamic pseudo-labeling thresholds to handle imbalanced class distribution, incorporating Spatial-Temporal Attention (STA) networks. Unlike using a fixed threshold for all unlabeled samples, our design implements class-independent thresholds for different classes, dynamically adjusting them by encoding pseudo-label distribution during training. To emphasize critical features in informative areas within the WiFi signals, we incorporate both spatial self-attention and temporal attention mechanisms to dynamically learn salient features and identify pivotal frames, respectively. Moreover, we introduce adaptive WiFi data augmentations that propel the semi-supervised framework and enhance model robustness. Experimental results on the Widar3.0 dataset reveal that our approach outperforms existing semi-supervised methods by large margins in accuracy, effectively mitigating imbalanced bias and enhancing model generalization. The code is publicly at https://github.com/onlinehuazai/Semi-Fi.},
  archive      = {J_TMC},
  author       = {Qihua Feng and Chunhui Duan and Jiawei Xue and Chaozhuo Li and Feiran Huang and Xi Zhang and Jian Weng and Philip S. Yu},
  doi          = {10.1109/TMC.2025.3592965},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Imbalanced semi-supervised learning for WiFi gesture recognition via dynamic threshold-based spatio-temporal attention networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of information in a fully-prioritized network. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3592939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the age of information (AoI) in a network consisting of multiple information streams with different priorities sharing a common server. In this network, the transmission of a packet is interrupted whenever a higher priority packet arrives. Regarding the behavior of AoI, for each stream, we consider a single buffer to enqueue the incoming packets based on a quasi-blocking (QB) policy. With the assumption of Poisson packet arrivals, we formulate the AoI and PAoI moment generating functions for each stream, while no assumption is considered for the service times of the packets. We also introduce a new semantic queueing policy. In this respect, we define the semantic (i.e., significance) of a packet as a linear combination of its age and remaining transmission time. When this metric is lower for a packet, the packet is more significant. In this policy, the decision to replace the available packet of a stream with a new arriving one is made based on the significance of these packets. This decision minimizes the sum of the next local peak and end-to-end delay in the AoI function. We investigate the superiority of this policy to the traditional ones in our numerical results.},
  archive      = {J_TMC},
  author       = {Sepehr Asvadi and Farid Ashtiani},
  doi          = {10.1109/TMC.2025.3592939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of information in a fully-prioritized network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time MU-MIMO beamforming with limited channel samples in 5G networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3592929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MU-MIMO beamforming is a key technology for 5G networks, relying on Channel State Information (CSI). However, in practice, the estimated CSI in reality is prone to uncertainty. Further, a MU-MIMO beamforming solution must be derived within a millisecond to be useful for real-time 5G applications. We present ReDBeam—a real-time data-driven beamforming solution for MU-MIMO using limited CSI data samples. The main novelties of ReDBeam are a parallel algorithm and an optimized GPU implementation. ReDBeam delivers a MU-MIMO beamforming solution within 1 millisecond to meet the probabilistic data rate requirements from the users, and minimize a base station's power consumption. Through extensive experiments, we show that ReDBeam consistently meets the stringent 1- millisecond real-time requirement and is orders of magnitude faster than other state-of-the-art algorithms. ReDBeam conclusively demonstrates that MU-MIMO beamforming with data rate requirements can be achieved in real-time using only limited CSI data samples.},
  archive      = {J_TMC},
  author       = {Shaoran Li and Nan Jiang and Chengzhang Li and Shiva Acharya and Yubo Wu and Weijun Xie and Wenjing Lou and Y. Thomas Hou},
  doi          = {10.1109/TMC.2025.3592929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time MU-MIMO beamforming with limited channel samples in 5G networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal planning for heterogeneous smart radio environments. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Radio Environment (SRE) is a central paradigm in 6G and beyond, where integrating SRE components into the network planning process enables optimized performance for high-frequency Radio Access Network (RAN). This paper presents a comprehensive planning framework utilizing realistic urban scenarios and channel models to analyze diverse SRE components, including Reconfigurable Intelligent Surface (RIS), Network-Controlled Repeater (NCR), and advanced technologies like Simultaneous Transmitting and Reflecting RIS (STAR-RIS) and Trisectoral NCR (NCR). We propose two optimization strategies— Full Coverage Minimum Cost (FCMC) and Maximum Budget-Constrained Coverage (MBCC)—that address key cost and coverage objectives by considering both physical characteristics and scalable costs of each component, influenced by factors such as NCR amplification gain and RIS dimensions. Extensive numerical results demonstrate the significant impact of these models in enhancing network planning efficiency for high-density urban environments.},
  archive      = {J_TMC},
  author       = {Reza Agahzadeh Ayoubi and Eugenio Moro and Marouan Mizmizi and Dario Tagliaferri and Ilario Filippini and Umberto Spagnolini},
  doi          = {10.1109/TMC.2025.3593191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimal planning for heterogeneous smart radio environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency minimization for movable relay-aided D2D-MEC communication systems. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-to-device (D2D)-aided mobile edge computing (MEC) has emerged as a key enabling technology for future sixth-generation (6G) wireless networks. The goal of D2D-MEC is to reduce system latency for edge user equipments (UEs) by enabling access to cloud computing capabilities at the network edge, thereby supporting high transmission rates. To address the vulnerability of communication signals to physical obstructions, we employ relay techniques to enhance system performance and extend coverage. However, relay nodes and base station (BS) are typically equipped with large-scale antenna arrays, which lead to significant implementation costs and limiting practical deployment. To address this issue in a cost-efficient manner without sacrificing system performance, movable antenna (MA) technology is introduced. The key idea of MA technology lies in dynamically optimizing antenna positions to improve system capacity. Therefore, we propose a novel resource allocation framework for an movable relay-aided D2D-MEC system. The proposed scheme jointly optimizes the MA positions at UEs, relays, and the BS, along with the associated beamforming vectors, MEC server resource allocation, and computational task offloading rates. The objective is to minimize the maximum system latency while satisfying both computation and communication rate constraints. Furthermore, considering that current MA control mechanisms primarily rely on mechanical actuation, MA movement delay is incorporated into the latency model to capture the trade-off between antenna mobility and system delay. The resulting optimization problem is non-convex and involves multiple coupled variables. To solve this problem, we develop a parallel and distributed algorithm based on the penalty dual decomposition (PDD) framework, which is further integrated with the successive convex approximation (SCA) method to obtain a suboptimal solution. Simulation results demonstrate that the proposed algorithm significantly reduces system latency and enhances overall efficiency compared to benchmark schemes employing conventional fixed-position antennas (FPAs) at the relays and BS.},
  archive      = {J_TMC},
  author       = {Yue Xiu and Yang Zhao and Ran Yang and Huimin Tang and Long Qu and Maurice Khabbaz and Chadi Assi and Ning Wei},
  doi          = {10.1109/TMC.2025.3593263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Latency minimization for movable relay-aided D2D-MEC communication systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time scheduling of CPU/GPU heterogeneous tasks in dynamic IoT systems: Enhancing GPU and memory efficiency. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time processing of large-scale, heterogeneous tasks—including CPU-only, general-purpose GPU, and specialized GPU tasks—poses significant challenges in Internet of Things (IoT) systems, driven by severe GPU resource fragmentation, inefficient CPU and memory resource utilization on edge servers. These issues often compromise system processing performance and server stability. To address these issues, we formulate a multi-stage mixed-integer nonlinear programming (MINLP) model, to jointly optimize GPU fragmentation rate and system processing capability. We then introduce a novel deviation-based Lyapunov optimization framework that explicitly maintains memory utilization around a predefined optimal threshold, effectively balancing resource usage and system stability. Finally, to achieve real-time decision-making for massive tasks in dynamic systems with randomly arriving tasks, we propose the MA-LHTO algorithm, a multi-agent deep reinforcement learning approach that incorporates a multi-head architecture, entropy-based exploration, and a parameter reset mechanism. Experimental results confirm that our algorithm significantly improves resource utilization, and exhibits good performance under various working conditions.},
  archive      = {J_TMC},
  author       = {Xiao He and Shanchen Pang and Sibo Qiao and Haiyuan Gui and Shihang Yu and Joel J. P. C. Rodrigues and Shahid Mumtaz and Zhihan Lyu},
  doi          = {10.1109/TMC.2025.3593250},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time scheduling of CPU/GPU heterogeneous tasks in dynamic IoT systems: Enhancing GPU and memory efficiency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RL-driven distributed on-orbit sparse coding for mobile space situational awareness. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3593228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space Situational Awareness (SSA) relies on Low Earth Orbit (LEO) satellites to capture continuous, highresolution imagery critical for identifying space threats. The vast volume of SSA images overwhelms satellite network throughput, hindering timely transmission and processing. This paper presents a reinforcement learning (RL)-driven distributed sparse coding framework that integrates novel compression algorithms with orbital RL to address these challenges. First, we introduce an Aggregated Dictionary Learning (ADL) algorithm and a Context-aware Adaptive Binary Arithmetic Coding (CABAC) algorithm, achieving a 93.78% compression ratio by exploiting the high sparsity and spatiotemporal redundancy of SSA images. Second, the proposed compression workflow is deployed across LEO satellites in a distributed manner, where both overlapping and non-overlapping regions of images are dynamically partitioned and processed in parallel to optimize resource utilization and reduce latency. An Orbital Double Deep Q-Network (DQN) framework is proposed to optimize task offloading decisions by (1) integrating orbital dynamics into the state space, and (2) adaptively partitioning images based on visible LEO resources. Evaluations demonstrate that our framework achieves 100% task completion under visibility constraints and a 51.61% reduction on CPU and RAM occupation time compared to centralized processing.},
  archive      = {J_TMC},
  author       = {Yutong Liu and Haiming Jin and Yunxiang Chen and Yinjie Wang Yao and Yimin Zhao and Linghe Kong and Lei Dong and Rui Li and Xiaoyang Liu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3593228},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RL-driven distributed on-orbit sparse coding for mobile space situational awareness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User context generation for large language models from mobile sensing data. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3591561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) exhibit remarkable capabilities in natural language understanding and generation. However, the accuracy of the inference depends deeply on the contexts of queries, especially for personal services. Abundant mobile sensing data collected by sensors embedded in smart devices can proactively capture real-time user contexts. However, raw sensing data are low-quality (e.g., existing missing data and data redundancy) and are incapable of providing accurate contexts. In this work, we present ConGen, a user context generation framework for LLMs, aiming at prompting users' contexts through their implicit mobile sensing information. ConGen integrates two components: refined data completion and multi-granularity context compression. Specifically, the refined data completion couples data-centric feature selection by leveraging the eXplainable AI (XAI) method into the data imputation model to generate fewer but more informative features for efficient and effective context generation. Additionally, we implement multi-granularity context compression, reducing timestep- and context-level data redundancy while further elevating context quality. Experiment results show that ConGen can generate more accurate context, surpassing competitive baselines by 1.3%-8.3% in context inference on all four datasets. Moreover, context compression significantly reduces redundancy to $1/70\sim 1/40$ of the original data amount, and further improves the context accuracy. Finally, the enhanced performance of LLMs, as demonstrated by both quantitative and qualitative evaluations of prompting ConGen-generated user contexts, underscores the effectiveness of ConGen.},
  archive      = {J_TMC},
  author       = {Rui Xing and Zhenzhe Zheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3591561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User context generation for large language models from mobile sensing data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FM-fi 2.0: Foundation model for cross-modal multi-person human activity recognition. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio-Frequency (RF)-based Human Activity Recognition (HAR) rises as a promising solution when low-light, obstructions, or privacy concerns render computer vision impractical. However, the scarcity of labeled RF data due to their non-interpretable nature poses a significant obstacle. Thanks to the recent breakthrough of foundation models (FMs), extracting deep semantic insights from unlabeled visual data become viable, yet these vision-based FMs fall short when applied to small RF datasets. To bridge this gap, we introduce FM-Fi 2.0, an innovative cross-modal framework engineered to translate the knowledge of vision-based FMs for enhancing RF-based, multi-person HAR systems. FM-Fi 2.0 first employs the intrinsic capabilities of FM and RF modality to associate both intra- and cross-modal features of each subject, while simultaneously filtering out irrelevant features to achieve better alignment between the two modalities. FM-Fi 2.0 also employs a cross-modal contrastive knowledge distillation mechanism, enabling an RF encoder to inherit the interpretative power of FMs for achieving zero-shot learning. The framework is further refined through metric-based few-shot learning techniques, aiming to boost the performance for predefined HAR tasks. Comprehensive evaluations evidently indicate that FM-Fi 2.0 rivals the effectiveness of vision-based methodologies, and the evaluation results provide empirical validation of FM-Fi 2.0's generalizability across various environments.},
  archive      = {J_TMC},
  author       = {Yuxuan Weng and Tianyue Zheng and Yanbing Yang and Jun Luo},
  doi          = {10.1109/TMC.2025.3593406},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FM-fi 2.0: Foundation model for cross-modal multi-person human activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Game theory and trust management driven dynamic proof-of-work blockchain consensus algorithm for securing internet of vehicles. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3592973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the dynamic Proof of Work (PoW) mechanism has attracted considerable attention as a promising consensus mechanism for implementing blockchain in the Internet of Vehicles (IoV) due to its superior scalability, security, and efficiency. However, existing protocols fail to address two critical issues, i.e., how to precisely quantify the relationship between dynamic difficulty and trust value and how to assess the motivation for mining in relation to mining costs. Addressing these issues is crucial for developing a secure, efficient, and adaptable dynamic PoW mechanism. To address these issues, we propose a trust management algorithm that assesses the trustworthiness of Roadside Units (RSUs) that function as consensus or mining nodes. Based on the results of this trust evaluation, a difficulty adjustment algorithm rooted in the Principal-Agent game theory was designed to implement a dynamic PoW mechanism for IoV. This approach leads to a Nash equilibrium between vehicles and RSUs, thereby ensuring the security, efficiency, and stability of the IoV blockchain system. Simulations demonstrated that the proposed dynamic PoW consensus algorithm significantly improves consensus efficiency, enhances system security, and reduces computational costs, offering a robust solution for data sharing in IoV.},
  archive      = {J_TMC},
  author       = {Lu Wei and Yuanzhi Cao and Jie Cui and Hong Zhong and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3592973},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Game theory and trust management driven dynamic proof-of-work blockchain consensus algorithm for securing internet of vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeMo: Experiences of deploying a large-scale indoor delivery monitoring system. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The delivery of goods to numerous indoor stores poses significant safety risks, with heavy, high-stacked packages on delivery trolleys posing a potential hazard to passersby. This paper reports our experiences of developing and operating DeMo, a practical system for real-time monitoring of indoor delivery. DeMo employs sensors attached to trolleys, utilizing Inertial Measurement Unit (IMU) and Bluetooth Low Energy (BLE) readings to detect delivery violations, such as speeding and the use of non-designated delivery paths, and ensure accurate matching of each delivery to its intended destination store. Unlike typical indoor localization applications, DeMo addresses unique challenges, including sensor placement and the complex electromagnetic characteristics encountered in underground settings. Specifically, DeMo adapts the classical logarithmic radio signal model to facilitate fingerprint-free localization, significantly reducing deployment and maintenance costs. DeMo has been operating since May 2020, covering more than 200 shops with 74,537 deliveries (6193.2 km) across 12 subway stations in Hong Kong. DeMo's 4-year operation witnessed a significant violation rate drop, from 19% (May 2020) to 0.9% (Mar 2024).},
  archive      = {J_TMC},
  author       = {Xiubin Fan and Zhongming Lin and Yuming Hu and Zhiqing Hong and Tianrui Jiang and Feng Qian and Zhimeng Yin and S.-H. Gary Chan and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2025.3588900},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeMo: Experiences of deploying a large-scale indoor delivery monitoring system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and fast authentication protocol for digital healthcare services. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the Internet of Medical Things (IoMT) and cloud computing, ensuring secure communication in e-health systems has become increasingly critical. However, many existing authentication solutions suffer from excessive overhead and security vulnerabilities. To address these challenges, we present a lightweight, high-speed authentication protocol that relies on secure hash functions and XOR operations, facilitating efficient mutual authentication among users, trusted servers, and medical servers while establishing session keys for data exchange. We then rigorously assess our protocol's security against a comprehensive threat model, employing both informal methods and formal analyses, including Real-Or-Random (ROR) model, BAN logic, and automated verification via ProVerif. The results demonstrate that our protocol remains resilient against known attacks and satisfies e-health security standards. Furthermore, a detailed performance comparison reveals that our approach significantly reduces some costs compared to existing schemes, while reinforcing security and privacy protections.},
  archive      = {J_TMC},
  author       = {Weizheng Wang and Qipeng Xie and Hongyang Du and Lejun Zhang and Joel J. P. C. Rodrigues},
  doi          = {10.1109/TMC.2025.3593533},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Lightweight and fast authentication protocol for digital healthcare services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed asynchronous service provisioning in edge-cloud multi-tier networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3593592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge-cloud multi-tier networks, datacenters must dynamically deploy and migrate services to meet diverse latency and computational constraints, especially for mobile users. Traditional solutions rely on centralized orchestration with full network visibility, which is impractical at scale and introduces reliability and privacy concerns. We propose DASDEC, a fully distributed and asynchronous framework for service placement and migration that operates without global coordination or private data exchange between providers. DASDEC enables local decision-making using lightweight control messages and supports cooperative placement across datacenters operated by distinct entities. Extensive simulations using real-world mobility traces show that DASDEC achieves performance very close to those obtained by optimal centralized solutions, while incurring negligible control-plane overhead (${\sim }100$ bytes per request). These results highlight DASDEC's scalability, resilience, and practical applicability to federated edge-cloud systems.},
  archive      = {J_TMC},
  author       = {Itamar Cohen and Antonio Calagna and Paolo Giaccone and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3593592},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed asynchronous service provisioning in edge-cloud multi-tier networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COTRA: A data trading framework for multi-source data cooperation. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data trading significantly enhances data-driven technologies by enabling efficient data sharing across mobile devices and communication systems. Despite the clear advantages of incorporating multiple sources often experienced by data users, the topic of multi-source data trading remains largely underexplored. This paper designs a data trading framework that enables multi-source data trading through structured and adaptive cooperation among data sources. The proposed framework aims to enhance data usage efficiency and seller revenue. Key technical challenges addressed include the complex interactions between data sources and buyers, the coupling among diverse data products, and the inherent non-convexity in optimization problems. We employ a Nash bargaining framework to model cooperative seller decisions and a two-stage Stackelberg game for dynamic seller-buyer interactions. By systematically addressing the coupling among data products, we derive closed-form solutions despite the non-convex nature of the optimization problem. Our results reveal that seller revenue remains stable with increasing product coupling until it reaches a threshold, beyond which further coupling increases revenue due to the substitute effect. Adaptive cooperation exhibits greater scalability and resilience, particularly in environments with numerous data sources and complex product interdependencies, while structured cooperation provides more predictability and efficiency in stable conditions. Experimental results demonstrate that this framework can improve seller profits by up to 46.32% compared to existing data trading methods in the current market.},
  archive      = {J_TMC},
  author       = {Jin Cheng and Ningning Ding and John C.S. Lui and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3593986},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {COTRA: A data trading framework for multi-source data cooperation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight federated learning over wireless edge networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3594102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of smart devices connected to wireless networks, data production is increasing rapidly, requiring machine learning (ML) techniques to unlock its value. However, the centralized ML paradigm raises concerns over communication overhead and privacy. Federated learning (FL) offers an alternative at the network edge, but practical deployment in wireless networks remains challenging. This paper proposes a lightweight FL (LTFL) framework integrating wireless transmission power control, model pruning, and gradient quantization. We derive a closed-form expression of the FL convergence gap, considering transmission error, model pruning error, and gradient quantization error. Based on these insights, we formulate an optimization problem to minimize the convergence gap while meeting delay and energy constraints. To solve the non-convex problem efficiently, we derive closed-form solutions for the optimal model pruning ratio and gradient quantization level, and employ Bayesian optimization for transmission power control. Extensive experiments on real-world datasets show that LTFL outperforms state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Xiangwang Hou and Jingjing Wang and Jun Du and Chunxiao Jiang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3594102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Lightweight federated learning over wireless edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REDS: Resource-efficient deep subnetworks for dynamic resource constraints. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models that are not capable to adapt at runtime. In this work, we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS leverages structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieves computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) taking advantage of data cache by re-arranging the order of operations in REDS computational graph. REDS supports conventional deep networks frequently deployed on the edge and provides computational benefits even for small and simple networks. We evaluate REDS on eight benchmark architectures trained on the Visual Wake Words, Google Speech Commands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four off-the-shelf mobile and embedded hardware platforms. We provide a theoretical result and empirical evidence demonstrating REDS' outstanding performance in terms of submodels' test set accuracy, and demonstrate an adaptation time in response to dynamic resource constraints of under 40$\mu$s, utilizing a fully-connected network on Arduino Nano 33 BLE.},
  archive      = {J_TMC},
  author       = {Francesco Corti and Balz Maag and Joachim Schauer and Ulrich Pferschy and Olga Saukh},
  doi          = {10.1109/TMC.2025.3594214},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REDS: Resource-efficient deep subnetworks for dynamic resource constraints},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erasure coding-based cost-optimized and latency-aware data storage in UAV-enabled edge systems. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV-enabled edge storage systems provide data storage services to users by deploying UAVs in areas lacking infrastructure coverage, overcoming delay limitations and improving Quality of Service (QoS). Most existing studies focus on storing replicas on UAVs to ensure low-latency data access. Nonetheless, replica-based strategies incur high storage cost, posing significant challenges for UAVs with limited storage resources. In this paper, we introduce erasure coding into the UAV-enabled edge storage system, aiming to reduce user data access latency while minimizing storage cost. However, the mobility of users and the non-fully-connected nature of the UAV network pose new challenges for the coupled decisions of data encoding, block placement, and access. In this paper, we propose a Mobility-Enhanced Hierarchical Deep Reinforcement Learning algorithm (ME-HDRL). Specifically, we design a trajectory prediction algorithm combining CNN and ConvLSTM to account for user mobility in decision-making. We further decompose the original problem into two subproblems: data encoding and placement, as well as block access. A hierarchical deep reinforcement learning algorithm involving multiple UAV agents and an edge agent is proposed to collaboratively learn optimal decisions. To improve the convergence of the algorithm, we design an invalid action filter to reduce the action space. Experimental results show that our approach outperforms existing rule-based and reinforcement learning-based algorithms in various scenarios, exhibiting significant convergence improvements and a substantial reduction in both storage cost and user data access latency.},
  archive      = {J_TMC},
  author       = {Zhaoxiang Huang and Zhiwen Yu and Liang Wang and Huan Zhou and Erhe Yang and and Bin Guo},
  doi          = {10.1109/TMC.2025.3594283},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Erasure coding-based cost-optimized and latency-aware data storage in UAV-enabled edge systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based user scheduling for collaborative perception. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stand-alone perception systems in autonomous driving suffer from limited sensing ranges and occlusions at extended distances, potentially resulting in catastrophic outcomes. To address this issue, collaborative perception is envisioned to improve perceptual accuracy by using vehicle-to-everything (V2X) communication to enable collaboration among connected and autonomous vehicles and roadside units. However, due to limited communication resources, it is impractical for all units to transmit sensing data such as point clouds or high-definition video. As a result, it is essential to optimize the scheduling of communication links to ensure efficient spectrum utilization for the exchange of perceptual data. In this work, we propose a deep reinforcement learning-based V2X user scheduling algorithm for collaborative perception. Given the challenges in acquiring perceptual labels, we reformulate the conventional label-dependent objective into a label-free goal, based on characteristics of 3D object detection. Incorporating both channel state information (CSI) and semantic information, we develop a double deep Q-Network (DDQN)-based user scheduling framework for collaborative perception, named SchedCP. Simulation results verify the effectiveness and robustness of SchedCP compared with traditional V2X scheduling methods. Finally, we present a case study to illustrate how our proposed algorithm adaptively modifies the scheduling decisions by taking both instantaneous CSI and perceptual semantics into account.},
  archive      = {J_TMC},
  author       = {Yandi Liu and Guowei Liu and Le Liang and Hao Ye and Chongtao Guo and Shi Jin},
  doi          = {10.1109/TMC.2025.3594222},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep reinforcement learning-based user scheduling for collaborative perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmBP+: Contact-free blood pressure measurement using millimeter-wave radar. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood pressure (BP) measurement is an indispensable tool in diagnosing and treating many diseases such as cardiovascular failure and stroke. Traditional direct measurement can be invasive, and wearable-based methods may have limitations of discomfort and inconvenience. Contact-free BP measurement has been recently advocated as a promising alternative. In particular, Millimeter-wave (mmWave) sensing has demonstrated its promising potential, however it is confronted with several challenges including noise and vulnerability to human's tiny motions which may occur intentionally and inevitably. In this paper, we propose mmBP+, a contact-free mmWave-based BP measurement system with high accuracy and motion robustness. Due to the high frequency and short wavelength, mmWave signals received in the time domain are dramatically susceptible to ambient noise, and deteriorating signal quality. To reduce noise,we propose a novel approach to exploit mmWave signal's characteristics and features in the delay-Doppler-fractional Fourier domain to significantly improve signal quality for pulse waveform construction. We also propose a periodic signal feature based functional link adaptive filter leveraging on the periodic and correlation characteristics of pulse waveform signals to alleviate the impact of human's tiny motions. Extensive experiment results achieved by the leave-one-out cross-validation (LOOCV) method demonstrate that mmBP+ achieves the mean errors of 0.65mmHg and 1.31mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively; and the standard deviation errors of 3.92mmHg and 3.99mmHg for SBP and DBP, respectively.},
  archive      = {J_TMC},
  author       = {Zhenguo Shi and Tao Gu and Yu Zhang and Xi Zhang},
  doi          = {10.1109/TMC.2025.3594316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmBP+: Contact-free blood pressure measurement using millimeter-wave radar},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deterministic-latency MAC protocol for future automotive ethernet. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3593939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive Ethernet as an in-vehicle networking paradigm has gradually become the main automotive backbone network. However, with the ever-increasing time-critical in-vehicle traffic, it is being confronted with enormous challenges to realize deterministic-latency communications, due to the inherent limitations of its distributed network architecture and the adopted MAC protocols, including limited-computing power, low-speed and unreliable traffic transmission. Furthermore, it is often incompatible with emerging vehicular functions and protocols, which can provide tremendous potential for better vehicular Quality-of-Service (QoS). Therefore, in this paper, we first design a Future Automotive Ethernet (FAE) architecture and then, built on the representative Time-Sensitive Networking (TSN) and industrial summation frame, propose a Deterministic-Latency MAC (DLM) protocol running in FAE to tackle these issues. The FAE architecture includes three functional domains, three in-vehicle computing units, and no fewer than three intelligent network processing modules, which can integrate with the emerging LAN protocols to realize cross-domain and intra-domain Ethernet-based communications. Under the umbrella of this architecture, DLM classifies the driving situations into driving, reversing, left turn, right turn and parking states, following the real-world vehicle behaviors, and assigns all traffic associated with driving safety in five states different recommended priorities to be delivered. Furthermore, an optimized deterministic-latency mechanism integrated with TSN and the industrial summation frame is developed to realize the timely and accurate transmission of high-priority and medium/low-priority traffic. Simulation results obtained from diversified scenarios demonstrate that the proposed DLM running in FAE can significantly improve transmission latency determinacy and reliability compared with the existing technical strategies.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Jieling Lei and Bang Wu and Geyong Min and Wang Miao},
  doi          = {10.1109/TMC.2025.3593939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A deterministic-latency MAC protocol for future automotive ethernet},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRAM: Digital twin-driven double-layer reverse auction method for multi-platform vehicular crowdsensing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3594488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, For-Hire Vehicles (FHVs) have emerged as major players in Vehicular CrowdSensing (VCS). However, the heterogeneity of tasks issued by Data Requesters (DRs) and the heterogeneity of sensors equipped on FHVs under different Vehicle Platforms (VPs) bring difficulties to task allocation and execution. It can be concluded that it is important to reasonably analyze the relationship among DRs, VPs, and FHVs, as well as to motivate VPs and FHVs to complete sensing tasks. Therefore, taking advantage of the real-time simulation and intelligent decision-making of Digital Twins (DT), this paper proposes a DT-driven Double-layer Reverse Auction Method (DRAM). In the first layer, the reverse auction is established between each DR and VPs, and in the second layer, the reverse auction is established between each VP and FHVs. Meanwhile, we also introduce a sensing fairness index to ensure the sensing balance of different sub-regions and consider it in the DRAM process. Here, the idea of backward induction is used to solve the above problems, with the goal of minimizing the overhead of winning VP and the average overhead of all DRs. Finally, the effectiveness of the DRAM proposed in this paper is verified based on the real data set. Compared with the baseline method, DRAM can reduce the average overhead of DR by about 4%-25%. Meanwhile, in terms of sensing fairness, it can be improved by up to 55%.},
  archive      = {J_TMC},
  author       = {Zhenning Wang and Yue Cao and Huan Zhou and Xiaokang Zhou and Jiawen Kang and Houbing Song},
  doi          = {10.1109/TMC.2025.3594488},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRAM: Digital twin-driven double-layer reverse auction method for multi-platform vehicular crowdsensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task assignment and exploration optimization for low altitude UAV rescue via generative AI enhanced multi-agent reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3594188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of emerging uncrewed aerial vehicle (UAV) with artificial intelligence (AI) and ground-embedded robots (GERs) has transformed emergency rescue operations in unknown environments. However, the high computational demands of such missions often exceed the capacity of a single UAV, making it difficult for the system to continuously and stably provide high-level services. To address these challenges, this paper proposes a novel cooperation framework involving UAVs, GERs, and airships. This framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship (U2A) communications, providing computing services for UAV offloaded tasks. Specifically, we formulate the multi-objective optimization problem of task assignment and exploration optimization in UAVs as a dynamic long-term optimization problem. Our objective is to minimize task completion time and energy consumption while ensuring system stability over time. To achieve this, we first employ the Lyapunov optimization method to transform the original problem, with stability constraints, into a per-slot deterministic problem. We then propose an algorithm named HG-MADDPG, which combines the Hungarian algorithm with a generative diffusion model (GDM)-based multi-agent deep deterministic policy gradient (MADDPG) approach, to jointly optimize exploration and task assignment decisions. In HG-MADDPG, we first introduce the Hungarian algorithm as a method for exploration area selection, enhancing UAV efficiency in interacting with the environment. We then innovatively integrate the GDM and multi-agent deep deterministic policy gradient (MADDPG) to optimize task assignment decisions, such as task offloading and resource allocation. Simulation results demonstrate the effectiveness of the proposed approach, with significant improvements in task offloading efficiency, latency reduction, and system stability compared to baseline methods.},
  archive      = {J_TMC},
  author       = {Xin Tang and Qian Chen and Wenjie Weng and Chao Jin and Zhang Liu and Jiacheng Wang and Geng Sun and Xiaohuan Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3594188},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task assignment and exploration optimization for low altitude UAV rescue via generative AI enhanced multi-agent reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing stability and resource efficiency in LLM training for edge-assisted mobile systems. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3570376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile devices continue to drive advanced applications, edge computing has emerged as a crucial solution to overcome their inherent computational constraints, especially in deploying and training large language models (LLMs). Despite progress in edge computing, significant challenges remain in achieving efficient LLM training while addressing computational demands, energy consumption, and model stability. This paper presents an enhanced collaborative training framework that integrates mobile users with edge servers to optimize resource allocation. We extend the framework by incorporating model stability into the optimization objectives, mitigating performance instability often observed during distributed LLM fine-tuning. A multi-objective optimization problem is formulated to minimize energy consumption, delay, and instability, with a novel fractional programming technique and Iterative Rank Penalization (IRP) method proposed to improve the resource allocation and user-to-edge server associations. Compared to traditional methods like Semidefinite Relaxation, IRP achieves higher accuracy and computational efficiency. Extensive simulations demonstrate that our approach outperforms existing methods in reducing energy consumption and delay, and improving LLM stability across various mobile edge computing environments.},
  archive      = {J_TMC},
  author       = {Chang Liu and Jun Zhao},
  doi          = {10.1109/TMC.2025.3570376},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing stability and resource efficiency in LLM training for edge-assisted mobile systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). COOPER-MATCH: Job offloading with a cooperative game for guaranteeing strict deadlines in MEC. <em>TMC</em>, 1. (<a href='https://doi.org/10.1109/TMC.2019.2921713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While mobile edge computing (MEC) holds promise to enhance users' mobile experience, building a framework under multiple MECs environment to make appropriate offloading decision is challenging. When involving quality of service (QoS), the problem becomes even harder. Few works can be found for this. In this work, we focus on QoS guaranteed offloading under multiple MECs environment. Specifically, there are multiple mobile devices (MDs) and each one is associated with a job which can be offloaded to an access point (AP) for execution. Each job is associated with a block of input data, an execution workload, and a QoS requirement, i.e., a time deadline that the job is expected to be completed before it if it is offloaded to an AP for execution. Our goal is to find an efficient offloading strategy which guides for offloading MDs' jobs to appropriate MEC servers, such that the number of jobs whose deadlines are satisfied is maximized. The problem is proved to be NP-hard. To solve the problem, we design an offloading framework COOPER-MATCH, which is based on cooperative game. Further, we propose a game based offloading policy (GOP) to offload jobs with QoS guarantees.},
  archive      = {J_TMC},
  author       = {Chubo Liu and Kenli Li and Jie Liang and Keqin Li},
  doi          = {10.1109/TMC.2019.2921713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {COOPER-MATCH: Job offloading with a cooperative game for guaranteeing strict deadlines in MEC},
  year         = {2019},
}
</textarea>
</details></li>
</ul>

</body>
</html>
