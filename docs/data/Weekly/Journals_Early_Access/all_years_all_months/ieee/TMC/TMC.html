<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc">TMC - 361</h2>
<ul>
<li><details>
<summary>
(2025). 2FDP-BRL: A new framework of distributed task offloading for IoAV in extreme weather scenarios. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3604461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Autonomous Vehicles (IoAV), task offloading is crucial for managing tasks that require extensive computing power to guarantee vehicle safety under different weather scenarios. However, extreme weather events can lead to infrastructure damage and network disruptions, significantly increasing the computational demands of autonomous vehicles. These vehicles require additional computing resources to navigate complex road conditions and risks, all while facing a high degree of uncertainty, such as fluctuations in vehicle resource utilization and task workloads. To address these challenges, a new and lightweight task offloading decision framework, named 2FDP-BRL, has been first proposed in this paper. This framework not only considers the fast response time required for autonomous driving, but also considers the resource shortage and offloading uncertainty caused by extreme weather. Therefore, we introduce the dynamic pricing idea and the Interval Type-2 Fuzzy Inference System (IT2FIS) utilizing broad reinforcement learning to deal with various dynamic uncertainties in the IoAV under extreme weather. For the authenticity of experimental results, we utilize the VISSIM platform to collect experimental data and conduct simulations. Moreover, to accurately simulate extreme weather scenarios, we also account for the variability of infrastructure and road elements, including reduced transmission rates and decreased efficiency in executing tasks. Furthermore, to enhance the realism of the simulation, we incorporate historical weather data from NOAA for Shenyang in 2024 to model dynamic uncertainties under extreme weather conditions and conduct comparative experimental analyses focusing on task completion rates. Finally, the proposed framework was implemented on both a local setup and the Huawei Atlas 200I DK A2 device, illustrating its efficacy design.},
  archive      = {J_TMC},
  author       = {Xiting Peng and Shun Song and Xiaoyu Zhang and Mianxiong Dong and Kaoru Ota and Lexi Xu},
  doi          = {10.1109/TMC.2025.3604461},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {2FDP-BRL: A new framework of distributed task offloading for IoAV in extreme weather scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sniffing the application usage information with the leakage current of laptops. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart devices are proliferating in every aspect of our lives, providing convenience but also exposing us to the risk of information leakage at any moment. Attackers can monitor the user and infer private information such as personality and preferences by stealing the behavioral information. In this paper, we investigated the potential threat of information stealing via the leakage current of laptops and electrodes in wearable devices (e.g., smart watches and bracelets). Specifically, the leakage current in the laptop adapter can flow from the metal casing into the human body and be collected by electrodes in wearable devices when the user is using a laptop with a metal casing (e.g., MacBook). We verified the correlation between leakage current and the working states of the laptop, where different operations corresponding to different CPU instructions can generate different leakage currents. Based on this, we propose LeakThief, a system that consists of three components: leakage current detection, application operation detection, and application recognition. The experiments in a real-world environment demonstrated that the proposed system can recognize 25 common applications with high accuracy, including launching-based (96.4%) and in-application operation-based recognition (81.2%).},
  archive      = {J_TMC},
  author       = {Dian Ding and Yijie Li and Yongzhao Zhang and Yi-Chao Chen and Xiaoyu Ji and Guangtao Xue},
  doi          = {10.1109/TMC.2025.3599338},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sniffing the application usage information with the leakage current of laptops},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rising from pieces: Effective inference at the edge via robust split ML. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3605382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing processing demands of today's mobile deep learning applications impose stringent requirements on edge devices. Offloading these tasks to the cloud, while being a potential solution, often results in significant data transfer overhead, as well as privacy and connectivity concerns. To address these challenges, split machine learning (split ML) has emerged as an innovative paradigm, enabling task distribution among edge devices themselves. However, split ML systems inherently exhibit instability due to the hardware and communication limitations of mobile devices, which frequently result in failures and malfunctions of client nodes. In light of these challenges, we present Axolotl, a fault-tolerant edge split ML inference system for addressing node failure with minimal performance impact. Specifically, we first design a novel curriculum dropout mechanism to enhance the model's resilience by gradually exposing it to potential server node failures. We then design inverse-proximal weight consolidation to mitigate catastrophic forgetting caused by curriculum dropout. To further tackle potential node failures, we innovate in a resource-aware substitution module that offload the functions of a failed node to neighboring ones, ensuring efficient information flow. Extensive experiments demonstrate the effectiveness and robustness of Axolotl in various deep learning networks and tasks in edge environments.},
  archive      = {J_TMC},
  author       = {Yuxuan Weng and Tianyue Zheng and Zhe Chen and Menglan Hu and Jun Luo},
  doi          = {10.1109/TMC.2025.3605382},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Rising from pieces: Effective inference at the edge via robust split ML},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uplink resource allocation for RSMA-aided digital twin-assisted user-centric cell-free massive MIMO systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3604722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates uplink radio resource optimization of a user-centric (UC) cell-free (CF) massive multiple-input multiple-output (mMIMO) system aided by the rate splitting multiple access (RSMA) technique subject to pilot contamination. We formulate problem to maximize the minimum spectral efficiency (SE) problem by jointly addressing decoding order selection, power allocation, and access point (AP) - user equipment (UE) association assignment. The envisioned optimization exhibits two challenges. First, it requires global channel state information (CSI) for near-optimal performance, which incurs substantial overhead and data collection costs in large-scale CF networks. Second, the optimization is intractable due to its NP-hard and discrete non-linear programming nature. To address the CSI acquisition issue, we utilize a digital twin (DT) of the CF mMIMO system, leveraging its context-awareness to acquire global CSI with reduced overhead. To address computational intractiablity of the optimization problem, we decompose it into three sub-problems. The power allocation sub-problem is transformed into a second-order cone programming problem and solved by the bisection method. Additionally, we propose a computationally efficient heuristic approach for power allocation. Next, we propose an analytical method for the decoding order selection by ranking the channels in descending order of strength. Simulation results validate the ability of the proposed approach to attain the near-optimal performance. Subsequently, the AP-UE association assignment problem is solved by a heuristic approach to further improve the SE performance. Finally, we solve the original NP-hard problem in a unified manner via the block-coordinate descent algorithm. Simulation results underscore a substantial 61% improvement in the SE performance when integrating the RSMA technique into a UC CF mMIMO system.},
  archive      = {J_TMC},
  author       = {Manobendu Sarker and Md. Zoheb Hassan and Georges Kaddoum and Abraham O. Fapojuwo},
  doi          = {10.1109/TMC.2025.3604722},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Uplink resource allocation for RSMA-aided digital twin-assisted user-centric cell-free massive MIMO systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure charging scheduling in wireless rechargeable sensor networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Rechargeable Sensor Networks (WRSNs) promise to address the limited energy resource issue for sensor nodes through wireless power transfer technology. However, WRSNs are vulnerable to various security threats, such as compromised node attack and malicious mobile charger (MC) attack, which can disrupt the charging process and degrade charging efficiency. In this work, we investigate the eneRgy conversion Efficiency maximization problem unDer chargIng attackS (REDIS). We propose a blockchain-based framework that employs a lightweight multi-layer storage approach tailored for resource-constrained sensor nodes and features consensus algorithms that validate charging transactions. Furthermore, we introduce a validation node selection strategy that integrates consensus execution with charging scheduling, reducing energy consumption, and improving energy efficiency. Extensive simulations and experiments validate the effectiveness of our framework, improving energy efficiency by 30% and as much as 5 times in networks without attacks and those under full attacks, respectively.},
  archive      = {J_TMC},
  author       = {Wei Yang and Chi Lin and Jing Deng and Haipeng Dai and Liming Chen and Xinxin Fan and Li Zhang},
  doi          = {10.1109/TMC.2025.3605390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure charging scheduling in wireless rechargeable sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully anonymous broadcast signcryption for secure health data transmission in WBANs. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support comprehensive and personalized healthcare delivery, sensitive personal health data must often be transmitted from wearable devices to multiple designated specialists via wireless body area networks. Although existing identity-based broadcast signcryption schemes provide secure one-to-many transmission, they typically fail to preserve recipient anonymity. In particular, each recipient is usually required to know the identities of all other recipients to unsigncrypt the message, which undermines privacy and enables inference attacks, especially problematic in telemedicine applications. In this paper, we propose a novel broadcast signcryption scheme that simultaneously achieves recipient anonymity, sender anonymity, and resilience to physical attacks, which are not realized in prior work. Our key innovation lies in an anonymity-preserving unsigncryption mechanism based on inner product, which enables each recipient to independently verify and decrypt the broadcast message without knowledge of others' identities. In addition, we incorporate a physical unclonable function with a fuzzy extractor to resist physical attacks in the presence of noise. Furthermore, a comprehensive security analysis demonstrates that our scheme satisfies essential security properties, including authentication, integrity, confidentiality, and full anonymity. It is resilient against common attacks such as impersonation, modification, replay, and man-in-the-middle attacks. Finally, performance evaluations confirm the practical feasibility of our scheme.},
  archive      = {J_TMC},
  author       = {Yangfan Liang and Gao Liu and Xianchao Zhang and Yiming Chen and Jingxue Chen and Yuanjun Xia and Yining Liu},
  doi          = {10.1109/TMC.2025.3605205},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fully anonymous broadcast signcryption for secure health data transmission in WBANs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIGC-enhanced federated learning: Addressing data scarcity in preference-based scenarios. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3605397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a decentralized machine learning paradigm that enables collaborative model training while preserving data privacy by avoiding sensitive data exfiltration from local storage. Despite its success in various domains, current FL frameworks lack mechanisms to accommodate personalized training objectives, particularly in optimizing performance for specific data classes. Recent advancements in Artificial Intelligence Generated Content (AIGC) present opportunities to address these limitations by supplementing training data for user-preferred classes using generative models. Nonetheless, incorporating AIGC into FL introduces significant challenges, including non-compliant data quality, disorganized data distributions, limited computational resources, and slow data generation speed on edge devices. To address these challenges, we propose AIGC-enhanced Federated Preference Learning (FPL), a novel framework designed to enhance FL performance for user-specified preference classes (PCs). Our approach employs pre-training and fine-tuning of generative models across diverse datasets to improve the quality of synthetic data. Additionally, we optimize FPL efficiency through a client selection strategy that matches tasks involving generated data with suitable clients and a data distribution mechanism that allocates synthetic data to where it is most needed. To further accelerate data supplementation, data augmentation is utilized on local clients. We provide theoretical convergence guarantees for AIGC-enhanced FPL and demonstrate its effectiveness through comprehensive experiments on MNIST and CIFAR-10 datasets, including ablation studies on various data supplementation techniques.},
  archive      = {J_TMC},
  author       = {Chenyu Wang and Zhi Zhou and Zixin Xu and Zhijie Wang and Shaoquan Wang and Xu Chen},
  doi          = {10.1109/TMC.2025.3605397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIGC-enhanced federated learning: Addressing data scarcity in preference-based scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-assisted message reporting scheme with weighted threshold signature for vehicular ad-hoc networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicular ad-hoc networks (VANETs), message reporting is an effective method for improving traffic safety and efficiency. Most existing VANET message reporting schemes rely on the trust value of a single vehicle to determine message authenticity, which leads to unreliable message sources. Even multi vehicle-assisted reporting schemes are limited by the assumption that all vehicles have the same credibility, which does not reflect the actual dynamic VANET environment in which vehicles have different credibilities. To address this issue, we propose a blockchain-assisted VANET message reporting scheme with weighted threshold signatures. Through the design of weights, the credibility of different vehicles is quantified, and the impact of vehicles on the signing process is differentiated. Threshold signature generation relies on the weight sum of all signatories reaching a predetermined threshold, to enable flexible and reliable message reporting. Security analysis shows that our proposed scheme combined with blockchain can satisfy the security and privacy requirements of VANET message reporting. Performance analysis indicates that our proposed scheme outperforms the most advanced VANET message reporting schemes in terms of transmission and computation performance.},
  archive      = {J_TMC},
  author       = {Ru Li and Jie Cui and Jing Zhang and Lu Wei and Hong Zhong and Debiao He},
  doi          = {10.1109/TMC.2025.3605858},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-assisted message reporting scheme with weighted threshold signature for vehicular ad-hoc networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AppGen: Mobility-aware app usage behavior generation for mobile users. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3605939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile app usage behavior reveals human patterns and is crucial for stakeholders, but data collection is costly and raises privacy issues. Data synthesis can address this by generating artificial datasets that mirror real-world data. In this paper, we propose AppGen, an autoregressive generative model designed to generate app usage behavior based on users' mobility trajectories, improving dataset accessibility and quality. Specifically, AppGen employs a probabilistic diffusion model to simulate the stochastic nature of app usage behavior. By utilizing an autoregressive structure, AppGen effectively captures the intricate sequential relationships between different app usage events. Additionally, AppGen leverages latent encoding to extract semantic features from spatio-temporal points, guiding behavior generation. These key designs ensure the generated behaviors are contextually relevant and faithfully represent users' environments and past interactions. Experiments with two real-world datasets show that AppGen outperforms state-of-the-art baselines by over 12% in critical metrics and accurately reflects real-world spatio-temporal patterns. We also test the generated datasets in applications, demonstrating their suitability for downstream tasks by maintaining algorithm accuracy and order.},
  archive      = {J_TMC},
  author       = {Zihan Huang and Tong Li and Yong Li},
  doi          = {10.1109/TMC.2025.3605939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AppGen: Mobility-aware app usage behavior generation for mobile users},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDGE360: Edge-enabled multi-agent DRL for region-aware rate adaptation solution to enhance quality of 360° video streaming. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal tile-based bitrate allocation improves the Quality of Experience (QoE) for adaptive 360° video streaming across multiple clients in heterogeneous network environments; however, it is challenging as it implies accurate viewport prediction, finest tile-based bitrate reservation, and maintaining QoE fairness, particularly under constrained network conditions. This paper proposes a strategy named EDGE360, that employs an edge-driven Multi-Agent Deep Reinforcement Learning (MADRL) solution for rate adaptation to improve the joint QoE in DASH-based rich media content delivery based on adaptive viewport prediction and Video Multi-method Assessment Fusion (VMAF) corresponding tiling granularity selection. Cooperative strategies among agents in the central critic network are crucial for addressing the complexity of network instances at the edge and optimizing media streaming bitrate assignment in multiple-client scenarios. Therefore, EDGE360 aims to implement the Counterfactual Multi-Agent Policy Gradients (COMA) based on 5G network traces to train agents in policies that optimize individual client QoE and fairness among clients, resulting in an improved rich streaming experience. At the edge, a tile-based quality monitor evaluates viewport trajectories, buffer status, and network throughput, employing deep learning to forecast optimal tile bitrate allocation, which is formulated as an MDP and solved with MADRL. Based on extensive experimentation, EDGE360 surpasses state-of-the-art adaptive bitrate algorithms by achieving the highest average reward, outperforming RAPT360, 360SRL, and BOLA360 by 8.12%, 11.86%, and 18.00%, respectively, demonstrating superior convergence and refinement.},
  archive      = {J_TMC},
  author       = {Fazal E Subhan and Abid Yaqoob and Cristina Hava Muntean and Gabriel-Miro Muntean},
  doi          = {10.1109/TMC.2025.3605849},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EDGE360: Edge-enabled multi-agent DRL for region-aware rate adaptation solution to enhance quality of 360° video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EOC-tracking: An environmental obstacles constrained adaptive wi-fi tracking framework. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3605936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi device-free tracking enables the inference of user behaviors without physical contact, which is crucial for intelligent indoor location-based services. Nevertheless, the practical implementation of current tracking systems is constrained by several critical limitations: 1) The low-quality sensing signals in complex scenarios lead to increased tracking errors; 2) Existing methods inadequately adjust to dynamic environments, necessitating additional data collection or retraining processes. To address these challenges, this paper introduces EOC-Tracking, a device-free Wi-Fi tracking system that dynamically incorporates environmental information. Our key innovation involves leveraging obstacles to correct illogical users' trajectories and facilitate adjustment to varying environments. This significantly improves the accuracy of the follow-up in complex and changing environments. The EOC-Tracking system is built upon three fundamental design principles: 1) A lightweight dual-branch neural network architecture that effectively fuses environmental data with Wi-Fi signal characteristics; 2) An autonomous map updating mechanism that facilitates real-time adaptation to environmental layout modifications without human intervention; 3) A sophisticated data-driven, phased training paradigm that optimizes the model's ability to learn and apply obstacle constraints. We implement EOC-Tracking using commercial Wi-Fi devices and deploy it on low-power embedded systems such as the MCU. Experimental results demonstrate that EOC-Tracking can reduce tracking errors by at most 49.48% compared to datadriven methods and 62.21% compared to model-based methods in various complex scenarios.},
  archive      = {J_TMC},
  author       = {Jinwei Gao and Qixuan Cai and Mengjie Yu and Xinyu Tong and Tony Xiao Han and Xiulong Liu and Xin Xie and Wenyu Qu},
  doi          = {10.1109/TMC.2025.3605936},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EOC-tracking: An environmental obstacles constrained adaptive wi-fi tracking framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service-oriented segmented trajectory design for low-altitude UAV-assisted MEC networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3605865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the integration of Unmanned Aerial Vehicles (UAV) with Internet of Things (IoT) infrastructure to enhance Mobile Edge Computing capabilities in urban environments. While UAVs offer promising solutions for mobile edge computing, their deployment in high-rise urban areas presents significant challenges, particularly in computational resource balancing, energy-efficient trajectory planning, and dynamic IoT service provisioning. We propose a comprehensive low-altitude UAV-assisted mobile edge computing framework that jointly optimizes UAV trajectory planning, the assignment of offloaded tasks to specific UAVs, and the strategic deployment and energy management of the UAV fleet to maximize system utility. We first formulate this as a multi-objective optimization problem and prove its NP-hardness due to its non-convex and integer linear programming nature. To tackle this challenge, we develop a decomposition-based approach that systematically addresses the coupled variables. We then propose a novel Variable Strategy Reinforcement Learning-based Lin-Kernighan-Helsgaun algorithm that synergistically combines Q-learning, Sarsa, and Monte Carlo methods with the LKH algorithm. The proposed solution is further enhanced by incorporating two refined trajectory optimization mechanisms, the Trajectory Refining Algorithm and the Service-Oriented Segmented Trajectory Refining Algorithm, specifically designed to improve the robustness and reliability in solving the Computation Offloading Trajectory Optimization Problem. Extensive simulation results demonstrate that our proposed algorithms consistently outperform state-of-the-art approaches, achieving faster convergence, higher energy efficiency for UAVs, and lower computational latency for IoT devices.},
  archive      = {J_TMC},
  author       = {Pengfei Wu and Fu Xiao and Chao Sha and Haiping Huang},
  doi          = {10.1109/TMC.2025.3605865},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service-oriented segmented trajectory design for low-altitude UAV-assisted MEC networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P2TS: A preemptive approach for priority-aware task scheduling in computing power networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3606454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging computing paradigm, Computing Power Networks (CPNs) are dedicated to coordinating and managing network resources and computing resources to achieve interconnectivity in computing power perception. Efficient collaborative computing of massive data can be achieved through the scheduling function of CPNs. However, existing scheduling research mainly focuses on selecting network links and computing nodes, lacking consideration for task execution after scheduling, which may degrade the Quality of Service (QoS), leading to widespread failures and significant losses. To address this issue, we design a priority-aware preemptive task scheduling (P2TS) strategy for CPNs to jointly optimize task scheduling and execution in terms of success rate, average processing delay, and load balancing. Specifically, at the execution level, we propose a priority-aware preemptive mechanism (P2M) to optimize post-scheduling task execution. Then, at the scheduling level, we apply deep reinforcement learning (DRL) to optimize the scheduling process supporting the P2M in CPNs. A series of simulations are conducted to demonstrate the superiority of our strategy.},
  archive      = {J_TMC},
  author       = {Tao Huang and Haoxiang Qiu and Qinqin Tang and Li Feng and Renchao Xie and Tianjiao Chen and Zehui Xiong},
  doi          = {10.1109/TMC.2025.3606454},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {P2TS: A preemptive approach for priority-aware task scheduling in computing power networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative highly-maneuvering target tracking using multi-AUV networks: A bearing-only approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3606690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target tracking is a fundamental technology for marine development, providing real-time position estimates of the interested targets. However, due to the harsh underwater environment and the noncooperativity of targets, improving tracking accuracy remains a challenge, especially for highly-maneuvering targets. To address this problem, based on multi-autonomous underwater vehicle (multi-AUV) networks, this paper extends the idea of interacting multiple models (IMM) and designs a bearing-only cooperative tracking algorithm in the consideration of the harsh underwater acoustic channels. Specifically, in position prediction, the combination of historical information and the concept of IMM reduces the severe time-lagged effect in traditional prediction methods and the model reliance in standard IMM filters. Then, during position update, a rigidity-assisted relative position representation is designed based solely on bearing measurements, which alleviates the impact of information loss due to communication interruptions, significantly enhancing the continuity of target tracking. Moreover, the algorithm design also considers various uncertainties that may concurrently occur underwater (e.g., error accumulation and model mismatches), and robust optimization strategies with the principle of maximum entropy are designed to enhance the environmental adaptability. Through various simulations and field experiments, the advantages of the proposed method have been validated.},
  archive      = {J_TMC},
  author       = {Yichen Li and Yang Yang and Wenbin Yu and Xinping Guan},
  doi          = {10.1109/TMC.2025.3606690},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cooperative highly-maneuvering target tracking using multi-AUV networks: A bearing-only approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic multi-modal UAV control for optimized coverage and backhaul connectivity in spatially unstructured and dispersed user environments. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3606778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have emerged as a promising solution for establishing wireless communications in regions lacking terrestrial network infrastructure, such as remote or emergency areas. Deploying UAV networks effectively in these scenarios poses significant challenges due to the unknown and potentially complex locations of users. In scenarios where users are dispersed in intricate spatial patterns, achieving high coverage and resilient network connectivity among the UAV networks is challenging. The irregular and arbitrary distribution of users can lead to gaps in coverage, as traditional UAV placement optimization approaches are often unable to adapt to such dynamic environments. This complexity necessitates advanced strategies to ensure reliable and continuous network service to users. In this paper, we propose a distributed approach that leverages flocking dynamics and distributed consensus algorithms for dynamic UAV positioning. By enabling a multi-modal UAV operation policy, we develop a framework which enables the network to dynamically respond to complex user locations and establish backhaul connectivity between dispersed user clusters. Simulation results demonstrate that our approach successfully establishes a robust and adaptable UAV network capable of providing seamless coverage for complex user configurations and also ensuring comprehensive inter-cluster connectivity among dispersed user clusters. Additionally, the network exhibits strong resilience against random failures, swiftly recovering from disruptions to ensure stable and reliable communication even when UAVs are compromised.},
  archive      = {J_TMC},
  author       = {Yuhui Wang and Junaid Farooq and Juntao Chen},
  doi          = {10.1109/TMC.2025.3606778},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic multi-modal UAV control for optimized coverage and backhaul connectivity in spatially unstructured and dispersed user environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaestroBot: Generalized gesture-driven hierarchical coordination for robotic formations. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3606847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic swarm coordination holds transformative potential for applications such as warehouse automation, search & rescue, and entertainment. However, approaches relying on wearable devices or vision-based systems are often constrained by hardware-intensive, high computational requirements, reliance on line-of-sight, and privacy concerns. Wireless sensing, particularly using Channel State Information (CSI), offers a promising alternative by translating environmental perturbations into CSI variation data. Nevertheless, existing CSI-based systems face significant challenges in domain adaptation, resource limitation, and scalability issues. This paper introduces MaestroBot, a hierarchical motion coordination system that combines distributed CSI-based wireless sensing with domain-adaptive learning to address these limitations. For leader robots, the system features a lightweight hand gesture recognition model, built on a “Hybrid-Single” knowledge distillation framework, achieving up to 95.87% accuracy while maintaining adaptability across diverse domains. For follower robots, the hierarchical motion propagation model leverages localized CSI analysis and dual-layer error correction mechanisms to deliver 97.2% accuracy with a low latency of 0.085 seconds, even in multi-row formations. Additionally, its cost-effective hardware design ensures practical scalability and real-world deployability. These results position MaestroBot as an efficient, robust, and privacy-preserving solution for large-scale robotic swarm coordination in dynamic environments.},
  archive      = {J_TMC},
  author       = {Yutong Liu and Zhiye Wang and Yuhan Xu and Yang Yue and Haiming Jin and Linghe Kong and Rui Li and Xi Chen and Qiao Xiang and Jun Zhang and Guihai Chen},
  doi          = {10.1109/TMC.2025.3606847},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MaestroBot: Generalized gesture-driven hierarchical coordination for robotic formations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility resilient vehicular federated learning: Enhancing training efficiency in dynamic environments. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicular environment presents unique challenges, including massive data generation, stringent latency requirements for safety-critical applications, bandwidth limitations, and intermittent connectivity, which make centralized learning approaches impractical. Vehicular Federated Learning (VFL) enables distributed model training by leveraging local data from connected vehicles, while preserving data privacy and reducing network overhead. However, the dynamic nature of VFL presents several additional challenges. High vehicle mobility and unstable channels lead to inconsistent client participation, while heterogeneous vehicle capabilities result in unbalanced training workloads and competitive resource allocation. These challenges significantly degrade VFL model performance and prolong training periods. In this paper, we propose a Mobility Resilient Vehicular Federated Learning (MR-VFL) scheme, which comprises two key components: an amplification-based adaptive vehicular FL (AVFL) training scheme and a dual-timescale FL scheduler. Specifically, AVFL adapts local training epochs to vehicle capabilities to improve scheduling flexibility and alleviate the impact of insufficient local epochs on model updates, which enhances training efficiency and reduces communication competition. The dual-timescale FL scheduler includes a macro scheduling strategy that optimizes long-term VFL performance based on the correlation between convergence speed and model accuracy, and a Mamba-based real-time scheduler that enhances training efficiency and reduces decision latency in massive vehicles scenarios. Extensive simulations show that MR-VFL effectively mitigates performance degradation due to complex vehicle mobility and heterogeneity, and improves training efficiency.},
  archive      = {J_TMC},
  author       = {Tianao Xiang and Yuanguo Bi and Lin Cai and Mingjian Zhi},
  doi          = {10.1109/TMC.2025.3607138},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility resilient vehicular federated learning: Enhancing training efficiency in dynamic environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilient topological control for dynamic underwater optical wireless networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Wireless Optical Networks (UWONs) play a critical role in tasks such as ocean monitoring and resource exploration, which require high connectivity and reliability. However, water turbidity, ocean currents, and ambient light noise significantly affect communication stability, creating serious deployment challenges. To address this, we propose a network topology optimization method based on resilience evaluation. First, a resilience evaluation method is designed to quantify the network's ability to adapt to disturbances. Then, an improved predecessor-based evolutionary algorithm (Pred-EA) is used for resilience-guided topology optimization. To improve algorithmic efficiency and search quality, the prim algorithm is introduced to ensure chromosome feasibility, enhancing both the diversity of the initial population and computational efficiency. Experimental results show that our method achieves better recovery performance than three comparison methods in all scenarios. The average number of recovered edges improves by up to 18.30% over the second-best method. Under non-recoverable conditions, resilience improves by up to 20.52%. These results confirm the strong topological robustness and practical value of the proposed method in dynamic underwater environments.},
  archive      = {J_TMC},
  author       = {Qing Zhang and Youling Huang and Lin Lin and Chi Lin},
  doi          = {10.1109/TMC.2025.3607035},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resilient topological control for dynamic underwater optical wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quick-pass continuous authentication with real-time biometrics extraction on COTS earphones using out-ear microphones. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3606846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous authentication is increasingly critical for cyber security. However, existing approaches are time-consuming due to their simplistic signal modulation and low efficiency in feature extraction. In this paper, we propose a continuous authentication technique, OnePiece. OnePiece is free from the requirement of in-ear microphones, which are necessary for existing earphone authentication systems. It exploits out-ear microphones for biometrics extraction, which are ubiquitous on off-the-shelf earphones. We analyze the acoustic response model of ears towards out-ear microphones via the air, which is different from that towards in-ear microphones. A frequency-varying ultrasonic modulation scheme is proposed to characterize in-depth ear biometrics in user-friendly, error-free, and time-efficient ways. Therefore, OnePiece enables quick-pass authentication once users wear the earphones, followed by continuous authentication covering the whole course. Moreover, we propose a wake-up mechanism to reduce the consumed power, which addresses the key power consumption issue in ultrasonic sensing techniques. Particularly, OnePiece can be smoothly deployed on off-the-shelf wired and wireless earphones. It performs good cross-device performance in which users just register only once. Extensive evaluations are conducted to validate its effectiveness under real-world scenarios.},
  archive      = {J_TMC},
  author       = {Ming Gao and Jiatong Chen and Xin Tong and Ruitong Ye and Yike Chen and Fu Xiao and Jinsong Han},
  doi          = {10.1109/TMC.2025.3606846},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quick-pass continuous authentication with real-time biometrics extraction on COTS earphones using out-ear microphones},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient iTreeKEM-based group key agreement protocol for flying ad-hoc networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Flying Ad-hoc Network (FANET) evolves toward larger scales and higher levels of autonomy, the importance of secure and efficient group communication continues to grow. However, resource-constrained unmanned aerial vehicles (UAVs) face dual challenges: limited computational power struggles to meet the high demands of complex cryptographic algorithms, while bandwidth constraints exacerbate communication overhead caused by multi-round interaction mechanisms. Moreover, existing solutions find it hard to support dynamic group environments and are prone to single point of failure (SPoF) in centralized architectures, which significantly compromises system reliability and scalability. To address these issues, this paper proposes a novel key agreement protocol for FANET. The protocol employs an improved tree-based key encapsulation mechanism (iTreeKEM) to support rapid key updates in highly dynamic environments. It reduces the computational cost for each group member by 90.08% even when the group size reaches 128. To further enhance system robustness, the protocol introduces a smart contract-based distributed leader election mechanism, effectively eliminating SPoF. The security of the proposed protocol is guaranteed by the CDH problem under the generalized selective decryption (GSD) model. Finally, we implement the protocol in NS-3 simulations, and the results demonstrate its effective applicability to FANET.},
  archive      = {J_TMC},
  author       = {Tianqi Zhou and Shijia Hong and Jian Shen and Md Zakirul Alam Bhuiyan and Pandi Vijayakumar and Debiao He},
  doi          = {10.1109/TMC.2025.3607316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient iTreeKEM-based group key agreement protocol for flying ad-hoc networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Microservice deployment in space computing power networks via robust reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for Earth observation, it is important to provide reliable real-time remote sensing inference services to meet the low-latency requirements. The Space Computing Power Network (Space-CPN) offers a promising solution by providing onboard computing and extensive coverage capabilities for real-time inference. This paper presents a remote sensing artificial intelligence applications deployment framework designed for Low Earth Orbit satellite constellations to achieve real-time inference performance. The framework employs the microservice architecture, decomposing monolithic inference tasks into reusable, independent modules to address high latency and resource heterogeneity. This distributed approach enables optimized microservice deployment, minimizing resource utilization while meeting quality of service and functional requirements. We introduce Robust Optimization to the deployment problem to address data uncertainty. Additionally, we model the Robust Optimization problem as a Partially Observable Markov Decision Process and propose a robust reinforcement learning algorithm to handle the semi-infinite Quality of Service constraints. Our approach yields sub-optimal solutions that minimize accuracy loss while maintaining acceptable computational costs. Simulation results demonstrate the effectiveness of our framework.},
  archive      = {J_TMC},
  author       = {Zhiyong Yu and Yuning Jiang and Xin Liu and Yuanming Shi and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3607488},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Microservice deployment in space computing power networks via robust reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CI-HDA: Heterogeneous deniable authentication based on CLC and IBC for location privacy in edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing improves the performance of Internet of Things (IoT) devices by moving cloud services closer to where these devices operate, thereby reducing delays and saving bandwidth. However, the IoT devices communicate wirelessly with edge servers, which creates a significant challenge in keeping the devices' locations private, especially when they use different methods. To address this, researchers have proposed various methods. However, these methods require a lot of computational power and storage, which makes them unsuitable for edge computing environments. To address these challenges, we propose a certificateless cryptography (CLC) and identity-based cryptography (IBC) based heterogeneous deniable authentication (CI-HDA) scheme. It enables an IoT device operating CLC to securely communicate with an edge server using IBC. The server verifies the devices authenticity without proving its participation to third parties, which ensures location privacy in edge computing environments. Additionally, our scheme supports batch verification, which enables the server to efficiently validate multiple authenticators simultaneously. The security of CI-HDA scheme is formally proven in the random oracle model, and performance evaluations demonstrate reduced computational and communication/storage overheads compared to existing methods. We also explored its application in military surveillance, which highlights its practicality in privacy-sensitive edge computing environments.},
  archive      = {J_TMC},
  author       = {Ikram Ali and Jianqiang Li and Jie Chen and Yong Chen and Shamsher Ullah and Abdul Wakeel},
  doi          = {10.1109/TMC.2025.3607414},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CI-HDA: Heterogeneous deniable authentication based on CLC and IBC for location privacy in edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Through-wall cross-domain user identification via lip movement micro-doppler and MIMO radar: An unsupervised domain adaptation approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip movement-based user identification holds significant promise for public security and intelligent surveillance due to its dynamic patterns, forgery resistance, and individual distinctiveness. Recently, millimeter-wave radar has been employed for contactless identification, offering advantages such as light insensitivity, privacy preservation, and sensitivity to fine motion. However, its limited wall penetration and vulnerability to occlusion present ongoing challenges. Moreover, existing recognition approaches rely heavily on supervised learning, demanding large labeled datasets and exhibiting poor generalization across domains. To overcome these limitations, we propose Lip-TWCDID, a lip movement-based cross-domain user identification system using 1–2 GHz MIMO radar. The use of low-frequency signals enhances penetration, while the MIMO architecture improves spatial resolution, enabling stable detection of fine-grained micro-Doppler signatures of lip movements through a 22 cm brick wall. To reduce dependence on labeled data and improve domain generalization, we introduce a novel unsupervised domain adaptation (UDA) framework, consistency-adversarial-contrastive learning (CACL), which integrates pseudo-label consistency learning, domain adversarial training, and pseudo-supervised contrastive learning. Specifically, pseudo-label consistency enforces prediction consistency under input perturbations, improving robustness; domain adversarial training introduces a domain discriminator to encourage domaininvariant feature learning and align feature distributions; pseudosupervised contrastive learning leverages high-confidence pseudolabels to perform contrastive learning in the feature space, enhancing inter-class separability and intra-class compactness. By jointly optimizing these components, CACL effectively adapts to unlabeled target domains while minimizing annotation costs. Extensive experiments demonstrate that CACL outperforms state-of-the-art UDA methods and significantly improves the generalization and robustness of through-wall user identification.},
  archive      = {J_TMC},
  author       = {Kai Yang and Dongsheng Zhu and Yujie Xu and Chong Han and Jian Guo and Lijuan Sun},
  doi          = {10.1109/TMC.2025.3607413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Through-wall cross-domain user identification via lip movement micro-doppler and MIMO radar: An unsupervised domain adaptation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware multi-task decentralized federated learning for vehicular networks: Modeling, analysis, and optimization. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3607496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising paradigm that can enable collaborative model training between vehicles while protecting data privacy, thereby significantly improving the performance of intelligent transportation systems (ITSs). In vehicular networks, due to mobility, resource constraints, and the concurrent execution of multiple training tasks, how to allocate limited resources effectively to achieve optimal model training of multiple tasks is an extremely challenging issue. In this paper, we propose a mobility-aware multi-task decentralized federated learning (MMFL) framework for vehicular networks. By this framework, we address task scheduling, subcarrier allocation, and leader selection, as a joint optimization problem, termed TSLP. For the case with a single FL task, we derive the convergence bound of model training. For general cases, we first model TSLP as a resource allocation game, and prove the existence of a Nash equilibrium (NE). Then, based on this proof, we reformulate the game as a decentralized partially observable Markov decision process (DEC-POMDP), and develop an algorithm based on heterogeneous-agent proximal policy optimization (HAPPO) to solve DEC-POMDP. Finally, numerical results are used to demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TMC},
  author       = {Dongyu Chen and Tao Deng and He Huang and Juncheng Jia and Mianxiong Dong and Di Yuan and Keqin Li},
  doi          = {10.1109/TMC.2025.3607496},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware multi-task decentralized federated learning for vehicular networks: Modeling, analysis, and optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-layer design for dynamic routing and MAC protocols in terahertz nanonetworks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3607434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of nanotechnology has enabled the deployment of medical nanonetworks within the human body, further accelerated by progress in terahertz communication technologies and nanonetwork routing protocols. However, nanonodes may move in dynamic environments due to environmental influences or self-propulsion mechanisms, posing significant challenges to routing protocol design. To address this, we propose a dynamic routing protocol for nanonetworks that integrates real-time velocity vectors to account for time-varying node positions. During relay node selection, key factors such as candidate nodes' velocity vectors are comprehensively evaluated to determine the optimal relay for next-hop transmission. To ensure efficient data exchange among multiple nodes, we design a time-division multiple access (TDMA)-based media access control (MAC) protocol to prevent packet collisions and losses. The protocol assigns distinct transmission and reception time slots to different node types and implements a countdown mechanism to manage channel access and eliminate conflicts. Numerical and simulation results demonstrate that the proposed protocol significantly outperforms benchmark protocols, achieving notable improvements in both time and energy efficiency.},
  archive      = {J_TMC},
  author       = {Duyu Dai and Yu Huang and Mingyue Cheng and Miaowen Wen and Nan Yang and Chan-Byoung Chae},
  doi          = {10.1109/TMC.2025.3607434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cross-layer design for dynamic routing and MAC protocols in terahertz nanonetworks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sacrifice for QoS-aware routing: A graph reinforcement learning approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet today hosts a multitude of communication sessions from diverse vertical industries, each with distinct and increasingly stringent quality-of-service (QoS) requirements across multiple performance metrics. However, QoS-aware routing remains a significant challenge in traffic engineering, as existing solutions struggle to adapt to dynamic network conditions and meet these rigorous QoS demands. To address this issue, this paper proposes a multi-agent graph reinforcement learning-based routing algorithm that provides differentiated treatment for multiple services. First, we explore both nodebased and link-based graph reinforcement learning paradigms for performance comparison. Second, two key mechanisms, i.e., a packet sacrifice mechanism and a Tchebycheff-based reward function, are designed to realize adaptive sacrifice behavior patterns, aiming to optimize the lower bound of service satisfaction rates and enhance fairness across services. Furthermore, to ensure practical applicability, we devise a distributed computing architecture featuring neighborhood-restricted data acquisition and asynchronous historical information retrieval. Extensive simulation results demonstrate that our proposed algorithms significantly outperform benchmark methods regarding the minimum service satisfaction rate, even under unseen networks. Besides, the distributed computing architecture is proven to incur no performance penalty, which can be generalized to other resource-constrained applications.},
  archive      = {J_TMC},
  author       = {Yuqian Song and Jingli Zhou and Shudan Yu and Jun Liu},
  doi          = {10.1109/TMC.2025.3607388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive sacrifice for QoS-aware routing: A graph reinforcement learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrivGuardInfer: Channel-level end-edge collaborative inference strategy protecting original inputs and sensitive attributes. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3607483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-edge collaborative inference improves computational efficiency by dividing a deep neural network into two parts, executed across the end device and the edge node in parallel. However, adversaries like malicious edge nodes can exploit transmitted data to reconstruct original inputs or infer sensitive attributes. Existing collaborative inference strategies upload the majority of input features to the edge node, significantly increasing the risk of privacy leakage, even without input reconstruction. Therefore, we propose PrivGuardInfer, a channel-level DNN end-edge collaborative inference strategy that optimizes intra-layer partition to simultaneously protect original inputs and sensitive attributes while ensuring latency constraints, supported by three key designs. First, the privacy measurements oriented both layer depth and channel count, jointly quantify the difficulty of reconstructing original inputs using varying numbers of feature maps across different layers. After assessing each channel's contribution, the information offset further measures the difficulty of inferring sensitive attributes. Finally, PrivGuardInfer models the privacy-optimal intra-layer partition under latency constraints as a grouped knapsack problem, mapping attack difficulty to item values and inference latency to item weights. Experimental results reveal that PrivGuardInfer achieves an average improvement of 80.54% in defending against model inversion attacks and 63.34% against attribute inference attacks compared to existing end-edge partition strategies. Moreover, it outperforms current privacy protection methods by an average of 69.37% and 49.75% in mitigating these two types of attacks.},
  archive      = {J_TMC},
  author       = {Yunhao Yao and Zhiqiang Wang and Puhan Luo and Yihang Cheng and Jiahui Hou and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3607483},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PrivGuardInfer: Channel-level end-edge collaborative inference strategy protecting original inputs and sensitive attributes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is FISHER all you need in the multi-AUV underwater target tracking task?. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is significant to employ multiple autonomous underwater vehicles (AUVs) to execute the underwater target tracking task collaboratively. However, it's pretty challenging to meet various prerequisites utilizing traditional control methods. Therefore, we propose an effective two-stage learning from demonstrations training framework, FISHER, to highlight the adaptability of reinforcement learning (RL) methods in the multi-AUV underwater target tracking task, while addressing its limitations such as extensive requirements for environmental interactions and the challenges in designing reward functions. The first stage utilizes imitation learning (IL) to realize policy improvement and generate offline datasets. To be specific, we introduce multi-agent discriminator-actor-critic based on improvements of the generative adversarial IL algorithm and multi-agent IL optimization objective derived from the Nash equilibrium condition. Then in the second stage, we develop multi-agent independent generalized decision transformer, which analyzes the latent representation to match the future states of high-quality samples rather than reward function, attaining further enhanced policies capable of handling various scenarios. Besides, we propose a simulation to simulation demonstration generation procedure to facilitate the generation of expert demonstrations in underwater environments, which capitalizes on traditional control methods and can easily accomplish the domain transfer to obtain demonstrations. Extensive simulation experiments from multiple scenarios showcase that FISHER possesses strong stability, multi-task performance and capability of generalization.},
  archive      = {J_TMC},
  author       = {Guanwen Xie and Jingzehua Xu and Ziqi Zhang and Xiangwang Hou and Dongfang Ma and Shuai Zhang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3607882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Is FISHER all you need in the multi-AUV underwater target tracking task?},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The survey hole inpainting problem: A machine learning approach. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3607935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the inpainting of missing data in an indoor field, such as geomagnetism and WiFi fingerprints. As opposed to typical image/video inpainting problems, this problem poses several new challenges. First, unlike images with rectangular shapes and fixed RGB channels, indoor geographic data are multi-channeled and highly influenced by building structures. Second, unlike natural objects with fixed shapes, each geographic field is distinct and geographic data are environmentsensitive, following complex physical laws. Consequently, learning from data in other fields is difficult. Third, such data may be obtained from manual surveys and crowdsourcing, which often results in weakly-labeled and noisy datasets. We model our field data as (i) manually surveyed labeled data with holes and (ii) crowdsourced weakly-labeled data without holes. We propose a two-level adversarial regularization inpainting model to conquer these challenges and validate our results with real field data.},
  archive      = {J_TMC},
  author       = {Wei-Zhi Lin and Jen-Jee Chen and Yu-Chee Tseng},
  doi          = {10.1109/TMC.2025.3607935},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The survey hole inpainting problem: A machine learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group-based federated learning with cost-efficient sampling mechanism in mobile edge computing networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) that preserves privacy has appeared as a prospective paradigm in mobile edge computing networks. However, due to the system and data heterogeneity of mobile clients (MCs), group-based FL with a sampling mechanism is crucial for minimizing model training costs. To address these challenges, we investigate and formulate the problem of group-based FL with a sampling mechanism for reducing model training cost (i.e., latency and energy consumption), and propose a group-based FL with a cost-efficient sampling mechanism (GFLCSM) framework to address it. More precisely, before training, each MC locally pre-trains a model, estimates its data distribution from the classifier's gradient norms, and uploads it to the central server (CS) instead of raw data to preserve privacy. Using this information, the CS transforms vanilla FL into a group-based FL. During training, GFLCSM replaces the random sampling mechanism with a cost-efficient one. Moreover, to enhance robustness against network dynamics, we extend GFLCSM with a backup resampling mechanism, termed GFLCSM-E. Experimental results indicate that GFLCSM surpasses the baseline frameworks, reducing latency by 24.63% and energy consumption by 11.47% on average across two datasets, while GFLCSM-E maintains high performance even under client dropout. The source code address is https://github.com/kt4ngw/GFLCSM.},
  archive      = {J_TMC},
  author       = {Jian Tang and Xiuhua Li and Guozeng Xu and Penghua Li and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3608024},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Group-based federated learning with cost-efficient sampling mechanism in mobile edge computing networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VR-PCT: Enhanced VR semantic performance via edge-client collaborative multi-modal point cloud transformers. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic recognition is crucial for virtual reality (VR) applications, but the efficient fusion of multi-modal data poses significant challenges under resource-constrained VR scenarios. While integrating millimeter-wave (mmWave) radar point clouds with vision data offers a promising solution, existing methods often suffer from excessive data overhead and degraded accuracy due to redundant and noisy information. To address this limitation, this paper presents VR-PCT, a multi-modal transformer for edge-client collaborative VR semantic recognition that fuses mmWave radar point cloud and vision data for VR applications. VR-PCT introduces a novel collaborative design where VR clients perform lightweight semantic region detection while VR edge processes multi-modal VR semantic recognition. Through efficient edge-client collaboration, VR-PCT optimizes the transmission of mmWave point cloud and vision data by transmitting only the VR semantic region of vision data instead of the entire video. Additionally, it incorporates adaptive cross-modal data selection and fusion strategies to achieve real-time semantic recognition while significantly reducing data redundancy. Across 22 participants engaged in four experimental scenes utilizing VR devices from three different manufacturers, our evaluation demonstrates that VR-PCT achieves 97.6% recognition accuracy while reducing transmission overhead by 81.5% compared to existing approaches. These results highlight the effectiveness of VR-PCT in enabling efficient and accurate multi-modal VR semantic recognition for VR applications. The code and data of VR-PCT are released on https://github.com/luoyumei1-a/VR-PCT.},
  archive      = {J_TMC},
  author       = {Luoyu Mei and Shuai Wang and Ruofeng Liu and Yun Cheng and Shuai Wang and Wenchao Jiang and Zhimeng Yin and Tian He},
  doi          = {10.1109/TMC.2025.3607791},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VR-PCT: Enhanced VR semantic performance via edge-client collaborative multi-modal point cloud transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic communication based on large language model for underwater image transmission. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater communication is essential for environmental monitoring, marine biology research, and underwater exploration. Traditional underwater communication faces limitations like low bandwidth, high latency, and susceptibility to noise, while semantic communication (SC) offers a promising solution by focusing on the exchange of semantics rather than symbols or bits. However, SC encounters challenges in underwater environments, including semantic information mismatch and difficulties in accurately identifying and transmitting critical information that aligns with the diverse requirements of underwater applications. To address these challenges, we propose a novel SC framework based on Large Language Models (LLMs). Our framework leverages visual LLMs to perform semantic compression and prioritization of underwater image data according to the query from users. By identifying and encoding key semantic elements within the images, the system selectively transmits high-priority information while applying higher compression rates to less critical regions. On the receiver side, an LLM-based recovery mechanism, along with Global Vision ControlNet and Key Region ControlNet networks, aids in reconstructing the images, thereby enhancing communication efficiency and robustness. Our framework reduces the overall data size to 0.8% of the original. Experimental results demonstrate that our method significantly outperforms existing approaches, ensuring high-quality, semantically accurate image reconstruction.},
  archive      = {J_TMC},
  author       = {Weilong Chen and Wenxuan Xu and Haoran Chen and Xinran Zhang and Zhijin Qin and Yanru Zhang and Zhu Han},
  doi          = {10.1109/TMC.2025.3607717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semantic communication based on large language model for underwater image transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable intelligent reflecting surface-assisted mobile edge computing systems: A physical layer security and encryption design. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3607599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has emerged as a promising technology to extend the functionality of end-users' wireless devices while prolonging their battery life by offloading computationally intensive tasks to remote edge servers. However, the inherent broadcast nature of wireless transmission during offloading introduces notable security challenges. To address this issue, we propose leveraging intelligent reflecting surface (IRS) technology to enhance physical layer security (PLS). Nevertheless, attaining high PLS for all users in dense networks with multiple malicious terminals is challenging. In this paper, we investigate the physical layer encryption (PLE) to complement the PLS in enabling secure wireless transmission. Since such encryption and decryption processes require computation resources, we aim to optimize the encryption decision, offloading decision, as well as wireless and computing resource allocations. Our objective is to minimize the maximum weighted energy consumption while satisfying practical constraints, including limited computing and wireless resources, fulfilling minimum user rate requirements, and complying with IRS conditions. To tackle the non-convex objective and constraints, we explore the utilization of bisection search and successive convex approximation (SCA) methods. Our numerical results confirm the efficiency of the proposed design in terms of energy consumption and network capacity within a secure MEC network.},
  archive      = {J_TMC},
  author       = {Ti Ti Nguyen and Vu Nguyen Ha and Thanh-Dung Le and Duc-Dung Tran and Symeon Chatzinotas and Kim-Khoa Nguyen},
  doi          = {10.1109/TMC.2025.3607599},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliable intelligent reflecting surface-assisted mobile edge computing systems: A physical layer security and encryption design},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cellular user re-identification risks with networking behaviors analysis and modeling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile network operators (e.g., China Mobile, Verizon) are significant for providing communication services and collecting massive amounts of data. However, operators are increasingly concerned about customer data breaches involving third-party application providers (e.g., Tencent, Apple, Netflix). This concern is particularly aggravated when anonymous datasets shared with third-party providers or publicly released can be linked to user data compromised in breaches, leading to severe re-identification attacks and privacy threats. However, comprehensive methods for identifying such privacy risks on a large scale are lacking due to limited networking behavioral data. To address this, we aim to measure the re-identification privacy risk associated with sharing or releasing cellular traces amidst data breaches. Based on the analysis of key privacyimpacting features in traffic usage and base station association data, we propose a novel re-identification method, SURE, which learns similarities between cellular traces to classify if traces belong to the same user. Extensive experiments on a largescale dataset of 10,000 users over four months demonstrate SURE's superior performance, with AUC scores exceeding 0.9. Our findings reveal significant re-identification risks in data sharing/release, influenced by data scale and user attributes, corroborated by a public dataset.},
  archive      = {J_TMC},
  author       = {Sijing Duan and Feng Lyu and Shanshan Wang and Yi Ding and Xiaohao He and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3607772},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring cellular user re-identification risks with networking behaviors analysis and modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sleep scheduling algorithm for the $k$-coverage problem in 3D heterogenous BF-WSNs. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3607841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Battery-free Wireless Sensor Networks (BF-WSNs) have emerged as an essential part of Internet of Things (IoT) systems. Although two-dimensional (2D) BF-WSNs have been researched, three-dimensional (3D) ones are more indicative of real-world applications. Achieving $k$-coverage in this scenario is a greater challenge and has yet to be investigated. This paper presents an optimization problem in heterogeneous 3D BF-WSNs to maximize $k$-coverage quality while considering the recharging and sampling rates. We prove the problem is NP-Hard and decouple it into two subproblems. The first optimizes $k$-coverage quality for each time slot without energy constraints. The second ensures that nodes scheduled for operation in each time slot can be immediately replenished with sufficient energy. We prove the near-optimal and optimal solutions to the two subproblems composes the near-optimal solution to the original problem. Following the development of Distributed Iterative Grouping (DIG) algorithm and Adaptive Sampling (AS) method to address two subproblems each, we propose a sleep scheduling algorithm to integrate them and solve the original problem. Simulation results verify the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_TMC},
  author       = {Yanlei Chen and Haoyang Zhou and Jingjing Li and Na Tang and Ping Li},
  doi          = {10.1109/TMC.2025.3607841},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sleep scheduling algorithm for the $k$-coverage problem in 3D heterogenous BF-WSNs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A superposition code-based semantic communication approach with quantifiable and controllable security. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of achieving security in semantic communication (SemCom) over a wiretap channel, where a legitimate receiver coexists with an eavesdropper experiencing a poorer channel condition. Despite previous efforts to secure SemCom against eavesdroppers, guarantee of approximately zero information leakage remains an open issue. In this work, we propose a secure SemCom approach based on superposition code, aiming to provide quantifiable and controllable security for digital SemCom systems. The proposed method employs a double-layered constellation map, where semantic information is associated with satellite constellation points and cloud center constellation points are randomly selected. By carefully allocating power between these two layers of constellation, we ensure that the symbol error probability (SEP) of the eavesdropper when decoding satellite constellation points is nearly equivalent to random guessing, while maintaining a low SEP for the legitimate receiver to successfully decode the semantic information. Simulation results demonstrate that the peak signal-to-noise ratio (PSNR) and mean squared error (MSE) of the eavesdropper's reconstructed data, under the proposed method, can range from decoding Gaussian-distributed random noise to approaching the variance of the data. This validates the effectiveness of our method in nearly achieving the experimental upper bound of security for digital SemCom systems when both eavesdroppers and legitimate users utilize identical decoding schemes. Furthermore, the proposed method consistently outperforms benchmark techniques, showcasing superior data security and robustness against eavesdropping. The implementation code is publicly available at: https://github.com/1weixuanchen/A-Superposition-Code-Based-Semantic-Communication.},
  archive      = {J_TMC},
  author       = {Weixuan Chen and Shuo Shao and Qianqian Yang and Zhaoyang Zhang and Ping Zhang},
  doi          = {10.1109/TMC.2025.3608054},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A superposition code-based semantic communication approach with quantifiable and controllable security},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online location planning for AI-defined vehicles: Optimizing joint tasks of order serving and spatio-temporal heterogeneous model fine-tuning. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living. Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model finetuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning. To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.},
  archive      = {J_TMC},
  author       = {Bokeng Zheng and Bo Rao and Tianxiang Zhu and Chee Wei Tan and Jingpu Duan and Zhi Zhou and Xu Chen and Xiaoxi Zhang},
  doi          = {10.1109/TMC.2025.3608179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online location planning for AI-defined vehicles: Optimizing joint tasks of order serving and spatio-temporal heterogeneous model fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WarmGait: Thermal array-based gait recognition for privacy-preserving person re-ID. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) can recognize users based on their clothing, body shape, and other information without the need for clear facial images, and is widely applied in the field of intelligent security. Traditional Re-ID systems mainly rely on high-definition RGB cameras, but the deployment of large-scale high-definition RGB cameras indoors has caused serious privacy and ethical concerns. Recently, wireless-based Re-ID systems (Wi-Fi, RFID, millimeter-wave radar, etc.) have shown promising prospects, but the limited sensing resolution hinders their practical deployment. In this paper, we propose WarmGait, a Re-ID system based on thermal array sensors, which can achieve high-precision Re-ID at low cost and minimize the invasion of user privacy. However, using thermal arrays for Re-ID still faces two major challenges. The first is the low and unclear texture resolution of images caused by low-cost infrared devices. The second is that existing gait recognition methods require maintaining the sequential constraint of gait images, which reduces the flexibility of gait recognition or Re-ID. To address these two challenges, we first designed an edge module inspired by Taylor Finite Difference (TFD) to aggregate image edge information to help improve the resolution of infrared devices. Then, we considered gait as a collection of gait profiles and extracted features from the frame level and collection level for recognition, breaking through the limitations of the number and order of input images. After extensive experimental evaluation, our model can achieve an average recognition accuracy of 87.3% in various scenarios, demonstrating the potential of WarmGait in Re-ID.},
  archive      = {J_TMC},
  author       = {Hongbo Jiang and Lei Ye and Jingyang Hu and Xiaotian Chen and Siyu Chen and Wei Zhang and Kehua Yang},
  doi          = {10.1109/TMC.2025.3608447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WarmGait: Thermal array-based gait recognition for privacy-preserving person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint inference offloading and model caching for small and large language model collaboration. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), with advanced content creation and inference capabilities, can provide immersive intelligent services to users in mobile edge networks. However, the increasing demand for real-time artificial intelligence (AI) applications aggravates the limitations of cloud-based LLMs due to the long response time. Meanwhile, Small Language Models (SLMs), which are cost-effective and locally deployable for terminal devices, can serve as an efficient supplement to LLMs for performing latency-sensitive tasks with lower generalization capability. Due to the resource constraints of edge networks and the diverse requirements of user tasks, it is critical to design an inference framework that effectively coordinates the deployment and collaboration of LLMs and SLMs. In this paper, we propose an LLM-SLM collaborative inference (LSCI) scheme under a mobile edge computing (MEC) architecture, which jointly decides where to cache models and how to offload inference tasks to balance latency, accuracy, and resource costs. To optimize inference performance subject to resource constraints, we jointly solve the inference task offloading and model caching problem in LSCI scheme. Specifically, we employ deep reinforcement learning (DRL) to select highly popular SLMs to be cached on the edge server, and distributed belief propagation technique to solve the associated inference task offloading issue. Numerical results show that the proposed LSCI scheme can achieve significant performance gain in terms of inference performance when compared with a number of baseline solutions.},
  archive      = {J_TMC},
  author       = {Xinyi Xu and Gang Feng and Yijing Liu and Shuang Qin and Jian Wang and Yunxiang Wang},
  doi          = {10.1109/TMC.2025.3608303},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint inference offloading and model caching for small and large language model collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising and adaptive online vertical federated learning for sequential multi-sensor data in IIoT. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of computational capabilities in edge devices such as intelligent sensors in the Industrial Internet of Things (IIoT), these sensors evolving beyond simple data collection to support complex computational tasks. This advancement provides new opportunities for adopting distributed learning approaches in IIoT. In this study, we focus on enhancing learning performance in an industrial assembly line scenario where multiple distributed sensors sequentially collect real-time data with distinct feature spaces. However, existing research lacks an online distributed learning framework tailored for such IIoT settings. To address this gap, we propose the Denoising and Adaptive Online Vertical Federated Learning (DAO-VFL) algorithm, a novel algorithm that leverages the computing potential of edge sensors while addressing key challenges such as communication overhead and data privacy. DAO-VFL effectively manages continuous data streams and adapts to shifting learning objectives. Furthermore, it can address critical challenges prevalent in industrial environment, such as communication noise and heterogeneity of sensor capabilities. To support the proposed algorithm, we provide a comprehensive theoretical analysis, highlighting the effects of noise reduction and adaptive local iteration decisions on the regret bound. Experimental results on two real-world datasets further demonstrate the superior performance of DAO-VFL compared to benchmarks.},
  archive      = {J_TMC},
  author       = {Heqiang Wang and Xiaoxiong Zhong and Kang Liu and Fangming Liu and Weizhe Zhang},
  doi          = {10.1109/TMC.2025.3608244},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Denoising and adaptive online vertical federated learning for sequential multi-sensor data in IIoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical MAFDRL-based resource allocation and incentive mechanism for TN-NTN in 6G networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3608291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of existing wireless networks for demanding applications like brain-computer interfaces and intelligent transportation systems, we propose an advanced framework for joint resource allocation and task offloading across integrated terrestrial and non-terrestrial networks (TN-NTN). This framework utilizes multiple layers, including ground users, UAVs, HAPs, and satellites, to improve service quality and immersive experiences, particularly in scenarios like Metaverse applications. Ground users request resources, while UAVs and HAPs serve as resource providers, and satellites ensure reliable communication during emergencies. A double auction-based incentive scheme is employed in which operators control UAV and HAP resources to maximize utility, and users aim to minimize computation costs and protect data privacy. To handle the complexity of the operator-user interaction, which results in an NP-hard optimization problem, we applied a hierarchical multi-agent federated deep reinforcement learning (FeDRL) approach. Our simulation results demonstrate that the FeDRL algorithm significantly improves social welfare by 6.38%, 17.43%, and 28.73% over modified MADDPG, FRL, and DDPG algorithms, respectively.},
  archive      = {J_TMC},
  author       = {Abegaz Mohammed Seid and Aiman Erbad and Hayla Nahom Abishu and Gordon Owusu Boateng and Latif U. Khan and Carla Fabiana Chiasserini and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3608291},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A hierarchical MAFDRL-based resource allocation and incentive mechanism for TN-NTN in 6G networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data divergence-aware client selection via knowledge graph for federated LLM fine-tuning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of edge devices and growing awareness of privacy protection, recent developers have transformed to fine-tune LLMs via federated learning instead of centralized training. Federated fine-tuning can leverage distributed data sources and computation power, but it also suffers from system and statistical heterogeneity. Client selection is an effective tool to solve the system and statistical heterogeneity in FL, but existing client selection schemes that involve online measurement will not be as effective in LLM fine-tuning as in conventional FL due to the huge LLM size and fewer fine-tuning rounds. In this paper, to the best of our knowledge, we are the first to consider both system and statistical heterogeneity in federated LLM fine-tuning, and we formulate a new latency minimization problem. We propose to measure client data overlap via knowledge graph offline to assist client selection in federated LLM fine-tuning. Our client selection scheme excels in both model accuracy and fine-tuning latency. We evaluate our scheme via two LLMs and two applications via four datasets. The experiment results illustrate that our scheme achieves the highest accuracy while 2.05x faster than the baselines.},
  archive      = {J_TMC},
  author       = {Bihai Zhang and Dan Wang and Yifei Zhu and Zhu Han},
  doi          = {10.1109/TMC.2025.3608172},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Data divergence-aware client selection via knowledge graph for federated LLM fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreeEnv: Enabling zero-effort RF-based micro-environment changes monitoring. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, a major issue of WiFi-based sensing technologies is how to adapt to changes in the surrounding environment. The extreme sensitivity of Channel State Information (CSI) makes many WiFi sensing arts frustrated when applied to the complex and unknown real world. To solve this problem, in this paper, we propose freeEnv designed to automatically identify the micro-environmental changes (even tiny movements of the laptop) using WiFi devices, which can coexist with other WiFi sensing tasks with zero effort. To achieve automatic identification of micro-environmental changes, we quantify micro-environmental changes based on the physical propagation laws of WiFi signals and the main factors that affect CSI measurements. Then, we design a micro-environmental changes identification method, which determines whether the environment has changed by calculating the Earth Mover's Distance (EMD) of the Probability Density Function (PDF) of continuous CSI, without requiring training data. To remove the influence of dynamic human behaviors, we design a human dynamic detection scheme, which is achieved by obtaining the average inter-cluster distance of performing Gaussian Mixture Model (GMM) clustering on CSI. We evaluate freeEnv in real-world scenarios with six different hardware, four different scenarios, and twenty-four ways of micro-environmental changes. The results show that our method is robust to different devices and scenarios, and can achieve the average precision of 96.1% and 93.2% for micro-environmental changes identification and human dynamic behavior detection. By testing on a case study of threshold-based human presence detection, freeEnv can effectively improve the detection performance.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Feiyu Han and Mingzhu Yang and Shanyue Wang and Panlong Yang and Yubo Yan},
  doi          = {10.1109/TMC.2025.3608245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreeEnv: Enabling zero-effort RF-based micro-environment changes monitoring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiered spatio-temporal difficulty: Curriculum scheduler for multi-sensor traffic flow prediction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of the Internet of Things (IoT) has enhanced smart city services for traffic monitoring, leading to numerous schemes for accurate flow prediction based on traffic sensors. However, existing approaches primarily capture spatio-temporal (ST) dependencies from traffic graphs and train their models using randomly ordered data. This overlooks the fact that the modeling difficulty of each sensor/node in the ST traffic graph can vary significantly due to its spatial dependencies and temporal trends, resulting in unreliable and unstable predictions in IoT scenarios. In this context, we argue that a well-designed curriculum with an easy-to-difficult order can improve the training of ST models. Therefore, this paper introduces an ST difficulty measurer to score the node-level difficulty of traffic graph from both spatial and temporal aspects, and then implements a curriculum in the ST model training process. More specifically, based on the tiered ST difficulty score, the ST model training begins with a subgraph consisting of “easy” nodes characterized by relatively consistent spatial relationships and regular temporal patterns. Gradually, more difficult nodes are incorporated into the subgraph and participate in subsequent training stages. Comprehensive experiments and analysis on two real-world traffic flow datasets confirm the effectiveness of our proposed approach.},
  archive      = {J_TMC},
  author       = {Zhiwen Zhang and Hongjun Wang and Zipei Fan and Renhe Jiang and Wei Yuan and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TMC.2025.3608620},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Tiered spatio-temporal difficulty: Curriculum scheduler for multi-sensor traffic flow prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deterministic backoff approach for wi-fi and NR-U coexistence in shared bands. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unlicensed (shared) bands, wireless technologies typically operate without central coordination, which can lead to unwanted transmission interruptions, collisions, and resource wastage. We focus on Wi-Fi and NR-U coexistence in shared bands and solve the aforementioned problem with a deterministic backoff approach. The proposed scheme allows active transmitters to learn the number of nearby interferers in a distributed manner and, as a result, implement an ordered round-robin transmission schedule to minimize collisions. We show analytically that the scheme converges not only in equilibrium (when collisions are negligible), but also in a general case (when collisions are frequent at the beginning or in the middle of the proposed scheme's operation). Furthermore, extensive simulations prove that the proposed scheme guarantees fairness between contenting cells and technologies (both in terms of throughput and delay) as well as optimizes channel efficiency. We also study the impact of imperfect readings of the number of contending nodes (which may be the result of, e.g., asymmetric channel conditions) on the performance of the proposed scheme. In most cases, the deterministic backoff approach outperforms legacy channel access schemes. Additionally, our proposal does not add additional signaling overhead and is backward compatible with standard Wi-Fi and NR-U operation.},
  archive      = {J_TMC},
  author       = {Ilenia Tinnirello and Menzo Wentink and Alice Lo Valvo and Szymon Szott and Katarzyna Kosek-Szott},
  doi          = {10.1109/TMC.2025.3608270},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A deterministic backoff approach for wi-fi and NR-U coexistence in shared bands},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwinULoc: Pre-trained swin transformer U-net with ToF offset correction for resource-efficient WiFi indoor localization. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquity of WiFi infrastructure has motivated significant research into WiFi-based indoor positioning systems as practical alternatives to GNSS. While deep learning approaches show promise, existing models face three critical limitations: (1) inadequate modeling of long-range feature dependencies, (2) difficulty in correcting time-of-flight (ToF) offsets induced by device clock asynchrony, and (3) prohibitive computational costs for environmental adaptation through model retraining. This paper introduces SwinULoc, a novel U-shaped indoor positioning framework that synergizes Swin Transformer blocks with 2D CSI heatmap processing. Our architecture uniquely addresses these challenges through three key innovations: First, the integration of shifted window attention mechanisms enables effective learning of long-range signal correlations. Second, a multi-access-point fusion strategy enhanced with skip connections achieves precise ToF offset correction through cross-device pattern analysis and multi-scale feature integration. Third, a transfer learning paradigm reduces retraining costs by 75% compared to conventional approaches. Extensive evaluations demonstrate SwinULoc's superiority, achieving 70% higher positioning accuracy than state-of-the-art baselines while requiring only 1/4 of the training resources for new environments.},
  archive      = {J_TMC},
  author       = {Wenhao Zhang and Xingfa Shen and Sicong Xia and Zhibo Wang},
  doi          = {10.1109/TMC.2025.3608157},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SwinULoc: Pre-trained swin transformer U-net with ToF offset correction for resource-efficient WiFi indoor localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semi-supervised indoor localization algorithm based on probabilistic distribution modeling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in wireless technology have spurred the growth of indoor localization applications based on wireless signals. Most existing methods model the relationship between input data and the target's location using the $l_{2}$ loss function, which assumes that the residual between the predicted location and the ground truth follows a Gaussian distribution with a fixed standard deviation. Nevertheless, this approach may not conform to the actual distribution and lacks confidence measures for individual predictions, potentially compromising accuracy. Furthermore, in the realm of regression, Semi-Supervised Learning (SSL) remains relatively unexplored due to the absence of a reliable method to quantify prediction uncertainty, especially when labeled data is scarce. To address these challenges, we developed a novel indoor localization algorithm that employs probabilistic distribution modeling. Our approach focuses on indoor localization with Channel Impulse Response (CIR) as the input. Crucially, it leverages Maximum Likelihood Estimation (MLE) to model the localization error as a Gaussian distribution, and utilizes Residual Log-likelihood Estimation (RLE) to capture arbitrary error distributions. This enables us to extract the confidence of each prediction and utilize the most reliable ones as pseudo-labels for the unlabeled data. By employing probabilistic distribution modeling, we observed a significant improvement in localization accuracy over the $l_{2}$ loss function. Additionally, by integrating pseudo-labeled data for model retraining, our algorithm achieves superior performance compared to existing state-of-the-art machine learning-based and SSL-based methods. This is demonstrated by our evaluation on two public datasets, showcasing the efficiency of the proposed method.},
  archive      = {J_TMC},
  author       = {Ruofei Gao and Xiaotao Li and Wai Chen},
  doi          = {10.1109/TMC.2025.3608276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A semi-supervised indoor localization algorithm based on probabilistic distribution modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEDS: An affinity-driven efficient DRL-based task scheduling framework for edge computing. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3608263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a promising paradigm that deploys computing resources at the network edge to provide services. Many existing solutions leverage deep reinforcement learning (DRL) to optimize task scheduling, yet they often rely on global scheduling approaches. However, such solutions result in an excessively large decision search space, reducing task scheduling efficiency in complex environments. Additionally, the cold start problem impedes the generation of optimal scheduling strategies. To address these challenges, we propose AEDS, a DRL-based task scheduling framework designed to enhance scheduling efficiency. AEDS optimizes the decision-making process from three aspects: (1) Decision Space Reduction. AEDS incorporates a novel affinity matching mechanism that identifies the most suitable edge cluster based on task characteristics, thereby significantly narrowing the decision search space. (2) Decision Process Optimization. AEDS adopts a hybrid strategy combining offline pre-training and online fine-tuning to address the cold start problem. Offline pre-training with historical task data ensures effective initial scheduling, while online fine-tuning periodically updates the DRL model to enhance long-term adaptability to dynamic system changes. (3) Decision Strategy Calibration. AEDS proposes a task migration solution to adapt to real-time workload variations dynamically. It utilizes triple queues to assess server overload and dynamically calibrates the scheduling strategy through task migration within interconnected clusters. Comprehensive experimental results validate the efficacy of AEDS. Compared with existing frameworks, AEDS reduces task latency by $28.23\%$ and enhances task completion rate by $10.28\%$. Furthermore, by effectively narrowing the decision scope, AEDS accelerates the decision-making process by a remarkable $88.06\%$},
  archive      = {J_TMC},
  author       = {Xu Liu and Zhaolong Jian and Xueshuo Xie and Qiankun Dong and Mulin Li and Xiaoyu Zhang and Tao Li},
  doi          = {10.1109/TMC.2025.3608263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AEDS: An affinity-driven efficient DRL-based task scheduling framework for edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal online federated learning with modality missing in internet of things. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3608269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) ecosystem generates vast amounts of multimodal data from heterogeneous sources such as sensors, cameras, and microphones. As edge intelligence continues to evolve, IoT devices have progressed from simple data collection units to nodes capable of executing complex computational tasks. This evolution necessitates the adoption of distributed learning strategies to effectively handle multimodal data in an IoT environment. Furthermore, the real-time nature of data collection and limited local storage on edge devices in IoT call for an online learning paradigm. To address these challenges, we introduce the concept of Multimodal Online Federated Learning (MMO-FL), a novel framework designed for dynamic and decentralized multimodal learning in IoT environments. Building on this framework, we further account for the inherent instability of edge devices, which frequently results in missing modalities during the learning process. We conduct a comprehensive theoretical analysis under both complete and missing modality scenarios, providing insights into the performance degradation caused by missing modalities. To mitigate the impact of modality missing, we propose the Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype learning to effectively compensate for missing modalities. Experimental results on two multimodal datasets further demonstrate the superior performance of PMM compared to benchmarks.},
  archive      = {J_TMC},
  author       = {Heqiang Wang and Xiang Liu and Xiaoxiong Zhong and Lixing Chen and Fangming Liu and Weizhe Zhang},
  doi          = {10.1109/TMC.2025.3608269},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multimodal online federated learning with modality missing in internet of things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sym-FEC: Enhancing error correction in LoRa PHY with a symbol-level FEC decoder. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3608893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, a leading wireless technology for Low Power Wide Area Networks (LPWAN), is well-known for its long transmission range and low power consumption. The extended range is primarily attributed to the Chirp Spread Spectrum technique. However, the LoRa physical layer (LoRa PHY) contributes only marginally to this advantage, as it employs an inefficient Forward Error Correction (FEC) strategy for error recovery. In this paper, we introduce Sym-FEC, a symbol-level FEC decoder designed to link the received signals' spectrum with the coding correlations inherent in LoRa PHY, thereby enhancing error recovery. The key enabler of Sym-FEC is signal copy retrieval. We begin by facilitating signal copy conversion between two symbols and extend this to the general case, where signal copy conversions can be performed between any symbols in a coding block. Approaches are also introduced to assess the validity of the block-wide decoding results. Extensive hardware evaluations demonstrate that Sym-FEC provides Signal-to-Noise-Ratio (SNR) improvement of 2.3dB to 3dB compared to the traditional decoder in LoRa PHY. Sym-FEC requires no modifications at the transmitter while incurs low storage and computational complexity at the gateway, thus can be easily integrated into gateway nodes.},
  archive      = {J_TMC},
  author       = {Weiwei Chen and Xianjin Xia and Shuai Wang and Xianjun Deng and Jiehong Wu and Caishi Huang},
  doi          = {10.1109/TMC.2025.3608893},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sym-FEC: Enhancing error correction in LoRa PHY with a symbol-level FEC decoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A privacy-preserving auction for task offloading and resource allocation in UAV-assisted MEC. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a complementary solution for Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAVs) can temporarily provide reliable and flexible offloading services when edge servers are damaged or unavailable. However, existing UAV-assisted MEC systems suffer from issues such as uneven resource allocation, low utilization efficiency, load imbalance, and poor dynamic adaptability, affecting service quality. Moreover, sensitive user equipment (UE) information faces leakage during the computational process of UAVs. How to jointly optimize the scheduling of servers and UAVs for task offloading and resource allocation without compromising UEs' privacy remains a significant challenge. Thus, this paper proposed a privacy-preserving auction framework (namely Prizty) by considering the trajectory of UAVs, their constrained energy and computational capabilities, and the variability in UE distribution. Prizty employs a combinatorial obfuscation method to protect UEs' privacy and links bidding prices to computational resources and energy characteristics. It calls the sub-algorithm WPA to determine the winners by balancing social costs and utility. Theoretical analysis demonstrates that Prizty satisfies truthfulness and individual rationality while maintaining scalability for large-scale resource allocation problems. Extensive experiments on real-world datasets validate Prizty's effectiveness in critical metrics, including offload rate, average service latency, energy consumption, and social cost.},
  archive      = {J_TMC},
  author       = {Jiajie Xu and Xiaolong Xu and Guangming Cui and Muhammad Bilal and Rong Gu and Wanchun Dou and Arumugam Nallanathan},
  doi          = {10.1109/TMC.2025.3609202},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A privacy-preserving auction for task offloading and resource allocation in UAV-assisted MEC},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the timeliness of radio channel access: Random access or scheduled access?. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the role of channel access schemes in enhancing the timeliness of status updates in sensor networks. Specifically, we model the large-scale sensor network as a Poisson cellular network and derive the network average age of information (AoI) under five different channel access schemes: slotted ALOHA, frame slotted ALOHA, random scheduling, round robin, and channel-aware. These schemes are categorized based on random vs. scheduled access and non-channel-aware vs. channel-aware. Our goal is to investigate when the additional overhead and complexity introduced by scheduling and channel state information (CSI) are beneficial, enabling better decisions in network design. Our findings reveal that the effectiveness of these schemes is influenced by the signal-to-interference ratio (SIR) decoding threshold, which often reflects the length of communication data. For short-packet communications, the performance differences among various channel access strategies are minimal, and the gains from scheduling are limited. Additionally, the inclusion of extra CSI does not yield performance improvements; in fact, some simple scheduling strategies, along with channelaware strategy that leverage CSI, may not outperform basic random access methods. Among the protocols we examined, the round robin scheme achieves the best performance. In contrast, scheduled access schemes exhibit a clear performance advantage in long-packet communications. Furthermore, the channel-aware scheme significantly enhances the network AoI performance, particularly in networks with higher transmitter competition.},
  archive      = {J_TMC},
  author       = {Zhiling Yue and Yuting Tang and Nikolaos Pappas and Yaru Fu and Tony Q. S. Quek and Howard H. Yang},
  doi          = {10.1109/TMC.2025.3609285},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On the timeliness of radio channel access: Random access or scheduled access?},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative access with waiting window: Enhancing age-of-information in CSMA networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3609393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next generation of industrial Internet-of-Things systems demands timely delivery of fresh data, quantified by the Age-of-Information (AoI) metric, from a vast number of sensors. In such systems, sensors usually adopt random access where the back-off mechanism is mostly crucial. This paper studies the back-off mechanism for Carrier Sense Multiple Access (CSMA) systems in collision-prone scenarios. Considering the fact that transmitting immediately after a prior successful transmission yields limited AoI reduction, we propose an enhanced waiting CSMA scheme by introducing a waiting window to the existing contention window. In the proposed scheme, each source waits after each successful transmission. It enables a collaborative channel access strategy, such that sources have recently transmitted grant greater opportunities for others to access the channel. To optimize the window sizes, we analyze the Normalized Average Peak AoI (NPAoI) and Normalized Average AoI (NAoI), which are normalized by the packet transmission duration. Our findings indicate that minimizing these metrics is feasible through maintaining a nearly constant collision probability. Specifically, NPAoI reaches its minimum by adjusting either the contention or the waiting window, while NAoI requires a larger waiting window combined with a smaller contention window. Given the network scale, we derive optimal window sizes in closed form. When the network scale is unknown or dynamic, we propose a data-driven approach to adjust window sizes based solely on the real-time measured collision probability. Simulation results confirm that the proposed waiting CSMA scheme significantly outperforms existing methods for both NPAoI and NAoI, particularly in large-scale networks.},
  archive      = {J_TMC},
  author       = {Yifan Gu and Suzhi Bi and Zhaoxu Wang and Zhi Quan},
  doi          = {10.1109/TMC.2025.3609393},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative access with waiting window: Enhancing age-of-information in CSMA networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow prioritization in asynchronous TSN with multiple ATS instances. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3609519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-sensitive networking (TSN) is an essential technology for the development of deterministic networks as it can offer deterministic quality of service (QoS) in terms of transmission delay. In addition, it is more scalable, more affordable, and simplifies the management of current industrial networks. This paper focuses on asynchronous traffic shaping (ATS) in TSN and proposes a novel solution to prioritize the flows being transmitted in a TSN with multiple ATS instances to meet their delay requirements. To this end, we formally formulate the flow prioritization assignment problem in an ATS-TSN network, demonstrate the correctness of the proposed algorithm, and study the solution's scalability. The results show that our solution is optimal, obtaining a shorter execution time than an exhaustive search with the same prioritization result. Furthermore, our solution scales correctly as a function of the number of flows with a considerably low execution time. Compared to another ATS prioritization method, our solution finds feasible solutions four orders of magnitude larger with reduced execution time. Moreover, the results show that per-flow prioritization has higher utilization than per-Priority Code Point (PCP) prioritization. Finally, an increase in the percentage of traffic with strict delay requirements harms the maximum achievable utilization.},
  archive      = {J_TMC},
  author       = {Julia Caleya-Sanchez and Jonathan Prados-Garzon and Pablo Muñoz and Juan M. Lopez-Soler and Pablo Ameigeiras},
  doi          = {10.1109/TMC.2025.3609519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Flow prioritization in asynchronous TSN with multiple ATS instances},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E2E hybrid computation offloading for complex MEC system. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3609546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) places computational resources at the network edge, thereby enabling compute-intensive applications through task offloading. However, in dynamic multi-user, multi-server environments, user mobility induces time-varying channel conditions, and the spatiotemporal heterogeneity of server loads further complicates system behavior. Consequently, the system must jointly optimize discrete offloading decisions and continuous resource-allocation parameters, forming a hybrid action space whose integrated decision-making mechanism is central to breaking the long-standing trade-off between latency and energy consumption. Traditional deep reinforcement learning (DRL) approaches that rely on a single policy network often suffer from strong strategy coupling and Q-value estimation bias, leading to policy oscillations and the curse of dimensionality in highly dynamic scenarios and thus impeding stable convergence. To address this problem, this paper proposes an innovative End-to-End Hybrid Computation Offloading (E2EHCO) framework based on an enhanced Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. By employing dual critic networks together with a delayed-update mechanism, the method effectively suppresses Q-value overestimation, while the integration of Softmax and Tanh activations in the actor network allows simultaneous handling of discrete and continuous actions, thereby achieving efficient and robust joint decision optimization under dynamically changing conditions. Experiments on real-world mobility traces show that, relative to benchmark methods, E2EHCO reduces total latency by at least 20% and energy consumption by approximately 16% in high-density user scenarios, providing an adaptive offloading solution with real-time responsiveness for large-scale, dynamic MEC systems.},
  archive      = {J_TMC},
  author       = {Jingjing Zhang and Xiaoheng Deng and Jian Yin and Xianjun Deng and Xuechen Chen and Jinsong Gui and Shichao Zhang},
  doi          = {10.1109/TMC.2025.3609546},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {E2E hybrid computation offloading for complex MEC system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving inter-logical channel interference for large-scale LoRa deployments. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3609316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWANs are envisioned to connect billions of IoT devices through thousands of physically overlapping yet logically orthogonal channels (termed logical channels). These logical channels hold significant potential for enabling highly concurrent scalable IoT connectivity. Large-scale deployments however face strong interference between logical channels. This practical issue has been largely overlooked by existing works but becomes increasingly prominent as LoRaWAN scales up. To address this issue, we introduce Canas, an innovative gateway design that is poised to orthogonalize the logical channels by eliminating mutual interference. To this end, Canas develops a series of novel solutions to accurately extract the meta-information of individual ultra-weak LoRa signals from the received overlapping channels. The meta-information is then leveraged to accurately reconstruct and subtract the LoRa signals over thousands of logical channels iteratively. Real-world evaluations demonstrate that Canas can enhance concurrent transmissions across overlapping logical channels by 2.3× compared to the best known related works.},
  archive      = {J_TMC},
  author       = {Shiming Yu and Ziyue Zhang and Xianjin Xia and Yuanqing Zheng and Jiliang Wang},
  doi          = {10.1109/TMC.2025.3609316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resolving inter-logical channel interference for large-scale LoRa deployments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing learning to communicate with reward-shaped curriculum and network awareness. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3608813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication enhances collaboration among artificial intelligence agents, for example, by sharing observations that contribute to safer driving. Given the conflicts between limited communication resources and communication needs, learning effective communication strategies is essential. We observe that incorporating learning to communicate can complicate mastering primary tasks, like vehicle control, the original focus in autonomous driving. This is due to the uncertainty in information acquisition during the learning process, which can lead to an unstable environment for primary tasks. In this paper, we introduce ReSCOM, an efficient joint learning framework that combines learning-to-communicate with primary tasks. ReSCOM progressively adjusts the learning emphasis through rewardshaped curriculum, allowing agents to shift their focus from primary tasks and basic communication tasks (e.g., how to encode) to advanced communication strategies (e.g., determining when it is worthwhile to communicate). This approach minimizes the impact on the learning efficiency of primary tasks while simultaneously facilitating communication learning. Besides, we explore the extent to which communication channel states (i.e., delays and packet loss) and protocols impact agent cooperation and learning. We evaluate ReSCOM against state-of-the-art methods in various tasks, demonstrating its strong performance. Furthermore, we verify that current modern wireless channels, includingWi-Fi, 4G, and 5G, provide low enough delays that their impact can be ignored. When packet loss occurs, we find that the UDP protocol performs better than TCP because, for agent cooperation, timely information is more valuable than reliability.},
  archive      = {J_TMC},
  author       = {Xinghai Wei and Jie Yuan and Tingting Yuan and Xiang Liu and Xiaoming Fu},
  doi          = {10.1109/TMC.2025.3608813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing learning to communicate with reward-shaped curriculum and network awareness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and security analysis of SDN-based IoT-oriented blockchain protected E-voting system. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3609480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electoral system is one of the fundamental pillars of democracy, but the traditional voting system suffers from several limitations such as fraud voting, vote tampering, impersonation, and inefficiencies. To overcome these limitations, several research works have been initiated to design a blockchain-based e-voting system. These designs addressed the loopholes of the existing ones to a limited extent. Here, a novel multi-level blockchain-secured SDN-based IoT enabled e-voting system has been proposed. The proposed system consists of booth, district, state, and country level systems. Here, a voter needs to be authenticated at the booth-level and then this valid vote data can be propagated to the upper hierarchical levels and stored there after signing and encrypting it using ECDSA and ECC respectively. Man-in-the-middle attacks, DoS/DDoS, unauthorized access, and impersonation attacks are avoided using flow rules in SDN controllers and firewalls installed in the servers. Furthermore, blockchain technology provides security for voting data stored at all levels. The security strengths were tested at different levels (e.g., programming, operating system, and network level) using open-source tools (i.e. scyther, nmap, metasploit, etc.). The performance of the proposed architecture was evaluated satisfactorily in a testbed. It also performed satisfactorily under both normal and stressed conditions in a scaled-up environment.},
  archive      = {J_TMC},
  author       = {Ngangbam Indrason and Kalyan Baital and Goutam Saha},
  doi          = {10.1109/TMC.2025.3609480},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and security analysis of SDN-based IoT-oriented blockchain protected E-voting system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient and scalable asynchronous federated learning via stragglers version control. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous Federated Learning (AFL) has emerged as a promising paradigm to address the challenges posed by heterogeneous device environments in federated learning systems. However, the problem of low accuracy and slow convergence due to inconsistent updates from stragglers and normal clients remains severe in AFL. Previous works either discard or penalize the updates from stragglers, which can lead to the loss of valuable data or introduce bias into the model. Furthermore, existing AFL frameworks integrating synchronous optimization algorithms face the challenges of weak compatibility and scalability, limiting large-scale training. In this paper, we propose DVAFL1 an efficient and scalable AFL framework that significantly improves the performance of AFL in terms of model accuracy and convergence speed by effectively utilizing and compensating for the updates from stragglers, while naturally integrating synchronous optimization algorithms. Specifically, DVAFL introduces a dynamic window protocol for adaptive aggregation to balance the contribution of stragglers and ensure faster and more stable convergence. Further, the version control mechanism corrects stale gradients by compensating for the missed model updates of stragglers, thereby improving model performance. Extensive experiments on three public datasets demonstrate that DVAFL achieves an average convergence speed 2.3× faster and an accuracy improvement of 5.5% compared to state-of-the-art AFL methods.},
  archive      = {J_TMC},
  author       = {Chuyi Chen and Yanchao Zhao and Zhe Zhang and Wenzhong Li and Jie Wu},
  doi          = {10.1109/TMC.2025.3609568},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards efficient and scalable asynchronous federated learning via stragglers version control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation resource management in mobile edge computing for healthcare using lyapunov-deep deterministic policy gradient. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3608771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mobile edge computing healthcare (MECH) system, the integration of MECH servers and wearable medical sensors can achieve real-time monitoring and analysis of user health. However, the system still faces key challenges such as network security risks and high energy consumption. To address these issues, this paper proposes a dual pronged solution. First, a new mechanism integrating smart contracts and asymmetric encryption is designed to achieve secure and efficient user authentication. Then, a method called Lyapunov Deep Deterministic Policy Gradient (L-DDPG) has been proposed to solve the resource optimization of the system. L-DDPG utilizes the Lyapunov optimization framework to transform the original long-term average constraint optimization problem into an instant optimization problem for each time slot, and solves the system optimization variables using the Deep Deterministic Policy Gradient algorithm. Through this design, L-DDPG effectively combines the advantages of Lyapunov optimization in ensuring system stability, as well as the decision modeling ability of deep reinforcement learning in complex state spaces, thereby simultaneously improving the resource utilization efficiency and safety of the system. The experimental results show that compared with existing baseline methods, L-DDPG significantly reduces the average energy consumption of equipment while effectively reducing task response delay, demonstrating better overall performance.},
  archive      = {J_TMC},
  author       = {Qiang He and Yang Xia and Zheng Feng and Lianbo Ma and Yingjie Lv and Keping Yu and Ammar Hawbani and Kaifa Zheng and Li Xu},
  doi          = {10.1109/TMC.2025.3608771},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computation resource management in mobile edge computing for healthcare using lyapunov-deep deterministic policy gradient},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-layer position-pose fusion framework for joint magnetoquasistatic field and IMU positioning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3608822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetoquasistatic (MQS) field positioning has demonstrated significant potential for emergency rescue applications due to its strong penetration and non-reliance on pre-deployment. However, its accuracy is notably impaired by metal interference and distance attenuation. Inertial Measurement Units (IMUs) can reliably provide motion data even in environments affected by metal and electromagnetic interference, but they suffer from cumulative drift over time. Effectively, combining MQS field and IMU positioning to harness their respective advantages presents a crucial challenge. To address this, we propose a Multi-Layer Position-Pose Fusion (MP2F) framework that integrates MQS field with IMU data to enhance position and pose estimation. The MP2F framework comprises three layers: a Quaternion-based Pose Fusion Layer (QPFL), a Kalman Filter-based Position Fusion Layer (KFFL), and a Global Position-Pose Fusion Layer (GP2FL). Specifically, QPFL utilizes the Extended Kalman Filter (EKF) to effectively mitigate magnetic field distortion and IMU drift, thereby significantly enhancing pose estimation precision. Next, KFFL incorporates the fused pose estimation from QPFL into an inertial navigation motion model, and leverages MQS field observations to further improve positional accuracy. Finally, GP2FL formulates a nonlinear least squares optimization problem by marginalizing prior factors, inertial sensor factors, and Kalman fusion outputs, enabling globally optimized state estimation. Comprehensive simulation results and analyses prove that the proposed MP2F framework achieves high-precision position and pose estimation in complex emergency scenarios, with strong robustness. Experimental results in real-world environments show that the proposed MP2F achieves improvements in positioning accuracy of 61.1%, 58.7%, 48.4%, and 50.2% over EKF, iMag+, GWO-PF, and MagLoc, respectively.},
  archive      = {J_TMC},
  author       = {Bocheng Qian and Lei Huang and Xiansheng Guo and Gordon Owusu Boateng and Rui Ma and Nirwan Ansari},
  doi          = {10.1109/TMC.2025.3608822},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A multi-layer position-pose fusion framework for joint magnetoquasistatic field and IMU positioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing dynamic task assignment in spatial crowdsourcing: Bilateral preference-aware approaches. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3603833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task assignment is a crucial challenge in spatial crowdsourcing (SC). Most existing studies have two limitations: Firstly, only one-sided preferences of workers or tasks are taken into account, and the satisfaction of workers or tasks could be improved; Secondly, tasks are always assigned based on the current locations of workers, which is not suitable for many real-life applications, such as carpooling, where the trajectories of workers require to be taken into account. To this end, we investigate a new problem of Bilateral Preference-aware Dynamic Task Assignment (BDTA), which is proven to be NP-hard, to maximize overall satisfaction by incorporating worker-task bilateral preferences and assigns tasks using the trajectories of workers. For the BDTA problem, we first propose a hybrid batch processing framework to address uneven data distribution. After that, a task-initiated bidirectional select algorithm is proposed to mitigates the impact of task order on the matching results. Furthermore, we propose an $\alpha$-approximate task-initiated generalized deferred-acceptance algorithm and a reverse generalized deferred-acceptance algorithm to enhance the stability and overall satisfaction of task assignment results. Extensive experiments are conducted on both real and synthetic datasets to validate the effectiveness and efficiency of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Yang Huang and Yumeng Liu and Xu Zhou and Tianyue Ren and Zhibang Yang and Keqin Li and Kenli Li},
  doi          = {10.1109/TMC.2025.3603833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing dynamic task assignment in spatial crowdsourcing: Bilateral preference-aware approaches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-efficient identifying key tag distribution in large-scale RFID systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of RFID-enabled applications, large-scale RFID systems often require multiple readers to ensure full coverage of numerous tags. In such systems, we sometimes pay more attention to a subset of tags instead of all, which are called key tags. This paper studies an under-investigated problem key tag distribution identification, which aims to identify which key tags are beneath which readers. This is crucial for efficiently managing specific items of interest, which can quickly pinpoint key tags and help RFID readers covering these tags collaborate to improve the tag inventory efficiency. We propose a protocol called Kadept that identifies the key tag distribution by designing a sophisticated Cuckoo filter that teases out key tags as well as assigns each of them a singleton slot for response. With this design, a great number of trivial (non-key) tags will keep silent and free up bandwidth resources for key tags, and each key tag is sorted in a collision-free way and can be identified with only 1-bit response, which significantly improves the time efficiency. To enhance the scalability and efficiency of Kadept for high key tag proportions, we propose E-Kadept protocol, which accelerates the identification process by designing an incremental Cuckoo filter that reduces false positives and improves space efficiency. We theoretically analyze how to optimize protocol parameters of Kadept and E-Kadept, and conduct extensive simulations under different tag distribution scenarios. Compared with the state-of-the-art, E-Kadept can improve the time efficiency by a factor of 1.75×, when the ratio of key tags to all tags is 0.3.},
  archive      = {J_TMC},
  author       = {Yanyan Wang and Jia Liu and Zhihao Qu and Shen-Huan Lyu and Bin Tang and Baoliu Ye},
  doi          = {10.1109/TMC.2025.3609967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Time-efficient identifying key tag distribution in large-scale RFID systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE maximization for laser-powered multi-UAV communication networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3610026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) communication is expected to play an important role in many applications, including emergency services, remote surveillance and even daily logistics, thanks to its great advantages such as flexible deployment and mobility support. In this paper, we propose a multi-UAV-aided communication scheme supported by laser power transfer (LPT), where the quality-of-experience (QoE) requirements of user equipment (UE) is considered. Specifically, to improve the sum QoE, we first maximize the sum of average data rates (ADRs) of all UEs through an alternating optimization of UAVs' positions, UE-network association and LPT station (LPTS)-network association. Then, we devise an advanced Gale-Shapley rematching (GSRM) scheme to address the intractable 0-1 programming problem in UE/LPTS-network association. Moreover, an L2-norm polynomial (L2NP) programming method is designed to transform the L2NP of the LPT-based UAV positioning problem into a convex form. Finally, a redundant resource reallocation (RRR) algorithm is designed to recycle and reallocate the excessive transmit power and backhaul capacity of both the base station (BS) and the UAVs, to further maximize the number of UEs satisfying the QoE requirements. Simulation results show that the proposed UAV-aided communication scheme can help more UEs achieve their QoE requirements at a reduced power consumption compared with existing solutions.},
  archive      = {J_TMC},
  author       = {Jianchao Chen and Ming Jiang},
  doi          = {10.1109/TMC.2025.3610026},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE maximization for laser-powered multi-UAV communication networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards the age of semantic information: A deep learning-enabled generalized deduplication-based semantic transmission mechanism. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3609792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the upcoming global-coverage 6G networks, high packet loss and long latency in long-distance transmissions exacerbate the trade-off between data timeliness and integrity, particularly in time-sensitive applications involving time-series data with stringent integrity requirements. This challenge exposes the limitations of existing transmission systems, such as source-channel coding and semantic communication, which fail to jointly address both dimensions. In this paper, we propose a deep learning (DL)-enabled generalized deduplication (GD)-based semantic transmission (DLGD-ST) mechanism for time-series data. By leveraging GD to address the impact of semantic ambiguity on data integrity, DLGD-ST exploits the semantic recovery and temporal discreteness of the data to effectively mitigate the conflict between integrity and timeliness. In particular, a well-designed long-short-term memory (LSTM)-based GD algorithm is developed to separate shallow semantic components and supplementary components, ensuring the integrity of semantic transmission. A deep semantic encoding process is then performed using a double-layer progressive dimension reduction (DPDR) and adaptive quantization (AQ) scheme, which capitalizes on the channel robustness of semantics to reduce transmission rounds and improve timeliness. Furthermore, an incremental dimension hybrid automatic repeat request (ID-HARQ) mechanism is introduced to improve semantic reliability by retransmitting high-dimensional semantics, thereby further minimizing end-to-end transmission rounds. To accurately evaluate performance, we introduce the Age of Semantic Information (AoSI), which incorporates integrity constraints into the generalized Age of Information (AoI) to jointly assess integrity and timeliness. Simulation results demonstrate that the proposed DLGD-ST mechanism, enabled by accurate data recovery and reduced transmission rounds, achieves better AoSI performance compared to existing communication systems under both high and low signal-to-noise ratio (SNR) conditions.},
  archive      = {J_TMC},
  author       = {Yunlai Xu and Ronghao Gao and Qinyu Zhang and Zhihua Yang},
  doi          = {10.1109/TMC.2025.3609792},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards the age of semantic information: A deep learning-enabled generalized deduplication-based semantic transmission mechanism},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase-proof: Robust mobile two-factor authentication via phase fingerprinting. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3610154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-factor authentication (2FA) has been widely applied with the proliferation of mobile devices. Currently, many existing 2FA solutions propose to use acoustic fingerprints as the second factor. However, these schemes mainly consider using the magnitude or non-linearity information of the acoustic signal for authentication and ignore the fingerprint variations caused by the change of transmission distance between devices, which could threaten the accuracy of the system. To address these vulnerabilities, we propose Phase-Proof, a secure and robust 2FA that utilizes the phase distortions incurred by the acoustic elements of mobile devices as the second proof. Given the received acoustic signal, our system first extracts phase information from the signal and designs a new distance effect elimination scheme to remove the impact of transmission distances for robust fingerprint extraction. Moreover, our device authentication component then adopts a transfer learning-based method to capture the subtle differences in devices' fingerprints for accurate device authentication. Moreover, to further withstand the co-located attacks, our proximity detection component collects certain environmental randomness from the enrolled phone, and then matches it with the environmental randomness collected from the login device to verify the proximity of two devices. Our experimental results show that our system is accurate in providing 2FA and robust to various attacks across different scenarios.},
  archive      = {J_TMC},
  author       = {Tingyuan Yang and Shuyu Liu and Yanzhi Ren and Haitao Jia and Ziyu Shao and Hongbo Liu and Jiadi Yu and Hongwei Li},
  doi          = {10.1109/TMC.2025.3610154},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Phase-proof: Robust mobile two-factor authentication via phase fingerprinting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed rate limiting under decentralized cloud networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of cloud applications has led to unprecedented increases in network traffic volume, diversity, and complexity. As Cloud Service Providers (CSPs) adopt decentralized, geographically distributed data centers, effective traffic management across these environments has become critical. Distributed Rate Limiting (DRL) has emerged as an essential tool to manage the complex traffic dynamics of decentralized networks, yet traditional centralized rate limiting methods fall short, facing limitations in scalability, adaptability to bursty traffic, and efficiency. This paper presents C3PDAR (Cloud Control with Constant Probabilities and Dynamic Adjustment Range), a novel DRL algorithm tailored for decentralized cloud infrastructures. C3PDAR introduces three key innovations: (1) CPS-BPS DualPoint Rate Limiting and Parent-Child Token Bucket mechanisms, which effectively mitigate burst traffic and short-lived connections while improving bandwidth fairness and inter-tenant isolation; (2) A vSwitch-CGW Cascade Rate Limiting architecture, which reduces CPU overhead in CGW clusters and accelerates convergence by 42%–78%; (3) Virtual Extensible Local Area Network (VXLAN) Padding scheme, which embeds rate-limiting information in existing traffic instead of transmitting new data packets, reducing the communication overhead of the C3PDAR algorithm by over 40%. By integrating these advancements, C3PDAR delivers a scalable, robust solution that outperforms traditional DRL approaches in performance, fault tolerance, and resource efficiency. C3PDAR uniquely empowers CSPs to manage complex, high-volume traffic dynamics in decentralized cloud environments, offering both theoretical insights and practical optimizations for next-generation network control.},
  archive      = {J_TMC},
  author       = {Xiang Hu and Tianyu Xu and Lilong Chen and Xiaochong Jiang and Ye Yang and Liming Ye and Xu Wang and Yilong Lv and Chenhao Jia and Yongwang Wu and Zhigang Zong and Xing Li and Bingqian Lu and Shunmin Zhu and Chengkun Wei and Wenzhi Chen},
  doi          = {10.1109/TMC.2025.3610314},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed rate limiting under decentralized cloud networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentivizing throughput enhancement in blockchain-based energy trading system. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based energy trading (BBET) systems depend on prosumers to allocate energy between trading activities and blockchain mining operations. However, inadequate incentive structures lead prosumers to under-contribute to mining, creating throughput bottlenecks and system performance degradation. This paper introduces the Fee and Two-Piece Compensation (FTPC) mechanism to optimize energy allocation and enhance system throughput. We formulate the interaction between the system designer and prosumers as a three-stage Stackelberg game where the system designer establishes the incentive framework in Stage I, while prosumers determine energy allocation in Stage II and set transaction fees in Stage III. Our analysis demonstrates that prosumers' failure to internalize mining's positive externality results in suboptimal throughput investment. Counterintuitively, we show that impatient prosumers may exploit others' mining contributions as free riders. The FTPC mechanism resolves these issues by jointly optimizing transaction fees and compensation structures to align individual incentives with social welfare. We prove that FTPC achieves socially optimal outcomes through fully decentralized decision-making. Numerical evaluation shows FTPC improves social welfare and prosumer payoffs by 88.1% and 87.8%, respectively. Ethereum testbed implementation validates equilibrium convergence through iterative best-response dynamics.},
  archive      = {J_TMC},
  author       = {Yunshu Liu and Man Hon Cheung and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3610648},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentivizing throughput enhancement in blockchain-based energy trading system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic searchable symmetric encryption with efficient and complete access control for multi-user cloud computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3609829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable symmetric encryption (SSE) enables the storage and retrieval of encrypted data on untrusted cloud servers, while dynamic searchable symmetric encryption (DSSE) further supports updating encrypted data. To date, in multi-user environments, most DSSE schemes cannot achieve simultaneous access control for both keyword retrieval and data updates. To address this issue, we propose a new DSSE scheme with efficient and complete (keyword retrieval and update) access control for multi-user environments, named EFCAM. Our work has simultaneously achieved efficient, flexible, and fine-grained access control for keyword retrieval and updating, this is extremely rare in existing research. For update operations, we combine file index encoding and homomorphic encryption (HE) technology, so that EFCAM optimizes the calculation; to achieve flexible access control, we adopt an equality test scheme that can supports three types of update authorization. For retrieval operations, users do not need to share keys. By executing a single query, the users can effectively retrieve all the data that they have permission to access. To enhance system security and operational efficiency, we have extended EFCAM with a dynamic policy update mechanism for flexible and real-time adjustment of access control policies. We formally analyze the security of EFCAM to prove that our scheme has forward security (FS) and backward security (BS). Experimental results show that, EFCAM maintains outstanding efficiency in encrypted data retrieval and update operations within multi-user environments, while also exhibiting strong scalability.},
  archive      = {J_TMC},
  author       = {Liqun Yang and Yuze Yang and Dusit Niyato and Zhoujun Li and Wanxu Xia and Liang Sun},
  doi          = {10.1109/TMC.2025.3609829},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic searchable symmetric encryption with efficient and complete access control for multi-user cloud computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Octopus: Optimizing interactive video QoE via loosely coupled codec-transport adaptation. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the quality of experience (QoE) in interactive video streaming (IVS) remains a persistent challenge due to the need for ultra-low latency and rising bandwidth demands. Conventional algorithms, whether rule-based or learning-based, are obsessed with achieving tight coupling between encoding and sending bitrate adaptations for low-latency guarantee. However, our measurement studies reveal alarming harms of tight coupling in suppressing throughput, encoding bitrates and smoothness, as application- and transport-layer bitrate adaptations inherently have different mechanisms and goals. To tackle this problem, we propose Octopus, the first loosely coupled cross-layer bitrate adaptation algorithm for IVS to maximize QoE. Instead of blind synchronization, Octopus promotes mutual cooperation and independence between encoding and sending bitrate adaptations by integrating a multi-head network with shortcut connections and auto-regressive action modules. Additionally, based on meta-imitation reinforcement learning, we design a network condition-aware online adaptation scheme that enables the loosely coupled policy to swiftly adapt to diverse and dynamic wireless networks. We implement Octopus on a testbed, a microcosm of real-world deployment, with transceiver pairs running WebRTC on the WeChat for Business dataset. Results show that Octopus outperforms state-of-the-art algorithms, either improving bitrates by 37.1%, or optimizing stalling rate and smoothness by 54.1% and 9.2%, or achieving all-around improvements.},
  archive      = {J_TMC},
  author       = {Xuedou Xiao and Mingxuan Yan and Yingying Zuo and Boxi Liu and Paul Ruan and Yang Cao and Yue Cao and Wei Wang},
  doi          = {10.1109/TMC.2025.3610501},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Octopus: Optimizing interactive video QoE via loosely coupled codec-transport adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERA: A QoE-aware collaborative inference algorithm for NOMA-based edge intelligence. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although AI has been extensively adopted and has profoundly transformed our lives, it is not feasible to directly deploy large AI models on edge devices with limited resources. To enhance the performance of Edge Intelligence (EI), model split inference has been proposed. In this approach, an AI model is segmented into sub-models, with the most resource-intensive parts offloaded wirelessly to the edge server. This reduces the resource demands and inference latency on the device. However, previous studies have primarily focused on enhancing and optimizing system Quality of Service (QoS), often overlooking Quality of Experience (QoE), which is another crucial aspect for users. Even though QoE has been extensively studied in Edge Computing (EC), the distinct differences between task offloading in EC and split inference in EI, along with specific QoE issues that remain unaddressed in both fields, render these algorithms ineffective for edge split inference scenarios. Therefore, this paper introduces an effective resource allocation algorithm, dubbed ERA, which aims to: 1) expedite split inference in EI, and 2) balance inference delay, QoE, and resource consumption. ERA incorporates resource consumption, QoE, and inference latency to determine the most optimal model split and resource allocation strategies. Given that it is impossible to simultaneously minimize inference delay and resource consumption while maximizing QoE, we employ a gradient descent-based algorithm to find the best possible compromise. Furthermore, to address the complexity arising from parameter discretization in the gradient descent algorithm, we have developed a pipeline gradient descent approach, known as PipGD. We have also examined the properties of the proposed algorithms, including their convergence, complexity, and approximation error. The experimental results clearly show that ERA outperforms previous studies significantly in terms of performance.},
  archive      = {J_TMC},
  author       = {Xin Yuan and Ning Li and Quan Chen and Wenchao Xu and Song Guo},
  doi          = {10.1109/TMC.2025.3610699},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ERA: A QoE-aware collaborative inference algorithm for NOMA-based edge intelligence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient privacy-preserving federated learning via homomorphic encryption-enabled over-the-air computation. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables collaborative model training across devices, but data exchanges pose privacy risks. Homomorphic Encryption (HE) is widely used to enhances privacy in FL but incurs significant communication and computation latency. Prior work reduced this latency using compressions, but sacrificed learning accuracy and overlooked the impact of the number of participating devices on latency. Over-the-air computation (AirComp) leverages wireless channels' superposition property to achieve high spectral efficiency and efficient aggregation irrespective of device number. In this paper, we propose HEAirFed, integrating AirComp with the state-ofthe-art HE scheme CKKS for efficient privacy-preserving FL. In HEAirFed, we develop a ciphertext-oriented wireless communication module to ensure homomorphic operations leverage AirComp's superposition property, enabling correct decryption. We further build a rigorous error analysis model, derive the worst-case upper bound of approximation error, and characterize this bound's impact on the convergence guarantee of HEAirFed, measured by the optimality gap with bounded approximation error. Then, we minimize this gap and derive a near-optimal solution in semi-closed form. Extensive experimental results on real-world datasets validate the ciphertext-oriented design's necessity, the error analysis's correctness, and demonstrate that HEAirFed achieves a substantial reduction in communication and aggregation latency compared to baseline, with minimal learning accuracy loss.},
  archive      = {J_TMC},
  author       = {Yehui Wang and Baoxian Zhang and Jinkai Zhang and Cheng Li},
  doi          = {10.1109/TMC.2025.3610887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient privacy-preserving federated learning via homomorphic encryption-enabled over-the-air computation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Singular value decomposition based indoor localization using small scale crowd sensing data. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3610882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional crowd sensing based indoor localization methods rely on large scale pre-collected fingerprint data to construct a radio map with cumbersome prior preparation. However, when they lack floor plan information or only have a little of data is willing to share, the tracking accuracy degrades significantly. In this paper, we propose a singular value decomposition (SVD) track matching scheme to obtain an effective radio map based on small scale crowd sensing data, which is a non-learning based system (SVD-CSP). SVD-CSP fuses received signal strength indicator (RSSI), inertial measurement unit (IMU), and magnetic field strength to label surrounding WiFi access points as marker points. The proposed scheme uses SVD method to directly compute the rotation matrix and displacement vector among the crowd sensing trajectories and attain the reliable tracks. The radio map is constructed and users are tracked according to our developed bidirectional Bayesian filter, which contains forward filter and reverse filter. The density-based spatial clustering of applications with noise (DBSCAN) is embedded within the forward filter to improve the radio map quality. Meanwhile, the reverse filter fuses pedestrian dead reckoning (PDR) and radio map-based localization to track users. Experimental results demonstrate that SVD-CSP can achieve robust localization using extremely sparse crowd trajectories (e.g., 4 trajectories in a 648 m2 scenario, 30 trajectories in a 2856 m2 scenario) without deep learning training or infrastructure knowledge.},
  archive      = {J_TMC},
  author       = {Xiaohao Liu and Yubin Zhao and Xiaofan Li and Huaming Wu and Cheng-Zhong Xu},
  doi          = {10.1109/TMC.2025.3610882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Singular value decomposition based indoor localization using small scale crowd sensing data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating stable matching between workers and spatial-temporal tasks for dynamic MCS: A stagewise service trading approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3610915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing effective incentive mechanisms in mobile crowdsensing (MCS) networks is crucial for engaging distributed mobile users (workers) to contribute heterogeneous data for various applications (tasks). In this paper, we propose a novel stagewise trading framework to achieve efficient and stable task-worker matching, explicitly accounting for task diversity (e.g., spatio-temporal limitations) and network dynamics inherent in MCS environments. This framework integrates both futures and spot trading stages. In the former, we introduce the futures trading-driven stable matching and pre-path-planning mechanism (FT-SMP3), which enables long-term taskworker assignment and pre-planning of workers' trajectories based on historical statistics and risk-aware analysis. In the latter, we develop the spot trading-driven DQN-based path planning and onsite worker recruitment mechanism (ST-DP2WR), which dynamically improves the practical utilities of tasks and workers by supporting real-time recruitment and path adjustment. We rigorously prove that the proposed mechanisms satisfy key economic and algorithmic properties, including stability, individual rationality, competitive equilibrium, and weak Pareto optimality. Extensive experiements further validate the effectiveness of our framework in realistic network settings, demonstrating superior performance in terms of service quality, computational efficiency, and decision-making overhead.},
  archive      = {J_TMC},
  author       = {Houyi Qi and Minghui Liwang and Xianbin Wang and Liqun Fu and Yiguang Hong and Li Li and Zhipeng Cheng},
  doi          = {10.1109/TMC.2025.3610915},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accelerating stable matching between workers and spatial-temporal tasks for dynamic MCS: A stagewise service trading approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive decentralized federated learning in energy and latency constrained wireless networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3611075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derived convergence bound reveals the impact of the rounds of local training on the model performance. Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained. Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different graph-based aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios. Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed. Simulation results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes.},
  archive      = {J_TMC},
  author       = {Zhigang Yan and Dong Li and Qiang Sun and Dusit Niyato and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3611075},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive decentralized federated learning in energy and latency constrained wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating interference for automotive millimeter-wave radar perception in dense traffic scenarios. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3610963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive Millimeter-wave (mmWave) radar is becoming an essential modality for autonomous vehicles to enable all-weather perception, especially when LiDAR and camera fail in foggy, rainy, or snowy conditions. It is expected that the mutual interference among multiple radars becomes a critical issue in dense traffic scenarios, which can severely degrade the radar performance and lead to accidents. Despite extensive interference mitigation techniques, none can meet the less valid signal distortion while high robustness requirements for automotive radar perception in dense traffic scenarios. To overcome this predicament, we propose mmMic, a novel multiple mutual interference mitigation system that can accurately separate interference and recover valid signals to maintain the reliability of the radar measurements. The key insight is to design an interference estimator that can accurately localize the interference signal according to its linear frequency modulation features in the time-frequency (TF) domain. In addition, mmMic also fully exploits undisturbed valid signal information within an extended time-frequency domain to reconstruct the damaged signal. Our experiments on a real testbed show that mmMic can improve SINR to interference-free levels from multiple radars, achieving an average SINR improvement of 17% compared to the best-performing baseline.},
  archive      = {J_TMC},
  author       = {Wei Wang and Chunshen Li and Bixin Zeng and Lieke Chen and Liang Sun and Kai Luo and Da Chen},
  doi          = {10.1109/TMC.2025.3610963},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating interference for automotive millimeter-wave radar perception in dense traffic scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedBRB: A solution to the small-to-large scenario in device-heterogeneity federated learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3610985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the success of large models has demonstrated the importance of scaling up model sizes. However, it is difficult to directly train large models locally on multiple mobile devices due to their intrinsic computational constraints. To address this challenge, it becomes a crucial need to train larger global models by training small local models on devices. As a distributed learning approach, federated learning (FL) allows multiple devices to train models locally and aggregate them to form the global model by sharing the updated parameters with the server, thus enabling the co-training of models. This promising feature has spurred an increasing interest in exploring the collaborative training of large models. Despite the advent of existing device-heterogeneity FL approaches, they still have limitations in fully covering the parameter space of the global model. To fill this gap, we propose a novel approach called FedBRB (Block- wise Rolling and weighted Broadcast). The core idea of FedBRB is to utilize local models of small devices to train all modules of a large global model and broadcast the trained parameters to the entire space, thereby enabling faster information sharing. This approach not only improves training efficiency but also fully utilizes limited computational resources. Experiments demonstrate that FedBRB can produce significant performance gains, achieving state-of-the-art results. Additionally, this paper provides theoretical and experimental analyses of FedBRB convergence, thereby paving a theoretical ground and providing practical guidance for further research and application of the FedBRB method.},
  archive      = {J_TMC},
  author       = {Tianchi Liao and Ziyue Xu and Qing Hu and Hong-Ning Dai and Huaiwei Huang and Zibin Zheng and Chuan Chen},
  doi          = {10.1109/TMC.2025.3610985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedBRB: A solution to the small-to-large scenario in device-heterogeneity federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid model with bayesian nonparametric inference for RF fingerprint identification. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency fingerprint identification (RFFI) aims to identify subtle impairments in hardware devices, which play an important role in the mobile environment security community. To identify various mobile devices in the complex electromagnetic environment, deep learning methods such as convolutional neural networks (CNN) and recurrent neural networks (RNN) have been adopted to extract device hardware-related features. However, the single network structure has difficulty in comprehensive feature extraction, as many factors can introduce hardware impairments. In this paper, we propose a hybrid model termed switching dynamical deep network (SDDN) for RFFI tasks, which can jointly extract both coarse-grained radio frequency fingerprints (RFFs) and fine-grained RFFs. Additionally, the proposed hybrid model consists of a probabilistic part and a deterministic part. Specifically, in the probabilistic part, the switching linear dynamical systems (SLDS) are incorporated to establish the correspondence between the signal slice and the feature extraction network (FEN). In the deterministic part, multiple independent FENs are established to extract the RFFs. Moreover, to automatically determine the suitable number of FENs, a Bayesian nonparametric prior distribution is placed over the probabilistic part. Finally, an end-to-end parameter optimization method that is based on variational inference and stochastic gradient descent is proposed. Experiments on a real-life Wi-Fi dataset demonstrate the superiority of the proposed method over existing methods.},
  archive      = {J_TMC},
  author       = {Jian Yang and Jiadi Bao and Luyao Zhang and Yatong Wang and Fang Yang and Shafei Wang},
  doi          = {10.1109/TMC.2025.3611135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A hybrid model with bayesian nonparametric inference for RF fingerprint identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple CPUs cooperation for CF massive MIMO with MmWave fronthaul and backhaul. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive multiple-input multiple-output (CF massive MIMO) is regarded as a promising technology for next-generation wireless communication systems. However, relying on a single central processing unit (CPU) in CF massive MIMO systems is not scalable in practical networks, requiring the introduction of multiple CPUs for more efficient and feasible transmission. In this paper, we investigate a CF massive MIMO system with multiple CPUs. To obtain flexible and cost-efficient deployment, we propose to use wireless x-haul links instead of wired ones. More specifically, we assume that both the fronthaul links from the APs to the corresponding CPU and the backhaul links between CPUs operate under millimeter wave (mmWave) networks. Taking into account a tradeoff between the degree of centralized coordination and the signal overhead on the backhaul links, we consider four levels of multiple CPUs cooperation schemes from fully centralized to fully distributed. In addition, we propose a binary search method to allocate the backhaul capacities for maximizing the sum spectral efficiency (SE). Simulation results show that mmWave backhaul amplifies the compression noise introduced by mmWave fronthaul, leading to a more pronounced impact on the SE of systems. In this case, the centralized processing scheme can generate more compression noise due to the larger data overhead on the backhaul link, making the distributed processing scheme a superior processing scheme, especially when dealing with a large number of APs or significant distances between CPUs.},
  archive      = {J_TMC},
  author       = {Feiyang Li and Qiang Sun and Jiayi Zhang and Cunhua Pan and Kai-Kit Wong},
  doi          = {10.1109/TMC.2025.3611133},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multiple CPUs cooperation for CF massive MIMO with MmWave fronthaul and backhaul},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active data routing based on reward backpropagation-enabled multi-agent Q-learning towards SDN-enabled wireless buoy networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3611873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in Wireless Buoy Network (WBN) have significantly accelerated the development of marine exploitation and monitoring, acting as a relay between underwater and surface networks in emerging 6G scenarios. Due to unstable maritime communication environment, it is a challenging issue to deploy the optimal data routing or collection strategies to ensure the collected data to be delivered to the target point. By employing the Software-Defined Networking (SDN) technology, this paper proposes the paradigm of Software-Defined WBN (SDWBN) to improve the network management efficiency and provide a platform to embed the Multi-Agent Reinforcement Learning (MARL) framework (for data routing intelligence), respectively. On account of the proposed SDWBN, this paper proposes a Reward Backpropagation-enabled Multi-Agent Deep Q-learning algorithm (RBMADQ)-based active routing scheme, which aims to assist buoys in making routing decisions and navigating the challenges posed by the dynamic and unstable communication environment. Further, this paper proposes a dual replay buffer-based training method, to enhance the convergence speed of the proposed RBMADQ-based routing scheme. Evaluation results demonstrate that the proposed routing scheme performs better compared with recent research products, with a higher packet delivery rate, lower network latency, and simultaneously, less communication overhead, etc.},
  archive      = {J_TMC},
  author       = {Yuan Liu and Guangjie Han and Chuan Lin and Shengchao Zhu},
  doi          = {10.1109/TMC.2025.3611873},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Active data routing based on reward backpropagation-enabled multi-agent Q-learning towards SDN-enabled wireless buoy networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint UAVs deployment and resource allocation for AoI-aware RIS-assisted UAV-USV MEC network. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3611808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age of information (AoI)-sensitive bidirectional computation tasks quality of service (QoS) for unmanned surface vehicles (USVs) is a critical issue in realizing ship-shore cooperative systems. In this paper, a reconfigurable intelligent surface (RIS)-assisted unmanned aerial vehicle (UAV)-USV mobile edge computing (MEC) network architecture is proposed, where one RIS-carried tethered UAV (TUAV) and rotary-wing UAVs (RUAVs) are cooperatively dispatched to serve USVs bidirectional data computation with average AoI (AAoI) constraint. The minimization of weighted sum USVs AAoI and RUAVs flight energy is formulated by jointly considering RUAVs service duration indicators, TUAV-mounted RIS phase shift, TUAV hovering altitude, and RUAVs' trajectories. A heursitic solution is proposed to address this minimized issue. In particular, a novel mixed linear quadratic Lyapunov framework is utilized to transform the original long-term stochastic problem into a list of deterministic single-slot problems. Then, each single-slot problem is divided into two subproblems. First, the subproblem of RUAVs' trajectories is tackled by an enhanced whale optimization algorithm. Second, the subproblem of RUAVs service duration indicators, TUAV-mounted RIS phase shift and TUAV hovering altitude is addressed by an enhanced alternating optimization algorithm. The results demonstrate that the proposed heuristic solution reduces long-term RUAVs flight energy consumption by approximately 50% while maintaining satisfactory USVs AAoI.},
  archive      = {J_TMC},
  author       = {Yangzhe Liao and Yuanyan Song and Dan Song},
  doi          = {10.1109/TMC.2025.3611808},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint UAVs deployment and resource allocation for AoI-aware RIS-assisted UAV-USV MEC network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-CrowdCache: A decentralized game-theoretic model for edge content sharing over time-varying communication networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3611963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a promising solution for enhancing user experience, minimizing content delivery expenses, and reducing backhaul traffic. This paper presents a game-theoretic framework to address the edge resource crowdsourcing problem, where mobile edge devices (MEDs) provide idle storage for content caching in exchange for rewards from a content provider (CP). We model the interaction between the CP and MEDs as a Stackelberg game, with the CP as the leader setting the reward structure and the MEDs as followers competing in a non-cooperative game for these rewards. We propose a novel privacy-preserving method to derive the Stackelberg equilibrium of the game. Notably, our algorithm is designed to operate effectively in time-varying communication networks, addressing the high mobility inherent in MEC environments. This contrasts with state-of-the-art algorithms, which assume a static communication network among MEDs–an impractical condition that does not account for the mobility of MEDs during algorithm execution. Specifically, our approach employs consensus-based algorithms to compute the Nash equilibrium (NE) for MEDs, with MEDs exchanging NE profile estimates with neighbors via row-stochastic mixing matrices and performing gradient steps to optimize their utility in a fully decentralized manner. Based on the computed NE strategies, we propose a zeroth-order reward search algorithm for the CP to determine the optimal strategy for profit maximization. Our comprehensive analysis details the properties of the equilibrium and establishes the geometric convergence of the proposed algorithms to the NE. We also derive explicit bounds for the stepsizes based on the game's properties and the graphs' connectivity structure. Extensive numerical results validate the efficacy of our proposed approach.},
  archive      = {J_TMC},
  author       = {Duong Thuy Anh Nguyen and Jiaming Cheng and Ni Trieu and Duong Tung Nguyen and Angelia Nedic},
  doi          = {10.1109/TMC.2025.3611963},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Bi-CrowdCache: A decentralized game-theoretic model for edge content sharing over time-varying communication networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient multi-UAV navigation for cooperative data sensing and transmission. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3612221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) hold significant potential for sensing services in a large scope of area, thanks to their wide coverage and adaptable deployment. Considering the complex environment dynamics and limited sensing range, navigating multiple UAVs in a distributed way becomes challenging to implement cooperative data sensing and transmission tasks. In this paper, we optimize the trajectory design of UAVs by jointly considering the collected data volume, geographical fairness and limited energy reserve during their service period. To achieve the long-term serving objective, a memory augmented multi-agent deep reinforcement learning approach is presented to ensure energy-efficient distributed trajectory design with partial observations. Specifically, the intrinsic criterion is developed to enhance UAV spatial exploration when reaching the boundary of explored regions. Then, to address the information loss caused by incomplete observations, the spatial-temporal memory augmented actor-critic architecture is designed to extract historical contextual features for multi-UAV cooperative navigation. Furthermore, the prioritized experience replay mechanism is incorporated to enhance important experience exploitation for UAV collaboration. Extensive simulations using two real-world datasets in Shenzhen and Beijing demonstrate that the proposed method outperforms the state-of-the-art methods in terms of data collection ratio, geographical fairness, and energy consumption ratio.},
  archive      = {J_TMC},
  author       = {Hu He and Jun Peng and Lin Cai and Weirong Liu and Chenglong Wang and Xin Gu and Zhiwu Huang},
  doi          = {10.1109/TMC.2025.3612221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient multi-UAV navigation for cooperative data sensing and transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossSense: Enabling cross-technology sensing between WiFi and LoRa. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3612289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive increase in wireless devices, enabling sensing between incompatible radios has become critically beneficial. Integrating diverse IoT devices enhances sensing accuracy by providing richer data, while utilizing the diverse characteristics of heterogeneous signals meets sensing needs in complex environments. However, most existing wireless sensing methods primarily focus on homogeneous signals, while research on sensing with heterogeneous signals is still in its infancy. In this paper, we propose CrossSense, a novel Cross-Technology Sensing (CTS) framework that enables sensing between incompatible WiFi and LoRa device. CrossSense recovers the fine-grained trajectory of a WiFi transmitter based on its emulated LoRa signals. To decompose the motion feature components of WiFi transmitter, we develop a chirp difference vector model that utilizes the energy peak within each chirp window for sensing. We model the relationship between sampling frequency offsets and oscillation frequency offsets among heterogeneous devices to guide the extraction of motion features from the emulated signal. We also propose a greedy-based peak enhancement method to calculate the optimized LoRa phases, minimizing the impact of phase discontinuity caused by cyclic prefix (CP) errors. We implement a prototype of CrossSense on the USRP platform. The extensive experiments demonstrate that CrossSense can achieve an efficient Cross-Technology Sensing with $2.92cm$ distance accuracy and $0.26cm/s$ speed accuracy over a $120m$ sensing range.},
  archive      = {J_TMC},
  author       = {Dan Xia and Guanghui Chen and Fu Yu and Yuxuan Wang and Xiaolong Zheng and Liang Liu and Shanguo Huang and Huadong Ma},
  doi          = {10.1109/TMC.2025.3612289},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrossSense: Enabling cross-technology sensing between WiFi and LoRa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unity is power: Semi-asynchronous collaborative training of large-scale models with structured pruning in resource-limited clients. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3612427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study to release the potential of massive heterogeneous weak computing power to collaboratively train large-scale models on dispersed datasets. In order to improve both efficiency and accuracy in resource-adaptive collaborative learning, we take the first step to consider the unstructured pruning, varying submodel architectures, knowledge loss, and straggler challenges simultaneously. We propose a novel semiasynchronous collaborative training framework, namely Co-S2P, with data distribution-aware structured pruning and cross-block knowledge transfer mechanism to address the above concerns. Furthermore, we provide theoretical proof that Co-S2P can achieve asymptotic optimal convergence rate of O(1/√ N∗EQ). Finally, we conduct extensive experiments on two types of tasks with a real-world hardware testbed including diverse IoT devices. The experimental results demonstrate that Co-S2P improves accuracy by up to 8.8% and resource utilization by up to 1.2× compared to state-of-the-art methods, while reducing memory consumption by approximately 22% and training time by about 24% on all resource-limited devices.},
  archive      = {J_TMC},
  author       = {Yan Li and Xiao Zhang and Mingyi Li and Guangwei Xu and Feng Chen and Yuan Yuan and Yifei Zou and Mengying Zhao and Jianbo Lu and Dongxiao Yu},
  doi          = {10.1109/TMC.2025.3612427},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unity is power: Semi-asynchronous collaborative training of large-scale models with structured pruning in resource-limited clients},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient detection framework adaptation for edge computing: A plug-and-play neural network toolbox enabling edge deployment. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3612469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, edge computing has emerged as a prevailing paradigm in applying deep learning-based object detection models, offering a promising solution for time-sensitive tasks. However, existing edge object detection faces several challenges: 1) These methods struggle to balance detection precision and model lightweightness. 2) Existing generalized edge-deployment designs offer limited adaptability for object detection. 3) Current works lack real-world evaluation and validation. To address these challenges, we propose the Edge Detection Toolbox (ED-TOOLBOX), which leverages generalizable plug-and-play components to enable edge-site adaptation of object detection models. Specifically, we propose a lightweight Reparameterized Dynamic Convolutional Network (Rep-DConvNet) that employs a weighted multi-shape convolutional branch structure to enhance detection performance. Furthermore, ED-TOOLBOX includes a Sparse Cross-Attention (SC-A) network that adopts a localized-mapping-assisted self-attention mechanism to facilitate a well-crafted Joint Module in adaptively transferring features for further performance improvement. Moreover, we propose an Efficient Head for the classification and location modules to achieve more efficient prediction. Additionally, in practical industrial scenarios, we identify that helmet detection-one of the most representative edge object detection tasks-overlooks band fastening, which introduces potential safety hazards. To address this, we build a Helmet Band Detection Dataset (HBDD) and apply an edge object detection model optimized by the ED-TOOLBOX to tackle this real-world task. Extensive experiments validate the effectiveness of components in ED-TOOLBOX. In visual surveillance simulations, ED-TOOLBOX-assisted edge detection models outperform six state-of-the-art methods, enabling real-time and accurate detection. These results demonstrate that our approach offers a superior solution for edge object detection.},
  archive      = {J_TMC},
  author       = {Jiaqi Wu and Shihao Zhang and Simin Chen and Lixu Wang and Zehua Wang and Wei Chen and Fangyuan He and Zijian Tian and F. Richard Yu and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3612469},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient detection framework adaptation for edge computing: A plug-and-play neural network toolbox enabling edge deployment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of sensing and data offloading in digital twin-assisted internet of vehicles. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3613397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Twin (DT) technology can build consistent virtual replicas of physical infrastructures by collecting sensed environmental information, and further offers a powerful paradigm for real-time monitoring, simulation, and optimization of vehicular networks. However, there exists a natural trade-off between information sensing and DT reconstruction. Specifically, insufficient sensing or unreliable sensing results may increase DT inconsistency, while exhaustive sensing can incur substantial energy overhead, especially for fast-changing, resource-constrained, and dynamic Internet of Vehicles (IoV) systems. To address this challenge, we investigate a multi-layer DT-IoV framework based on cloud-edge-vehicle collaborations. Through the joint design of task sensing, vehicle selection, and data offloading strategies, we maximize the consistency of DT reconstruction under limited resources. Furthermore, we develop a spatio-temporal similarity-based hierarchical clustering reinforcement learning (HC-RL) algorithm in a mesh-free environment to minimize the selection of invalid vehicles. Simulation results demonstrate the effectiveness of our proposed framework and method in preserving DT model fidelity and minimizing energy consumption and latency.},
  archive      = {J_TMC},
  author       = {Mingan Luan and Jinyang Wu and Zheng Chang and Yuan Gao and Shahid Mumtaz},
  doi          = {10.1109/TMC.2025.3613397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of sensing and data offloading in digital twin-assisted internet of vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards generalization fairness in federated learning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3613253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training in mobile computing, where fairness holds paramount importance. Traditional efforts have largely concentrated on ensuring fairness across clients with different data distributions (i.e., domain- wise generalization fairness). However, the aspect of fairness within classes (class- wise generalization fairness) remains largely unexplored. Thus, an important question arises: is it possible to simultaneously achieve domain- wise and class- wise generalization fairness? Moreover, current approaches often improve model performance on weaker distributions at the cost of compromising performance on stronger ones, introducing another dimension of unfairness. So, can we boost performance on weaker distributions without compromising that on stronger ones? To this end, we introduce a global classifier with super logits distillation strategy to achieve comprehensive generalization fairness. The basic idea is to guide local updating by selecting reliable global supervision via a unified global classifier across multiple clients. First, we use these super logits to improve underperforming distributions while preserving the performance of well-performing distributions, promoting domain- wise generalization fairness. Second, we dynamically allocate local distillation loss weights according to the intra-client and inter-client class fairness, accelerating the training of underperforming classes and enhancing class- wise generalization fairness. Comprehensive experiments demonstrate the enhanced fairness and superior performance of our method, highlighting the significance of fairness in mobile computing environments.},
  archive      = {J_TMC},
  author       = {Mang Ye and Yuhang Chen and Wenke Huang and Hui Cai and Laizhong Cui},
  doi          = {10.1109/TMC.2025.3613253},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards generalization fairness in federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer scheduling in gig platforms using a generative diffusion model with duality guidance. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3613450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, gig platforms have emerged as a new paradigm, seamlessly connecting workers and tasks while leveraging workers' collective intelligence, participation, and shared resources. Traditionally, platforms have operated under the assumption of worker homogeneity, where service capabilities and associated service costs are similar. However, in mobile computing scenarios, such as mobile crowdsensing, the diversity in worker capabilities and costs renders the supply and demand matching into a complex problem characterized by multiple layers of workers possessing distinct attributes. The dynamic nature of incoming task requests requires the continual reallocation of these workers, thereby introducing a time-dependent overhead. In this paper, we introduce a framework, called the Generative Diffusion Model with Duality Guidance, termed Guid, to address the intricate multi-layer scheduling problem. We formalize a time-slotted long-term optimization problem that captures the spatiotemporal dynamics of task requests and worker services, as well as the intricate time-coupled overhead. Our framework employs a generative diffusion model to explore the complex solution space of the problem and generate superior solutions. To effectively manage time coupling, we utilize dual optimization theory to generate time slot-aware information, guiding the generative diffusion model towards solutions that assure long-term performance. We provide a rigorous theoretical analysis demonstrating that our guidance solution ensures a parameterized competitive ratio guarantee relative to the theoretically optimal solution. Our comprehensive experiments further illustrate that the proposed method outperforms benchmark techniques, achieving reduced overhead compared to seven baseline methods.},
  archive      = {J_TMC},
  author       = {Xinyu Lu and Zhanbo Feng and Jiong Lou and Chentao Wu and Guangtao Xue and Wei Zhao and Jie Li},
  doi          = {10.1109/TMC.2025.3613450},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-layer scheduling in gig platforms using a generative diffusion model with duality guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Courier working time aware vehicle scheduling for efficient urban logistics. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3613677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Order Transfer from the transfer center to delivery stations is an essential and expensive part of urban logistics. Vehicles are scheduled to send transferred orders to multiple delivery stations sequentially in one transfer trip. A key problem is to generate the vehicle's route for efficient order transfer, i.e., minimizing average order transfer time. In this paper, we explore fine-grained delivery station features, i.e., downstream couriers' remaining working times in last-mile delivery trips and the transferred order distribution to design a Prediction-and-Scheduling framework for efficient Order Transfer called PSOT+, including three main components: i) a Courier's Remaining Working Time Prediction component to predict each courier's working time for conducting heterogeneous tasks with attention-based route generation and multi-task-based working time and workload coprediction; ii) a Single Vehicle Scheduling component to generate a vehicle's route to delivery stations with an order-transfertime-aware heuristic algorithm; and iii) a Capacity-Constrained Vehicle Scheduling component to generate multiple vehicles' cooperative order transfer routes with a capacity-constrained inter-exchange algorithm. The evaluation results with real-world data from one of the largest logistics companies in China show PSOT+ improves the courier's remaining working time prediction effectively and reduces the average order transfer time by up to 59% compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Wenjun Lyu and Haotian Wang and Yiwei Song and Shuai Wang and Yunhuai Liu and Tian He and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3613677},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Courier working time aware vehicle scheduling for efficient urban logistics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end orchestration of NextG media services over the distributed compute continuum. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3613949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NextG (5G and beyond) networks, through the increasing integration of cloud/edge computing technologies, are becoming highly distributed compute platforms ideally suited to host emerging resource-intensive and latency-sensitive applications (e.g., industrial automation, extended reality, distributed AI). The end-to-end orchestration of such demanding applications, which involves function/data placement, flow routing, and joint communication/computation/storage resource allocation, requires new models and algorithms able to capture: (i) their disaggregated microservice-based architecture, (ii) their complex processing graph structures, including multiple-input multiple-output processing stages, and (iii) the opportunities to efficiently share and replicate real-time data streams that may be useful for multiple functions and/or end users. To this end, we first identify the technical gaps in existing literature that prevent efficiently addressing the optimal orchestration of emerging applications described by information-aware directed acyclic graphs (DAGs). We then leverage the recently proposed Cloud Network Flow optimization framework and a novel functionally-equivalent DAG-to-Forest graph transformation procedure to design IDAGO (Information-Aware DAG Orchestration), a polynomial-time multi-criteria approximation algorithm for the optimal orchestration of NextG media services over NextG compute-integrated networks. Results show that IDAGO's multiplicative cost reductions over leading baselines scale linearly with aggregate service load, reaching up to 3X gains in scenarios based on AWS and Unreal Engine data under moderate service loads.},
  archive      = {J_TMC},
  author       = {Alessandro Mauro and Antonia M. Tulino and Jaime Llorca},
  doi          = {10.1109/TMC.2025.3613949},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-to-end orchestration of NextG media services over the distributed compute continuum},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreeBeacon: Efficient communication and data aggregation in battery-free IoT. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3614227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve sustainability, Internet-of-Things (IoT) is increasingly adopting battery-free devices powered by ambient energy scavenged from the environment. The unpredictable availability of ambient energy leads to device intermittency, bringing critical challenges to device communication and related fundamental operations like data aggregation. We propose FreeBeacon, a novel scheme for efficient communication and data aggregation in battery-free IoT. We argue that the communication challenge between battery-free devices originates from the complete uncertainty of the environment. FreeBeacon is built on the insight that by introducing just a small degree of certainty into the system, the communication problem can be largely simplified. To this end, FreeBeacon first introduces a small number of battery-powered devices as beacons for battery-free devices. Then, FreeBeacon features protocols for battery-free devices to achieve interaction with the beacon and to perform communication efficiently following customized schedules that implement different data aggregation schemes while achieving resilience. We evaluate FreeBeacon with extensive prototype-based experiments and simulation studies. Results show that FreeBeacon can consistently achieve an order of magnitude data aggregation efficiency when compared with the state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Gaosheng Liu and Kasım Sinan Yıldırım and Lin Wang},
  doi          = {10.1109/TMC.2025.3614227},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreeBeacon: Efficient communication and data aggregation in battery-free IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-based radio scheduler for RAN slicing: Learning to deal with different network scenarios. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3614453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future mobile network schedulers have the complex mission of distributing radio resources among various applications with different requirements. The radio access network (RAN) slicing enables the creation of different logical networks by using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources among the slices to fulfill their requirements. Several recent studies have proposed advances in machine learning-based RRS. However, these works often evaluate their models under limited scenarios and with minimal slice diversity, raising concerns about their real-world applicability. The generalization capabilities of these models remain uncertain without rigorous testing across diverse network conditions and slice configurations, which may hinder their effectiveness upon deployment in operational networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a RAN slicing context. The proposed method protects high-priority slices when the available radio resources are insufficient, using transfer learning to reduce the number of required training steps. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines.},
  archive      = {J_TMC},
  author       = {Cleverson V. Nahum and Salvatore D'Oro and Pedro Batista and Cristiano B. Both and Kleber V. Cardoso and Aldebaro Klautau and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3614453},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intent-based radio scheduler for RAN slicing: Learning to deal with different network scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced security for NextG mobile networks: A hybrid fuzzing approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3614127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents HyFuzz, a hybrid intelligent fuzz testing platform designed to enhance the security validation of next generation (NextG) mobile networks. HyFuzz integrates symbolic formal analysis with adaptive fuzzing to enable the discovery of vulnerabilities that emerge from subtle state inconsistencies and session level command manipulations. Specifically, HyFuzz demonstrates support for multi step intra session fuzzing, where carefully crafted command sequences cause persistent state desynchronization between User Equipment (UE) and the network. Complementing this, HyFuzz employs formal guided deep fuzzing, directing fuzzing efforts to high risk protocol states identified by symbolic analysis. Through a dual mode architecture supporting both virtual (ZMQ) and over the air (OTA) fuzzing, HyFuzz provides an extensible testbed for low level and behavioral vulnerability discovery. Experimental results across 1,281 test cases reveal 1,105 failure instances, including stealthy failures that manifest only under extended interaction contexts. Our findings suggest HyFuzz provides a foundational capability toward more realistic and semantically rich vulnerability detection in modern mobile infrastructure.},
  archive      = {J_TMC},
  author       = {Jingda Yang and Paul Ratazzi and Ying Wang},
  doi          = {10.1109/TMC.2025.3614127},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Advanced security for NextG mobile networks: A hybrid fuzzing approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmWave radar-based unsupervised gesture recognition via image-aligned heterogeneous domain transfer. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3614353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Gesture Recognition (HGR) using mmWave radar has become increasingly promising due to its exceptional contactless perception sensitivity. Conventional approaches predominantly rely on supervised models to learn radar signals, thus incurring substantial costs associated with annotation. To address this limitation, certain works embrace transfer learning to effectively transfer knowledge from labeled source domain to unlabeled target domain, achieving unsupervised recognition in the target domain. However, existing transfer-based methods still necessitate large-scale labeled source domain radar data, thereby constraining their practical applicability. To this end, we propose a novel unsupervised solution for mmWave-based HGR by transferring public image gestures to radar data, eliminating the need for acquiring labeled radar data in source domain. We aim to establish heterogeneous alignment between images and radar signals, facilitating cross-domain transfer. Initially, we mitigate the negative impact of data heterogeneity by employing sophisticated signal processing techniques to convert raw radar signals into gesture trajectories. Subsequently, we introduce an Adversarial-Contrastive Domain Transfer Model (ACDTM) to achieve fine-grained alignment. ACDTM not only confuses the source and target domains by adversarial learning, enabling the acquisition of domain-invariant features, but also designs a robust similarity matrix to facilitate intra-class alignment through contrastive learning. Additionally, ACDTM conducts adversarial self-training on target domain with pseudo-labeled distribution. Our experimental findings substantiate that the unsupervised accuracy achieves about 80$\sim$92% on different mmWave gesture datasets, outperforming existing unsupervised HGR schemes by large margins. Code is available at https://github.com/onlinehuazai/mmGesture.},
  archive      = {J_TMC},
  author       = {Qihua Feng and Kunpeng Cheng and Chunhui Duan},
  doi          = {10.1109/TMC.2025.3614353},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmWave radar-based unsupervised gesture recognition via image-aligned heterogeneous domain transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two time-scale DRL for service caching and task offloading in cross-domain marine networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3594602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing computational demands and limited network resources in marine environments, efficient service caching and task offloading have become critical. In such environments, Autonomous Underwater Vehicles (AUVs) rely on Unmanned Surface Vehicles (USVs) as relays, forming a cross-domain network comprising underwater acoustic and above-water RF links. However, the heterogeneity in bandwidth, latency, and bit error rates introduces challenges for reachability analysis and delay estimation. This paper addresses the joint optimization of caching, task offloading, and resource allocation in a cross-domain marine network composed of offshore base stations, USVs, and AUVs. To tackle the inherent heterogeneity in network links and decision timescales, we formulate the problem as a two-time-scale Hierarchical Markov Decision Process (H-MDP) and propose a Two Time-Scale Deep Reinforcement Learning (T2S-DRL) approach that integrates a hybrid policy network and a lightweight structure-aware action masking mechanism. The large time-scale agent optimizes caching decisions, while the short time-scale agent focuses on offloading and resource allocation. Extensive simulations show that our approach significantly reduces task execution delay and energy consumption, validating its effectiveness.},
  archive      = {J_TMC},
  author       = {Zhaoxiang Huang and Zhiwen Yu and Liang Wang and Yingnan Zhao and Huan Zhou and and Bin Guo},
  doi          = {10.1109/TMC.2025.3594602},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two time-scale DRL for service caching and task offloading in cross-domain marine networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeurIT: Pushing the limit of neural inertial tracking for indoor robotic IoT. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3594717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost inertial measurement units and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor have they maximized the potential of deep learning to achieve the desired accuracy. To address these limitations, we introduce NeurIT, which elevates tracking accuracy to a new level. NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining both RNN and Transformer to learn representative features in both time and frequency domains. To fully utilize IMU information, we strategically employ body-frame differentiation of magnetometers, considerably reducing the tracking error. We implement NeurIT on a customized robotic platform and conduct evaluation in various indoor environments. Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance. Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data. Moreover, NeurIT demonstrates robustness in large urban complexes and performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions while surpassing it in feature-sparse settings. We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things. NeurIT is open-sourced here: https://github.com/aiot-lab/NeurIT.},
  archive      = {J_TMC},
  author       = {Xinzhe Zheng and Sijie Ji and Yipeng Pan and Kaiwen Zhang and Chenshu Wu},
  doi          = {10.1109/TMC.2025.3594717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NeurIT: Pushing the limit of neural inertial tracking for indoor robotic IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cer-FeaUn: Certified feature unlearning in vertical federated learning. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3594851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature unlearning, forgetting sensitive features while maintaining the accuracy of models, is a pressing issue against Feature Inference Attacks (FIA) in Vertical Federated Learning (VFL). This issue is addressed by retraining the model from scratch on a dataset without the sensitive features from scratch or Federated Unlearning (FU) for all samples. However, they either introduce high overheads due to retraining or reduce the accuracy of the unlearned model. In this paper, we proposed Cer-FeaUn, a certified feature unlearning, trading off between the overheads and accuracy. Specifically, a novel feature perturbation strategy is first proposed to construct a perturbed dataset, where the sensitive features are perturbed with noises. Then, the effect is defined as the parameters difference between models trained with the original and perturbed dataset. Finally, an unlearned model is trained in first-order, where the effect is removed from the original model in one epoch. Furthermore, Cer-FeaUn performs certified removal for server-side models with strongly convex loss functions. That is, the distribution of the unlearned model is statistically indistinguishable from that of the retrained model. For the scenario with a few sensitive features, simulation results show that the accuracy of the unlearned model is up to 84.79%, and the runtime of Cer-FeaUn is 15 times faster than that of the retrained model.},
  archive      = {J_TMC},
  author       = {Yilei Wang and Zhaobo Lu and Zhiquan Liu and Tao Li and Zhenhua Chen and Willy Susilo},
  doi          = {10.1109/TMC.2025.3594851},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cer-FeaUn: Certified feature unlearning in vertical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable heading tracking for pedestrian road crossing prediction using commodity devices. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3594740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian heading tracking enables applications in pedestrian navigation, traffic safety, and accessibility. Previous works, using inertial sensor fusion or machine learning, are limited in that they assume the phone is fixed in specific orientations, hindering their generalizability. We propose a new heading tracking algorithm, the Orientation-Heading Alignment (OHA), which leverages a key insight: people tend to carry smartphones in certain ways due to habits, such as swinging them while walking. For each smartphone attitude during this motion, OHA maps the smartphone orientation to the pedestrian heading and learns such mappings efficiently from coarse headings and smartphone orientations. To anchor our algorithm in a practical scenario, we apply OHA to a challenging task: predicting when pedestrians are about to cross the road to improve road user safety. In particular, using 755 hours of walking data collected since 2020 from 60 individuals, we develop a lightweight model that operates in real-time on commodity devices to predict road crossings. Our evaluation shows that OHA achieves 3.4 times smaller heading errors across nine scenarios than existing methods. Furthermore, OHA enables the early and accurate detection of pedestrian crossing behavior, issuing crossing alerts 0.35 seconds, on average, before pedestrians enter the road range.},
  archive      = {J_TMC},
  author       = {Yucheng Yang and Jingjie Li and Kassem Fawaz},
  doi          = {10.1109/TMC.2025.3594740},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliable heading tracking for pedestrian road crossing prediction using commodity devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALCS: An adaptive latency compensation scheduler for multipath TCP in satellite-terrestrial integrated networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end transmission by simultaneously utilizing terrestrial and satellite networks, offering significant benefits in scenarios like emergency response and cross-continental communication. Low Earth Orbit (LEO) satellite networks offer reduced Round Trip Time (RTT) for long-distance data transmission and serve as a crucial backup during terrestrial network failures. Meanwhile, terrestrial networks are characterized by ample bandwidth resources and generally more stable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN is vital for optimizing resource utilization and ensuring efficient data transfer by exploiting the complementary strengths of both networks. However, the inherent challenges of STIN, such as heterogeneity, instability, and handovers, pose difficulties for traditional multipath schedulers, which are typically designed for terrestrial networks. We propose a novel multipath data scheduling approach for STIN, the Adaptive Latency Compensation Scheduler (ALCS), to address these issues. ALCS refines transmission latency estimates by incorporating RTT, congestion window size, inflight and queuing packets, and satellite trajectory information. It further employs adaptive mechanisms for latency compensation and proactive handover management. Implemented in the MPTCP Linux Kernel and evaluated in a simulated STIN testbed, ALCS outperforms existing multipath schedulers, delivering faster data transmission and achieving throughput gains of 9.8% to 44.0% compared to benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Lin Wang and Ze Wang and Zeyi Deng and Jingjing Zhang and Yue Gao},
  doi          = {10.1109/TMC.2025.3594896},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ALCS: An adaptive latency compensation scheduler for multipath TCP in satellite-terrestrial integrated networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced dual-currency VCG auction mechanism for resource allocation in IoV: A value of information perspective. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6 G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the “value of information” as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6 G networks.},
  archive      = {J_TMC},
  author       = {Wei Wang and Nan Cheng and Conghao Zhou and Haixia Peng and Haibo Zhou and Zhou Su and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3594358},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An enhanced dual-currency VCG auction mechanism for resource allocation in IoV: A value of information perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VitalEar: An earable heartbeat and respiratory rate monitoring system under aerobic exercises. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart rate (HR) and respiratory rate (RR) are essential physiological indicators of people's physical function and exercise performance. Advancement in sensor technology has rendered earable devices with in-ear microphones feasible for vital sign monitoring. However, it is rather challenging to monitor heart rate and respiration simultaneously with a single earable device especially when a person is doing exercises. This is because intense physical activities can lead to significant noise interference which can easily obscure physiological signals. To address this challenge, this paper presents VitalEar, an exercise physiological monitoring system based on in-ear microphones, designed to estimate HR and RR while addressing complex motion interference and variability in users and activities. VitalEar employs Empirical Wavelet Transform (EWT) to decompose heartbeats into periodic and harmonic coefficients, enhancing noise reduction in the ECG spectrogram reconstruction model. Additionally, VitalEar incorporates a DCN-LSTM-based breathing curve reconstruction model to mitigate background noise and variability in user and activity. The experiments show that VitalEar achieves an average MAE of 5.61 BPM and 2.31 RPM, MAPE of 4.16% and 10.58% for HR and RR estimation, respectively. Compared to related work, our approach offers significant advantages in robustness against intense physical activities},
  archive      = {J_TMC},
  author       = {Yuzheng Zhu and Zhangxin Liang and Jie Zheng and Yongpan Zou and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3595178},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VitalEar: An earable heartbeat and respiratory rate monitoring system under aerobic exercises},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential privacy space decomposition algorithm based on hierarchical model. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3595426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing an appropriate division method is crucial for partitioning two-dimensional spatial data under the constraints of differential privacy. The current mainstream partitioning methods include grid-based partitioning and hierarchical partitioning. In order to optimize query accuracy while satisfying differential privacy conditions, it remains challenge to achieve the sum minimization of noise error and uniformity assumption error. To address this issue, we propose the HOLG (Hierarchical Optimization of Logical Grids) algorithm, employing a ”divide-merge-divide” approach. It begins with fine-grained grid partitioning of the data domain, followed by heuristic merging of grids with similar data distributions. After determining the scale of the query domain, the merged regions are further subdivided into smaller regions with similar query probabilities, constructing a hierarchical structure to reduce uniformity assumption errors. Additionally, we design a novel noise injection method and introduce consistency constraints to further minimize noise errors. To reduce the time complexity of the HOLG partitioning method, Huffman trees is employed to optimize the processing of the hierarchical tree set generated by HOLG, ensuring query utility while effectively reducing the query response time for the partitioning algorithm. Experimental results on large-scale spatial datasets demonstrate that HOLG outperforms similar algorithms in query accuracy. Furthermore, when combined with the Huffman tree optimization, it effectively reduces query response time.},
  archive      = {J_TMC},
  author       = {Haiping Huang and Chaorun Sun and Zhenqi Shi and Wei Zhang and Jiyun Cang and Fu Xiao},
  doi          = {10.1109/TMC.2025.3595426},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Differential privacy space decomposition algorithm based on hierarchical model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDT-SaFL: Semi-asynchronous federated learning for edge digital twin in industrial internet-of-things. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through conducting equivalent model training within the paradigm of edge intelligence, the Digital Twin Edge Networks (DITEN) have been widely employed in the Industrial Internet-of-Things (IIoT) to facilitate the cost-effective execution without the operational disruption. However, due to the insufficient consideration of heterogeneity in computing and communication capabilities of distinct industrial terminals in the Digital Twin (DT) model training, the existing approaches of DT construction/update have unbalanced model training cost and loss in the whole life cycle of DT model, hindering the abilities of quick responding to complex and dynamic productions and ensuring the data consistency of virtual-real space. To address this issue, we define a global loss minimization problem with constraint, and propose an original approach of semi-asynchronous federated learning, named EDT-SaFL, as a promising solution. Considering the collaborative utilization of heterogeneous resources, and the contribution of local data quantity and quality to the global model update, the EDT-SaFL consists of three important operations, Terminal Selection for Model Training, Self-Adaptation of Local Training Iterations, and Semi-asynchronous Global Aggregation. With the analysis of convergence, complexity and communication overhead, the experiments have evidently demonstrated the superiority of EDT-SaFL on the datasets of CIFAR-10 and Industrial-Equipment.},
  archive      = {J_TMC},
  author       = {Ming Tao and Lingling Liao and Yin Zhang and Lei Liu and Geyong Min and Dusit Niyato and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3595117},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EDT-SaFL: Semi-asynchronous federated learning for edge digital twin in industrial internet-of-things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control plane initial synchronization optimization for ultra-dense cell-free massive MIMO. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3595656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-dense cell-free Massive MIMO (CF-MM) is a promising technology for future 6 G networks. Unlike previous research, we focus on the challenges of CF-MM in the control plane. Specifically, we investigate the initial synchronization processing for CF-MM, which is fundamental for user equipment (UE) to access the wireless network. Regardless of whether LTE- or New Radio (NR)-based synchronization signals are used, the existing initial access procedures are incompatible with CF-MM, leading to severe interference on the broadcast channel and thus a high initial access failure rate. To address this issue, we design a new synchronization scheme for CF-MM based on the latest NR standard. With this new scheme, the initial synchronization interference problem is then transformed into a transmission and reception points (TRP) ID partition problem. To solve problem, we then propose an approach that combines the latest multiple operator heuristic (MOH) method with a parameterized local search(PLS) method. Simulation results show that the proposed method can achieve the optimal solution for scenarios with a low number of TRPs. Furthermore, the gap between the proposed method and the optimal solution is less than 2% for high number TRPs cases, demonstrating its effectiveness.},
  archive      = {J_TMC},
  author       = {Miaona Huang and Jun Chen},
  doi          = {10.1109/TMC.2025.3595656},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Control plane initial synchronization optimization for ultra-dense cell-free massive MIMO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ground-assisted LEO satellite federated learning: Dynamic, efficient, distributed learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3593252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of Low Earth Orbit (LEO) satellites, they generate a vast amount of data. This data has been instrumental in supporting machine learning (ML) in various terrestrial services to address global challenges such as monitoring climate change and natural disasters. However, many national regulations restrict the direct transmission of satellite data to ground stations (GSs). Therefore, ground-assisted satellite federated learning (FL) has emerged as a paradigm to safeguard data privacy by transferring model parameters instead of raw data for collaborative training. At present, the existing groundassisted satellite FL methods encounter practical challenges: 1) The dynamic environment of LEO satellites results in continuous changes in the types of data collected by satellites, making it difficult for traditional FL models to adapt to these changes. This can lead to a deterioration in model accuracy over extended periods of model training. 2) Communication between satellites and GS is affected by atmospheric interference and weather factors, resulting in increased transmission delays and affecting the realtime efficiency of the FL system. In response to these challenges, we propose a dynamic, efficient, and distributed ground-assisted LEO satellite federated learning (DEDFL) framework to improve model accuracy and reduce satellite communication delays. In DEDFL, we design a Balanced Class Memory Extraction and an information playback strategy that enables the onboard FL model to adapt to changing satellite data types, thus achieving a performance balance across different classes. Additionally, we propose an adaptive fine coding method for parameter adoption prior to satellite transmission, effectively reducing the delay caused by satellites and ground-specific environmental variations. Experimental results demonstrate that the DEDFL method offers better accuracy and communication efficiency than other baseline algorithms.},
  archive      = {J_TMC},
  author       = {Fuyao Zhang and Dan Wang and Jiawen Kang and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3593252},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ground-assisted LEO satellite federated learning: Dynamic, efficient, distributed learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAP-UAV-assisted maritime IoT communication network. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3596169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wireless networks has spurred an increasing demand for high-quality maritime communication services. This study presents an innovative unicast-multicast access and backhaul maritime communication network (UMABMCN), in which a high-altitude platform (HAP) provides HAP-to-vessel (H2V) unicast services to vessels and backhaul support to unmanned aerial vehicles (UAVs) through HAP-to-UAV (H2U) links. Additionally, multiple UAVs are deployed to deliver UAV-to-vessel (U2V) multicast transmission services to vessels. Specifically, we formulate a HAP-UAV-assisted unicast-multicast cooperation multi-objective optimization problem (UMCMOP) aimed at maximizing the sum achievable rate of base stations (BS)-to-vessel (B2V), maximizing the sum backhaul rate of H2U, and minimizing the energy consumption of UAVs via jointly optimizing communication connection between BSs and vessels, power allocations of UAVs, along with the placement of UAVs. The formulated UMCMOP is a mixed integer non-linear programming (MINLP) problem. To address this, we propose an enhanced multi-objective multi-verse optimization (EMOMVO-CGD) algorithm, which integrates a chaos probability operator, gray wolf exploitation operator, and discrete update operator. To further validate the performance of EMOMVO-CGD, a joint communication connection, power allocation and placement optimization (JCCPAPO) method is proposed. Simulation results demonstrate that the two proposed algorithms outperform benchmark strategies in optimizing the aforementioned objectives.},
  archive      = {J_TMC},
  author       = {Lingling Liu and Chong Shen and Feng Shu and Feng Wang and Shujing Li and Tony Q.S. Quek},
  doi          = {10.1109/TMC.2025.3596169},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HAP-UAV-assisted maritime IoT communication network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability evaluation for WSNs based on deep reinforcement learning and graph neural networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3595199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Network (WSN) reliability evaluation is essential for ensuring the stable operation of network. Traditional methods usually focus on the network topology structure, and calculate the normal operation probability of WSNs. However, these methods usually ignore the energy consumption and network lifetime. In this paper, a novel reliability evaluation algorithm TLR is proposed, which calculates the network lifetime under dynamic network environment according to the pre-set network topology structure reliability threshold, and realizes the comprehensive reliability analysis of network topology and lifetime. In addition, as the basis for reliability evaluation, this paper proposes a new deep reinforcement learning network framework GNN-AC combining graph neural network and actor-critic network, which solves the challenge of constructing Virtual Backbone Network (VBN) in dynamically operating networks. Based on the self-defined fitness matrix and fitness value, the objective function is set to optimize the VBN construction scheme to accurately calculate the network lifetime, and the relationship between the reliability of network topology and network lifetime is discussed. Simulations are carried out for various sizes of WSNs to show the advantages and effectiveness of the proposed approach in estimating network lifetime and reliability evaluation.},
  archive      = {J_TMC},
  author       = {Ziheng Xiao and Shenghao Liu and Hongwei Lu and Lingzhi Yi and Hanjun Gao and Xianjun Deng and Heng Wang and Jong Hyuk Park},
  doi          = {10.1109/TMC.2025.3595199},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliability evaluation for WSNs based on deep reinforcement learning and graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollaboRadio: A hybrid device-edge-cloud collaboration paradigm for fine-grained radio map construction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3596312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio map presents communication parameters of interest, e.g., received signal strength, across a geographical region in a specific frequency band. It can be leveraged to improve the efficiency of spectrum utilization. With the rapid development of radio technology and the proliferation of radioenabled devices, there is an increasing demand for finer granularity (higher resolution) in radio maps. However, the problem of fine-grained radio map construction is uniquely challenging, as it requires to utilize an extremely small number of radio samples collected by sparsely distributed sensor devices to infer the map. To address the challenge, we propose a hybrid device-edge-cloud collaboration paradigm called CollaboRadio. CollaboRadio first groups sensor devices into multiple clusters, with cluster locations optimized based on principles of radio propagation. Next, it leverages a small AI model on each edge server to generate a local radio map for the respective cluster region from radio samples collected by intra-cluster sensors. Finally, it employs a large AI model in the cloud to construct a global radio map for the entire region from the local maps produced by edge servers of different clusters. For the implementation of CollaboRadio, we develop an UNet-based small model for the edge server and a Transformerbased large model for the cloud. Extensive simulations show that CollaboRadio is capable of constructing fine-grained radio maps from an ultra-low sampling rate of 0.1%, and significantly outperforms state-of-the-art.},
  archive      = {J_TMC},
  author       = {Shuai Shao and Lu Cheng and Ke Chen and Qingyu Liu and Shuhang Zhang and Hongliang Zhang and Lingyang Song},
  doi          = {10.1109/TMC.2025.3596312},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CollaboRadio: A hybrid device-edge-cloud collaboration paradigm for fine-grained radio map construction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service satisfaction-aware adaptive service migration and resource allocation in vehicular edge computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3596342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of vehicle-to-everything (V2X) technology, service migration has become an important approach to provide low-latency computing services and ensure service continuity for high-speed moving vehicles in vehicular edge computing (VEC), which enables VEC to efficiently support advanced transportation services. However, optimizing service satisfaction for service migration in multi-vehicle heterogeneous VEC networks is challenging, since the complex, multifactorial, and nonlinear dependencies between service satisfaction and quality of service (QoS) metrics is intractable, and the rapidly changing computational loads in edge server results in inefficient utilization of edge resources. In this paper, we propose a service Satisfaction-based Adaptive service Migration and resource Allocation joint Optimization scheme (SAMAO) to improve service migration efficiency and edge resource utilization in VEC. Firstly, we develop an adaptive computation resource allocation algorithm that can adjust resource allocation strategy according to load status of edge servers to improve vehicle service satisfaction. Then, to minimize energy consumption and ensure service satisfaction for vehicles, we propose a utility maximization algorithm to formulate migration decisions based on pre-allocated computation resources on servers. Finally, numerous simulations based on Shanghai Telecom real-world dataset show that SAMAO can achieve significant advantages in terms of average service satisfaction and computation cost.},
  archive      = {J_TMC},
  author       = {Yufei Liu and Yuanguo Bi and Yuheng Liu and Dusit Niyato and Kaiqi Yang and Liang Zhao and Ammar Hawbani},
  doi          = {10.1109/TMC.2025.3596342},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service satisfaction-aware adaptive service migration and resource allocation in vehicular edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient and real-time sensing for federated continual learning via sample-driven control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3597713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent Real-Time Sensing (RTS) system must continuously acquire, update, integrate, and apply knowledge to adapt to real-world dynamics. Managing distributed intelligence in this context requires Federated Continual Learning (FCL). However, effectively capturing the diverse characteristics of RTS data in FCL systems poses significant challenges, including severely impacting computational and communication resources, escalating energy costs, and ultimately degrading overall system performance. To overcome these challenges, we investigate how the data distribution shift from ideal to practical RTS scenarios affects Artificial Intelligence (AI) model performance by leveraging the generalization gap concept. In this way, we can analyze how sampling time in RTS correlates with the decline in AI performance, computation cost, and communication efficiency. Based on this observation, we develop a novel Sample-driven Control for Federated Continual Learning (SCFL) technique, specifically designed for mobile edge networks with RTS capabilities. In particular, SCFL is an optimization problem that harnesses the sampling process to concurrently minimize the generalization gap and improve overall accuracy while upholding the energy efficiency of the FCL framework. To solve the highly complex and time-varying optimization problem, we introduce a new soft actor-critic algorithm with explicit and implicit constraints (A2CEI). Our empirical experiments reveal that we can achieve higher efficiency compared to other DRL baselines. Notably, SCFL can significantly reduce energy consumption up to 85% while maintaining FL convergence and timely data transmission.},
  archive      = {J_TMC},
  author       = {Minh Ngoc Luu and Minh-Duong Nguyen and Ebrahim Bedeer and Van Duc Nguyen and Dinh Thai Hoang and Diep N. Nguyen and Quoc-Viet Pham},
  doi          = {10.1109/TMC.2025.3597713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient and real-time sensing for federated continual learning via sample-driven control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPACE: Speaker adaptation for acoustic eavesdropping using mmWave radio signals. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3598703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of voice-related interaction and communication has raised concerns about privacy leakage and security. For example, millimeter-wave (mmWave) radio signals have been exploited as a potential attacker for acoustic eavesdropping. However, speaker variability and low-quality input pose significant challenges for the practical deployment of mmWave-based eavesdropping. In this paper, we propose SPACE, an acoustic eavesdropping system to recover intelligible speech from low-quality mmWave signals, which can adapt to numerous different speakers and unseen ones. SPACE is a two-stage system that first reconstructs the spectrogram using a novel Radio TransUNet and then synthesizes the waveform through a neural vocoder. Specifically, to alleviate the negative effect of speaker variability, we introduce a speaker encoder to capture speaker features and a fusion network to condition the spectrogram reconstruction based on the extracted speaker characteristics. Further, to facilitate intelligible speech recovery from low-quality input, we design a Frequency Transformation Layer to exploit the correlation among all frequency harmonics and incorporate the neural vocoder to synthesize the speech waveform from the reconstructed spectrogram without using the contaminated phase. The experimental results show that SPACE outperforms existing mmWave-based approaches in scenarios with numerous different speakers and unseen speakers.},
  archive      = {J_TMC},
  author       = {Running Zhao and Luca Jiang-Tao Yu and Tingle Li and Zhihan Jiang and Chenwei Zhang and Chenshu Wu and Hang Zhao and Edith C.H. Ngai},
  doi          = {10.1109/TMC.2025.3598703},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SPACE: Speaker adaptation for acoustic eavesdropping using mmWave radio signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-resolution massive MIMO channel estimation with LSTM attention-based CBDNet. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel estimation of a massive multi-input multi-output (MIMO) system that utilizes a one-bit analog-to-digital converter (ADC) is a foremost challenge. Traditional deep learning (DL) approaches have been recently employed to circumvent this problem; however, they are limited to noise levels. Unlike the existing works, we use a DL-based denoise architecture for channel estimation from one-bit received signals, improving the estimation performance as the signal-to-noise ratio (SNR) increases. The model leverages a dual-branch architecture to estimate and remove noise from input data. We propose a DL model: a long short-term memory (LSTM) attention-based convolutional blind denoising network (LA-CBDNet) comprising the noise estimation subnetwork and the non-blind denoising subnetwork. The noise estimation subnetwork comprises convolutional layers estimating the noise map, followed by an LSTM to converge the noise estimation. The non-blind subnetwork comprises accompanying attention and LSTM layers estimating the noise matrix. The numerical results demonstrate that our model performs better than benchmark approaches for varying SNRs and base station (BS) antennas. In addition, it outperforms the comparative methods for different pilot lengths and number of users.},
  archive      = {J_TMC},
  author       = {Islam Helmy and Wooyeol Choi},
  doi          = {10.1109/TMC.2025.3599399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Low-resolution massive MIMO channel estimation with LSTM attention-based CBDNet},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-tier submodel partition framework for enhancing UAV swarm robustness in forest fire detection. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of Unmanned Aerial Vehicle (UAV) swarm for Forest Fire Detection (FFD) missions presents unique challenges, e.g., the early forest fires are difficult to identify due to environment diversity and feature complexity, especially when some UAVs could be destroyed in harsh environments. To address these challenges, UAV swarm-based FFD missions can leverage advanced deep learning techniques, where online model updates, robustness, and communication overhead control become crucial for ensuring the effectiveness and adaptability of these missions. In this paper, we propose a Two-tier Submodel Partition Framework (TSPF) to enhance the robustness of UAV swarm conducting FFD missions. TSPF utilizes online model updates to adapt to diverse mission environments, thus strengthening the generalization capability of the model. In addition, a graph coloring method, an intragroup backup mechanism, and a Dynamic Server Selection (DSS) mechanism for the grouping are employed to enhance the robustness of FFD missions when some UAVs are destroyed, hence maintaining the high performance of FFD missions in harsh environments. Moreover, TSPF enables submodel updates by aggregating the parameters of selected layers within/between UAV groups, thereby effectively reducing the model parameter uploads (communication overhead) in model training. Experimental evaluations demonstrate that our proposed TSPF significantly improves the detection accuracy of forest fires, enhances the robustness of FFD missions against the destruction of some UAVs, and reduces the communication overhead in FFD missions.},
  archive      = {J_TMC},
  author       = {Xingyu Li and Wenzhe Zhang and Linfeng Liu and Ping Wang},
  doi          = {10.1109/TMC.2025.3599384},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two-tier submodel partition framework for enhancing UAV swarm robustness in forest fire detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From preparation to execution: Security protocol for third-party MES-enabled 5 g support handover authentication and key evolution. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) platforms offered by third-party providers increasingly complement fifth-generation (5 G) mobile networks. This business model separates the Mobile Network Operator (MNO)-managed gNodeB (gNB), from the Mobile Edge Host (MEH) belonging to a different management domain. Such separation exposes existing schemes that ignore key evolutions in the MEC domain to User Equipment (UE) privacy leakage. This paper proposes a secure two-phase handover scheme. The preparation phase builds a dedicated MEH-to-MEH channel to conveys MNO-independent context and pre-authentication data that enables UE to verify the target pair before the handover decision. The execution phase performs dual-track authentication and key evolution between the target gNB-MEH pair belonging to different registration domains and UE, establishing two isolated sessions to protect UE privacy in both domains. The proposed scheme seamlessly integrates with existing standards while overcoming the security threats inherent in the access layer, such as fake base station attacks. We show that our scheme satisfies key forward/backward secrecy, anonymity, and unlinkability using various formal and informal analysis methods. The prototype implemented on NS-3 5 G mmWave has only an 8.344 ms latency increase over the Conventional-5 G protocol stack, and this latency increment is minimally reduced by 33.98% compared to the state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Ye Bi and Chunfu Jia},
  doi          = {10.1109/TMC.2025.3599376},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From preparation to execution: Security protocol for third-party MES-enabled 5 g support handover authentication and key evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-heterogeneous federated learning with bidirectional knowledge distillation. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) faces significant challenges in addressing the system heterogeneity of edge devices, which often exhibit diverse computational and resource constraints. To overcome these limitations, we propose FedBid, a novel model-heterogeneous FL framework based on bidirectional, data-free knowledge distillation. By synthesizing data on the server and integrating forward-backward knowledge distillation with group aggregation, FedBid allows devices to adopt different models tailored to their resources and enabling efficient knowledge sharing across diverse model architectures without relying on public datasets. In addition, we further introduce progressive learning and generator perturbation strategies to enhance the efficiency of bidirectional knowledge distillation. Extensive experiments on benchmark datasets (Fashion-MNIST, CIFAR-10) and a real-world dataset (FLAIR) under varied data distributions demonstrate that FedBid achieves superior model performance than the state-of-the-art methods while reducing computational overhead on resource-constrained clients.},
  archive      = {J_TMC},
  author       = {Hao Zhang and Yaolin Zhu and Tingting Wu and Siyao Cheng and Jie Liu},
  doi          = {10.1109/TMC.2025.3599315},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Model-heterogeneous federated learning with bidirectional knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal pyramid-based multi-scale data completion in sparse crowdsensing. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Crowdsensing has emerged as a crucial and flexible method for collecting spatio-temporal data in various applications, such as traffic management, environmental monitoring, and disaster response. By recruiting users and utilizing their diverse mobile devices, this approach often results in data that is both sparse and multi-scale, complicating the data completion process. Although numerous data completion algorithms have been developed to address data sparsity, most assume that the collected data is of the same or similar scale, rendering them ineffective for multi-scale data. To overcome this limitation, in this paper, we propose a spatio-temporal pyramid-based multi-scale data completion framework in Sparse Crowdsensing. The basic idea is to leverage a pyramid structure to efficiently capture the complex interrelations between different scales. We first develop a Spatial-Temporal Pyramid Construction Module (ST-PC) to handle multi-scale inputs, and then propose a Spatial-Temporal Pyramid Attention Mechanism (ST-PAM) to capture multi-scale correlations while reducing computational complexity. Furthermore, our method incorporates cross-scale constraints to optimize completion performance. Extensive experiments on four real-world spatio-temporal datasets demonstrate the effectiveness of our framework in multi-scale data completion.},
  archive      = {J_TMC},
  author       = {Wenbin Liu and Hao Du and En Wang and Jiajian Lv and Weiting Liu and Bo Yang and Jie Wu},
  doi          = {10.1109/TMC.2025.3599322},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatio-temporal pyramid-based multi-scale data completion in sparse crowdsensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WaveRoRA: Wavelet rotary route attention for multivariate time series forecasting. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various sensors of Internet of Things (IoT) generate massive amounts of mobile traffic data, forming multivariate time series (MTS). Accurate forecasting of MTS facilitates the enhancement of proactive autoscaling and resource allocation in edge networks. While recent Transformer-based models (Transformers) have achieved significant success in MTS forecasting (MTSF), they tend to rely solely on either time-domain or frequency-domain features, which captures inadequate trends and periodic characteristics. To this end, we propose a wavelet learning framework that seamlessly integrates wavelet transforms with Transformers to benefit from time and frequency characteristics. We design a mixing-splitting architecture to model multi-scale wavelet coefficients and utilizes the attention mechanism to capture inter-series dependencies in the wavelet domain. However, the vanilla softmax self-attention (SA) is high-computational-cost and its smoothing effect diminishes the contrast between strong and weak variable correlations. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). RoRA incorporates rotary positional embeddings to enhance feature diversity and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix. Such design strengthens interactions among strongly correlated variables while mitigating the impact of weakly correlated noise. We further propose WaveRoRA, a unified model that leverages RoRA capturing inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs. Our code is available at https://github.com/Leopold2333/WaveRoRA.},
  archive      = {J_TMC},
  author       = {Aobo Liang and Yan Sun and Nadra Guizani},
  doi          = {10.1109/TMC.2025.3599406},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WaveRoRA: Wavelet rotary route attention for multivariate time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-resistant cross-domain authentication scheme based on certificate conversion in V2G. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-grid (V2G) is an important technology that utilizes mobile computing technology and mobile networks to enable two-way energy and information exchange between electric vehicles and the grid. Due to the openness and instability of mobile networks, V2G system faces many security challenges. Therefore, it is necessary to authenticate cross-domain vehicles to ensure the security and stability of grid services. However, existing schemes face various challenges in terms of security and efficiency. Notably, most of the existing V2G certificate-based cross-domain authentication schemes are based on traditional classical cryptography, which are unable to resist quantum attacks. Additionally, in these schemes, the grid server has to generate certificates not only for vehicles in its own domain but also for vehicles from other domains that travel to its domain. This not only increases the computational burden on the grid server but also reduces the overall authentication efficiency. To address these challenges, this paper proposes a lattice-based cross-domain authentication scheme for V2G networks, which integrates a novel certificate conversion mechanism. The key innovation lies in enabling vehicles to achieve seamless crossdomain transitions through converted certificates, allowing for multiple re-authentications without repeated certificate issuance. This approach significantly reduces computational overhead for both electric vehicles and grid servers while maintaining robust security guarantees. Specifically, we introduce the proxy resignature technology to generate the converted certificates, which combines the Chinese Remainder Theorem and secure multiparty computation to generate proxy rekeys. Crucially, to make our scheme quantum-resistant, we employ lattice-based cryptography. Finally, compared to similar competing schemes, our scheme improves computation efficiency and communication efficiency, and reduces storage cost while maintaining security.},
  archive      = {J_TMC},
  author       = {Jing-yi Yang and Run-hua Shi and Shao-fu Zhang},
  doi          = {10.1109/TMC.2025.3599433},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum-resistant cross-domain authentication scheme based on certificate conversion in V2G},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRDST: Low-latency DAG consensus through robust dynamic sharding and tree-broadcasting for IoV. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) is emerging as a pivotal technology for enhancing traffic management and safety. Its rapid development demands solutions for enhanced communication efficiency and reduced latency. However, traditional centralized networks struggle to meet these demands, prompting the exploration of decentralized solutions such as blockchain. Addressing blockchain's scalability challenges posed by the growing number of nodes and transactions calls for innovative solutions, among which sharding stands out as a pivotal approach to significantly enhance blockchain throughput. However, existing schemes still face challenges related to a) the impact of vehicle mobility on blockchain consensus, especially for cross-shard transaction; and b) the strict requirements of low latency consensus in a highly dynamic network. In this paper, we propose a DAG (Directed Acyclic Graph) consensus leveraging Robust Dynamic Sharding and Tree-broadcasting (DRDST) to address these challenges. Specifically, we first develop a standard for evaluating the network stability of nodes, combined with the nodes' trust values, to propose a novel robust sharding model that is solved through the design of the Genetic Sharding Algorithm (GSA). Then, we optimize the broadcast latency of the whole sharded network by improving the tree-broadcasting to minimize the maximum broadcast latency within each shard. On this basis, we also design a DAG consensus scheme based on an improved hashgraph protocol, which can efficiently handle crossshard transactions. Finally, the simulation proves the proposed scheme is superior to the comparison schemes in latency, throughput, consensus success rate, and node traffic load.},
  archive      = {J_TMC},
  author       = {Runhua Chen and Haoxiang Luo and Gang Sun and Hongfang Yu and Dusit Niyato and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3599385},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRDST: Low-latency DAG consensus through robust dynamic sharding and tree-broadcasting for IoV},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework of arithmetic-level variable precision computing for in-memory architecture: Case study in MIMO signal processing. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational complexity poses a significant challenge in wireless communication. Most existing attempts aim to reduce it through algorithm-specific approaches. However, the precision of computing, which directly relates to both computing performance and computational complexity, is a dimension that is fundamental but rarely explored in the literature. With the emerging architecture of in-memory computing, variable precision computing (VPC) is enabled, allowing each arithmetic operation to be processed with a distinct and specifically optimized computing precision. In this paper, we establish a unified framework of arithmetic-level variable precision computing (ALVPC), which aims to determine the optimized computing precision for each arithmetic operation. We first develop an arithmetic propagation error model exploiting stochastic analysis, and then formulate a mathematical optimization problem to strike balance between computing performance and computational complexity. Two algorithms, namely, offline VPC and online VPC, are proposed to solve the problem considering various practical concerns. Particularly, in a case study on zero-forcing (ZF) precoding, we reveal the Pareto boundary between computing performance and complexity, which exhibits up to a 60% sumrate enhancement or equivalently up to a 30% complexity reduction compared to the traditional fixed-length methods.},
  archive      = {J_TMC},
  author       = {Kaixuan Bao and Wei Xu and Xiaohu You and Derrick Wing Kwan Ng},
  doi          = {10.1109/TMC.2025.3599529},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A framework of arithmetic-level variable precision computing for in-memory architecture: Case study in MIMO signal processing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-enhanced spatial range query over large-scale encrypted mobile cloud datasets. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving spatial range query allows users to obtain valid data based on specific spatial attributes or geographical location while ensuring privacy. However, many existing Privacy-Preserving Spatial Range Query (PSRQ) schemes generally face the problems of low query efficiency and insufficient security when dealing with large-scale mobile cloud data sets, and it is difficult to resist Indistinguishability under Chosen-Plaintext Attack (IND-CPA). To solve these challenges, we first propose an Efficient and Secure Spatial Range Query scheme (ESSRQ), which is based on a dual mobile cloud architecture by integrating Geohash algorithm, Circular Shift Coalesce Zero-Sum Garbled Bloom Filter (CSC-ZGBF) and Symmetric Homomorphic Encryption (SHE), achieving a constant search complexity. However, ESSRQ cannot protect the access patterns, where the cloud server still has the potential to infer attacks based on the index position and even obtain plaintext queries. On this basis, we further propose an extended scheme ESSRQ-PIR, which introduces Private Information Retrieval (PIR) into single mobile cloud-based architecture, effectively prevents the leakage of access patterns, enhances the security of ESSRQ and can also realize efficient query on large-scale cloud datasets. Formal security analysis proves that our proposed schemes are secure against IND-CPA, and extensive experiments demonstrate that our schemes improve the query efficiency by up to nearly 20 times when compared with previous solutions. These features make the proposed schemes particularly suitable for privacy-preserving spatial queries in mobile cloud computing environments.},
  archive      = {J_TMC},
  author       = {Yinbin Miao and Jiaqi Yu and Jiliang Li and Xinghua Li and Jun Feng and Zhiquan Liu and Robert H. Deng},
  doi          = {10.1109/TMC.2025.3599519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-enhanced spatial range query over large-scale encrypted mobile cloud datasets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards accurate training time estimation for on-device heterogeneous federated learning. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimation of on-device model training time is increasingly required for emerging learning paradigms on mobile edge devices, such as heterogeneous federated learning (HFL). HFL usually customizes the model architecture according to the different capabilities of mobile edge devices to ensure efficient use of local data from all devices for training. However, due to oversimplification of training time modeling, existing methods rely on a single coefficient to represent computational heterogeneity, resulting in sub-optimal HFL efficiency. We find that existing methods ignore the important impact of runtime optimization of deep learning frameworks, which we call development-chain diversity. Specifically, layers of a model may have different algorithm implementations, and deep learning frameworks often have different strategies for selecting the algorithm they believe is the best based on a range of runtime factors, resulting in different training latencies and invalid predictions from existing methods. In this paper, in addition to considering this diversity to ensure synchronized completion time of model training, we also study how to select the best algorithm each time to reduce the latency of the per-round training, thereby further improving the overall efficiency of federated training. To this end, we propose LATTE, which consists of a novel data-driven selector that identifies the best algorithm at runtime based on relative runtime factors. By further integrating it into our training latency model, LATTE provides accurate training time estimation, significantly outperforming traditional heuristic approaches. To further improve the robustness of LATTE, we proposed dynamic device adapter to cope with the dynamic joining and exiting of the clients. We develop LATTE as middleware, compatible with different deep learning frameworks. Extensive results show significantly improved training convergence speed and model accuracy compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Kun Wang and Zimu Zhou and Zhenjiang Li},
  doi          = {10.1109/TMC.2025.3599524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards accurate training time estimation for on-device heterogeneous federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing secrecy performance of full-duplex relaying systems using IRS and RSMA. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a secure wireless system that integrates rate-splitting multiple access (RSMA), intelligent reflecting surfaces (IRS), and full-duplex relaying (FDR) to enhance secrecy performance against multiple colluding eavesdroppers. Closed-form expressions for the secrecy outage probabilities (SOPs) and average secrecy capacities (ASCs) of both common and private messages are derived. Comparative analysis with a baseline RSMA-FDR system (without IRS) demonstrates the benefits of IRS in improving secrecy. Numerical results reveal that RSMA-IRS-FDR achieves superior secrecy performance, with SOPs and ASCs strongly influenced by transmission power, particularly differing between message types. Moreover, the adverse effect of residual self-interference (RSI) is substantially mitigated in the IRS-aided system. The secrecy performance is further enhanced by increasing the number of IRS elements. The study also investigates the impact of key parameters such as power allocation, target secrecy rate, fading order, Wi-Fi frequency, and number of eavesdroppers, offering practical insights for secure RSMA-IRS-FDR system design.},
  archive      = {J_TMC},
  author       = {Phuong T. Tran and Thong-Nhat Tran and Ba Cao Nguyen and Tran Manh Hoang and Le The Dung and Taejoon Kim},
  doi          = {10.1109/TMC.2025.3599513},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing secrecy performance of full-duplex relaying systems using IRS and RSMA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-USV coverage path planning using spatial graph multi-actor-attention-critic reinforcement learning framework with operator pooling. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-unmanned surface vehicle coverage path planning (MCPP) presents significant challenges in large-scale aquatic environments due to dynamic ocean currents, non-Euclidean spatial structures, and task load imbalance. To address these challenges, we propose the spatial graph multi-agent actorattention- critic (SGMAAC) framework, which integrates a novel spatial graph attention network (SpGAT) for adaptive non- Euclidean feature extraction and operator pooling for dynamic path optimization and task load balance. Specifically, SpGAT captures global-local topological dependencies to enhance decisionmaking under irregular topography, while operator pooling employs multi-step grow, deduplicate, and exchange operations to eliminate redundant paths and balance task loads across USVs. Additionally, SGMAAC introduces a multi-objective reward function that jointly optimizes coverage efficiency, collision avoidance, and energy consumption, enabling coordinated path planning in large-scale aquatic environments. Experimental results demonstrate that SGMAAC outperforms baseline methods across diverse aquatic scenarios, achieving improvements in convergence speed, makespan, task load balance, and path costs.},
  archive      = {J_TMC},
  author       = {Yuanbo Zhu and Guangjie Han and Chuan Lin and Fan Zhang and Yun Hou},
  doi          = {10.1109/TMC.2025.3599616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-USV coverage path planning using spatial graph multi-actor-attention-critic reinforcement learning framework with operator pooling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge data auditing method supporting multi-keyword validation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3599724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of latency-sensitive applications like VR/AR-based immersive gaming, fueled by 5G and edge computing, demands ultra-low latency. While deploying data replicas on edge servers addresses latency, the highly distributed and dynamic edge environment makes these replicas vulnerable to corruption. Furthermore, the constrained resources of edge servers, compared to cloud infrastructure, render traditional data integrity auditing schemes inefficient or impractical. Crucially, existing solutions lack the ability to perform targeted integrity checks on specific data segments (e.g., files containing sensitive user information), leaving critical vulnerabilities undetected. To overcome these limitations, this paper introduces EDI-K, a novel batch auditing scheme featuring multi-keyword authentication. EDI-K's key contributions are: (1) Enabling efficient, targeted integrity verification for data containing specific keywords; (2) Guaranteeing keyword privacy during the auditing process; and (3) Incorporating a novel data structure that facilitates highly efficient dynamic operations (updates, inserts, deletes) on the audited data. Security analysis confirms EDI-K's robustness, while comprehensive performance evaluations demonstrate its significant efficiency advantages over existing approaches, making it particularly suitable for the resource-scarce edge computing landscape.},
  archive      = {J_TMC},
  author       = {Jun Ye and Yu Jiang},
  doi          = {10.1109/TMC.2025.3599724},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge data auditing method supporting multi-keyword validation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum multi-agent reinforcement learning for cooperative mobile access in space-air-ground integrated networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving global space-air-ground integrated network (SAGIN) access only with CubeSats presents significant challenges such as the access sustainability limitations in specific regions (e.g., polar regions) and the energy efficiency limitations in CubeSats. To tackle these problems, high-altitude long-endurance unmanned aerial vehicles (HALE-UAVs) can complement these CubeSat shortcomings for providing cooperatively global access sustainability and energy efficiency. However, as the number of CubeSats and HALE-UAVs, increases, the scheduling dimension of each ground station (GS) increases. As a result, each GS can fall into the curse of dimensionality, and this challenge becomes one major hurdle for efficient global access. Therefore, this paper provides a quantum multi-agent reinforcement Learning (QMARL)-based method for scheduling between GSs and CubeSats/HALE-UAVs in order to improve global access availability and energy efficiency. The main reason why the QMARL-based scheduler can be beneficial is that the algorithm facilitates a logarithmic-scale reduction in scheduling action dimensions, which is one critical feature as the number of CubeSats and HALE-UAVs expands. Additionally, individual GSs have different traffic demands depending on their locations and characteristics, thus it is essential to provide differentiated access services. The superiority of the proposed scheduler is validated through data-intensive experiments in realistic CubeSat/HALE-UAV settings.},
  archive      = {J_TMC},
  author       = {Gyu Seon Kim and Yeryeong Cho and Jaehyun Chung and Soohyun Park and Soyi Jung and Zhu Han and Joongheon Kim},
  doi          = {10.1109/TMC.2025.3599683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum multi-agent reinforcement learning for cooperative mobile access in space-air-ground integrated networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMIT: Reflection-based charging jamming attack. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Wireless Rechargeable Sensor Networks (WRSNs) based platforms have become promising for broad applications. However, if an adversary disrupts the wireless charging process in WRSNs, sensors may die due to lack of timely energy supply, compromising the reliability and availability of systems relying on sensing tasks. In this paper, we develop a zero-cost power jamming attack in WRSNs, termed rEflection-based jaMmIng aTtack (EMIT), which introduces an off-the-shelf and inconspicuous reflector such as a Coca-Cola can that intentionally reflects the wave from the charger to destructively interfere with the charging wave at the target sensor. Our approach lifts the limitations of traditional charging attacks, including high cost, complex implementation and ease of detection. We conduct extensive field experiments to evaluate EMIT attack in different types of WRSNs. The results show that on average, the success rate of EMIT attack is 90% in WRSNs with fixed charging locations, and 75% in WRSNs with dynamic charging locations. Finally, we build a real-world WRSN on university campus to study the effectiveness of EMIT attack in complex scenarios. In total, EMIT attack causes 134 sensor deaths over 66 days.},
  archive      = {J_TMC},
  author       = {Tang Liu and Jing Gao and Yuan Yin and Die Wu and Jian Peng and Wenzheng Xu and Baijun Wu and Yazhou Tu},
  doi          = {10.1109/TMC.2025.3600093},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EMIT: Reflection-based charging jamming attack},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel secure split federated semantic learning framework and its optimization for digital twin network evolution. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel secure split federated semantic learning (SFsL) framework to facilitate the maintenance and evolution of digital twin networks (DTNs). Efficiently updating and evolving DTNs generally involves several critical processes: semantic extraction and transmission for physical-to-virtual synchronization, virtual model transformation and verification, and ensuring the security and privacy of physical entity data. While conventional semantic communication frameworks can effectively address semantic extraction and transmission, the complexities of virtual model transformation, verification, and data security demand a more comprehensive approach. To address these challenges, the proposed SFsL framework integrates split federated learning with task-oriented secure semantic communication schemes. In addition, it incorporates a token-based semantic defence method to distinguish between adversarial and authentic semantic data and an asynchronous secure model aggregation mechanism to enhance data-sharing efficiency. The system reliability is then formulated as a stochastic optimization problem, aiming to minimize cost complexity while maintaining high accuracy during periodic model aggregation. Evaluation results, obtained using performance metrics such as privacy loss, experienced loss, accuracy, cost and reliability, demonstrate that the SFsL framework outperforms other commonly adopted security and privacy schemes, offering improved efficiency towards the maintenance and evolution of such dynamic systems. This highlights the capability of SFsL to enable adaptive, efficient and reliable network evolutions when deployed in practical DTNs with dynamic resource constraints.},
  archive      = {J_TMC},
  author       = {Samuel D. Okegbile and Haoran Gao and Jun Cai},
  doi          = {10.1109/TMC.2025.3599838},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel secure split federated semantic learning framework and its optimization for digital twin network evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SC-GIR: Goal-oriented semantic communication via invariant representation learning for image transmission. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-oriented semantic communication (SC) aims to revolutionize communication systems by transmitting only task-essential information. However, current approaches face challenges such as joint training at transceivers, leading to redundant data exchange and reliance on labeled datasets, which limits their task-agnostic utility. To address these challenges, we propose a novel framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for image transmission. Our framework leverages self-supervised learning to extract an invariant representation that encapsulates crucial information from the source data, independent of the specific downstream task. This compressed representation facilitates efficient communication while retaining key features for successful downstream task execution. Focusing on machine-to-machine tasks, we utilize covariance-based contrastive learning techniques to obtain a latent representation that is both meaningful and semantically dense. To evaluate the effectiveness of the proposed scheme on downstream tasks, we apply it to various image datasets for lossy compression. The compressed representations are then used in a goal-oriented AI task. Extensive experiments on several datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%, and achieves over 85% classification accuracy for compressed data under different SNR conditions. These results underscore the effectiveness of the proposed framework in learning compact and informative latent representations.},
  archive      = {J_TMC},
  author       = {Senura Hansaja Wanasekara and Van-Dinh Nguyen and Kok-Seng Wong and M.-Duong Nguyen and Symeon Chatzinotas and Octavia A. Dobre},
  doi          = {10.1109/TMC.2025.3600434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SC-GIR: Goal-oriented semantic communication via invariant representation learning for image transmission},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RipeTrack: Assessing fruit ripeness and remaining lifetime using smartphones. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3599917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have shown that a significant fraction of fresh fruits is discarded at the retail and consumer levels, wasting precious resources, polluting the environment, and contributing to increased food prices. An important factor contributing to this problem is the lack of scalable solutions for determining fruit ripeness and remaining lifetime. We propose a cost-effective solution that leverages the sensing capabilities of phones and machine learning models to analyze the optical properties of fruits at various ripening stages. The proposed solution is non-invasive, works for different fruits, and produces intuitive outputs, e.g. Unripe/Ripe/Expired and the percentage of remaining lifetime, enabling retailers and consumers to minimize food waste. We implement a proof-of-concept mobile application, RipeTrack, and demonstrate the accuracy and robustness of the proposed approach using an extensive empirical study with multiple fruits, including avocados, pears, bananas, nectarines, and mangoes. Our results show, for example, that RipeTrack can identify the ripeness level of avocados and pears with an accuracy of 95% and 98%, respectively, and it can predict their remaining lifetimes with an accuracy of 93% and 97%. Our results also show that RipeTrack can easily be extended to new fruits using transfer learning, and it functions in realistic environments, e.g. homes and grocery stores, that have diverse illuminations.},
  archive      = {J_TMC},
  author       = {Muhammad Shahzaib Waseem and Neha Sharma and Mohamed Hefeeda},
  doi          = {10.1109/TMC.2025.3599917},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RipeTrack: Assessing fruit ripeness and remaining lifetime using smartphones},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference service fidelity maximization in DT-assisted edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin (DT) technology enables smooth integrations of cyber and physical worlds in alignment with the Industry 4.0 initiative. DTs are virtual presentations of physical objects. Through synchronizations with physical objects in real-time, DTs can reflect the states of their objects with high fidelity. Orthogonal to the DT technology, mobile edge computing (MEC) is a promising computing paradigm that shifts computing power to the edge network, which is appropriate for delay-sensitive intelligent services. In this paper, we study fidelity-aware inference services in a DT-assisted MEC environment, where machine learning-based inference models must be continuously retrained using updated DT data in order to provide high-fidelity services for consumers. To this end, we first formulate two novel optimization problems: the initial DT and model placement problem with the aim of minimizing the total cost of various resources consumed, and the cumulative fidelity maximization problem to maximize the long-term cumulative fidelity of service models while minimizing the cost of resource consumption on service model fidelity enhancements over a given time horizon, through jointly scheduling mobile devices to upload their update data to synchronize with their DTs and determining whether DTs and/or models to be migrated at each time slot. We then develop an efficient algorithm for the initial DT and model placement problem, through a reduction to a series of minimum-cost maximum matching problems in auxiliary graphs. We also devise an online algorithm with a provable competitive ratio for the cumulative fidelity maximization problem, by designing an elegant service request admission strategy. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms are promising, and outperform their baselines by no less than 28%.},
  archive      = {J_TMC},
  author       = {Jing Li and Jianping Wang and Weifa Liang and Xiaohua Jia and Albert Y. Zomaya},
  doi          = {10.1109/TMC.2025.3600390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Inference service fidelity maximization in DT-assisted edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed resource allocation and coordinated scheduling for end-edge-cloud collaborative computing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3599885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tier computation offloading is crucial to address capacity constraints and improve flexibility for mobile devices. However, existing research on multi-layer computing offloading faces challenges like inefficient resource utilization and poor scalability, particularly in handling diverse computational tasks. To address these challenges, this paper proposes a distributed resource allocation and mixed task offloading framework for end-edge-cloud collaborative systems that support partial and full task offloading modes. First, we propose a three-tier network computing architecture and formulate a task-offloading utility maximization problem by jointly optimizing mixed task-offloading and resource allocation. The proposed problem is a mixed integer nonlinear program (MINLP), which we solve by decomposing it into two subproblems resource allocation and task offloading. Edge computing resources and bandwidth allocation can be independently optimized at each edge node with a fixed task offloading strategy. Cloud computing resource allocation, while convex, involves a global constraint, which we solve in a decentralized manner using a multi-agent optimization approach. Then, we propose a joint task offloading and resource allocation optimization algorithm, CNO-TORA, to obtain the solution to the formulated problem. The algorithm is supported by strong theoretical guarantees and is almost surely convergent to a globally optimal solution. Experimental results on a real dataset demonstrate that our algorithm is scalable to large-scale networks and outperforms baselines, achieving improvements in average system utility ranging from 4.01%-28.15%.},
  archive      = {J_TMC},
  author       = {Changqing Long and Wenchao Meng and Shizhong Li and Shibo He and Chaojie Gu and Lin Cai},
  doi          = {10.1109/TMC.2025.3599885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed resource allocation and coordinated scheduling for end-edge-cloud collaborative computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifying AI for networking and networking for AI: The self-evolving edge learning. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3600270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Learning environments, characterized by limited wireless resources, encounter significant bottlenecks in network performance, particularly in Federated Learning (FL) tasks. Current resource allocation strategies are primarily classified into “AI for Networking” and “Networking for AI”. However, both approaches fail to adequately address the interaction between network states and AI task requirements, thereby limiting their overall effectiveness. To address this, we propose a novel bidirectional dynamic collaborative optimization mechanism that enables real-time interaction between AI task performance and network states. This mechanism adjusts both AI task resource requirements and network configurations based on performance feedback, breaking away from traditional unidirectional optimization approaches. We introduce the AI-network unified algorithm, which incorporates data-driven dynamic sensing and enhances system adaptability and robustness, achieving self-optimization in edge learning. Theoretical analysis and simulation results demonstrate the significant advantages of our approach in simultaneously improving network resource utilization and AI task performance, providing an effective solution for the future wireless network.},
  archive      = {J_TMC},
  author       = {Jie Tang and Yeguang Qin and Fengxiao Tang and Ming Zhao and Nei Kato},
  doi          = {10.1109/TMC.2025.3600270},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unifying AI for networking and networking for AI: The self-evolving edge learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G-TPS: A two-phase real-time scheduling and adaptation framework for 5G radio access networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3599880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the many industrial wireless solution candidates, 5G New Radio (NR) has drawn significant attention in recent years due to its capabilities to support ultra-high-speed communication, wide coverage, ultra-low latency, and massive connectivity. Despite its great potential, 5G NR also brings significant complexity in scheduling data flows to meet their hard real-time requirements in industrial applications. In this paper, we first leverage a 5G RAN testbed to benchmark the downlink throughput and explore the impact of modulation and coding scheme (MCS) selection on the network performance. We then formulate a real-time flow scheduling problem in industrial 5G NR, which features per-flow real-time schedulability guarantee through time-frequency resource allocation. We propose a novel two-phase scheduling framework, named 5G-TPS, to construct a schedule that meets the deadlines of all the flows. To adapt to dynamic channel conditions, 5G-TPS enables online schedule adjustment for affected flows to meet their timing requirements. For large-scale multi-cell 5G industrial systems with cloud radio access network (C-RAN) architecture, we further introduce a user association algorithm respecting the real-time requirements of individual user equipment (UEs). Extensive experimental studies show that 5G-TPS can achieve schedulability ratios comparable to the Satisfiability Modulo Theory (SMT)-based exact solution and outperform many other state-of-the-art scheduling approaches, including the built-in 5G NR schedulers.},
  archive      = {J_TMC},
  author       = {Tianyu Zhang and Jiachen Wang and X. Sharon Hu and Song Han},
  doi          = {10.1109/TMC.2025.3599880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {5G-TPS: A two-phase real-time scheduling and adaptation framework for 5G radio access networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An offline multi-agent reinforcement learning framework for radio resource management. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a $15\%$ improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.},
  archive      = {J_TMC},
  author       = {Eslam Eldeeb and Hirley Alves},
  doi          = {10.1109/TMC.2025.3599918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An offline multi-agent reinforcement learning framework for radio resource management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigFormer: A transformer for signaling data augmentation via location reconstruction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3599925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile signaling data has been used to discover comprehensive and fine-grained patterns of individual travel activities. However, mobile signaling data often exhibits notable data quality issues caused by systematic factors (e.g., weather, construction, congestion) or individual factors (e.g., low battery, poor reception). Improving the quality of signaling data is a crucial yet challenging task due to irregular spatiotemporal intervals and complex inter-correlations among base stations. In this paper, we formalize the augmentation of signaling data as a base station location reconstruction task. We propose a location-aware transformer structure named SigFormer, which employs self-supervised learning to reconstruct the location information of missing base stations based on the sampled signaling sequence. Specifically, we first design a Continuous Signaling Encoder to model irregularly sampled signaling sequences and encode interval information generated by base station transitions. Then, we learn the Base Station Embedding to describe implicit features of base stations and design a Neighbor Region Encoder to incorporate geographic information as an auxiliary for base station representations. Finally, through an attention-based encoder-decoder framework, we aggregate location information and base station features, employing sequence learning to capture spatiotemporal dependencies and reconstruct base station locations. Experiment results on real-world datasets indicate that our method outperforms existing approaches for location reconstruction. The proposed method originates from practical demands in telecom systems, addressing the challenge of missing and abnormal base station locations by modeling spatiotemporal transition patterns in signaling data. Our approach supports various tasks in practical applications, including imputing missing base station locations and correcting anomalous locations, effectively improving the quality of signaling data.},
  archive      = {J_TMC},
  author       = {Mingzhe Liu and Haiyang Jiang and Tongyu Zhu and Leilei Sun and Jibin Wang and Hao Sheng},
  doi          = {10.1109/TMC.2025.3599925},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SigFormer: A transformer for signaling data augmentation via location reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FullPerception: Network-level collaborative perception for eliminating vehicular blind spots. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3600060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative perception can significantly enhance the perceptual capabilities of autonomous vehicles by sharing sensing information through vehicular communications. However, large-scale sharing of sensing information often results in unsustainable network loads, making it challenging to maximize perception performance with limited communication resources in complex environments. To address this challenge, we propose FullPerception, an innovative cooperative perception framework that jointly orchestrates sensing information sharing and communication resource allocation at the network level. FullPerception advocates for the sharing of semantic information (neural network features) within critical areas, i.e., blind spots. With limited communication resources, FullPerception strategically eliminates these blind spots to maximize the accumulated perception performance. We formulate this strategy as a weighted optimization problem and prove its NP-hardness. We propose a simple yet effective algorithm, Proactive Conflict-free Scheduling (PCS), which guarantees a good performance ratio by considering broader contexts. PCS is meticulously combined with recursive structure, accounting for both the overall and future contexts to determine link scheduling and resource allocation. We demonstrate that FullPerception improves perception accuracy by 20% relative to single-vehicle systems and by 10% compared to existing scheduling methods through large-scale comprehensive joint simulation experiments.},
  archive      = {J_TMC},
  author       = {Lin Liang and Guiyang Luo and Yijing Lin and Lei Deng and Nan Cheng and Quan Yuan and Jinglin Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3600060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FullPerception: Network-level collaborative perception for eliminating vehicular blind spots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-sustainable multi-functional RIS-enabled integrated sensing and communication systems. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3599887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surface (RIS)-enabled integrated sensing and communication (ISAC) systems enhance spectrum efficiency and sensing accuracy. Building on this, we propose a novel self-sustainable multi-functional RIS (S-MFRIS) concept that supports multiple functionalities: reflection, refraction, amplification, energy harvesting, and target sensing. By harvesting energy from incident signals, the S-MFRIS can reflect, refract, and amplify signals without needing an external power supply, effectively overcoming double-fading attenuation. Furthermore, by deploying low-cost sensor elements, the S-MFRIS can capture echo signals from multiple targets, mitigating the signal attenuation commonly associated with multi-hop links. Then, we establish an S-MFRIS-enabled ISAC system and formulate an optimization problem to maximize the signal-to-interference-plus-noise ratio (SINR) of the sensing targets, subject to constraints on communication rate, power budget, and reflection coefficients. To solve this non-convex problem, we decompose it into three sub-problems, which are efficiently addressed using an iterative algorithm. Simulation and numerical results demonstrate the following key findings: (1) The proposed algorithm achieves better convergence and performance than the semidefinite relaxation-based and random-based algorithms. (2) The performance of the MFRIS-aided system varies under different operating protocols, with the self-sustainable MFRIS outperforming other schemes, particularly when the power budget is sufficient. (3) The proposed S-MFRIS achieves $30\%-46\%$ sensing SINR gains at most for the same total power budget or element configuration. (4) The number of sensing elements improves sensing performance up to a certain point, after which further increases in the number of elements yield diminishing returns.},
  archive      = {J_TMC},
  author       = {Xueyan Cao and Shubin Wang and Yuzheng Ren},
  doi          = {10.1109/TMC.2025.3599887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Self-sustainable multi-functional RIS-enabled integrated sensing and communication systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\pi$-eLight: Programmatic interpretable reinforcement learning for effective traffic signal control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in Deep Reinforcement Learning (DRL) have significantly improved the performance of adaptive Traffic Signal Control (TSC). However, DRL policies are typically represented by over-parameterized neural networks, which function as black-box models. Consequently, the learned policies often lack interpretability and are challenging to deploy on resource-constrained edge hardware. Moreover, the DRL methods frequently exhibit poor generalization, struggling to transfer the learned policies across different geographical regions. These limitations hinder the real-world applicability of learning-based approaches. To address these issues, we suggest the use of an inherently interpretable program for representing the control policy. We present Programmatic Interpretable reinforcement learning for effective traffic signal control ($\pi$-eLight), a new approach designed to autonomously discover non-differentiable programs. Specifically, we first define an effective program framework as the control policy, where certain components remain learnable. Next, we introduce a Domain Specific Language (DSL) for constructing interpretable programs and transformation rules for generating programs with hierarchical structures. Last, we utilize Monte Carlo Tree Search (MCTS) to find the optimal program in a discrete space. Extensive experiments demonstrate that $\pi$-eLight consistently outperforms DRL-based baselines while exhibiting superior generalization across intersections in different cities. Moreover, the learned programmatic policies can be directly deployed on edge devices with minimal computational resources, further enhancing real-world applicability.},
  archive      = {J_TMC},
  author       = {Yin Gu and Kai Zhang and Qi Liu and Haojie Yuan and Runlong Yu},
  doi          = {10.1109/TMC.2025.3600533},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {$\pi$-eLight: Programmatic interpretable reinforcement learning for effective traffic signal control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPdetector: A multi-party collaborative federated transfer learning approach for IoT intrusion detection. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3600306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive adoption of the Internet of Things (IoT) is accompanied by numerous network security threats, making the timely detection of anomalies in traffic data through intrusion detection increasingly critical. The existing intrusion detection methods based on federated learning can achieve good results under the condition of sufficient labeled data. However, the traffic data of participants in real IoT environments have various characteristics and there are a large amount of unlabeled data, which easily leads to the performance degradation of the intrusion detection model. To address this challenge, this paper proposes a novel federated transfer learning approach for IoT intrusion detection based on multi-party collaboration called MPdetector. MPdetector uses an encoder to extract the feature representation of diverse traffic data from heterogeneous clients, and maps the feature representation of each client to a unified feature space. In addition, a label transfer strategy is introduced to make full use of unlabeled data, and a new mapping function is used to reconstruct the traffic data of each client to expand client's local data set, which can further improve the detection performance of the model in varied and complex IoT environments. Theoretical analysis proves that the entire transfer learning process of MPdetector is conducted within a secure context. Experiments on four widely used intrusion detection datasets show that MPdetector can detect known and unknown abnormal traffic more accurately than the existing three classical intrusion detection algorithms, and has strong generalization. Meanwhile, the detection effect of MPdetector will be further improved with the increase of the volume of labeled traffic.},
  archive      = {J_TMC},
  author       = {Li Lin and ZhenKun Chen},
  doi          = {10.1109/TMC.2025.3600306},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MPdetector: A multi-party collaborative federated transfer learning approach for IoT intrusion detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFF-SLAM: Dynamic feature filtering-based simultaneous localization and mapping for UAV positioning in IoT-enabled complex environments. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the 5G RedCap, the upcoming 6G and the proliferation of the Internet of Things (IoT) have catalyzed the rapid advancement of unmanned aerial vehicle (UAV) technology while also promoting UAVs' widespread application. In IoT-enabled environments where the global positioning system (GPS) signals are compromised, visual simultaneous localization and mapping (V-SLAM) technology has emerged as an effective positioning solution, valued for its reliability. However, the presence of dynamic elements in complex environments, such as pedestrians and vehicles, poses challenges to the positioning accuracy of UAVs employing V-SLAM for navigation. This paper proposes a dynamic feature filtering-based SLAM (DFF-SLAM) approach to eliminate the impact of dynamic factors in dynamic environments, thereby enhancing the positioning accuracy of UAVs in IoT-enabled complex environments. Firstly, a semantic detection thread is designed to identify semantic information in the scene and acquire prior dynamic targets, facilitating the filtering of prior dynamic feature points. Secondly, optical flow tracking conducted at each level of the image pyramid facilitates feature point matching across consecutive images. Finally, the epipolar geometry constraint is utilized to determine the motion status of remaining feature points, further filtering out dynamic feature points. Simulation results demonstrate that compared to traditional visual SLAM systems, the UAV equipped with the DFF-SLAM system achieves more accurate positioning and meets real-time positioning requirements when navigating through IoT enabled complex environments},
  archive      = {J_TMC},
  author       = {Jinglei Li and Yiming Jia and Meng Qin and Qinghai Yang and Tony Q. S. Quek and Wen Gao and Kyung Sup Kwak},
  doi          = {10.1109/TMC.2025.3600661},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DFF-SLAM: Dynamic feature filtering-based simultaneous localization and mapping for UAV positioning in IoT-enabled complex environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of UAV-carried IRS for urban low altitude mmWave communications with deep reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3600682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging technologies in sixth generation (6G) of wireless communications, such as terahertz communication and ultra-massive multiple-input multiple-output, present promising prospects. Despite the high data rate potential of millimeter wave communications, millimeter wave (mmWave) communications in urban low altitude economy (LAE) environments are constrained by challenges such as signal attenuation and multipath interference. Specially, in urban environments, mmWave communication experiences significant attenuation due to buildings, owing to its short wavelength, which necessitates developing innovative approaches to improve the robustness of such communications in LAE networking. In this paper, we explore the use of an unmanned aerial vehicle (UAV)-carried intelligent reflecting surface (IRS) to support low altitude mmWave communication. Specifically, we consider a typical urban low altitude communication scenario where a UAV-carried IRS establishes a line-of-sight (LoS) channel between the mobile users and a source user (SU) despite the presence of obstacles. Subsequently, we formulate an optimization problem aimed at maximizing the transmission rates and minimizing the energy consumption of the UAV by jointly optimizing phase shifts of the IRS and UAV trajectory. Given the non-convex nature of the problem and its high dynamics, we propose a deep reinforcement learning-based approach incorporating neural episodic control, long short-term memory, and an IRS phase shift control method to enhance the stability and accelerate the convergence. Simulation results show that the proposed algorithm effectively resolves the problem and surpasses other benchmark algorithms in various performances.},
  archive      = {J_TMC},
  author       = {Wenwen Xie and Geng Sun and Bei Liu and Jiahui Li and Jiacheng Wang and Hongyang Du and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3600682},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of UAV-carried IRS for urban low altitude mmWave communications with deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing network reliability in UASNs: A collision-aware critical node identification algorithm. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3600460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical node identification is essential for Underwater Acoustic Sensor Networks (UASNs) to ensure network connectivity and reliability. Existing methods identify critical nodes by evaluating their contributions to network connectivity and node communication count. However, these methods identify critical nodes inaccurately due to neglecting the influence of packet collisions, leading to unreliable network. Packet collisions disrupt connected links and cause communication failures, resulting in unreliable network connectivity and improper communication count. To this end, we propose the Collision-Aware Critical Node Identification Algorithm (CCNIA), which accounts for the impact of packet collisions to improve the accuracy of critical node identification and enhance network reliability. CCNIA identifies critical nodes with high connectivity, large collision probability, and heavy network load, through building the three following interdependent models. Specifically, Topological Connectivity Model (TCM) evaluates link reachability by analyzing connectivity and density within a node's local network. Based on TCM, Collision Probability Model (CPM) further ensures packet reliability by quantifying the impact of packet collisions on critical node identification. Through CPM's reliable packet transmissions, Network Load Model (NLM) assesses network efficiency by analyzing node occurrence count within global end-to-end communication paths. Experiments show that CCNIA outperforms existing methods across diverse network configurations, enhancing network reliability in terms of packet delivery ratio, delay, and energy efficiency.},
  archive      = {J_TMC},
  author       = {Shanshan Song and Xiujuan Wu and Cangzhu Xu and Miao Pan and Guangjie Han},
  doi          = {10.1109/TMC.2025.3600460},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing network reliability in UASNs: A collision-aware critical node identification algorithm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning on heterogeneous and long-tailed data via disentangled representation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3600767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a popular distributed machine learning method that enables the development of a robust global model through decentralized computation and periodic model aggregation, without requiring direct access to clients' data. However, data heterogeneity poses a significant challenge in FL, and the global long-tail distribution exacerbates this issue. While substantial research has focused on mitigating performance degradation caused by long-tailed distributions, existing methods typically concentrate on addressing discrepancies between local and global class distributions, often overlooking the fact that these discrepancies stem from variations in the data itself. To address this, we propose a novel approach, Federated Context Optimization and Feature Information Decoupling (FedDR), which generates partition strategies for each sample to extract and leverage long-tail, global, personalized, and label-text information within its features to enhance the representational distinction of tail classes. Specifically, we first design a Feature Information Decoupling module that separates global, personalized, and long-tail information within the features and incorporates this information into the loss function to strengthen the global model's focus on personalized information in tail samples. Furthermore, to exploit the textual label information embedded in the samples, we integrate a cross-modal model, CoOp, which utilizes open-vocabulary prior knowledge, and implement dynamic knowledge distillation between the client model and CoOp to enhance the client model's feature representation capability. Extensive experimental results on multiple benchmarks demonstrate that the proposed FedDR outperforms state-of-the-art methods in the federated long-tailed learning setting.},
  archive      = {J_TMC},
  author       = {Yizhi Zhou and Junxiao Wang and Yuchen Qin and Xin Xie and Zhipeng Song and Heng Qi},
  doi          = {10.1109/TMC.2025.3600767},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated learning on heterogeneous and long-tailed data via disentangled representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BOTH: Efficient coordination of mobile agents with graph-enhanced bayesian online learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3600920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative agents, consisting of at least one human and one mobile robot agent working toward a common objective, are increasingly prevalent and effective in both social and industrial spheres, such as manufacturing. The inherent heterogeneity of these agents requires efficient and scalable Task Scheduling and Allocation (TSA) schemes that match individuals to tasks based on their abilities and meet specific temporal constraints, maximizing performance in less time. Existing works face challenges as exact methods rely on assumptions and deterministic models, which struggle to scale and infer time-varying, stochastic human task performance. While offline reinforcement learning shows promise, it is time-consuming and heavily dependent on training data that is often scarce in practical factory settings. To address these challenges, we formulate the TSA problem in mobile multi-agent teams as a temporal-constrained contextual decision-making process and propose the Bayesian Optimization-augmented Team coordination among Heterogeneous agents (BOTH), a novel scalable and training-free scheduling approach. The core idea is to use Gaussian Processes (GP) to iteratively infer agent dynamics in real-time, enabling the automatic derivation of a robust TSA solution that requires no prior data and adapts to varying problem sizes. We start by employing a heterogeneous graph-based encoder to extract representative context from the individual differences among team agents and tasks, considering strict temporal constraints. Following this, we propose a GP-driven Bayesian optimizer to intelligently explore and exploit optimal task assignments for each context, without making assumptions about the system. Experiments on synthetic and real datasets demonstrate that BOTH boosts accuracy and time efficiency compared to competing baselines, even within a few iterations.},
  archive      = {J_TMC},
  author       = {Hui Wang and Zhiwen Yu and Yao Zhang and Jiaqi Liu and Liekang Zeng and Huan Zhou and Bin Guo and Guoliang Xing},
  doi          = {10.1109/TMC.2025.3600920},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BOTH: Efficient coordination of mobile agents with graph-enhanced bayesian online learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO-split: A semi-supervised split learning framework over LEO satellite networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3601371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the increasing deployment of LEO satellite systems has enabled various space analytics (e.g., crop and climate monitoring), which heavily relies on the advancements in deep learning (DL). However, the intermittent connectivity between LEO satellites and ground station (GS) significantly hinders the timely transmission of raw data to GS for centralized learning, while the scaled-up DL models hamper distributed learning on resource-constrained LEO satellites. Though split learning (SL) can be a potential solution to these problems by partitioning a model and offloading primary training workload to GS, the labor-intensive labeling process remains an obstacle, with intermittent connectivity and data heterogeneity being other challenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL design tailored for satellite networks to combat these challenges. Leveraging SS learning to handle (labeled) data scarcity, we construct an auxiliary model to tackle the training failure of the satellite-GS non-contact time. Moreover, we propose a pseudo-labeling algorithm to rectify data imbalances across satellites. Lastly, an adaptive activation interpolation scheme is devised to prevent the overfitting of server-side sub-model training at GS. Extensive experiments with real-world LEO satellite traces (e.g., Starlink) demonstrate that our LEO-Split framework achieves superior performance compared to state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Yuxin Zhang and Zhe Chen and Zihan Fang and Cong Wu and Xianhao Chen and Yue Gao and Jun Luo},
  doi          = {10.1109/TMC.2025.3601371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LEO-split: A semi-supervised split learning framework over LEO satellite networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiFuse: A high-precision on-device depth perception system with multi-data fusion. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3601357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MobiFuse, a high-precision depth perception system on mobile devices that combines dual RGB and Time-of-Flight (ToF) cameras. To achieve this, we leverage physical principles from various environmental factors to propose the Depth Error Indication (DEI) modality, characterizing the depth error of ToF and stereo-matching. Furthermore, we employ a progressive fusion strategy, merging geometric features from ToF and stereo depth maps with depth error features from the DEI modality to create precise depth maps. Additionally, we create a new ToF-Stereo depth dataset, RealToF, to train and validate our model. Our experiments demonstrate that MobiFuse excels over baselines by significantly reducing depth measurement errors by up to 77.7%. It also showcases strong generalization across diverse datasets and proves effectiveness in two downstream tasks: 3D reconstruction and 3D segmentation. The demo video of MobiFuse in real-life scenarios is available at the de-identified YouTube link.},
  archive      = {J_TMC},
  author       = {Jinrui Zhang and Deyu Zhang and Tingting Long and Wenxin Chen and Ju Ren and Yunxin Liu and Yudong Zhao and Yaoxue Zhang and Youngki Lee},
  doi          = {10.1109/TMC.2025.3601357},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiFuse: A high-precision on-device depth perception system with multi-data fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-aware perturbation optimization for robust deep reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3601531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and statespecific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks},
  archive      = {J_TMC},
  author       = {Zongyuan Zhang and Tianyang Duan and Zheng Lin and Dong Huang and Zihan Fang and Zekai Sun and Ling Xiong and Hongbin Liang and Heming Cui and Yong Cui},
  doi          = {10.1109/TMC.2025.3601531},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {State-aware perturbation optimization for robust deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Provably secure and reliable privacy-preserving authentication scheme for drone-to-drone communications in internet of autonomous things. <em>TMC</em>, 1-10. (<a href='https://doi.org/10.1109/TMC.2025.3601369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancements in wireless communication technologies, Unmanned Aerial Vehicles (UAVs), also known as Small Unmanned Aerial Vehicles (SUAVs) or drones, have been increasingly used in various applications, including the civilian sector. As a result, the security of SUAVs has garnered significant attention from the research community. Furthermore, drones are resource-constrained in nature and can be vulnerable to various known cybersecurity attacks over wireless communication. In light of these considerations, we propose a Provably Secure and Reliable Privacy-Preserving Authentication Scheme for Drone-to-Drone Communications in Internet of Autonomous Things (PSRS-D2D). The proposed scheme employs a secure one-way cryptographic hash and Elliptic Curve Cryptography (ECC) to accomplish a certain level of security. We provide security and privacy analysis, comparing it with competing UAV authentication schemes. This ensures that the PSRS-D2D scheme can withstand various prominent security properties, including mutual authentication and strong anonymity, and is secure against several attacks, such as replay, impersonation, and Man-In-The-Middle (MITM) attacks. We evaluated the performance of the proposed scheme in terms of computational and communicational costs. Furthermore, we conducted a formal security analysis using the Real-Or-Random (ROR) model and the Scyther simulation tools, which demonstrate that our scheme offers significant advantages in terms of security and performance.},
  archive      = {J_TMC},
  author       = {Mohd Shariq and Norziana Jamil and Gopal Singh Rawat and Shehzad Ashraf Chaudhry and Mehedi Masud and Ashok Kumar Das},
  doi          = {10.1109/TMC.2025.3601369},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Provably secure and reliable privacy-preserving authentication scheme for drone-to-drone communications in internet of autonomous things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto actor-critic for communication and computation co-optimization in non-cooperative federated learning services. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3601833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations – PAC-MCoFL achieves approximately $5.8\%$ and $4.2\%$ improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.},
  archive      = {J_TMC},
  author       = {Renxuan Tan and Rongpeng Li and Xiaoxue Yu and Xianfu Chen and Xing Xu and Zhifeng Zhao},
  doi          = {10.1109/TMC.2025.3601833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pareto actor-critic for communication and computation co-optimization in non-cooperative federated learning services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal flight speed scheduling and battery swapping in UAV-enabled mobile edge computing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3601743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-distance and long-duration flight missions of unmanned aerial vehicles (UAVs), optimal scheduling of flight speed and energy replenishment is crucial to ensure flight efficiency and safety. This paper focuses on a UAV-based patrol inspection system, where a UAV is scheduled to visit multiple task nodes that are geographically distributed in the communication coverage of a base station (BS). The UAV hovers at each task node, performing data collection and data processing. The BS is equipped with a mobile edge computing (MEC) server and a battery swapping station, offering computation and energy support to the UAV. A decision-making model customized for the UAV is proposed, jointly optimizing flight speed selection, battery swapping, and task offloading to minimize the UAV's total operational cost in its flight. By introducing virtual nodes in the flight network, we construct a unidirectional extended graph, based on which the original nonconvex cost minimization problem is reformulated to a tractable mixed-integer convex problem. Further, a fast heuristic based on analytical target cascading (ATC) is developed to obtain suboptimal solutions to large-scale problems. Results demonstrate that the proposed model can lower the UAV's total operational cost by providing greater flexibility in terms of speed selection and battery swapping, and the proposed heuristic shows high computational efficiency for large-scale network scenarios.},
  archive      = {J_TMC},
  author       = {Dongmei Ye and Zhengqing Sun and Weifeng Zhong and Jiawen Kang and Xumin Huang and Dong In Kim and Shengli Xie and Chau Yuen},
  doi          = {10.1109/TMC.2025.3601743},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimal flight speed scheduling and battery swapping in UAV-enabled mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARWIN: Digital twin assisted robot navigation and WIreless network management. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3602213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated warehouses involve robots that move across the floor, avoiding obstacles while remaining connected via an access point (AP) to a central controller that instructs the robots. The complex propagation environment and presence of metallic surfaces results in spotty coverage, which changes over time as the location of stored products and machinery changes. Thus, maintaining an assured connectivity to APs while performing navigation is a challenge, although it is needed to relay local sensor data from the robots to the controller and receive directions from the latter. $\rm{DARWIN}$, involves creating a digital twin of the warehouse for training the robots by jointly optimizing the navigation and avoiding wireless dead-spots. $\rm{DARWIN}$ has three key capabilities: First, it captures the features of both physical and RF environments in the digital world. Second, it allows real-time updating of the digital twin if significant disparity is detected compared to the physical environment. Finally, it includes a reinforcement learning algorithm that jointly optimizes navigation and network resource management, while accounting for handover and outage. We validate $\rm{DARWIN}$ on an emulation environment consisting of Robot Operating System and Gazebo platforms along with real-world RF measurements. Results reveal that $\rm{DARWIN}$ reduces the number of steps by 43% compared to choosing the closest AP, while detecting environmental changes with maximum 96% accuracy to maintain a high-fidelity digital twin.},
  archive      = {J_TMC},
  author       = {Batool Salehi and Debashri Roy and Mark Eisen and Amit Baxi and Dave Cavalcanti and Kaushik Chowdhury},
  doi          = {10.1109/TMC.2025.3602213},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DARWIN: Digital twin assisted robot navigation and WIreless network management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlueKey: Exploiting bluetooth low energy for enhanced physical-layer key generation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3602221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) is a prevalent technology in various applications due to its low power consumption and wide device compatibility. Despite its numerous advantages, the encryption methods of BLE often expose devices to potential attacks. To fortify security, we investigate the application of Physical-layer Key Generation (PKG), a promising technology that enables devices to generate a shared secret key from their shared physical environment. Although extensively investigated, PKG is generally discussed in the context of Wi-Fi, and existing solutions for BLE demonstrate significantly lower performance. To bridge this gap, we propose a distinctive approach that capitalizes on the inherent characteristics of BLE to facilitate efficient PKG. We utilize the constant tone extension within BLE protocols to extract comprehensive physical layer information and introduce an innovative method that employs Legendre polynomial quantization for PKG. This method facilitates the exchange of secret keys with a high key matching rate and a high key generation rate. The efficacy of our approach is validated through extensive experiments on a software-defined radio platform, underscoring its potential to enhance security in the rapidly expanding field of BLE applications. A pilot study on commercial off-the-shelf BLE devices further validates the system's practicality, revealing important trade-offs between performance and hardware constraints in real-world deployments.},
  archive      = {J_TMC},
  author       = {Yawen Zheng and Fan Dang and Zihao Yang and Jinyan Jiang and Xu Wang and Lin Wang and Kebin Liu and Xinlei Chen and Yunhao Liu},
  doi          = {10.1109/TMC.2025.3602221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BlueKey: Exploiting bluetooth low energy for enhanced physical-layer key generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leverage the duty ratio of frequency-shift wave to design a novel amplitude modulation for backscatter communications. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3602718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for ultra-low-power wireless connectivity motivates the study of backscatter communication technology. There are already some related products on the market based on excitation signals from commercial radios. However, their modulation techniques, which are the key to backscatter communications, mainly focus on the phase or frequency domain while amplitude modulation is largely ignored. Most research works either deploy one finely tuned RF impedance port for every needed reflection state or connect a nonlinear device to antenna and tune the reflection amplitude by adjusting its biasing voltage, where the former is too complex and the latter is unstable under variable incident signal power, limiting the backscatter applications. For this reason, we introduce AMscatter to leverage the duty ratio of the frequency-shift wave (FS-wave) to design a novel amplitude modulation. Both theoretical analysis and experimental results show that the reflection amplitude approximates a sinusoidal function of the duty ratio. This method requires only two fixed RF impedances, making AMscatter simple and stable. Moreover, we show how to use AMscatter to design quadrature amplitude modulation (QAM) and pulse shaping to improve backscatter communication performance. Extensive experimental results show that the throughput can be as high as 3.9 Mbps, the supported operational range can reach 20 m with 16-QAM modulation, and the out-of-band interference can be suppressed by 15 dB without negatively affecting the communication performance through our pulse shaping.},
  archive      = {J_TMC},
  author       = {Longzhi Yuan and Hangcheng Cao and Yanan Ma and Wei Gong and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3602718},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Leverage the duty ratio of frequency-shift wave to design a novel amplitude modulation for backscatter communications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uplink and downlink subband resource allocation for subband full-duplex enabled industrial intelligent manufacturing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3602872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of industrial intelligent manufacturing necessitates wireless communication systems capable of replacing conventional wired infrastructures, offering superior flexibility, scalability, and reduced maintenance overhead. While 5 G New Radio (NR) Ultra-Reliable Low-Latency Communication (uRLLC) standards (Release 15-17) have shown promise for mission-critical applications, current implementations remain constrained by their unidirectional optimization paradigm, unable to simultaneously satisfy the dual imperatives of sub-millisecond latency ($\lt 1$ ms) and 99.9999% reliability demanded by industrial control systems. To address these challenges, we present a transformative subband full-duplex (SBFD) network architecture that ensures persistent time-domain spectral availability for concurrent uplink/downlink operations, thereby eliminating direction-switching latency. Our solution introduces three key innovations: (1) an interference-aware SBFD resource allocation framework that strategically isolates UL/DL subbands to minimize cross-link interference (CLI), (2) a dual-optimization algorithm that jointly maximizes spectral efficiency while guaranteeing channel-adaptive reliability thresholds, and (3) a practical implementation scheme compatible with existing 5G NR physical layer specifications. Extensive simulations under realistic factory channel models demonstrate 58.3% reduction in aggregate CLI and 41.2% improvement in control command decoding accuracy compared to legacy half-duplex systems. This research establishes a new paradigm for wireless industrial networks, effectively closing the performance gap between 5G URLLC specifications and the exacting demands of Industry 4.0 applications.},
  archive      = {J_TMC},
  author       = {Zheng Jiang and Dingyou Ma and Bowen Wang and Ningyan Guo and Kan Yu and Qixun Zhang},
  doi          = {10.1109/TMC.2025.3602872},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Uplink and downlink subband resource allocation for subband full-duplex enabled industrial intelligent manufacturing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated distributionally robust optimization with non-convex objectives: Algorithm and analysis. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3602796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. Nevertheless, prevailing DRO techniques encounter three primary challenges in distributed environments: 1) addressing asynchronous updating efficiently; 2) leveraging the prior distribution effectively; 3) appropriately adjusting the degree of robustness based on varying scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. In addition, a new uncertainty set, i.e., constrained $D$-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. We further enhance the proposed framework by integrating various uncertainty sets and conducting a comprehensive theoretical analysis of the computational complexity associated with each uncertainty set. To expedite convergence speed, we also introduce ASPIRE-ADP, a method that can dynamically adjust the number of active workers. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity and communication complexity are also analyzed. Extensive empirical studies on real-world datasets validate that the proposed method excels not only in achieving fast convergence and robustness against data heterogeneity and malicious attacks but also in effectively managing the trade-off between robustness and performance.},
  archive      = {J_TMC},
  author       = {Yang Jiao and Kai Yang and Dongjin Song},
  doi          = {10.1109/TMC.2025.3602796},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated distributionally robust optimization with non-convex objectives: Algorithm and analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPBR: Privacy-preserving and byzantine-robust edge-assisted hierarchical federated learning in mobile networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3602911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-assisted Hierarchical Federated Learning (EHFL) accelerates global model training across mobile devices by hierarchically aggregating models. However, EHFL encounters critical challenges such as privacy risks for local and edge-level models, vulnerability to collusive Byzantine attacks, and issues with model diversity and heterogeneity due to Non-Independent and Identically Distributed (Non-IID) data. In this paper, we propose PPBR, a novel hybrid scheme that subtly integrates Condensed Local Differential Privacy (CLDP) and Packed Linearly Homomorphic Encryption (PLHE) to achieve strong privacy protection and resilience against various Byzantine attacks in Non-IID data scenarios. Specifically, PPBR clusters the sign statistics of local models and clips the norms of edge-level momenta to filter anomalous models and mitigate Byzantine faults while retaining diverse models coming from Non-IID data. To enhance privacy protection with acceptable accuracy loss, the sign tuples of local models are perturbed with CLDP guarantees, and the momenta of edge-level models are encrypted under PLHE. Meanwhile, PPBR enhances privacy in single-edge-server and single-cloud-server aggregations by using random perturbations, secret sharing, and PLHE. In addition to safeguarding privacy with accommodating abrupt dropouts of mobile devices and edge servers, the aggregations effectively mitigate the adverse effects of Non-IID data under advanced Byzantine attacks. Theoretical analysis and comprehensive experiments validate PPBR's strong privacy guarantees and resilience to various Byzantine attacks under Non-IID data.},
  archive      = {J_TMC},
  author       = {Yuanyuan He and Jie Zhang and Peng Yang and Zhe Sun and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3602911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PPBR: Privacy-preserving and byzantine-robust edge-assisted hierarchical federated learning in mobile networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multi-class novelty detection in structural vibrations with modified contrastive loss. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3603092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a framework for multi-class novelty detection using structural vibration signals. Structural vibration-based person identification is a promising soft-biometric approach with potential applications in elderly care and access control. However, current research faces two key challenges. The first challenge is the lack of large-scale datasets necessary for thorough evaluation in structural vibration gait recognition. To address this, we created a new dataset with recordings from fifty individuals. The second challenge lies in the limited exploration of deep learning methods for large-scale multi-class novelty detection in structural vibration data. To fill this gap, we propose the energy-shifted contrastive loss function, specifically designed for this task. Our results demonstrate that the proposed framework achieves 96.57% accuracy in multi-class classification. For novelty detection, it achieves an Receiver Operating Characteristic-Area Under the Curve (ROC-AUC) score of 89.15% for single footsteps, which improves to 93.83% with five consecutive footsteps.},
  archive      = {J_TMC},
  author       = {Mainak Chakraborty and Chandan and Bodhibrata Mukhopadhyay and Subrat Kar},
  doi          = {10.1109/TMC.2025.3603092},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep multi-class novelty detection in structural vibrations with modified contrastive loss},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the optimal path and DNN partition for collaborative edge inference. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3602966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Neural Networks (DNNs) have enabled a wide range of intelligent mobile applications, but their computational demands pose challenges for resource-constrained devices. Collaborative edge inference addresses this by partitioning a DNN inference task into several subtasks across multiple network nodes. However, most existing methods assume known network parameters or fixed processing paths. In this paper, we consider a more complex setting where network parameters are unknown and multiple paths are available. The goal is to learn both the optimal path and the DNN layer assignment along it, while accounting for security threats and path-switching costs. We first derive structural insights under full information to reduce the decision space. We then formulate the learning problem as an adversarial group linear bandit with switching costs, where rewards follow a hybrid stochastic-adversarial process. To solve this, we propose B-EXPUCB, a new algorithm that combines ideas from blocked EXP3 and LinUCB, and show that it achieves sublinear regret. Extensive simulations demonstrate that B-EXPUCB outperforms existing methods in collaborative edge inference.},
  archive      = {J_TMC},
  author       = {Yin Huang and Letian Zhang and Jie Xu},
  doi          = {10.1109/TMC.2025.3602966},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Learning the optimal path and DNN partition for collaborative edge inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed optimization of task offloading and resource allocation for mobile edge computing with multifactorial uncertainty. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3603064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for computation-intensive and lowlatency services grows, mobile edge computing (MEC) has been widely applied in smart devices to provide efficient and real-time assistance. However, most existing studies impose fixed assumptions and lack consideration for the uncertainty within MEC. This makes it difficult for these studies to reasonably offload tasks in complex and highly volatile scenarios. Therefore, we construct an MEC task offloading system considering multifactorial uncertainty (MECTOS-MU), which involves multiple devices and MEC servers (MSs). In MECTOS-MU, task offloading and resource allocation are jointly optimized while complying with the constraint on latency to minimize the energy consumption of all devices, which is an NP-hard problem. To address this issue, we propose a novel algorithm called distributed game offloading based on load balancing (DGOLB). This method integrates task offloading prioritization, static game theory, and load balancing to formulate efficient task offloading decisions and resource allocation schemes. Extensive simulation results demonstrate that DGOLB outperforms other baseline algorithms in terms of energy consumption, ratio of dropped tasks, and average task response time, especially in scenarios with a large number of devices.},
  archive      = {J_TMC},
  author       = {Bin Xu and Honggen Bian and Qiulan Cui and Xiaohui Yu and Jin Qi and Yimu Ji},
  doi          = {10.1109/TMC.2025.3603064},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed optimization of task offloading and resource allocation for mobile edge computing with multifactorial uncertainty},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AoI-aware incentive mechanism for UAV-assisted mobile crowdsensing: A contract-theoretic approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3604073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of mobile devices, mobile crowdsensing (MCS) has become a paradigm with broad application prospects. However, traditional MCS face numerous challenges, such as surges in network traffic and infrastructure failures. To address these issues, we leverage flexible and low-cost Unmanned Aerial Vehicles (UAVs) in the MCS framework. UAV-assisted crowdsensing (UCS) provides an innovative approach to data collection that effectively addresses problems such as insufficient network coverage and congestion. In the UCS framework, UAVs can serve not only as temporary base stations (BSs) but also participate in collecting data and processing tasks. Nevertheless, the lack of adequate incentive mechanisms may lead both UAVs and mobile users to be reluctant to participate in sensing tasks. Therefore, this paper aims to investigate hierarchical incentive mechanisms for UCS. Considering the freshness of the collected data and the benefits of the platform, we adopt the Age of Information (AoI) metric to measure the quality of data. To ensure AoI of data, we model the incentive mechanisms from both the UAV and user perspectives, and we formulate them as single-dimensional and multi-dimensional contract-based incentives under scenarios of information asymmetry. Furthermore, we derive the optimal contract scheme under the constraints of individual rationality and incentive compatibility. Finally, experimental results confirm the effectiveness of the proposed contract design and maximize the utility of the model owner.},
  archive      = {J_TMC},
  author       = {Yuran Guo and Ying Chen and Hongtao Li and Yuan Wu and Jiwei Huang},
  doi          = {10.1109/TMC.2025.3604073},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AoI-aware incentive mechanism for UAV-assisted mobile crowdsensing: A contract-theoretic approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning assisted detection of anomalous events with insufficient primary attribute data samples in MEC networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3604253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays IoT devices in Mobile Edge Computing (MEC) networks have been deployed in large-scale quantities to guarantee sensing data collection for anomalous event detection as full as possible even if some devices are in fault. Some techniques, such as clustering and dimensionality reduction, are adopted to eliminate redundant sensing data collection in this large-scale deployment. However, they not only have high computational complexity and easily cause the loss of information on the primary sensing attributes for detection, but also bring certain errors to the detection because of their low sensitivity to data processed. In addition, insufficient collection of primary attribute data samples often results from physical or human factors, and blind imputation of large-scale data gaps without basis may lead to greater irreparable losses. To address the above challenges, we first complete the selection of optimal primary attribute device collection and aggregation (PADCA) path based on minimum spanning tree, reducing data communication cost for redundant primary attributes collection. Then, we propose an anomalous impact correlation search strategy to quickly locate all MEC servers whose management regions have cascading anomalous event and help determine the transferable source MEC servers. Leveraging this, we use transfer learning to help detect anomalous events in the management regions of the MEC servers with insufficient primary attribute data samples, where a particle swarm optimization based back-propagation (PSO-BP) neural network model is used to infer the fusion weight of each primary attribute. Experimental results show that our method achieves higher detection performance in terms of detection time, energy consumption, accuracy, and receiver operating characteristic (ROC) curve compared to the benchmarks by at least 24%, 34%, 0.5 and 0.05.},
  archive      = {J_TMC},
  author       = {Jine Tang and Xiaotong Ma and Song Yang and Yong Xiang and Zhangbing Zhou},
  doi          = {10.1109/TMC.2025.3604253},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Transfer learning assisted detection of anomalous events with insufficient primary attribute data samples in MEC networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BatTera: Non-destructive lithium-ion battery coating measurement with terahertz. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3604081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrode coating measurement is a crucial task in practical lithium-ion battery systems, where the thickness and refractive index of the electrode coating directly reflect the battery's quality, energy density, capacity, and lifespan. In this paper, we propose the design, implementation, and evaluation of BatTera, the practical system for an accurate, high-resolution, non-destructive, and safe electrode coating measurement, with the ability to simultaneously measure coating thickness and refractive index. BatTera's contributions are twofold. Firstly, we build a comprehensive mathematical model that characterizes the arrival time of echo signals from both sides of the electrode coating by thoroughly analyzing the electrode structure based on “coating-foil-coating”. This model serves as a theoretical foundation guiding the measurement of coating thickness and refractive index. Secondly, we propose a series of effective signal-processing algorithms to address the practical challenges of double-side coating misalignment and deformation interference, thus adaptive improving the signal-to-noise ratio of Terahertz signals and pushing BatTera one big step closer to real adoptions. We implement BatTera based on the commercial Terahertz device QT-TO1000 and conduct extensive experiments using five types of cathode electrode samples in three different sizes, collected from one of the world's largest new energy battery manufacturers. The results show that BatTera achieves high measurement accuracy with a mean average error of 6.106$\upmu$m for thickness and 0.230 for refractive index.},
  archive      = {J_TMC},
  author       = {Long Tan and Xu Chen and Xiuzhen Guo and Xinghua Guo and Yuanchao Shu and Jiming Chen},
  doi          = {10.1109/TMC.2025.3604081},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BatTera: Non-destructive lithium-ion battery coating measurement with terahertz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling reliable and anonymous data collection for fog-assisted mobile crowdsensing with malicious user detection. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3602659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid developments of mobile devices and fog computing have facilitated the data collection paradigm of fog-assisted mobile crowdsensing, providing great convenience for individuals with limited resources to collect large-scale data. However, the openness of crowdsensing network and the untrusted behaviors of some task participants raise concerns regarding participants' privacy and data reliability. Previous works mostly focus on preserving the privacy of task participants and often overlook the issue of data reliability in the presence of dishonest participants. In this paper, we propose a new data collection scheme tailored for fog-assisted mobile crowdsensing. It enables the cloud to detect invalid sensing data in the ciphertext domain, simultaneously ensuring data confidentiality and reliability. Additionally, our scheme is designed to protect the anonymity of honest task participants while guaranteeing the traceability of dishonest participant once invalid data are detected. Formal analysis is provided to prove the correctness and security of our scheme. Furthermore, we implement our scheme to evaluate its performance, and the experimental results demonstrate that it can achieve the aforementioned security properties with modest performance overhead.},
  archive      = {J_TMC},
  author       = {Mingyang Song and Zhongyun Hua and Yifeng Zheng and Rushi Lan and Qing Liao and Guoai Xu},
  doi          = {10.1109/TMC.2025.3602659},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling reliable and anonymous data collection for fog-assisted mobile crowdsensing with malicious user detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3585033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables edge devices to collaboratively train a global model using local data. However, the increasing prevalence of watermarks in datasets presents a new challenge to efficient FL. While watermarks assert data ownership and copyright, they introduce complexities that can lead to shortcut learning problems and mislead utility measurements for client selection. These issues are further exacerbated by batch size variations in efficient FL frameworks, ultimately undermining their time-to-accuracy performance. We introduce LotusFL, an FL system designed to address the challenges posed by watermarked datasets in efficient FL. Specifically, it tackles the increased time-to-accuracy due to erroneous client selection and the accuracy degradation observed with larger batch sizes. LotusFL first estimates the characteristics of watermarks through statistical estimation and then adjusts the batch size using this estimated watermark information to balance the negative impact of the watermark against device idle waiting time. Additionally, its client selection mechanism, based on historical information, avoids the misleading utility signals from watermarks. This mechanism, working in conjunction with batch size adjustment, aims to accurately predict device runtime and identify potentially valuable devices. We evaluated LotusFL through a real-world deployment on 40 edge devices. Compared to state-of-the-art efficient FL frameworks, LotusFL achieves superior performance, enhancing accuracy by up to 8.2% and reducing training time by 1.97×.},
  archive      = {J_TMC},
  author       = {Tao Ling and Siping Shi and Hao Wang and Chuang Hu and Dan Wang},
  doi          = {10.1109/TMC.2025.3585033},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-aware neural radio-frequency radiance fields. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3583580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Maxwell discovered the physical laws of electromagnetic waves about 160 years ago, accurately modeling the propagation of RF signals in large and complex electrical environments remains a persistent challenge. This complexity arises from the interactions between the RF signal and various obstacles, including reflection and diffraction. Inspired by the success of neural networks in mapping the optical field in computer vision, we introduce the neural radio-frequency radiance field, or $\mathbf{NeRF}^{2}$. This represents a continuous volumetric scene function that effectively models RF signal propagation. Remarkably, after only a sparse amount of training with signal measurements, $\mathbf{NeRF}^{2}$ can accurately predict the nature and origin of signals received at any location, assuming the transmitter's position is known. Additionally, we propose the frequency-aware $\mathbf{NeRF}^{2}$ to enhance channel prediction performance for wideband signals using an RF prism module. Compared to the vanilla $\mathbf{NeRF}^{2}$, the frequency-aware $\mathbf{NeRF}^{2}$ achieves a 4 dB improvement in SNR for FDD OFDM channel estimation and is nearly 3.5 × faster. Functioning as a physical-layer neural network, $\mathbf{NeRF}^{2}$ also supports application-layer artificial neural networks (ANNs) by generating synthetic training datasets. Our empirical results demonstrate that augmented sensing enhances the accuracy of AoA estimation, achieving an approximate 50% improvement.},
  archive      = {J_TMC},
  author       = {Xiaopeng Zhao and Zhenlin An and Qingrui Pan and Lei Yang},
  doi          = {10.1109/TMC.2025.3583580},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Frequency-aware neural radio-frequency radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3586259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the advantages of device ubiquity, natural interaction and privacy preservation, acoustic-based gesture input has received widespread attention. Researchers have proposed various techniques for different applications. However, the existing work has shortcomings of heavy data-collection overhead, non-continuous input, and performance degradation in crossuser scenarios. To overcome these shortcomings, we propose UltraWrite, an acoustic-based gesture input system that only needs extremely low data-collection overhead, supports continuous input, and achieves high cross-user recognition accuracy. The key idea of our solution is to synthesize training data of continuous gestures from isolated ones, build a lightweight continuous gesture recognition model based on connectionist temporal classification (CTC) mechanism, and design a novel decoupled model training strategy to improve its cross-user recognition capability. We have implemented prototype systems on commercial devices and conducted comprehensive experiments to evaluate their performance. The results show that UltraWrite achieves an average top-1 word accuracy of 99.3% and top-1 word error rate of 0.34%. In addition, we have also evaluated UltraWrite's robustness to the sensing distance, angle, background noise, and device. The results reveal that UltraWrite possesses strong robustness to these factors.},
  archive      = {J_TMC},
  author       = {Yongpan Zou and Weiyu Chen and Yunshu Wang and Canlin Zheng and Wenfeng He and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3586259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robust topology recovery of UAV swarm for detection and localization of electronic signals. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, Unmanned Aerial Vehicle (UAV) swarm has been extensively applied in various fields. In the application of detection and localization of electronic signals, some UAVs could become disabled due to some abnormal events (e.g. electromagnetic interference and battery electricity exhaustion), and the topology connectivity of UAV swarm could be impaired, i.e., the topology of UAV swarm could be partitioned. For the topology recovery issue, we first propose Robust Topology Recovery Algorithm of UAV swarm (RTRA) to recover the topology connectivity of UAV swarm and enhance the topology robustness (reduce the number of potential topology recoveries in future) by relocating some UAVs to new positions with shortest flight distance. Furthermore, we note that the relocated UAVs are easy to exhaust the battery electricity and fail due to the extra flight movements for the topology recoveries, which affects the topology robustness. To this end, we present Cascading Robust Recovery Topology Algorithm of UAV swarm (CRTRA), which adopts a cascading movement strategy to share the flight movements among multiply relocated UAVs, thus avoiding the battery electricity exhaustion of the relocated UAVs. Extensive simulations and comparisons demonstrate that our proposed CRTRA can effectively recover the topology connectivity of UAV swarm while enhancing the topology robustness and shortening the flight distance of relocated UAVs, and CRTRA is especially suitable for some missions such as the detection and localization of electronic signals where UAVs are prone to fail.},
  archive      = {J_TMC},
  author       = {Linfeng Liu and Wenzhe Zhang and Xingyu Li and Jia Xu},
  doi          = {10.1109/TMC.2025.3586447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On the robust topology recovery of UAV swarm for detection and localization of electronic signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3586262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth orbit satellite networks (LSNs) are envisioned as key enablers of 6 G by offering ubiquitous, low-latency connectivity. Their mesh topology enables multipath differential routing, which improves bandwidth utilization and reduces transmission delay. However, the growing demand for data and the dynamic, self-organizing nature of LSNs pose significant challenges for joint multipath routing and traffic scheduling under strict latency and energy constraints. To address these challenges, this paper proposes a multipath routing optimization (MRO) and traffic scheduling method tailored for multipath differential routing. Specifically, a dynamic multi-attribute graph model is developed to precisely capture the dynamic properties of LSNs. Building on this model, a MRO algorithm, integrated with a Stackelberg game framework, is introduced. The MRO algorithm employs a decomposition-based approach to identify multiple optimal paths that minimize delay and energy consumption, while the Stackelberg game framework ensures efficient traffic distribution across these paths. Numerical results demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving cumulative reward improvements of 26.77% to 43.8% across four real-world network topologies and exhibiting better Pareto front coverage. Furthermore, by leveraging the rapid convergence properties of the Stackelberg game model, the proposed method enhances network throughput by 12% to 43% and reduces transmission time by 14% to 49%.},
  archive      = {J_TMC},
  author       = {Shuyang Li and Qiang Wu and Ran Wang and Long Chen and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3586262},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3586429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile crowd sensing systems, existing flight control methods enable unmanned aerial vehicles (UAVs) to provide high-quality data collection services for various applications. However, due to limited communication range, UAVs typically collect data under partial observability, hindering optimal performance without global environmental information. Additionally, many methods fail to enforce critical safety constraints. This paper proposes a communication-assisted safe multi-agent actor-critic-based UAV flight control method (CSMAAC). First, we propose an independent prediction communication partner model to address the partial observability problem. Based on the UAV's local observation, causal inference is used to obtain prior communication information between UAVs through a feed-forward neural network to help UAVs determine potential communication partners. Second, we utilize a critic-network to predict and quantify inter-UAV influence and determine the necessity of communication. By exchanging necessary information inter-UAV, UAVs can perceive global information, thereby solving the UAV's partial observability problem and reducing communication overhead. Moreover, we propose a similarity enhancement mechanism to improve the learning efficiency of the model by enhancing the connection between UAV observations and the policies of other UAVs. Finally, we introduce a safety layer to Actor-Network to ensure safe UAV flight. The simulation results show that the proposed method outperforms the baselines.},
  archive      = {J_TMC},
  author       = {Zhen Gao and Gang Wang and Lei Yang and Chenhao Ying},
  doi          = {10.1109/TMC.2025.3586429},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FednP: Federated unlearning with multiple client set partitions. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3586441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has garnered increased attention in the field of distributed machine learning and privacy computing. In the FL setup, effective and efficient unlearning algorithms are required to remove the impact of specific training data from the trained model, called federated unlearning. However, traditional machine unlearning algorithms face limitations in FL systems because the client data is private and even non-IID. In this paper, we propose a new federated unlearning algorithm called FednP. Our approach involves dividing the client set into subsets using multiple different partitions. We then train constituent models for each client subset within these partitions using existing FL algorithms and aggregate the results of constituent models for predictions. With multiple partitions, FednP limits the influence of the data to be erased within its belonging subsets, while it also improves the accuracy of the aggregated prediction. Based on the multiple-partition framework, we design partition creation methods to effectively enhance the prediction accuracy. Furthermore, we propose a cost reduction method to reduce the cost of training/retraining. Our extensive experiments on various datasets and model architectures demonstrate that FednP improves prediction accuracy while well-controls the additional cost.},
  archive      = {J_TMC},
  author       = {Juncheng Jia and Weipeng Zhu and Bing Luo and Xiaodong Lin and Liuchen Ma},
  doi          = {10.1109/TMC.2025.3586441},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FednP: Federated unlearning with multiple client set partitions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical optimizing UAV trajectory in wireless charging networks: An approximated approach. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3586457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) can be easily deployed as auxiliary base stations due to their convenience and flexibility. However, limited battery capacity becomes a bottleneck. Promising wireless power transfer (WPT) technologies can provide a continuous power supply for UAVs. Many of the recent works treat the UAV battery capacity as a constraint, which hinders the assurance of continuous UAV operation. Furthermore, most studies employ intelligent path-planning algorithms that lack explicit performance guarantees. In this paper, we study the problem of Practical Optimizing UAV Trajectory in Wireless Charging Networks (POTWCN), which involves planning the trajectory of the wireless-powered UAV in the practical environment with obstacles by selecting candidate passing positions and determining the access order in the charging network. The goal is to maximize the benefit, i.e., balancing the total task completion time and the number of charging stations visited, so as to minimize path length and flight time, and ensure energy constraints with performance bound. To solve this problem, we first formalize the problem and prove its submodularity. Then, we propose the obstacle-aware weighted graph generation algorithm (OWGGA) to deal with the obstacles in the environment, which forms an obstacle-avoidance path using tangents and arcs between two hovering positions and the blocking obstacles. Next, we propose a dynamic charging station selection algorithm (ACSA), which maximizes the UAV's energy utilization by limiting the number of charging stations that can be included. In the algorithm, we introduce the Christofides algorithm and use the path length calculated by OWGGA as the edge weights of the graph. Subsequently, considering the UAV's energy constraints, we iteratively solve the UAV trajectory planning problem by adding the charging station with a maximized marginal benefit to the path. We prove that the proposed algorithm achieves an approximation ratio 1 – 1/e as well as the path length is at most $3\pi /4$ times the optimal solution. change Simulation results show that our algorithm reduces the flight distance by 38.01% and the task completion time by 34.00% on average.},
  archive      = {J_TMC},
  author       = {Yundi Wang and Xiaoyu Wang and He Huang and Haipeng Dai},
  doi          = {10.1109/TMC.2025.3586457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Practical optimizing UAV trajectory in wireless charging networks: An approximated approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust contract theory for edge AIGC services in teleoperation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3586606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, we employ contract theory to model information asymmetry while utilizing DRO to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unitybased teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7% to 10.74% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory. The code and data are publicly available at https://github.com/Zijun0819/DROContract-Theory},
  archive      = {J_TMC},
  author       = {Zijun Zhan and Yaxian Dong and Daniel Mawunyo Doe and Yuqing Hu and Shuai Li and Shaohua Cao and Lei Fan and Zhu Han},
  doi          = {10.1109/TMC.2025.3586606},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributionally robust contract theory for edge AIGC services in teleoperation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On information collection in multi-tagged COTS RFID systems. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3586660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of target object information collection in multi-tagged COTS RFID systems. Unlike its singletagged peers, the multi-tagged COTS RFID scenario poses new challenges in devising information collection algorithms: 1) Tags attached to the same object carry identical information. Hence, reusing single-tagged information collection algorithms leads to unnecessary redundancy; 2) Multi-tagged RFID systems are often deployed in applications where tags are vulnerable to damage. Such faulty tags may severely degrade the performance of information collection; 3) Most state-of-the-art information collection algorithms rely heavily on the hashing operation that is not seamlessly supported by the C1G2 standard, rendering these solutions inefficient and impractical, especially in largescale RFID systems. To tackle these technical challenges, this paper makes three contributions. First, we develop an efficient and compact tag pseudo-ID design, enabling the reader to select a single tag from each target object to collect information with only one SELECT command. Second, we construct a robust faulthandling mechanism capable of recognizing faulty tags without executing the entire slot. Third, armed with the above two techniques, we develop a novel information collection algorithm by leveraging the functionality offered by C1G2 to optimize the information collection sequence, thus minimizing the overall execution time. Empirical experiments on a COTS RFID system prototype demonstrate that our algorithm outperforms the best existing solution by 35-50% on average.},
  archive      = {J_TMC},
  author       = {Kanghuai Liu and Jihong Yu and Lin Chen},
  doi          = {10.1109/TMC.2025.3586660},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On information collection in multi-tagged COTS RFID systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression detection via wearable Electroencephalogram (EEG) sensor-assisted diagnosis system demands computationally efficient models compatible with resource-constrained edge devices. Spiking Neural Networks (SNNs) offer inherent advantages for processing the spatio-temporal patterns of EEG through event-driven manner. In this study, we innovatively present LSNNet, a lightweight SNN model specifically designed for wearable EEG sensors. The model exhibits low computational complexity with 7.18 K parameters and 67.68 M Floating-Point Operations (FLOPs). It requires only 246.88 KB of Random Access Memory (RAM) and 57.33 KB of Read-Only Memory (ROM) for on-board execution, and has been validated on both the single-core STM32U535CET6 and the multi-core GAP8 microcontrollers. Despite its minimal computational and memory requirements, LSNNet achieves impressive performance metrics, with a classification accuracy of 89.2%, specificity of 92.4%, and sensitivity of 86.4% in independent tests conducted on EEG data collected from 73 depressed patients and 108 healthy controls using our three-lead EEG sensor. Especially, when running on the GAP8 microcontrollers, the LSNNet model has a low power consumption of 21.43 mW and a satisfactory inference time of 0.63 s while maintaining a classification accuracy of 87.5% (only with a reduction of 1.98%). These results underscore the potential of integrating wearable EEG sensors with the LSNNet model for depression detection in the Internet of Things (IoT) era.},
  archive      = {J_TMC},
  author       = {Qinglin Zhao and Lixin Zhang and Haojie Zhang and Hua Jiang and Kunbo Cui and Zhongqing Wu and Jingyu Liu and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TMC.2025.3586591},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct acyclic graph (DAG)-based ledgers and distributed consensus algorithms have been proposed for use in the Internet of Things (IoT). The DAG-based ledgers have many advantages over single-chain blockchains, such as low resource consumption, low transaction fee, high transaction throughput, and short confirmation delay. However, the scalability of the DAG consensus has not been comprehensively verified on a large scale. This paper explores the scalability of DAG consensus within the low-to-high load regime (L2HR) using the tangle model, where L2HR characterizes the transition from a phase of low network load to another phase of high network load. In particular, we determine the average number of tips in the tangle in L2HR when adopting the uniform random tip selection (URTS) and rigorously prove that using the tangle model, the average number of tips at the end of L2HR converges to a constant. We also analyze the probability that a transaction in L2HR becomes an abandoned tip, the approximate average time required for the network load to transition from low load regime (LR) to high load regime (HR), and the average time required for a tip being approved for the first time in L2HR. All analytics are verified by numerical simulations.},
  archive      = {J_TMC},
  author       = {Qingwen Wei and Shuping Dang and Zhihui Ge and Xiangcheng Li and Zhenrong Zhang},
  doi          = {10.1109/TMC.2025.3586668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3586623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has become an effective paradigm to support computation-intensive applications by providing services in close proximity to user devices (UDs). In MEC networks, computation offloading technology is devoted to balancing system load and prolonging UDs' battery life. However, most existing studies on computation offloading take the impractical assumption of the MEC scenario with homogeneous users, ignoring security requirement from certain users. Moreover, with users mobility and task arrivals correlation, most existing computing offloading approaches suffer from inefficient or suboptimal decision making in practical MEC environments. To tackle these issues, by integrating task arrivals correlation within a time slot and environment dynamics between time slots, we propose an adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes. First, considering additional security requirement from very important people (VIP) users, we present a novel collaborative architecture by separating edge/cloud servers into public and private nodes. Then, with the architecture, we develop a dynamic computation offloading (DCO) algorithm to realize adaptive computation offloading scheme in MEC environment with mobile users. Particularly, the algorithm involves three stages. 1) By extending Poisson process into Markovian arrival process (MAP), we construct an MAP-based system model to capture the behavior of time-dependent task arrivals and then analyze the system model to derive the system delay in steady state. 2) For the purpose of minimizing the system delay in each time slot, we formulate a computation offloading problem in MEC environment with mobile users. 3) Under a deep reinforcement learning (DRL) framework, by taking the system delay as environmental feedback, we solve the formulated problem and provide offloading decisions in each time slot. We evaluate the performance of DCO algorithm by comparing it with other benchmark algorithms in various application scenarios. Results demonstrate that the proposed DCO algorithm outperforms the compared algorithms in response performance.},
  archive      = {J_TMC},
  author       = {Haixing Wu and Jiameng Zheng and Shunfu Jin},
  doi          = {10.1109/TMC.2025.3586623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling effective OOD detection via plug-and-play network for mobile visual applications. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices have increasingly integrated with numerous deep learning-based visual applications, such as object classification and recognition models. While these models perform well in controlled environments, their effectiveness declines in real-world environment due to out-of-distribution (OOD) data not seen during training. Existing methods for detecting OOD data often compromise normal data recognition and require extensive training on unattainable OOD data. To address these issues, we propose $\mathtt {POD}$, a framework designed to enhance mobile visual applications by providing high-precision OOD detection without affecting original model performance. In the offline phase, $\mathtt {POD}$ generates OOD detectors from any classification model by analyzing model's neuron responses to various data types. In the online phase, it continuously adjusts decision boundaries by integrating results from both the original model and the detector. Evaluated on two public datasets and one self-collected dataset across various popular classification models, $\mathtt {POD}$ significantly improves OOD detection performance while maintaining the accuracy of original models.},
  archive      = {J_TMC},
  author       = {Zixiao Wang and Qi Dong and Tianzhang Xing and Zhidan Liu and Zhenjiang Li and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3586625},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling effective OOD detection via plug-and-play network for mobile visual applications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classic data processing tool, Principal Component Analysis (PCA) has been widely applied in various data analysis applications. To mitigate the high computational complexity of PCA on big data, distributed PCA methods have been extensively studied, which disperse the computational tasks across multiple computation units while guaranteeing the accuracy. For the scenarios of distributed PCA in wireless networks, as the data is originally dispersed across different locations, it is further required to reduce the communication cost of distributed PCA in networks, which however has been seldom studied. Reducing the communication cost of distributed PCA in wireless networks requires not only appropriately partitioning the computation of PCA, ensuring accuracy, but also effectively assigning the partitioned computations and routing strategies to the nodes. In this paper, we propose CD-PCA, a communication-efficient distributed PCA (CD-PCA) scheme. This scheme implements a transmission-benefit equipartition strategy for the network to facilitate high-accuracy distributed computation and designs novel routing strategies for nodes to execute the distributed PCA within each partitioned region. Extensive simulation results demonstrate that the proposed CD-PCA scheme can reduce transmission costs by over 30% on average compared to related methods and baseline approaches.},
  archive      = {J_TMC},
  author       = {Yiyi Zhang and Peng Guo and Xuefeng Liu and Chao Cai and Kui Zhang and Jiang Liu},
  doi          = {10.1109/TMC.2025.3586615},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-splitting multiple access (RSMA), space division multiple access (SDMA), and non-orthogonal multiple access (NOMA) have gained significant popularity and are extensively utilized across various domains. However, it is still unclear whether hybrid RSMA-SDMA-NOMA (HybridRDN) would seamlessly combine the advantages of RSMA, SDMA, and NOMA to contribute to the computation offloading of autonomous vehicle systems. To address the above issue, this paper introduces a novel HybridRDN-assisted computation offloading fleet (COF) scheme tailored for autonomous vehicle systems. First, we propose a stochastic-geometry-aided method to model the offloading framework. Afterwards, the task vehicles (TVs) ingeniously employ the proposed HybridRDN scheme to offload tasks to the resource vehicles (RVs) in each COF to relieve their computational burden. Diverging from the sole optimization of the task segmentation ratio or the transmission rate, a joint optimization problem involving the transmission weighting factor, the HybridRDN precoding matrix, the common rate, and the task segmentation ratio, is formulated, which aims to minimize the average delay of the COF system while approaching the rate performance of the ideal HybridRDN. Furthermore, a delay-optimal alternating optimization algorithm (DOAOA) is developed to obtain the solution for the optimization problem. Experimental results validate the plausibility and superiority of the proposed framework compared to the state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Zhijian Lin and Yang Xiao and Yi Fang and Hongbing Chen and Xiaoqiang Lu},
  doi          = {10.1109/TMC.2025.3586638},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range-free localization algorithms have attracted considerable attention for outdoor wireless sensor network (WSN) positioning because they are less susceptible to environmental factors when estimating inter node distances and require only a few beacon nodes with known locations to rapidly determine all node positions. Among these, the connectivity based DV Hop algorithm has become widely used due to its simplicity and ease of implementation. However, its localization accuracy is limited and it is easily degraded by non uniform node distributions and obstacle environments. To address these shortcomings, this paper proposes a novel range free localization algorithm (RF-DEGO). First, a new distance estimation formula is derived from node connectivity and the probability distribution of distances. Next, the estimated distances are corrected using the local node density along communication paths, and paths identified as detouring around obstacles receive a further correction. Finally, an enhanced hierarchical Grey Wolf Optimization algorithm computes the node positions. Extensive simulation experiments under various network scenarios and parameter settings show that the proposed algorithm outperforms several existing localization methods in both accuracy and computation time, demonstrating superior overall performance and strong competitiveness.},
  archive      = {J_TMC},
  author       = {Haibin Sun and Yongzheng Zhang},
  doi          = {10.1109/TMC.2025.3586636},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, scalable video coding (SVC) has gained significant recognition in mobile video streaming because it can adapt bitstreams to time-varying transmission conditions. However, the coding performance of SVC, which is determined by its coding structure, has not been thoroughly studied. To address this issue, we propose analyzing the redundancy, reduction, distortion, and mutuality of video information within the video coding processes. This analysis facilitates the development of a novel information-theoretical framework for quantifying coding performance, which includes an information theory (IT)-based quantification method and a graphical representation system. The representation system accurately delineates the coding reference structure for encoding each video frame, while the proposed method utilizes mutual information to quantify the achievable coding performance of SVC under the delineated structure. To demonstrate the significance of our research, we apply the proposed framework to encode a basic coding unit, showcasing its effectiveness in improving SVC schemes. Consequently, our framework not only provides an efficient approach for quantifying the coding performance of SVC but also serves as an invaluable tool for optimizing SVC in various applications.},
  archive      = {J_TMC},
  author       = {Weijia Han and Chuan Huang and Yanjie Dong and Yangyingzi Zhang and Yuxiang Yue and Wei Teng},
  doi          = {10.1109/TMC.2025.3586587},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive parameter-efficient federated fine-tuning on heterogeneous devices. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3586644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8× and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.},
  archive      = {J_TMC},
  author       = {Jun Liu and Yunming Liao and Hongli Xu and Yang Xu and Jianchun Liu and Chen Qian},
  doi          = {10.1109/TMC.2025.3586644},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive parameter-efficient federated fine-tuning on heterogeneous devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing a pedestrian's conveyor state of “elevator,” “escalator,” or “neither” is fundamental to many applications such as indoor navigation and people flow management. Previous studies on classifying the conveyor state often rely on specially designed body-worn sensors or make strong assumptions on pedestrian behaviors, which greatly strangles their deployability. To overcome this, we study the classification problem under arbitrary pedestrian behaviors using the inertial navigation system (INS) of the commonly available smartphones (including accelerometer, gyroscope, and magnetometer). This problem is challenging, because the INS signals of the conveyor states are entangled by the arbitrary and diverse pedestrian behaviors. We propose ELESON, a novel and lightweight deep-learning approach that uses phone INS to classify a pedestrian to elevator, escalator, or neither. Using causal decomposition and adversarial learning, ELESON extracts the motion and magnetic features of conveyor state independent of pedestrian behavior, based on which it estimates the state confidence by means of an evidential classifier. We curate a large and diverse dataset with 36,420 instances of pedestrians randomly taking elevators and escalators under arbitrary unknown behaviors. Our extensive experiments show that ELESON is robust against pedestrian behavior, achieving a high accuracy of over 0.9 in F1 score, strong confidence discriminability of 0.81 in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements fit for common smartphone deployment.},
  archive      = {J_TMC},
  author       = {Tianlang He and Zhiqiu Xia and S.-H. Gary Chan},
  doi          = {10.1109/TMC.2025.3586618},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3586797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) offers users immersive experiences to interact with digital contents in their physical space. However, practical AR applications are challenged by the tight coupling of algorithm and engineering during the development and deployment phases as well as the execution requirements of hybrid AR subtasks on heterogeneous and resource-constraint mobile devices. In this work, we build an end-to-end, cross-platform, and efficient AR system, called ARSys. The infrastructure in ARSys adopts the new principle of integrated design, unifies and refines AR fundamental capabilities, supports streaming media processing, model inference, and real-time rendering by exposing high-performance tensor compute engine to top, and constructs a Python multi-instance virtual machine as the cross-platform AR task execution container. The runtime mechanism of ARSys schedules AR tasks in a pipeline parallelism way and allocates subtasks to hardware backends by optimizing the slowest node. The development workbench and the deployment platform in ARSys allow the decoupling of algorithms written in Python from engineering components in C/C++ and further support remote debugging and quick validation of AR algorithms. We extensively evaluate ARSys in practical AR applications across high-end, mid-end, and low-end Android and iOS devices, demonstrating higher development, deployment, and runtime efficiency than existing MediaPipe-oriented framework. ARSys has been integrated into Mobile Taobao for production use.},
  archive      = {J_TMC},
  author       = {Chengfei Lv and Chaoyue Niu and Yu Cai and Xiaotang Jiang and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3586797},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achievable rate maximization for multi-IRS assisted UAV-NOMA networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution towards Internet of Things (IoT) in the forthcoming sixth generation (6 G) is facing massive amounts of transmitted data and harsh wireless transmission environment, which severely degrade the quality of communication. To overcome these difficulties, a novel multiple intelligent reflecting surfaces (IRSs) assisted unmanned aerial vehicle (UAV) network framework with non-orthogonal multiple access (NOMA) is proposed in this article, where the UAV applies the NOMA scheme to deliver the information to the ground users assisted by multiple IRSs. We aim to maximize the achievable rate of the considered network while guaranteeing the minimum communication rate of each user, by jointly optimizing the multi-IRS phase shifts, UAV transmit power, UAV trajectory, and NOMA decoding order. To handle the coupled variables and integer constraints, we decompose the original problem into three subproblems based on the block coordinate descent (BCD) framework. Specifically, we first obtain the multi-IRS phase shifts by applying the semidefinite relaxation (SDR) technique. Next, the UAV transmit power allocation is derived by exploiting the concave convex procedure (CCCP) method. The UAV trajectory and NOMA decoding order are finally obtained by invoking the penalty-based method and the successive convex approximation (SCA) technique. Based on these, an alternating optimization algorithm is proposed. The numerical results show that: 1) the NOMA scheme enhances the utilization of the spectrum and enhances the access capacity of the communication system; 2) the multi-IRS cooperative structure increases the reflective channels and effectively improves the air-ground transmission environment, thus enhancing the system achievable rate; 3) the proposed multi-IRS assisted UAV NOMA algorithm achieves a significant network rate improvement compared to other benchmark schemes.},
  archive      = {J_TMC},
  author       = {Dingcheng Yang and Kangqing Wu and Yu Xu and Fahui Wu and Tiankui Zhang},
  doi          = {10.1109/TMC.2025.3586768},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Achievable rate maximization for multi-IRS assisted UAV-NOMA networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHandover: Parallel handover in mobile satellite network. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3582245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of Low Earth Orbit satellite constellations has recently spurred tremendous attention from both academia and industry. 5G and 6G standards have specified the LEO satellite network as a key component of the mobile network. However, due to the satellites' fast traveling speed, ground terminals usually experience frequent and high-latency handover, which significantly deteriorates the performance of latencysensitive applications. To address this challenge, we propose a parallel handover mechanism for the mobile satellite network which can considerably reduce the handover latency. The main idea is to use plan-based handovers instead of measurementbased handovers to avoid interactions between the access and core networks, hence eliminating the significant time overhead in the traditional handover procedure. Specifically, we introduce a novel network function named Satellite Synchronized Function (SSF), which is designed for being compliant with the standard 5G core network. Moreover, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments and results demonstrate that our proposed handover scheme can considerably reduce the handover latency by 21× compared to the standard NTN handover scheme and two other existing handover schemes, along with significant improvements in network stability and user-level performance.},
  archive      = {J_TMC},
  author       = {Jiasheng Wu and Shaojie Su and Wenjun Zhu and Xiong Wang and Jingjing Zhang and Xingqiu He and Yue Gao},
  doi          = {10.1109/TMC.2025.3582245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PHandover: Parallel handover in mobile satellite network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum-driven efficient learning model for enhancing robustness of IoT topology. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of Internet of Things (IoT) topologies measures a network structure's tolerance to random failures, or attacks, which is crucial for stable network communication. Research on optimizing network topology robustness has shifted from empirical rules and heuristics to machine learning, which can extract the features of robust network from topology data, thereby reducing the complexity of traditional topology optimization. However, machine learning approaches typically require a large number of parameters, resulting in high costs associated with parameter tuning and inference. To address these issues, this paper combines parameterized quantum circuits, and proposes a Quantum-Driven efficient Learning Model (QDLM) for enhancing robustness of IoT topology. This model leverages quantum exponential states to significantly reduce the number of training parameters while preserving learning performance. For inputs, QDLM integrates arithmetic encoding and quantum state encoding based on topological adjacency matrix, reducing the number of neurons. In training phase, parameterized quantum rotation gates and controlled quantum gates are used to achieve efficient training. A quantum measurement method is designed to ensure the output topology is a connected graph with the required number of edges. Compared to existing topology learning models, QDLM achieves an order-of-magnitude reduction in training parameters while maintaining topology learning effectiveness.},
  archive      = {J_TMC},
  author       = {Songwei Zhang and Tie Qiu and Xiaobo Zhou and Yusheng Ji},
  doi          = {10.1109/TMC.2025.3586749},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A quantum-driven efficient learning model for enhancing robustness of IoT topology},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient sensor fusion at the edge via system-wide dynamic gated neural networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3586882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation mobile systems will support multiple AI-based applications, each leveraging heterogeneous sensors and data sources through deep neural network (DNN) architectures collaboratively executed within the network. In this context, to minimize the cost of the AI inference task subject to requirements on latency, quality, and – crucially – reliability of the inference process, it is vital to optimize (i) the set of sensors/data sources and (ii) the DNN architecture, (iii) the network nodes executing sections of the DNN, and (iv) the resources to use. To achieve these goals, we leverage dynamic gated neural networks with branches, and propose a novel algorithmic strategy called Quantile-constrained Inference (QIC), based upon quantile-Constrained policy optimization. QIC makes joint, high-quality, swift decisions on all the above aspects of the system, with the aim to minimize inference energy cost. We remark that this is the first contribution connecting gated dynamic DNNs with infrastructure-level decision making. We evaluate QIC using a dynamic gated DNN with stems and branches for optimal sensor fusion and inference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera data, and real-world wireless measurements. Our results confirm that QIC closely matches the optimum and outperforms existing approaches in reducing energy consumption (compute, communication, and total) and application requirements failure by over 70%.},
  archive      = {J_TMC},
  author       = {Chetna Singhal and Yashuo Wu and Francesco Malandrino and Sharon G. L. Contreras and Marco Levorato and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3586882},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource-efficient sensor fusion at the edge via system-wide dynamic gated neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and trustworthy block propagation for blockchain-enabled mobile embodied AI networks: A graph resfusion approach. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3587006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By synergistically integrating mobile networks and embodied artificial intelligence (AI), mobile embodied AI networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of messages or vulnerability to malicious tampering, potentially causing severe accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the validator trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.},
  archive      = {J_TMC},
  author       = {Jiawen Kang and Jiana Liao and Runquan Gao and Jinbo Wen and Huawei Huang and Maomao Zhang and Changyan Yi and Tao Zhang and Dusit Niyato and Zibin Zheng},
  doi          = {10.1109/TMC.2025.3587006},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and trustworthy block propagation for blockchain-enabled mobile embodied AI networks: A graph resfusion approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MultiScanner: Enabling simultaneous detection of multiple liquids with mmWave radar based on a composite reflection model. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional liquid detection approaches are often time-intensive and invasive, typically requiring the opening of containers for examination. While recent initiatives have proposed several innovative solutions, including camera-based and vibration sensor-based techniques, these approaches still face limitations in terms of convenience. The development of radio frequency (RF) technology, particularly millimeter-wave (mmWave) radar, offers a promising solution for non-invasive and contactless liquid detection. In particular, during the past few years, a number of radar-based sensing systems have been developed to detect or identify liquids. However, little work has been done on the simultaneous detection of multiple liquids. To fill this gap, we design a novel composite reflection model, which overcomes the detection challenges due to composite interference and environmental reflections, by utilizing the consistency and uniqueness of the reflection signals from multiple liquid targets. Based on the proposed model, we develop a system named MultiScanner, which is able to detect different types of liquids in multi-target scenarios, exhibiting high location independence without the need for extensive data training. Extensive experiments validate the effectiveness of MultiScanner, achieving up to 95.91% accuracy in detecting 10 hazardous-normal liquid combinations in 2-target scenarios. Moreover, even in more complex 5-target scenarios, an detection accuracy of 86.49% can be obtained. To the best of our knowledge, this is the first study that uses RF signals for multi-liquid detection.},
  archive      = {J_TMC},
  author       = {Yifan Guo and Zhu Wang and Zhihui Ren and Wei Xu and Yangqian Lei and Qian Qin and Zhuo Sun and Chao Chen and Bin Guo and Zhiwen Yu and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3587079},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MultiScanner: Enabling simultaneous detection of multiple liquids with mmWave radar based on a composite reflection model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EagleEye: Balancing latency, accuracy, and power on edge-assisted UAVs for urban crowd surveillance. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) equipped with cameras provide a promising way for large-scale urban crowd surveillance due to their convenient deployment and flexible mobility. However, UAVs are constrained by limited power and computing resources, hindering existing work in achieving efficient UAV-based crowd surveillance, i.e., long flight time, high accuracy, and low latency. To this end, we propose EagleEye, a low-power, high-precision, and low-latency crowd surveillance system empowered by edge-assisted UAVs. It leverages lightweight devices on UAV-side to compress video information edge-independently, then transmits key video information instead of raw high-definition videos. Furthermore, we propose a novel spatio-temporal Compressive-Sensing-based video feature compression algorithm to achieve efficient, low-latency video compression. It can reduce video volumes greatly while minimizing the loss of crowd-surveillance-related information. Specifically, inspired by the Compressive Sensing theory, we compress the video content from both the temporal and spatial perspectives by accounting for inter-frame redundancy and intra-frame information saliency, respectively. Finally, we implement a prototype system and conduct extensive experiments based on four large-scale datasets with over 20,000 frames. The experimental results demonstrate that EagleEye can reduce transmission latency by 31.4% only with no more than 4% of accuracy loss in urban crowd detection.},
  archive      = {J_TMC},
  author       = {Chaocan Xiang and Zhenghan Li and Qianyuan Zhang and Xuangou Wu and Xiao Zheng and Yulan Guo},
  doi          = {10.1109/TMC.2025.3586860},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EagleEye: Balancing latency, accuracy, and power on edge-assisted UAVs for urban crowd surveillance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRaMirror: Illuminating blind spots in urban LPWAN with reflective smart surfaces. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3587057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of low-power wide-area networks (LPWAN) in urban environments faces a critical challenge with signal blockage caused by dense obstacles like buildings, resulting in blind spots where end nodes have difficulty reaching the gateway. This paper proposes LoRaMirror, a reflective smart surface design, to essentially eliminate these blind spots and improve overall communication in urban LoRaWAN. LoRaMirror is different from existing smart surface designs, as it addresses unprecedented challenges posed by LPWAN's unique application scenario of extremely long communication distances, extremely low data transmission rates, and extremely wide coverage. LoRaMirror is prototyped with a 16-antenna multi-layer array and the experimental results show significant performance gains in real world practice.},
  archive      = {J_TMC},
  author       = {Songfan Li and Yanbo Zhang and Jansen Christian Liando and Li Lu and Mo Li},
  doi          = {10.1109/TMC.2025.3587057},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LoRaMirror: Illuminating blind spots in urban LPWAN with reflective smart surfaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When good becomes evil: Exploring crosstalk attack surfaces on multi-port USB chargers. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-port chargers, designed to simultaneously charge multiple mobile devices such as smartphones, have gained significant popularity, with millions of units sold in recent years. However, this multi-device charging feature introduces security and privacy risks. If not properly designed and implemented, these chargers can enable communication between connected devices because they are inherently interconnected, which leads to crosstalk voltage leakages. Despite their widespread use, these risks have not been thoroughly investigated. We have identified novel attack surfaces in the circuit design of multi-port chargers that allow an adversary who shares the multi-port charger with the target victim in close proximity to exploit one port to (i) recognize fine-grained user activities of other devices being charged, (ii) eavesdrop on secret audio transmission from USB-C audio pins, and (iii) inject malicious audio commands into built-in voice assistants of charging devices (e.g., Siri, Google Assistant). In this paper, we design and implement XPORTHEFT, a novel system to analyze and demonstrate the uncovered security and privacy threats in multi-port chargers. Specifically, it leverages changes in voltage signals in one neighbor port to monitor voltage changes in the charging port induced by user activities in various user interfaces, such as recognizing running apps and detecting keystrokes. Moreover, XPORTHEFT can also achieve audio transmission eavesdropping and launch inaudible audio injection attacks from the neighbor port to the charging mobile device via the USB-C interface. We extensively evaluate the effectiveness of XPORTHEFT using five commercial multi-port chargers and five mobile devices. The evaluation results show its high effectiveness in recognizing the launch of 20 mobile apps (88.7%) and revealing unlocking passcodes (98.8%), as well as eavesdropping on the audios of numeric digits (97.1%) and alphabetic characters (98.0%). Furthermore, XPORTHEFT achieves 100% success rates in inaudible audio injection attacks on three commercial voice assistants. In addition, our study also shows that XPORTHEFT is resilient to various impact factors and presents the potential to attack multiple victims.},
  archive      = {J_TMC},
  author       = {Tao Ni and Zehua Sun and Yongliang Chen and Yihe Zhou and Jiayimei Wang and Weitao Xu and Qingchuan Zhao and Cong Wang},
  doi          = {10.1109/TMC.2025.3587292},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {When good becomes evil: Exploring crosstalk attack surfaces on multi-port USB chargers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative diffusion model-assisted efficient fingerprinting for in-orchard localization. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise robot localization at the tree level is essential for smart agriculture applications such as precision disease management and targeted nutrient distribution. Existing methods fail to achieve the required accuracy. We propose OrchLoc, a fingerprinting-based localization solution that achieves treelevel precision using a single Long Range (LoRa) gateway. Our approach utilizes channel state information (CSI) across eight channels as a localization fingerprint. To minimize labor-intensive site surveys for fingerprint database construction and maintenance, we develop a CSI generative model (CGM) that learns the relationship between CSI vectors and their corresponding locations. The CGM is fine-tuned using CSI data from static agricultural LoRa sensor nodes, enabling continuous fingerprint database updates. Extensive experiments in two orchards demonstrate that OrchLoc effectively achieves accurate tree-level localization with minimal overhead, improving robot navigation},
  archive      = {J_TMC},
  author       = {Kang Yang and Yuning Chen and Wan Du},
  doi          = {10.1109/TMC.2025.3587206},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Generative diffusion model-assisted efficient fingerprinting for in-orchard localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3585007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding mobile traffic data and predicting future trends are essential for wireless operators and service providers to allocate resources efficiently and manage energy effectively. Despite the strong performance of existing models, accurately forecasting mobile traffic remains a challenge due to limited spatial and temporal modeling capabilities and high computational complexity. This paper introduces MobiMixer, a lightweight and efficient multi-scale spatiotemporal mixing model. Its core concept is to integrate multi-scale information from both spatial and temporal dimensions to improve performance on mobile traffic data. We develop a hierarchical interaction module that incorporates super nodes to enable global high-level feature interactions among nodes with common patterns. Additionally, we employ a dynamic time warping strategy to decouple mobile traffic sequences into stable and seasonal components, which are then modeled at different scales using a multi-scale temporal mixing module. We conduct extensive experiments on mobile traffic datasets collected from four international cities. Compared with 21 state-of-the-art benchmark models, MobiMixer demonstrates highly competitive performance, achieving a maximum improvement of 48.49% on the Milan mobile dataset. The model achieves an improvement in training efficiency of up to 10.69 times and reduces memory usage by 33.01%. The source code is available at https://github.com/PoorOtterBob/Submitted_Code.},
  archive      = {J_TMC},
  author       = {Jiaming Ma and Binwu Wang and Pengkun Wang and Zhengyang Zhou and Yudong Zhang and Xu Wang and Yang Wang},
  doi          = {10.1109/TMC.2025.3585007},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-rate uncoordinated concurrent random access in underwater acoustic networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncoordinated random-access protocols are well-suited for underwater acoustic (UWA) networks due to their simplicity and low overhead. However, their performance is hindered by severe collisions and the challenging characteristics of UWA channels such as rich multipath and Doppler effect. Existing UWA physical layer waveforms struggle to resolve collisions while maintaining high data rates. This paper introduces ZCMod, a high-rate waveform allowing uncoordinated concurrent random access in UWA networks. ZCMod employs a Zadoff–Chu (ZC) sequence-based modulation that assigns unique ZC sequences to users to minimize inter-user interference and encodes multiple bits through cyclic shifts of the sequences to improve data rates. ZCMod further addresses the unique challenges of UWA channels via two new designs: 1) a shape-based demodulation approach that estimates the data-induced shift of channel response shape between the preamble and data symbols to handle rich multipath, and 2) an auxiliary modulation approach that modulates each data symbol with two ZC sequences, one for extracting current channel response shape and the other for data modulation, to handle the fast time-varying channel. Experimental results in a lake and a swimming pool and extensive simulation results show that a) ZCMod achieves around 100% higher throughput compared with the state-of-the-art (SOTA) approaches in quasi-static channels, and b) ZCMod maintains comparable throughput in fast time-varying channels as in quasi-static conditions, where the SOTA approaches experience significant degradation.},
  archive      = {J_TMC},
  author       = {Enqi Zhang and Lei Liang and Lizhao You and Zhaorui Wang and Deqing Wang and Liqun Fu},
  doi          = {10.1109/TMC.2025.3586911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {High-rate uncoordinated concurrent random access in underwater acoustic networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint DNN model deployment, selection and configuration for heterogeneous inference services towards edge intelligence. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3586793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence is an emerging paradigm in edge computing that deploys Deep Neural Network (DNN) models on edge servers with limited storage and computation capacities to provide inference services for high mobility and real-time applications, such as autonomous driving or smart surveillance, with varying accuracy and delay requirements. Adapting application configurations (e.g., image resolution or video frame rate) while selecting different DNN models and deployment locations can provide high-accuracy, low-delay inference services that meet user requirements. However, the configurations and DNN models of various inference services are highly heterogeneous. As balancing inference accuracy, resource cost, and delay is a multi-objective programming problem, it is a great challenge to obtain the optimal solution. To address this challenge, we propose a novel online framework to jointly optimize the configuration adaption, DNN model selection, and deployment for heterogeneous inference services. Specifically, we first formulate this joint optimization problem as an integer linear programming problem and prove it is NP-hard. Then, we further model the problem as a Partial Observable Markov Decision Process (POMDP) and solve it by developing a Heterogeneous-Agent Reinforcement Learning (HARL) based algorithm, named Heterogeneous Inference Service ProvidER (HISPER). It allows agents to have different action spaces corresponding to different types of configurations and DNN models. Finally, extensive experiments demonstrate that the proposed algorithm outperforms other state-of-the-art counterparts.},
  archive      = {J_TMC},
  author       = {Hebin Huang and Junbin Liang and Geyong Min},
  doi          = {10.1109/TMC.2025.3586793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint DNN model deployment, selection and configuration for heterogeneous inference services towards edge intelligence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source routing for LEO mega-constellations based on bloom filter. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3586626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-earth-orbit (LEO) mega-constellations with inter-satellite links (ISLs) are becoming the Internet backbone in space. Satellites within LEO often need the capability to enforce data forwarding paths. For example, they may need to bypass the satellites over the untrusted areas for the data of mission-critical applications or minimize latency for the data of time-sensitive applications. However, typical source/segment routing techniques (e.g., SRv6) suffer from scalability issue, since they record source-route-style forwarding information via the list-based structure. This results in great payload and forwarding overhead. To overcome this drawback, we propose a source/segment routing architecture for LEO mega-constellations, which is named as Link-identified Routing (LiR). LiR leverages in-packet bloom filter (BF) to record source-route-style forwarding information. BF could efficiently record multiple elements via a probabilistic data structure, but overlooks the order of the encoded elements. To address this, LiR identifies each unidirectional ISL, and represents the path by encoding ISL identifiers into BF. We investigate how to optimize BF configuration and ISL encoding policy to address false positives caused by BF. We implement LiR in Linux kernel and develop a container-based emulator for performance evaluation. Results show that LiR significantly outperforms SRv6 in terms of packet forwarding and data delivery efficiency.},
  archive      = {J_TMC},
  author       = {Hefan Zhang and Zhiyuan Wang and Wenhao Lu and Shan Zhang and Hongbin Luo},
  doi          = {10.1109/TMC.2025.3586626},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Source routing for LEO mega-constellations based on bloom filter},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous federated learning driven by multi-knowledge distillation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3586921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a fully heterogeneous federated learning environment, the client has significant differences in model structure and local data distribution (Non-IID), and the joint learning of the client model is blocked due to the limited communication content available for interaction in a fully heterogeneous scenario. In this context, the global knowledge constructed by the server through the simple aggregation of the client logits is essentially a fuzzy representation containing a lot of noise and information loss, which is difficult to effectively guide the client model update. To solve these problems, this paper proposes a heterogeneous federated learning framework (FedMkd) based on multi-knowledge distillation fusion to cope with multiple challenges in heterogeneous environments. The FedMkd framework uses a class-grained logits interaction architecture (CLIA) and introduces an efficient knowledge sharing mechanism. It innovatively integrates two knowledge distillation methods: 1) Temperature-Adaptive Knowledge Distillation (TAKD), which provides differentiated temperatures for teacher and student models by adaptively adjusting the distillation temperature, maximizing knowledge transfer between them; 2) Class-related Knowledge Distillation (CRKD), which introduces batch-level sample correlation loss to reduce over-reliance on specific samples or classes and improve the model's understanding of overall data features. We conducted a large number of experiments on four public data sets. The results show that in a variety of data and model heterogeneous scenarios, FedMkd still performs better than the comparison method when the communication overhead is reduced by more than one order of magnitude.},
  archive      = {J_TMC},
  author       = {Bin Xu and Longgang Cheng and Qing Wen and Zhensheng Zou and Xiaoxuan Hu and Zhenjiang Dong and Jin Qi},
  doi          = {10.1109/TMC.2025.3586921},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Heterogeneous federated learning driven by multi-knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-timescale service caching, computing, and communication optimization for low-latency high-reliability edge-cloud networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the mixed-timescale joint optimization of service caching, computing, and communication for edge-cloud networks. By jointly considering the caching update, user association, task offloading, and computing and communication resource allocation in different timescales with queueing dynamics, we first formulate a long-term optimization problem whose goal is to minimize the network energy consumption while stabilizing the edge-cloud network. By exploiting the Lyapunov optimization techniques, we decompose the overall problem into large-, medium-, and small-timescale problems, and their individual solution approaches are provided. To further improve the latency and reliability, an end-to-end latency measure is proposed along with the use of extreme value theory to derive an enhancement approach. Performance analysis of our approaches is provided and computer simulations are conducted to evaluate the proposed approaches. Results show that our approaches have better latency performance as compared to reference schemes. Furthermore, results also show that our proposed enhancement approach can effectively improve the latency and reliability.},
  archive      = {J_TMC},
  author       = {Chang-Lin Ye and Ming-Chun Lee and Chen-Yuan Wu},
  doi          = {10.1109/TMC.2025.3587713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixed-timescale service caching, computing, and communication optimization for low-latency high-reliability edge-cloud networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-inspired decentralized satellite learning in space computing power networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3587796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring. However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data. To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on-board data processing. Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks. To tackle this issue, we propose to employ spiking neural networks (SNNs) for on-board data processing, which is supported by the neuromorphic computing architecture. The extreme sparsity in its computation enables a high energy efficiency. Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum. We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed. We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance. Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks.},
  archive      = {J_TMC},
  author       = {Peng Yang and Ting Wang and Haibin Cai and Yuanming Shi and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3587796},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Brain-inspired decentralized satellite learning in space computing power networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalizable prompt-based prototypical framework for CSI-based few-shot and cross-domain activity recognition. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3587702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensing systems for human activity recognition (HAR) have made great strides in recent years. However, current HAR models face challenges in generalization due to a limited number of training samples (i.e., few-shot problem) and pattern differences of the collected channel state information (CSI) data across diverse domains (i.e., cross-domain problem). To address the above problems, in this paper, we propose a prompt-based prototypical framework called Wi-Prompt. Our Wi-Prompt framework consists of the following three modules: Prompt generation module, prototypical representation module, and activity recognition module, which works as follows. Prompt generation module is developed to extract prior knowledge from samples in the source domains, providing insightful guidance for establishing class prototypes in the target domains. Prototypical representation module effectively captures the most representative prototype vector of each class with a temporal convolutional network (TCN)-attention model. Activity recognition module determines the activity class of new sample by comparing the Euclidean distance between its corresponding prototype vector and the prototype vector of each class. The greatest advantage of Wi-Prompt is its utilization of prompt-based prototype representation, which eliminates the need for prior domain-specific knowledge about the original CSI samples, making it highly adaptable to a wide variety of CSI datasets collected across different domains. Extensive experiment results based on real-world traces show that our proposed Wi-Prompt outperforms state-of-the-art models under various cross-domain scenarios.},
  archive      = {J_TMC},
  author       = {Yunming Zhao and Wei Gong and Minghui Liwang and Li Li and Baoxian Zhang and Cheng Li},
  doi          = {10.1109/TMC.2025.3587702},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A generalizable prompt-based prototypical framework for CSI-based few-shot and cross-domain activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing QoE in collaborative edge systems with feedback diffusion generative scheduling. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3587744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing is a promising approach for delivering low-delay services to computation-intensive Internet of Things applications. Deep Reinforcement Learning (DRL) has become an effective way to solve task scheduling decisions in edge systems due to its adaptive learning ability to interact with the environment. However, current DRL-based task scheduling methods still face several challenges, such as limited exploration, sample inefficiency, and performance instability, which can lead to degraded user Quality of Experience (QoE). To address these challenges, we observe that diffusion models, famous for their performance in image generation, exhibit strong exploration, data efficiency, and performance stability. This inspires us to propose FDEdge, a novel feedback diffusion generative scheduling method for enhancing user QoE in collaborative edge systems. We first design an innovative Feedback Diffusion (FDN) model by leveraging historical action probability information during the denoising process. We then incorporate the FDN model into DRL, forming an effective and efficient framework for task scheduling in collaborative edge systems. We also present a probability derivation to ensure the FDEdge's rationality. Extensive experimental results demonstrate that our FDEdge method significantly reduces service delays by $45.42\%$ to $87.57\%$ and speeds up training episode durations by $2.5\times$ times for a higher QoE than state-of-the-art methods. We release our open-source code at https://github.com/ChangfuXu/FDEdge.},
  archive      = {J_TMC},
  author       = {Changfu Xu and Jianxiong Guo and Yuzhu Liang and Haodong Zou and Jiandian Zeng and Haipeng Dai and Weijia Jia and Jiannong Cao and Tian Wang},
  doi          = {10.1109/TMC.2025.3587744},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing QoE in collaborative edge systems with feedback diffusion generative scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GATO: Global transmission optimization for SAGIN-assisted IoRT data collection. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3587634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 6G networks, the increasing demand for global coverage has catalyzed a significant expansion in the deployment of the Internet of Remote Things (IoRT). Supported by Space-Air-Ground Integrated Networks (SAGIN), IoRT can efficiently transmit data from areas lacking infrastructure. In this paper, we explore a key yet challenging issue faced by SAGIN: can we exploit the mobility and cooperation of SAGIN heterogeneous nodes to enhance overall transmission performance in all-weather? We approach this issue in three steps. Firstly, to ensure the all-weather transmission, we propose a layered SAGIN transmission design that combines the advantages of radio-frequency (RF) and free-space-optics (FSO) transmissions. Secondly, we formulate an overall transmission performance optimization problem. We aim to maximize the total upload data amount of SAGIN by jointly optimizing the satellite selection, the trajectories of high-altitude-platforms (HAPs) and unmanned aerial vehicles (UAVs), and the UAVs' transmit power. However, the formulated optimization problem falls into a mixed-integer nonlinear programming (MINLP), which is challenging to address. Thirdly, to solve this problem, we propose a global transmission optimization (GATO) strategy by employing block coordinate descent and successive convex approximation technologies. Finally, extensive experimental results demonstrate that the proposed strategy can significantly improve SAGIN's overall transmission performance.},
  archive      = {J_TMC},
  author       = {Yanbo Fan and Yuanguo Bi and Yufei Liu and Dusit Niyato and Liang Zhao and Qiang He and Ammar Hawbani},
  doi          = {10.1109/TMC.2025.3587634},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GATO: Global transmission optimization for SAGIN-assisted IoRT data collection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient cooperative mechanism for distributed multi-agent traffic signal control. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3587257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control (TSC) provides a cost-effective approach to alleviating urban traffic congestion without requiring modifications to physical road infrastructure. As a classic multi-agent coordination problem, TSC can be optimized with multi-agent reinforcement learning (MARL). However, understanding the cooperative strategies among traffic signals is challenging due to the spatiotemporal dynamics inherent in distributed traffic signal controllers. Previous MARL approaches for TSC often rely on imprecise and redundant cooperation mechanisms, leading to excessive communication costs and inefficient training. In this study, we propose a novel dynamic and controllable cooperation MARL algorithm (DCoo) to optimize traffic signals coordination for alleviating traffic congestion. DCoo introduces a collaboration scheduling module (CSM) to obtain spatiotemporal collaborative information and incorporates a cooperative hyper-decision network (HDN) to optimize decisions by leveraging the processed information. In particular, the CSM first captures crucial temporal information for cooperation by analyzing the relationships between historical and current observations of intersections. CSM then frames the selection of spatial information as a binary classification problem to effectively expand the scope of collaboration among intersections. Additionally, HDN integrates an attention mechanism and permutation invariance to enhance the scalability of DCoo. We conduct comparative experiments on three real-world traffic datasets, and the results demonstrate that DCoo outperforms comparable algorithms in both training and testing phases.},
  archive      = {J_TMC},
  author       = {Lulu Li and Yafei Li and Shaohui Zhang and Yuanyuan Jin and Shuo He and Ke Wang and Mingliang Xu},
  doi          = {10.1109/TMC.2025.3587257},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient cooperative mechanism for distributed multi-agent traffic signal control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FREAUTH$^{+}$: A robust frequency feature-based device authentication mechanism for magnetic wireless power transfer system. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3587937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device authentication is critical for preventing unauthorized access and ensuring streamlined operation in magnetic wireless power transfer (WPT) systems. However, existing authentication techniques often suffer from security vulnerabilities and lack compatibility with low-cost receiver devices, limiting their practical applications. In this paper, we introduce FREAUTH$^{+}$ (FREquency feature-based AUTHentication), a novel authentication mechanism tailored specifically for magnetic-based WPT systems. Our approach involves acquiring impedance solely at the transmitter side without requiring active cooperation from the receiver. Using a dual-frequency interleaved subtraction technique, we eliminate ideal receiver impedance to reveal subtle frequency characteristics. These frequency features are then normalized to mitigate the effects of device positioning, and a temperature compensation method is applied to account for thermal variations. This process enables the generation of a robust hardware fingerprint based on frequency characteristics. For authentication, we employ a discrete Fréchet distance-based algorithm for effective fingerprint matching. We also develop a prototype of FREAUTH$^{+}$ and conduct extensive experiments. The results demonstrate its reliability, achieving 96.23% authentication performance across more than 60 devices with an average response time of 1.6 seconds. Furthermore, FREAUTH$^{+}$ exhibits excellent stability, effectively mitigating interference caused by device payload, device positioning, and temperature variations.},
  archive      = {J_TMC},
  author       = {Shenyao Jiang and Hao Zhou and Wangqiu Zhou and Xinyu Wang and Zhenjiang Li and Yusheng Ji},
  doi          = {10.1109/TMC.2025.3587937},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FREAUTH$^{+}$: A robust frequency feature-based device authentication mechanism for magnetic wireless power transfer system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating tail latency for on-device inference with load-balanced heterogeneous models. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3588430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serving machine learning models on edge, mobile, and embedded devices places stringent requirements on inference latency. From operating a real enterprise service, we observed that even a fully optimized model could lead to severe violations of latency objectives when the load surges. A straightforward and mature approach is to auto-scale multiple models to balance the load. However, unlike cloud clusters, edge or mobile devices usually cannot afford to deploy multiple model replicas. Therefore, in this paper, we explore a new idea: in addition to the original model, we deploy one (or more) heterogeneous model(s) with much smaller resource overhead on the device, and perform load balancing among all models. We overcame the technical challenges posed by performance dynamics and developed InferRouter based on queuing theory. We implement and evaluate InferRouter on three real on-device inference systems, covering mobile sensing, video analytics, and natural language processing applications. Experimental results show that compared with strong baselines, InferRouter can decrease 85.2% P99 latency (5.8x faster) and improve 5.9% accuracy on the mobile workload. For a traffic video analytics task, InferRouter achieves 55.1% higher accuracy with zero deadline misses. InferRouter also shows its advantages in saving resources compared with auto-scaling and offloading approaches.},
  archive      = {J_TMC},
  author       = {Mu Yuan and Lan Zhang and Di Duan and Liekang Zeng and Miao-Hui Song and Zichong Li and Guoliang Xing and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3588430},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating tail latency for on-device inference with load-balanced heterogeneous models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized DP-DFL: Towards differentially private decentralized federated learning via randomized model interaction. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional federated learning (FL) frameworks rely on a central server for model coordination among distributed mobile terminals (MTs). The centralization faces two critical challenges, i.e., single point of failure and potential privacy leakage. Differentially private decentralized FL (DP-DFL) has been proposed to address these challenges, wherein the MTs exchange models in a decentralized manner and maintain the differential privacy (DP) guarantee by adding noise to local models before model interaction. However, existing DP-DFL frameworks confront difficulty in achieving the expected privacy and convergence performance, simultaneously. To address this issue, we propose a novel DP-DFL framework (called randomized DP-DFL) that employs a randomized model interaction scheme to lower the model exposure frequency and hence reduce privacy budget consumption. Specifically, the scheme includes two sequential steps, i.e., randomized terminal assignment and randomized model transmission. In Step 1), the model interaction phase of DFL is further divided into several sequential substages. MTs are randomly assigned to each sub-stage. In Step 2), each MT sequentially transmits either a model previously received from its neighbors or its own local model according to the assigned sub-stage order. The proposed scheme enhances the MTs' privacy of DFL since the exposure probabilities of the MTs' local models are significantly reduced via these two randomized steps. Besides, we theoretically analyze the convergence and privacy performance of randomized DP-DFL. In particular, properly tuning the number of sub-stages in randomized DP-DFL can achieve an optimal balance between privacy and convergence. Experimental results show that randomized DP-DFL consistently outperforms traditional frameworks. Compared with baselines, randomized DP-DFL reduces 40.9% privacy loss under the same target accuracy while improving 9.5% learning accuracy under the same privacy loss on EMNIST and CIFAR-10, respectively},
  archive      = {J_TMC},
  author       = {Weihao Zhu and Long Shi and Kang Wei and Yipeng Zhou and Zhe Wang and Zehui Xiong and Jun Li},
  doi          = {10.1109/TMC.2025.3588537},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Randomized DP-DFL: Towards differentially private decentralized federated learning via randomized model interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localization in 5G and beyond: A multi-objective approach for accuracy, latency, and resilience. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3588712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of localization capabilities within the cellular architecture through dedicated 5G network functions has notably enhanced cellular positioning accuracy and enabled new location-based services. However, this architectural shift requires placing measurement acquisition and computation at the network edge and core, resulting in distributed computational resources and increased latency and security risks. As a result, minimizing latency and ensuring resilience against security threats, in addition to achieving high accuracy, become critical performance indicators in location-based services. This paper examines both 3GPP-standardized and O-RAN-based 5G architectures, detailing the key functions, interfaces, and parameters influencing the localization process, from measurement acquisition to position estimation. We define performance indicators for evaluating localization services and develop a system model that quantifies costs related to latency, accuracy, computation, and resilience against security threats. By jointly considering these factors, we formulate a multi-objective optimization problem that guides the selection of an optimal system configuration to simultaneously satisfy multiple localization requirements. We validate our approach through a case study of an end-to-end 5G system using both simulations and experimental data. Specifically, we evaluate various algorithms and implementations across standardized channels and scenarios. Furthermore, we conduct experimental measurements using Software-Defined Radios (SDRs) and open-source 5G platforms to assess operational latency with commercial-off-the-shelf (COTS) devices.},
  archive      = {J_TMC},
  author       = {Luca Petrucci and Samuele Zanini and Ivan Palamà and Nicola Blefari Melazzi and Stefania Bartoletti},
  doi          = {10.1109/TMC.2025.3588712},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Localization in 5G and beyond: A multi-objective approach for accuracy, latency, and resilience},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring upper-6GHz and mmWave in urban 5G networks: A direct on-field comparison. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3587904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for mobile bandwidth is driving 5G networks toward the use of high-frequency spectrum, particularly the upper-6 GHz and mmWave bands. While these bands offer vast bandwidth potential, their propagation characteristics raise critical deployment challenges. This paper presents the first direct, on-field comparative evaluation of 5G standalone (SA) macro-cell deployments operating in these two bands, conducted in Milan, Italy. We show that the upper-6 GHz band can deliver wide-area urban coverage (up to 600 meters) with stable gigabit-level downlink throughput, even in (NLoS) scenarios. mmWave, traditionally deemed unsuitable for NLoS, exhibits strong performance via urban reflections, achieving up to 1.3 Gbps in downlink and 250 Mbps in uplink. Furthermore, outdoor-to-indoor connectivity at mmWave frequencies proves viable through glass facades, challenging pessimistic assumptions about penetration losses. These findings, derived from synchronized deployments and extensive measurements, provide new insights into the complementary roles of these bands and the practical feasibility of their integration into future 5G networks.},
  archive      = {J_TMC},
  author       = {Marcello Morini and Eugenio Moro and Chiara Rubaltelli and Ilario Filippini and Antonio Capone and Danilo De Donno},
  doi          = {10.1109/TMC.2025.3587904},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring upper-6GHz and mmWave in urban 5G networks: A direct on-field comparison},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrowdNet: Adaptive collaborative inference for dynamic mobile intelligent service. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) such as convolutional networks and Transformers are increasingly deployed to provide intelligent service. However, enabling large-scale DNNs in an infrastructure-less mobile crowd encounters the challenges of high resource consumption, poor performance, and low service availability. This paper proposes CrowdNet, a novel device-todevice (D2D) collaborative inference framework for dynamic mobile environments. CrowdNet introduces a CellNet architecture as its core component, designed as lightweight DNNs ideal for deployment and operation on resource-constrained mobile devices. The CellNets can perform inference tasks either independently or collaboratively to ensure robustness against network disruptions. A topology expansion method is utilized to create an inference flow from the physical communication topology, enabling the distributed operation of inference tasks. To handle the dynamic participation of mobile devices, CrowdNet employs fine-tuning adaptation for flexible assembly and collaborative inference. A reinforcement learning (RL)-based approach is introduced to optimize inference topology. Trained with a multiobjective optimization strategy, CrowdNet can enhance overall performance while maintaining individual CellNet functionality. Extensive experiments based on mobile network testbed and realworld datasets validate the effectiveness of CrowdNet on various intelligent tasks, exhibiting remarkable performance gains and robustness compared to state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Gong Chen and Wenzhong Li and Yuchu Fang and Yi Zhang and Sanglu Lu},
  doi          = {10.1109/TMC.2025.3588100},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrowdNet: Adaptive collaborative inference for dynamic mobile intelligent service},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling real-time video detection with adaptive and distributed scheduling in mobile edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time video detection is essential for many mobile visual applications, which brings the heavy computational burden of deep neural networks. Mobile edge computing offers a promising solution by deploying computational resources near mobile devices. However, achieving efficient video detection on mobile devices requires addressing challenges such as different performance requirements, diverse computing and network conditions, and system dynamics. We propose a realtime video detection framework in mobile edge computing, where multiple video streams from mobile devices are processed while balancing key performance metrics with consideration of grouping. A joint optimization problem of task scheduling, model selection, and resource provisioning is formulated for the system, where decisions are made on two timescales. To this end, we propose a window controller to unify decision-making at the time-slot level. We design an online scheduling algorithm based on multi-agent deep reinforcement learning to enable adaptive and distributed scheduling, while a masking-enhanced attention mechanism enables efficient explicit information exchange between mobile devices. Experimental evaluations across different numbers of mobile devices demonstrate that, in terms of average reward, the proposed algorithm outperforms local processing by 14.600%, fixed offloading by 10.007%, and four learning-based scheduling baselines by an average of 2.267%.},
  archive      = {J_TMC},
  author       = {Zhicheng Liu and Yilan Wang and Yunfeng Zhao and Chao Qiu and Cheng Zhang and Xiaofei Wang and Mianxiong Dong},
  doi          = {10.1109/TMC.2025.3588142},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling real-time video detection with adaptive and distributed scheduling in mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel spatial-temporal learning method for enhancing generalization in adaptive video streaming. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3588135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive video streaming has become a fundamental technology for video delivery. With the rise of deep reinforcement learning (DRL), streaming vendors are increasingly adopting DRL-driven adaptive bitrate (ABR) algorithms. In real-world deployments, most ABR approaches are developed with the aim of maintaining good performance across a wide variety of network environments. However, contrary to this expectation, our empirical findings show that even when trained on extensive real-world network trace data, these DRL-based ABR algorithms achieve only 43.1% to 48.9% of Quality-of-Experience (QoE) under highly diverse network conditions, which falls significantly short of the 100% optimum. We termed this problem as “ABR Under-Generalization”. To overcome this problem, we introduce BETA – a novel DRL-based ABR framework that incorporates both spatial and temporal learning mechanisms: 1) Spatially, BETA features a detector that flags the network conditions likely to cause poor performance, then trains specialized ABR models tailored for those conditions; 2) Temporally, BETA enhances its learning by incorporating multi-step decision experiences at each training epoch, enabling the trained model to account for long-term environmental dynamics. Comprehensive evaluations show that BETA outperforms state-of-the-art ABR algorithms, yielding average QoE gains of 19.4% to 50.9%, and achieving improvements of up to 244.1% under severely fluctuating network conditions.},
  archive      = {J_TMC},
  author       = {Guanghui Zhang and Ziming Wang and Huaren Wei and Mengbai Xiao and Hui Yuan and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2025.3588135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel spatial-temporal learning method for enhancing generalization in adaptive video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeVA: An edge-assisted video analytics framework for depth estimation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3588864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-assisted video analytics frameworks, which offload vision-based tasks to edge servers, offer a promising approach to enhance accuracy while minimizing network resource overhead. However, these frameworks often overlook depth estimation, a critical task for applications like augmented reality and intelligent surveillance. Depth estimation, which calculates the distance between objects and the camera, generates depth images with unique characteristics, making existing approaches impractical or inefficient for video analytics in this context. In this work, we present DeVA, an edge-assisted video analytics framework for depth estimation that ensures accuracy with minimal network resource overhead. We examine the impact of various video analytics configurations, including resolution and quantization parameter (QP), on accuracy. Additionally, we analyze the region of interest (RoI) for depth estimation and propose methods for tracking RoI areas locally on the device. DeVA features an adaptive video encoding mechanism that dynamically adjusts the resolution for offloaded video and optimizes QPs for RoI and non-RoI areas. We implement DeVA and evaluate its performance using public video datasets. The results show that DeVA reduces 57.12% of the bandwidth overhead while keeping depth estimation errors within acceptable limits, demonstrating a great balance between accuracy and network resource usage.},
  archive      = {J_TMC},
  author       = {Shutong Chen and Jingwen Yin and Ruichao Zhong and Fangming Liu},
  doi          = {10.1109/TMC.2025.3588864},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeVA: An edge-assisted video analytics framework for depth estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On joint covert and secure communications in D2D-enabled cellular systems. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3589011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the joint covert and secure communications in a device-to-device (D2D)-enabled cellular system (DCS) consisting of a base station BS, an eavesdropper Eve, and two user equipments UE and UR. To conduct secure communications with UE against Eve, BS works either under the cellular mode using direct transmission or under the D2D mode replying through UR, while UR is greedy since it opportunistically transmits its own covert message to UE against the detection from BS. To understand the fundamental performance of secrecy rate and covert rate in DCS, we first develop theoretical models to depict the detection probability/secrecy rate of BS and covert rate of UR under different modes (i.e., underlay, overlay, or cellular). Based on these models, we further explore the secrecy rate maximization (SRM) for BS subject to the constraints of detection probability at BS and transmit power at both BS and UR, as well as the covert rate maximization (CRM) for UR subject to the constraints of covertness requirement and covert transmit power. Finally, we employ the Newton-based searching method to solve the SRM/CRM problems and illustrate via numerical results the achievable secrecy rate and covert rate of BS and UR under various DCS scenarios.},
  archive      = {J_TMC},
  author       = {Ranran Sun and Bin Yang and Yulong Shen and Xiaohong Jiang and Tarik Taleb},
  doi          = {10.1109/TMC.2025.3589011},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On joint covert and secure communications in D2D-enabled cellular systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link performance for mobile LoRa networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, as a typical representative of Low Power Wide Area Networks (LPWAN), has been widely used to connect massive IoT devices. However, in mobile applications, there is significant packet loss in LoRa transmission due to link performance degradation. Existing studies take little account of end-devices' movement, particularly when the movement pattern is unknown. We propose LMLoRa to enhance the Link Performance for Mobile LoRa networks in general scenarios for both single-gateway and multi-gateway applications. The key observation is that, due to LoRa's unique feature, repeating the original packet content enables the use of smaller, more energy-saving transmission parameters, which not only enhances link performance but also reduces energy consumption. Technically, we propose a link performance estimation model based on packet content repetition for both single-gateway and multi-gateway mobile networks. Then, we propose the corresponding channel frequency selection model to avoid transmission collisions. Finally, we design low-overhead communication mechanisms to operate the system. To evaluate the performance of LMLoRa in various scenarios, we design and implement real-world testbeds and a simulation platform for both single-gateway and multi-gateway scenarios. Extensive results show that LMLoRa improves packet delivery ratio by an average of 33.4% to 69.2% compared with the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Ciyuan Chen and Zhuqing Xu and Runqun Xiong and Dian Shen and Weizheng Wang and Junzhou Luo and Xiaohua Jia},
  doi          = {10.1109/TMC.2025.3588923},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing link performance for mobile LoRa networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing task migration for public and private services in vehicular edge networks: A dual-layer graph neural network approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3589245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the vehicular edge networks (VEN), task migration is complicated by issues like vehicle movement, diverse resource allocation, and integrating sensing with communication technologies. This paper presents a task migration strategy to optimize task flow under limited resources in PMN-assisted VEN. Vehicles can send public and private tasks to roadside units (RSUs), constrained by bandwidth, computational power, and storage space. Public tasks aim at data collection for road transportation management, while private tasks cover a spectrum of services from work to entertainment. To address the limitations imposed by resource scarcity and meet the demands of task migration, we have developed a dual-layer graph neural network (GNN) that leverages vehicle mobility patterns. In particular, the first layer of GNN acquires vehicle information and the latest surrounding information, and sends it to the nearby RSU. Considering the variety of tasks and multi-dimensional resource constraints, the second GNN layer forecasts RSU resource availability and vehicular trajectories. Subsequently, a task-based maximum flow algorithm (T-MFA) is proposed to refine task migration paths and resource allocation strategies to maximize task flow. Simulation experiments validate the efficacy of the proposed algorithm, demonstrating its capability to achieve optimal task migration by accommodating differences in tasks, resources, and capacities.},
  archive      = {J_TMC},
  author       = {Xiaowen Huang and Tao Huang and Peng Cheng and Jinhong Yuan and Shuguang Zhao and Guanglin Zhang},
  doi          = {10.1109/TMC.2025.3589245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing task migration for public and private services in vehicular edge networks: A dual-layer graph neural network approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EASTER: Embedding aggregation-based heterogeneous models training in vertical federated learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3589426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) allows collaborative machine learning without sharing local data. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization of participants' local knowledge aggregation. To address this challenge, this paper proposes a novel approach called Embedding Aggregation-based Heterogeneous Models Training in Vertical Federated Learning (EASTER). EASTER focuses on aggregating the local embeddings of each participant's knowledge during forward propagation. We propose an embedding protection method based on lightweight blinding factors, which injects the blinding factors into the local embedding of the passive party. However, the passive party does not own the sample labels, so the local model's gradient cannot be calculated locally. To overcome this limitation, we propose a new method in which the active party assists the passive party in computing its local heterogeneous model gradients. Theoretical analysis and extensive experiments demonstrate that EASTER can simultaneously train multiple heterogeneous models and outperform some recent methods in model performance. For example, compared with the state-of-the-art method, the model accuracy of EASTER was improved by 7.22% under the CIFAR-10 dataset.},
  archive      = {J_TMC},
  author       = {Shuo Wang and Keke Gai and Jing Yu and Liehuang Zhu and Weizhi Meng and Bin Xiao},
  doi          = {10.1109/TMC.2025.3589426},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EASTER: Embedding aggregation-based heterogeneous models training in vertical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-CU: Intelligent cache replacement and content update for data freshness in cloud-edge networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3589609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand on time-sensitive contents increases, data freshness recently becomes an important performance metric in the cache-enabled networks. Therefore, in this paper, we design the joint cache replacement and content update algorithm in the cloud-edge networks considering the data freshness at both the cloud server and the edge server. We define a fresh content acquisition with cache hit (FACH) ratio as a performance metric, which shows the portion of users obtaining the requesting content from the edge server while satisfying the freshness constraint. To maximize the FACH ratio, we propose the reinforcement learning (RL)-based algorithm, named the intelligent Cache replacement and content Update (i-CU) algorithm. In the proposed algorithm, we newly suggest the score-based action decision to reduce the action space while guaranteeing the constraints of the problem. In the simulation results, we develop and evaluate the i-CU algorithm for various datasets, which verifies that the i-CU algorithm can achieve the higher FACH ratio compared to the existing baselines under the various network parameters.},
  archive      = {J_TMC},
  author       = {Sinwoong Yun and Dongsun Kim and Sungjin Lee and Jemin Lee},
  doi          = {10.1109/TMC.2025.3589609},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {I-CU: Intelligent cache replacement and content update for data freshness in cloud-edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3587655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a mobile augmented reality (AR) system designed to enhance user experiences at cultural heritage (CH) sites by providing personalized content and points of interest (POI) recommendations. Applying semantic relatedness, we address the heterogeneity of CH POIs to improve personalization accuracy. Our system constructs a POI-based co-occurrence graph to model semantic relationships, enriching users' visited AR content items for better recommendations. The proposed method integrates content-filtering-based recommendations with this graph and recommends personalized POIs and AR content items. A user study demonstrated that our system outperforms conventional methods by 9.4% in recommendation precision and recall, significantly enhancing user engagement and attention focus during CH tours.},
  archive      = {J_TMC},
  author       = {Maryam Shakeri and Abolghasem Sadeghi-Niaraki and Jens Grubert and Soo-Mi Choi},
  doi          = {10.1109/TMC.2025.3587655},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFFGCN: Multimodal feature fusion graph convolution network for radio map estimation with uneven spatial sampling. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3590335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio map estimation (RME) is a crucial method for analyzing spectrum space utilization and network coverage, serving as an essential tool for the mobile communication. However, physical constraints, security, privacy, and other issues often render some areas inaccessible, resulting in extremely sparse and unevenly distributed measurement data. To address these challenges, we propose a multimodal feature fusion graph convolution network (MFFGCN). The model incorporates a dual-encoder architecture with an adaptive multi-feature fusion module to exploit environmental information and learn the shadowing effects of radio-signal propagation. We then convert the coarse estimation into regional feature patches and construct a graph over these patches. A graph neural network aggregates contextual information among them, thereby alleviating the impact of uneven spatial sampling. Extensive experiments on open datasets demonstrate that our method achieves state-of-the-art performance, effectively reducing the effects of uneven sampling.},
  archive      = {J_TMC},
  author       = {Han Zhang and Yu Han and Lingxin Meng and Guan Gui and Wei Xiang and Yun Lin},
  doi          = {10.1109/TMC.2025.3590335},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MFFGCN: Multimodal feature fusion graph convolution network for radio map estimation with uneven spatial sampling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-aware neural adaptive scaling for vRAN resource allocation. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3590472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although virtualized radio access networks (vRANs) offer flexibility and scalability, current scaling methods in vRANs tend to overlook the costs associated with energy consumption and service interruptions during the reconfiguration of containerized network functions (cNFs), leading to inefficient resource utilization. Moreover, traditional vertical and horizontal scaling approaches exacerbate these issues by requiring cNFs to be stopped and restarted or by deploying cNFs with fixed resource sizes. To address these challenges, we propose a cost-aware neural adaptive scaling (CNAS) framework, which adjusts cNF resource allocation dynamically, minimizing service disruptions and avoiding overprovisioning. The cost model incorporates energy consumption during the activation, allocation, and termination of cNFs and servers, as well as the operational cost of maintaining active cNFs and servers. We then formulate the integer linear programming (ILP) problem to minimize total costs. Due to the NP-hard complexity of this problem, two heuristic algorithms are applied: one utilizes dynamic programming to establish the cost-aware resource allocation, while the other uses a greedy approach to handle cNFs and servers following the determined resource allocation. Trace-driven simulation results demonstrate that CNAS can reduce the scaling costs by 53.4% and the total costs by 21.3% compared to the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Taeyun Kim and Daeyoung Jung and Yujin Kim and Sangheon Pack},
  doi          = {10.1109/TMC.2025.3590472},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-aware neural adaptive scaling for vRAN resource allocation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the generalization ability of spatiotemporal model in urban scenario. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal neural networks have shown great promise in urban scenarios by effectively capturing temporal and spatial correlations. However, urban environments are constantly evolving, and current model evaluations are often limited to traffic scenarios and use data mainly collected only a few weeks after training period to evaluate model performance. The generalization ability of these models remains largely unexplored. To address this, we propose a Spatiotemporal Out-of-Distribution (ST-OOD) benchmark, which comprises six urban scenario: bike-sharing, 311 services, pedestrian counts, traffic speed, traffic flow, ride-hailing demand, and bike-sharing, each with in-distribution (same year) and out-of-distribution (next years) settings. We extensively evaluate state-of-the-art spatiotemporal models and find that their performance degrades significantly in out-of-distribution settings, with most models performing even worse than a simple Multi-Layer Perceptron (MLP). Our findings suggest that current leading methods tend to over-rely on parameters to overfit training data, which may lead to good performance on in-distribution data but often results in poor generalization. We also investigated whether dropout could mitigate the negative effects of overfitting. Our results showed that a slight dropout rate could significantly improve generalization performance on most datasets, with minimal impact on in-distribution performance. However, balancing in-distribution and out-of-distribution performance remains a challenging problem. We hope that the proposed benchmark will encourage further research on this critical issue.},
  archive      = {J_TMC},
  author       = {Hongjun Wang and Jiyuan Chen and Tong Pan and Zheng Dong and Renhe Jiang and Xuan Song},
  doi          = {10.1109/TMC.2025.3590606},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Evaluating the generalization ability of spatiotemporal model in urban scenario},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contributed perception-based dynamic evolution method for autonomous vehicle groups in open scenes. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately handling dynamic evolution events is a significant challenge for autonomous vehicle groups (AVGs) in open scenes, which can be affected by complex road conditions and various interference factors. Existing work on the dynamic evolution of AVGs in open scenes concentrates on semi-centralized groups, assessing communication links as the sole criterion. However, there lack the mathematical analysis of and methods for the dynamic evolution of events in distributed AVGs with cooperative perception. To address this issue, we propose a contributed perception-based dynamic evolution method designed for distributed AVGs. This method ensures that group members can continuously and timely exchange valid perceptual information. Firstly, we investigate the impact of external interference on the contributed perception of vehicle groups to understand the drivers behind their dynamic evolution. Secondly, we define a range of vehicle group evolution behaviors and corresponding handling methods in response to external interference. Lastly, we introduce group states and perceptibility to delineate the evolution dynamics. Simulation results demonstrate the superiority of our proposed method over existing ones in terms of average group contribution, accessibility, persistence, timeliness, and perceptibility.},
  archive      = {J_TMC},
  author       = {Qichao Mao and Jiujun Cheng and MengChu Zhou and Zhangkai Ni and Guiyuan Yuan and Shangce Gao and Chuanhuang Li},
  doi          = {10.1109/TMC.2025.3590653},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Contributed perception-based dynamic evolution method for autonomous vehicle groups in open scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge large AI model agent-empowered cognitive multimodal semantic communication. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communications (SemCom) provide efficient transmission for mobile edge computing (MEC) services by extracting critical semantics from raw information. Although widely adopted in various scenarios, existing single-modal Sem Com systems struggle to efficiently support edge multimodal data transmission. Additionally, mobile end users have varying communication requirements across different modalities. However, existing work lacks the ability to generate personalized communication policies tailored to diverse intents (Typically, communication policies include bandwidth allocation and modulation and coding schemes, etc.). In this paper, we propose an edge Cognitive SemCom Agent (CSCA) to facilitate edge multimodal SemCom. Specifically, CSCA leverages an edge Large AI Model (LAM) to realize modality alignment and natural language intent understanding. Moreover, we develop a communication planning module to realize the planning capability, which generates personalized wireless communication policies based on LAM's environment and intent cognition. Particularly, to assess the efficiency of communication policies in multimodal SemCom and capture intent competition, we present a novel indicator named cognitive SemCom quality indicator (CSCQI). Then, we use the denoising diffusion probabilistic model to optimize the generation policy. Extensive experimental results demonstrate that CSCA achieves an average improvement in intent satisfaction rate and semantic accuracy by 42.19% and 29.75% respectively, while reducing communication delay by 33.40%.},
  archive      = {J_TMC},
  author       = {Yan Sun and Yinqiu Liu and Shaoyong Guo and Xuesong Qiu and Jiewei Chen and Jiakai Hao and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3590723},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge large AI model agent-empowered cognitive multimodal semantic communication},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scenario robust stochastic optimization based approach for scheduling of mobile charging stations. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, mobile charging stations (MCSs) have emerged as a complement of fixed charging stations. An idle MCS could receive few charging requests due to the lack of neighboring electric vehicles to be charged (EVCs), and thus its charging profit could be affected. To decrease the scheduling cost and increase the charging profit of MCSs, idle MCSs should proactively move to the regions with high charging demand and with low charging supply. However, the future charging demand of EVCs is uncertain and related to some external factors. To this end, we introduce the robust stochastic optimization (RSO) model to formulate the future charging demand as an uncertain variable, and then a scheduling approach based on the multi-scenario robust stochastic optimization (SA-MSRSO) is proposed. In SAMSRSO, the probability distributions of the uncertain variable across a range of potential scenarios are approximatively obtained by analyzing the historical charging demand, and a multivariate regression tree (MRT) is applied for solving the model. Extensive simulations and comparisons demonstrate the performance superiority of our proposed SA-MSRSO, i.e., the charging profit of MCSs can be significantly increased, and the proportion of successfully charged EVCs can be effectively enhanced.},
  archive      = {J_TMC},
  author       = {Linfeng Liu and Youheng Zheng and Yu Tang and Jia Xu},
  doi          = {10.1109/TMC.2025.3590688},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-scenario robust stochastic optimization based approach for scheduling of mobile charging stations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust model for underwater wireless sensor networks based on variational autoencoders. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With sensitive applications in marine exploration, disaster warning, and oil monitoring, ensuring the data integrity of Underwater Wireless Sensor Networks (UWSNs) has become an increasingly critical challenge. Existing trust models primarily rely on Euclidean distance to evaluate the trustworthiness of nodes. However, the lack of a probabilistic framework for representing trust evidence undermines the accuracy and reliability of these models. In this paper, we propose a novel trust model based on a Variational Autoencoder (VAE) that maps trust evidence into a latent space to model the normal data distribution, thus enabling the identification of malicious nodes. Recognizing the limitations of Euclidean latent spaces in capturing the non-linear characteristics of underwater acoustic signals, we extend this framework by introducing a Riemannian manifold as the latent space. The proposed method enhances scalability by reconstructing implicit features, thereby offering the potential to uncover previously unrecognized threats and improving the accuracy of malicious node identification. Simulation results demonstrate that the proposed trust model achieves a detection accuracy of 94%. Compared to existing trust models, this approach exhibits superior verification accuracy and robustness, making it a promising solution for ensuring the reliability of UWSNs in underwater environments characterized by high noise levels and dynamic topology changes.},
  archive      = {J_TMC},
  author       = {Na Xia and Sizhou Wei and Meng Li and Yin Wang and Jiashan Wan},
  doi          = {10.1109/TMC.2025.3590741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trust model for underwater wireless sensor networks based on variational autoencoders},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated MADDPG-based collaborative scheduling strategy in vehicular edge computing. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3590747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Vehicular Edge Computing (VEC), vehicles of fload computational tasks to Roadside Units (RSUs) equipped with edge servers to achieve efficient processing. Considering that vehicles switch connections between RSUs during highspeed movement, obtaining the state information of other RSUs is crucial for achieving global collaborative decision-making. However, frequent sharing of RSUs' state data during the training of scheduling models may result in privacy leakage risks. To address this issue, we federally train a joint scheduling model for task offloading and resource allocation without the need for state sharing among RSUs. We prove that the proposed task offloading problem influenced by resource allocation is a strict multi-node non-cooperative potential game problem, and use the potential function as the reward function for MultiAgent Deep Deterministic Policy Gradient (MADDPG). Finally, we propose the Fed-MADDPG algorithm to find the equilibrium point of task offloading and apply the gradient descent method and the Lagrange multiplier method to maximize the average task completion rate among RSUs under constraints, ensuring the framework has optimal computational and transmission performance. We conduct simulation experiments using realworld datasets, and the results show that this method has superior performance compared to previous approaches.},
  archive      = {J_TMC},
  author       = {Songxin Lei and Huijun Tang and Chuangyi Li and Xueying Zhang and Chenli Xu and Huaming Wu},
  doi          = {10.1109/TMC.2025.3590747},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated MADDPG-based collaborative scheduling strategy in vehicular edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3585538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying federated learning (FL) in wireless network with hierarchical client-edge-cloud architecture enables large-scale distribution collaboration without long-distance communication latency. However, the ongoing edge dynamics with uncertain client mobility and imbalanced data distributions, poses great challenge for collaboration efficiency of FL. In this work, we first model the client mobility with a Markov chain, and formulate the minimization of performance degradation as a client-edge association control problem. With the analysis of client mobility patterns, we propose ALPHA, a new client-edge association control framework for mobility-aware FL, to reshape the edge-level data distributions close to i.i.d in both offline and online mobility scenarios. In the offline scenario with deterministic client mobility trajectories, we leverage alternating optimization theory to transform the client-edge association control problem into a weighted bipartite b-matching problem, and derive an efficient solution with linear relaxation and dependent rounding techniques. As for the online scenario, where each client arrives at different edge access points (APs) in an online manner, we design a fast and simple online subgradient projection algorithm with a bounded regret to make an online decision on client-edge association. Extensive experiment results on three public datasets and a real-world mobility trajectory dataset show that ALPHA has a superior learning performance with 1.40× – 2.89× convergence speedup compared to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Haibo Liu and Zhenzhe Zheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3585538},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An underwater secure localization scheme based on physical layer cryptographic learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open underwater environments, ensuring accurate positions of sensors while protecting private information of localization systems presents a significant challenge. The physical channel differences between terrestrial and underwater networks render most existing privacy protection schemes designed for terrestrial networks inapplicable underwater. Moreover, limited research on underwater privacy protection has led to high implementation complexity and communication expenses. In this paper, to reduce the complexity of privacy protection, a secure mobile localization scheme using autonomous underwater vehicles (AUVs) as anchors is proposed for underwater sensor networks, based on adversarial neural cryptography utilizing acoustic channel features. Depending on whether eavesdroppers show interest in keys, two adversarial cryptography models are proposed to protect transmission of legitimate localization information and to actively counter eavesdroppers with learning capabilities in real time. Furthermore, to obtain effective keys and minimize unnecessary key transmission, random physical layer channel features are dynamically utilized as real-time keys for the cryptography system, and a synchronous channel probing protocol is designed for key generation. Simulation and experimental results demonstrate that, compared to other approaches, the proposed secure localization scheme effectively prevents the leakage of position information and maintains localization accuracy while operating at lower implementation complexity and communication expenses.},
  archive      = {J_TMC},
  author       = {Rong Fan and Azzedine Boukerche and Pan Pan and Zhigang Jin and Yishan Su},
  doi          = {10.1109/TMC.2025.3591016},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An underwater secure localization scheme based on physical layer cryptographic learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private and effective range counting query over evolving data in internet of things. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range counting query is the fundamental task for data analysis and data mining in Internet of Things (IoT). However, it poses a threat to the data privacy of data contributors, which is exacerbated by evolving data in particular. Several studies focus on range counting query on a timestamp over the finite evolving data, and thus are not applicable to the longitudinal range counting query on the infinite evolving data. To this end, we propose the Private and Effective Range counting query over Evolving data (PERE) that supports both the finite evolving data and the infinite evolving data, and is applicable for both the range counting query on a specific timestamp and the longitudinal range counting query. Specifically, we first design a Private Infinite Update Framework for IoT evolving data while providing meaningful privacy protection. The framework is coupled with a general and practical data evolution paradigm. Then, we propose a Optimized Frequency Perturbation consisting of an enhanced frequency oracle protocol and random sampling attribute. On this basis, we further propose a novel Adaptive Interval Merging mechanism that dynamically considers all potential interval consolidation possibilities and the reasonable selection of intervals for merging, to balance non-uniform error and noise error. Thereafter, we further reduce the estimated error in query results by Frequency Adjustment that consists of Norm-Sub and weighted average process. Last, we theoretically prove that the proposed PERE satisfies Local Differential Privacy (LDP), that the query results of PERE are unbiased, and that the variance of the query results is desirable. Furthermore, the extensive experiments on multiple real-world and synthetic datasets validate the effectiveness of PERE, as well as its advantages over the state-of-the-art works in answering range counting queries of evolving data.},
  archive      = {J_TMC},
  author       = {Ping Zhao and Dazhi Hu},
  doi          = {10.1109/TMC.2025.3590918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Private and effective range counting query over evolving data in internet of things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-of-service aware LLM routing for edge computing with multiple experts. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.},
  archive      = {J_TMC},
  author       = {Jin Yang and Qiong Wu and Zhiying Feng and Zhi Zhou and Deke Guo and Xu Chen},
  doi          = {10.1109/TMC.2025.3590969},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quality-of-service aware LLM routing for edge computing with multiple experts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient and secure federated learning for edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the collaborative machine learning nature of Federated Learning (FL), it enables the training of machine learning models on large-scale distributed datasets in edge computing environments. Nevertheless, the application of FL in edge computing still faces three crucial challenges: resource constraint, privacy leakage, and Byzantine failures. Unfortunately, current approaches lack the ability to effectively balance these three challenges. In this paper, we propose FedEdge, a cost-efficient and secure FL for edge computing. FedEdge contains two main mechanisms: adaptive compression perturbation and dynamic update filtering. The adaptive compression perturbation mechanism reduces the communication overhead, provides different levels of privacy protection for edge nodes, and prevents Byzantine attacks. The dynamic update filtering mechanism is used to further filter Byzantine attacks and limit the impact of adaptive compression perturbation on the global model performance. The experimental results on the MNIST, CIFAR-10, CIFAR-100, and CelebA datasets demonstrate the effectiveness of FedEdge against free-riders, label-flipping, and sign-flipping attacks. Theoretical analysis also demonstrate that FedEdge can still converge even when the majority of edge nodes are malicious.},
  archive      = {J_TMC},
  author       = {Zhuangzhuang Zhang and Libing Wu and Zhibo Wang and Jiahui Hu and Chao Ma and Qin Liu},
  doi          = {10.1109/TMC.2025.3590799},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-efficient and secure federated learning for edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniTrans: A unified vertical federated knowledge transfer framework for enhancing edge healthcare collaboration. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-hospital collaboration has the potential to mitigate disparities in medical resources across different regions. However, strict privacy regulations prohibit the direct sharing of sensitive patient information between hospitals. Vertical Federated Learning (VFL) provides a novel privacy-preserving machine learning paradigm designed to maximizes data utility across multiple hospitals. Nevertheless, traditional VFL methods primarily benefit patients with overlapping data, leaving non-overlapping patients without guaranteed improvements in distributed healthcare prediction services. While some existing knowledge transfer techniques attempt to improve prediction performance for non-overlapping patients, they fail to adequately address scenarios where overlapping and non-overlapping patients originate from different domains, resulting in challenges such as feature and label heterogeneity. To address these issues, we propose UniTrans, a unified vertical federated knowledge transfer framework for edge healthcare collaboration. Our framework consists of three key steps. First, we extract the federated representation of overlapping patients by employing an effective vertical federated representation learning method to model multi-party joint features online. Next, each hospital learns a local knowledge transfer module offline, enabling the domain-adaptive transfer of knowledge from the federated representation of overlapping patients to the enriched representation of local non-overlapping patients. Finally, hospitals utilize these enriched local representations to enhance performance across various downstream medical prediction tasks. Extensive experiments on real-world medical datasets demonstrate the effectiveness and scalability of UniTrans in both intra-domain and cross-domain knowledge transfer. The code of UniTrans is available at https://github.com/Chung-ju/Unitrans.},
  archive      = {J_TMC},
  author       = {Chung-ju Huang and Yuanpeng He and Xiao Han and Wenpin Jiao and Zhi Jin and Leye Wang},
  doi          = {10.1109/TMC.2025.3590813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UniTrans: A unified vertical federated knowledge transfer framework for enhancing edge healthcare collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADGTrace: Achieving adaptive trajectory synthesis with generated data. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3590770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User trajectory publication has promoted various location-based applications like user travel recommendation. However, possible privacy leakages have hindered more inclusive trajectory data analysis and utilization. Privacy-preserving trajectory synthesis is a popular approach to address the above privacy issues. Existing methods unavoidably produce low trajectory utility since they usually apply perturbed versions of human moving patterns. Worse still, they cannot adaptively adjust this synthesis according to the varying granularity demands of different users. This paper proposes a novel adaptive trajectory synthesis framework with generated data, namely ADGTrace. Our model achieves privacy preservation without introducing additional noise while maintaining high adaptation. ADGTrace directly synthesizes artificial trajectories that share the similar patterns with real ones through a generative and selective optimization process. Additionally, we present a grid granularity alignment strategy to achieve adaptive trajectory synthesis, satisfying varying user demands. Extensive experiments on real-world datasets demonstrate the superiority of ADGTrace over the state-of-the art methods under various utility metrics, maintaining strong attack resilience.},
  archive      = {J_TMC},
  author       = {Hui Cai and Chen Lan and Biyun Sheng and Jian Zhou and Yuanyuan Yang and Yanmin Zhu and Fu Xiao},
  doi          = {10.1109/TMC.2025.3590770},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ADGTrace: Achieving adaptive trajectory synthesis with generated data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing non-intrusive load monitoring privacy invasion: A precise adversarial attack scheme for networked smart meters. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3590765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grid, through networked smart meters employing the non-intrusive load monitoring (NILM) technique, can considerably discern the usage patterns of residential appliances. However, this technique also incurs privacy leakage. To address this issue, we propose an innovative scheme based on adversarial attack in this paper. The scheme effectively prevents NILM models from violating appliance-level privacy, while also ensuring accurate billing calculation for users. To achieve this objective, we overcome two primary challenges. First, as NILM models fall under the category of time-series regression models, direct application of traditional adversarial attacks designed for classification tasks is not feasible. To tackle this issue, we formulate a novel adversarial attack problem tailored specifically for NILM and providing a theoretical foundation for utilizing the Jacobian of the NILM model to generate imperceptible perturbations. Leveraging the Jacobian, our scheme can produce perturbations, which effectively misleads the signal prediction of NILM models to safeguard users' appliance-level privacy. The second challenge pertains to fundamental utility requirements, where existing adversarial attack schemes struggle to achieve accurate billing calculation for users. To handle this problem, we introduce an additional constraint, mandating that the sum of added perturbations within a billing period must be precisely zero. Experimental validation on real-world power datasets REDD and U.K.-DALE demonstrates the efficacy of our proposed solutions, which can significantly amplify the discrepancy between the output of the targeted NILM model and the actual power signal of appliances, and enable accurate billing at the same time. Additionally, our solutions exhibit transferability, making the generated perturbation signal from one target model applicable to other diverse NILM models.},
  archive      = {J_TMC},
  author       = {Jialing He and Jiacheng Wang and Ning Wang and Shangwei Guo and Liehuang Zhu and Dusit Niyato and Tao Xiang},
  doi          = {10.1109/TMC.2025.3590765},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Preventing non-intrusive load monitoring privacy invasion: A precise adversarial attack scheme for networked smart meters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal based 3D localization via the channel adjustment LED-tag. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3590801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of intelligent systems like assisted driving and robotics, all-weather target identification and 3D localization systems have become crucial for reliable obstacle avoidance and navigation. However, vision-based methods struggle to provide accurate target locations under low light or bad weather. Radar-based solutions like mmWave radar and LiDAR are robust but hindered by high costs and challenges in recognizing target identities at scale. In this paper, we propose a low-cost, all-weather target identification and 3D localization system based on LED-tags, which system can address the needs of intelligent systems for obstacle avoidance in complex environments. We explore the backscatter communication of LED devices and design a dual-modal LED-Tag, which includes two features: a backscatter RF signal detectable by RF devices and visual light spot information detectable by cameras, both sharing the same ID. To enhance the limited backscatter capability, we propose a multi-branch parallel model that enhances the signal strength using beamforming synthesis and a channel adjustment mechanism to improve robustness in complex environments, ensuring accurate 3D localization. For multi-target identification, we design an LED-tag encoding system, assigning each tag a unique encoding sequence. Each target's identity can be recognized with our customized ID decoding method, which leverages prior information and time-domain sampling characteristics. Extensive experimental results show that the backscatter communication and target detection range of LED-tags can reach 15m. Moreover, the system achieves an average localization error of 7.3cm within a 5m range, demonstrating the system's excellent performance in terms of practicality and accuracy.},
  archive      = {J_TMC},
  author       = {Shiyuan Ma and Lei Xie and Chuyu Wang and Yanling Bu and Long Fan and Jingyi Ning and Qing Guo and Baoliu Ye and Sanglu Lu},
  doi          = {10.1109/TMC.2025.3590801},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-modal based 3D localization via the channel adjustment LED-tag},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCFI-net: Global-local cross-spatial-channel feature interaction network for point cloud geometry compression. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3590775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently compressing large-scale point cloud data under limited bandwidth and computing resource conditions has become a critical issue to be addressed in mobile computing platforms. Although the octree structure can efficiently represent large-scale and complex point clouds, existing octree-based Point Cloud Geometry Compression (PCGC) approaches typically focus on exploiting either spatial or channel features individually, neglecting the interaction across spatial-channel dimensions. In addition, current approaches are also limited to small-scale point clouds due to reliance on global Transformer or local convolutional neural network (CNN). To solve these issues, we introduce GCFI-Net, a global-local cross-spatial-channel feature interaction network for predicting the occupancy probability distribution of each octree node in this paper. In the GCFI-Net, we propose a Multiscale Convolutional Fusion-based Spatial Interaction (MCFSI) module to capture global context and model spatial interactions, and a Global-Local Cross-Channel Interaction (GLCCI) module with dual pathways to integrate global and local cross-channel information. Additionally, we propose a Multiscale-enhanced Spatial and Channel Interaction (MSCI) module to aggregate features from ancestor and sibling nodes, which further enhances the octree node representation ability. Extensive experiments on large-scale sparse LiDAR and dense human body point clouds demonstrate that the proposed GCFI-Net achieves superior compression performance with fewer parameters compared to state-of-the-art PCGC methods.},
  archive      = {J_TMC},
  author       = {Xinjie Wang and Yifan Zhang and Xinpu Liu and Ke Xu and Jianwei Wan and Yulan Guo and Hanyun Wang},
  doi          = {10.1109/TMC.2025.3590775},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GCFI-net: Global-local cross-spatial-channel feature interaction network for point cloud geometry compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive interference alignment for underwater optical wireless sensor networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3590884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Optical Wireless Sensor Networks (UOWSNs) have emerged as a promising solution for high-speed underwater communication. However, these networks face a critical challenge of mutual interference among optical nodes, which occurs when the directional optical beams intersect or coverage areas overlap due to node mobility in dynamic underwater environments. Existing interference management approaches demonstrate limited effectiveness due to their reliance on simplified channel models and inability to handle rapid topology changes, resulting in significant network performance degradation. This paper presents a novel framework that systematically addresses interference management in UOWSNs through two key innovations. First, we propose a Sparse Bayesian Learning-based Interference Detection (SBL-ID) algorithm that enables real-time identification and characterization of interference patterns under complex underwater channel conditions. Second, we develop an Adaptive Interference Alignment and Delay Compensation (AIADC) algorithm that projects interference signals into a reduced-dimensional subspace, thereby enhancing the signal-to-interference ratio and facilitating accurate detection of desired signals amid interference. Our framework transforms the NP-hard interference management problem into tractable optimizations, achieving near-optimal solutions with polynomial time complexity. Extensive simulations demonstrate that our approach reduces BER by 95% and improves network throughput by 67% compared to state-of-the-art techniques. Testbed experiments conducted in both pool and lake further validate our framework's effectiveness, maintaining consistent performance improvements under diverse underwater conditions.},
  archive      = {J_TMC},
  author       = {Yang Chi and Chi Lin and Fengqi Li and Feng Ding and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TMC.2025.3590884},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive interference alignment for underwater optical wireless sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asynchronous consensus method with low communication traffic and high efficiency for distributed multi-agent scheduling. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Internet of Things (AIoT) is growing into a new frontier field with broad development prospects, which essence is the collaborative enhancement of networked heterogeneous agent swarms. The market-based approach is an effective way for the cooperative scheduling of agent swarm, where networked agents need to distributedly select and arrange tasks meeting the spatio-temporal constraints. This paper proposes a new asynchronous consensus method aimed at substantially mitigating the communication traffic and decreasing the message transmission requirements associated with the market-based approach, ultimately leading to a reduction in scheduling time. Firstly, the method innovatively introduces timestamps of agent information updates, which are more informative, thereby reducing inter-agent communication volume to $ n/m$ of that in the original protocol (where $ n$ represents the number of agents and $ m$ denotes the number of tasks, with $ m\gt n$). Secondly, agent-centric asynchronous consensus protocols are designed based on the new timestamps, which can resolve inter-agent task conflicts more rapidly and efficiently. Additionally, a mechanism for avoiding message flooding is proposed to prevent endless broadcasts caused by communication issues such as packet loss, link disruptions, and node withdrawals. Finally, through a self-developed ad-hoc network simulation system, the swarm scheduling under real networking conditions is simulated. The validation results demonstrate that the algorithm can significantly reduce communication traffic and scheduling time.},
  archive      = {J_TMC},
  author       = {Runfeng Chen and Jie Li and Yiting Chen and Yuchong Huang and Xiangke Wang and Lincheng Shen},
  doi          = {10.1109/TMC.2025.3591038},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An asynchronous consensus method with low communication traffic and high efficiency for distributed multi-agent scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DepGuard: Depression recognition and episode monitoring system with a ubiquitous wrist-worn device. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3591096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression significantly impacts mental health, severely disrupting patients' daily lives. During depressive episodes, individuals may experience symptoms such as excessive guilt, self-harm, and suicidal ideation. Compared to proprietary devices like brain electrode caps, wearable technologies for depression detection have gained attention due to their affordability and portability—enabling real-time monitoring of depressive states. However, challenges such as low-quality data from ubiquitous devices, individual variability, and the complexity of multimodal physiological signal analysis limit model generalizability. To address these issues, we present DepGuard, a novel ubiquitous wearable system for depression assessment based on multimodal physiological signals. DepGuard performs a two-stage detection process: depression recognition and real-time episode monitoring. For depression recognition, we propose an unsupervised domain adaptation method to reduce the domain gap between source and target subjects. For episode monitoring, we employ a few-shot learning strategy to enable personalized modeling. Both approaches enhance cross-subject generalization. Our system achieves 90.75% accuracy in cross-subject depression recognition using 30 unlabeled samples per target subject, and 93.52% accuracy in episode monitoring using 15 labeled samples per class.},
  archive      = {J_TMC},
  author       = {Yufei Zhang and Shuo Jin and Wenting Kuang and Yuda Zheng and Qifeng Song and Changhe Fan and Yongpan Zou and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3591096},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DepGuard: Depression recognition and episode monitoring system with a ubiquitous wrist-worn device},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EP-GSPR: An efficient privacy-preserving graph shortest path retrieval scheme. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous development of mobile terminal applications, online maps, and other navigation services have become widely used, simultaneously giving rise to significant security risks. To address the issues of privacy leakage and low efficiency in traditional graph shortest path retrieval schemes, an efficient privacy-preserving graph shortest path retrieval scheme is proposed, called EP-GSPR. Specifically, this scheme addresses the privacy security problems in the existing graph shortest path retrieval solutions by ensuring the bilateral privacy protection of the user's query location and the database privacy of the cloud server. Throughout the retrieval process, the cloud server cannot obtain the user's location information, and the user cannot access any database information other than the retrieval results. To overcome the performance bottlenecks in existing schemes, a progressive iterative retrieval framework is designed as the fundamental modular, called Pirf, achieving sub-linear retrieval costs and low storage overhead on the cloud server side. Finally, the security analyses demonstrate the EP-GSPR scheme achieves the bilateral privacy-preserving in terms of user and server sides. The comprehensive experiment evaluations also state the efficiency and practicality of the proposed scheme},
  archive      = {J_TMC},
  author       = {Chenbin Zhao and Ruifeng Zhu and Jing Chen and Ruiying Du and Kun He and Jianting Ning and Yang Xiang},
  doi          = {10.1109/TMC.2025.3591097},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EP-GSPR: An efficient privacy-preserving graph shortest path retrieval scheme},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal diffusion model for cellular traffic generation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, the increasing demand for network traffic necessitates strategic network infrastructure planning. Accurate modeling of traffic demand through cellular traffic generation is crucial for optimizing base station deployment, enhancing network efficiency, and fostering technological innovation. In this paper, we introduce STOUTER, a spatio-temporal diffusion model for cellular traffic generation. STOUTER incorporates noise into traffic data through a forward diffusion process, followed by a reverse reconstruction process to generate realistic cellular traffic. To effectively capture the spatio-temporal patterns inherent in cellular traffic, we pre-train a temporal graph and a base station graph, and design the Spatio-Temporal Feature Fusion Module (STFFM). Leveraging STFFM, we develop STUnet, which estimates noise levels during the reverse denoising process, successfully simulating the spatio-temporal patterns and uncertainty variations in cellular traffic. Extensive experiments conducted on five cellular traffic datasets across two regions demonstrate that STOUTER improves cellular traffic generation by 52.77% in terms of the Jensen-Shannon Divergence (JSD) metric compared to existing models. These results indicate that STOUTER can generate cellular traffic distributions that closely resemble real-world data, providing valuable support for downstream applications.},
  archive      = {J_TMC},
  author       = {Xiaosi Liu and Xiaowen Xu and Zhidan Liu and Zhenjiang Li and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3591183},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatio-temporal diffusion model for cellular traffic generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel integrated sensing and communication scheme in UAVs-enabled vehicular networks with MARL-driven adaptive control. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3591259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel integrated sensing and communication (ISAC) scheme tailored for UAVs-enabled vehicular networks, which leverages the information coverage capabilities of multiple UAVs and addresses critical challenges posed by multiple moving users. Unlike many traditional scheme, our scheme efficiently leverages ISAC signal echoes and real-time data uploads to provide communication services while achieving accurate sensing, thereby overcoming issues of resource waste and low operational efficiency. In the scheme, we aim to optimize both communication and sensing indicators, taking into account practical issues such as energy saving and collision avoidance for UAVs. However, the inherent complexity of multi-objective stochastic optimization in dynamic environments and limited communication resources render centralized UAV control inconvenient. To address the above challenges, we propose a novel multi-agent reinforcement learning (MARL) algorithm based on local information to realize the distributed adaptive control of motion decision, power selection, and channel allocation for UAVs. The algorithm combines random network distillation (RND) and dynamic data augmentation with multi-agent deep deterministic policy gradient (MADDPG) to encourage agents to explore effectively under sparse rewards and improve MADDPG's policy learning ability in finite data, thus approaching the global optimal solution. Experimental results demonstrate that the proposed algorithm can improve communication and sensing performance by more than 16.71% and 68.26% compared with other baselines and satisfy the set constraints. Furthermore, by adjusting hyperparameters, we can optimize the ISAC performance while achieving different energy savings levels for UAVs, proving that the designed scheme can reduce the waste of resources and improve the ISAC operation efficiency.},
  archive      = {J_TMC},
  author       = {Ziyuan Wang and Xiao-Ping Zhang and Wenbo Ding and Yuhan Dong and Xinlei Chen},
  doi          = {10.1109/TMC.2025.3591259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel integrated sensing and communication scheme in UAVs-enabled vehicular networks with MARL-driven adaptive control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeOAR: Real-time online action recognition on edge devices. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenges of Online Action Recognition (OAR), a framework that involves instantaneous analysis and classification of behaviors in video streams. OAR must operate under stringent latency constraints, making it an indispensable component for real-time feedback for edge computing. Existing methods, which typically rely on the processing of entire video clips, fall short in scenarios requiring immediate recognition. To address this, we designed EdgeOAR, a novel framework specifically designed for OAR on edge devices. EdgeOAR includes the Early Exit-oriented Task-specific Feature Enhancement Module (TFEM), which comprises lightweight submodules to optimize features in both temporal and spatial dimensions. We design an iterative training method to enable TFEM learning features from the beginning of the video. Additionally, EdgeOAR includes an Inverse Information Entropy (IIE) and Modality Consistency (MC)-driven fusion module to fuse features and make better exit decisions. This design overcomes the two main challenges: robust modeling of spatio-temporal action representations with limited initial frames in online video streams and balancing accuracy and efficiency on resource-constrained edge devices. Experiments show that on the UCF-101 dataset, our method EdgeOAR reduces latency by 99.23% and energy consumption by 99.28% compared to state-of-the-art (SOTA) method. And achieves an adequate accuracy on edge devices.},
  archive      = {J_TMC},
  author       = {Wei Luo and Deyu Zhang and Yin Tang and Fan Wu and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3591188},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeOAR: Real-time online action recognition on edge devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chirp-level information-based collaborative key generation for LoRa networks via perturbed compressed sensing. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical-layer key generation holds significant potential in establishing cryptographic key pairs for emerging LoRa networks. Nevertheless, current key generation solutions may underperform due to critically impaired channel reciprocity, attributed to the low data rate and long range inherent in LoRa networks. In this study, we present ChirpKey, a novel key generation scheme for LoRa networks. We pinpoint the key hurdles as the coarse-grained channel measurement, inefficient quantization methods, and out-of-range device constraints. To capture fine-grained channel information, we introduce a unique, LoRa-specific channel measurement method that focuses on analyzing chirp-level variations in LoRa packets. We also propose a LoRa channel state estimation algorithm to neutralize asynchronous channel sampling. Instead of the traditional quantization approach, we propose an innovative key delivery method based on perturbed compressed sensing, offering enhanced robustness and security. For LoRa devices beyond each other's communication reach, we integrate relay nodes to ensure reliable key generation. To foster secure group communication, we formulate two protocols that facilitate collaborative key generation across both star and chain configurations. Evaluation across diverse real-world scenarios reveals that ChirpKey enhances the key matching rate by 11.03–26.58% and increases the key generation rate by 27–49× in comparison to existing leading systems. Our security analysis shows that ChirpKey can effectively withstand a variety of prevalent attacks. Furthermore, we implement a ChirpKey prototype, demonstrating its capability to operate within 0.2 s.},
  archive      = {J_TMC},
  author       = {Huanqi Yang and Zehua Sun and Hongbo Liu and Xianjin Xia and Yu Zhang and Tao Gu and Gerhard Hancke and Weitao Xu},
  doi          = {10.1109/TMC.2025.3591298},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Chirp-level information-based collaborative key generation for LoRa networks via perturbed compressed sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end coordinated spatio-temporal redundancy elimination for fast video analytics. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3591307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge video analytics typically rely on conventional encoding standards to transmit device visual data for server-side inference. Unfortunately, general-purpose compression solutions retain unnecessary visual data that does not contribute to accuracy, resulting in significant latency throughout Video Analytics Pipeline (VAP). While previous approaches have made partial progress, they cannot systematically eliminate VAP redundancy due to uncoordinated subsystem-level optimization. Achieving complete redundancy elimination presents a major challenge, as a lack of spatio-temporal coordination risks offsetting latency gains with computational overhead (associated with redundancy elimination). Crucio overcomes these limitations with an end-to-edge framework that integrates temporally adaptive frame filtering and coordinated video compression. It leverages redesigned asymmetric autoencoders to synchronize inter-frame temporal compression with intra-frame spatial feature extraction. Additionally, Crucio employs a one-pass decoding mechanism for encoded critical frames and dynamically adjusts batching scales to minimize latency. Empirical results demonstrate Crucio's superiority, outperforming existing solutions (e.g., DDS, Reducto, and STAC) by over a 31% reduction in end-to-end latency at 0.9 accuracy thresholds.},
  archive      = {J_TMC},
  author       = {Andong Zhu and Sheng Zhang and Lingkun Meng and Xiaohang Shi and Xiangyu Li and Dongxu Wang and Ke Cheng and Hesheng Sun and Sanglu Lu and Jie Wu and Yu Liang},
  doi          = {10.1109/TMC.2025.3591307},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-to-end coordinated spatio-temporal redundancy elimination for fast video analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified diffusion framework for traffic imputation and prediction with physical priors. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3591423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread occurrence of missing data in traffic sensor networks critically undermines the performance of intelligent transportation systems. Existing data-driven models often fail to accurately capture dynamic spatiotemporal dependencies, particularly in highly heterogeneous road environments. To address this issue, this paper presents a diffusion-based framework with physically consistent priors for modeling spatiotemporal traffic data (DCDM), which integrates physical constraints to improve the robustness and interpretability of the imputation and forecasting processes. Specifically, we develop a spatio-temporal feature extractor grounded in the infiltration principle of Fick's law to model directional flow dynamics between adjacent road nodes. To mitigate the impact of missing data at critical nodes, we introduce a global information compensation mechanism that enhances the denoising process by capturing long range spatiotemporal dependencies. The proposed model jointly optimizes data imputation and prediction tasks within a unified diffusion framework. Extensive experiments on three real-world datasets demonstrate that DCDM consistently outperforms state-of-theart methods under various missing data conditions, achieving superior reconstruction accuracy and predictive stability. Moreover, statistical tests and visualization results further confirm the model's robustness, interpretability, and strong generalization ability in generating high-quality data that closely matches the true distribution.},
  archive      = {J_TMC},
  author       = {Peng Liu and Yaodong Zhu and Yang Yang and Caixia Wang and Jingfeng Jie},
  doi          = {10.1109/TMC.2025.3591423},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A unified diffusion framework for traffic imputation and prediction with physical priors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedGraft: Memory-aware heterogeneous federated learning via model grafting. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3591537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Federated Learning (FL) is good at collaborative learning among devices without compromising their data privacy, it suffers from the problem of large-scale deployment in Mobile Edge Computing (MEC) applications. This is mainly because the varying memory sizes of edge devices inevitably result in limited sizes of their hosting models. According to the Cannikin Law, when dealing with heterogeneous devices with different memory sizes, the learning capability of existing homogeneous FL schemes is greatly restricted by the weakest device. Worse still, although existing heterogeneous FL methods enable a MEC application to involve numerous devices equipped with heterogeneous models, their knowledge aggregation processes require either extra training data or architecture similarity of models. To address the above issues, this paper presents a novel FL method named FedGraft that enables effective knowledge sharing among heterogeneous device models of different sizes without imposing unrealistic assumptions. In FedGraft, all the device models are grafted to a common rootstock based on our proposed model partitioning and grafting mechanism, facilitating knowledge sharing among heterogeneous models on top of a tree-like global model. Meanwhile, using our proposed device selection strategy, the reassembled submodels extracted from the global model can be reasonably dispatched to corresponding devices with sufficient memory, thus enhancing the overall FL performance. Comprehensive experimental results show that, compared with state-of-the-art heterogeneous FL methods, FedGraft can improve inference accuracy by up to 17% in various memory-constrained scenarios.},
  archive      = {J_TMC},
  author       = {Ruixuan Liu and Ming Hu and Zeke Xia and Xiaofei Xie and Jun Xia and Pengyu Zhang and Yihao Huang and Mingsong Chen},
  doi          = {10.1109/TMC.2025.3591537},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedGraft: Memory-aware heterogeneous federated learning via model grafting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing joint speed and altitude schedule for UAV data collection in low-altitude airspace. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-altitude airspace in major cities across the world is increasingly congested with unmanned aerial vehicles (UAVs) and other aircraft. Emerging technologies, innovative business models, and supportive government policies are driving the growth of the low-altitude economy, where UAVs play a crucial role. Given the limited on-board energy of UAVs, this paper investigates the Joint UAV Speed and Altitude Scheduling (JUSAS) problem for data collection from sensors deployed along power transmission lines, bridges, highways, railways, water/gas/oil pipelines, or rivers/coasts. Distinct from existing work, the paper focuses on jointly optimizing UAV speed and altitude scheduling while determining the wireless sensor collection order. It accounts for the altitude-specific sensor transmission range model and the complexities of overlapping range relationships. We first propose the Slowest Segment First (SSF) policy to obtain an optimal UAV speed scheduling for fixed-altitude scenarios. Building upon this, we then reformulate JUSAS as a shortest-path-type problem using our novel flight scheduling graph, solved efficiently through the SSF-based Ant Colony Optimization (SSF-ACO) algorithm. To handle practical scenarios without prior sensor information along the path, we develop SSF-ACO-Online for real-time scheduling. Extensive simulations demonstrate that SSF-ACO significantly outperforms four other algorithms (i.e., SSF-Only, SSF-GA, SSF-PSO, and SSF-SA) in energy efficiency, and reduces 13.11% energy consumption on average. SSF-ACO-Online achieves comparable performance with energy consumption 1.24% higher than offline counterpart in average.},
  archive      = {J_TMC},
  author       = {Yiqian Wang and Jianping Huang and Feng Shan and Yuming Gao and Runqun Xiong and Junzhou Luo},
  doi          = {10.1109/TMC.2025.3591698},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing joint speed and altitude schedule for UAV data collection in low-altitude airspace},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overlapping coalition game approach for collaborative block mining and edge task offloading in MEC-assisted blockchain networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a promising technology that enhances the efficiency of mobile blockchain networks, by enabling miners, often acted by mobile users (MUs) with limited computing resources, to offload resource-intensive mining tasks to nearby edge computing servers. Collaborative block mining can further boost mining efficiency by allowing multiple miners to form coalitions, pooling their computing resources and transaction data together to mine new blocks collaboratively. Therefore, an MEC-assisted collaborative blockchain network can leverage the strengths of both technologies, offering improved efficiency, security, and scalability for blockchain systems. While existing research in this area has mainly focused on the singlecoalition collaboration mode, where each miner can only join one coalition, this work explores a more comprehensive multicoalition collaboration mode, which allows each miner to join multiple coalitions. To analyze the behavior of miners and the edge computing service provider (ECP) in this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the ECP, as the leader, determines the prices of computing resources for all MUs. In Stage II, each MU decides the coalitions to join, resulting in an overlapping coalition formation (OCF) game; Subsequently, each coalition decides how many edge computing resources to purchase from the ECP, leading to an edge resource competition (ERC) game. We derive the closed-form Nash equilibrium for the ERC game, based on which we further propose an OCFbased alternating algorithm to achieve a stable coalition structure for the OCF game and develop a near-optimal pricing strategy for the ECP's resource pricing problem. Simulation results show that the proposed multi-coalition collaboration mode can improve the system efficiency by 12.64% ∼ 17.63%, compared to the traditional single-coalition collaboration mode.},
  archive      = {J_TMC},
  author       = {Licheng Ye and Zehui Xiong and Lin Gao and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3591822},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An overlapping coalition game approach for collaborative block mining and edge task offloading in MEC-assisted blockchain networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated knowledge distillation using hierarchical reinforcement learning in resource-constrained IoT edge-cloud computing environments. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Federated Learning (FL) in IoT Edge-Cloud Computing environments, mobile terminals are able to cooperate without the leakage on raw data. However, factors including the terminals' high mobility and the network fluctuations make the cooperator selection during FL training extremely complex. Under the distributed cooperation, traditional FL strategies show certain limitations and cannot always select the available nodes when training, leading to the difficulties in energy and latency optimization. In this paper, we propose a Hierarchical Reinforcement Learning (HRL)-based federated knowledge distillation (HRL-FedKD) framework in which both high-level and low-level controllers utilize the Double Deep Q-Network (DDQN) algorithm. The high-level controller selects the nodes participating in FL training, while the low-level controller determines the number of local training epochs for each node. After training, the global model will be compressed into a lightweight model by knowledge distillation (KD) in deployment while preserving the personalization of local models. The experiments were conducted using Chest X-Ray and Brain Tumor MRI datasets to validate the proposed FL strategy. The results demonstrate that the HRL-FedKD framework can effectively optimize latency and energy consumption in complex state spaces.},
  archive      = {J_TMC},
  author       = {Yishan Chen and Zhiqiang Wang and Huashuai Cai and Zhen Qin and Shuiguang Deng},
  doi          = {10.1109/TMC.2025.3591610},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated knowledge distillation using hierarchical reinforcement learning in resource-constrained IoT edge-cloud computing environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-trust based robust federated learning against betrayal behaviors. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its advantage of protecting data privacy and reducing communication overhead, Federated Learning (FL) is becoming a promising machine learning paradigm. However, resource limitations and unstable communication connections on the participating client end can lead to unintentional failures that degrade FL performance. Moreover, as FL systems scale and interconnect increasingly, they face growing exposure to intentional network risks. Furthermore, the assumption of continued trust in historically benign clients introduces vulnerabilities to potential internal betrayal within FL systems. In this paper, we enhance the robustness of FL by incorporating the zero-trust principle, which eliminates implicit trust in clients and mitigates unintentional failures, intentional attacks, and strategic betrayal risks. The framework incorporates dynamic client selection and aggregation weight allocation through trustworthiness evaluation and sustained skepticism toward each potential betrayal behavior. Specifically, a Dirichlet-based trust evaluation technique is presented to update clients' trustworthiness with evolving observations. Then, to reduce potential betrayal loss, we formulate a min-max optimization problem that minimizes the worst-case betrayal loss. Next, we transform the formulation into a convex programming problem for solution. Extensive simulations are conducted to demonstrate the efficacy of the zero-trust based FL in the accurate trust assessment and the system's betrayal-aware robustness enhancement.},
  archive      = {J_TMC},
  author       = {Xinran Zhang and Dan Wang and Yifei Zhu and Weilong Chen and Zheng Chang and Zhu Han},
  doi          = {10.1109/TMC.2025.3591632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Zero-trust based robust federated learning against betrayal behaviors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust lyapunov optimization for LEO satellite networks routing control. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellite networks are emerging as crucial components of space-air-ground integrated networks (SAGINs), extending beyond terrestrial capabilities to provide global data transmission services for the Internet of Things (IoT) and mobile devices. The proliferation of connected devices has led to increased data volumes and highly variable, bursty traffic patterns, thus posing significant challenges for network stability and necessitating effective routing control mechanisms. Traditional Lyapunov optimization methods have been fundamental in network optimization, offering stability guarantees under the assumption that traffic flows remain strictly within the network's capacity region. However, this assumption is often violated in LEO satellite networks due to their dynamic and bursty nature, thereby rendering conventional approaches inadequate for ensuring stability. To address this challenge, we propose a robust Lyapunov optimization framework tailored for LEO satellite networks. Our method relaxes the strict requirements of traditional Lyapunov optimization by allowing the network to tolerate finite violations of the capacity region while still ensuring overall system stability. This approach demonstrates that, for a stabilizable network system, it is not necessary for traffic to remain within the capacity region at every time slot. We validate the effectiveness of the proposed robust Lyapunov optimization through extensive simulations under various traffic conditions and LEO satellite network configurations. The results confirm that LEO satellite networks can maintain stability despite finite violations of the capacity region, ensuring reliable performance amid dynamic and bursty traffic demands.},
  archive      = {J_TMC},
  author       = {Zhemin Huang and Zhong-Ping Jiang and Zhu Han and Yong Liu},
  doi          = {10.1109/TMC.2025.3591766},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust lyapunov optimization for LEO satellite networks routing control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated split learning with improved communication and storage efficiency. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is one of the popular distributed machine learning (ML) solutions but incurs significant communication and computation costs at edge devices. Federated split learning (FSL) can train sub-models in parallel and reduce the computational burden of edge devices by splitting the model architecture. However, it still requires a high communication overhead due to transmitting the smashed data and gradients between clients and the server in every global round. Furthermore, the server must maintain separate partial models for every client, leading to a significant storage requirement. To address these challenges, this paper proposes a novel communication and storage efficient federated split learning method, termed CSE-FSL, which utilizes an auxiliary network to locally update the weights of the clients while keeping a single model at the server, hence avoiding frequent transmissions of gradients from the server and greatly reducing the storage requirement of the server. Additionally, a new model update method of transmitting the smashed data in selected epochs can reduce the amount of smashed data sent from the clients. We provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its convergence under non-convex loss functions. The extensive experimental results further indicate that CSE-FSL achieves a significant communication reduction over existing FSL solutions using real-world FL tasks.},
  archive      = {J_TMC},
  author       = {Yujia Mu and Cong Shen},
  doi          = {10.1109/TMC.2025.3591744},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated split learning with improved communication and storage efficiency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS prediction for component services in 5 g via graph-based deep reinforcement learning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3591783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of Quality of Service (QoS) in terms of response time, packet loss rate, latency and throughput for component services is essential for 5 G to fulfill specific Service Level Agreements (SLAs). However, most current efforts failed to fully exploit the time-varying mobility features of users and parallel iteration multi-rules to perform QoS prediction for component services, incurring poor prediction accuracy. Therefore, in this paper, we are devoted to accurate QoS Prediction of Component Services (QPCS) for 5 G via Graph-based Deep Reinforcement Learning (GDRL) to tackle this issue. Towards this end, the QoS prediction is modeled as GDRL-based QoS tensor factorization by designing a Spatio-Temporal-Recurrent-based Graph Attention Network (STR-GAT) and introducing it into Deep Deterministic Policy Gradient (DDPG) to factorize QoS tensor with multiple available rules in parallel. Specifically, a low-rank QoS tensor and an adjacency tensor are established, which include partial QoS observations of component services in each Base Station (BS), along with some missing elements, and evolving spatial information of users across these BSs, respectively. Then, the novel STR-GAT is designed by introducing spatio-temporal relations into conventional GAT to fully derive the mobility features of users to explore potential actions, while the derivative DDPG is adopted to perform tensor factorization with multiple available rules in parallel. Furthermore, the action smoothing and hierarchical-based replay buffer with priority-based and random sampling are designed and introduced into DDPG to stabilize training process and accelerate model convergence. Experimental simulation results on real-world datasets validate the superiorities of QPCS compared with the state-of-the-art approaches in predicting the QoS of component services in 5 G.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Qifan Wang and Geyong Min and Miao Wang and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2025.3591783},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS prediction for component services in 5 g via graph-based deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAFE: Towards practical WiFi localization via continuous angle focusing effect. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3591919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based indoor localization serves as a critical foundation for numerous real-world applications and has attracted widespread attentions over the past decade. Recent advancements have demonstrated the feasibility of achieving decimeter-level accuracy by leveraging Angle of Arrival (AoA) information. However, existing commercial WiFi Access Points (APs) suffer from phase offset across different antennas, which significantly degrade the performance of AoA-based methods. Previous works either relied on labor-intensive manual calibration or involved inaccurate and non-robust automatic calibration, which hinders their widespread use in large-scale deployments. Moreover, to expand the signal coverage and enhance communication performance, the inter-antenna spacing in existing commercial APs typically exceeds the standard half-wavelength. The resulting angle ambiguity problem can mislead target detection results, which has not been well resolved in existing works. To address the above two practical challenges, in this paper, we propose CAFE, a practical WiFi indoor localization system based on the Continuous Angle Focusing Effect. The key insight lies on the fact that the angle information of multiple APs originates from the same client, and thus exhibits highly convergent properties in both the temporal and spatial dimensions. By further exploring the binary nature of phase offset and the periodicity of grating lobes, our approach can efficiently resolve the above two practical challenges. Extensive experiments are provided to demonstrate the effectiveness of the proposed CAFE system, which outperforms state-of-the-art methods by $22.1\%$ in median localization error for simple scenarios and by $37.1\%$ for complex multipath scenarios.},
  archive      = {J_TMC},
  author       = {Shuai Yang and Pengfei Yin and Guanzhong Wang and Tianyu Zhang and Dongheng Zhang and Yan Chen},
  doi          = {10.1109/TMC.2025.3591919},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAFE: Towards practical WiFi localization via continuous angle focusing effect},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing operator intent for haptic teleoperation under high latencies. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3591197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic teleoperation is a promising technology with applications in telemaintenance and disaster management. However, it faces significant challenges when the application is subjected to a high network latency and environments with moving objects. This work aims to extend Model Mediated Teleoperation (MMT) to overcome challenges in supporting dynamic environments. Instead of striving for perfect model alignment, we acknowledge the inevitable mismatch between the remote environment and its model at the operator. We propose a set of design principles and an accompanying framework for designing MMT solutions that prioritize operator intent. Our approach is exemplified through an application where an operator, located 8000 km away (The Netherlands – India) and subjected to an average of 179 ms end-to-end latency, guides a robot arm to draw on a whiteboard whose position is actively altered. We evaluate the effectiveness of our approach through a user study. We show a 3-point improvement on a 7-point Likert scale when users utilize our approach to teleoperate over significant network latency of up to 1 second.},
  archive      = {J_TMC},
  author       = {H.J.C. Kroep and P. Makridis and J. Huidobro and K. Wösten and D. Choudhary and N. Gnani and T.V. Prabhakar and S. Coppens and K. Van Berlo and R.Venkatesha Prasad},
  doi          = {10.1109/TMC.2025.3591197},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Utilizing operator intent for haptic teleoperation under high latencies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RFAR: Action recognition based on single tag. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition in classrooms has recently become a research hotspot. Traditional solutions usually rely on sensors or computer vision methods. However, these methods have some disadvantages, such as difficulty in deployment, susceptibility to ambient light, and privacy and security issues. This paper proposes RFAR, a contactless method for classroom action recognition. This method utilizes an RFID tag placed on the desktop to capture various actions and subsequently evaluate the student's learning status. To enhance the reliability of singletag identification, fused data consisting of two or three types of data sequences (RSSI, phase, and Doppler shift) are incorporated. Furthermore, a dynamic antenna system is utilized to identify the optimal angle for tag-antenna alignment. Notably, the single-tagper-person design eliminates severe interference among multiple tags and simplifies device deployment in multi-person scenarios. This method is proposed based on COTS RFID devices and shows high robustness across different environments and equipment. Experimental results show a recognition accuracy of 93.9% in single-person scenarios and 81.5% in five-person scenarios.},
  archive      = {J_TMC},
  author       = {Zhanjun Hao and Jiang Zhang and Xiang Li and Yuejiao Wang and Guowei Wang and Fenfang Li and Hao Liu and Chengrui Tao},
  doi          = {10.1109/TMC.2025.3592191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RFAR: Action recognition based on single tag},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic routing mechanism for load distribution in UAV swarm networks with edge caching. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3589569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of the UAV swarm network has made its widespread application across a multitude of domains. However, the inherently dynamic nature of the network often gives rise to intermittent connectivity issues, leading to a significant reduction in the data transmission capacity. To address this challenge, this study explores the integration of Information-centric Network (ICN) with the delay-tolerant network (DTN). This design aims to enhance message delivery rates by caching content data packets in UAV nodes. Building upon this architecture, we study the congestion control and load balancing problem. We design an on-demand collaborative communication routing algorithm. In our design, we first propose a routing decision model that incorporates multiple routing metrics to capture the dynamic evolution patterns of network nodes, effectively controlling local congestion issues. Subsequently, we employ Lyapunov optimization techniques to achieve a network load balancing. By integrating the Lyapunov drift function, we ensure the stability of a feasible solution space within the model. Additionally, considering the high communication overhead caused by the sparse communication characteristics of DTN, we deploy a Multi-Agent Incentivized Communication (MAIC) algorithm to optimize routing scheduling strategies. Within the MAIC framework, each agent develops unique models for its teammates to generate customized information and minimize network information redundancy. Simulation results demonstrate that this algorithm effectively ensures a congestion control and a load balancing within the UAV swarm network while maintaining communication overhead in routing computations at a minimal level.},
  archive      = {J_TMC},
  author       = {Qun Li and Zunliang Wang and Haipeng Yao and Tianle Mai and Zhipei Li and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3589569},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic routing mechanism for load distribution in UAV swarm networks with edge caching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeFL: Model-agnostic federated learning with generative models. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3592483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed training paradigm that enables collaborative learning across clients without sharing local data, thereby preserving privacy. However, the increasing scale and complexity of modern deep models often exceed the computational or memory capabilities of edge devices. Furthermore, clients may be constrained to use heterogeneous model architectures due to hardware variability (e.g., ASICs, FPGAs) or proprietary requirements that prevent the disclosure or modification of local model structures. These practical considerations motivate the need for model-heterogeneous FL, where clients participate using distinct model architectures. In this work, we propose Generative Model-Aided Federated Learning (GeFL), a framework that enables cross-client knowledge sharing via a generative model trained in a federated manner. This generative model captures global data semantics and facilitates local training without requiring model homogeneity across clients. While GeFL achieves strong performance, empirical analysis reveals limitations in scalability and potential privacy leakage due to generative sample memorization. To address these concerns, we propose GeFL-F, which utilizes feature-level generative modeling. This approach enhances scalability to large client populations and mitigates privacy risks. Extensive experiments across image classification tasks demonstrate that both GeFL and GeFL-F offer competitive performance in heterogeneous settings. Code is available at [1].},
  archive      = {J_TMC},
  author       = {Honggu Kang and Seohyeon Cha and Joonhyuk Kang},
  doi          = {10.1109/TMC.2025.3592483},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GeFL: Model-agnostic federated learning with generative models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divide and conquer: Advancing large-scale multi-agent pathfinding with hierarchical reinforcement learning. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3592410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-robot systems face the intricate multi-agent pathfinding (MAPF) challenge as a pivotal hurdle. It has been uncovered through recent research that tackling MAPF issues can be effectively approached through reinforcement learning, offering a fully decentralized solution. Nonetheless, the escalation in the scale of the multi-robot system introduces sample inefficiency, posing a significant barrier for learning-based methods. We introduce a novel hierarchical reinforcement learning architecture aimed at addressing large-scale MAPF by leveraging spatial and temporal abstraction. This approach enhances exploration efficiency by recognizing intermediate rewards. The framework employs an upper-tier controller that segments the map into linked regions, thereby streamlining the optimization of agents' paths on a regional basis to foster improved global outcomes. To tackle each segmented problem, a subordinate-level controller is designed, which integrates heuristic directions and an inter-agent communication strategy. The merit of our methodology is confirmed by empirical experiments, showcasing advancements over prevailing methods in success rates and reduction in completion time across test scenarios of various magnitudes.},
  archive      = {J_TMC},
  author       = {Bing Li and Kaixin Chen and Zhaoyi Song and Rongqing Zhang and Xiang Cheng},
  doi          = {10.1109/TMC.2025.3592410},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Divide and conquer: Advancing large-scale multi-agent pathfinding with hierarchical reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incentive mechanism for federated learning with time-varying client availability. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3592202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), distributed users collaboratively train a neural network model under the coordination of a central server. However, time-varying client availability, coupled with non-independent and non-identically distributed (non-IID) datasets, leads to a biased convergence. In this work, we prove the convergence of FL under time-varying client availability. The theoretical result shows that biased convergence occurs when available client distribution does not algin with the client population distribution. To address this challenge, we propose a pricing-based incentive mechanism to encourage clients to adjust their availability. First, we model the strategic interactions among clients as a non-cooperative game under an arbitrary pricing scheme. We prove that this game is a potential game and its equilibrium can be found through optimization. Second, we derive an optimal pricing scheme for large client populations and propose a bi-level optimization algorithm using Particle Swarm Optimization (PSO) for general scenarios. Through analysis of client availability evolution, we prove the effectiveness of our scheme in mitigating biased convergence. Experimental results using real-world client availability dataset show that our approach addresses time-varying client availability issue, achieving up to 99.5% improvement over benchmarks and enhancing FL convergence rates by up to 2.49 times.},
  archive      = {J_TMC},
  author       = {Shuo Wang and Bing Luo and Ming Tang},
  doi          = {10.1109/TMC.2025.3592202},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An incentive mechanism for federated learning with time-varying client availability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FluidEdge: Expediting serverless machine learning inference via bottleneck-aware auto-scaling on edge SoCs. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3592334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile applications based on machine learning (ML) are increasingly relying on offloading to the edge devices for low-latency, resource-efficient computation. Applying serverless computing for these ML applications on the edge offers a promising solution for handling dynamic workloads while meeting user-specified latency service-level objectives (SLOs). However, existing serverless frameworks, with their coarse-grained data parallelism and rigid model partitioning, are inadequate for ML inference on widely adopted edge System-on-Chip (SoC) devices. This paper presents FluidEdge, an edge-native serverless inference framework. FluidEdge identifies bottleneck operators in ML models and addresses them through a novel fine-grained intra-function latency-sensitive auto-scaling approach that dynamically scales inference bottlenecks during online serving. Additionally, it employs inter-function scaling to further prevent latency SLO violations and leverages the unified memory of edge SoCs for efficient data sharing during inference. Experimental results demonstrate that FluidEdge achieves a 37.4% latency improvement and 67.3%-87.6% SLO violation reduction compared to best-performed state-of-the-art serverless inference frameworks.},
  archive      = {J_TMC},
  author       = {Borui Li and Tiange Xia and Weilong Wang and Jingyuan Zhang and Shuai Wang and Chenhong Cao and Zheng Dong and Shuai Wang},
  doi          = {10.1109/TMC.2025.3592334},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FluidEdge: Expediting serverless machine learning inference via bottleneck-aware auto-scaling on edge SoCs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Throughput-aware cooperative task offloading in dynamic mobile edge computing systems. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3592450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the commercialization of fifth-generation (5G) mobile communication technology and the rapid proliferation of mobile devices (MDs), demand for data computation is surging. This growth increases the reliance of MDs on low latency and high throughput. For this purpose, Mobile Edge Computing (MEC) enhances the user's data processing capability by offloading computation tasks to servers at the network edge. However, achieving high efficiency in task offloading is challenging due to factors such as decision complexity, network dynamics, and user data privacy protection. Additionally, energy causal constraints and the coupling between offloading proportions and resource distribution cannot be ignored. In this paper, we first establish a dynamic task offloading problem to optimize the long-term throughput of the system. Using perturbed Lyapunov optimization, we transform MD delay and energy threshold constraints into the stability control of corresponding virtual queues. Then, we propose the Lyapunov-guided federated deep reinforcement learning (DRL) online task offloading algorithm called LyFOTO, which combines a federated learning (FL) framework and an Actor-Critic (AC) model. Under favorable communication conditions, the LyFOTO algorithm adaptively boosts system throughput; under poorer conditions, it properly delays task offloading, without violating queue backlog constraints. Through mathematical analysis, we discuss the performance of the LyFOTO algorithm. Simulation experiments validate that LyFOTO effectively balances system throughput and device battery energy. Finally, Comparative results show that LyFOTO outperforms other benchmark algorithms in maximizing system throughput while ensuring task backlog and energy threshold constraints.},
  archive      = {J_TMC},
  author       = {Longbao Dai and Fanzi Zeng and Haoran Kong and Jianghao Cai and Hongbo Jiang and Keqin Li},
  doi          = {10.1109/TMC.2025.3592450},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Throughput-aware cooperative task offloading in dynamic mobile edge computing systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Squeezer: Efficient multi-DNN inference for edge video analytics via cross-model scheduling. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analytics at the edge is becoming increasingly prevalent in many scenarios, such as smart campuses and intelligent factories. These applications often consist of multiple subtasks, which necessitates the optimization for multi-DNN (Deep Neural Network) inference. Due to limited consideration over cross-model scheduling, current practices cannot fully leverage available computing resources, leading to suboptimal performance. To address this, we propose Squeezer, a multiDNN serving framework that holistically schedules multiple DNN models on an edge server with a single GPU. Squeezer decouples the cross-model scheduling into a two-layered approach, which involves (1) balanced operator grouping which partitions operators of multiple DNN models into groups, significantly reducing the scheduling complexity and (2) kernel scheduler which orchestrates parallel execution within each group by considering the interplay among kernels running in parallel, thereby enabling cross-model optimizations in multi-DNN inference. Performance evaluation results demonstrate that Squeezer outperforms state-of-the-art baselines, achieving up to 1.91× improvement in system throughput.},
  archive      = {J_TMC},
  author       = {Xiang Wang and Lingxiao Ma and Ziyan Fu and Xiangyu Li and Yuanchun Li and Ju Ren and Yaoxue Zhang and Yunxin Liu},
  doi          = {10.1109/TMC.2025.3592647},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Squeezer: Efficient multi-DNN inference for edge video analytics via cross-model scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing communication and device clustering for clustered federated learning with differential privacy. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3592885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.},
  archive      = {J_TMC},
  author       = {Dongyu Wei and Xiaoren Xu and Shiwen Mao and Mingzhe Chen},
  doi          = {10.1109/TMC.2025.3592885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing communication and device clustering for clustered federated learning with differential privacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing age of semantic information for analytics-oriented video streaming systems. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video streaming systems are critical for intelligent applications to transmit video data from end devices to servers for real-time analysis. In contrast to traditional humancentric streaming systems, which prioritize user-perceived metrics, machine-centric streaming systems are designed to continuously provide fresh and accurate information for analytics purposes. Although numerous studies have investigated policies to optimize streaming performance, most of them employ the segment-by-segment streaming framework from human-centric systems. Through comprehensive theoretical analysis and experimentation, we uncover that the segmented streaming approach is sub-optimal for machine-centric streaming systems compared to the straightforward frame-by-frame streaming approach. Furthermore, instead of relying on conventional frame-level metrics, we introduce a novel metric called the Age of Semantic Information (AoSI) to evaluate the performance of analyticsoriented streaming systems. This metric balances the quantity and timeliness of the semantic information. Consequently, we propose a compression ratio adaption method tailored to optimize AoSI performance for frame-by-frame streaming systems. This method leverages a deep learning (DL)-based predictor to discover the dynamic, latent relationships between compression and inference accuracy. Evaluated on actual streaming prototypes and real-world datasets, our method significantly surpasses both segmented and frame-by-frame baseline methods in terms of worst-case and average AoSI performance.},
  archive      = {J_TMC},
  author       = {Ziyao Huang and Weiwei Wu and Kui Wu and Guanyu Gao and Jianping Wang},
  doi          = {10.1109/TMC.2025.3588474},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Minimizing age of semantic information for analytics-oriented video streaming systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain mmWave gesture recognition via parameter-free attention under human activity interference. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3592959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition provides an effective human-computer interaction that makes device control more intuitive and convenient. Although the research on mmWave radar-based gesture recognition has demonstrated promising results, existing studies have exclusively addressed the cross-domain challenge or the human activity interference problem, and no attention has been paid to the cross-domain problem in the presence of human activity interference. To address these issues, we propose a novel mmWave radar-based gesture recognition system, named GestSAM, which leverages a parameter-free attention mechanism to effectively extract gesture features that are less affected by environmental noise. By integrating this mechanism with deep learning techniques, GestSAM significantly reduces the impact of human activity interference while maintaining robust cross-domain gesture recognition performance. This approach ensures robust, high-accuracy recognition of gestures. In order to evaluate the performance of our system, we construct a dataset containing six different gesture types performed by fifteen volunteers in seven different scenarios and simulate three interference conditions. The experimental results show that under human activity interference, the model achieves average recognition accuracies of 92.79% and 94.62% in cross-user and cross-scenario, respectively.},
  archive      = {J_TMC},
  author       = {Yunyi Li and Yang Yang and Lian Xiao and Shuai Wang and Linqing Gui and Fu Xiao},
  doi          = {10.1109/TMC.2025.3592959},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cross-domain mmWave gesture recognition via parameter-free attention under human activity interference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced semi-supervised learning for WiFi gesture recognition via dynamic threshold-based spatio-temporal attention networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3592965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi sensing advancements facilitate the capture of human gestures from wireless signals, ensuring both privacy preservation and robustness under low-light conditions. Deep learning-based WiFi Human Gesture Recognition (HGR) demonstrates remarkable performance in handling complex gestures. To reduce labeling efforts, recent years have seen the emergence of semi-supervised WiFi HGR, leveraging massive amounts of unlabeled data. However, existing semi-supervised schemes often assume a balanced class distribution and utilize a fixed threshold for selecting pseudo-labels of unlabeled samples, leading to low performance for minority classes and decreased model generalization on real-world imbalanced datasets. To address this issue, we propose a novel semi-supervised WiFi HGR approach with dynamic pseudo-labeling thresholds to handle imbalanced class distribution, incorporating Spatial-Temporal Attention (STA) networks. Unlike using a fixed threshold for all unlabeled samples, our design implements class-independent thresholds for different classes, dynamically adjusting them by encoding pseudo-label distribution during training. To emphasize critical features in informative areas within the WiFi signals, we incorporate both spatial self-attention and temporal attention mechanisms to dynamically learn salient features and identify pivotal frames, respectively. Moreover, we introduce adaptive WiFi data augmentations that propel the semi-supervised framework and enhance model robustness. Experimental results on the Widar3.0 dataset reveal that our approach outperforms existing semi-supervised methods by large margins in accuracy, effectively mitigating imbalanced bias and enhancing model generalization. The code is publicly at https://github.com/onlinehuazai/Semi-Fi.},
  archive      = {J_TMC},
  author       = {Qihua Feng and Chunhui Duan and Jiawei Xue and Chaozhuo Li and Feiran Huang and Xi Zhang and Jian Weng and Philip S. Yu},
  doi          = {10.1109/TMC.2025.3592965},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Imbalanced semi-supervised learning for WiFi gesture recognition via dynamic threshold-based spatio-temporal attention networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of information in a fully-prioritized network. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3592939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the age of information (AoI) in a network consisting of multiple information streams with different priorities sharing a common server. In this network, the transmission of a packet is interrupted whenever a higher priority packet arrives. Regarding the behavior of AoI, for each stream, we consider a single buffer to enqueue the incoming packets based on a quasi-blocking (QB) policy. With the assumption of Poisson packet arrivals, we formulate the AoI and PAoI moment generating functions for each stream, while no assumption is considered for the service times of the packets. We also introduce a new semantic queueing policy. In this respect, we define the semantic (i.e., significance) of a packet as a linear combination of its age and remaining transmission time. When this metric is lower for a packet, the packet is more significant. In this policy, the decision to replace the available packet of a stream with a new arriving one is made based on the significance of these packets. This decision minimizes the sum of the next local peak and end-to-end delay in the AoI function. We investigate the superiority of this policy to the traditional ones in our numerical results.},
  archive      = {J_TMC},
  author       = {Sepehr Asvadi and Farid Ashtiani},
  doi          = {10.1109/TMC.2025.3592939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of information in a fully-prioritized network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time MU-MIMO beamforming with limited channel samples in 5G networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3592929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MU-MIMO beamforming is a key technology for 5G networks, relying on Channel State Information (CSI). However, in practice, the estimated CSI in reality is prone to uncertainty. Further, a MU-MIMO beamforming solution must be derived within a millisecond to be useful for real-time 5G applications. We present ReDBeam—a real-time data-driven beamforming solution for MU-MIMO using limited CSI data samples. The main novelties of ReDBeam are a parallel algorithm and an optimized GPU implementation. ReDBeam delivers a MU-MIMO beamforming solution within 1 millisecond to meet the probabilistic data rate requirements from the users, and minimize a base station's power consumption. Through extensive experiments, we show that ReDBeam consistently meets the stringent 1- millisecond real-time requirement and is orders of magnitude faster than other state-of-the-art algorithms. ReDBeam conclusively demonstrates that MU-MIMO beamforming with data rate requirements can be achieved in real-time using only limited CSI data samples.},
  archive      = {J_TMC},
  author       = {Shaoran Li and Nan Jiang and Chengzhang Li and Shiva Acharya and Yubo Wu and Weijun Xie and Wenjing Lou and Y. Thomas Hou},
  doi          = {10.1109/TMC.2025.3592929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time MU-MIMO beamforming with limited channel samples in 5G networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal planning for heterogeneous smart radio environments. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Radio Environment (SRE) is a central paradigm in 6G and beyond, where integrating SRE components into the network planning process enables optimized performance for high-frequency Radio Access Network (RAN). This paper presents a comprehensive planning framework utilizing realistic urban scenarios and channel models to analyze diverse SRE components, including Reconfigurable Intelligent Surface (RIS), Network-Controlled Repeater (NCR), and advanced technologies like Simultaneous Transmitting and Reflecting RIS (STAR-RIS) and Trisectoral NCR (NCR). We propose two optimization strategies— Full Coverage Minimum Cost (FCMC) and Maximum Budget-Constrained Coverage (MBCC)—that address key cost and coverage objectives by considering both physical characteristics and scalable costs of each component, influenced by factors such as NCR amplification gain and RIS dimensions. Extensive numerical results demonstrate the significant impact of these models in enhancing network planning efficiency for high-density urban environments.},
  archive      = {J_TMC},
  author       = {Reza Agahzadeh Ayoubi and Eugenio Moro and Marouan Mizmizi and Dario Tagliaferri and Ilario Filippini and Umberto Spagnolini},
  doi          = {10.1109/TMC.2025.3593191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimal planning for heterogeneous smart radio environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency minimization for movable relay-aided D2D-MEC communication systems. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-to-device (D2D)-aided mobile edge computing (MEC) has emerged as a key enabling technology for future sixth-generation (6G) wireless networks. The goal of D2D-MEC is to reduce system latency for edge user equipments (UEs) by enabling access to cloud computing capabilities at the network edge, thereby supporting high transmission rates. To address the vulnerability of communication signals to physical obstructions, we employ relay techniques to enhance system performance and extend coverage. However, relay nodes and base station (BS) are typically equipped with large-scale antenna arrays, which lead to significant implementation costs and limiting practical deployment. To address this issue in a cost-efficient manner without sacrificing system performance, movable antenna (MA) technology is introduced. The key idea of MA technology lies in dynamically optimizing antenna positions to improve system capacity. Therefore, we propose a novel resource allocation framework for an movable relay-aided D2D-MEC system. The proposed scheme jointly optimizes the MA positions at UEs, relays, and the BS, along with the associated beamforming vectors, MEC server resource allocation, and computational task offloading rates. The objective is to minimize the maximum system latency while satisfying both computation and communication rate constraints. Furthermore, considering that current MA control mechanisms primarily rely on mechanical actuation, MA movement delay is incorporated into the latency model to capture the trade-off between antenna mobility and system delay. The resulting optimization problem is non-convex and involves multiple coupled variables. To solve this problem, we develop a parallel and distributed algorithm based on the penalty dual decomposition (PDD) framework, which is further integrated with the successive convex approximation (SCA) method to obtain a suboptimal solution. Simulation results demonstrate that the proposed algorithm significantly reduces system latency and enhances overall efficiency compared to benchmark schemes employing conventional fixed-position antennas (FPAs) at the relays and BS.},
  archive      = {J_TMC},
  author       = {Yue Xiu and Yang Zhao and Ran Yang and Huimin Tang and Long Qu and Maurice Khabbaz and Chadi Assi and Ning Wei},
  doi          = {10.1109/TMC.2025.3593263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Latency minimization for movable relay-aided D2D-MEC communication systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time scheduling of CPU/GPU heterogeneous tasks in dynamic IoT systems: Enhancing GPU and memory efficiency. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time processing of large-scale, heterogeneous tasks—including CPU-only, general-purpose GPU, and specialized GPU tasks—poses significant challenges in Internet of Things (IoT) systems, driven by severe GPU resource fragmentation, inefficient CPU and memory resource utilization on edge servers. These issues often compromise system processing performance and server stability. To address these issues, we formulate a multi-stage mixed-integer nonlinear programming (MINLP) model, to jointly optimize GPU fragmentation rate and system processing capability. We then introduce a novel deviation-based Lyapunov optimization framework that explicitly maintains memory utilization around a predefined optimal threshold, effectively balancing resource usage and system stability. Finally, to achieve real-time decision-making for massive tasks in dynamic systems with randomly arriving tasks, we propose the MA-LHTO algorithm, a multi-agent deep reinforcement learning approach that incorporates a multi-head architecture, entropy-based exploration, and a parameter reset mechanism. Experimental results confirm that our algorithm significantly improves resource utilization, and exhibits good performance under various working conditions.},
  archive      = {J_TMC},
  author       = {Xiao He and Shanchen Pang and Sibo Qiao and Haiyuan Gui and Shihang Yu and Joel J. P. C. Rodrigues and Shahid Mumtaz and Zhihan Lyu},
  doi          = {10.1109/TMC.2025.3593250},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time scheduling of CPU/GPU heterogeneous tasks in dynamic IoT systems: Enhancing GPU and memory efficiency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RL-driven distributed on-orbit sparse coding for mobile space situational awareness. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3593228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space Situational Awareness (SSA) relies on Low Earth Orbit (LEO) satellites to capture continuous, highresolution imagery critical for identifying space threats. The vast volume of SSA images overwhelms satellite network throughput, hindering timely transmission and processing. This paper presents a reinforcement learning (RL)-driven distributed sparse coding framework that integrates novel compression algorithms with orbital RL to address these challenges. First, we introduce an Aggregated Dictionary Learning (ADL) algorithm and a Context-aware Adaptive Binary Arithmetic Coding (CABAC) algorithm, achieving a 93.78% compression ratio by exploiting the high sparsity and spatiotemporal redundancy of SSA images. Second, the proposed compression workflow is deployed across LEO satellites in a distributed manner, where both overlapping and non-overlapping regions of images are dynamically partitioned and processed in parallel to optimize resource utilization and reduce latency. An Orbital Double Deep Q-Network (DQN) framework is proposed to optimize task offloading decisions by (1) integrating orbital dynamics into the state space, and (2) adaptively partitioning images based on visible LEO resources. Evaluations demonstrate that our framework achieves 100% task completion under visibility constraints and a 51.61% reduction on CPU and RAM occupation time compared to centralized processing.},
  archive      = {J_TMC},
  author       = {Yutong Liu and Haiming Jin and Yunxiang Chen and Yinjie Wang Yao and Yimin Zhao and Linghe Kong and Lei Dong and Rui Li and Xiaoyang Liu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3593228},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RL-driven distributed on-orbit sparse coding for mobile space situational awareness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User context generation for large language models from mobile sensing data. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3591561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) exhibit remarkable capabilities in natural language understanding and generation. However, the accuracy of the inference depends deeply on the contexts of queries, especially for personal services. Abundant mobile sensing data collected by sensors embedded in smart devices can proactively capture real-time user contexts. However, raw sensing data are low-quality (e.g., existing missing data and data redundancy) and are incapable of providing accurate contexts. In this work, we present ConGen, a user context generation framework for LLMs, aiming at prompting users' contexts through their implicit mobile sensing information. ConGen integrates two components: refined data completion and multi-granularity context compression. Specifically, the refined data completion couples data-centric feature selection by leveraging the eXplainable AI (XAI) method into the data imputation model to generate fewer but more informative features for efficient and effective context generation. Additionally, we implement multi-granularity context compression, reducing timestep- and context-level data redundancy while further elevating context quality. Experiment results show that ConGen can generate more accurate context, surpassing competitive baselines by 1.3%-8.3% in context inference on all four datasets. Moreover, context compression significantly reduces redundancy to $1/70\sim 1/40$ of the original data amount, and further improves the context accuracy. Finally, the enhanced performance of LLMs, as demonstrated by both quantitative and qualitative evaluations of prompting ConGen-generated user contexts, underscores the effectiveness of ConGen.},
  archive      = {J_TMC},
  author       = {Rui Xing and Zhenzhe Zheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3591561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User context generation for large language models from mobile sensing data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FM-fi 2.0: Foundation model for cross-modal multi-person human activity recognition. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio-Frequency (RF)-based Human Activity Recognition (HAR) rises as a promising solution when low-light, obstructions, or privacy concerns render computer vision impractical. However, the scarcity of labeled RF data due to their non-interpretable nature poses a significant obstacle. Thanks to the recent breakthrough of foundation models (FMs), extracting deep semantic insights from unlabeled visual data become viable, yet these vision-based FMs fall short when applied to small RF datasets. To bridge this gap, we introduce FM-Fi 2.0, an innovative cross-modal framework engineered to translate the knowledge of vision-based FMs for enhancing RF-based, multi-person HAR systems. FM-Fi 2.0 first employs the intrinsic capabilities of FM and RF modality to associate both intra- and cross-modal features of each subject, while simultaneously filtering out irrelevant features to achieve better alignment between the two modalities. FM-Fi 2.0 also employs a cross-modal contrastive knowledge distillation mechanism, enabling an RF encoder to inherit the interpretative power of FMs for achieving zero-shot learning. The framework is further refined through metric-based few-shot learning techniques, aiming to boost the performance for predefined HAR tasks. Comprehensive evaluations evidently indicate that FM-Fi 2.0 rivals the effectiveness of vision-based methodologies, and the evaluation results provide empirical validation of FM-Fi 2.0's generalizability across various environments.},
  archive      = {J_TMC},
  author       = {Yuxuan Weng and Tianyue Zheng and Yanbing Yang and Jun Luo},
  doi          = {10.1109/TMC.2025.3593406},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FM-fi 2.0: Foundation model for cross-modal multi-person human activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Game theory and trust management driven dynamic proof-of-work blockchain consensus algorithm for securing internet of vehicles. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3592973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the dynamic Proof of Work (PoW) mechanism has attracted considerable attention as a promising consensus mechanism for implementing blockchain in the Internet of Vehicles (IoV) due to its superior scalability, security, and efficiency. However, existing protocols fail to address two critical issues, i.e., how to precisely quantify the relationship between dynamic difficulty and trust value and how to assess the motivation for mining in relation to mining costs. Addressing these issues is crucial for developing a secure, efficient, and adaptable dynamic PoW mechanism. To address these issues, we propose a trust management algorithm that assesses the trustworthiness of Roadside Units (RSUs) that function as consensus or mining nodes. Based on the results of this trust evaluation, a difficulty adjustment algorithm rooted in the Principal-Agent game theory was designed to implement a dynamic PoW mechanism for IoV. This approach leads to a Nash equilibrium between vehicles and RSUs, thereby ensuring the security, efficiency, and stability of the IoV blockchain system. Simulations demonstrated that the proposed dynamic PoW consensus algorithm significantly improves consensus efficiency, enhances system security, and reduces computational costs, offering a robust solution for data sharing in IoV.},
  archive      = {J_TMC},
  author       = {Lu Wei and Yuanzhi Cao and Jie Cui and Hong Zhong and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3592973},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Game theory and trust management driven dynamic proof-of-work blockchain consensus algorithm for securing internet of vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeMo: Experiences of deploying a large-scale indoor delivery monitoring system. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3588900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The delivery of goods to numerous indoor stores poses significant safety risks, with heavy, high-stacked packages on delivery trolleys posing a potential hazard to passersby. This paper reports our experiences of developing and operating DeMo, a practical system for real-time monitoring of indoor delivery. DeMo employs sensors attached to trolleys, utilizing Inertial Measurement Unit (IMU) and Bluetooth Low Energy (BLE) readings to detect delivery violations, such as speeding and the use of non-designated delivery paths, and ensure accurate matching of each delivery to its intended destination store. Unlike typical indoor localization applications, DeMo addresses unique challenges, including sensor placement and the complex electromagnetic characteristics encountered in underground settings. Specifically, DeMo adapts the classical logarithmic radio signal model to facilitate fingerprint-free localization, significantly reducing deployment and maintenance costs. DeMo has been operating since May 2020, covering more than 200 shops with 74,537 deliveries (6193.2 km) across 12 subway stations in Hong Kong. DeMo's 4-year operation witnessed a significant violation rate drop, from 19% (May 2020) to 0.9% (Mar 2024).},
  archive      = {J_TMC},
  author       = {Xiubin Fan and Zhongming Lin and Yuming Hu and Zhiqing Hong and Tianrui Jiang and Feng Qian and Zhimeng Yin and S.-H. Gary Chan and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2025.3588900},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeMo: Experiences of deploying a large-scale indoor delivery monitoring system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and fast authentication protocol for digital healthcare services. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the Internet of Medical Things (IoMT) and cloud computing, ensuring secure communication in e-health systems has become increasingly critical. However, many existing authentication solutions suffer from excessive overhead and security vulnerabilities. To address these challenges, we present a lightweight, high-speed authentication protocol that relies on secure hash functions and XOR operations, facilitating efficient mutual authentication among users, trusted servers, and medical servers while establishing session keys for data exchange. We then rigorously assess our protocol's security against a comprehensive threat model, employing both informal methods and formal analyses, including Real-Or-Random (ROR) model, BAN logic, and automated verification via ProVerif. The results demonstrate that our protocol remains resilient against known attacks and satisfies e-health security standards. Furthermore, a detailed performance comparison reveals that our approach significantly reduces some costs compared to existing schemes, while reinforcing security and privacy protections.},
  archive      = {J_TMC},
  author       = {Weizheng Wang and Qipeng Xie and Hongyang Du and Lejun Zhang and Joel J. P. C. Rodrigues},
  doi          = {10.1109/TMC.2025.3593533},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Lightweight and fast authentication protocol for digital healthcare services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed asynchronous service provisioning in edge-cloud multi-tier networks. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3593592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge-cloud multi-tier networks, datacenters must dynamically deploy and migrate services to meet diverse latency and computational constraints, especially for mobile users. Traditional solutions rely on centralized orchestration with full network visibility, which is impractical at scale and introduces reliability and privacy concerns. We propose DASDEC, a fully distributed and asynchronous framework for service placement and migration that operates without global coordination or private data exchange between providers. DASDEC enables local decision-making using lightweight control messages and supports cooperative placement across datacenters operated by distinct entities. Extensive simulations using real-world mobility traces show that DASDEC achieves performance very close to those obtained by optimal centralized solutions, while incurring negligible control-plane overhead (${\sim }100$ bytes per request). These results highlight DASDEC's scalability, resilience, and practical applicability to federated edge-cloud systems.},
  archive      = {J_TMC},
  author       = {Itamar Cohen and Antonio Calagna and Paolo Giaccone and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3593592},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed asynchronous service provisioning in edge-cloud multi-tier networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COTRA: A data trading framework for multi-source data cooperation. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3593986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data trading significantly enhances data-driven technologies by enabling efficient data sharing across mobile devices and communication systems. Despite the clear advantages of incorporating multiple sources often experienced by data users, the topic of multi-source data trading remains largely underexplored. This paper designs a data trading framework that enables multi-source data trading through structured and adaptive cooperation among data sources. The proposed framework aims to enhance data usage efficiency and seller revenue. Key technical challenges addressed include the complex interactions between data sources and buyers, the coupling among diverse data products, and the inherent non-convexity in optimization problems. We employ a Nash bargaining framework to model cooperative seller decisions and a two-stage Stackelberg game for dynamic seller-buyer interactions. By systematically addressing the coupling among data products, we derive closed-form solutions despite the non-convex nature of the optimization problem. Our results reveal that seller revenue remains stable with increasing product coupling until it reaches a threshold, beyond which further coupling increases revenue due to the substitute effect. Adaptive cooperation exhibits greater scalability and resilience, particularly in environments with numerous data sources and complex product interdependencies, while structured cooperation provides more predictability and efficiency in stable conditions. Experimental results demonstrate that this framework can improve seller profits by up to 46.32% compared to existing data trading methods in the current market.},
  archive      = {J_TMC},
  author       = {Jin Cheng and Ningning Ding and John C.S. Lui and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3593986},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {COTRA: A data trading framework for multi-source data cooperation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight federated learning over wireless edge networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3594102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of smart devices connected to wireless networks, data production is increasing rapidly, requiring machine learning (ML) techniques to unlock its value. However, the centralized ML paradigm raises concerns over communication overhead and privacy. Federated learning (FL) offers an alternative at the network edge, but practical deployment in wireless networks remains challenging. This paper proposes a lightweight FL (LTFL) framework integrating wireless transmission power control, model pruning, and gradient quantization. We derive a closed-form expression of the FL convergence gap, considering transmission error, model pruning error, and gradient quantization error. Based on these insights, we formulate an optimization problem to minimize the convergence gap while meeting delay and energy constraints. To solve the non-convex problem efficiently, we derive closed-form solutions for the optimal model pruning ratio and gradient quantization level, and employ Bayesian optimization for transmission power control. Extensive experiments on real-world datasets show that LTFL outperforms state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Xiangwang Hou and Jingjing Wang and Jun Du and Chunxiao Jiang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3594102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Lightweight federated learning over wireless edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REDS: Resource-efficient deep subnetworks for dynamic resource constraints. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models that are not capable to adapt at runtime. In this work, we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS leverages structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieves computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) taking advantage of data cache by re-arranging the order of operations in REDS computational graph. REDS supports conventional deep networks frequently deployed on the edge and provides computational benefits even for small and simple networks. We evaluate REDS on eight benchmark architectures trained on the Visual Wake Words, Google Speech Commands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four off-the-shelf mobile and embedded hardware platforms. We provide a theoretical result and empirical evidence demonstrating REDS' outstanding performance in terms of submodels' test set accuracy, and demonstrate an adaptation time in response to dynamic resource constraints of under 40$\mu$s, utilizing a fully-connected network on Arduino Nano 33 BLE.},
  archive      = {J_TMC},
  author       = {Francesco Corti and Balz Maag and Joachim Schauer and Ulrich Pferschy and Olga Saukh},
  doi          = {10.1109/TMC.2025.3594214},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REDS: Resource-efficient deep subnetworks for dynamic resource constraints},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erasure coding-based cost-optimized and latency-aware data storage in UAV-enabled edge systems. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV-enabled edge storage systems provide data storage services to users by deploying UAVs in areas lacking infrastructure coverage, overcoming delay limitations and improving Quality of Service (QoS). Most existing studies focus on storing replicas on UAVs to ensure low-latency data access. Nonetheless, replica-based strategies incur high storage cost, posing significant challenges for UAVs with limited storage resources. In this paper, we introduce erasure coding into the UAV-enabled edge storage system, aiming to reduce user data access latency while minimizing storage cost. However, the mobility of users and the non-fully-connected nature of the UAV network pose new challenges for the coupled decisions of data encoding, block placement, and access. In this paper, we propose a Mobility-Enhanced Hierarchical Deep Reinforcement Learning algorithm (ME-HDRL). Specifically, we design a trajectory prediction algorithm combining CNN and ConvLSTM to account for user mobility in decision-making. We further decompose the original problem into two subproblems: data encoding and placement, as well as block access. A hierarchical deep reinforcement learning algorithm involving multiple UAV agents and an edge agent is proposed to collaboratively learn optimal decisions. To improve the convergence of the algorithm, we design an invalid action filter to reduce the action space. Experimental results show that our approach outperforms existing rule-based and reinforcement learning-based algorithms in various scenarios, exhibiting significant convergence improvements and a substantial reduction in both storage cost and user data access latency.},
  archive      = {J_TMC},
  author       = {Zhaoxiang Huang and Zhiwen Yu and Liang Wang and Huan Zhou and Erhe Yang and and Bin Guo},
  doi          = {10.1109/TMC.2025.3594283},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Erasure coding-based cost-optimized and latency-aware data storage in UAV-enabled edge systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based user scheduling for collaborative perception. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3594222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stand-alone perception systems in autonomous driving suffer from limited sensing ranges and occlusions at extended distances, potentially resulting in catastrophic outcomes. To address this issue, collaborative perception is envisioned to improve perceptual accuracy by using vehicle-to-everything (V2X) communication to enable collaboration among connected and autonomous vehicles and roadside units. However, due to limited communication resources, it is impractical for all units to transmit sensing data such as point clouds or high-definition video. As a result, it is essential to optimize the scheduling of communication links to ensure efficient spectrum utilization for the exchange of perceptual data. In this work, we propose a deep reinforcement learning-based V2X user scheduling algorithm for collaborative perception. Given the challenges in acquiring perceptual labels, we reformulate the conventional label-dependent objective into a label-free goal, based on characteristics of 3D object detection. Incorporating both channel state information (CSI) and semantic information, we develop a double deep Q-Network (DDQN)-based user scheduling framework for collaborative perception, named SchedCP. Simulation results verify the effectiveness and robustness of SchedCP compared with traditional V2X scheduling methods. Finally, we present a case study to illustrate how our proposed algorithm adaptively modifies the scheduling decisions by taking both instantaneous CSI and perceptual semantics into account.},
  archive      = {J_TMC},
  author       = {Yandi Liu and Guowei Liu and Le Liang and Hao Ye and Chongtao Guo and Shi Jin},
  doi          = {10.1109/TMC.2025.3594222},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep reinforcement learning-based user scheduling for collaborative perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmBP+: Contact-free blood pressure measurement using millimeter-wave radar. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3594316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood pressure (BP) measurement is an indispensable tool in diagnosing and treating many diseases such as cardiovascular failure and stroke. Traditional direct measurement can be invasive, and wearable-based methods may have limitations of discomfort and inconvenience. Contact-free BP measurement has been recently advocated as a promising alternative. In particular, Millimeter-wave (mmWave) sensing has demonstrated its promising potential, however it is confronted with several challenges including noise and vulnerability to human's tiny motions which may occur intentionally and inevitably. In this paper, we propose mmBP+, a contact-free mmWave-based BP measurement system with high accuracy and motion robustness. Due to the high frequency and short wavelength, mmWave signals received in the time domain are dramatically susceptible to ambient noise, and deteriorating signal quality. To reduce noise,we propose a novel approach to exploit mmWave signal's characteristics and features in the delay-Doppler-fractional Fourier domain to significantly improve signal quality for pulse waveform construction. We also propose a periodic signal feature based functional link adaptive filter leveraging on the periodic and correlation characteristics of pulse waveform signals to alleviate the impact of human's tiny motions. Extensive experiment results achieved by the leave-one-out cross-validation (LOOCV) method demonstrate that mmBP+ achieves the mean errors of 0.65mmHg and 1.31mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively; and the standard deviation errors of 3.92mmHg and 3.99mmHg for SBP and DBP, respectively.},
  archive      = {J_TMC},
  author       = {Zhenguo Shi and Tao Gu and Yu Zhang and Xi Zhang},
  doi          = {10.1109/TMC.2025.3594316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmBP+: Contact-free blood pressure measurement using millimeter-wave radar},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deterministic-latency MAC protocol for future automotive ethernet. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3593939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive Ethernet as an in-vehicle networking paradigm has gradually become the main automotive backbone network. However, with the ever-increasing time-critical in-vehicle traffic, it is being confronted with enormous challenges to realize deterministic-latency communications, due to the inherent limitations of its distributed network architecture and the adopted MAC protocols, including limited-computing power, low-speed and unreliable traffic transmission. Furthermore, it is often incompatible with emerging vehicular functions and protocols, which can provide tremendous potential for better vehicular Quality-of-Service (QoS). Therefore, in this paper, we first design a Future Automotive Ethernet (FAE) architecture and then, built on the representative Time-Sensitive Networking (TSN) and industrial summation frame, propose a Deterministic-Latency MAC (DLM) protocol running in FAE to tackle these issues. The FAE architecture includes three functional domains, three in-vehicle computing units, and no fewer than three intelligent network processing modules, which can integrate with the emerging LAN protocols to realize cross-domain and intra-domain Ethernet-based communications. Under the umbrella of this architecture, DLM classifies the driving situations into driving, reversing, left turn, right turn and parking states, following the real-world vehicle behaviors, and assigns all traffic associated with driving safety in five states different recommended priorities to be delivered. Furthermore, an optimized deterministic-latency mechanism integrated with TSN and the industrial summation frame is developed to realize the timely and accurate transmission of high-priority and medium/low-priority traffic. Simulation results obtained from diversified scenarios demonstrate that the proposed DLM running in FAE can significantly improve transmission latency determinacy and reliability compared with the existing technical strategies.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Jieling Lei and Bang Wu and Geyong Min and Wang Miao},
  doi          = {10.1109/TMC.2025.3593939},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A deterministic-latency MAC protocol for future automotive ethernet},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRAM: Digital twin-driven double-layer reverse auction method for multi-platform vehicular crowdsensing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3594488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, For-Hire Vehicles (FHVs) have emerged as major players in Vehicular CrowdSensing (VCS). However, the heterogeneity of tasks issued by Data Requesters (DRs) and the heterogeneity of sensors equipped on FHVs under different Vehicle Platforms (VPs) bring difficulties to task allocation and execution. It can be concluded that it is important to reasonably analyze the relationship among DRs, VPs, and FHVs, as well as to motivate VPs and FHVs to complete sensing tasks. Therefore, taking advantage of the real-time simulation and intelligent decision-making of Digital Twins (DT), this paper proposes a DT-driven Double-layer Reverse Auction Method (DRAM). In the first layer, the reverse auction is established between each DR and VPs, and in the second layer, the reverse auction is established between each VP and FHVs. Meanwhile, we also introduce a sensing fairness index to ensure the sensing balance of different sub-regions and consider it in the DRAM process. Here, the idea of backward induction is used to solve the above problems, with the goal of minimizing the overhead of winning VP and the average overhead of all DRs. Finally, the effectiveness of the DRAM proposed in this paper is verified based on the real data set. Compared with the baseline method, DRAM can reduce the average overhead of DR by about 4%-25%. Meanwhile, in terms of sensing fairness, it can be improved by up to 55%.},
  archive      = {J_TMC},
  author       = {Zhenning Wang and Yue Cao and Huan Zhou and Xiaokang Zhou and Jiawen Kang and Houbing Song},
  doi          = {10.1109/TMC.2025.3594488},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRAM: Digital twin-driven double-layer reverse auction method for multi-platform vehicular crowdsensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task assignment and exploration optimization for low altitude UAV rescue via generative AI enhanced multi-agent reinforcement learning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3594188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of emerging uncrewed aerial vehicle (UAV) with artificial intelligence (AI) and ground-embedded robots (GERs) has transformed emergency rescue operations in unknown environments. However, the high computational demands of such missions often exceed the capacity of a single UAV, making it difficult for the system to continuously and stably provide high-level services. To address these challenges, this paper proposes a novel cooperation framework involving UAVs, GERs, and airships. This framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship (U2A) communications, providing computing services for UAV offloaded tasks. Specifically, we formulate the multi-objective optimization problem of task assignment and exploration optimization in UAVs as a dynamic long-term optimization problem. Our objective is to minimize task completion time and energy consumption while ensuring system stability over time. To achieve this, we first employ the Lyapunov optimization method to transform the original problem, with stability constraints, into a per-slot deterministic problem. We then propose an algorithm named HG-MADDPG, which combines the Hungarian algorithm with a generative diffusion model (GDM)-based multi-agent deep deterministic policy gradient (MADDPG) approach, to jointly optimize exploration and task assignment decisions. In HG-MADDPG, we first introduce the Hungarian algorithm as a method for exploration area selection, enhancing UAV efficiency in interacting with the environment. We then innovatively integrate the GDM and multi-agent deep deterministic policy gradient (MADDPG) to optimize task assignment decisions, such as task offloading and resource allocation. Simulation results demonstrate the effectiveness of the proposed approach, with significant improvements in task offloading efficiency, latency reduction, and system stability compared to baseline methods.},
  archive      = {J_TMC},
  author       = {Xin Tang and Qian Chen and Wenjie Weng and Chao Jin and Zhang Liu and Jiacheng Wang and Geng Sun and Xiaohuan Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3594188},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task assignment and exploration optimization for low altitude UAV rescue via generative AI enhanced multi-agent reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing stability and resource efficiency in LLM training for edge-assisted mobile systems. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3570376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile devices continue to drive advanced applications, edge computing has emerged as a crucial solution to overcome their inherent computational constraints, especially in deploying and training large language models (LLMs). Despite progress in edge computing, significant challenges remain in achieving efficient LLM training while addressing computational demands, energy consumption, and model stability. This paper presents an enhanced collaborative training framework that integrates mobile users with edge servers to optimize resource allocation. We extend the framework by incorporating model stability into the optimization objectives, mitigating performance instability often observed during distributed LLM fine-tuning. A multi-objective optimization problem is formulated to minimize energy consumption, delay, and instability, with a novel fractional programming technique and Iterative Rank Penalization (IRP) method proposed to improve the resource allocation and user-to-edge server associations. Compared to traditional methods like Semidefinite Relaxation, IRP achieves higher accuracy and computational efficiency. Extensive simulations demonstrate that our approach outperforms existing methods in reducing energy consumption and delay, and improving LLM stability across various mobile edge computing environments.},
  archive      = {J_TMC},
  author       = {Chang Liu and Jun Zhao},
  doi          = {10.1109/TMC.2025.3570376},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing stability and resource efficiency in LLM training for edge-assisted mobile systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3580396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, designed for Low-Power, Wide-Area Networks (LPWANs), is widely used in the Internet of Things (IoT). In contrast, Wireless Personal Area Network (WPAN) technologies like ZigBee struggle to connect directly to LPWANs due to their limited communication range and differing modulation schemes. ZigBee uses Offset Quadrature Phase-Shift Keying (OQPSK) modulation, while LoRa employs Chirp Spread Spectrum (CSS) modulation, complicating cross-technology communication. To address this challenge, we propose a novel approach for seamless physical-layer cross-technology communication between ZigBee and LoRa networks, bridging the gap between short-range and long-range communication technologies. We introduce ZigRa, a communication method that leverages neural networks for efficient modulation translation between ZigBee's IEEE 802.15.4 standard and LoRa's CSS modulation. The core of ZigRa is a deep learning model that adapts and optimizes the transformation of ZigBee signals into ultra-narrowband single-tone sinusoidal signals, which can be reliably detected by LoRaWAN base stations. Our solution enables ZigBee devices to seamlessly connect to LoRa-based LPWANs, overcoming modulation mismatches and providing long-range connectivity. Extensive evaluations with both USRP hardware and commercial devices demonstrate that ZigRa achieves a frame reception rate exceeding 85% at distances up to 500 meters, significantly enhancing the interoperability and coverage of heterogeneous IoT networks.},
  archive      = {J_TMC},
  author       = {Demin Gao and Yongrui Chen and Ye Liu and Honggang Wang},
  doi          = {10.1109/TMC.2025.3580396},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online collaborative resource allocation and task offloading for multi-access edge computing. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3580365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) is emerging as a promising paradigm to provide flexible computing services close to user devices (UDs). However, meeting the computation-hungry and delay-sensitive demands of UDs faces several challenges, including the resource constraints of MEC servers, inherent dynamic and complex features in the MEC system, and difficulty in dealing with the time-coupled and decision-coupled optimization. In this work, we first present an edge-cloud collaborative MEC architecture, where the MEC servers and cloud collaboratively provide offloading services for UDs. Moreover, we formulate an energy-efficient and delay-aware optimization problem (EEDAOP) to minimize the energy consumption of UDs under the constraints of task deadlines and long-term queuing delays. Since the problem is proved to be non-convex mixed integer nonlinear programming (MINLP), we propose an online joint communication resource allocation and task offloading approach (OJCTA). Specifically, we transform EEDAOP into a real-time optimization problem by employing the Lyapunov optimization framework. Then, to solve the real-time optimization problem, we propose a communication resource allocation and task offloading optimization method by employing the Tammer decomposition mechanism, convex optimization method, bilateral matching mechanism, and dependent rounding method. Simulation results demonstrate that the proposed OJCTA can achieve superior system performance compared to the benchmark approaches.},
  archive      = {J_TMC},
  author       = {Geng Sun and Minghua Yuan and Zemin Sun and Jiacheng Wang and Hongyang Du and Dusit Niyato and Zhu Han and Dong In Kim},
  doi          = {10.1109/TMC.2025.3580365},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online collaborative resource allocation and task offloading for multi-access edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cellular infrastructure sharing for network robustness: A citywide empirical study. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3580605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual cellular networks have been very robust to random cell tower failures due to redundant cell tower deployments. However, a large-scale clustered failure with multiple cell towers can lead to the loss of services of a cellular network. Recently, off-the-shelf smartphones can support multiple network standards, so cellular network infrastructure sharing is a promising direction to improve the service robustness under potential large-scale clustered tower failures. The existing work on cellular network robustness is usually limited to large-scale studies of individual networks or small-scale studies of multiple networks. In this work, we conduct the first investigation, to our knowledge, on cross-network infrastructure sharing benefits for enhancing robustness with a full cellular penetration rate. Our work is based on all cellular networks in Shenzhen, China, covering over 10 million cellular users. Specifically, we design a new metric to quantify cellular network robustness with or without cross-network sharing under both random and clustered cell tower failures. We further study the impact of different factors on robustness, including the number of networks, spatiotemporal dynamics, contextual factors, and a case study at two key transportation hubs. We provide a set of lessons learned based on our study, along with discussions of the results.},
  archive      = {J_TMC},
  author       = {Zhihan Fang and Guang Yang and Wenjun Lyu and Zhiqing Hong and Shuxin Zhong and Weijian Zuo and Yuelei Xie and Yu Yang and Guang Wang and Yunhuai Liu and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3580605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cellular infrastructure sharing for network robustness: A citywide empirical study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3580764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Fifth generation (5G) cellular systems transition to softwarized, programmable, and intelligent networks, it becomes fundamental to enable public and private 5G deployments that are (i) primarily based on software components while (ii) maintaining or exceeding the performance of traditional monolithic systems and (iii) enabling programmability through bespoke configurations and optimized deployments. This requires hardware acceleration to scale the Physical (PHY) layer performance, programmable elements in the Radio Access Network (RAN) and intelligent controllers at the edge, careful planning of the Radio Frequency (RF) environment, as well as end-to-end integration and testing. In this paper, we describe how we developed the programmable X5G testbed, addressing these challenges through the deployment of the first 8-node network based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air (ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent Controller (RIC). The Aerial Software Development Kit (SDK) provides the PHY layer, accelerated on Graphics Processing Unit (GPU), with the higher layers from the OAI open-source project interfaced with the PHY through the Small Cell Forum (SCF) Functional Application Platform Interface (FAPI). An E2 agent provides connectivity to the O-RAN Software Community (OSC) nearreal-time RIC. We discuss software integration, network infrastructure, and a digital twin framework for RF planning. We then profile the performance with up to 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with iPerf and video streaming applications, as well as up to 25 emulated User Equipments (UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in uplink.},
  archive      = {J_TMC},
  author       = {Davide Villa and Imran Khan and Florian Kaltenberger and Nicholas Hedberg and Rúben Soares da Silva and Stefano Maxenti and Leonardo Bonati and Anupa Kelkar and Chris Dick and Eduardo Baena and Josep M. Jornet and Tommaso Melodia and Michele Polese and Dimitrios Koutsonikolas},
  doi          = {10.1109/TMC.2025.3580764},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling state shifting via local-global distillation for event-frame gaze tracking. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3581317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the problem of passive gaze estimation using both event and frame (or 2D image) data. Considering the inherently different physiological structures, it is intractable to accurately estimate gaze purely based on a given state. Thus, we reformulate gaze estimation as the quantification of the state shifting from the current state to several prior registered anchor states. Specifically, we propose a two-stage learning-based gaze estimation framework that divides the whole gaze estimation process into a coarse-to-fine approach involving anchor state selection and final gaze location. Moreover, to improve the generalization ability, instead of learning a large gaze estimation network directly, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion techniques to iteratively remove inherent noise in event data. Extensive experiments demonstrate the effectiveness of the proposed method, which surpasses state-of-the-art methods by a large margin of 15$\%$. The code will be publicly available at https://github.com/ZHU-Zhiyu/Event_Gaze_Tracking.},
  archive      = {J_TMC},
  author       = {Zhiyu Zhu and Jinhui Hou and Jiading Li and Jinjian Wu and Junhui Hou},
  doi          = {10.1109/TMC.2025.3581317},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Modeling state shifting via local-global distillation for event-frame gaze tracking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-knowledge neighbor discovery for underwater optical wireless sensor networks. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3581371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbor discovery poses significant challenges in Underwater Optical Wireless Sensor Networks (UOWSNs) due to the unique characteristics of directional transceivers, line-of-sight communication, and mobility induced by water currents. Traditional methods typically rely on prerequisites and prior knowledge, such as centralized coordination, time synchronization, and information about the number of neighbors, which are often unavailable or impractical in underwater environments. In this paper, we make the first attempt to address the issue of Robust and Efficient Neighbor Discovery (termed the REND problem) in UOWSNs with zero-knowledge. Here, zero-knowledge refers to the capability that enables sensors to identify neighbors in dynamic underwater optical channel conditions without prerequisites or prior knowledge. We design a zero-knowledge distributed directional neighbor discovery scheme inspired by gear meshing. We then propose a deterministic algorithm for the REND problem based on theoretical analysis. Additionally, to further reduce the discovery delay for the periodic REND problem, we develop a greedy-based approximation algorithm with a performance guarantee. Finally, extensive simulations demonstrate that the proposed scheme reduces the discovery delay by 34.9% on average and achieves an additional 54.4% reduction for periodic neighbor discovery. Furthermore, test-bed experiments are carried out to verify the applicability of our zero-knowledge scheme in real-world scenarios.},
  archive      = {J_TMC},
  author       = {Yu Tian and Lei Wang and Chi Lin and Bin Han and Lupeng Zhang and Zhiyi Zhou and Yu Sun and Bingxian Lu},
  doi          = {10.1109/TMC.2025.3581371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Zero-knowledge neighbor discovery for underwater optical wireless sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3581397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicular Fog Computing (VFC) provides low-latency computing service to support emerging intelligent transportation applications in Internet of Vehicles (IoV). Multi-node cooperative VFC can utilize Connected and Autonomous Vehicles (CAVs) to implement cooperative intelligence. Due to the mobility of vehicles, service migration is necessary when task offloading service providers change. This paper proposes an Asynchronous Task Offloading Scheme (ATO-S) that allows each CAV to choose an independent optimization period and provides robust task offloading services under unknown vehicle mobility probability distribution. To the best of our knowledge, this is the first work to investigate asynchronous and robust multi-node cooperative task offloading in dynamic VFC-enhanced IoV scenarios. Furthermore, we formulate the long-term energy consumption minimization problem of VFC and transfer it into each time slot problem by Lyapunov optimization. Then we design Asynchronous Task Offloading Algorithm (ATO-A) to jointly optimizing CAVs matching, communication and computation resource allocation, and transmission power based on multiple mathematical techniques and hybrid heuristic algorithm. Extensive simulations based on real-world traffic scenario are conducted by varying multiple crucial parameters. Simulation results demonstrate the energy efficiency and task queue stability achieved by ATO-A, and service robustness achieved by ATO-S, in comparison with benchmark solutions.},
  archive      = {J_TMC},
  author       = {Yifeng Zhao and Chenyi Liang and Zhibin Gao and Lianfen Huang},
  doi          = {10.1109/TMC.2025.3581397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconsidering sparse sensing techniques for channel sounding using splicing. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-band splicing offers a promising solution to extend existing band-limited communication systems to support high-precision sensing applications. This technique involves performing narrow-band measurements at multiple center frequencies, which are then combined to effectively increase the bandwidth without changing the sampling rate. In this paper, we introduce a mmWave channel sounder based on multiband splicing, leveraging the sparse nature of wireless channels through compressed sensing and sparse recovery techniques for channel reconstruction. We focus on three sparse recovery methods: the widely used grid-based orthogonal matching pursuit (OMP) algorithm as a baseline, our newly developed two-stage mmSplicer algorithm, which extends the OMP method by introducing an additional stage for improving its performance for our application, and our adaptation of sparse reconstruction by separable approximation (SpaRSA), named Net-SpaRSA, optimized for wireless applications. All three algorithms are integrated into an experimental OFDM-based IEEE 802.11ac system. Our analysis centers on evaluating the performance of these algorithms under limited number of narrow-band measurements, demonstrating that accurate CIR estimation is achievable even using only 50% of the full wideband spectrum. Additionally, we analyze and compare the computational complexity of these algorithms to assess their practical feasibility},
  archive      = {J_TMC},
  author       = {Sigrid Dimce and Anatolij Zubow and Alireza Bayesteh and Giuseppe Caire and Falko Dressler},
  doi          = {10.1109/TMC.2025.3581446},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reconsidering sparse sensing techniques for channel sounding using splicing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3581523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the IFresher framework to improve the timeliness of multi-agent mobile augmented reality (MAR) systems. Existing works have made strides in accuracy-latency trade-offs, but fail to directly address realtime task responsiveness and multi-agent contention challenges. To bridge this gap, we introduce the concept of the age of analytics information (AoAI), which quantifies the combined impact of video analytics (VA) accuracy, transmission delay, and computational efficiency. By deriving a closed-form expression for AoAI, IFresher establishes a central control mechanism that jointly optimizes bandwidth allocation and video configuration to minimize AoAI while ensuring accuracy. Due to the mixed-integer nonlinear characteristics of the problem and the fact that each agent only has local observations, the problem is reformulated into a decentralized partially observable Markov decision process (Dec-POMDP). We propose a multi-agent reinforcement learning (MARL) algorithm, named convex-embedded transformer QMIX (CTQMIX), using the centralized training and decentralized execution (CTDE) framework for agent collaboration. Specifically, the convex optimization ensures optimal bandwidth distribution, and the transformer captures temporal dependencies between observations and actions across time steps to improve decisionmaking in dynamic environments. Evaluations with real-world experiments show that the CTQMIX outperforms state-of-theart (SOTA) algorithms.},
  archive      = {J_TMC},
  author       = {Shuang Cheng and Zhaoyang Wang and Fangzheng Feng and Yu Zhang and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581523},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial keyword query has emerged as a critical service in mobile cloud, enabling cloud servers to retrieve spatiotextual objects within a mobile user's query range that contain specified query keywords. Numerous secure spatial keyword query schemes have been developed to enable geometric range queries and keyword searches on encrypted spatial data. However, spatial keyword queries are typically designed for searching high-dimensional spatial data across arbitrary geographic ranges. Most of them fail to handle arbitrary geometric range queries and efficient spatial keyword query over high-dimensional encrypted data. To address these issues, we propose a high-dimEnsional and Privacy-preserving Spatial Keyword Query (EPSKQ) scheme with arbitrary geometric ranges over encrypted spatial data, leveraging Hilbert curve encoding and Enhanced Matrix-based Inner Product Encryption (EMIPE). In EPSKQ, spatial locations and multi-keywords are encoded into compact vectors, and arbitrary geometric range queries are transformed into range intersection tests. To reduce computational overhead, we employ vector bucketing technique to partition large-size vectors into several small-size sub-vectors. Furthermore, we design a novel index structure called Hilbert Binary tree (HB-tree) to optimize range intersection tests. Based on HB-tree, we propose an enhanced spatial keyword query scheme, named EPSKQ+, which further improves query performance. Security analysis demonstrates that both EPSKQ and EPSKQ+ achieve semantic security against indistinguishability under chosen-plaintext attack (INDCPA). Extensive experimental evaluations show that the proposed EPSKQ and EPSKQ+ schemes significantly outperform state-ofthe-art schemes in terms of computational and communication costs, with EPSKQ+ being 9× and 3× faster than the state-ofthe-art schemes in the index build and query phase, respectively},
  archive      = {J_TMC},
  author       = {Fuyuan Song and Yunlong Gao and Mingyang Zhao and Chuan Zhang and Zheng Qin and Bin Xiao},
  doi          = {10.1109/TMC.2025.3581562},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3581607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low earth orbit (LEO) constellation integrated maritime networks have recently attracted much interest due to the rapid development of maritime applications and services. LEO satellites have the advantages of wide coverage to provide seamless connection for maritime wireless devices. However, due to the limited battery and computing capacity of unmanned aerial vehicles (UAVs) for ocean information perception and processing, the computing-intensive and delay-sensitive oceanic data suffer from long latency and high energy consumption, which degrades the efficiency of maritime services. In this paper, to enhance the perception and offloading endurance of UAVs in maritime networks, we propose an energy efficient multi-access edge computing scheme for heterogeneous satellite-maritime networks, with the objective of minimizing the cumulative transmitted energy for UAVs. Specifically, we first present a heterogeneous satellite-maritime network framework in which LEO satellites and unmanned surface vehicles (USVs) equipped with edge servers can process workloads simultaneously. Next, considering the limited battery supply of UAVs, we propose a hybrid harvesting-and-offloading scheme for resource allocation, where UAVs first harvest energy from solar power and radio frequency power from USV, and then UAVs determine the offloading strategy for task processing. Moreover, a joint optimization problem is formulated to optimize the offloading decision, the time scheduling, and the transmitting power. We also exploit a vertical architecture to solve the formulated problem. Regarding each decomposed sub-problem, we propose efficient algorithms to derive the corresponding solutions. Finally, we provide numerical results to validate the performance of our proposed algorithms in comparison with several benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Minghui Dai and Shan Chang and Yixuan Wang and Zhou Su},
  doi          = {10.1109/TMC.2025.3581607},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge computing for underwater optical wireless sensor networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Optical Wireless Sensor Networks (UOWSNs) play important roles in resource exploration and maritime rescue. However, they face significant challenges in real-time data transmission due to the limited propagation range of optical signals (typically 10-100 m), frequent link disconnections caused by node mobility, and the extended distances to onshore servers. Traditional cloud computing solutions, designed for stable terrestrial networks with stationary edge servers and continuous connectivity, experience high latency (3-15 s) in UOWSNs, rendering them unsuitable for real-time applications in underwater environments. To address this issue, we propose a cloud-edge-end architecture tailored for UOWSNs, which can not only combat unique underwater environmental interference on link connection and topological changes but also guarantee robust and real-time communication. We develop a dynamic link-stability-based task offloading path selection (DLS-TOPS) algorithm for maximizing network resource profits. Afterward, we propose an online primal-dual task offloading (OPD-TO) algorithm for minimizing task completion time. Simulation results indicate that the proposed method significantly improves the real-time performance and resource profits of the network, reducing the total task completion time by more than 50% compared to baseline algorithms. We implemented a UOWSN with a cloud-edge-end architecture using commercial off-the-shelf and verified the applicability and effectiveness of the proposed scheme in emergency detection through testbed experiments.},
  archive      = {J_TMC},
  author       = {Yang Chi and Chi Lin and Jing Deng and Kaiwen Ning and Xin Fan},
  doi          = {10.1109/TMC.2025.3581690},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge computing for underwater optical wireless sensor networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3581798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of wireless communication technologies, interference has become a key impediment to the improvement of wireless data transmission performance. Traditional interference management (IM) suppresses or adjusts interference at the cost of additional communication resources without exploiting interference effectively. Especially when the interference is strong, simply eliminating it may be inefficient and costly. Moreover, wirelessly transmitted data is susceptible to threats such as eavesdropping, which also necessitates a thorough investigation. To address these issues cooperatively, we propose Opportunistic Parasitic Communication with Asymmetric Demodulation (OPC-AD). In particular, we consider the interference experienced by the intended/target communication (i.e., parasitic) receiver (Rx) as the host signal. The target communication constructs a selection signal carrying parasitic indication information based on the data it intends to send and the data decoded by its Rx using asymmetric demodulation from the host signal, and then sends it to its Rx. This signal is used to instruct the parasitic Rx to extract the desired information from the host signal. OPC-AD allows for the exploitation of the interference (i.e., host signal) for data transmission to an interfered Rx. Using AD can also ensure the privacy of the host communication. Since the parasitic communication is concealed within the host signal, eavesdroppers cannot compromise the confidentiality of the parasitic transmission without precisely decoding the selection signal. Furthermore, considering more practical situations, such as non-zero delay/phase difference between the host signal and the selection signal, the implementation of the proposed method under various modulation schemes, and the enhancement of the probability of successful parasitism, we extend the OPC-AD design to cover a broader range of realistic scenarios. Our experimental results validate the applicability of OPC-AD, while our in-depth simulations demonstrate that parasitic communication can effectively thwart eavesdropping and achieve higher spectral efficiency (SE) than other existing IM methods, particularly in strong interference environments.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Kang G. Shin and Jia Liu and Yicheng Liu and Pintian Lyu and Zheng Yan},
  doi          = {10.1109/TMC.2025.3581798},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3581561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent node's routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed “Licenses.” LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.},
  archive      = {J_TMC},
  author       = {Shefali Goel and Vinod Kumar Jain and Abhishek Verma},
  doi          = {10.1109/TMC.2025.3581561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-design of sensing, communications, and control for low-altitude wireless networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3581616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Internet of Things (IoT) services and the evolution toward the sixth generation (6 G) have positioned unmanned aerial vehicles (UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This work investigates the co-design of integrated sensing, communication, and control ($\mathbf {SC^{2}}$) for multi-UAV cooperative systems with finite blocklength (FBL) transmission. In particular, the UAVs continuously monitor the state of the field robots and transmit their observations to the robot controller to ensure stable control while cooperating to localize an unknown sensing target (ST). To this end, a weighted optimization problem is first formulated by jointly considering the control and localization performance in terms of the linear quadratic regulator (LQR) cost and the determinant of the Fisher information matrix (FIM), respectively. The resultant problem, optimizing resource allocations, the UAVs' deployment positions, and multi-user scheduling, is non-convex. To circumvent this challenge, we first derive a closed-form expression of the LQR cost with respect to other variables. Subsequently, the non-convex optimization problem is decomposed into a series of sub-problems by leveraging the alternating optimization (AO) approach, in which the difference of convex functions (DC) programming and projected gradient descent (PGD) method are employed to obtain an efficient near-optimal solution. Furthermore, the convergence and computational complexity of the proposed algorithm are thoroughly analyzed. Extensive simulation results are presented to validate the effectiveness of our proposed approach compared to the benchmark schemes and reveal the trade-off between control and sensing performance.},
  archive      = {J_TMC},
  author       = {Haijia Jin and Jun Wu and Weijie Yuan and Fan Liu and Yuanhao Cui},
  doi          = {10.1109/TMC.2025.3581616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Co-design of sensing, communications, and control for low-altitude wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3581687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition is a technology that involves analyzing and interpreting human facial expressions to determine individual expressions or states. Mobile crowdsensing (MCS), a promising sensing paradigm, makes it easy to capture facial images and benefits facial expression recognition. Existing inference models for facial expression recognition usually rely on facial feature vectors or facial images, increasing privacy concerns about expression. For this reason, this paper proposes a privacy-preserving facial expression recognition scheme through MCS, named RAPOO, which falls in a client-server architecture. Roughly speaking, a user captures facial images using mobile devices and requests a recognition service provided by a cloud computing center. To protect the privacy of expressions, our approach focuses on designing secure computation protocols required by facial expression recognition necessarily, such as secure vector distance calculation and secure top-$k$ query. These protocols enable facial expression recognition over encrypted data directly. To speed up the recognition and store encrypted feature vectors, a $k$-D tree data structure is introduced. The security analysis confirms that RAPOO effectively preserves the confidentiality of personal expressions. Extensive experimental evaluations show that our solution obtains a three-order-of-magnitude speedup in terms of computational overhead compared with the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Bo Tian and Bowen Zhao and Yang Xiao and Yang Liu and Qingqi Pei and Yulong Shen},
  doi          = {10.1109/TMC.2025.3581687},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson's disease. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile phones have evolved from basic communication tools to feature-rich mobile devices. These ubiquitous and portable devices, equipped with inertial sensors and high-speed network access, create opportunities for remote health monitoring, especially for movement disorders such as Parkinson's disease (PD). Inertial sensors (gyroscopes and accelerometers) endow smartphones with a natural ability to monitor movement disorders. Based on this, we develop a novel vision-based time-series feature augmentation framework for remote diagnosis and severity grading of PD using mobile phone walking records. Specifically, preprocessed time-series data is encoded into RGB images for the teacher model, while the time-series data is input into the student model, with the teacher guiding the student's learning. The teacher model is based on MobileNetV2 and incorporates spatial and channel relation-aware attention mechanisms to capture important features and filter out irrelevant information. The inter-modal feature fusion module combines attention and CNN to emphasize both global and local features. The student model utilizes a simple CNN to directly extract features from time-series data and perform classification. For the three-level classification task, the teacher model achieves accuracies of 0.887, 0.886, and 0.896 across the three datasets, while the distillation student model reaches 0.779, 0.828, and 0.827, generally surpassing state-of-the-art algorithms.},
  archive      = {J_TMC},
  author       = {Tongyue He and Junxin Chen and Chi Lin and Wei Wang},
  doi          = {10.1109/TMC.2025.3581525},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson's disease},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3581549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with increasing user demands for convenience, privacy, and personalized experiences, gait recognition has been widely studied across various domains, such as indoor intrusion detection and smart homes. Although computer vision solutions are extensively researched for their visual intuitiveness, Wi-Fi sensing is emerging as a new research focus due to its ability to preserve privacy. However, previous studies have primarily relied on abstract features with limited interpretability or required multiple Wi-Fi links. To address these issues, we propose Wi-GR, which utilizes a Wi-Fi link to extract robust and highly interpretable gait features for user recognition. First, we construct a multi-path gait signal model to establish a clear relationship between Channel State Information (CSI) and gait motion. Then, we design a gait signal separation and enhancement method to mitigate the effects of external non-target reflections and internal multi-part reflections, which significantly impact the extraction and interpretability of gait features. Finally, fine-grained gait features that visualize gait patterns are generated using MUSIC-based and GAN-based multi-part velocity profile generation algorithms, tailored for single-person and multi-person scenarios, respectively. Numerous experiments have demonstrated that Wi-GR achieves single-person recognition accuracies of 95.3%, 94.0%, and 93.2% for 30 persons in the meeting room, corridor, and lobby, respectively, and an average accuracy of 88.3% for two-person recognition.},
  archive      = {J_TMC},
  author       = {Hao Chen and Penghao Wang and Jingyang Hu and Feng Li and Hongbo Jiang and Minglu Li and Chao Liu},
  doi          = {10.1109/TMC.2025.3581549},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3581600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of information and the continuous expansion of applications, Industrial Internet of Things (IIoT) is facing huge data processing and storage pressure. With mobile edge computing (MEC) technology, the computing power network connects the geographically distributed computing nodes and then coordinates the allocation and scheduling of resources, transmits data, and eventually relieves the pressure of the industrial site. However, the rigorous demands of IIoT for real-time and stability pose some daunting challenges. To this end, we propose an online computation offloading approach with dual stability guarantee, named OCODSG. Specifically, the Lyapunov function is used to optimize the stability of the virtual queue, and the system stability is optimized based on the network jitter measurement. Moreover, the Dueling Double Deep Q Network (D3QN) algorithm based on deep reinforcement learning (DRL) is used for model autonomous training, while Gaussian noise is added to the network parameter space to encourage exploration and enhance algorithm robustness. Finally, experimental results on both simulated and real datasets demonstrate that OCODSG improves service efficiency and system stability.},
  archive      = {J_TMC},
  author       = {Kai Peng and Chengfang Ling and Shangguang Wang and Victor C.M. Leung},
  doi          = {10.1109/TMC.2025.3581600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3581512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater acoustic sensor network (UASN) is a pivotal paradigm within the underwater Internet of Things, where multi-hop forwarding-based underwater data routing is essential for information acquisition. However, the dynamic nature of underwater network topology and the instability of underwater acoustic communication pose significant challenges to achieving efficient and reliable data transmission. In light of unreliable underwater environments and potential malicious attacks, studying trusted routing strategies for UASNs is crucial. This study introduces a context-aware trust routing scheme (CADTR) based on deep reinforcement learning (DRL), which integrates real-time environmental state perception with AI-driven routing decisions, thereby enhancing the reliability and robustness of data routing in dynamic and potentially hostile underwater scenarios. Firstly, a unified trust evidence framework is developed to strengthen the support of evidence experience for subsequent trust decisions by mapping multi-dimensional trust evidence to a unified scale. This framework is tightly coupled with the DRL agent, allowing the agent to evaluate and update trust levels based on real-time evidence. Secondly, a dynamic topology perception model and an underwater acoustic communication perception model are constructed to enable real-time perception of the interactive experience context. These models provide continuous input to the DRL agent, enabling it to adapt to topological changes and communication conditions dynamically. This facilitates priority experience sampling during the training process of the routing decision model, indirectly boosting model training efficiency and decision accuracy. Finally, the DRL agent learns optimal routing policies by interacting with the environment, leveraging the trust evidence and perception models to make informed decisions. Experimental results demonstrate that the proposed CADTR algorithm significantly improves the overall performance of the routing strategy in terms of packet delivery rate, energy utilization efficiency, and data transmission delay compared to the benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Yu He and Guangjie Han and Jinfang Jiang and Xin Cheng and Pengfei Xu},
  doi          = {10.1109/TMC.2025.3581512},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3581836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target hunting (UTH) is a critical and complex mission involving the search, monitoring, and hunting of targets in an underwater environment. However, the unpredictable trajectories and flexibility of these targets, along with complex underwater environments, significantly impede the efficiency and success of traditional schemes that depend solely on unmanned underwater vehicles (UUVs). Consequently, this paper presents the “3U network”, a novel heterogeneous framework integrating unmanned aerial vehicles (UAVs), unmanned surface vehicles (USVs), and UUVs for UTH. Within this framework, a UAV searches and monitors the target, a USV acts as a communication relay, and a swarm of UUVs hunts the target. Moreover, to improve the timeliness of target search, we propose the age of information (AoI)-based UAV search strategy. Additionally, we construct a constrained multi-objective optimization problem aiming to minimize energy consumption and mission duration by optimizing vehicles' trajectories, considering mobility limitations, safety, and connectivity constraints. To tackle this problem, we design an AoI- and energy-aware deep reinforcement learning (DRL) algorithm to optimize control policies for heterogeneous vehicles. The experimental results demonstrate that the proposed scheme outperforms the baseline schemes in terms of energy consumption, and mission duration and success rates.},
  archive      = {J_TMC},
  author       = {Xiangwang Hou and Tianyu Xing and Jingjing Wang and Jun Du and Chunxiao Jiang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3581836},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic topology and resource allocation for distributed training in mobile edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3581510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC), edge servers and mobile terminals use federated learning distributed architecture to build a deep model, so that terminals can cooperate in training without sharing data. Distributed training requires network virtualization to provide high bandwidth and low latency characteristics to support large-scale parallel computing. Traditional virtual network embedding (VNE) relies on a static network topology, which lacks flexibility and incurs high resource costs during model training. To improve the efficiency of embedding distributed training tasks, we propose a novel Node Selection and Dynamic Topology resource allocation scheme for VNE of distributed training, NSDT-VNE, based on reconfigurable network topology. This algorithm divides the underlying network into static and dynamic topologies, enhancing low latency for small flows while providing high bandwidth for large flows as needed. Additionally, we introduce a two-phase coordinated alternating optimization algorithm that optimizes embedding decisions at both computational and topological levels, ensuring optimal node selection. Overall, NSDT-VNE follows demand-aware network design principles, allowing continuous optimization of the underlying topology. Compared to state-of-the-art heuristic and reinforcement learning-based virtual network algorithms, NSDT-VNE achieves superior performance, with request acceptance rates improving by 6.67% to 25.68% and embedding revenue increasing by approximately 7% to 32%.},
  archive      = {J_TMC},
  author       = {Weibei Fan and Donglai Wang and Fu Xiao and Yiping Zuo and Mengjie Lv and Lei Han and Sun-Yuan Hsieh},
  doi          = {10.1109/TMC.2025.3581510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic topology and resource allocation for distributed training in mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring sashimi freshness, i.e., histamine levels, in showcases poses a critical challenge for sushi restaurants and fresh food stores. Current histamine monitoring methods involve labor-intensive chemical experiments or expensive devices, making affordable on-site monitoring difficult. This paper proposes FreshSpec, a low-cost and automatic spectral imaging system capable of precisely monitoring histamine levels in sashimi with minimal human intervention. The low concentration of histamine, combined with the potential for other ingredients to mask its spectral characteristics, complicates precise histamine level predictions using coarse or redundant spectral data from low-cost devices. To address this issue, FreshSpec employs an innovative feature- wise spectral reconstruction (SR) framework that effectively eliminates irrelevant and redundant data while preserving critical histamine-related spectral features. Specifically, we redefine the SR reconstruction target by utilizing features derived from the encoder of the spectral foundation model that is enhanced to focus on histamine-related spectral features. Furthermore, inspired by the monotonic accumulation properties of histamine over time, we propose a histamine regression model with unsupervised continual adaptation to new sashimi samples during practical deployment. Experimental results from 240 samples of salmon, tuna, and snapper demonstrate that FreshSpec achieves an R2 of 0.9319 and an RMSE of 3.101 mg/100 g, comparable to laboratory spectral imaging systems, while outperforming baseline schemes with a 46.95% RMSE reduction and a 0.1631 R2 improvement.},
  archive      = {J_TMC},
  author       = {Yinan Zhu and Haiyan Hu and Baichen Yang and Qianyi Huang and Qian Zhang and Wei Li},
  doi          = {10.1109/TMC.2025.3581714},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3581716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the system design and experimental validation of integrated sensing and communication (ISAC) for environmental sensing, which is expected to be a critical enabler for next-generation wireless networks. We advocate exploiting orthogonal time frequency space (OTFS) modulation for its inherent sparsity and stability in delay- Doppler (DD) domain channels, facilitating a low-overhead environment sensing design. Moreover, a comprehensive environmental sensing framework is developed, encompassing DD domain channel estimation, target localization, and experimental validation. In particular, we first explore the OTFS channel estimation in the presence of fractional delay and Doppler shifts. Given the estimated parameters, we propose a three-ellipse positioning algorithm to localize the target's position, followed by determining the mobile transmitter's velocity. Additionally, to evaluate the performance of our proposed design, we conduct extensive simulations and experiments using a software-defined radio (SDR)-based platform with universal software radio peripheral (USRP). The experimental validations demonstrate that our proposed approach outperforms the benchmarks in terms of localization accuracy and velocity estimation, confirming its effectiveness in practical environmental sensing applications.},
  archive      = {J_TMC},
  author       = {Jun Wu and Yuye Shi and Weijie Yuan and Qingqing Cheng and Buyi Li and Xinyuan Wei},
  doi          = {10.1109/TMC.2025.3581716},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3581922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an edge-assisted massive mobile live streaming (MMLS) framework named LOCA, integrating chunk-level analysis and long-term optimization to design resource allocation, bitrate adaptation, and source selection strategies. The proposed method ensures sustained real-time video delivery while minimizing latency and communication costs. Firstly, a chunk-level analysis of the entire process of video streaming is introduced, aiming at modeling fetch queue waiting time and rebuffering duration in each time slot. By embedding this mathamatical model into consideration, a long-term optimization is formulated to minimize rebuffering and communication overhead while maintaining high video qualities for massive users. Leveraging Lyapunov optimization, we transform this problem into a computationally tractable form. Further simplification via linearization achieves near-optimal solutions by adopting the mixed-integer linear programming method with enhanced computational efficiency. Simulation results demonstrate superior stability and long-term performance compared to the state-of-theart and baseline methods, validating the framework's efficacy in MMLS scenarios},
  archive      = {J_TMC},
  author       = {Fangzheng Feng and Yu Zhang and Xinkun Zheng and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581922},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling cross-band backscatter communication with twaltz. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3581900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency switching is a fundamental capability for wireless communication systems. However, this capability is significantly constrained in backscatter systems. The difficulty is to generate tunable high-frequency modulation signals on a backscatter tag at an acceptable power budget. In this paper, we present Twaltz, a new design paradigm for backscatter communication that enables frequency switching across large frequency bands. By exploiting a low-power semiconductor device, i.e., tunnel diode, and carefully addressing its physical features, Twaltz generates oscillation signals up to 1.2 GHz while maintaining micro-watt level power consumption. Twaltz further facilitates on-tag oscillation signal stabilization and programmable oscillation frequency tuning. We prototype Twaltz on a PCB board, demonstrating its efficiency in cross-band communication for LoRa backscatter, and verifying its performance in concurrent transmission, channel hopping, and data transmission.},
  archive      = {J_TMC},
  author       = {Xiuzhen Guo and Boya Liu and Nan Jing and Chaojie Gu and Yuanchao Shu and Jiming Chen},
  doi          = {10.1109/TMC.2025.3581900},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling cross-band backscatter communication with twaltz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3581983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage the vast amounts of onboard data while ensuring privacy and security, federated learning (FL) is emerging as a promising technology for supporting a wide range of vehicular applications. Although FL has great potential to improve the vehicular edge intelligence(VEI), challenges arise due to vehicle mobility, wireless channel instability, and data heterogeneity. To mitigate the issue of heterogeneous data across vehicles in FL, artificial intelligence-generated content (AIGC) can be employed as an innovative data synthesis technique to enhance FL model performance. In this paper, we propose AIGCassisted Federated Learning for Vehicular Edge Intelligence (GenFV). We further propose a weighted policy using the Earth Mover's Distance (EMD) to measure data distribution heterogeneity and introduce a convergence analysis for GenFV. Subsequently, we analyze system delay and formulate a mixedinteger nonlinear programming (MINLP) problem to minimize system delay. To solve this MINLP NP-hard problem, we propose a two-scale algorithm. At large communication scale, we implement label sharing and vehicle selection based on mobility and data heterogeneity. At the small computation scale, we optimally allocate bandwidth, transmission power and amount of generated data. Extensive experiments show that GenFV significantly improves the performance and robustness of FL in dynamic, resource-constrained environments, outperforming other schemes and confirming the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Xianke Qiang and Zheng Chang and Geyong Min},
  doi          = {10.1109/TMC.2025.3581983},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3581929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are being widely employed in wireless communication applications, e.g., collecting data from ground nodes (GNs). Minimizing UAV energy in these applications is crucial due to the limited energy supply onboard. Unlike previous studies that assume UAVs fly at a fixed altitude and simplify the energy consumption model of UAVs, we consider the impact of varying UAV altitudes on the ground-to-air communication and utilize a general communication model for GN. Furthermore, we conduct real-world flight tests and introduce a practical speed-related flight energy consumption model of UAVs. This paper focuses on the UAV altitude-speed scheduling and GN transmission switching (UASS-GTS) problem, specifically in scenarios where the UAV flies straight for monitoring applications such as power transmission lines, roads, and water/oil/gas pipes. However, minimizing energy consumption presents challenges due to the tight coupling of altitude scheduling and speed scheduling. To tackle this, first, we develop the looking before crossing algorithm for speed scheduling. We then extend this algorithm by integrating altitude scheduling to propose the Altitude-Speed Scheduling of UAV for Minimizing Energy (ASSUME) algorithm, using a dynamic programming method. The ASSUME algorithm is theoretically proven to be optimal. Additionally, based on ASSUME, we propose an offline-inspired online heuristic algorithm to handle agnostic situations where GN information is not available unless flies close. Simulations indicate that the ASSUME algorithm saves an average of 26.1%–62.7% energy compared to the baseline methods, and the performance gap between the online algorithm and the offline optimal algorithm ASSUME is 22.8%.},
  archive      = {J_TMC},
  author       = {Jianping Huang and Feng Shan and Junzhou Luo and Runqun Xiong and Wenjia Wu},
  doi          = {10.1109/TMC.2025.3581929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3581905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For reliability, security and scalability, Multicast Request flows (MRs) need to traverse a Service Function Chain (SFC) that consists of series of Virtual Network Functions (VNFs) such as firewalls, encoder-decoder in Network Function Virtualization-enabled Software-Defined Networks (NFV-enabled SDNs). There are typically multiple identical VNF-instances in the network, it brings significant challenges when dynamically choosing or placing the requisite VNF-instances to construct a Service Function Tree (SFT) consisting of SFCs for fulfilling the MRs's routing. This paper investigates the Delay and Jitter Aware Dynamic SFT Embedding and Routing Problem (DJADSERP) considering VNF placement, network resources, delay and jitter constraints as well as network load balance in NFVEnabled SDNs. First, we formulate DJA-DSERP as an integer linear programming model and prove it to be NP-hard. Then, an auxiliary edge-weighted graph and an Optimal Link Selection Function (OLSF) are devised, and SFT Embedding Algorithm (SFT-EA) is proposed to address the problem aiming at minimizing the resource consumption costs while satisfying multiple QoS constraints and network load balance. Furthermore, we theoretically prove the effectiveness of the OLSF and the SFTEA. Simulation results demonstrate that the SFT-EA exhibits superior performance compared to existing algorithms in terms of throughput, traffic acceptance rate, and network load balance.},
  archive      = {J_TMC},
  author       = {Liang Liu and Siyuan Tan and Songtao Guo and Guiyan Liu and Hao Feng},
  doi          = {10.1109/TMC.2025.3581905},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control (TSC) plays a crucial role in the intelligent transportation system. Among existing TSC approaches, Proactive Traffic Signal Control (PTSC) predicts future traffic states at intersections and proactively adjusts control policies. It is evident that PTSC methods are highly effective in alleviating both current and future traffic congestion at intersections. However, existing PTSC methods focus on point estimation prediction while neglecting prediction reliability. Additionally, they fail to adaptively coordinate between current and future traffic states for optimal control. To address these limitations, we propose an innovative Proactive-Plugin that can be combined with existing TSC methods to enhance the accuracy and robustness of traffic signal control policies. This plugin enhances two critical aspects: 1) Prediction reliability is achieved through Fine-grained Traffic Uncertainty Quantification. This module generates probabilistic forecasts along with confidence intervals to explicitly indicate the credibility of the predictions. 2) Coordination adaptiveness is enabled by a Current-Future Tradeoff Integration mechanism. This mechanism dynamically adjusts the relative influence of current traffic states and probabilistic forecasts on control policies. To further ensure robustness, we design a multi-task joint optimization to reduce the negative impact of inaccurate predictions during training. Experimental results on six real-world datasets demonstrate consistent improvements in traffic efficiency, validating the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Yang Jiang and Shengnan Guo and Hanyang Chen and Xiaowei Mao and Youfang Lin and Huaiyu Wan},
  doi          = {10.1109/TMC.2025.3581938},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient verifiable data streaming without cryptographic accumulator. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3582087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) enables the client to incrementally store a sequence of ordered data on an untrusted cloud server, and verify the validity of the retrieved data. Moreover, the client can replace a data with another value. The common security problem caused by updating operation is the cloud server may use old authentication information to make expired data pass the verification. To solve this problem, the known approaches use the cryptographic accumulator that actually influences the performance of VDS scheme. The main concerns can be generalized as how to design a VDS scheme without cryptographic accumulator, in such a way that further optimizes the performance of VDS scheme. We put forward the idea to convert the standard digital signature relevant to the updated data into chameleon digital signature whose non-transferability is the key to solve the problem. This is the first attempt to securely authenticate the dynamic data without cryptographic accumulator. In the proposed VDS scheme, the client's local storage overhead, computation overheads of the cloud server in responding to a query and updating the data are constant. As the experimental results shown, the proposed VDS scheme outperforms the scheme in terms of the efficiency.},
  archive      = {J_TMC},
  author       = {Haining Yang and Jinlu Liu and Pingyuan Zhang and Jing Qin and Huaxiong Wang},
  doi          = {10.1109/TMC.2025.3582087},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards efficient verifiable data streaming without cryptographic accumulator},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3582084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human digital twin bridges humans with digital avatars in the fabric metaverse, assisting users and healthcare professionals with real-time visualization, analysis, and prediction of personal data sensed by fabric sensors. The human digital twin-assisted healthcare monitoring (HHM) service refreshment refers to sending personal health data to corresponding services hosted on nearby edge servers and receiving the results to update local digital avatars continuously. However, the malicious nature and resource limitations of edge servers may lead to user privacy leaks and refreshment timeout, thereby impacting diagnostics. In this paper, we investigate a novel privacy-enhanced HHM service refreshment maximization problem in the fabric metaverse by considering privacy data encryption, model compression, and personalized user requirements. To this end, we first formulate the above issue as an Integer Linear Programming (ILP) problem, and prove its NP-hardness. Then, a resource scheduler named Wiper is designed, consisting of a shallow-deep distiller and an agile refresher library. To enable efficient inference while preserving user privacy, the former replaces violation modules in existing models with approximations and conducts shallow distillation on model layers to meet operation type and depth limits of homomorphic encryption, and then deep distillation on model parameters to decrease end-to-end refreshment delay. Finally, to satisfy user requirements on accuracy and delay during encrypted refreshments while maximizing the throughput of HHM services in offline and online situations with different problem scales, a series of HHM service refreshment algorithms are merged into the latter, including exact, performance-guaranteed approximation, and residual diffusion reinforcement learning algorithms. Theoretical analyses and experiments demonstrate that our algorithms are promising compared with baseline algorithms.},
  archive      = {J_TMC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato and Gang Wei},
  doi          = {10.1109/TMC.2025.3582084},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3582195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Network Functions Virtualization (NFV) systems into mobile edge and core networks has heightened the need for effective anomaly detection and localization methods. The complexity of NFV demands robust mechanisms for network resilience, security, and performance. Machine Learning approaches have demonstrated promising solutions in crafting adaptive and efficient mechanisms for detecting and localizing potential anomalies within NFV systems. Particularly, Unsupervised Learning (UL) methods have garnered significant attention for their potential to detect anomalies without the need for labeled data. However, UL methods are susceptible to even minor levels of anomalous samples in the training data, termed contamination, which can severely compromise their performance. This paper proposes a novel approach using the Noisy-Student technique for anomaly detection. It addresses data contamination by combining a density-estimation teacher model for pseudo-labeling with a weakly-supervised student model based on a Masked Autoencoder trained on the pseudo-labeled data. For anomaly localization, we introduce a heuristic tailored for our anomaly detection model and two Explainable Artificial Intelligence (XAI)-based approaches applicable to any detection model. Extensive experiments on three NFV datasets demonstrate superior performance, with up to a 20% improvement in anomaly detection and up to a 22% improvement in localization, in terms of F1-score.},
  archive      = {J_TMC},
  author       = {Seyed Soheil Johari and Nashid Shahriar and Massimo Tornatore and Raouf Boutaba and Aladdin Saleh},
  doi          = {10.1109/TMC.2025.3582195},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3582237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of network technology has allowed numerous vehicular applications to be deployed in vehicles, thereby enriching the driving experience of users. However, the openness of vehicular networks enables attackers to launch network attacks on vehicles through network ports, leading to the destruction of vehicular networks. To develop an intrusion detection system suitable for distributed vehicular networks, researchers have utilized federated learning to train detection models. Nevertheless, most federated learning-based vehicular intrusion detection systems seldom consider rapidly updating the detection model and fail to detect unknown attacks effectively. In this study, we propose a federated learning-based vehicular intrusion detection system that fully considers the traffic characteristics of multiple network regions and selects representative clients to participate in model aggregation, thereby accelerating the convergence of the global model. Furthermore, to enhance the robustness of the detection system, we utilize extreme value theory and multilayer activation vectors to construct an unknown attack discriminator that can determine whether a network flow is an unknown attack. Comprehensive experiments on three open datasets demonstrate that the proposed intrusion detection system can quickly update and effectively identify known/unknown attacks in open vehicular networks},
  archive      = {J_TMC},
  author       = {Chunyang Fan and Jie Cui and Hulin Jin and Hong Zhong and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3582237},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video conferencing with predictive generation and collaborative computation across mobile headsets. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3582284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has emerged as a transformative platform for remote collaboration, but its adoption for video conferencing is hindered by challenges related to facial expression reconstruction and computational resource constraints, especially on economical mobile VR headsets. This paper introduces a novel system for VR video conferencing that addresses these challenges through two key modules: Predictive Generation and Collaborative Computation. Predictive Generation leverages multimodal inputs, including voice, head motion, and eye blinks, to synthesize realistic facial animations with low latency, eliminating the need for high-precision hardware. Collaborative Computation enhances computational efficiency by employing a game-theoretic framework for resource sharing among users. Experimental evaluations demonstrate that our system delivers immersive and realistic VR video conferencing experiences with superior facial expression reconstruction and efficient resource utilization. Our approach makes VR video conferencing more accessible and practical for a broader audience across mobile headsets.},
  archive      = {J_TMC},
  author       = {Yili Jin and Xize Duan and Kaiyuan Hu and Fangxin Wang and Xue Liu and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3582284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Video conferencing with predictive generation and collaborative computation across mobile headsets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3582545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce mmOrbit, a mmWave-based rotor orbit measurement system that can estimate rotor orbit by analyzing machinery surface vibration. To measure the 2D rotor orbit, two synchronized mmWave radars are deployed to build the orbit from different viewpoints. The existing literature shows that the micro-displacement measurement accuracy of mmWave radar is not enough to meet the application requirements of micron-level resolution without appropriate fine-gained processing methods. Therefore, we propose a three-step vibration displacement extraction algorithm with sliding table to extract mechanical vibration micro-displacement and increase measurement precision. Accurate mechanical vibration displacements in two vertical directions are used to estimate the 2D rotor orbit. Our extensive experiments indicate that mmOrbit can accurately measure mechanical vibration micro-displacement with an error of 4.4 μm for the 80th percentile. Furthermore, mmOrbit can estimate 2D rotor orbit with high precision, showing an orbit eccentricity error of 7%, an orbit direction error of 7 °, and a disjoint area proportion of 17% for the 80th percentile. The imbalance detection experiment verifies the accuracy and dependability of our measured rotor orbit.},
  archive      = {J_TMC},
  author       = {Changlin Mao and Haocheng Ni and Jianpin Han and Yingxiao Wu},
  doi          = {10.1109/TMC.2025.3582545},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast multimodal edge inference via selective feature distillation. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3580102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring user status at the edge is essential for delivering personalized services, such as detecting emotional states. However, deploying large-scale models directly on user devices is impractical due to substantial computational overhead and the scarcity of labeled data. Conversely, uploading raw data to the cloud for processing raises significant privacy concerns and incurs prohibitive communication costs. To address this challenge, we propose a privacy-preserving multimodal inference framework that leverages large-scale public data while safeguarding sensitive information and optimizing computational efficiency. Specifically, we first train a teacher model in the cloud using publicly available data. Through a feature distillation process, the knowledge from this teacher model is transferred to a lightweight encoder deployed at the user end. This transfer is tailored to the user's data, ensuring that only relevant knowledge is distilled. To accommodate varying communication constraints, we introduce a feature compression mechanism that significantly reduces communication overhead without compromising inference accuracy. Extensive experiments on emotion recognition tasks demonstrate that the proposed framework effectively balances privacy preservation, resource efficiency, and inference accuracy, facilitating seamless collaboration between cloud and edge devices.},
  archive      = {J_TMC},
  author       = {Jinyu Chen and Wenchao Xu and Yunfeng Fan and Haozhao Wang and Quan Chen and Jing Li},
  doi          = {10.1109/TMC.2025.3580102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast multimodal edge inference via selective feature distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of computing: A metric of computation freshness in communication and computation cooperative networks. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3582741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication and computation cooperative networks (3CNs), timely computation is crucial but not always guaranteed. There is a strong demand for a computational task to be completed within a given deadline. The time taken involves processing time, transmission time, and the impact of the deadline. However, a measure of such timeliness in 3CNs is lacking. To address this gap, we propose the novel concept of Age of Computing (AoC) to quantify computation freshness in 3CNs. Built on task timestamps, AoC serves as a practical metric for dynamic and complex real-world 3CNs. We evaluate AoC under two types of deadlines: (i) soft deadline, tasks can be fed back to the source if delayed beyond the deadline, but with additional latency; (ii) hard deadline, tasks delayed beyond the deadline are discarded. We investigate AoC in two distinct networks. In point-to-point, time-continuous networks, tasks are processed sequentially using a first-come, first-served discipline. We derive a general expression for the time-average AoC under both deadlines. Utilizing this expression, we obtain a closed-form solution for M/M/1-M/M/1 systems under soft deadlines and propose an accurate approximation for hard deadlines. These results are further extended to G/G/1-G/G/1 systems. Additionally, we introduce the concept of computation throughput, derive its general expression and an approximation, and explore the trade-off between freshness and throughput. In the multi-source, time-discrete networks, tasks are scheduled for offloading to a computational node. For this scenario, we develop AoC-based Max-Weight policies for real-time scheduling under both deadlines, leveraging a Lyapunov function to minimize its drift.},
  archive      = {J_TMC},
  author       = {Xingran Chen and Yi Zhuang and Kun Yang},
  doi          = {10.1109/TMC.2025.3582741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of computing: A metric of computation freshness in communication and computation cooperative networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NestQuant: Post-training integer-nesting quantization for on-device DNN. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3582583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models. Code: https://github.com/jianhayes/NESTQUANT.},
  archive      = {J_TMC},
  author       = {Jianhang Xie and Chuntao Ding and Xiaqing Li and Shenyuan Ren and Yidong Li and Zhichao Lu},
  doi          = {10.1109/TMC.2025.3582583},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NestQuant: Post-training integer-nesting quantization for on-device DNN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3582683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional collaborative machine learning (CML) faces significant challenges in resource-constrained environments, such as emergency scenarios with limited power, bandwidth, and computing resources, leading to increased communication delays and energy consumption. To address these issues, this paper introduces Air-CoKD, a novel CML framework designed to reduce resource consumption and training latency while preserving model performance. Air-CoKD leverages knowledge distillation (KD) to minimize data transmission by avoiding the direct sharing of model parameters. It also integrates over-the-air computation (AirComp) to aggregate local logits, optimizing bandwidth utilization. To address the dimensional differences in local logits caused by the unbalanced device data class, Air-CoKD employs orthogonal frequency division multiplexing (OFDM) to transmitting local logits for different target classes. To handle aggregation errors introduced by AirComp, we conduct a detailed analysis of error bounds. Specifically, we convert the Kullback-Leibler (KL) divergence, used in KD loss function, into a quadratic upper bound for precise error quantification and effective optimization. Based on these insights, we propose a strategy to manage bandwidth constraints, transmission power limits, and device energy budgets within Air-CoKD. Extensive simulations demonstrate that Air-CoKD surpasses state-of-the-art methods, effectively balancing training efficiency and model performance. The framework proves to be a robust solution for CML in resource-constrained networks.},
  archive      = {J_TMC},
  author       = {Yue Zhang and Guopeng Zhang and Kun Yang and Yao Wen and Kezhi Wang},
  doi          = {10.1109/TMC.2025.3582683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3582755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel federated framework for constructing the digital twin (DT) model, referring to a living and self-evolving visualization model empowered by artificial intelligence, enabled by distributed sensing under edge-cloud collaboration. In this framework, the DT model to be built at the cloud is regarded as a global one being split into and integrating from multiple functional components, i.e., partial-DTs, created at various edge servers (ESs) using feature data collected by associated sensors. Considering time-varying DT evolutions and heterogeneities among partial-DTs, we formulate an online problem that jointly and dynamically optimizes partial-DT assignments from the cloud to ESs, ES-sensor associations for partial-DT creation, and as well as computation and communication resource allocations for global-DT integration. The problem aims to maximize the constructed DT's model quality while minimizing all induced costs, including energy consumption and configuration costs, in long runs. To this end, we first transform the original problem into an equivalent hierarchical game with an upper-layer two-sided matching game and a lower-layer overlapping coalition formation game. After analyzing these games in detail, we apply the Gale-Shapley algorithm and particularly develop a switch rules-based overlapping coalition formation algorithm to obtain short-term equilibria of upper-layer and lower-layer subgames, respectively. Then, we design a deep reinforcement learning-based solution, called DMO, to extend the result into a long-term equilibrium of the hierarchical game, thereby producing the solution to the original problem. Simulations show the effectiveness of the introduced framework, and demonstrate the superiority of the proposed solution over counterparts.},
  archive      = {J_TMC},
  author       = {Ruoyang Chen and Changyan Yi and Fuhui Zhou and Jiawen Kang and Yuan Wu and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3582755},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-edge collaborative optimization of microservice caching in D2D-assisted network. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3582579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing the caching resources of end users via Device-to-Device (D2D) communication to assist the edge server in microservice caching is promising to further alleviate the network congestion of the Internet of Things (IoT). However, significant extra energy consumption prevents the caching system from maximizing cache utility if all end users cache simultaneously. In this paper, we propose two novel end-edge collaborative microservice caching algorithms in D2D-assisted networks. First, we construct a D2D caching sharing link graph from the aspects of physical and social attributes of end users and introduce the Entropy-based Partitioning Around Medoid (EPAM) algorithm to identify critical users. Second, to address the challenges posed by unknown time-varying user preferences, we model the end-edge collaborative caching problem as a Multi-Agent Multi-Armed Bandit (MAMAB) problem, thus developing two caching decision schemes, i.e, Edge-Centric Scheme (ECS) and User-Centric Scheme (UCS), to accommodate different decision sequences. The simulation results show that the EPAM-ECS and EPAM-UCS have at least 29.2% and 39.3% improvement compared with other baseline algorithms.},
  archive      = {J_TMC},
  author       = {Qingyong Deng and Mengyao Li and Zhetao Li and Haolin Liu and Yong Xie},
  doi          = {10.1109/TMC.2025.3582579},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-edge collaborative optimization of microservice caching in D2D-assisted network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user-centric energy-saving method for dynamic 5 g heterogeneous networks using deep reinforcement learning. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3582623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption (EC) represents a significant challenge for 5 G and 6 G mobile networks, necessitating a primary focus on optimizing energy savings (ES). This paper illustrates the practical benefits of a user-centric deep reinforcement learning (DRL) models in achieving a green cellular network. The primary objective is to optimize energy usage in a heterogeneous network (HetNet). The optimization of power consumption (PC) in such networks is a non-convex and NP-hard problem. To address this challenge, we propose using reinforcement learning (RL). Due to the extensive state and action space, classical RL approaches are unsuitable. Therefore, the adoption of DRL methods, notably the deep Q-network (DQN) and deep deterministic policy gradient (DDPG) methods, is necessary. The proposed approach entails a user-centric connection establishment, whereby small base stations (SBSs) are switched to an on mode. The mode switching determined by the DRL methods is controlled by an anti-abrupt transition mechanism, which prevents unnecessary oscillations in the network. The results are benchmarked against existing approaches, specifically genetic algorithm (GA) and particle swarm optimization (PSO) for ES. The proposed methods outperform both GA and PSO optimization techniques in terms of ES and significantly reduce time consumption, enhancing its practical implementation feasibility.},
  archive      = {J_TMC},
  author       = {Mohammad Ali Arami and Erfan Rasti and Abbas Mohammadi},
  doi          = {10.1109/TMC.2025.3582623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A user-centric energy-saving method for dynamic 5 g heterogeneous networks using deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems. <em>TMC</em>, 1-11. (<a href='https://doi.org/10.1109/TMC.2025.3582750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed reconfigurable intelligent surfaces (RISs) provide rich macro-diversity coverage due to different locations of the RISs, which is beneficial to combat coverage holes. However, the system performance relies on the effective coordination of multiple RISs. In particular, distributed RIS-assisted power allocation and the phase shifts of RISs should be jointly designed under nonlinear scheduling constraints. Thus, the resource allocation scheme for distributed RIS-assisted multiuser system is a crucial challenge. To tackle these issues, joint power allocation, phase shifts and communication scheduling design for distributed RIS-assisted systems is investigated in this paper, where all RISs simultaneously and cooperatively serve multiple users. To overcome the formulated nonconvex optimization problem, the original problem is decoupled into three subproblems and solved in an iterative manner. Specifically, we first consider the subproblem of power allocation, which can be solved via maximizing the ergodic achievable rate. By applying the ergodic rate, an approximate closed-form solution is formed for the power allocation. Subsequently, the phase shifts are optimized using the minimization-maximization optimization methods. Finally, a communication scheduling scheme is presented to address the scheduling variables. Numerical simulations are conducted to demonstrate that the considered solution outperforms the existing benchmark and achieves a near-optimal spectral efficiency.},
  archive      = {J_TMC},
  author       = {Zhen Chen and Gaojie Chen and Xiu Yin Zhang and Jie Tang and Shi Jin and Kai-Kit Wong and Jonathon Chambers},
  doi          = {10.1109/TMC.2025.3582750},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3582833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of future network and control technologies has enabled unmanned aerial vehicles (UAVs) to collaborate across diverse geographical areas and task domains, enhancing task execution efficiency through data and resource sharing. In response to the increasing demand for cross-domain task allocation and operations for UAVs, establishing robust authentication mechanisms within trusted domains has become a critical foundation for ensuring secure cross-domain access. Despite significant progress in UAV identity authentication and cross-domain access, challenges persist, such as cumbersome and inefficient processes, UAV resource limitations, and establishing trust relationships across different domains. To address these challenges, this paper introduces a dual blockchain-assisted trusted authentication scheme for UAVs' cross-domain access. Our approach utilizes a certificateless signcryption algorithm for lightweight UAV authentication, thereby eliminating the need for certificate management. Then, an efficient credit-based trust model is designed to measure the trustworthiness of data-in-transit and cross-domain entities. Furthermore, blockchain technology is introduced to store the relevant information of UAVs and credibility to assist cross-domain authentication. Theoretical security analysis and extensive simulations have been conducted, demonstrating the effectiveness and efficiency of our proposed scheme.},
  archive      = {J_TMC},
  author       = {Mingyue Xie and Zheng Chang and Li Wang and Geyong Min},
  doi          = {10.1109/TMC.2025.3582833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3582864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating vision language models (VLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model's focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36% compared to DDPG and accelerates convergence by reducing required steps by up to 47% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4% improvement in QoE when scaling from 4 to 8 vehicles.},
  archive      = {J_TMC},
  author       = {Ruichen Zhang and Changyuan Zhao and Hongyang Du and Dusit Niyato and Jiacheng Wang and Suttinee Sawadsitang and Xuemin Shen and Dong In Kim},
  doi          = {10.1109/TMC.2025.3582864},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3582060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has driven the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival time of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slow data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to opportunistic inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a computational approach to control this affinity in open-world mobile environments. ${\sf AdaFlowLite}$ pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an multi-modal lightweight Swin Transformer (MMLST), ${\sf AdaFlowLite}$ facilitates real-time and flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that ${\sf AdaFlowLite}$ significantly reduces inference latency by up to 80.4% and enhances accuracy by up to 62.1%, while achieving nearly a 50% reduction in energy consumption, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.},
  archive      = {J_TMC},
  author       = {Sicong Liu and Fengmin Wu and Yuan Gao and Bin Guo and Zimu Zhou and Hongkai Wen and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3582060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cell-free massive MIMO detection: A distributed expectation propagation approach. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3583019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive MIMO is one of the core technologies for next-generation wireless networks. It is expected to bring enormous benefits, including ultra-high reliability, data throughput, energy efficiency, and uniform coverage. However, the radically distributed architecture of cell-free massive MIMO necessitates new paradigms for transceiver design, especially by exploiting efficient distributed processing algorithms. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO, which consists of two modules: a nonlinear module at the central processing unit (CPU) and a linear module at each access point (AP). The turbo principle in iterative channel decoding is utilized to compute and pass the extrinsic information between the two modules. An analytical framework is provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Furthermore, a distributed iterative channel estimation and data detection (ICD) algorithm is developed to handle the practical scenario with imperfect channel state information (CSI). Simulation results will show that the proposed method outperforms existing detectors for cell-free massive MIMO systems in terms of the bit-error rate and the developed theoretical analysis can be utilized as an asymptotic lower bound. Finally, it is shown that with imperfect CSI, the proposed ICD algorithm can significantly improve the system performance and reduce the pilot overhead.},
  archive      = {J_TMC},
  author       = {Hengtao He and Xianghao Yu and Jun Zhang and Shenghui Song and Ross D. Murch and Khaled B. Letaief},
  doi          = {10.1109/TMC.2025.3583019},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cell-free massive MIMO detection: A distributed expectation propagation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pricing game for federated learning supporting lightweight local model training. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3583013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive distribution of data across clients with privacy concerns and heterogeneous performance in edge networks presents a significant opportunity to enhance AI model performance. Federated learning (FL) enables a model owner (MO) to recruit these clients, offering compensation for their contributions, and to improve model quality by aggregating knowledge from their locally trained models. However, several challenges arise in this process. Clients may decline participation if they do not achieve positive utility. Moreover, due to constraints in memory, computing, and communication resources, some clients can only train lightweight models that represent partial versions of the global model. Importantly, the MO's pricing for client contributions and the proportions of local model training are interdependent, collectively influencing client utilities and participation decisions. To address these challenges, we first model the utility functions of both the MO and the clients, accommodating the support for lightweight local models. We then formulate their interactions as a Stackelberg game and theoretically prove the existence of a Nash equilibrium. Based on this equilibrium, we derive optimal collaboration strategies for both the MO and the clients. Additionally, we design an efficient approximation algorithm to enable the MO to maximize its utility by selecting suitable clients to participate in FL. Finally, extensive experiments validate our theoretical findings, demonstrating the superior performance and effectiveness of the proposed algorithms},
  archive      = {J_TMC},
  author       = {Fengsen Tian and Mingzi Wang and Yu Zhang and Guoqiang Deng and Lingyu Liang and Xinglin Zhang},
  doi          = {10.1109/TMC.2025.3583013},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A pricing game for federated learning supporting lightweight local model training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks for the optimization of collaborative federated learning energy efficiency. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3582911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the design of an energy efficient collaborative federated learning (CFL) methodology using which mobile devices exchange their FL model with a subset of their neighbors without reliance on a parameter server based on the distributed graph neural network (GNN) method. Each device is unable to send its FL model to every neighboring device due to device mobility and wireless resource limitations. To reduce the energy consumption of FL model transmission, each device must choose a subset of devices with which to share its FL model. This problem is formulated as an optimization problem to meet the constraints of delay and training loss while minimizing the energy consumption for model transmission. However, the formulated problem is difficult to solve since the device mobility patterns, and the relationship between the device connection scheme and CFL performance are unknown. To address this challenge, we analytically characterize the relationship between dynamic device connections and the performance of CFL methodology. Based on the analysis, a GNN based algorithm is proposed to enable each device to select a subset of its neighbors and the transmit power in a decentralized method. Compared to standard optimization methods that must determine device connections in a centralized manner, the GNN based method enables each device to use its neighboring devices' location and connection information to individually determine a subset of devices to transmit the local model. Given the device connections, the optimal transmit power of each device can be determined by convex optimization. Simulation results show that the proposed method can reduce the energy consumption for model transmission and training loss by up to 46% and 2%, respectively},
  archive      = {J_TMC},
  author       = {Nuocheng Yang and Sihua Wang and Yuchen Liu and Christopher G. Brinton and Changchuan Yin and Mingzhe Chen},
  doi          = {10.1109/TMC.2025.3582911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Graph neural networks for the optimization of collaborative federated learning energy efficiency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The price of forgetting: Incentive mechanism design for machine unlearning. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3582904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data protection policies (e.g., GDPR) enforce the right to be forgotten and require companies to perform machine unlearning once users request data removal. This process incurs costs for server and degrades model performance, impacting users' satisfaction. In this paper, we propose the first incentive mechanism for machine unlearning, where server compensates users to retain their data. We characterize server's major unlearning costs, accuracy degradation and consumed time, in data redemption amount through experiments on three datasets and two unlearning algorithms. We model server-users interaction as a two-stage Stackelberg game. In Stage I, server optimizes compensation unit prices to minimize costs. In Stage II, users jointly decide data redemption amounts as a non-cooperative game. By restricting the feasible set of Stage I to Nash Equilibrium of Stage II, we formulate a challenging non-convex bilevel optimization problem. We propose an iterative algorithm to compute optimal unit prices in Stage I and equilibrium data redemption amounts in Stage II by characterizing bilevel problem's convexity. We prove the distributed convergence of best response updates to the unique Nash equilibrium by showing Stage II is a submodular game. Experimental results show that our mechanism minimizes server cost and maximizes social welfare over two practical baselines},
  archive      = {J_TMC},
  author       = {Yue Cui and Man Hon Cheung},
  doi          = {10.1109/TMC.2025.3582904},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The price of forgetting: Incentive mechanism design for machine unlearning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning. <em>TMC</em>, 1-17. (<a href='https://doi.org/10.1109/TMC.2025.3582980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of low-cost mobile edge devices, security and reliability have become crucial for mobile edge computing networks, especially for high-stakes applications. To facilitate it, Radio Frequency Fingerprint Identification (RFFI) has emerged as a promising physical layer security paradigm, offering a non-cryptographic and lightweight solution. However, most existing Deep Learning (DL) based RFFI methods operate under a closed-set assumption, limiting their ability to recognize devices not seen during training and posing a risk of misclassification. Addressing the open-set RFFI problem is critical for real-world deployments, where the system must handle both known and unknown devices, ensuring robust security in dynamic environments. In this paper, we propose OpenRFI, a novel test-time fine-tuning-based RFFI framework, consisting of two sequential stages: pre-training and test-time fine-tuning. During the pre-training stage, we design a data augmentation module, a feature extraction module, and an efficient hybrid loss function to minimize intra-class feature distances and tighten decision boundaries, enhancing the model's ability to distinguish between different classes. In the test-time fine-tuning stage, we introduce a fine-tuning dataset construction module and a full-parameter fine-tuning module to dynamically adapt to the test environment and capture information from unknown samples, further improving open-set recognition. We theoretically establish the performance boundary of the fine-tuning dataset construction method, providing insights into its robustness and scalability. Extensive numerical results based on an open source dataset demonstrate the effectiveness of the proposed OpenRFI framework in comparison with existing baselines.},
  archive      = {J_TMC},
  author       = {Jian Yang and Shuai Feng and Yatong Wang and Xinghang Wu and Mu Yan},
  doi          = {10.1109/TMC.2025.3582980},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3583153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers have become popular for deploying applications in Edge Computing (EC) for their seamless integration and easy deployment. Frequent container updates are essential to enhance performance and introduce new challenges for cutting-edge applications such as large language models and digital twins. However, traditional container update methods result in substantial download costs and task interruptions, which are unacceptable for latency-sensitive tasks in resource-constrained EC. Existing work has largely overlooked the layered structure of container images. By leveraging this layered structure, duplicate downloads can be reduced, and various layers can be transferred from other edges, reducing burden on the remote cloud. In this paper, we model the layer-aware container update problem with edge-cloud collaboration to minimize update and scheduling costs. We present the Layer-aware Edge-cloud collaborative Container Update (LECU) algorithm based on reinforcement learning to make container update decisions. Moreover, a task scheduling algorithm is devised to schedule tasks affected by container updates to other edges, minimizing the impact of task interruptions. We implement our LECU algorithm on an edge system with real-world data traces to demonstrate its effectiveness and conduct larger-scale simulations to evaluate its scalability. Results demonstrate that our algorithms reduce container update and task scheduling costs by 14% and 19%, respectively, compared to baselines.},
  archive      = {J_TMC},
  author       = {Hanshuai Cui and Zhiqing Tang and Yuan Wu and Weijia Jia},
  doi          = {10.1109/TMC.2025.3583153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3583055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.},
  archive      = {J_TMC},
  author       = {Lan Sun and Songpengcheng Xia and Jiarui Yang and Ling Pei},
  doi          = {10.1109/TMC.2025.3583055},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3583257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) is a prominent technology for wireless localization, mainly attributed to its superior ranging performance enabled by the large signal bandwidth. However, its bearing capability remains underdeveloped due to practical issues such as phase deviations, antenna coupling, and phase ambiguity. This paper presents a high-accuracy stereo UWB bearing scheme through network ambiguity resolution and online phase calibration. Specifically, we propose a sparse variational Gaussian process regression-based calibration technique to eliminate phase deviations and a range-assisted network solution to resolve phase ambiguities. Building on these techniques, we present an online angle estimation scheme that performs real-time phase calibration, ambiguity resolution, and calibration model updates, significantly reducing calibration complexity in large-scale networks. Real-world experiments on 4-element stereo UWB platforms achieve root mean square errors of 2.3$^{\circ }$ and 1.1$^{\circ }$ for azimuth and elevation angles, respectively. The success rate for ambiguity resolution exceeds 96%, a 20% improvement over existing methods.},
  archive      = {J_TMC},
  author       = {Yi Li and Hanying Zhao and Yiman Liu and Yuan Shen},
  doi          = {10.1109/TMC.2025.3583257},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs). <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3583345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative sensing has emerged as a novel sensing paradigm, entailing multi-sensor data sharing and multimodal modeling to collaboratively understand sensing behaviors. However, current solutions, i.e., data-level and decision-level fusion methods, fall short of generality, expert knowledge, and holistic/chronic perspective. In this paper, we propose LLMCoSen to revisit collaborative sensing with Large Language Models (LLMs). Specifically, LLM-CoSen designs a semantic-level fusion approach for inference results for collaborative sensing. Such an approach is characterized by its generality, making it applicable to any heterogeneous devices, and its expert knowledge incorporation, which provides chronic, holistic, and insightful perspectives on the inference results. Regarding inference absence challenges, we propose a personalized model design method to constrain inference time, and a voting-based two-pass prompt engineering strategy for token completion. Regarding inference error challenges, we propose an accuracy restoration strategy for personalized models, and a two-level error estimator coupled with self-correction. Experimental results of human digital system use case on four corresponding benchmark datasets show LLM-CoSen can decrease inference absence by 72.83% and inference errors by 7.65% on average.},
  archive      = {J_TMC},
  author       = {Xingyu Feng and Zehua Sun and Zhuangzhuang Chen and Chengwen Luo and Zhangbing Zhou and Victor C.M. Leung and Weitao Xu},
  doi          = {10.1109/TMC.2025.3583345},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs)},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3583499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By harnessing the capabilities of unmanned aerial and ground vehicles (UAVs and UGVs), equipped with high-precision sensors, air-ground mobile crowdsensing (AG-MCS) has proven to be effective for data collection in urban environments. In this paper, by optimizing the metric of age-of-information (AoI) that measures the freshness of collected data, we consider the problem of AoI-Aware AG-MCS (A3G-MCS), where UGVs dispatch UAVs from multiple UGV stops to collect data from point-of-interests (PoIs). We propose a novel multi-agent curriculum learning framework called “MACL(MCS)”, that explicitly balances the individual and team goals of both UAV/UGV controllers to facilitate the exploration of policy towards globally-optimal performance. It is further enhanced by a UAV/UGV collaborative observation augmentation (COA) module for improved inter-controller communication. Extensive results reveal that MACL(MCS) consistently outperforms five baselines, and achieves comparable performance to exact method with better scalability and efficiency. It also showcases strong generalization capability towards real-world scenarios on both TSPLIB and Purdue, KAIST and NCSU datasets.},
  archive      = {J_TMC},
  author       = {Yuxiao Ye and Yuxuan Tian and Chi Harold Liu and Linkang Dong and Guangpeng Qi and Dapeng Wu},
  doi          = {10.1109/TMC.2025.3583499},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach. <em>TMC</em>, 1-18. (<a href='https://doi.org/10.1109/TMC.2025.3583510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-altitude economy (LAE), as a new economic paradigm, plays an indispensable role in cargo transportation, healthcare, infrastructure inspection, and especially post-disaster communications. Specifically, unmanned aerial vehicles (UAVs), as one of the core technologies of the LAE, can be deployed to provide communication coverage, facilitate data collection, and relay data for trapped users, thereby significantly enhancing the efficiency of post-disaster response efforts. However, conventional UAV self-organizing networks exhibit low reliability in long-range cases due to their limited onboard energy and transmit ability. Therefore, in this paper, we design an efficient and robust UAV-swarm enabled collaborative self-organizing network to facilitate post-disaster communications. Specifically, a ground device transmits data to UAV swarms, which then use collaborative beamforming (CB) technique to form virtual antenna arrays and relay the data to a remote access point (AP) efficiently. Then, we formulate a rescue-oriented post-disaster transmission rate maximization optimization problem (RPTRMOP), aimed at maximizing the transmission rate of the whole network. Given the challenges of solving the formulated RPTRMOP by using traditional algorithms, we propose a two-stage optimization approach to address it. In the first stage, the optimal multi-path traffic routing and the theoretical upper bound on the transmission rate of the network are derived. In the second stage, we transform the formulated RPTRMOP into a variant named V-RPTRMOP based on the obtained optimal multi-path traffic routing, aimed at rendering the actual transmission rate closely approaches its theoretical upper bound by optimizing the excitation current weight and the placement of each participating UAV via a diffusion model-enabled particle swarm optimization (DM-PSO) algorithm. Simulation results show the effectiveness of the proposed two-stage optimization approach in improving the transmission rate of the constructed network, which demonstrates the great potential for post-disaster communications. Moreover, the robustness of the constructed network is also validated via evaluating the impact of three unexpected situations on the system transmission rate.},
  archive      = {J_TMC},
  author       = {Xiaoya Zheng and Geng Sun and Jiahui Li and Jiacheng Wang and Qingqing Wu and Dusit Niyato and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3583510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph fusion based semantic communication framework. <em>TMC</em>, 1-14. (<a href='https://doi.org/10.1109/TMC.2025.3583605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communication (SemCom), a paradigm that emphasizes conveying the meaning of information, faces challenges in precise reasoning in semantic coding models. Knowledge graphs (KGs) offer a potential solution by providing structured triples (entities and relations), enabling inference via entity attributes and relational logic. Several key challenges exist in leveraging KGs within SemCom. The first challenge lies in developing methods to create semantic representations aligning and integrating source data and KG information. Second, reconstructing the original data using KGs becomes challenging particularly under poor communication conditions. Moreover, integrating KGs with source data inevitably increases the transmission overhead. In this paper, we propose a novel SemCom framework named KG-SemCom with sophisticated KG-based semantic encoding and decoding designs to solve these challenges. This framework aligns KG entities with message tokens, and then encodes messages into a semantic fusion of contextual and knowledge-based information. Furthermore, KG-SemCom can utilize the KG and contextual relationships to assist in predicting incomplete or distorted messages during the decoding process. Finally, simulation results demonstrate that KG-SemCom achieves higher accuracy and greater robustness compared to existing benchmarks without incorporating KGs, especially in challenging communication environments.},
  archive      = {J_TMC},
  author       = {Chengsi Liang and Yao Sun and Dusit Niyato and Muhammad Ali Imran},
  doi          = {10.1109/TMC.2025.3583605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Knowledge graph fusion based semantic communication framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3583816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Unmanned Aerial Vehicles (UAVs) and autonomous vehicles within the crowdsensing paradigm offers a promising approach to collecting environment-relevant data over large spatial areas, particularly in disaster-stricken or high-risk regions. However, deploying crowdsensing systems in emergency response scenarios presents substantial challenges. The lack of prior environmental knowledge complicates the selection of optimal sensing locations and strategy optimization, often relying on costly trial-and-error methods. Additionally, realtime decision-making is critical in such scenarios, requiring the rapid identification of optimal deployment strategies. Yet, the absence of prior knowledge further complicates the assessment of the optimality of these strategies. This gap remains inadequately addressed in existing research. To address this, we present the first framework that frames these challenges as a rapid online strategy optimization problem for mobile agent-based crowdsensing systems operating in unknown environments during emergency response scenarios. We propose DGap-UCB, a novel approach within the multi-armed bandit (MAB) framework, which efficiently identifies the optimal sensing strategy with highconfidence guarantees. Leveraging the Upper-Confidence Bound (UCB) technique, DGap-UCB iteratively refines strategy selection based on reward feedback. To accelerate learning, we introduce a gap-confidence pair (Δt, δt)-based Quick Stopping Criterion, enabling rapid and high-confidence identification of the optimal strategy. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of DGap-UCB over stateof-the-art techniques},
  archive      = {J_TMC},
  author       = {Shan Su and Liang Wang and Zhiwen Yu and Xiaofang Xia and Lianbo Ma and Fei Xiong and Yao Zhang and Bin Guo},
  doi          = {10.1109/TMC.2025.3583816},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covert communications for intelligent reflecting surface-enabled D2D networks. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3583779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore covert communications in a device-to-device (D2D) network consisting of an intelligent reflecting surface (IRS), a base station, a cellular user, a D2D pair, and an adversary warden. With the help of the IRS, the D2D pair attempts to perform covert communication, while the warden also tries to detect the very existence of such a transmission. To investigate the covert performance under the scenario, we derive the detection error probability at Warden, the optimal detection threshold for minimizing the probability, and the transmission outage probabilities for D2D and cellular communications, respectively. We further jointly optimize the transmission powers of the cellular user and the D2D transmitter, the reflection phase shifts, and the amplitudes of the IRS reflecting elements to improve covert communication performance. Finally, we provide numerical results to reveal the impact of system parameters on the covert performance and also to exhibit the merits of IRS-enabled D2D networks for achieving covert communications.},
  archive      = {J_TMC},
  author       = {Yihuai Yang and Bin Yang and Shikai Shen and Yumei She and Xiaohong Jiang and Tarik Taleb},
  doi          = {10.1109/TMC.2025.3583779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Covert communications for intelligent reflecting surface-enabled D2D networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation. <em>TMC</em>, 1-13. (<a href='https://doi.org/10.1109/TMC.2025.3582421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Time-Frequency Space (OTFS) modulation is an emerging technique that characterizes wireless channels and transmits information in the delay-Doppler domain. This work focuses on estimating fundamental sensing parameters, i.e., the delay and Doppler shifts of individual propagation paths, which serve as critical enablers for downstream positioning techniques, such as time-difference-of-arrival (TDOA)-based localization. Specifically, we propose a parameter-inherited (PI) channel estimation method that integrates sparse Bayesian learning (SBL) with unitary approximate message passing (UAMP), achieving low computational complexity and high estimation robustness. To accelerate the convergence of the UAMP-based iterative estimation, we explore the strategy of initializing parameters by inheriting prior estimates from adjacent OTFS transmission blocks. Furthermore, the overall computational burden is significantly reduced by employing large-scale matrix operations via two-dimensional fast Fourier transform (2D FFT). The proposed algorithms are implemented and evaluated on a software-defined radio (SDR)-based ISAC platform. Experimental results demonstrate that the proposed dual-functional system outperforms existing benchmarks in both communication quality and sensing parameter accuracy.},
  archive      = {J_TMC},
  author       = {Xinyuan Wei and Weijie Yuan and Kecheng Zhang and Fan Liu},
  doi          = {10.1109/TMC.2025.3582421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3583916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi technology has emerged as a promising solution for contact-free sensing owing to the pervasiveness of Wi-Fi signals in indoor environments. However, Wi-Fi sensing faces several fundamental issues, including limited sensing range and unstable orientation-dependent sensing performance, hindering the widespread adoption of Wi-Fi sensing in real-life scenarios. In this paper, we propose RISensing, a novel system that leverages Reconfigurable Intelligent Surfaces (RIS) to address these two fundamental issues of Wi-Fi sensing and bring Wi-Fi sensing one step closer to real-world adoption. Unlike prior Wi-Fi sensing works which typically rely on a single target reflection signal to capture the target movement, RISensing utilizes two target reflection signals, i.e., the direct target reflection signal and RIS-based target reflection signal, to boost the sensing capability. RISensing characterizes the RIS-based target reflection signal, and constructively combines it with the direct target reflection. We evaluate the sensing performance of RISensing in various environments, including corridor, office and lab. Extensive experiments demonstrate RISensing can improve the sensing range of Wi-Fi from 4 m to 23 m, and effectively mitigate the orientation-dependent issue.},
  archive      = {J_TMC},
  author       = {Xiaojing Wang and Binbin Xie and Guanghui Lv and Boyang Liu and Chenhao Ma and Renjie Zhao and Chao Feng and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3583916},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSID: Enhancing wi-fi based gait recognition via adversarial learning. <em>TMC</em>, 1-12. (<a href='https://doi.org/10.1109/TMC.2025.3583946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Wi-Fi sensing, wireless-based gait recognition has become increasingly important as it supports a wide range of applications (person identification, disease diagnosis, etc.). However, two serious challenges limit the universal deployment of such Wi-Fi vision schemes: i) the limited bandwidth of Wi-Fi severely restricts the granularity of gait recognition, and ii) users non-gait behaviors (e.g., stopping and turning) interfere with the extraction of gait-related features. In this paper, we propose CSID, which can achieve robust gait recognition under the limited bandwidth conditions of commercial Wi-Fi devices. Specifically, we use a neural network to generate super-resolution spectrograms of channel state information (CSI), overcoming the limitation of insufficient Wi-Fi bandwidth. To overcome the challenge of non-gait behavior interference, considering the human-incomprehensible nature of Wi-Fi spectrograms, we adopt cross-domain adversarial training and further extract gait features that are independent of the interference behaviors by learning domain-independent representations. We conducted a large number of experiments in different indoor environments, and the average person identification rate of the CSID system reached 91.6%. These results demonstrate that the CSID system is promising and could be used as a complement to visual person identification systems in the future.},
  archive      = {J_TMC},
  author       = {Yu Liu and Jingyang Hu and Hongbo Jiang and Kehua Yang and Wei Zhang and Zheng Qin},
  doi          = {10.1109/TMC.2025.3583946},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSID: Enhancing wi-fi based gait recognition via adversarial learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical and heterogeneous federated learning via a learning-on-model paradigm. <em>TMC</em>, 1-16. (<a href='https://doi.org/10.1109/TMC.2025.3581534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) collaboratively trains a shared global model without exposing clients' private data. In practical FL systems, clients (e.g., smartphones and wearables) typically have disparate system resources. Traditional FL, however, adopts a one-size-fits-all solution, where a homogeneous large model is sent to and trained on each client. This method results in an overwhelming workload for less capable clients and starvation for others. To tackle this, we propose FedConv, a client-friendly FL framework, minimizing the system overhead on resource-constrained clients by providing heterogeneous customized sub-models. FedConv features a novel learning-on-model paradigm that learns the parameters of heterogeneous sub-models via convolutional compression. To aggregate heterogeneous sub-models, we propose transposed convolutional dilation to convert them back to large models with a unified size while retaining personalized information. The compression and dilation processes, transparent to clients, are tuned on the server using a small public dataset. We further propose a hierarchical and clustering-based local training strategy for enhanced performance. Extensive experiments on six datasets show that FedConv outperforms state-of-the-art FL systems in terms of model accuracy (by more than 35% on average), computation and communication overhead (with 33% and 25% reduction, respectively).},
  archive      = {J_TMC},
  author       = {Leming Shen and Qiang Yang and Kaiyan Cui and Yuanqing Zheng and Xiao-Yong Wei and Jianwei Liu and Jinsong Han},
  doi          = {10.1109/TMC.2025.3581534},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical and heterogeneous federated learning via a learning-on-model paradigm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative edge and cloud computing: Optimal configuration and computation management. <em>TMC</em>, 1-15. (<a href='https://doi.org/10.1109/TMC.2025.3584524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) plays an increasingly important role in the rapidly increasing mobile applications by providing high-quality computing services. The majority of current research has focused on designing efficient computing task offloading schemes to ensure the effectiveness of the MEC system. However, the configuration and resource management of the MEC system, which are crucial for its scattered feature, have not received due attention. This paper investigates the configuration and computation resource management problem for the MEC system by formulating a profit maximization problem. To address this problem, we first analyze the relationship among mobile users' offloading decisions, the configuration and computation management of the MEC system, and the service quality. Then, we design an optimal configuration and computation management scheme of the MEC system, which can not only maintain the efficiency of computing processes but also make a good trade-off between the profitability and the service quality. In such a way, the total expected profit of the MEC system can be maximized. Numerical evaluations show that the proposed optimal configuration and computation management scheme can efficiently improve the total profit of the MEC system.},
  archive      = {J_TMC},
  author       = {Yongmin Zhang and Wei Wang and Rui Huang and Junfan Zhou and Yang Xu and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3584524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative edge and cloud computing: Optimal configuration and computation management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). COOPER-MATCH: Job offloading with a cooperative game for guaranteeing strict deadlines in MEC. <em>TMC</em>, 1. (<a href='https://doi.org/10.1109/TMC.2019.2921713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While mobile edge computing (MEC) holds promise to enhance users' mobile experience, building a framework under multiple MECs environment to make appropriate offloading decision is challenging. When involving quality of service (QoS), the problem becomes even harder. Few works can be found for this. In this work, we focus on QoS guaranteed offloading under multiple MECs environment. Specifically, there are multiple mobile devices (MDs) and each one is associated with a job which can be offloaded to an access point (AP) for execution. Each job is associated with a block of input data, an execution workload, and a QoS requirement, i.e., a time deadline that the job is expected to be completed before it if it is offloaded to an AP for execution. Our goal is to find an efficient offloading strategy which guides for offloading MDs' jobs to appropriate MEC servers, such that the number of jobs whose deadlines are satisfied is maximized. The problem is proved to be NP-hard. To solve the problem, we design an offloading framework COOPER-MATCH, which is based on cooperative game. Further, we propose a game based offloading policy (GOP) to offload jobs with QoS guarantees.},
  archive      = {J_TMC},
  author       = {Chubo Liu and Kenli Li and Jie Liang and Keqin Li},
  doi          = {10.1109/TMC.2019.2921713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {COOPER-MATCH: Job offloading with a cooperative game for guaranteeing strict deadlines in MEC},
  year         = {2019},
}
</textarea>
</details></li>
</ul>

</body>
</html>
