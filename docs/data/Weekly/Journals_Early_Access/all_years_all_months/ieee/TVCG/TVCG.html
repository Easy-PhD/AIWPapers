<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg">TVCG - 122</h2>
<ul>
<li><details>
<summary>
(2025). RP-SLAM: Real-time photorealistic SLAM with efficient 3D gaussian splatting. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3616173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting has emerged as a promising technique for high-quality 3D rendering, leading to increasing interest in integrating 3DGS into realism SLAM systems. However, existing methods face challenges such as Gaussian primitives redundancy, forgetting problem during continuous optimization, and difficulty in initializing primitives in monocular case due to lack of depth information. In order to achieve efficient and photorealistic mapping, we propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian primitives optimization and consists of three key components. Firstly, we propose an efficient incremental mapping approach to achieve a compact and accurate representation of the scene through adaptive sampling and Gaussian primitives filtering. Secondly, a dynamic window optimization method is proposed to mitigate the forgetting problem and improve map consistency. Finally, for the monocular case, a monocular keyframe initialization method based on sparse point cloud is proposed to improve the initialization accuracy of Gaussian primitives, which provides a geometric basis for subsequent optimization. The results of numerous experiments demonstrate that RP-SLAM achieves state-of-the-art map rendering accuracy while ensuring real-time performance and model compactness.},
  archive      = {J_TVCG},
  author       = {Lizhi Bai and Chunqi Tian and Jun Yang and Siyu Zhang and Masanori Suganuma and Takayuki Okatani},
  doi          = {10.1109/TVCG.2025.3616173},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RP-SLAM: Real-time photorealistic SLAM with efficient 3D gaussian splatting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LeOp-GS: Learned optimizer with dynamic gradient update for sparse-view 3DGS. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3616156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) achieves remarkable speed and performance in novel view synthesis but suffers from overfitting and degraded reconstruction when handling sparse-view inputs. This paper innovatively addresses this challenge from a learning-to-optimize perspective by leveraging a learned optimizer (i.e., a multi-layer perceptron, MLP) to update the relevant parameters of 3DGS during the optimization process. Evidently, using a single MLP to handle all optimization variables, whose numbers may even vary during the optimization process, is impossible. Therefore, we present a point-wise position-aware optimizer that updates the parameters for each 3DGS point individually. Specifically, it takes the point coordinates and corresponding parameter values as input to predict the updates, thereby allowing efficient and adaptive optimization. In the case of sparse view modeling, the learned optimizer imposes position-aware constraints on the parameter updates during optimization. This effectively encourages the relevant parameters to converge stably to better solutions. To update the optimizer's parameters, we propose a dynamic gradient update strategy based on spatial perturbation and weighted fusion, enabling the optimizer to capture broader contextual information. Experiments demonstrate that our method effectively addresses the problem of modeling 3DGS from sparse training views, achieving state-of-the-art results across multiple datasets.},
  archive      = {J_TVCG},
  author       = {Xinyu Lei and Xuan Wang and Longjun Liu and Haoteng Li and Haonan Zhang},
  doi          = {10.1109/TVCG.2025.3616156},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LeOp-GS: Learned optimizer with dynamic gradient update for sparse-view 3DGS},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of realism on hand redirection in immersive environments. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirection in virtual reality (VR) enhances haptic feedback versatility by relaxing the need for precise alignment between virtual and physical objects. In mixed reality (MR), where users see the real world and their own hands, haptic redirection enables a physical interaction with virtual objects but poses greater challenges due to altering real-world perception. This paper investigates the effect of the realism of the user's surroundings and of the user's hand on haptic redirection. The user's familiarity with their actual physical surroundings and their actual hand could make the redirection manipulations easier–or harder–to detect. In a user study (N = 30) participants saw either a virtual environment or their actual physical surroundings, and saw their hand rendered either with a generic 3D model or with a live 2D video sprite of their actual hand. The study used a two-alternative forced choice (2AFC) design asking participants to detect hand redirections that bridged physical to virtual offsets of varying magnitudes. The results show that participants were not more sensitive to 2D video sprite hand redirection than to VR hand redirection, which supports the use of haptic redirection in MR.},
  archive      = {J_TVCG},
  author       = {Shuqi Liao and Yuqi Zhou and Voicu Popescu},
  doi          = {10.1109/TVCG.2025.3616743},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of realism on hand redirection in immersive environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACT: Modeling coordination dynamics in scale-asymmetric virtual reality collaboration. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality (VR), collaborators often experience the same environment at different visual scales, disrupting shared attention and increasing coordination difficulty. While prior work has focused on preventing misalignment, less is known about how teams recover when alignment fails. We examine collaboration under scale asymmetry, a particularly disruptive form of perceptual divergence. In a study with 36 VR teams, we identify behavioral patterns that distinguish adaptive recovery from persistent breakdown. Successful teams flexibly shifted between user-driven and system-supported cues, while others repeated ineffective strategies. Based on these findings, we introduce the Perceptual Asymmetry Coordination Theory (PACT), a dual-pathway model that describes coordination as an evolving process shaped by cue integration and strategic responsiveness. PACT reframes recovery not as a return to alignment, but as a dynamic adaptation to misalignment. These insights inform the design of VR systems that support recovery through multi-channel, adaptive coordination in scale-divergent environments.},
  archive      = {J_TVCG},
  author       = {Hayeon Kim and In-Kwon Lee},
  doi          = {10.1109/TVCG.2025.3616831},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PACT: Modeling coordination dynamics in scale-asymmetric virtual reality collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of AI-based real-time gesture generation and immersion on the perception of others and interaction quality in social XR. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how people interact in dyadic social eXtended Reality (XR), focusing on two main factors: the animation type of a conversation partner's avatar and how immersed the user feels in the virtual environment. Specifically, we investigate how 1) idle behavior, 2) AI-generated gestures, and 3) motion-captured movements from a confederate (a controlled partner in the study) influence the quality of conversation and how that partner is perceived. We examined these effects in both symmetric interactions (where both participants use VR headsets and controllers) and asymmetric interactions (where one participant uses a desktop setup). We developed a social XR platform that supports asymmetric device configurations to provide varying levels of immersion. The platform also supports a modular avatar animation system providing idle behavior, real-time AI-generated co-speech gestures, and full-body motion capture. Using a 2×3 mixed design with 39 participants, we measured users' sense of spatial presence, their perception of the confederate, and the overall conversation quality. Our results show that users who were more immersed felt a stronger sense of presence and viewed their partner as more human-like and believable. Surprisingly, however, the type of avatar animation did not significantly affect conversation quality or how the partner was perceived. Participants often reported focusing more on what was said rather than how the avatar moved.},
  archive      = {J_TVCG},
  author       = {Christian Merz and Niklas Krome and Carolin Wienrich and Stefan Kopp and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3616864},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of AI-based real-time gesture generation and immersion on the perception of others and interaction quality in social XR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radiance fields in XR: A survey on how radiance fields are envisioned and addressed for XR research. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.},
  archive      = {J_TVCG},
  author       = {Ke Li and Mana Masuda and Susanne Schmidt and Shohei Mori},
  doi          = {10.1109/TVCG.2025.3616794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Radiance fields in XR: A survey on how radiance fields are envisioned and addressed for XR research},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of visual augmentation on embodied skill acquisition across perspectives and body representations. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive embodiment holds great promise for motor skill acquisition, however the design and effect of real-time visual guidance across perspectives and body representations remain underexplored. This study introduces a puppet-inspired visual feedback framework that uses continuous visual linkages — line, color, and thickness cues — to externalize spatial deviation and scaffold embodied learning. To evaluate its effectiveness, we conducted a controlled virtual reality experiment (N = 40) involving gesture imitation tasks with fine (sign language) and gross (aviation marshalling) motor components, under first- and third-person viewpoints. Results showed that color-based guidance significantly improved imitation accuracy, short-term learning, and perceived embodiment, especially in finger-based and first-person settings. Subjective assessments (NASA-TLX, Motivation, IPQ, Embodiment) confirmed improvements in presence, agency, and task engagement.},
  archive      = {J_TVCG},
  author       = {Ruochen Cao and Zequn Liang and Chenkai Zhang and Andrew Cunningham and James A. Walsh and Rui Cao},
  doi          = {10.1109/TVCG.2025.3616832},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of visual augmentation on embodied skill acquisition across perspectives and body representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal calibration and ground truth estimation for high-precision SLAM benchmarking in extended reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.},
  archive      = {J_TVCG},
  author       = {Zichao Shu and Shitao Bei and Lijun Li and Zetao Chen},
  doi          = {10.1109/TVCG.2025.3616838},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatiotemporal calibration and ground truth estimation for high-precision SLAM benchmarking in extended reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free model adaptation for unsupervised 3D object retrieval. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of 3D objects yet expensive annotation costs, unsupervised 3D object retrieval has become a popular but challenging research area. Existing labeled resources have been utilized to aid this task via transfer learning, which aligns the distribution of unlabeled data with the source one. However, the labeled resource are not always accessible due to the privacy disputes, limited computational capacity and other thorny restrictions. Therefore, we propose source-free model adaptation task for unsupervised 3D object management, which utilizes a pre-trained model to boost the performance with no access to source data and labels. Specifically, we compute representative prototypes to assume the source feature distribution, and design a bidirectional cumulative confidence-based adaptation strategy to adaptively align unlabeled samples towards prototypes. Subsequently, a dual-model distillation mechanism is proposed to generate source hypothesis for remedying the absence of ground-truth labels. The experiments on a cross-domain retrieval benchmark NTU-PSB (PSB-NTU) and a cross-modality retrieval benchmark MI3DOR also demonstrate the superiority of the proposed method even without access to raw data. Code is available at: https://github.com/Wyyspace1203/MA},
  archive      = {J_TVCG},
  author       = {Dan Song and Yiyao Wu and Yuting Ling and Diqiong Jiang and Yao Jin and Ruofeng Tong},
  doi          = {10.1109/TVCG.2025.3617082},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Source-free model adaptation for unsupervised 3D object retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bundling-aware graph drawing revisited. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3616583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge bundling algorithms can significantly improve the visualization of dense graphs by identifying and bundling together suitable groups of edges and thus reducing visual clutter. As such, bundling is often viewed as a post-processing step applied to a drawing, and the vast majority of edge bundling algorithms consider a graph and its drawing as input. A different way of thinking about edge bundling is to simultaneously optimize both the drawing and the bundling, which we investigate in this paper. We build on an earlier work where we introduced a novel algorithmic framework for bundling-aware graph drawing consisting of three main steps, namely Filter for a skeleton subgraph, Draw the skeleton, and Bundle the remaining edges against the drawing of the skeleton. We propose several alternative implementations and experimentally compare them against each other and the simple idea of first drawing the full graph and subsequently applying edge bundling to it. The experiments confirm that bundled drawings created by our Filter-Draw-Bundle framework outperform previous approaches according to metrics for edge bundling and graph drawing.},
  archive      = {J_TVCG},
  author       = {Markus Wallinger and Tommaso Piselli and Alessandra Tappini and Daniel Archambault and Giuseppe Liotta and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2025.3616583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bundling-aware graph drawing revisited},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of transitional mixed reality interfaces: A co-design study with flood-prone communities. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flood risk communication in disaster-prone communities often relies on traditional tools (e.g., paper and browser-based hazard/flood maps) that struggle to engage community stakeholders and reflect intuitive flood situations. In this paper, we applied the transitional mixed reality (MR) interface concept from pioneering work and extended it for flood risk communication scenarios through co-design with community stakeholders to help vulnerable residents understand flood risk and facilitate preparedness. Starting with an initial transitional MR prototype, we conducted three iterative workshops - each dedicated to device usability, visualization techniques, and interaction methods. We collaborated with diverse community stakeholders in flood-prone areas, collecting feedback to refine the system according to community needs. Our preliminary evaluation indicates that this co-designed system significantly improves user understanding and engagement compared to traditional tools, though some older residents faced usability challenges. We detailed this iterative co-design process, critical insights and design implications, offering our work as a practical case of mixed reality application in strengthening flood risk communication. We also discuss the system's potential to support community-driven collaboration in flood preparedness.},
  archive      = {J_TVCG},
  author       = {Zhiling Jie and Geert Lugtenberg and Renjie Zhang and Armin Teubert and Makoto Fujisawa and Hideaki Uchiyama and Kiyoshi Kiyokawa and Isidro Butaslac and Taishi Sawabe and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3616755},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Application of transitional mixed reality interfaces: A co-design study with flood-prone communities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving mid-air sketching in room-scale virtual reality with dynamic color-to-depth and opacity cues. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive 3D mid-air sketching systems liberate users from the confines of traditional 2D sketching canvases. However, complications from perceptual challenges in Virtual Reality (VR), combined with the ergonomic and cognitive challenges of sketching in all three dimensions in mid-air lower the accuracy and aesthetic quality of 3D sketches. This paper explores how color-to-depth and opacity cues support users to create and perceive freehand, 3D strokes in room-scale sketching, unlocking a full 360°of freedom for creation. We implemented three graphic depth shader cues modifying the (1) alpha, (2) hue, and (3) value levels of a single color to dynamically adjust the color and transparency of meshes relative to their depth from the user. We investigated how these depth cues influence sketch efficiency, sketch quality, and total sketch experience with 24 participants in a comparative, counterbalanced, 4 × 1 within-subjects user study. First, with our graphic depth shader cues we could successfully transfer results of prior research in seated sketching tasks to room-scale scenarios. Our color-to-depth cues improved the similarity of sketches to target models. This highlights the usefulness of the color-to-depth approach even for the increased range of motion and depth in room-scale sketching. Second, our shaders assisted participants to complete tasks faster, spend a greater percentage of task time sketching, reduced the feeling of mental tiredness and improved the feeling of sketch efficiency in room-scale sketching. We discuss these findings and share our insights and conclusions to advance the research on improving spatial cognition in immersive sketching systems.},
  archive      = {J_TVCG},
  author       = {Samantha Monty and Dennis Mevißen and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3616867},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving mid-air sketching in room-scale virtual reality with dynamic color-to-depth and opacity cues},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards augmented reality support for swarm monitoring: Evaluating visual cues to prevent fragmentation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm fragmentation, the breakdown of communication and coordination among robots, can critically compromise a swarm's mission. Integrating Augmented Reality support into swarm monitoring—especially through co-located visualisations anchored directly on the robots— may enable human operators to detect early signs of fragmentation and intervene effectively. In this work, we propose three localised visual cues—targeting robot connectivity, dominant decision influences, and movement direction—to make explicit the underlying Perception-Decision-Action (PDA) loop of each robot. Through an immersive Virtual Reality user study, 51 participants were tasked with both anticipating potential fragmentation and selecting the appropriate control to prevent it, while observing swarms exhibiting expansion, densification, flocking, and swarming behaviours. Our results reveal that a visualisation emphasising inter-robot connectivity significantly improves anticipation of fragmentation, though none of the cues consistently enhance control selection over a baseline condition. These findings underscore the potential of co-located AR-enhanced visual feedback to support human-swarm interaction and inform the design of future AR-based supervisory systems for robot swarms. A free copy of this paper and all supplemental materials are available at https://osf.io/49gny.},
  archive      = {J_TVCG},
  author       = {Aymeric Hénard and Étienne Peillard and Jérémy Rivière and Sébastien Kubicki and Gilles Coppin},
  doi          = {10.1109/TVCG.2025.3616840},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards augmented reality support for swarm monitoring: Evaluating visual cues to prevent fragmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WarpVision: Using spatial curvature to guide attention in virtual reality. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of consumer-targeted, low-cost virtual reality devices and facile authoring technologies, the development and design of experiences in virtual reality are also becoming more accessible to non-expert authors. However, the inherent freedom of exploration in these virtual spaces presents a significant challenge for designers seeking to guide user attention toward points and objects of interest. This paper proposes the new technique WarpVision, which utilizes spatial curvature to subtly guide the user's attention in virtual reality. WarpVision distorts an area around the point of interest, thus changing the size, form, and location of all objects and the space around them. In this way, the user's attention can be guided even when the point of interest is not in the immediate field of vision. WarpVision is evaluated in a user study based on a within-subjects design, comparing it to the state-of-the-art technique Deadeye. Participants completed visual search tasks across two virtual environments being supported with WarpVision at four different intensities. Results show that WarpVision significantly reduces search times compared to Deadeye. While both techniques introduce comparable levels of immersion disruption, WarpVision has a lower reported impact on the user's well-being.},
  archive      = {J_TVCG},
  author       = {Jérôme Kudnick and Martin Weier and Colin Groth and Biying Fu and Robin Horst},
  doi          = {10.1109/TVCG.2025.3616806},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WarpVision: Using spatial curvature to guide attention in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeing what matters: Attentional (Mis-)Alignment between humans and AI in VR-simulated prediction of driving accidents. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how human and AI visual attention differ in a short-term prediction task, particularly in the moments before an accident is about to happen. Since real-world studies of this kind would pose ethical and safety risks, we employed virtual reality (VR) to simulate an accident scenario. In the scenario, the driver approaches a fork in the road, knowing that one path would lead off a cliff crashing the car fatally—as the fork comes closer, the other, safe, path is suddenly blocked by trees, forcing the driver to make a split-second decision where to go. A total of $N = 71$ drivers completed the task, and we asked another $N = 30$ observers to watch short video clips leading up to the final event and to predict which way the driver would take. We then compared both prediction accuracy as well as attention patterns—how focus is distributed across objects—with AI systems, including vision language models (VLMs) and vision-only models. We found that overall, prediction performance increased as the accident time point approached; interestingly, humans fared better than AI systems overall except for the final time period just before the event. We also found that humans adapted their attention dynamically, shifting focus to important scene elements before an event, whereas AI attention remained static, overlooking key details of the scene. Importantly, as the accident time point approached, human-AI attentional alignment decreased, even though both types of models improved in prediction accuracy. Despite distinct temporal trajectories—vision-only models declining from an early advantage and VLMs peaking in the middle—both models achieved low to zero alignment with human attention. These findings highlight a critical dissociation: AI models make accurate predictions, but rely on visual strategies diverging from human processing, underscoring a gap between explainability and task performance.},
  archive      = {J_TVCG},
  author       = {Hoe Sung Ryu and Uijong Ju and Christian Wallraven},
  doi          = {10.1109/TVCG.2025.3616811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seeing what matters: Attentional (Mis-)Alignment between humans and AI in VR-simulated prediction of driving accidents},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical evaluation of how virtual hand visibility affects near-field size perception and reporting of tangible objects in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In immersive virtual environments (IVEs), accurate size perception is critical, especially in training simulations designed to mimic real-world tasks, such as, nuclear power plant control room or medical procedures. These simulations have dials or instruments of varying sizes. Visual information of the objects alone, often fails to capture subtle size differences in virtual reality (VR). However, integrating haptic and hand-avatars may potentially improve accuracy and performance. This improvement could be especially beneficial for real-world scenarios where hand(s) are intermittently visible or obscured. To investigate how this intermittent presence or absence of body-scaled hand-avatars affects size perception when integrated with haptic information, we conducted 2×2 mixed-factorial experiment design using a near-field, size-estimation task in VR. The experiment conditions compared size estimations with or without virtual hand visibility in the perception and reporting phases. The task involved 16 graspable objects of varying sizes and randomly repeated 3 times across 48 trials per participant (total 80 participants). We employed Linear Mixed Models (LMMs) analysis to objective measures: perceived size, residual error and proportional errors. Results revealed that as the tangible-graspable size increases, overestimation reduces if the hand-avatars are visible in the reporting phase. Also, overestimation reduces as the number of trials increases, if the hand-avatars are visible in the reporting phase. Thus, the presence of hand-avatars facilitated perceptual calibration. This novel study, with different combinations of hand-avatar visibility, taking perception and reporting of size as two separate phases, could open future research directions in more complex scenarios for refined integration of sensory modalities and consequently enhance real-world application performance.},
  archive      = {J_TVCG},
  author       = {Chandni Murmu and Rohith Venkatakrishnan and Roshan Venkatakrishnan and Wen-Chieh Lin and Andrew C. Robb and Christopher Pagano and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2025.3616829},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An empirical evaluation of how virtual hand visibility affects near-field size perception and reporting of tangible objects in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A handheld stiffness display with a programmable spring and electrostatic clutches for haptic interaction in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handheld haptic devices often face challenges in delivering stiffness feedback with both high force output and good backdrivability, especially under practical constraints on power consumption, size, and weight. These difficulties stem from the inherent performance limitations of conventional actuation mechanisms. To address this issue, we propose a lightweight, low-power handheld device that provides wide-range stiffness feedback through a novel dual actuation design composed of two key components. A programmable spring (PS), implemented via an adjustable lever arm, enables tunable physical stiffness. Two electrostatic clutches (ECs) are integrated to compensate for the inherent limitations of PS-based interactions in stiffness display range, rendered object size, and free motion capability. The feedback force arises passively from the reaction of the PS and ECs to user input, effectively lowering both power consumption and actuator torque demands. A fully integrated prototype was developed, incorporating wireless communication, control, and power modules. The results of the evaluation experiments and user studies demonstrate that the device effectively renders stiffness across the full range, from free motion to full rigidity, and delivers more realistic elastic feedback compared to conventional electric motor-based systems.},
  archive      = {J_TVCG},
  author       = {Ke Shi and Quan Xiong and Maozeng Zhang and Aiguo Song and Lifeng Zhu},
  doi          = {10.1109/TVCG.2025.3616795},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A handheld stiffness display with a programmable spring and electrostatic clutches for haptic interaction in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Influence of object height, shadow and adapting luminance on outdoor depth perception in augmented reality. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) technology has great potential in the applications of training, exhibition, and visual guidance, all of which demand precise virtual-real registration in perceived depth. Many AR applications such as navigation and tourism guidance are usually implemented in outdoor environments. However, prior research on depth perception in AR predominantly focused on the indoor environment, characterized by a lower illumination level and more confined space compared to outdoor settings. To address this gap, this paper presented a systematic investigation into the depth perception in outdoor environments. Two experiments were conducted in this study: the first one aimed to explore how to eliminate the bias induced by the floating object and how the knowledge of object height influences the perceived depth. The second experiment examined how ambient luminance affects depth estimation in AR. Our findings revealed an overestimation of perceived depth when participants were unaware of the actual height of the floating object, but an underestimation when they were informed of this information prior to the experiment. Additionally, shadows effectively reduced depth errors regardless of whether participants were informed of the object's height. The second experiment further indicated that, in outdoor environments, reducing ambient luminance significantly improves the accuracy of depth perception in AR.},
  archive      = {J_TVCG},
  author       = {Shining Ma and Chaochao Liu and Jingyuan Wang and Yue Liu and Yongtian Wang and Weitao Song},
  doi          = {10.1109/TVCG.2025.3616839},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence of object height, shadow and adapting luminance on outdoor depth perception in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZonAware: Identifying zoning out and increasing engagement in upper limb virtual reality rehabilitation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zoning out, a form of cognitive disengagement, seriously challenges the effectiveness of virtual reality (VR) based upper limb rehabilitation. As therapy often involves repetitive tasks requiring sustained attention, undetected lapses in focus can reduce motor learning, engagement, and overall recovery outcomes. This research addresses this gap by proposing ZonAware, a novel strategy integrating real-time zoning out detection with adaptive intervention to enhance user engagement during VR rehabilitation. ZonAware identifies zoning out using five eye-tracking metrics: blink frequency, blink duration, pupil size, eye openness, and gaze duration. These signals are analysed through lightweight statistical models (Z-Score, Boxplot, and Modified Z-Score), with a hard voting mechanism producing binary classifications in real-time. Upon detection, a pattern changing intervention subtly modulates task difficulty by temporarily increasing, then decreasing it, to regain user focus without breaking immersion. Three user studies involving 70 healthy participants and 22 patients demonstrated the strategy's effectiveness. ZonAware achieved 98.24% detection accuracy with low latency (82–150 ms), reducing zoning out frequency by 53.57% and shortening disengagement duration from 18.1 to 4.8 seconds. The approach also improved user engagement, performance, and emotional motivation. ZonAware delivers one of the first real-time zoning out solutions for VR rehabilitation, offering an interpretable, theory-driven approach that enhances attention, engagement, and adaptability in human-computer interaction.},
  archive      = {J_TVCG},
  author       = {Kai-Lun Liao and Mengjie Huang and Jiajia Shi and Min Chen and Rui Yang},
  doi          = {10.1109/TVCG.2025.3616818},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ZonAware: Identifying zoning out and increasing engagement in upper limb virtual reality rehabilitation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the effects of haptic illusions in collaborative virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our sense of touch plays a crucial role in physical collaboration, yet rendering realistic haptic feedback in collaborative extended reality (XR) remains a challenge. Co-located XR systems predominantly rely on prefabricated passive props that provide high-fidelity interaction but offer limited adaptability. Haptic Illusions (HIs), which leverage multisensory integration, have proven effective in expanding haptic experiences in single-user contexts. However, their role in XR collaboration has not been explored. To examine the applicability of HIs in multi-user scenarios, we conducted an experimental user study (N=30) investigating their effect on a collaborative object handover task in virtual reality. We manipulated visual shape and size individually and analyzed their impact on users' performance, experience, and behavior. Results show that while participants adapted to the illusions by shifting sensory reliance and employing specific sensorimotor strategies, visuo-haptic mismatches reduced both performance and experience. Moreover, mismatched visualizations in asymmetric user roles negatively impacted performance. Drawing from these findings, we provide practical guidelines for incorporating HIs into collaborative XR, marking a first step toward richer haptic interactions in shared virtual spaces.},
  archive      = {J_TVCG},
  author       = {Yannick Weiss and Julian Rasch and Jonas Fischer and Florian Müller},
  doi          = {10.1109/TVCG.2025.3616760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the effects of haptic illusions in collaborative virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFM-GS: Half-face mapping 3DGS avatar based real-time HMD removal. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In extended reality (XR) applications, enhancing user perception often necessitates head-mounted display (HMD) removal. However, existing methods suffer from low time performance and suboptimal reconstruction quality. In this paper, we propose a half face mapping 3D Gaussian splatting avatar based HMD removal method (HFM-GS), which can perform real-time and high-fidelity online restoration of the complete face in HMD-occluded videos for XR applications after a short un-occluded face registration. We establish a mapping field between the upper and lower face Gaussians to enhance the adaptability to deformation. Then, we introduce correlation weight-based sampling to improve time performance and handle variations in the number of Gaussians. At last, we ensure model robustness through Gaussian Segregation Strategy. Compared to two state-of-the-art methods, our method achieves better quality and time performance. The results of the user study show that fidelity is significantly improved with our method.},
  archive      = {J_TVCG},
  author       = {Kangyu Wang and Jian Wu and Runze Fan and Hongwen Zhang and Sio Kei Im and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HFM-GS: Half-face mapping 3DGS avatar based real-time HMD removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection at a distance through a large transparent touch screen. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large transparent touch screens (LTTS) have recently become commercially available. These displays have the potential for engaging Augmented Reality (AR) applications, especially in public and shared spaces. However, the interaction with objects in the real environment behind the display remains challenging: Users must combine pointing and touch input if they want to select objects at varying distances. There is a lot of work on wearable or mobile AR displays, but little on how users interact with LTTS. Our goal is to contribute to a better understanding of natural user interaction for these AR displays. To this end, we developed a prototype and evaluated different pointing techniques for selecting 12 physical targets behind an LTTS, with distances ranging from 6 to 401 cm. We conducted a user study with 16 participants and measured user preferences, performance, and behavior. We analyzed the change in accuracy depending on the target position and the selection technique used. Our fndings include: (a) Users naturally align the touch point with their line of sight for targets farther than 36 cm behind the LTTS. (b) This technique provides the lowest angular deviation compared to other techniques. (c) Some user close one eye to improve their performance. Our results help to improve future AR scenarios using LTTS systems.},
  archive      = {J_TVCG},
  author       = {Sebastian Rigling and Steffen Koch and Dieter Schmalstieg and Bruce H. Thomas and Michael Sedlmair},
  doi          = {10.1109/TVCG.2025.3616756},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selection at a distance through a large transparent touch screen},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGSG: Stroke-guided scene graph generation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene graph generation is essential for spatial computing in Extended Reality (XR), providing structured semantics for task planning and intelligent perception. However, unlike instance-segmentation-driven setups, generating semantic scene graphs still suffer from limited accuracy due to coarse and noisy point cloud data typically acquired in practice, and from the lack of interactive strategies to incorporate users, spatialized and intuitive guidance. We identify three key challenges: designing controllable interaction forms, involving guidance in inference, and generalizing from local corrections. To address these, we propose SGSG, a Stroke-Guided Scene Graph generation method that enables users to interactively refine 3D semantic relationships and improve predictions in real time. We propose three types of strokes and a lightweight SGstrokes dataset tailored for this modality. Our model integrates stroke guidance representation and injection for spatio-temporal feature learning and reasoning correction, along with intervention losses that combine consistency-repulsive and geometry-sensitive constraints to enhance accuracy and generalization. Experiments and the user study show that SGSG outperforms state-of-the-art methods 3DSSG and SGFN in overall accuracy and precision, surpasses JointSSG in predicate-level metrics, and reduces task load across all control conditions, establishing SGSG as a new benchmark for interactive 3D scene graph generation and semantic understanding in XR. Implementation resources are available at: https://github.com/Sycamore-Ma/SGSG-runtime.},
  archive      = {J_TVCG},
  author       = {Qixiang Ma and Runze Fan and Lizhi Zhao and Jian Wu and Sio-Kei Im and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616751},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SGSG: Stroke-guided scene graph generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viewpoint-tolerant depth perception for shared extended space experience on wall-sized display. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We proposed viewpoint-tolerant shared depth perception without individual tracking by leveraging human cognitive compensation in universally 3D rendered images on a wall-sized display. While traditional 3D-perception-enabled display systems have primarily focused on single-user scenarios—adapting rendering based on head and eye tracking—the use of wall-sized displays to extend spatial experiences and support perceptually coherent multi-user interactions remains underexplored. We investigated the effects of virtual depths (dv) and absolute viewing distance (da) on human cognitive compensation factors (perceived distance difference, viewing angle threshold, and perceived presence) to construct the wall display-based eXtended Reality (XR) space. Results show that participants experienced a compelling depth perception even from off-center angles of 23°–37°, and largely increasing virtual depth worsens depth perception and presence factors, highlighting the importance of balancing extended depth of virtual space and viewing distance from the wall-sized display. Drawing on these findings, wall-sized displays in venues such as museums, galleries, and classrooms can evolve beyond 2D information sharing to offer immersive, spatially extended group experiences without individualized tracking or wearables.},
  archive      = {J_TVCG},
  author       = {Dooyoung Kim and Jinseok Hong and Heejeong Ko and Woontack Woo},
  doi          = {10.1109/TVCG.2025.3616758},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Viewpoint-tolerant depth perception for shared extended space experience on wall-sized display},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the effects of augmented reality guidance position within a body-fixed coordinate system on pedestrian navigation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AR head-mounted displays (HMDs) facilitate pedestrian navigation by integrating AR guidance into users' field of view (FOV). Displaying AR guidance using a body-fixed coordinate system has the potential to further leverage this integration by enabling users to control when the guidance appears in their FOV. However, it remains unclear how to effectively position AR guidance within this coordinate system during pedestrian navigation. Therefore, we explored the effects of three AR guidance positions (top, middle, and bottom) within a body-fixed coordinate system on pedestrian navigation in a virtual environment. Our results showed that AR guidance position significantly influenced eye movements, walking behaviors, and subjective evaluations. The top position resulted in the shortest duration of fixations on the guidance compared to the middle and bottom positions, and lower mental demand than the bottom position. The middle position had the smallest rate of vertical eye movement during gaze shifts between the guidance and the environment, and the smallest relative difference in walking speed between fixations on the guidance and the environment compared to the top and bottom positions. The bottom position led to the shortest duration and smallest amplitude of gaze shifts between the guidance and the environment compared to the top and middle positions, and lower frustration than the top position. Based on these findings, we offer design implications for AR guidance positioning within a body-fixed coordinate system during pedestrian navigation.},
  archive      = {J_TVCG},
  author       = {Shunbo Wang and Qing Xu and Klaus Schoeffmann},
  doi          = {10.1109/TVCG.2025.3616773},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the effects of augmented reality guidance position within a body-fixed coordinate system on pedestrian navigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of avatar-locomotion congruence on user experience and identification in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual reality (VR) continues to expand, particularly in social VR platforms and immersive gaming environments, understanding the factors that shape user experience is becoming increasingly important. Avatars and locomotion methods play central roles in influencing how users identify with their virtual representations and navigate virtual spaces. Despite extensive research on these elements individually, their relationship remains underexplored. In particular, little is known about how congruence between avatar appearance and locomotion method affects user perceptions. This study investigates the impact of avatar-locomotion congruence on user experience and avatar identification in VR. We conducted a within-subjects experiment with 30 participants, employing two visually distinct avatar types (human and gorilla) and two locomotion methods (human-like arm-swinging and gorilla-like arm-rolling), to assess their individual and combined effects. Our results indicate that congruence between avatar appearance and locomotion method enhances both avatar identification and user experience. These findings contribute to the understanding of the relationship between avatars and locomotion in VR, with potential applications in enhancing user experience in immersive gaming, social VR, and gamified remote physical therapy},
  archive      = {J_TVCG},
  author       = {Omar Khan and Hyeongil Nam and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616836},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of avatar-locomotion congruence on user experience and identification in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersive intergroup contact: Using virtual reality to enhance empathy and reduce stigma towards schizophrenia. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stigma towards individuals with schizophrenia reduces quality of life, creating a barrier to accessing education and employment opportunities. Schizophrenia is one of the most stigmatized mental health conditions, and stigma is prevalent particularly among healthcare professionals. In this study, we investigated whether Virtual Reality (VR) can be incorporated into interventions to reduce stigma. In particular, we compared the effectiveness of three VR conditions based on intergroup contact theory in reducing stigma in form of implicit and explicit attitudes, and behavioral intentions. Through an immersive virtual consultation in a clinical setting, participants (N = 60) experienced one of three different conditions: the Doctor's perspective (embodiment in a majority group member during contact), the Patient's perspective (embodiment in a minority group member) and a Third-person perspective (vicarious contact). Results demonstrated an increase of stigma on certain explicit measures (perceived recovery and social restriction) but also an increase of empathy (perspective-taking, empathic concern) across all conditions regardless of perspective. More importantly, participants' viewpoint influenced the desire for social distance differently depending on the perspective: the Third-person observation significantly increased the desire for social distance, Doctor embodiment marginally decreased it, while Patient embodiment showed no significant change. No change was found in the Implicit Association Test. These findings suggest that VR intergroup contact can effectively reduce certain dimensions of stigma toward schizophrenia, but the type of perspective experienced significantly impacts outcomes.},
  archive      = {J_TVCG},
  author       = {Jiaqi Yin and Shihan Liu and Shao-Wen Lee and Andreas Kitsios and Marco Gillies and Michèle Denise Birtel and Harry Farmer and Xueni Pan},
  doi          = {10.1109/TVCG.2025.3616759},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive intergroup contact: Using virtual reality to enhance empathy and reduce stigma towards schizophrenia},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended reality check: Evaluating XR prototyping for human-robot interaction in contact-intensive tasks. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended Reality (XR) has the potential to improve efficiency and safety in the user-centered development of human-robot interaction. However, the validity of using XR prototyping for user studies for contact-intensive robotic tasks remains underexplored. These in-contact tasks are particularly relevant due to challenges arising from indirect force perception in robot control. Therefore, in this work, we investigate a representative example of such a task: robotic ultrasound. A user study was conducted to assess the transferability of results from a simulated user study to real-world conditions, comparing two force-assistance approaches. The XR simulation replicates the physical study set-up employing a virtual robotic arm, its control interface, ultrasound imaging, and two force-assistance methods: automation and force visualization. Our results indicate that while differences in force deviation, perceived workload, and trust emerge between real and simulated setups, the overall findings remain consistent. Specifically, partial automation of robot control improves performance and trust while reducing workload, and visual feedback decreases force deviation in both real and simulated conditions. These findings highlight the potential of XR for comparative studies, even in complex robotic tasks.},
  archive      = {J_TVCG},
  author       = {Tonia Mielke and Mareen Allgaier and Christian Hansen and Florian Heinrich},
  doi          = {10.1109/TVCG.2025.3616753},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extended reality check: Evaluating XR prototyping for human-robot interaction in contact-intensive tasks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MR-CoCo: An open mixed reality testbed for co-located couple product configuration and decision-making – A sailboat case study. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature has demonstrated the advantages of Mixed Reality (MR) for product configuration by providing a more engaging and effective end-user experience. While collaborative and remote design tools in MR have been widely explored in previous studies, a noticeable gap remains in the exploration of co-located product configuration for couples. This gap is noteworthy since in many industries, couples (e.g., friends, partners) often make purchasing decisions together in physical retail environments. In this paper, we introduce MR-CoCo, an open MR testbed designed to explore collaborative configurations by co-located couples, both in the role of customers. The testbed is developed in Unity and features: (i) a shared MR space with virtual product 3D model anchoring, (ii) shared visualization of the current configuration, (iii) a versatile UI for selecting configuration areas, (iv) hand gestures for 3D drag and drop of colors and materials from 3D catalog to the product. A case study of the personalization of a sailboat is provided as proof of concept. The user study involved 24 couples (48 participants in total), simulating a purchasing experience and the related configuration using MR-CoCo. We assessed usability through post-experience evaluations, with the System Usability Scale (SUS) and the Co-Presence Configuration Questionnaire (CCQ) to measure collaboration and decision-making. The results demonstrated a high level of usability and perceived quality of collaboration. We also explore guidelines that can be used for remote collaboration applications, enabling configuration across a wide range of industries (e.g., automotive and clothing).},
  archive      = {J_TVCG},
  author       = {Fabio Vangi and Daniel Medeiros and Mine Dastan and Michele Fiorentino},
  doi          = {10.1109/TVCG.2025.3616734},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MR-CoCo: An open mixed reality testbed for co-located couple product configuration and decision-making – A sailboat case study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experiencing immersive virtual nature for well-being, restoration, performance, and nature connectedness: A scoping review. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a scoping review of immersive virtual nature experiences delivered via head-mounted displays (HMDs) and their role in promoting well-being, psychological restoration, cognitive performance, and nature connectedness. As access to natural environments becomes increasingly constrained by urbanization, technological lifestyles, and environmental change, immersive technologies offer a scalable and accessible alternative for engaging with nature. Guided by three core research questions, this review explores how HMD-mediated immersive technologies have been used to promote nature connectedness and well-being, what trends and outcomes have been observed across applications, and what methodological gaps or limitations exist in this growing body of work. Fifty-five peer-reviewed studies were analyzed and categorized into six key implication areas: emotional well-being, stress reduction, cognitive performance, attention recovery, restorative benefits, and nature connectedness. The review identifies immersive virtual nature as a promising application of extended reality (XR) technologies, with potential across healthcare, education, and daily life, while also emphasizing the need for more consistent methodologies and long-term research.},
  archive      = {J_TVCG},
  author       = {Jeewoo Kim and Svara Patel and Hyeongil Nam and Janghee Cho and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616762},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Experiencing immersive virtual nature for well-being, restoration, performance, and nature connectedness: A scoping review},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of AI-powered embodied avatars on communication quality and social connection in asynchronous virtual meetings. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive technologies such as virtual and augmented reality (VR/AR) allow remote users to meet and interact in a shared virtual space using embodied virtual avatars, creating a sense of co-presence. However, asynchronous communication—essential in many real-world contexts—remains underexplored in these environments. Traditional playback-based systems lack interactivity and often fail to preserve critical contextual cues necessary for effective asynchronous communication. In this paper, we introduce AVAGENTs, AI-powered virtual avatars that replicate users' verbal and nonverbal cues from recordings of past meetings. Avagents can interpret meeting context and generate appropriate responses to questions posed by asynchronous viewers. Through a user study (N =30), we evaluated Avagents against a traditional playback method and a voice-based AI assistant across two asynchronous meeting scenarios: analytic reasoning and affective resonance. Results showed that Avagents enhance the asynchronous communication experience by increasing social presence, sense of belonging, emotional intimacy, and other user perceptions. We discuss the findings and their implications for designing effective AI-driven asynchronous communication tools in VR/AR environments.},
  archive      = {J_TVCG},
  author       = {Hyeongil Nam and Muskan Sarvesh and Seoyoung Kang and Woontack Woo and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of AI-powered embodied avatars on communication quality and social connection in asynchronous virtual meetings},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing learning and knowledge retention of abstract physics concepts with virtual reality. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is increasingly recognized as a powerful tool for science education, offering interactive environments to explore intangible concepts. Traditional teaching methods often struggle to convey abstract concepts in science, where many phenomena are not directly observable. VR can address this issue by modeling and visualizing complex and unobservable entities and processes, allowing learners to dynamically interact with what would otherwise not be directly perceptible. However, relatively few controlled studies have compared immersive VR learning with equivalent hands-on laboratory learning in physics education, particularly for more abstract topics. In this work, we designed a VR-based physics lab that is capable of visualizing electrons and electromagnetic fields to teach fundamental concepts of electronics and magnetism, closely replicating a traditional electronics learning kit used as a baseline for comparison. We evaluated the impact of the two conditions (VR versus traditional) on students' learning outcomes, motivation, engagement, and cognitive load. Our results show significantly higher knowledge retention in the VR group compared to the traditional group. Also, while there were no significant differences in immediate comprehension between the two groups, participants in the VR group spent substantially more time engaged with the learning content. These findings highlight the potential of visually enriched virtual environments to enhance the learning experience and improve knowledge retention of intangible scientific concepts.},
  archive      = {J_TVCG},
  author       = {M. Akif Akdag and Jean Botev and Steffen Rothkugel},
  doi          = {10.1109/TVCG.2025.3616826},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing learning and knowledge retention of abstract physics concepts with virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-illumination-interfered neural holography with expanded eyebox. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography has immense potential for near-eye displays in virtual and augmented reality (VR/AR), providing natural 3D depth cues through wavefront reconstruction. However, balancing the field of view (FOV) with the eyebox remains challenging, constrained by the étendue limitation. Additionally, holographic image quality is often compromised due to differences between actual wave propagation and simulation models. This study addresses these by expanding the eyebox via multi-angle illumination, and enhancing image quality with end-to-end pupil-aware hologram optimization. Further, energy efficiency is improved by incorporating higher-order diffractions and pupil constraints. We explore a Pupil-HOGD algorithm for multi-angle illumination and validate it with a dual-angle holographic display prototype. Integrated with camera calibration and tracked eye position, the developed Pupil-HOGD algorithm improves image quality and expands the eyebox by 50% horizontally. We envision this approach extends the space-bandwidth product (SBP) of holographic displays, enabling broader applications in immersive, high-quality visual computing.},
  archive      = {J_TVCG},
  author       = {Xinxing Xia and Pengfei Mi and Yiqing Tao and Xiangyu Meng and Wenbin Zhou and Yingjie Yu and Yifan Peng},
  doi          = {10.1109/TVCG.2025.3616793},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-illumination-interfered neural holography with expanded eyebox},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When LLMs recognize your space: Research on experiences with spatially aware LLM agents. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have evolved into LLM agents that can use the user conversation context and respond according to the roles of the LLM agents. Recent studies have suggested that LLM-based agents can be used as human-like partners in social interactions. However, the role of the environmental context, particularly spatial information of user space, in the interaction between humans and LLM agents has not been explored. In this study, participants engaged in counselling conversations under three different conditions based on their spatial awareness levels. The dependent measures included copresence, trust, therapist alliances, and self-disclosure. The results suggested that participants in the condition where the LLM actively reflected spatial information generally reported higher levels of user experience. Interestingly, when the LLM actively reflected the spatial context of the user, the participants tended to describe themselves and express their emotions more. These findings suggest that spatially aware LLM agents can contribute to better social interactions between humans and LLM agents. Our findings can be used to design future augmented reality applications in the counselling, education, and healthcare industries.},
  archive      = {J_TVCG},
  author       = {Seungwoo Oh and Nakyoung An and Youngwug Cho and Myeongul Jung and Kwanguk Kenny Kim},
  doi          = {10.1109/TVCG.2025.3616809},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {When LLMs recognize your space: Research on experiences with spatially aware LLM agents},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAT: Neural acoustic transfer for interactive scenes in real time. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3617802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous acoustic transfer methods rely on extensive precomputation and storage of data to enable real-time interaction and auditory feedback. However, these methods struggle with complex scenes, especially when dynamic changes in object position, material, and size significantly alter sound effects. These continuous variations lead to fluctuating acoustic transfer distributions, making it challenging to represent with basic data structures and render efficiently in real time. To address this challenge, we present Neural Acoustic Transfer, a novel approach that leverages implicit neural representations to encode acoustic transfer functions and their variations. This enables real-time prediction of dynamically evolving sound fields and their interactions with the environment under varying conditions. To efficiently generate high-quality training data for the neural acoustic field while avoiding reliance on mesh quality of a model, we develop a fast and efficient Monte-Carlo-based boundary element method (BEM) approximation, suitable for general scenarios with smooth Neumann boundary conditions. In addition, we devise strategies to mitigate potential singularities during the synthesis of training data, thereby enhancing its reliability. Together, these methods provide robust and accurate data that empower the neural network to effectively model complex sound radiation space. We demonstrate our method's numerical accuracy and runtime efficiency (within several milliseconds for 30s audio) through comprehensive validation and comparisons in diverse acoustic transfer scenarios. Our approach allows for efficient and accurate modeling of sound behavior in dynamically changing environments, which can benefit a wide range of interactive applications such as virtual reality, augmented reality, and advanced audio production.},
  archive      = {J_TVCG},
  author       = {Xutong Jin and Bo Pang and Chenxi Xu and Xinyun Hou and Guoping Wang and Sheng Li},
  doi          = {10.1109/TVCG.2025.3617802},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NAT: Neural acoustic transfer for interactive scenes in real time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entering your space: How agent entrance styles shape social presence in AR. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied conversational agents (ECAs) capable of non-verbal behaviors have been developed to address the limitations of voice-only assistants, with research exploring their use in mixed and augmented reality (AR), suggesting they may soon interact with us more naturally in physical spaces. Traditionally, AI voice assistants are activated through wake-up keywords, and since they are invisible, their method of appearance has not been a concern. However, for ECAs in AR, the question of how they should enter the user's space when summoned remains underexplored. In this paper, we focused on the plausibility of ECAs' entering action into the user's field of view in AR. We analyzed its impact on user experience, concentrating on perceived social presence and co-presence of the agent. Three entrance styles were chosen for comparison: an obviously impossible one, a possible one, and an intermediate one, alongside a voice-only condition. We designed and conducted a within-subjects study with 38 participants. Our results indicated that while the plausibility of the action had less impact on functionality compared to the embodiment itself, it significantly affected social/co-presence. These findings highlight the importance of entrance design for future AR agent experiences.},
  archive      = {J_TVCG},
  author       = {Junyeong Kum and Seungwon Kim and Myungho Lee},
  doi          = {10.1109/TVCG.2025.3616757},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Entering your space: How agent entrance styles shape social presence in AR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visuo-tactile feedback with hand outline styles for modulating affective roughness perception. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a visuo-tactile feedback method that combines virtual hand visualization and fingertip vibrations to modulate affective roughness perception in VR. While prior work has focused on object-based textures and vibrotactile feedback, the role of visual feedback on virtual hands remains underexplored. Our approach introduces affective visual cues including line shape, motion, and color applied to hand outlines, and examines their influence on both affective responses (arousal, valence) and perceived roughness. Results show that sharp contours enhanced perceived roughness, increased arousal, and reduced valence, intensifying the emotional impact of haptic feedback. In contrast, color affected valence only, with red consistently lowering emotional positivity. These effects were especially noticeable at lower haptic intensities, where visual cues extended affective modulation into mid-level perceptual ranges. Overall, the findings highlight how integrating expressive visual cues with tactile feedback can enrich affective rendering and offer flexible emotional tuning in immersive VR interactions.},
  archive      = {J_TVCG},
  author       = {Minju Baeck and Yoonseok Shin and Dooyoung Kim and Hyunjin Lee and Sang Ho Yoon and Woontack Woo},
  doi          = {10.1109/TVCG.2025.3616805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visuo-tactile feedback with hand outline styles for modulating affective roughness perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visceral notices and privacy mechanisms for eye tracking in augmented reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-worn augmented reality (AR) continues to evolve through critical advancements in power optimizations, AI capabilities, and naturalistic user interactions. Eye-tracking sensors play a key role in these advancements. At the same time, eye-tracking data is not well understood by users and can reveal sensitive information. Our work contributes visualizations based on visceral notice to increase privacy awareness of eye-tracking data in AR. We also evaluated user perceptions towards privacy noise mechanisms applied to gaze data visualized through these visceral interfaces. While privacy mechanisms have been evaluated against privacy attacks, we are the first to evaluate them subjectively and understand their influence on data-sharing attitudes. Despite our participants being highly concerned with eye-tracking privacy risks, we found 47% of our participants still felt comfortable sharing raw data. When applying privacy noise, 70% to 76% felt comfortable sharing their gaze data for the Weighted Smoothing and Gaussian Noise privacy mechanisms, respectively. This implies that participants are still willing to share raw gaze data even though overall data-sharing sentiments decreased after experiencing the visceral interfaces and privacy mechanisms. Our work implies that increased access and understanding of privacy mechanisms are critical for gaze-based AR applications; further research is needed to develop visualizations and experiences that relay additional information about how raw gaze data can be used for sensitive inferences, such as age, gender, and ethnicity. We intend to open-source our codebase to provide AR developers and platforms with the ability to better inform users about privacy concerns and provide access to privacy mechanisms. A pre-print of this paper and all supplemental materials are available at https://bmdj-vt.github.io/project_pages/privacy_notice.},
  archive      = {J_TVCG},
  author       = {Nissi Otoo and Kailon Blue and G. Nikki Ramirez and Evan Selinger and Shaun Foster and Brendan David-John},
  doi          = {10.1109/TVCG.2025.3616837},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visceral notices and privacy mechanisms for eye tracking in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ada: A distributed, power-aware, real-time scene provider for XR. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time scene provisioning—reconstructing and delivering scene data to requesting XR applications during runtime—is central to enabling spatial computing in modern XR systems. However, existing solutions struggle to balance latency, power and scene fidelity under XR device constraints, and often rely on designs that are either closed, application-specific designs, or both. We present Ada, the first open distributed, power-aware, application-agnostic real-time scene provisioning system. Through computation offloading along with algorithmic and system innovations, Ada provides high-fidelity scenes with stable performance across all evaluated scene sizes and with low power consumption. To isolate the benefits of Ada's algorithmic and design innovations over the closest prior work [82], which is on-device and CPU-based, we configure a comparable on-device, CPU-based variant of Ada (AdaLocal- CPU). We show this variant achieves up to 6.8× lower scene request latency and higher scene fidelity compared to the prior work. Furthermore, Ada's final distributed GPU-accelerated implementation reduces latency by an additional 2×, highlighting the benefits of GPU acceleration and distributed computing. Additionally, Ada also lowers the incremental power cost of scene provisioning by 24% compared to the best on-device variant (AdaLocal-GPU). Finally, Ada flexibly adapts to diverse latency, power, scene fidelity, and network bandwidth requirements.},
  archive      = {J_TVCG},
  author       = {Yihan Pang and Sushant Kondguli and Shenlong Wang and Sarita Adve},
  doi          = {10.1109/TVCG.2025.3616835},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ada: A distributed, power-aware, real-time scene provider for XR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handows: A palm-based interactive multi-window management system in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Window management in virtual reality (VR) remains a challenging task due to the spatial complexity and physical demands of current interaction methods. We introduce Handows, a palm-based interface that enables direct manipulation of spatial windows through familiar smartphone-inspired gestures on the user's non-dominant hand. Combining ergonomic layout design with body-centric input and passive haptics, Handows supports four core operations: window selection, closure, positioning, and scaling. We evaluate Handows in a user study (N = 15) against two common VR techniques (virtual hand and controller) across four core window operations. Results show that Handows significantly reduces physical effort and head movement while improving task efficiency and interaction precision. A follow-up case study (N = 8) demonstrates Handows' usability in realistic multitasking scenarios, highlighting user-adapted workflows and spontaneous layout strategies. Our findings also suggest the potential of embedding mobile-inspired metaphors into proprioceptive body-centric interfaces to support low-effort and spatially coherent interaction in VR.},
  archive      = {J_TVCG},
  author       = {Jin-Du Wang and Ke Zhou and Haoyu Ren and Per Ola Kristensson and Xiang Li},
  doi          = {10.1109/TVCG.2025.3616843},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handows: A palm-based interactive multi-window management system in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of hand visibility in AR: Comparing dexterity and interaction with virtual and real objects. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand-tracking technologies allow us to use our own hands to interact with real and virtual objects in Augmented Reality (AR) environments. This enables us to explore the interplay between hand-visualizations and hand-object interactions. We present a user study that examines the effect of different hand visualizations (invisible, transparent, opaque) on manipulation performance when interacting with real and virtual objects. For this, we implemented video-see-through (VST) AR-based virtual building blocks and hot wire tasks with real one-to-one counterparts that require participants to use gross and fine motor hand movements. To evaluate manipulation performance, we considered three measures: task completion time, number of collisions (hot wire task), and percentage of object displacement (building block task). Additionally, we explored the sense of agency and subjective impressions (preference, ease of interaction, successful and awkwardness) evoked by the different hand-visualizations. The results show that (1) manipulation performance is significantly higher when interacting with real objects compared to virtual ones, (2) invisible hands lead to fewer errors, higher agency, higher perceived success and ease of interaction during fine manipulation tasks with real objects, and (3) having some visualization of the virtual hands (transparent or opaque) overlayed on the real hands is preferred when manipulating virtual objects even when there are no significant performance improvements. Our empirical findings about the differences when interacting with real and virtual objects can aid hand visualization choices for manipulation tasks in AR.},
  archive      = {J_TVCG},
  author       = {Jakob Hartbrich and Stephanie Arévalo Arboleda and Steve Göring and Alexander Raake},
  doi          = {10.1109/TVCG.2025.3616868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of hand visibility in AR: Comparing dexterity and interaction with virtual and real objects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal contrastive learning for cybersickness recognition using brain connectivity graph representation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness significantly impairs user comfort and immersion in virtual reality (VR). Effective identification of cybersickness leveraging physiological, visual, and motion data is a critical prerequisite for its mitigation. However, current methods primarily employ direct feature fusion across modalities, which often leads to limited accuracy due to inadequate modeling of inter-modal relationships. In this paper, we propose a multimodal contrastive learning method for cybersickness recognition. First, we introduce Brain Connectivity Graph Representation (BCGR), an innovative graph-based representation that captures cybersickness-related connectivity patterns across modalities. We further develop three BCGR instances: E-BCGR, constructed based on EEG signals; MV-BCGR, constructed based on video and motion data; and S-BCGR, obtained through our proposed standardized decomposition algorithm. Then, we propose a connectivity-constrained contrastive fusion module, which aligns E-BCGR and MV-BCGR into a shared latent space via graph contrastive learning while utilizing S-BCGR as a connectivity constraint to enhance representation quality. Moreover, we construct a multimodal cybersickness dataset comprising synchronized EEG, video, and motion data collected in VR environments to promote further research in this domain. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across four critical evaluation metrics: accuracy, sensitivity, specificity, and the area under the curve. Source code: https://github.com/PEKEW/cybersickness-bcgr.},
  archive      = {J_TVCG},
  author       = {Peike Wang and Ming Li and Ziteng Wang and Yong-Jin Liu and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal contrastive learning for cybersickness recognition using brain connectivity graph representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “Heart flows with zen”: Exploring multi-modal mixed reality to promote the inheritance and experience of cultural heritage. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preservation of cultural heritage (CH) is a complex and promising field. Driven by technological advancements, digitization has emerged as a crucial approach for revitalizing tangible/intangible cultural heritage (TCH/ICH). However, current research and practice remain limited in their exploration of abstract forms of ICH, such as traditional philosophies and ideologies. In this study, utilizing Zen as a context, we designed an immersive mixed reality (MR) experience system, Flowing with Zen, based on formative study and cultural symbol analysis. The MR system integrates multi-modal interfaces, motion capture, environmental sensing, and generative computing, enabling users to engage with four scenarios through meditation, life appreciation, and experiential Zen practice, providing the embodied experience of Zen. Comparative user evaluation (N = 51) revealed that the MR system has significant advantages in eliciting engagement and interest from users, enhancing their aesthetic appreciation and cultural understanding, and increasing the accessibility of Zen. Our research proposes a novel approach and design inspiration for the digital inheritance of abstract ICH.},
  archive      = {J_TVCG},
  author       = {Wenchen Guo and Zhirui Chen and Guoyu Sun and Hailiang Wang},
  doi          = {10.1109/TVCG.2025.3616750},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“Heart flows with zen”: Exploring multi-modal mixed reality to promote the inheritance and experience of cultural heritage},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GS-ProCams: Gaussian splatting-based projector-camera systems. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GSProCams is not only view-agnostic but also significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional co-located light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the projection surface's geometry and materials represented by Gaussians, and the global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It also uses only 1/10 of the GPU memory for training and is 900 times faster in inference speed. Please refer to our project page for the code and dataset: https://realqingyue.github.io/GS-ProCams/.},
  archive      = {J_TVCG},
  author       = {Qingyue Deng and Jijiang Li and Haibin Ling and Bingyao Huang},
  doi          = {10.1109/TVCG.2025.3616841},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GS-ProCams: Gaussian splatting-based projector-camera systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-blend: Privacy- and utility-preserving blendshape perturbation against re-identification attacks in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose p-Blend, an efficient and effective blendshape perturbation mechanism designed to defend against both intra- and cross-app re-identification attacks in virtual reality. p-Blend provides privacy protection when streaming blendshape data to third-party applications on VR devices. In its design, we consider both privacy and utility. p-Blend not only perturbs blendshape values to resist re-identification attacks but also preserves the smoothness of facial animations and the naturalness of facial expressions, ensuring the continued usability of the data. We validate the effectiveness of p-Blend through extensive empirical evaluations and user studies. Quantitative experiments on a large-scale dataset collected from 45 participants demonstrate that p-Blend significantly reduces re-identification accuracy across a range of machine learning models. While pure-random perturbation fails to prevent attacks that exploit statistical features, p-Blend effectively mitigates these risks in both raw and statistical blendshape data. Additionally, user study results show that facial animations generated from p-Blend-perturbed blendshapes maintain greater smoothness and naturalness compared to those using purely random perturbation. The codes and dataset are available at https://github.com/jingwei1016/p-Blend.},
  archive      = {J_TVCG},
  author       = {Jingwei Liu and Lai Wei and Yan Hu and Guangrong Zhao and Qing Yang and Guangdong Bai and Yiran Shen},
  doi          = {10.1109/TVCG.2025.3616736},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {P-blend: Privacy- and utility-preserving blendshape perturbation against re-identification attacks in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiteAT: A data-lightweight and user-adaptive VR telepresence system for remote education. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In educators' ongoing pursuit of enriching remote education, Virtual Reality (VR)-based telepresence has shown significant promise due to its immersive and interactive nature. Existing approaches often rely on point cloud or NeRF-based techniques to deliver realistic representations of teachers and classrooms to remote students. However, achieving low latency is non-trivial, and maintaining high-fidelity rendering under such constraints poses an even greater challenge. This paper introduces LiteAT, a data-lightweight and user-adaptive VR telepresence system, to enable real-time, immersive learning experiences. LiteAT employs a Gaussian Splatting-based reconstruction pipeline that integrates an SMPL-X–driven dynamic human model with a static classroom, supporting lightweight data transmission and high-quality rendering. To enable efficient and personalized exploration in the virtual classroom, we propose a user-adaptive viewpoint recommendation framework that dynamically suggests high-quality viewpoints tailored to user preferences. Candidate viewpoints are evaluated based on multiple visual quality factors and are continuously optimized based on recent user behavior and scene dynamics. Quantitative experiments and user studies validate the effectiveness of LiteAT across multiple evaluation metrics. LiteAT establishes a versatile and scalable foundation for immersive telepresence, potentially supporting real-time scenarios such as procedural teaching, multimodal instruction, and collaborative learning.},
  archive      = {J_TVCG},
  author       = {Yuxin Shen and Wei Liang and Jianzhu Ma},
  doi          = {10.1109/TVCG.2025.3616747},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiteAT: A data-lightweight and user-adaptive VR telepresence system for remote education},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral and symbolic fillers as delay mitigation for embodied conversational agents in virtual reality. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When communicating with embodied conversational agents (ECAs) in virtual reality, there might be delays in the responses of the agents lasting several seconds, for example, due to more extensive computations of the answers when large language models are used. Such delays might lead to unnatural or frustrating interactions. In this paper, we investigate filler types to mitigate these effects and lead to a more positive experience and perception of the agent. In a within-subject study, we asked 24 participants to communicate with ECAs in virtual reality, comparing four strategies displayed during the delays: a multimodal behavioral filler consisting of conversational and gestural fillers, a base condition with only idle motions, and two symbolic indicators with progress bars, one embedded as a badge on the agent, the other one external and visualized as a thinking bubble. Our results indicate that the behavioral filler improved perceived response time, three subscales of presence, humanlikeness, and naturalness. Participants looked away from the face more often when symbolic indicators were displayed, but the visualizations did not lead to a more positive impression of the agent or to increased presence. The majority of participants preferred the behavioral fillers, only 12.5% and 4.2% favored the symbolic embedded and external conditions, respectively.},
  archive      = {J_TVCG},
  author       = {Denmar Mojan Gonzales and Snehanjali Kalamkar and Sophie Jörg and Jens Grubert},
  doi          = {10.1109/TVCG.2025.3616865},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Behavioral and symbolic fillers as delay mitigation for embodied conversational agents in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring and modeling the effects of eye-tracking accuracy and precision on gaze-based steering in virtual environments. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in eye-tracking technology have positioned gaze as an efficient and intuitive input method for Virtual Reality (VR), offering a natural and immersive user experience. As a result, gaze input is now leveraged for fundamental interaction tasks such as selection, manipulation, crossing, and steering. Although several studies have modeled user steering performance across various path characteristics and input methods, our understanding of gaze-based steering in VR remains limited. This gap persists because the unique qualities of eye movements—involving rapid, continuous motions—and the variability in eye-tracking make findings from other input modalities nontransferable to a gaze-based context, underscoring the need for a dedicated investigation into gaze-based steering behaviors and performance. To bridge this gap, we present two user studies to explore and model gaze-based steering. In the first one, user behavior data are collected across various path characteristics and eye-tracking conditions. Based on this data, we propose four refined models that extend the classic Steering Law to predict users' movement time in gaze-based steering tasks, explicitly incorporating the impact of tracking quality. The best-performing model achieves an adjusted R2 of 0.956, corresponding to a 16% improvement in movement time prediction. This model also yields a substantial reduction in AIC (from 1550 to 1132) and BIC (from 1555 to 1142), highlighting improved model quality and better balance between goodness of fit and model complexity. Finally, data from a second study with varied settings, such as a different eye-tracking sampling rate, illustrate the strong robustness and predictability of our models. Finally, we present scenarios and applications that demonstrate how our models can be used to design enhanced gaze-based interactions in VR systems.},
  archive      = {J_TVCG},
  author       = {Xuning Hu and Yichuan Zhang and Yushi Wei and Liangyuting Zhang and Yue Li and Wolfgang Stuerzlinger and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2025.3616824},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring and modeling the effects of eye-tracking accuracy and precision on gaze-based steering in virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facilitating the exploration of linearly aligned objects in controller-free 3D environment with gaze and microgestures. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Extended Reality (XR) environments increasingly involve large amounts of data and content, designing effective exploration techniques has become critical. Depth-based object exploration is a common but underexplored task in XR environments, especially in settings without physical devices. Prior studies have largely focused on horizontal or planar interactions, leaving depthoriented exploration relatively overlooked. To bridge this gap, we propose three linearly aligned layer transition techniques (Continuous Push, Push&Return, and Tilt&Return) specifically designed to support efficient, precise, and continuous object exploration along the depth axis within depth-based UIs. In a user study with 30 participants, we compared their performance, usability, and user preference across two different layer configurations (8-layer vs. 16-layer). The results highlight that Continuous Push enables faster exploration with lower effort, while Push&Return provides the highest accuracy and is most preferred by users. Based on these findings, we discuss design implications for depth-based interaction techniques in controller-free XR environments.},
  archive      = {J_TVCG},
  author       = {Jihyeon Lee and Jinwook Kim and Jeongmi Lee},
  doi          = {10.1109/TVCG.2025.3616833},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Facilitating the exploration of linearly aligned objects in controller-free 3D environment with gaze and microgestures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LITFORAGER: Exploring multimodal literature foraging strategies in immersive sensemaking. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring and comprehending relevant academic literature is a vital yet challenging task for researchers, especially given the rapid expansion in research publications. This task fundamentally involves sensemaking—interpreting complex, scattered information sources to build understanding. While emerging immersive analytics tools have shown cognitive benefits like enhanced spatial memory and reduced mental load, they predominantly focus on information synthesis (e.g., organizing known documents). In contrast, the equally important information foraging phase—discovering and gathering relevant literature—remains underexplored within immersive environments, hindering a complete sensemaking workflow. To bridge this gap, we introduce LITFORAGER, an interactive literature exploration tool designed to facilitate information foraging of research literature within an immersive sensemaking workflow using network-based visualizations and multimodal interactions. Developed with WebXR and informed by a formative study with researchers, LITFORAGER supports exploration guidance, spatial organization, and seamless transition through a 3D literature network. An observational user study with 15 researchers demonstrated LITFORAGER's effectiveness in supporting fluid foraging strategies and spatial sensemaking through its multimodal interface.},
  archive      = {J_TVCG},
  author       = {Haoyang Yang and Elliott H. Faa and Weijian Liu and Shunan Guo and Duen Horng Chau and Yalong Yang},
  doi          = {10.1109/TVCG.2025.3616732},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LITFORAGER: Exploring multimodal literature foraging strategies in immersive sensemaking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Let's do it my way: Effects of personality and age of virtual characters. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing interactions between humans and virtual characters requires careful consideration of various human perceptions and user experiences. While numerous studies have explored the effects of several virtual characters' properties, the impacts of the virtual character's personality and age on human perceptions and experiences have yet to be thoroughly investigated. To address this gap, we conducted a within-group study (N = 28) following a 2 (personality: egoism vs. altruism) × 2 (age: child vs. adult) design to explore how the personality and age factors influence human perception and experience during interactions with virtual characters. In each condition of our study, our participants co-solved a jigsaw puzzle with a virtual character that embodied combinations of personality and age. After each condition, participants completed a survey. We also asked them to provide written feedback at the end of the study. Our statistical analyses revealed that the virtual character's personality and age significantly influenced participants' perceptions and experiences. The personality factor affected perceptions of altruism, anthropomorphism, likability, safety, and all aspects of user experience, including perceived collaboration, rapport, emotional reactivity, and the desire for future interaction. Additionally, the virtual character's age affected our participants' ratings of the uncanny valley and likability. We also identified an interaction effect between personality and age factors on the virtual character's anthropomorphism. Based on our findings, we offered guidelines and insights for researchers aiming to design collaborative experiences with virtual characters of different personalities and ages.},
  archive      = {J_TVCG},
  author       = {Minsoo Choi and Dixuan Cui and Siqi Guo and Dominic Kao and Christos Mousas},
  doi          = {10.1109/TVCG.2025.3616815},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Let's do it my way: Effects of personality and age of virtual characters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic eyebox steering for improved pinlight AR near-eye displays. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optical-see-through near-eye display (NED) for augmented reality (AR) allows the user to perceive virtual and real imagery simultaneously. Existing technologies for optical-see-through AR NEDs involve trade-offs between key metrics such as field of view (FOV), eyebox size, form factor, etc. We have enhanced an existing compact wide-FOV pinlight AR NED design with real-time 3D pupil localization in order to dynamically steer and thus effectively enlarge the usable eyebox. This is achieved with a dual-camera rig that captures stereoscopic views of the pupils. The 3D pupil location is used to dynamically calculate a display pattern that spatio-temporally modulates the light entering the wearer's eyes. We have built a demonstrable compact prototype and have conducted a user study that indicates the effectiveness of our eyebox steering method (e.g., without eyebox steering, in 10.5% of our tests, users were unable to perceive the test pattern correctly before experiment timeout; with eyebox steering, that fraction decreased dramatically to 1.25%). This is a small yet crucial step in making simple wide-FOV pinlight NEDs usable for human users and not just as demonstration prototypes filmed with a precisely positioned camera standing in for the user's eye. Further contributions of this paper include a detailed description of display design, calibration technique, and user study design, all of which may benefit other NED research.},
  archive      = {J_TVCG},
  author       = {Xinxing Xia and Zheye Yu and Dongyu Qiu and Andrei State and Tat-Jen Cham and Frank Guan and Henry Fuchs},
  doi          = {10.1109/TVCG.2025.3616807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic eyebox steering for improved pinlight AR near-eye displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of user performance and experience between light field and conventional AR glasses. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3617940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field AR glasses can provide better visual comfort than conventional AR glasses; however, studies on user performance comparison between them are notably scarce. In this paper, we present a systematic method employing a serial visual search task without confounding factors to quantify and compare the user performance and experience between these two types of AR glasses at two different viewing distances, 30 cm and 60 cm, and in two modes, purely virtual VR mode and virtualreal integration AR mode. The results show that the light field AR glasses led to a significantly faster reaction speed and higher accuracy than the conventional AR glasses at 30 cm in the AR mode. The participant feedback also shows that the former led to better virtual-real integration. User performance and experience of the light field AR glasses remained consistent across different viewing distances. Although the conventional AR glasses had a better search efficiency than the light field AR glasses at 60 cm in both AR and VR modes, it had more negative feedback from the participants. Overall, the design of this experiment successfully allows us to quantify the effect of VAC and underscores the strength of the evaluation method},
  archive      = {J_TVCG},
  author       = {Wei-An Teng and Su-Ling Yeh and Homer H. Chen},
  doi          = {10.1109/TVCG.2025.3617940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison of user performance and experience between light field and conventional AR glasses},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing hand and forearm gestures to control virtual forearm for user-initiated forearm deformation. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3616825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the development of virtual reality (VR) technology, there is growing research on VR avatar body deformation effects. However, previous research mainly focused on passive body deformation expression, leaving users with limited methods to actively control their virtual bodies. To address this gap, we explored user-controlled forearm deformation by investigating how hand and forearm gestures can be mapped to various degrees of avatar forearm deformation. We conducted a gesture design workshop with six designers to generate gesture sets for different forearm deformations and deformation degrees, resulting in 15 gesture sets. Then, we selected the three highest-rated gesture sets and conducted a comparative study to evaluate the sense of embodiment and user performance across the three gesture sets. Our findings provide design suggestions for gesture-controlled forearm deformation in VR.},
  archive      = {J_TVCG},
  author       = {Yilong Lin and Han Shi and Weitao Jiang and Xuesong Zhang and Hye-Young Jo and Yoonji Kim and Seungwoo Je},
  doi          = {10.1109/TVCG.2025.3616825},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing hand and forearm gestures to control virtual forearm for user-initiated forearm deformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaRoundWorm: A virtual reality escape room game for learning the lifecycle and immune response to parasitic infections. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promoting public health is challenging owing to its abstract nature, and individuals may be apprehensive about confronting it. Recently, there has been an increasing interest in using the metaverse and gamification as novel educational techniques to improve learning experiences related to the immune system. Thus, we present MetaRoundWorm, an immersive virtual reality (VR) escape room game designed to enhance the understanding of parasitic infections and host immune responses through interactive, gamified learning. The application simulates the lifecycle of Ascaris lumbricoides and corresponding immunological mechanisms across anatomically accurate environments within the human body. Integrating serious game mechanics with embodied learning principles, MetaRoundWorm offers players a task-driven experience combining exploration, puzzle-solving, and immune system simulation. To evaluate the educational efficacy and user engagement, we conducted a controlled study comparing MetaRoundWorm against a traditional approach, i.e., interactive slides. Results indicate that MetaRoundWorm significantly improves immediate learning outcomes, cognitive engagement, and emotional experience, while maintaining knowledge retention over time. Our findings suggest that immersive VR gamification holds promise as an effective pedagogical tool for communicating complex biomedical concepts and advancing digital health education.},
  archive      = {J_TVCG},
  author       = {Xuanru Cheng and Xian Wang and Chi-lok Tai and Lik-Hang Lee},
  doi          = {10.1109/TVCG.2025.3616752},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MetaRoundWorm: A virtual reality escape room game for learning the lifecycle and immune response to parasitic infections},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttentionPainter: An efficient and adaptive stroke predictor for scene painting. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3618184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke-based Rendering (SBR) aims to decompose an input image into a sequence of parameterized strokes, which can be rendered into a painting that resembles the input image. Recently, Neural Painting methods that utilize deep learning and reinforcement learning models to predict the stroke sequences have been developed, but suffer from longer inference time or unstable training. To address these issues, we propose AttentionPainter, an efficient and adaptive model for single-step neural painting. First, we propose a novel scalable stroke predictor, which predicts a large number of stroke parameters within a single forward process, instead of the iterative prediction of previous Reinforcement Learning or auto-regressive methods, which makes AttentionPainter faster than previous neural painting methods. To further increase the training efficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times acceleration for training. Moreover, we propose Stroke-density Loss, which encourages the model to use small strokes for detailed information, to help improve the reconstruction quality. Finally, we design a Stroke Diffusion Model as an application of AttentionPainter, which conducts the denoising process in the stroke parameter space and facilitates stroke-based inpainting and editing applications helpful for human artists' design. Extensive experiments show that AttentionPainter outperforms the state-of-the-art neural painting methods.},
  archive      = {J_TVCG},
  author       = {Yizhe Tang and Yue Wang and Teng Hu and Ran Yi and Xin Tan and Lizhuang Ma and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TVCG.2025.3618184},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AttentionPainter: An efficient and adaptive stroke predictor for scene painting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EnVisionVR: A scene interpretation tool for visual accessibility in virtual reality. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective visual accessibility in Virtual Reality (VR) is crucial for Blind and Low Vision (BLV) users. However, designing visual accessibility systems is challenging due to the complexity of 3D VR environments and the need for techniques that can be easily retrofitted into existing applications. While prior work has studied how to enhance or translate visual information, the advancement of Vision Language Models (VLMs) provides an exciting opportunity to advance the scene interpretation capability of current systems. This paper presents EnVisionVR, an accessibility tool for VR scene interpretation. Through a formative study of usability barriers, we confirmed the lack of visual accessibility features as a key barrier for BLV users of VR content and applications. In response, we used our findings from the formative study to inform the design and development of EnVisionVR, a novel visual accessibility system leveraging a VLM, voice input and multimodal feedback for scene interpretation and virtual object interaction in VR. An evaluation with 12 BLV users demonstrated that EnVisionVR significantly improved their ability to locate virtual objects, effectively supporting scene understanding and object interaction.},
  archive      = {J_TVCG},
  author       = {Junlong Chen and Rosella P. Galindo Esparza and Vanja Garaj and Per Ola Kristensson and John Dudley},
  doi          = {10.1109/TVCG.2025.3617147},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EnVisionVR: A scene interpretation tool for visual accessibility in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DenseSplat: Densifying gaussian splatting SLAM with neural radiance prior. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian SLAM systems excel in real-time rendering and fine-grained reconstruction compared to NeRF-based systems. However, their reliance on extensive keyframes is impractical for deployment in real-world robotic systems, which typically operate under sparse-view conditions that can result in substantial holes in the map. To address these challenges, we introduce DenseSplat, the first SLAM system that effectively combines the advantages of NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for initializing primitives that densely populate maps and seamlessly fill gaps. It also implements geometry-aware primitive sampling and pruning strategies to manage granularity and enhance rendering efficiency. Moreover, DenseSplat integrates loop closure and bundle adjustment, significantly enhancing frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale datasets demonstrate that DenseSplat achieves superior performance in tracking and mapping compared to current state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Mingrui Li and Shuhong Liu and Tianchen Deng and Hongyu Wang},
  doi          = {10.1109/TVCG.2025.3617961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DenseSplat: Densifying gaussian splatting SLAM with neural radiance prior},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WonderHuman: Hallucinating unseen parts in dynamic 3D human reconstruction. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3618268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.},
  archive      = {J_TVCG},
  author       = {Zilong Wang and Zhiyang Dou and Yuan Liu and Cheng Lin and Xiao Dong and Yunhui Guo and Chenxu Zhang and Xin Li and Wenping Wang and Xiaohu Guo},
  doi          = {10.1109/TVCG.2025.3618268},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WonderHuman: Hallucinating unseen parts in dynamic 3D human reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). See what i mean? mobile eye-perspective rendering for optical see-through head-mounted displays. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based scene understanding allows Augmented Reality (AR) systems to provide contextual visual guidance in unprepared, real-world environments. While effective on video see-through (VST) head-mounted displays (HMDs), such methods suffer on optical see-through (OST) HMDs due to misregistration between the world-facing camera and the user's eye perspective. To approximate the user's true eye view, we implement and evaluate three software-based eye-perspective rendering (EPR) techniques on a commercially available, untethered OST HMD (Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distance plane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and (3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns the projection with the user's gaze depth. A user study on real-world tasks underscores the importance of accurate EPR and demonstrates gaze-proxy as a lightweight alternative to geometry-based methods. We release our EPR framework as open source.},
  archive      = {J_TVCG},
  author       = {Gerlinde Emsenhuber and Tobias Langlotz and Denis Kalkofen and Markus Tatzgern},
  doi          = {10.1109/TVCG.2025.3616739},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {See what i mean? mobile eye-perspective rendering for optical see-through head-mounted displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portable silent room: Exploring VR design for anxiety and emotion regulation for neurodivergent women and non-binary individuals. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodivergent individuals, particularly those with Autism and Attention Deficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic attacks, meltdowns, and emotional dysregulation due to societal pressures and inadequate accommodations. These challenges are especially pronounced for neurodivergent women and non-binary individuals navigating intersecting barriers of neurological differences and gender expectations. This research investigates virtual reality (VR) as a portable safe space for emotional regulation, addressing challenges of sensory overload and motion sickness while enhancing relaxation capabilities. Our mixed-methods approach included an online survey (N = 223) and an ideation workshop (N = 32), which provided key design elements for creating effective calming VR environments. Based on these findings, we developed and iteratively tested VR prototypes with neurodivergent women and non-binary participants (N = 12), leading to a final version offering enhanced adaptability to individual sensory needs. This final prototype underwent a comprehensive evaluation with 25 neurodivergent participants to assess its effectiveness as a regulatory tool. This research contributes to the development of inclusive, adaptive VR environments that function as personalized “portable silent rooms” offering neurodivergent individuals on-demand access to sensory regulation regardless of physical location.},
  archive      = {J_TVCG},
  author       = {Kinga Skierś and Yun Suen Pai and Marina Nakagawa and Kouta Minamizawa and Giulia Barbareschi},
  doi          = {10.1109/TVCG.2025.3616828},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Portable silent room: Exploring VR design for anxiety and emotion regulation for neurodivergent women and non-binary individuals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “I was truly able to express the image of myself that i have within”: Exploring VR group therapy approaches with the LGBTQIA+ community. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Members of the LGBTQIA+ community are more likely to face mental health challenges. However, stigma and the fear of being outed often prevent them from seeking professional support. To address this, we collaborated with mental health professionals and LGBTQIA+ communities in Japan to develop a multi-user Virtual Reality (VR) platform that facilitates access to group therapy sessions. The system allows users to participate using personalized avatars and customized voices, preserving anonymity while enabling them to present themselves as they wish. We conducted a user study with 21 LGBTQIA+ participants and two qualified counselors to evaluate their experiences with VR-based therapy. Findings revealed that the created avatars enabled participants to express their chosen gender identity and increase confidence, acting as protective intermediaries. However, participants also noted how anonymity could affect trust, and suggested that better representation of body language and the introduction of trust-building activities could help compensate for such ambivalence. Overall, the platform fostered a strong sense of co-presence, and both counselors and LGBTQIA+ members felt that, with some ergonomic adjustment to improve the comfort of the headset during longer sessions, VR platforms could offer substantial opportunities for safe and representative access to mental health services.},
  archive      = {J_TVCG},
  author       = {Kinga Skierś and Danyang Peng and Anish Kundu and Tanner Person and Kenkichi Takase and Tamii Nagoshi and Sawako Nakayama and Yano Yuichiro and Tomoyuki Miyazaki and Kouta Minamizawa and Giulia Barbareschi},
  doi          = {10.1109/TVCG.2025.3616754},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“I was truly able to express the image of myself that i have within”: Exploring VR group therapy approaches with the LGBTQIA+ community},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nanouniverse: Virtual instancing of structural detail and adaptive shell mapping. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3618914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches. In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing. Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements. Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration. We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing. We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment. Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes. Our method also supports advanced effects such as clipping planes and animations. We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms.},
  archive      = {J_TVCG},
  author       = {Ruwayda Alharbi and Ondřej Strnad and Markus Hadwiger and Ivan Viola},
  doi          = {10.1109/TVCG.2025.3618914},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Nanouniverse: Virtual instancing of structural detail and adaptive shell mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EgoTrigger: Toward audio-driven image capture for human memory enhancement in all-day energy-efficient smart glasses. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All-day smart glasses are likely to emerge as platforms capable of continuous contextual sensing, uniquely positioning them for unprecedented assistance in our daily lives. Integrating the multi-modal AI agents required for human memory enhancement while performing continuous sensing, however, presents a major energy efficiency challenge for all-day usage. Achieving this balance requires intelligent, context-aware sensor management. Our approach, EgoTrigger, leverages audio cues from the microphone to selectively activate power-intensive cameras, enabling efficient sensing while preserving substantial utility for human memory enhancement. EgoTrigger uses a lightweight audio model (YAMNet) and a custom classification head to trigger image capture from hand-object interaction (HOI) audio cues, such as the sound of a drawer opening or a medication bottle being opened. In addition to evaluating on the QA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement Question-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated first-person QA pairs from full-length Ego4D videos that were curated to ensure that they contained audio, focusing on HOI moments critical for contextual understanding and memory. Our results show EgoTrigger can use 54% fewer frames on average, significantly saving energy in both power-hungry sensing components (e.g., cameras) and downstream operations (e.g., wireless transmission), while achieving comparable performance on datasets for an episodic memory task. We believe this context-aware triggering strategy represents a promising direction for enabling energy-efficient, functional smart glasses capable of all-day use — supporting applications like helping users recall where they placed their keys or information about their routine activities (e.g., taking medications).},
  archive      = {J_TVCG},
  author       = {Akshay Paruchuri and Sinan Hersek and Lavisha Aggarwal and Qiao Yang and Xin Liu and Achin Kulshrestha and Andrea Colaco and Henry Fuchs and Ishan Chatterjee},
  doi          = {10.1109/TVCG.2025.3616866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EgoTrigger: Toward audio-driven image capture for human memory enhancement in all-day energy-efficient smart glasses},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do we read texts in VR?: The effects of text segment quantity and social distraction on text readability in virtual museum contexts. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual museums are increasingly used to present cultural and educational content, often relying on textual descriptions to convey essential information. However, when users interact with text objects in virtual environments, the optimal text segment quantity for readability and comprehension remains unclear, especially when social distractions such as other visitors are present. This study investigated the effects of text segment quantity and the presence of other visitors on text readability and comprehension in the context of virtual museums. Participants read exhibit descriptions under four text-segment length conditions (1, 2, 4, and 8 lines) with or without simulated visitor agents. The results indicated that readability and comprehension were maximized when text was presented in intermediate segmentation lengths (2 and 4 Lines), while both excessively short (1 line) and long (8 lines) text segments hindered reading performance. Additionally, a significant interaction between text length and the presence of visitors was observed. Specifically, the presence of visitors led to increased comprehension task completion times only in the intermediate segmentation conditions, suggesting that social presence imposes an additional cognitive demand as social distractions in optimal text segment conditions. These findings provide empirical guidelines for designing effective text-based information systems in virtual museums, optimizing both user engagement and learning outcomes in immersive cultural environments.},
  archive      = {J_TVCG},
  author       = {Jungmin Lee and Hyuckjin Jang and Jeongmi Lee},
  doi          = {10.1109/TVCG.2025.3616803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How do we read texts in VR?: The effects of text segment quantity and social distraction on text readability in virtual museum contexts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Casual-VRAuth: A design framework bridging focused and casual interactions for behavioral authentication in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current behavioral authentication systems in Virtual Reality (VR) require sustained focused interaction during task execution - an assumption frequently incompatible with real-world constraints across two factors: (1) physical limitations (e.g., restricted hand/eye mobility), and (2) psychological barriers (e.g., task-switching fatigue or break-in-presence). To address this attentional gap, we propose a design framework bridging focused and casual interactions in behavior-based VR authentication (Casual-VRAuth). Based on this framework, we designed an authentication prototype using a modified ball-and-tunnel task (propelling a ball along a circular path), supporting three interaction modes: baseline Touch, and two eyes-free options (Hover/Tapping). Experimental results demonstrate that our framework effectively guides the design of authentication systems with varying interaction engagement levels (Touch >Hover >Tapping) to accommodate scenarios requiring casual interaction (e.g., multitasking or eyes-free operation). Furthermore, we revealed that reducing interaction engagement enhances resistance to mimicry attacks while decreasing cognitive workload and error rates in multitasking or eyes-free environments. However, this approach compromises the average classification accuracy of Interaction behavior under different algorithms (including InceptionTime, FCN, ResNet, CNN, MLP, and MCDCNN). Notably, moderate reduction of interaction engagement enhances authentication speed, while excessive reduction may conversely slow it down. Overall, our work establishes a novel design paradigm for VR authentication that supports casual interactions and offers valuable insights into balancing usability and security.},
  archive      = {J_TVCG},
  author       = {GuanYu Ye and BoYu Gao and Huawei Tu},
  doi          = {10.1109/TVCG.2025.3616834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Casual-VRAuth: A design framework bridging focused and casual interactions for behavioral authentication in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SERES: Semantic-aware neural reconstruction from sparse views. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3619144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semantic-aware neural reconstruction method to generate 3D high-fidelity models from sparse images. To tackle the challenge of severe radiance ambiguity caused by mismatched features in sparse input, we enrich neural implicit representations by adding patch-based semantic logits that are optimized together with the signed distance field and the radiance field. A novel regularization based on the geometric primitive masks is introduced to mitigate shape ambiguity. The performance of our approach has been verified in experimental evaluation. The average chamfer distances of our reconstruction on the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When working as a plugin for those dense reconstruction baselines such as NeuS and Neuralangelo, the average error on the DTU dataset can be reduced by 69% and 68% respectively.},
  archive      = {J_TVCG},
  author       = {Bo Xu and Yuhu Guo and Yuchao Wang and Wenting Wang and Yeung Yam and Charlie C.L. Wang and Xinyi Le},
  doi          = {10.1109/TVCG.2025.3619144},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SERES: Semantic-aware neural reconstruction from sparse views},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Upright-net+: Enhanced learning of upright orientation for 3D point clouds. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3605201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D shape analysis is heavily influenced by the pose of input 3D models, as the continuous nature of pose space introduces complexities that usually exceed the encoding capacities of standard deep learning frameworks. To tackle this challenge, we present Upright-Net+, an enhancement of our previous model, Upright-Net, specifically developed for estimating upright orientation in 3D point clouds. Our approach is grounded in the design principle that ”form ever follows function,” treating the natural base of an object as a functional structure that stabilizes it in its typical pose, influenced by physical laws and geometric properties. We reformulate the continuous orientation problem into a discrete classification task, focusing on learning the points that constitute the natural base of a 3D model. The upright orientation is determined by aligning the normal orientation of this base towards the mass center. To mitigate over-smoothing in the global feature embeddings from stacked graph convolutional layers, we introduce a Global Positional Encoding Module using Relative Distance Histogram Statistics Embedding (GPE-RDHS), which reduces structural ambiguity and enhances orientation estimation. We also enhanced a weighted residual loss term to penalize false positive predictions, enhancing overall model performance. Our method demonstrates exceptional performance in upright orientation estimation and reveals that the learned orientation-aware features significantly benefit downstream tasks, particularly in classification.},
  archive      = {J_TVCG},
  author       = {Xufang Pang and Feng Li and Hongjie Zhuang and Ning Ding and Xiaopin Zhong and Shengfeng He and Wenxi Liu and Bo Jiang},
  doi          = {10.1109/TVCG.2025.3605201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Upright-net+: Enhanced learning of upright orientation for 3D point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectrum alignment for robust 3D point cloud correspondences estimation. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3605711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating dense point-to-point correspondences between two isometric shapes represented as 3D point clouds is a fundamental problem in geometry processing, with applications in texture and motion transfer. However, this task becomes particularly challenging when the shapes undergo non-rigid transformations, as is often the case with approximately isometric point clouds. Most existing algorithms address this challenge by establishing correspondences between functions defined on the shapes, rather than directly between points, because function mappings admit a linear representation in the spectral domain. State-of-the-art methods compute this linear representation using the eigenfunctions of the Laplace–Beltrami Operator (LBO) along with a small set of initial corresponding functions between the shapes. However, for approximately isometric point clouds, two key issues arise: (1) the eigenfunctions of the LBO may become misaligned, and (2) the initial corresponding functions may include outliers, both of which degrade the quality of the resulting correspondences. In this work, we propose an efficient approach to align the spectra of the LBOs of the two shapes, enabling the eigenfunctions to remain compatible even for approximately isometric 3D point clouds. Additionally, we introduce a technique to make function correspondence estimation robust to outliers. We validate our approach by comparing it with state-of-the-art 3D shape-matching algorithms on benchmark datasets, demonstrating its effectiveness.},
  archive      = {J_TVCG},
  author       = {Deepanshu Solanki and Rajendra Nagar},
  doi          = {10.1109/TVCG.2025.3605711},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectrum alignment for robust 3D point cloud correspondences estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3606509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography is a promising approach to recreate lifelike 3D scenes. However, due to the current Spatial Light Modulators (SLMs) lacking sufficient pixels, the defocused planes of holograms always exhibit obvious interference phenomena. The methods based on random phase can alleviate this problem, but they always affect the imaging quality of the focal plane. Meanwhile, direct current (DC) noise of nondiffracted light in SLMs, coupled with ubiquitous dynamic noise, has long been a fundamental issue affecting holographic display quality. In this study, we proposed a method based on static high-pass filtering and time-multiplexing that overcomes the traditional tradeoff between divergence capability of holograms and display quality in focal planes. Simultaneously, the proposed method can eliminate DC and dynamic noise with a simple and robust structure. Moreover, we further extended artificial intelligencedriven algorithms to achieve higher-quality on-axis amplitudeonly holograms. The Simulations and experiments demonstrated that the developed method is a promising and easily generalized solution for time-multiplexing holography},
  archive      = {J_TVCG},
  author       = {Chenhang Shen and Yuhang Zheng and Yifei Xie and Zhu Wang and Yulang Peng and Weilong Zhou and Junming Zhu and Zichun Le},
  doi          = {10.1109/TVCG.2025.3606509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceived weight of mediated reality sticks. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3591181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediated reality, where augmented reality (AR) and diminished reality (DR) meet, enables visual modifications to real-world objects. A physical object with a mediated reality visual change retains its original physical properties. However, it is perceived differently from the original when interacted with. We present such a mediated reality object, a stick with different lengths or a stick with a missing portion in the middle, to investigate how users perceive its weight and center of gravity. We conducted two user studies ($N=10$), each of which consisted of two substudies. We found that the length of mediated reality sticks influences the perceived weight. A longer stick is perceived as lighter, and vice versa. The stick with a missing portion tends to be recognized as one continuous stick. Thus, its weight and center of gravity (COG) remain the same. We formulated the relationship between inertia based on the reported COG and perceived weight in the context of dynamic touch.},
  archive      = {J_TVCG},
  author       = {Satoshi Hashiguchi and Yuta Kataoka and Asako Kimura and Shohei Mori},
  doi          = {10.1109/TVCG.2025.3591181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceived weight of mediated reality sticks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3609456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution images come with rich color information and texture details. Due to the rapid upgrading of display devices and rendering technologies, high-resolution real-time rendering faces the computational overhead challenge. To address this, the current mainstream solution is to render at a lower resolution and then upsample to the target resolution by supersampling techniques. However, while many prior supersampling approaches have attempted to exploit rich rendered data such as color, depth, motion vectors at low resolution, there is little discussion on how to harness high-frequency information that is readily available in the high-resolution (HR) G-buffers of modern renders. In this paper, we seek to investigate how to fully leverage information from HR G-buffers to maximize the visual quality of supersampling results. We propose a neural network for real-time supersampling of rendered content, which is based on several core designs, including gated G-buffers encoder, G-buffers attended encoder and reflection-aware loss. These designs are especially made for the sake of effectively using HR G-buffers, enabling faithful recovery of a variety of high-frequency scene details from low-resolution, highly aliased inputs. Furthermore, a simple occlusion-aware blender is proposed to efficiently rectify invalid features in the warped previous frame, allowing us to better exploit history information to improve temporal stability. The experiments show that our method, equipped with strong ability to harness HR G-buffer information, significantly improves the visual fidelity of high-resolution reconstructions upon previous state-of-the-art methods, even for challenging $4 \times 4$ upsampling, while still being compute-efficient.},
  archive      = {J_TVCG},
  author       = {Pengjie Wang and Chengzhi Yuan and Jie Guo and Xiaosong Yang and Houjie Li and Ian Stephenson and Jian Chang and Ying Cao},
  doi          = {10.1109/TVCG.2025.3609456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-FRAME: Direction-field-based wireframe extraction for complex CAD models. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3609350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting wireframes from CAD models represented by point cloud remains a significant challenge in computer graphics. This difficulty arises from two main factors: first, imperfections in the point cloud data, such as lack of orientation, noise, and sparsity; and second, the inherent complexity of geometric shapes, which often feature a high density of sharp edges in close proximity. In this paper, we propose D-FRAME, a multi-stage wireframe extraction framework that incorporates a novel direction field to improve edge detection quality and connectivity, a refinement strategy to address sparse or noisy edge points, and a final coarse-to-fine connection module to extract a robust wireframe. The direction field not only facilitates connectivity but also enhances the precision of extracted edges by mitigating the impact of misclassified points. By combining the Restricted Voronoi Diagram (RVD) with the extracted wireframes and the original point cloud, our approach also achieves highly faithful reconstruction of CAD model. Experiments conducted on synthetic and real-world scanned CAD datasets demonstrate that D-FRAME effectively manages noise, sparsity, and complex geometries, yielding high-fidelity wireframes. Code is available at https://github.com/yuanfeng-01/D-FRAME-test.},
  archive      = {J_TVCG},
  author       = {Yuan Feng and Honghao Dai and Guangshun Wei and Long Ma and Pengfei Wang and Yuanfeng Zhou and Ying He},
  doi          = {10.1109/TVCG.2025.3609350},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {D-FRAME: Direction-field-based wireframe extraction for complex CAD models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive affordances in visualization: Related constructs, design factors, and framework. <em>TVCG</em>, 1-17. (<a href='https://doi.org/10.1109/TVCG.2025.3610803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classically, affordance research investigates how the shape of objects communicates actions to potential users. Cognitive affordances, a subset of this research, characterize how the design of objects influences cognitive actions, such as information processing. Within visualization, cognitive affordances inform how graphs' design decisions communicate information to their readers. Although several related concepts exist in visualization, a formal translation of affordance theory to visualization is still lacking. In this paper, we review and translate affordance theory to visualization by formalizing how cognitive affordances operate within a visualization context. We also review common methods and terms, and compare related constructs to cognitive affordances in visualization. Based on a synthesis of research from psychology, human-computer interaction, and visualization, we propose a framework of cognitive affordances in visualization that enumerates design decisions and reader characteristics that influence a visualization's hierarchy of communicated information. Finally, we demonstrate how this framework can guide the evaluation and redesign of visualizations.},
  archive      = {J_TVCG},
  author       = {Racquel Fygenson and Lace Padilla and Enrico Bertini},
  doi          = {10.1109/TVCG.2025.3610803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cognitive affordances in visualization: Related constructs, design factors, and framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytical texture mapping. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3611315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling of warped images has been a topic of research for a long time but only seldomly has focused on theoretically exact resampling. We present a resampling method for minification, applied on the texture mapping function of a 3D graphics pipeline, that is derived from sampling theory without making any approximations. Our method supports freely selectable 2D integratable prefilter (anti-aliasing) functions and uses a 2D box reconstruction filter. We have implemented our method both for CPU and GPU (OpenGL) using multiple prefilter functions defined by piece-wise polynomials. The correctness of our exact resampling method has been made plausible by comparing texture mapping results of our method with those of extreme supersampling. We additionally show how the prefilter of our method can also be applied for high quality polygon edge anti-aliasing. Since our proposed method does not use any approximations, up to numerical precision, it can be used as a reference for approximate texture mapping methods.},
  archive      = {J_TVCG},
  author       = {Koen Meinds and Elmar Eisemann},
  doi          = {10.1109/TVCG.2025.3611315},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analytical texture mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LucidDreamer: Domain-free generation of 3D gaussian splatting scenes. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3611489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality 3D scenes is a critical challenge in computer vision, driven by advances in 3D graphics and the growing demand for immersive environments. While object-centric 3D generation has achieved significant progress, scene generation remains difficult due to the scarcity of large-scale 3D scene datasets and scalability constraints of conventional 3D representations, which hinder efficient large-scale expansion. To address these challenges, we propose LucidDreamer, a novel pipeline that synthesizes diverse, high-quality, and expandable 3D scenes using a unified 3D Gaussian splatting representation. Our approach employs an iterative Navigation-Dreaming-Alignment process, leveraging 2D image generation and depth estimation to construct photorealistic, scalable 3D environments. By iteratively generating images and navigating through the scene, LucidDreamer fully utilizes the power of image generation models, enabling the creation of highly detailed and expandable 3D scenes. LucidDreamer supports various input modalities, including text, RGB, and RGBD, and enables dynamic modifications during generation. Experimental results demonstrate that LucidDreamer outperforms existing methods in generating high-quality, diverse, structurally consistent, and navigable 3D scenes. The project page is available on: https://luciddreamer-cvlab.github.io/.},
  archive      = {J_TVCG},
  author       = {Jaeyoung Chung and Suyoung Lee and Hyeongjin Nam and Jaerin Lee and Kyoung Mu Lee},
  doi          = {10.1109/TVCG.2025.3611489},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LucidDreamer: Domain-free generation of 3D gaussian splatting scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic importance monte carlo SPH vortical flows with lagrangian samples. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3612190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Lagrangian dynamic importance Monte Carlo method without non-trivial random walks for solving the Velocity-Vorticity Poisson Equation (VVPE) in Smoothed Particle Hydrodynamics (SPH) for vortical flows. Key to our approach is the use of the Kinematic Vorticity Number (KVN) to detect vortex cores and to compute the KVN-based importance of each particle when solving the VVPE. We use Adaptive Kernel Density Estimation (AKDE) to extract a probability density distribution from the KVN for the the Monte Carlo calculations. Even though the distribution of the KVN can be non-trivial, AKDE yields a smooth and normalized result which we dynamically update at each time step. As we sample actual particles directly, the Lagrangian attributes of particle samples ensure that the continuously evolved KVN-based importance, modeled by the probability density distribution extracted from the KVN by AKDE, can be closely followed. Our approach enables effective vortical flow simulations with significantly reduced computational overhead and comparable quality to the classic Biot-Savart law that in contrast requires expensive global particle querying.},
  archive      = {J_TVCG},
  author       = {Xingyu Ye and Xiaokun Wang and Yanrui Xu and Alexandru C. Telea and Jiří Kosinka and Lihua You and Jian Jun Zhang and Jian Chang},
  doi          = {10.1109/TVCG.2025.3612190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic importance monte carlo SPH vortical flows with lagrangian samples},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFG-PCN: Point cloud completion with degree-flexible point graph. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3612379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is a vital task focused on reconstructing complete point clouds and addressing the incompleteness caused by occlusion and limited sensor resolution. Traditional methods relying on fixed local region partitioning, such as k-nearest neighbors, which fail to account for the highly uneven distribution of geometric complexity across different regions of a shape. This limitation leads to inefficient representation and suboptimal reconstruction, especially in areas with fine-grained details or structural discontinuities. This paper proposes a point cloud completion framework called Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns node degrees using a detail-aware metric that combines feature variation and curvature, focusing on structurally important regions. We further introduce a geometry-aware graph integration module that uses Manhattan distance for edge aggregation and detail-guided fusion of local and global features to enhance representation. Extensive experiments on multiple benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Jian Yao and Shiqing Xin},
  doi          = {10.1109/TVCG.2025.3612379},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DFG-PCN: Point cloud completion with degree-flexible point graph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene-based foveated fluid animation in virtual reality. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3609904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically-based fluid animation in Virtual Reality (VR) significantly enhances the user experience through visually engaging flow motions. Nonetheless, such simulations are often limited by their substantial computational demands. A tailored adaptive simulation algorithm is important for high-performance VR fluid simulations, which dynamically allocate degrees of freedom (DoF) while accounting for user perception in VR. This paper proposes a novel scene-based gaze-contingent fluid simulation system for VR, featuring a highly adaptive fluid simulator integrated with a VR perceptual model that accounts for the foveation and geometry of fluid. Our method leverages an eccentricity and curvature-dependent perceptual model to dynamically allocate computational resources, improving the efficiency and maintaining spatio-temporal stability of fluid animation in VR. A user study was conducted to measure the simulation resolution thresholds for fluid animations in VR, considering various levels of eccentricity and curvature. Our findings indicate notable differences in perceptual thresholds based on these metrics. By incorporating these insights into our adaptive fluid simulator as a unified sizing function, we maintain perceptually optimal particle resolution, achieving up to a 3.62× performance improvement while delivering superior perceptual realism and user experience, as validated by a subjective evaluation study.},
  archive      = {J_TVCG},
  author       = {Yue Wang and Yan Zhang and Xuanhui Yang and Hui Wang and Xubo Yang},
  doi          = {10.1109/TVCG.2025.3609904},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene-based foveated fluid animation in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Textured mesh quality assessment using geometry and color field similarity. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3612942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textured mesh quality assessment (TMQA) is critical for various 3D mesh applications. However, existing TMQA methods often struggle to provide accurate and robust evaluations. Motivated by the effectiveness of fields in representing both 3D geometry and color information, we propose a novel point-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes signed distance fields and a newly proposed color field named nearest surface point color field to realize effective mesh feature description. Four features related to visual perception are extracted from the geometry and color fields: geometry similarity, geometry gradient similarity, space color distribution similarity, and space color gradient similarity. Experimental results on three benchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA) TMQA metrics. Furthermore, FMQM exhibits low computational complexity, making it a practical and efficient solution for real-world applications in 3D graphics and visualization. Our code is publicly available at: https://github.com/yyyykf/FMQM.},
  archive      = {J_TVCG},
  author       = {Kaifa Yang and Qi Yang and Yiling Xu and Zhu Li},
  doi          = {10.1109/TVCG.2025.3612942},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Textured mesh quality assessment using geometry and color field similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SketchRefiner: Text-guided sketch refinement through latent diffusion models. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3613388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-hand sketches serve as efficient tools for creativity and communication, yet expressing ideas clearly through sketches remains challenging for untrained individuals. Optimizing sketches through text guidance can enhance individuals' ability to effectively convey their ideas and improve overall communication efficiency. While recent advancements in Artificial Intelligence Generated Content (AIGC) have been notable, research on optimizing free-hand sketches remains relatively unexplored. In this paper, we introduce SketchRefiner, an innovative method designed to refine rough sketches from various categories into polished versions guided by text prompts. SketchRefiner utilizes a latent diffusion model with ControlNet to guide a differentiable rasterizer in optimizing a set of Bézier curves. We extend the score distillation sampling (SDS) loss and introduce a joint semantic loss to encourage sketches aligned with given text prompts and free-hand sketches. Additionally, we propose a fusion attention-map stroke initialization strategy to improve the quality of refined sketches. Furthermore, SketchRefiner provides users with fine-grained control over text guidance. Through extensive experiments, we demonstrate that our method can generate accurate and aesthetically pleasing refined sketches that closely align with input text prompts and sketches.},
  archive      = {J_TVCG},
  author       = {Yingjie Tian and Minghao Liu and Haoran Jiang and Yunbin Tu and Duo Su},
  doi          = {10.1109/TVCG.2025.3613388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SketchRefiner: Text-guided sketch refinement through latent diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent space map for visual utilization of generated data. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3614247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Samples produced by generative models, called Generated Samples (GSs), have become a critical supplement to those collected from the real world in data-centric applications. Domain experts typically randomly collect many GSs and manually select a few of interest for applications. However, the methodology lacks guidance to locate desirable ones that exhibit specific features or adhere to application-oriented metrics among infinite generable candidates. These samples are generally concentrated in a few small regions of the generative model's latent space, called Generative Latent Space (GLS). This paper presents Latent Space Map that projects a GLS onto a plane to help users locate regions rich in desirable GSs. Our research revolves around two challenges in constructing the map. First, many GSs in a GLS are low-quality and useless for applications. Excluding them from the projection is challenging for their irregular distribution. We employ a Monte Carlo-based method to capture a manifold for projection, where high-quality GSs are mainly distributed. Second, the GLS is high-dimensional and unbounded, complicating the projection. We design a manifold projection method that endows the map with desirable characteristics to achieve high display accuracy and effective pattern perception for users freely observing the manifold. We further develop a system integrating Latent Space Map to aid in GS selection and refinement. Real-world cases, quantitative experiments, and feedback from domain experts confirm the usability and effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Yang Zhang and Jie Li and Wei Zeng},
  doi          = {10.1109/TVCG.2025.3614247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Latent space map for visual utilization of generated data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptual model for foveated rendering with illuminance demodulation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3614349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated rendering exploits the non-uniform acuity of human vision to allocate computational resources more efficiently by reducing image fidelity in the peripheral field of view. While existing perceptual models for foveated rendering focus primarily on spatial resolution and contrast sensitivity, they overlook the perceptual asymmetry between direct and indirect illumination. In this work, we introduce a novel perceptual model that incorporates illuminance demodulation to account for this distinction. Our model adaptively modulates the foveation rate based on the relative contributions of direct and indirect illumination. Building on this model, we develop a practical rendering framework that separately applies tailored foveation strategies to direct and indirect illumination effects. Quantitative metrics and user studies confirm that our method maintains perceptual equivalence to full-resolution rendering. The sparse rendering stage achieves a $2.18\times$ to $7.10\times$ speedup, contributing to an overall acceleration of $1.71\times$ to $3.26\times$.},
  archive      = {J_TVCG},
  author       = {Xiao Hu and Xiang Xu and JiuXing Zhang and YanNing Xu and Lu Wang},
  doi          = {10.1109/TVCG.2025.3614349},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual model for foveated rendering with illuminance demodulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic sphere tracing of signed distance fields for old glass modelling and rendering. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3613853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for modeling and rendering irregular and heterogeneous glass objects, with a specific emphasis on stained glass windows and window works often encountered in architecture from middle age to 18th century. The artisanal production of sheet glass results in glass panels displaying a vast variety of surface and volume irregularities like bubbles, irregular surface or smoothly varying refractive index, all of which contribute to the specific visual aspect of old glass. We propose to account for all the aforementioned effects in a unified framework based on signed distance functions and an analytic solution of the ray tracing equations on tetrahedral volume elements. We demonstrate how to construct an unbiased estimator for the transmitted lighting produced by such panels by using Fermat's principle and results from seismic ray theory. We use texture coordinates to map arbitrary sections of a complex glass panel onto the individual faces of a mesh, allowing the modeling and rendering of complex 3-dimensional objects composed of colored glass facets such as stained glass windows.},
  archive      = {J_TVCG},
  author       = {Quentin Huan and François Rousselle and Christophe Renaud},
  doi          = {10.1109/TVCG.2025.3613853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parabolic sphere tracing of signed distance fields for old glass modelling and rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PICA: Physics-integrated clothed avatar. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3614642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce PICA, a novel representation for high-fidelity animatable clothed human avatars with physics-plausible dynamics, even for loose clothing. Previous neural rendering-based representations of animatable clothed humans typically employ a single model to represent both the clothing and the underlying body. While efficient, these approaches often fail to represent complex garment dynamics, leading to incorrect deformations and noticeable rendering artifacts, especially for sliding or loose garments. Furthermore, most previous works represent garment dynamics as pose-dependent deformations and facilitate novel pose animations in a data-driven manner. This often results in outcomes that do not faithfully represent the mechanics of motion and are prone to generating artifacts in out-of-distribution poses. To address these issues, we employ two individual 2D Gaussian Splatting (2DGS) models with different deformation characteristics, modeling the human body and clothing separately. This distinction allows for better handling of their respective motion characteristics. With this representation, we integrate a graph neural network (GNN)-based clothing physics simulation module to ensure a better representation of clothing dynamics. Our method, through its carefully designed features, achieves high-fidelity rendering of clothed human bodies in complex and novel driving poses, outperforming previous methods under the same settings. The source code will be available on our project page: https://ustc3dv.github.io/PICA/},
  archive      = {J_TVCG},
  author       = {Bo Peng and Yunfan Tao and Haoyu Zhan and Yudong Guo and Juyong Zhang},
  doi          = {10.1109/TVCG.2025.3614642},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PICA: Physics-integrated clothed avatar},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strange familiars: Exploring the design of avatars and virtual environments for reconnecting dormant ties in virtual reality. <em>TVCG</em>, 1-13. (<a href='https://doi.org/10.1109/TVCG.2025.3614445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rekindling old social bonds with individuals who were once a part of our lives but have since faded away is crucial for our well-being. Such connections with dormant ties help us overcome loneliness and provide social support. Recently, virtual reality (VR) emerged as a promising tool for facilitating social interactions, such as online gatherings for formal or casual activities. VR can offer immersive and shared experiences, facilitating genuine connections between people. This provides a unique advantage over traditional computer-mediated communication methods. However, while prior research has explored how VR can aid in forming new social connections, its potential to reconnect dormant ties is largely unexplored. This paper aims to bridge this gap by examining how different features of VR, specifically avatar appearance and virtual environments, influence reactivations of dormant ties. We conducted an experiment involving 24 dyads to investigate the effect of different avatar-self similarities and virtual environments on the perceptions and interactions between dormant ties. Our findings indicate that avatars resembling oneself and dormant ties promote social closeness. Familiar virtual environments evoke shared memories, while unfamiliar ones stimulate more conversations. We discuss the impact of VR features on reconnecting dormant ties and provide implications for re-connecting relationships in VR.},
  archive      = {J_TVCG},
  author       = {Yu-Ting Yen and Fang-Ying Liao and Chi-Lan Yang and Ruei-Che Chang and Fu-Yin Cherng and Bing-Yu Chen},
  doi          = {10.1109/TVCG.2025.3614445},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Strange familiars: Exploring the design of avatars and virtual environments for reconnecting dormant ties in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manual-free gaze interaction via bayesian-based implicit intention prediction. <em>TVCG</em>, 1-13. (<a href='https://doi.org/10.1109/TVCG.2025.3615198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye gaze is regarded as a promising interaction modality in extended reality (XR) environments. However, to address the challenges posed by the Midas touch problem, the determination of selection intention frequently relies on the implementation of additional manual selection techniques, such as explicit gestures (e.g., controller/hand inputs or dwell), which are inherently limited in their functionality. We hereby present a machine learning (ML) model based on the Bayesian framework, which is employed to predict user selection intention in real-time, with the unique distinction that all data used for training and prediction are obtained from gaze data alone. The model utilizes a Bayesian approach to transform gaze data into selection probabilities, which are subsequently fed into an ML model to discern selection intentions. In Study 1, a high-performance model was constructed, enabling real-time inference using solely gaze data. This approach was found to enhance performance, thereby validating the efficacy of the proposed methodology. In Study 2, a user study was conducted to validate a manual-free technique based on the prediction model. The advantages of eliminating explicit gestures and potential applications were also discussed.},
  archive      = {J_TVCG},
  author       = {Taewoo Jo and Ho Jung Lee and Sulim Chun and In-Kwon Lee},
  doi          = {10.1109/TVCG.2025.3615198},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Manual-free gaze interaction via bayesian-based implicit intention prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion-aware brushing for reliable cluster analysis in multidimensional projections. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3615314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brushing is a common interaction technique in 2D scatterplots, allowing users to select clustered points within a continuous, enclosed region for further analysis or filtering. However, applying conventional brushing to 2D representations of multidimensional (MD) data, i.e., Multidimensional Projections (MDPs), can lead to unreliable cluster analysis due to MDP-induced distortions that inaccurately represent the cluster structure of the original MD data. To alleviate this problem, we introduce a novel brushing technique for MDPs called Distortion-aware brushing. As users perform brushing, Distortion-aware brushing correct distortions around the currently brushed points by dynamically relocating points in the projection, pulling data points close to the brushed points in MD space while pushing distant ones apart. This dynamic adjustment helps users brush MD clusters more accurately, leading to more reliable cluster analysis. Our user studies with 24 participants show that Distortion-aware brushing significantly outperforms previous brushing techniques for MDPs in accurately separating clusters in the MD space and remains robust against distortions. We further demonstrate the effectiveness of our technique through two use cases: (1) conducting cluster analysis of geospatial data and (2) interactively labeling MD clusters.},
  archive      = {J_TVCG},
  author       = {Hyeon Jeon and Michaël Aupetit and Soohyun Lee and Kwon Ko and Youngtaek Kim and Ghulam Jilani Quadri and Jinwook Seo},
  doi          = {10.1109/TVCG.2025.3615314},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Distortion-aware brushing for reliable cluster analysis in multidimensional projections},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semantic talking style space for speech-driven facial animation. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3615390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a latent talking style space with semantic meanings for speech-driven 3D facial animation. The style space is learned from 3D speech facial animations via a self-supervision paradigm without any style labeling, leading to an automatic separation of high-level attributes, i.e., different channels of the latent style code possess different semantic meanings, such as a wide/slightly open mouth, a grinning/round mouth, and frowning/raising eyebrows. The style space enables intuitive and flexible control of talking styles in speech-driven facial animation through manipulating the channels of style code. To effectively learn such a style space, we propose a two-stage approach, involving two deep neural networks, to disentangle the person identity, speech content, and talking style contained in 3D speech facial animations. The training is performed on a novel dataset of 3D talking faces of various styles, constructed from over ten hours of videos of 200 subjects collected from the Internet.},
  archive      = {J_TVCG},
  author       = {Yujin Chai and Yanlin Weng and Tianjia Shao and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3615390},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A semantic talking style space for speech-driven facial animation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeG-Gaussian:Segmentation-guided 3D gaussian optimization for novel view synthesis. <em>TVCG</em>, 1-13. (<a href='https://doi.org/10.1109/TVCG.2025.3615421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiance field based methods have recently revolutionized novel view synthesis of scenes captured with multi-view photos. A significant recent advance is 3D Gaussian Splatting (3DGS), which utilizes a set of 3D Gaussians to represent a radiance field, yielding high-fidelity results in real-time rendering. However, we have observed that 3DGS struggles to capture the necessary details in sparsely observed regions, where there is not enough gradient for effective split and clone operations. In this paper, we present a novel solution to address this limitation. Our key idea is to leverage segmentation information to identify poorly optimized regions within the 3D Gaussian representation. By applying split or clone operations on the corresponding 3D Gaussians in these regions, we aim to refine the spatial distribution of Gaussians and enhance the overall quality of high-fidelity 3D scene reconstruction. To further optimize the reconstruction process, we introduce two spatial regularization terms: repulsion loss and smoothness loss. These terms effectively minimize overlap and redundancy among Gaussians, reducing outliers in the synthesized geometry. By incorporating these regularization techniques, our approach achieves state-of-the-art performance in real-time novel view synthesis and significantly improves visibility in less observed regions, leading to a more compact and accurate 3D scene representation.},
  archive      = {J_TVCG},
  author       = {Ling-Xiao Zhang and Chenbo Jiang and Yu-Kun Lai and Lin Gao},
  doi          = {10.1109/TVCG.2025.3615421},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SeG-Gaussian:Segmentation-guided 3D gaussian optimization for novel view synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From simple to polychromatic: An empirical study on optimal color schemes for optical see-through head-mounted displays. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3590876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical see-through head-mounted displays (OHMDs) blend digital content with the physical world, presenting unique color management challenges. Previous literature suggests using green as the main color, but this severely limits creative freedom. To address this, we conducted an empirical study with 30 participants, evaluating 216 colors under various OHMD usage conditions. Based on the results, we propose color guidelines indicating each hue's clear and comfortable saturation and brightness ranges, along with clarity and comfort scores across hues for different devices and lighting conditions. Our color guidelines expand the usable color palette, offering designers a wider range of color options. These guidelines were used and iteratively refined through feedback in a workshop with 12 designers, integrating them into practical design workflows. The resulting comprehensive color guide provides a valuable resource for OHMD interface designers, enhancing both the aesthetic possibilities and functional effectiveness of augmented reality experiences.},
  archive      = {J_TVCG},
  author       = {Yue Gu and Runze Cai and Ashwin Ram and Yuxuan Li and Haimo Zhang and Shengdong Zhao},
  doi          = {10.1109/TVCG.2025.3590876},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From simple to polychromatic: An empirical study on optimal color schemes for optical see-through head-mounted displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointDreamer: Zero-shot 3D textured mesh reconstruction from colored point cloud. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3595987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faithfully reconstructing textured meshes is crucial for many applications. Compared to text or image modalities, leveraging 3D colored point clouds as input (colored-PC-to-mesh) offers inherent advantages in comprehensively and precisely replicating the target object's $360^{\circ }$ characteristics. While most existing colored-PC-to-mesh methods suffer from blurry textures or require hard-to-acquire 3D training data, we propose PointDreamer, a novel framework that harnesses 2D diffusion prior for superior texture quality. Crucially, unlike prior 2D-diffusion-for-3D works driven by text or image inputs, PointDreamer successfully adapts 2D diffusion models to 3D point cloud data by a novel project-inpaint-unproject pipeline. Specifically, it first projects the point cloud into sparse 2D images and then performs diffusion-based inpainting. After that, diverging from most existing 3D reconstruction or generation approaches that predict texture in 3D/UV space thus often yielding blurry texture, PointDreamer achieves high-quality texture by directly unprojecting the inpainted 2D images to the 3D mesh. Furthermore, we identify for the first time a typical kind of unprojection artifact appearing in occlusion borders, which is common in other multiview-image-to-3D pipelines but less-explored. To address this, we propose a novel solution named the Non-Border-First (NBF) unprojection strategy. Extensive qualitative and quantitative experiments on various synthetic and real-scanned datasets demonstrate that PointDreamer, though zero-shot, exhibits SoTA performance ( 30% improvement on LPIPS score from 0.118 to 0.068), and is robust to noisy, sparse or even incomplete input data. Code at: https://github.com/YuQiao0303/PointDreamer.},
  archive      = {J_TVCG},
  author       = {Qiao Yu and Xianzhi Li and Yuan Tang and Xu Han and Jinfeng Xu and Long Hu and Min Chen},
  doi          = {10.1109/TVCG.2025.3595987},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PointDreamer: Zero-shot 3D textured mesh reconstruction from colored point cloud},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DFaceSculptor: A common framework for image-guided 3D face deformation. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3596482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose 3DFaceSculptor, a general-purpose framework for interactive 3D face editing. Given a source 3D face mesh with semantic materials, and a user-specified semantic image, 3DFaceSculptor can accurately edit the source mesh following the shape guidance of the semantic image, while preserving the source topology as rigid as possible. Recent studies on generating 3D faces focus on learning neural networks to predict 3D shapes, which requires high-cost 3D training datasets. These learning-based methods are limited in compatibility and can only handle face styles involved in the training datasets. Unlike these methods, our 3DFaceSculptor is a non-training and common framework, which only requires supervision from readily-available semantic images, and is compatible with producing various face styles unlimited by datasets. In 3DFaceSculptor, based on the differentiable renderer technique, we deform the source face mesh according to the correspondences between semantic images and mesh materials. However, guiding complex 3D shapes with a simple 2D image incurs extra challenges, that is, the deformation accuracy, surface smoothness, geometric rigidity, and global synchronization of the edited mesh must be guaranteed. To address these challenges, we propose a hierarchical optimization architecture to balance the global and local shape features, and further propose various strategies and losses to improve properties of accuracy, smoothness, rigidity, and so on. Extensive experiments show that our 3DFaceSculptor is able to produce impressive results and has reached the state-of-the-art level.},
  archive      = {J_TVCG},
  author       = {Hao Su and Xuxi Wang and Jianwei Niu and Xuefeng Liu and Xinghao Wu and Nana Wang},
  doi          = {10.1109/TVCG.2025.3596482},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3DFaceSculptor: A common framework for image-guided 3D face deformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thunderstruck: Visually simulating electrical storms. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3596334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thunderstorms are complex multiphysics phenomena driven by charge transfer processes arising from interactions between ice and water particles in the atmosphere. We present a physically grounded model for simulating cloud electrification and lightning discharge, capable of generating diverse lightning types as emergent responses to evolving atmospheric conditions. Our approach requires only a minimal set of atmospheric parameters and no user-defined triggers. Charge separation is modeled at the microphysical level using a statistical mechanics framework, while discharges are captured through a novel gauge-invariant dielectric breakdown model that accounts for bipolar channels, dynamic electric fields, and air resistance. We validate our method through comparisons with observational data and prior models, demonstrating its ability to simulate distinct discharge types and the full life cycle of thunderstorms. Beyond scientific accuracy, our framework supports real-time nowcasting, civil engineering assessments, virtual environment generation, and the simulation of complex dielectric breakdown in varied contexts.},
  archive      = {J_TVCG},
  author       = {Jorge Alejandro Amador Herrera and Jonathan Klein and Daniel T. Banuti and Wojtek Pałubicki and Sören Pirk and Dominik L. Michels},
  doi          = {10.1109/TVCG.2025.3596334},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Thunderstruck: Visually simulating electrical storms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty visualization for biomolecular structures: An empirical evaluation. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3596385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty is an intrinsic property of almost all data, regardless of the data being measured, simulated, or generated. It can significantly influence the results and reliability of subsequent analysis steps. Clearly communicating uncertainties is crucial for informed decision-making and understanding, especially in biomolecular data, where uncertainty is often difficult to infer. Uncertainty visualization (UV) is a powerful tool for this purpose. However, previously proposed UV methods lack sufficient empirical evaluation. We collected and categorized visualization methods for portraying positional uncertainty in biomolecular structures. We then organized the methods into metaphorical groups and extracted nine representatives: color, clouds, ensemble, hulls, sausages, contours, texture, waves, and noise. We assessed their strengths and weaknesses in a twofold approach: expert assessments with six domain experts and three perceptual evaluations involving 1,756 participants. Through the expert assessments, we aimed to highlight the advantages and limitations of the individual methods for the application domain and discussed areas for necessary improvements. Through the perceptual evaluation, we investigated whether the visualizations are intuitively associated with uncertainty and whether the directionality of the mapping is perceived as intended. We also assessed the accuracy of inferring uncertainty values from the visualizations. Based on our results, we judged the appropriateness of the metaphors for encoding uncertainty and suggest further areas for improvement.},
  archive      = {J_TVCG},
  author       = {Anna Sterzik and Michael Krone and Daniel Baum and Douglas W. Cunningham and Kai Lawonn},
  doi          = {10.1109/TVCG.2025.3596385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty visualization for biomolecular structures: An empirical evaluation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffCap: Diffusion-based real-time human motion capture using sparse IMUs and a monocular camera. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3596403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. The code will be released.},
  archive      = {J_TVCG},
  author       = {Shaohua Pan and Xinyu Yi and Yan Zhou and Weihua Jian and Yuan Zhang and Pengfei Wan and Feng Xu},
  doi          = {10.1109/TVCG.2025.3596403},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DiffCap: Diffusion-based real-time human motion capture using sparse IMUs and a monocular camera},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeSC: Learning deep semantic descriptor for NeRF registration. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3596289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NeRF registration has gained increasing attention recently. While existing research demonstrates considerable potential for this task, most methods primarily focus on either global geometric or rendering photometric information during feature learning, overlooking the rich cross-modal information inherent in the NeRF embedding feature space. In this paper, we propose DeSC, a novel NeRF registration approach that leverages the rich cross-modal features from NeRF to learn robust semantic descriptors. In particular, we propose a Deep Semantic Aggregation module, which employs a weighted graph convolution network to capture high-frequency texture details in NeRF patches. This approach reveals the underlying semantics shared across different NeRFs of the same scene, thereby yielding more robust global feature descriptors that lead to better alignment accuracy and robustness. In addition, we design a density-aware photometric consistency loss that facilitates the learning of robust features. Extensive experimental results on Objaverse datasets demonstrate that our approach produces superior registration performance to state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Sheldon Fung and Wei Pan and Kui Su and Hui Cui and Xinkui Zhao and Xuequan Lu},
  doi          = {10.1109/TVCG.2025.3596289},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeSC: Learning deep semantic descriptor for NeRF registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). More like vis, less like vis: Comparing interactions for integrating user preferences into partial specification recommenders. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3596541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization recommendation systems make data exploration less tedious by automating the process of visualization generation. They are particularly helpful for non-expert users who may not be familiar with a data set or the process of visualization specification. These systems allow users to input their preferences in the form of partial specifications to steer the recommendations made. However, the interaction approaches for partial specification input and their trade-offs have not been explored in prior work. In this paper, we compare three different combinations of interaction approaches and granularities for users to indicate a preferred partial specification: 1) manual input, 2) inferring preferred partial specifications from binary like/dislike ratings for a visualization as a whole, or 3) inferring preferred partial specifications from binary like/dislike ratings for granular components of a visualization specification. In a between-subjects study, participants were assigned to one of three conditions and asked to complete a data exploration task. Our results indicate that manual input led to a greater coverage of data dimensions, while like/dislike ratings led to a greater diversity of marks and channels used. Qualitative participant feedback also reveals differences in user strategy and visualization comprehension across the three interaction conditions. Finally, we conclude with a discussion on implications for multiplicity and visualization comprehension during visual data exploration.},
  archive      = {J_TVCG},
  author       = {Grace Guo and Subhajit Das and Jian Zhao and Alex Endert},
  doi          = {10.1109/TVCG.2025.3596541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {More like vis, less like vis: Comparing interactions for integrating user preferences into partial specification recommenders},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic changes of latency perception threshold in virtual reality: Behavioral and EEG evidence. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3596919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) technologies in fields such as telehealth, teleconferencing, and virtual education are significantly affected by end-to-end latency, which notably impacts users' interactive experience and performance. Previous research suggests that a perceptual threshold may exist—once latency is reduced below a certain level, users no longer perceive it, and their interactive performance remains largely unaffected. However, there is no consensus on the exact value of this absolute latency perception threshold. In this study, we employed an experimental design based on Fitts' law to investigate whether interaction strategies and task difficulty can alter the latency perception threshold (LPT), and how variations in this threshold influence users' interactive performance. The results show that the LPT is approximately 130-170 ms, and that when interaction strategies prioritize speed or when tasks become more challenging, users exhibit heightened sensitivity to latency. Due to the presence of the LPT, the effect of latency on interactive performance follows a nonlinear pattern, and building on this finding, we refined a Fitts' law model to incorporate the influence of latency. Notably, electroencephalogram (EEG) signals can still capture users' perception of latency when they are unaware of minor latency, demonstrating a level of sensitivity that exceeds conscious awareness. Our findings provide insights into latency effects on performance and perception, guiding the design of more responsive VR interaction systems.},
  archive      = {J_TVCG},
  author       = {Songyue Yang and Kang Yue and Haolin Gao and Mei Guo and Yu Liu and Dan Zhang and Yue Liu},
  doi          = {10.1109/TVCG.2025.3596919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic changes of latency perception threshold in virtual reality: Behavioral and EEG evidence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PKSS-align: Robust point cloud registration on pre-kendall shape space. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3597017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a classical topic in the field of 3D Vision and Computer Graphics. Generally, the implementation of registration is typically sensitive to similarity transformations (translation, scaling, and rotation), noisy points, and incomplete geometric structures. Especially, the non-uniform scales and defective parts of point clouds increase probability of struck local optima in registration task. In this paper, we propose a robust point cloud registration PKSS-Align that can handle various influences, including similarity transformations, non-uniform densities, random noisy points, and defective parts. The proposed method measures shape feature-based similarity between point clouds on the Pre-Kendall shape space (PKSS), which is a shape measurement-based scheme and doesn't require point-to-point or point-to-plane metric. The employed measurement can be regarded as the manifold metric that is robust to various representations in the Euclidean coordinate system. Benefited from the measurement, the transformation matrix can be directly generated for point clouds with mentioned influences at the same time. The proposed method does not require data training and complex feature encoding. Based on a simple parallel acceleration, it can achieve significant improvement for efficiency and feasibility in practice. Experiments demonstrate that our method outperforms the relevant state-of-the-art methods. Project link: https://github.com/vvvwo/PKSS-Align.},
  archive      = {J_TVCG},
  author       = {Chenlei Lv and Hui Huang},
  doi          = {10.1109/TVCG.2025.3597017},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PKSS-align: Robust point cloud registration on pre-kendall shape space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersive ecological virtual environment for inducing balance disturbances. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3598464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to assess the impact of different visual perturbations on user balance within an immersive Virtual Environment (VE), emphasising ecological validity. Utilising a Virtual Reality headset, the researchers implemented various visual perturbations to discern their effects. The objective was to create a versatile tool featuring animations simulating diverse indoor and outdoor settings within a typical household, providing a range of ecological scenarios for evaluating user balance in the context of these visual disturbances. Participants were exposed to unpredictable visual disturbances within the VE, and their responses were captured using a combination of inertial sensors, electromyography and galvanic skin response. A comprehensive dataset replicating real-world fall scenarios on balance disturbances was collected. Throughout the trials, participants moved around the house, unaware of the timing or nature of visual perturbations. Statistical analysis identified variables with significant mean changes when visual disturbances were introduced. Muscle groups crucial for balance recovery following external disturbances exhibited the most substantial associations, akin to kinematic variables linked to loss of balance. Visual disturbances alone could induce balance disturbances, which were further categorized based on a statistical analysis. This tool offered a versatile platform for assessing balance under various challenging scenarios, facilitating research into balance control and training methods. The dataset generated through this protocol enabled a wide understanding of the impact of visual disturbances on balance and the associated compensatory reactions, shedding light on their potential applications in clinical and rehabilitative settings.},
  archive      = {J_TVCG},
  author       = {Nuno Ferrete Ribeiro and André Veloso and Henrique Pires and Cristina P. Santos},
  doi          = {10.1109/TVCG.2025.3598464},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive ecological virtual environment for inducing balance disturbances},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VIVA: Virtual healthcare interactions using visual analytics, with controllability through configuration. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3599458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly integrated physicians into the triage process of their virtual healthcare service to improve patient outcomes and satisfaction with this service and preserve health care system capacity. We present the design and implementation of a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual Analytics), to support HLBC in analysing various forms of usage data from the service. We abstract HLBC's data and data analysis tasks, which we use to inform our design of VIVA. We also present the interactive workflow abstraction of Scan, Act, Adapt. We validate VIVA's design through three case studies with stakeholder domain experts. We also propose the Controllability Through Configuration model to conduct and analyze design studies, and discuss architectural evolution of VIVA through that lens. It articulates configuration, both that specified by a developer or technical power user and that constructed automatically through log data from previous interactive sessions, as a bridge between the rigidity of hardwired programming and the time-consuming implementation of full end-user interactivity. Availability: Supplemental materials at https://osf.io/wv38n.},
  archive      = {J_TVCG},
  author       = {Jürgen Bernard and Mara Solen and Helen Novak Lauscher and Kurtis Stewart and Kendall Ho and Tamara Munzner},
  doi          = {10.1109/TVCG.2025.3599458},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIVA: Virtual healthcare interactions using visual analytics, with controllability through configuration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo label learning for partial point cloud registration. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3600395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial point cloud registration plays a crucial role in computer vision and has widespread applications in 3D map construction, pose estimation, and high-precision localization. However, the collected point clouds often contain missing data due to hardware limitations and complex environments. Various partial registration algorithms have been proposed, most of which rely on estimating overlap regions. However, a significant proportion of these algorithms rely heavily on ground truth labels. Manual labeling is both time-consuming and labor-intensive, whereas algorithmic automatic labeling lacks sufficient accuracy. To tackle this issue, we present PSEudo Label learning for unsupervised partial point cloud registration (PSEL). This method utilizes complementary tasks to learn reliable pseudo labels for overlap regions and correspondences without depending on ground truth labels. The key idea is to use the complementarity between overlap estimation and registration to generate two types of pseudo labels based on the nearest points in pairs of aligned point clouds. These pseudo labels are then employed to supervise the learning of overlap regions and correspondences, gradually enhancing their accuracy throughout the learning process and ultimately establishing an unsupervised learning framework. PSEL consists of an overlap estimation module and a correspondence filtering module. The pseudo labels generated after registration are used to supervise both modules. Notably, the correspondence filtering module has two pipelines. The similarity and difference of the corresponding point features are used to eliminate false correspondences during the training and inference stages, respectively, with only the latter being optimized with pseudo labels. To validate the effectiveness of our registration method, we conducted experiments using the synthetic dataset ModelNet40, the indoor dataset 3DMatch, and the outdoor dataset KITTI. The code is available at https://github.com/yifans923/PSEL.},
  archive      = {J_TVCG},
  author       = {Wenping Ma and Yifan Sun and Yue Wu and Yue Zhang and Hao Zhu and Biao Hou and Licheng Jiao},
  doi          = {10.1109/TVCG.2025.3600395},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pseudo label learning for partial point cloud registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StrucADT: Generating structure-controlled 3D point clouds with adjacency diffusion transformer. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3600392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 3D point cloud generation, numerous 3D generative models have demonstrated the ability to generate diverse and realistic 3D shapes. However, the majority of these approaches struggle to generate controllable 3D point cloud shapes that meet user-specific requirements, hindering the large-scale application of 3D point cloud generation. To address the challenge of lacking control in 3D point cloud generation, we are the first to propose controlling the generation of point clouds by shape structures that comprise part existences and part adjacency relationships. We manually annotate the adjacency relationships between the segmented parts of point cloud shapes, thereby constructing a StructureGraph representation. Based on this StructureGraph representation, we introduce StrucADT, a novel structure-controllable point cloud generation model, which consists of StructureGraphNet module to extract structure-aware latent features, cCNF Prior module to learn the distribution of the latent features controlled by the part adjacency, and Diffusion Transformer module conditioned on the latent features and part adjacency to generate structure-consistent point cloud shapes. Experimental results demonstrate that our structure-controllable 3D point cloud generation method produces high-quality and diverse point cloud shapes, enabling the generation of controllable point clouds based on user-specified shape structures and achieving state-of-the-art performance in controllable point cloud generation on the ShapeNet dataset.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Jiajun Shen and Zhongui Chen and Xiaoguang Han and Shiqing Xin},
  doi          = {10.1109/TVCG.2025.3600392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StrucADT: Generating structure-controlled 3D point clouds with adjacency diffusion transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on annotations in information visualization: Empirical studies, applications and challenges. <em>TVCG</em>, 1-20. (<a href='https://doi.org/10.1109/TVCG.2025.3600957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotations are widely used in information visualization to guide attention, clarify patterns, and support interpretation. We present a comprehensive survey of 191 research papers describing empirical studies, tools, techniques, and systems that incorporate annotations across various visualization contexts. Based on a structured analysis, we characterize annotations by their types, generation methods, and targets, and examine their use across four primary application domains: user engagement, storytelling, collaboration, and exploratory data analysis. We also discuss key trends, practical challenges, and open research directions. These findings offer a foundation for designing more effective annotation systems and advancing future research on annotation in visualization. An interactive web resource detailing the surveyed papers is available at https://shape-vis.github.io/annotation_star/.},
  archive      = {J_TVCG},
  author       = {Md Dilshadur Rahman and Bhavana Doppalapudi and Ghulam Jilani Quadri and Paul Rosen},
  doi          = {10.1109/TVCG.2025.3600957},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on annotations in information visualization: Empirical studies, applications and challenges},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransGI: Real-time dynamic global illumination with object-centric neural transfer model. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3596146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural rendering algorithms have revolutionized computer graphics, yet their impact on real-time rendering under arbitrary lighting conditions remains limited due to strict latency constraints in practical applications. The key challenge lies in formulating a compact yet expressive material representation. To address this, we propose TransGI, a novel neural rendering method for real-time, high-fidelity global illumination. It comprises an object-centric neural transfer model for material representation and a radiance-sharing lighting system for efficient illumination. Traditional BSDF representations and spatial neural material representations lack expressiveness, requiring thousands of ray evaluations to converge to noise-free colors. Conversely, realtime methods trade quality for efficiency by supporting only diffuse materials. In contrast, our object-centric neural transfer model achieves compactness and expressiveness through an MLPbased decoder and vertex-attached latent features, supporting glossy effects with low memory overhead. For dynamic, varying lighting conditions, we introduce local light probes capturing scene radiance, coupled with an across-probe radiance-sharing strategy for efficient probe generation. We implemented our method in a real-time rendering engine, combining compute shaders and CUDA-based neural networks. Experimental results demonstrate that our method achieves real-time performance of less than 10 ms to render a frame and significantly improved rendering quality compared to baseline methods.},
  archive      = {J_TVCG},
  author       = {Yijie Deng and Lei Han and Lu Fang},
  doi          = {10.1109/TVCG.2025.3596146},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TransGI: Real-time dynamic global illumination with object-centric neural transfer model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GECO: Fast generative image-to-3D within one SECOnd. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3602405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in single-image 3D generation have produced two main categories of methods: reconstruction-based and generative methods. Reconstruction-based methods are efficient but lack uncertainty handling, leading to blurry artifacts in unseen regions. Generative approaches that based on score distillation [47], [71] are slow due to scene-specific optimization. Other methods, like InstantMesh [76], use a two-stage process - generating multi-view images with a diffusion model and then reconstructing 3D - which is inefficient due to multiple denoising steps of the diffusion model. To overcome these limitations, we introduce GECO, a feed-forward method for fast and high-quality single-image-to-3D generation within one second on a single GPU. Our approach resolves uncertainty and inefficiency issues through a two-stage distillation process. In the first stage, we distill a multi-step diffusion model [56] into a one-step model using score distillation for single-image-to-multi-view synthesis. To mitigate the synthesis quality degradation caused by the one-step model, we introduce a second distillation stage to learn to predict high-quality 3D from imperfect multi-view generated images by performing distillation directly on 3D representations. Experiments demonstrate that GECO offers significant speed improvements and comparable reconstruction quality compared to prior two-stage methods. Code: https://cwchenwang.github.io/geco.},
  archive      = {J_TVCG},
  author       = {Chen Wang and Jiatao Gu and Xiaoxiao Long and Yuan Liu and Lingjie Liu},
  doi          = {10.1109/TVCG.2025.3602405},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GECO: Fast generative image-to-3D within one SECOnd},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMATO: Bridging local and global structures for reliable visual analytics with dimensionality reduction. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3602735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality reduction (DR) techniques cannot preserve all the structural characteristics of the original data. Therefore, DR techniques focus on preserving either local neighborhood structures (local techniques) or global structures such as pairwise distances between points (global techniques). However, both approaches can mislead analysts to erroneous conclusions about the overall arrangement of manifolds in HD data. For example, local techniques may exaggerate the compactness of individual manifolds, while global techniques may fail to separate clusters that are well-separated in the original space. In this research, we provide a deeper insight into Uniform Manifold Approximation with Two-phase Optimization (UMATO), a DR technique that addresses this problem by effectively capturing local and global structures. UMATO achieves this by dividing the optimization process of UMAP into two phases. In the first phase, it constructs a skeletal layout using representative points, and in the second phase, it projects the remaining points while preserving the regional characteristics. Quantitative experiments validate that UMATO outperforms widely used DR techniques, including UMAP, in terms of global structure preservation, with a slight loss in local structure. We also confirm that UMATO outperforms baseline techniques in terms of scalability and stability against initialization and subsampling, making it more effective for reliable HD data analysis. Finally, we present a case study and a qualitative demonstration that highlight UMATO's effectiveness in generating faithful projections, enhancing the overall reliability of visual analytics using DR.},
  archive      = {J_TVCG},
  author       = {Hyeon Jeon and Kwon Ko and Soohyun Lee and Jake Hyun and Taehyun Yang and Gyehun Go and Jaemin Jo and Jinwook Seo},
  doi          = {10.1109/TVCG.2025.3602735},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UMATO: Bridging local and global structures for reliable visual analytics with dimensionality reduction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CausalChat: Interactive causal model development and refinement using large language models. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3602448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal networks are widely used in many fields to model the complex relationships between variables. A recent approach has sought to construct causal networks by leveraging the wisdom of crowds through the collective participation of humans. While this can yield detailed causal networks that model the underlying phenomena quite well, it requires a large number of individuals with domain understanding. We adopt a different approach: leveraging the causal knowledge that large language models, such as OpenAI's GPT-4, have learned by ingesting massive amounts of literature. Within a dedicated visual analytics interface, called CausalChat, users explore single variables or variable pairs recursively to identify causal relations, latent variables, confounders, and mediators, constructing detailed causal networks through conversation. Each probing interaction is translated into a tailored GPT-4 prompt and the response is conveyed through visual representations which are linked to the generated text for explanations. We demonstrate the functionality of CausalChat across diverse data contexts and conduct user studies involving both domain experts and laypersons.},
  archive      = {J_TVCG},
  author       = {Yanming Zhang and Akshith Kota and Eric Papenhausen and Klaus Mueller},
  doi          = {10.1109/TVCG.2025.3602448},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CausalChat: Interactive causal model development and refinement using large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChronoDeck: A visual analytics approach for hierarchical time series analysis. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3602273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical time series data comprises a collection of time series aggregated at multiple levels based on categorical, geographical, or physical constraints, the analysis of which aids analysts across various domains like retail, finance, and energy, in gaining valuable insights and making informed decisions. However, existing interactive exploratory analysis approaches for hierarchical time series data fall short in analyzing time series across different aggregation levels and supporting more complex analytical tasks beyond common ones like summarize and compare. These limitations motivate us to develop a new visual analytics approach. We first generalize a taxonomy to delineate various tasks in hierarchical time series analysis, derived from literature survey and expert interviews. Based on this taxonomy, we develop ChronoDeck, an interactive system that incorporates a multi-column hierarchical time series visualization for implementing various analytical tasks and distilling insights from the data. ChronoDeck visualizes each aggregation level of hierarchical time series with a combination of coordinated dimensionality reduction and small multiples visualizations, alongside interactions including highlight, align, filter, and select, assisting users in the visualization, comparison, and transformation of hierarchical time series, as well as identifying the entities of interest. The effectiveness of ChronoDeck is demonstrated by case studies on three real-world datasets and expert interviews.},
  archive      = {J_TVCG},
  author       = {Lingyu Meng and Shuhan Liu and Keyi Yang and Jiabin Xu and Zikun Deng and Di Weng and Yingcai Wu},
  doi          = {10.1109/TVCG.2025.3602273},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChronoDeck: A visual analytics approach for hierarchical time series analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and efficient preservation of high-order continuous geometric validity. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3603025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to robustly and efficiently compute the maximum allowable step sizes so that the 3D high-order finite elements continuously preserve geometric validity when moving along the given directions with positive step sizes smaller than the computed ones. We transform the problem of finding the maximum allowable step sizes to one of solving roots of cubic polynomials. To use interval arithmetic to avoid numerical issues in cubic equation solving, we completely enumerate the roots of cubic polynomials and apply the interval version of the Newton-Raphson iteration. The effectiveness of our algorithm is demonstrated through extensive testing. Compared to the state-of-the-art method, our algorithm achieves higher efficiency.},
  archive      = {J_TVCG},
  author       = {Wei Du and Shibo Liu and Jia-Peng Guo and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1109/TVCG.2025.3603025},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust and efficient preservation of high-order continuous geometric validity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of the sense of embodiment in virtual co-embodiment rehabilitation: A structural equation modeling approach. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3589111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sense of embodiment (SoE) refers to the participant's perception of a virtual avatar as an extension of their own body, involving both physical and functional aspects. Research has highlighted the importance of SoE for the effectiveness of virtual rehabilitation. Virtual co-embodiment technology, an emerging virtual reality (VR) application, has the potential to enhance users' engagement and SoE, which demonstrates significant promise for motor rehabilitation. However, the exploration of factors influencing embodiment in virtual co-embodiment is still limited, particularly regarding both internal and external factors, which constrains its rehabilitation applications. This study investigates factors influencing SoE changes in Virtual Co-embodiment Rehabilitation by developing a theoretical model, based on 859 valid trials collected from 40 healthy participants, and analyzing the data using Structural Equation Modeling (SEM) and Partial Least Squares Structural Equation Modeling (PLS-SEM). The results suggest that both “visual consistency” (external factor) and “individual sensitivity” (internal factor) may influence changes in SoE, with “visual consistency” appearing to have a predominant effect. These findings contribute to understanding changes in SoE in virtual co-embodiment and offer insights for optimizing the technology in rehabilitation.},
  archive      = {J_TVCG},
  author       = {Chengjie Zhang and Suiran Yu},
  doi          = {10.1109/TVCG.2025.3589111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis of the sense of embodiment in virtual co-embodiment rehabilitation: A structural equation modeling approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From air to wear: Personalized 3D digital fashion with AR/VR immersive 3D sketching. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3593504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of immersive consumer electronics, such as AR/VR headsets and smart devices, people increasingly seek ways to express their identity through virtual fashion. However, existing 3D garment design tools remain inaccessible to everyday users due to steep technical barriers and limited data. In this work, we introduce a 3D sketch-driven 3D garment generation framework that empowers ordinary users — even those without design experience — to create high-quality digital clothing through simple 3D sketches in AR/VR environments. By combining a conditional diffusion model, a sketch encoder trained in a shared latent space, and an adaptive curriculum learning strategy, our system interprets imprecise, free-hand input and produces realistic, personalized garments. To address the scarcity of training data, we also introduce KO3DClothes, a new dataset of paired 3D garments and user-created sketches. Extensive experiments and user studies confirm that our method significantly outperforms existing baselines in both fidelity and usability, demonstrating its promise for democratized fashion design on next-generation consumer platforms.},
  archive      = {J_TVCG},
  author       = {Ying Zang and Yuanqi Hu and Xinyu Chen and Suhui Wang and Yuxia Xu and Chunan Yu and Lanyun Zhu and Deyi Ji and Xin Xu and Tianrun Chen},
  doi          = {10.1109/TVCG.2025.3593504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From air to wear: Personalized 3D digital fashion with AR/VR immersive 3D sketching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAR: Skeleton-aware text-based 4D avatar generation with in-network motion retargeting. <em>TVCG</em>, 1-13. (<a href='https://doi.org/10.1109/TVCG.2025.3559988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently animates them with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for na¨ıve Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars leads to translation artifacts and misalignment. To address these issues, we propose Skeletonaware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multiview and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies show the contributions of each component in STAR. The source code and demos are available at: https://star-avatar.github.io.},
  archive      = {J_TVCG},
  author       = {Zenghao Chai and Chen Tang and Yongkang Wong and Mohan Kankanhalli},
  doi          = {10.1109/TVCG.2025.3559988},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STAR: Skeleton-aware text-based 4D avatar generation with in-network motion retargeting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualizing causality in mixed reality for manual task learning: A study. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3542949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality (MR) is gaining prominence in manual task skill learning due to its in-situ, embodied, and immersive experience. To teach manual tasks, current methodologies break the task into hierarchies (tasks into subtasks) and visualize not only the current subtasks but also the future ones that are causally related. We investigate the impact of visualizing causality within an MR framework on manual task skill learning. We conducted a user study with 48 participants, experimenting with how presenting tasks in hierarchical causality levels (no causality, event-level, interaction-level, and gesture-level causality) affects user comprehension and performance in a complex assembly task. The research finds that displaying all causality levels enhances user understanding and task execution, with a compromise of learning time. Based on the results, we further provide design recommendations and in-depth discussions for future manual task learning systems.},
  archive      = {J_TVCG},
  author       = {Rahul Jain and Jingyu Shi and Andrew Benton and Moiz Rasheed and Hyungjun Doh and Subramanian Chidambaram and Karthik Ramani},
  doi          = {10.1109/TVCG.2025.3542949},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing causality in mixed reality for manual task learning: A study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-CAD: Controllable diffusion model for generating computer-aided design models. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3535797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative methods for creating computer-aided design (CAD) models have gained significant attention over the past two years. However, existing methods lack fine-grained control over the generated CAD models, making it difficult to manage details such as model dimensions and the relative structure of components. To address these limitations, this study introduces Diffusion-CAD, a diffusion-based generative approach that outputs CAD construction sequences. Diffusion-CAD iteratively denoises Gaussian noise into continuous CAD vectors, which are then transformed into discrete CAD sequences. We designed classifier-free and classifier-guided methods to control the distribution of Gaussian noise, CAD sequences, and noisy CAD vectors separately, thereby achieving a variety of fine-grained control tasks. Extensive experiments demonstrated the superior performance and novel capabilities of the proposed method for conditional generation tasks.},
  archive      = {J_TVCG},
  author       = {Aijia Zhang and Weiqiang Jia and Qiang Zou and Yixiong Feng and Xiaoxiang Wei and Ye Zhang},
  doi          = {10.1109/TVCG.2025.3535797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Diffusion-CAD: Controllable diffusion model for generating computer-aided design models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory vorticity - Computation and visualization of rotational trajectory behavior in an objective way. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2024.3421555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory data consisting of a low number of smooth parametric curves are standard data sets in visualization. For a visual analysis, not only the behavior of the individual trajectories is of interest but also the relation of the trajectories to each other. Moving objects represented by the trajectories may rotate around each other or around a moving center. We present an approach to compute and visually analyze such rotational behavior in an objective way. We introduce trajectory vorticity (TRV), a measure of rotational behavior of a low number of trajectories. We show that it is objective and that it can be introduced in two independent ways: by approaches for unsteadiness minimization and by considering the relative spin tensor. We compare TRV against single-trajectory methods and apply it to a number of constructed and real trajectory data sets, including drifting buoys in the Atlantic, midge swarm tracking data, pedestrian tracking data, pigeon flocks, and a simulated vortex street},
  archive      = {J_TVCG},
  author       = {Anke Friederici and Holger Theisel and Tobias Gunther},
  doi          = {10.1109/TVCG.2024.3421555},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Trajectory vorticity - Computation and visualization of rotational trajectory behavior in an objective way},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMetaTown: A metaverse system with multiple interactive functions based on virtual reality. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2024.3372055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to pioneer the development of a real-time interactive and immersive Metaverse Human-Computer Interaction (HCI) system leveraging Virtual Reality (VR). The system incorporates a three-dimensional (3D) face reconstruction method, grounded in weakly supervised learning, to enhance player-player interactions within the Metaverse. The proposed method, two-dimensional (2D) face images, are effectively employed in a 2D Self-Supervised Learning (2DASL) approach, significantly optimizing 3D model learning outcomes and improving the quality of 3D face alignment in HCI systems. The work outlines the functional modules of the system, encompassing user interactions such as hugs and handshakes and communication through voice and text via blockchain. Solutions for managing multiple simultaneous online users are presented. Performance evaluation of the HCI system in a 3D reconstruction scene indicates that the 2DASL face reconstruction method achieves noteworthy results, enhancing the system's interaction capabilities by aiding 3D face modeling through 2D face images. The experimental system achieves a maximum processing speed of 18 frames of image data on a personal computer, meeting real-time processing requirements. User feedback regarding social acceptance, action interaction usability, emotions, and satisfaction with the VR interactive system reveals consistently high scores. The designed VR HCI system exhibits outstanding performance across diverse applications.},
  archive      = {J_TVCG},
  author       = {Zhihan Lyu and Mikael Fridenfalk},
  doi          = {10.1109/TVCG.2024.3372055},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IMetaTown: A metaverse system with multiple interactive functions based on virtual reality},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Viewpoint recommendation for point cloud labeling through interaction cost modeling. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2024.3376951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of 3D point clouds is important for many applications, such as autonomous driving. To train semantic segmentation models, labeled point cloud segmentation datasets are essential. Meanwhile, point cloud labeling is time-consuming for annotators, which typically involves tuning the camera viewpoint and selecting points with a lasso tool. To reduce the time cost of point cloud labeling, we propose a viewpoint recommendation approach to reduce annotators' labeling time costs. We adapt Fitts' law to model the time cost of lasso selection in point clouds. Using the modeled time cost, the viewpoint that minimizes the lasso selection time cost is recommended to the annotator. We build a data labeling system for semantic segmentation of 3D point clouds that integrates our viewpoint recommendation approach. The system enables users to navigate to recommended viewpoints for efficient annotation. Through a user study, we observed that our approach effectively reduced the data labeling time cost. We also qualitatively compare our approach with previous viewpoint selection approaches on different datasets.},
  archive      = {J_TVCG},
  author       = {Yu Zhang and Xinyi Zhao and Chongke Bi and Siming Chen},
  doi          = {10.1109/TVCG.2024.3376951},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Viewpoint recommendation for point cloud labeling through interaction cost modeling},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subspace-map: Interactive visual analysis for subspace data with a map metaphor. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2024.3368094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace analysis of high-dimensional data is extremely challenging due to the huge exploration space. We propose Subspace-Map, a novel approach with a map metaphor for interactive exploration of various subspaces. We utilize a subspace search algorithm to identify a moderate number of potentially valuable subspaces, each visualized as a city on the map. Similar cities are clustered into provinces and countries, highlighting common data and dimensional patterns that can guide users in constructing desired subspaces. With the map, users can grasp an overview of the exploration space and explore different subspaces via recommended tour routes in more detail. We demonstrate the effectiveness of Subspace-Map through cases with real-world data, experiments with user feedback, and a comparison with state-of-the-art subspace data visualizations.},
  archive      = {J_TVCG},
  author       = {Jincheng Li and Chufan Lai and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2024.3368094},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Subspace-map: Interactive visual analysis for subspace data with a map metaphor},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RankFIRST: Visual analysis for factor investment by ranking stock timeseries. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2022.3209414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of quantitative investment, factor-based investing models are widely adopted in the construction of stock portfolios. These models explain the performance of individual stocks by a set of financial factors, e.g., market beta and company size. In industry, open investment platforms allow the online building of factor-based models, yet set a high bar on the engineering expertise of end-users. State-of-the-art visualization systems integrate the whole factor investing pipeline, but do not directly address domain users' core requests on ranking factors and stocks for portfolio construction. The current model lacks explainability, which downgrades its credibility with stock investors. To fill the gap in modeling, ranking, and visualizing stock time series for factor investment, we designed and implemented a visual analytics system, namely RankFIRST. The system offers built-in support for an established factor collection and a cross-sectional regression model viable for human interpretation. A hierarchical slope graph design is introduced according to the desired characteristics of good factors for stock investment. A novel firework chart is also invented extending the well-known candlestick chart for stock time series. We evaluated the system on the full-scale Chinese stock market data in the recent 30 years. Case studies and controlled user evaluation demonstrate the superiority of our system on factor investing, in comparison to both passive investing on stock indices and existing stock market visual analytics tools.},
  archive      = {J_TVCG},
  author       = {Huijie Guo and Meijun Liu and Bowen Yang and Ye Sun and Huamin Qu and Lei Shi},
  doi          = {10.1109/TVCG.2022.3209414},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RankFIRST: Visual analysis for factor investment by ranking stock timeseries},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LoopGrafter: Visual support for the grafting workflow of protein loops. <em>TVCG</em>, 1. (<a href='https://doi.org/10.1109/TVCG.2021.3114755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of understanding and redesigning the function of proteins in modern biochemistry, protein engineers are increasingly focusing on the exploration of regions in proteins called loops. Analyzing various characteristics of these regions helps the experts to design the transfer of the desired function from one protein to another. This process is denoted as loop grafting. As this process requires extensive manual treatment and currently there is no proper visual support for it, we designed LoopGrafter: a web-based tool that provides experts with visual support through all the loop grafting pipeline steps. The tool is logically divided into several phases, starting with the definition of two input proteins and ending with a set of grafted proteins. Each phase is supported by a specific set of abstracted 2D visual representations of loaded proteins and their loops that are interactively linked with the 3D view onto proteins. By sequentially passing through the individual phases, the user is shaping the list of loops that are potential candidates for loop grafting. In the end, the actual in-silico insertion of the loop candidates from one protein to the other is performed and the results are visually presented to the user. In this way, the fully computational rational design of proteins and their loops results in newly designed protein structures that can be further assembled and tested through in-vitro experiments. LoopGrafter was designed in tight collaboration with protein engineers, and its final appearance reflects many testing iterations. We showcase the contribution of LoopGrafter on a real case scenario and provide the readers with the experts' feedback, confirming the usefulness of our tool.},
  archive      = {J_TVCG},
  author       = {Filip Opaleny and Pavol Ulbrich and Joan Planas-Iglesias and Jan Byska and Gaspar P. Pinto and David Bednar and Katarina FurmanovA and Barbora KozlikovA},
  doi          = {10.1109/TVCG.2021.3114755},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LoopGrafter: Visual support for the grafting workflow of protein loops},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
