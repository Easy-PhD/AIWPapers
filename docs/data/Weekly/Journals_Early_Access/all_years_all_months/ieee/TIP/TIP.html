<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 54</h2>
<ul>
<li><details>
<summary>
(2025). Action quality assessment via hierarchical pose-guided multi-stage contrastive regression. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA), which aims at the automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains different durations. However, existing methods focus on segmenting the video into fixed frames, which disrupts the temporal continuity of sub-actions resulting in unavoidable prediction errors. To address these challenges, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. Firstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture fine-grained spatio-temporal visual and skeletal features. Compared to mask or auxiliary visual features, skeletal features provide a more accurate representation during athletic movements. Then, a procedure segmentation network is introduced to separate different sub-actions and obtain segmented features. Afterwards, the segmented visual and skeletal features are both fed into a multi-modal fusion module as physics structural priors, to guide the model in learning refined activity similarities and variances. Finally, a multi-stage contrastive learning regression approach is employed to learn discriminative representations and output prediction results. In addition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the current low-quality human pose labels. In experiments, the results on FineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority of our proposed approach. Our source code and dataset are available at https://github.com/Lumos0507/HP-MCoRe.},
  archive      = {J_TIP},
  author       = {Mengshi Qi and Hao Ye and Jiaxuan Peng and Huadong Ma},
  doi          = {10.1109/TIP.2025.3613952},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action quality assessment via hierarchical pose-guided multi-stage contrastive regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGHL: Anchor-guided point cloud registration network with hybrid local feature perception. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration, which estimates a rigid transformation matrix between two point clouds, is a fundamental process in numerous applications. While existing detector-free techniques present exceptional performance, they overlook the extraction of hybrid local features that capture correlations between points and their neighbours, thereby limiting the quality of point cloud recognition. Moreover, these approaches typically treat point clouds as sequential data and employ the transformer to integrate global context from all points, which inevitably introduces interference from irrelevant regions, hence affecting the registration accuracy. In this work, we propose a novel detector-free approach AGHL to address these challenges. For the first issue, AGHL introduces a hybrid local feature perception module that designs two parallel branches to concurrently extract low-level and high-level local features, which effectively encode the correlations between each point and its neighborhood points in both Euclidean space and high-dimensional feature space. For the second issue, AGHL develops an anchor-guided cross attention that adheres to the local geometric consistency to constrain the network’s attention on reliable anchors, thereby effectively suppressing interference from irrelevant regions. Benefiting from these techniques, AGHL achieves impressive point cloud registration accuracy across all synthetic, indoor, and outdoor datasets. Furthermore, we build an experimental platform and conduct a real-world robot localization experiment, with results showing the strong generalization ability of AGHL.},
  archive      = {J_TIP},
  author       = {Kun Dai and Tao Xie and Zhiqiang Jiang and Ke Wang and Ruifeng Li and Lijun Zhao and Chuqing Cao},
  doi          = {10.1109/TIP.2025.3613987},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AGHL: Anchor-guided point cloud registration network with hybrid local feature perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BITS: Bit-extendable incremental hashing in open environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an effective technique for large-scale image retrieval. However, traditional hashing models typically follow a closed-set assumption, which fails to satisfy the practicality of real-world tasks. In this paper, we explore a meaningful yet overlooked question: is there a hashing paradigm that not only supports rehearsal-free online incremental coding for single-pass data streams but also adapts to potentially expanding concept spaces in open environments? Instead of presetting fixed bit lengths, we suggest adjusting the bit length dynamically based on the number of encountered categories, meanwhile enabling bit extension of existing hash codes to match the adaptive code lengths without knowledge forgetting. Therefore, we propose a Bit-extendable IncremenTal haShing (BITS) method for image retrieval in open environments. Specifically, we identify a blurry incremental setup to better simulate realistic scenarios, revisiting the widely-used data-incremental and class-incremental settings. With this challenging setup, a three-phase framework is designed to efficiently perform incremental hashing, which jointly solves online continual coding and bit extension with adaptive code lengths. Through the well-designed hashing paradigm, BITS achieves comparable performance to offline hashing methods while significantly saving computational resources. Comprehensive experiments on six benchmarks demonstrate the superiority of our BITS in dynamic scenarios. The source code is available at https://github.com/yxinwang/BITS.},
  archive      = {J_TIP},
  author       = {Yongxin Wang and Zhen-Duo Chen and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2025.3613924},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BITS: Bit-extendable incremental hashing in open environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust reversible watermarking with invisible distortion against VAE watermark removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Moment-based Robust Reversible Watermarking (OM-RRW) is crucial for intellectual property protection, providing the dual benefits of robustness and reversibility. However, OM-RRW embeds watermarks into visually sensitive global low-frequency features, which easily leads to ring-like distortions that expose watermark locations, making them vulnerable to removal through image inpainting. To address this issue, this paper makes the first attempt to introduce an innovative strategy to eliminate these visible distortions, thereby overcoming OM-RRW’s inherent limitations. The strategy innovates on two fronts: first, it customizes varying embedding step sizes based on the stability differences of moment values to minimize distortion; second, it designs a texture-aware adaptive basis function fine-tuning strategy. This strategy adjusts the representation capability of the basis functions in different regions based on the human eye’s sensitivity to various texture areas, helping to avoid visible ring-like distortions. The performance of the proposed method is evaluated using Polar Harmonic Transform (PHT) moments, comprising three moments that exhibit remarkable performance in existing OM-RRW methods. Extensive experiments show that the proposed method can embed 128-bit watermarks with no visible distortions while minimizing the loss of robustness. In addition, this paper finds that OM-RRW demonstrates satisfactory robustness against VAE watermark removal attacks.},
  archive      = {J_TIP},
  author       = {Bobiao Guo and Ping Ping and Fan Liu and Feng Xu},
  doi          = {10.1109/TIP.2025.3613958},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust reversible watermarking with invisible distortion against VAE watermark removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of blood vessels is essential for various clinical assessments and postoperative analyses. However, the inherent challenges of vascular imaging—such as sparsity, fine granularity, low contrast, data distribution variability, and the critical need for preserving topological integrity—make generalized vessel segmentation particularly complex. While specialized segmentation methods have been developed for specific anatomical regions, their over-reliance on tailored models hinders broader applicability and generalization. General-purpose segmentation models introduced in medical imaging often fail to address critical vascular characteristics, including the connectivity of segmentation results. In this study, we propose OVS-Net, an optimized vessel segmentation framework designed to generalize across diverse vessel structures and imaging modalities. It introduces a dual-branch architecture design for improving small vessel segmentation and a morphology-aware correction module to preserve vascular topology and connectivity. We compiled a comprehensive multi-modality dataset from 17 sources to train and benchmark the proposed OVS-Net against 6 SAM-based methods and 17 expert models under various conditions. The results demonstrate that our approach achieves superior segmentation accuracy, generalization, and a 34.6% improvement in connectivity, underscoring its potential for clinical applications. The code and dataset will be released at github.com.},
  archive      = {J_TIP},
  author       = {Dongning Song and Weijian Huang and Jiarun Liu and Md Jahidul Islam and Hao Yang and Shuqiang Wang and Hairong Zheng and Shanshan Wang},
  doi          = {10.1109/TIP.2025.3607583},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space normalizing flow for unsupervised video anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3614006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional reconstruction-based video anomaly detection (VAD) methods implicitly model normality in latent spaces, which is limited by the generalization ability of latent features. Normalizing Flow (NF)-based methods have been introduced to address this issue, as they explicitly model the distribution of input data and achieve significant performance in VAD. However, existing NF-based methods are confined to Euclidean space, limiting their ability to model action hierarchies. While effective at capturing local joint dynamics and short-term temporal variations, they fail to encode kinematic dependencies and long-term pose evolution, ultimately struggling to discern ambiguous anomalies that deviate minimally from normal motion. In contrast, hyperbolic representation learning, with its ability to model hierarchical and complex relationships among actions, offers a promising solution to enhance the discriminative power between similar skeletal actions. Motivated by this, we propose a novel Dual-Space Normalizing Flow (DSNF) method. Specifically, we design a Dual-Space Parallel Graph Convolutional Network (DSPGCN) that synergistically integrates the strengths of both Euclidean and hyperbolic geometries to simultaneously capture local detail features of poses and intrinsic hierarchical relationships of actions. To enhance the model’s focus on discriminative features, we design an Adaptive Weighted Approximation Mass (AWAM) loss that dynamically adjusts weights to impose stronger constraints on regions with low discriminability in the dual space, encouraging the model to focus more on key discriminative features in hyperbolic space that reflect complex relationships between actions. Extensive experiments on public datasets demonstrate the effectiveness and robustness of our method in various VAD scenarios.},
  archive      = {J_TIP},
  author       = {Jiaxu Leng and Yumeng Zhang and Mingpi Tan and Changjiang Kuang and Zhanjie Wu and Ji Gan and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3614006},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space normalizing flow for unsupervised video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring vision-based active 3D object detection by informativeness characterization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based 3D object detection (3DOD) gains lots of attention due to its low cost for deployment compared to Lidar-based tasks, while it suffers from labor-expensive data annotations. At the same time, active learning (AL) has shown great potential in reducing annotation costs in related tasks, which can maximize model performance within very limited labeled data. In this paper, we explore active learning for vision-based 3DOD for the first time. Inspired by the entropy analysis, we involve three concerns to characterize the sample informativeness: sample diversity in input space, feature informativeness in BEV space, and result distribution in prediction space. Based on these concerns, we propose a novel AL framework named HMAD, which utilizes Height Modeling and Adaptive Diversity-based sampling for comprehensive informativeness characterization. In HMAD, we first propose a novel height-guided adversarial module in BEV space, which measures the informativeness of height modeling for 2D-to-3D mapping in an adversarial manner. Furthermore, Budget-aware SpatioTemporal diversity Sampling (BSTS) and Class Balance Sampling (CBS) are proposed to adaptively measure the sample informativeness in input and prediction space, respectively. Finally, the three components are integrated into a two-stage sampling strategy, with which the most informative samples can be selected and annotated for the next iteration. Experiments evidence that HMAD achieves comparable performances by only using 50% annotated training data, and can generalize well on different conditions.},
  archive      = {J_TIP},
  author       = {Ruixiang Li and Yiming Wu and Yehao Lu and Xuewei Li and Xian Wang and Xiubo Liang and Xi Li},
  doi          = {10.1109/TIP.2025.3613927},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring vision-based active 3D object detection by informativeness characterization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised discovery of cross-lingual shared knowledge for continual text recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3614773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental multilingual text recognition (IMLTR) aims to advance continual learning by retaining knowledge from previously learned languages while adapting to new ones. Existing methods typically perform under a constrained assumption that each text instance originates from a specific single-language domain. However, this assumption is inaccurate in multilingual scenarios, as it overlooks the inherent cross-lingual knowledge, i.e., the incremental sharing problem. To address this issue, we propose a novel self-supervised cross-lingual knowledge discovery framework, CrossKnow, tailored for IMLTR tasks. Specifically, an innovative shared knowledge discovery strategy is developed to identify potential shared knowledge by leveraging prediction consistency across multiple recognizers, thus eliminating the reliance on language labels of all characters. Building upon this shared knowledge, we further design a multi-granularity, multi-task language domain discriminator to capture dependency relationships among incremental languages, which could adequately guide the hierarchical sequence decoding. By mining shared knowledge, CrossKnow can not only mitigate the forgetting of old knowledge but also efficiently achieve cross-lingual knowledge transfer, thereby promoting the continual learning of incremental multilingual text recognition models. Experiments on two widely used datasets, MLT17 and MLT19, demonstrate the superiority of CrossKnow. Compared to methods that leverage additional language supervision of characters, CrossKnow achieves competitive performance while eliminating storage overhead and improving computation efficiency.},
  archive      = {J_TIP},
  author       = {Xiao-Qian Liu and Zhen-Duo Chen and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2025.3614773},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised discovery of cross-lingual shared knowledge for continual text recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot image recognition via learning dual prototype accordance across meta-domains. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen categories. However, existing methods often struggle with the persistent semantic gap caused by limited semantic descriptors and rigid visual feature modeling. In particular, modeling pre-defined class-level attribute descriptions as ground truth hinders effective semantic-to-visual alignment to some extent. To mitigate these issues, we propose the Bilateral-guided Prototype Refinement Network(BPRN), a novel ZSL framework designed to refine dual prototypes across meta-domains of varying scales. Specifically, we first disentangle the relationships among class-level semantics and use them to generate corresponding pseudo-visual prototypes. Then, by leveraging distribution information across dual prototypes in different meta-domains, BPRN achieves bidirectional calibration between visual-to-semantic and semantic-to-visual modalities. Finally, a synthesized class-level representation derived from the refined dual prototypes is employed for inference, instead of relying on a single prototype. Extensive experiments conducted on five widely-used ZSL benchmark datasets demonstrate that BPRN consistently achieves competitive or even superior performance. Specifically, in the GZSL scenario, BPRN shows improvements of 2.1%, 7.3%, 6.1%, and 4.8% on AWA1, AWA2, SUN, and aPY, respectively, compared to existing embedding-based ZSL methods. Ablation studies and visualization analyses further validate the effectiveness of the proposed components.},
  archive      = {J_TIP},
  author       = {Bocheng Ren and Yuanyuan Yi and Qingchen Zhang and Debin Liu},
  doi          = {10.1109/TIP.2025.3607588},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image recognition via learning dual prototype accordance across meta-domains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient open vocabulary 3D scene understanding based on semantic probability. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D scene understanding methods heavily depend on 3D annotation and training, which allow for the identification of seen classes but struggle to recognize unseen classes. In this paper, we leverage the open vocabulary inference capabilities of pre-trained models, enabling the encoding of open vocabulary concepts. However, unlike existing open vocabulary 3D scene understanding methods, we propose a framework based on semantic probability. This innovation significantly reduces computational cost and is compatible with state-of-the-art two-stage 2D pre-trained models. Specifically, we align the text features from the CLIP model with the pixel features from the 2D pre-trained models, inferring semantic probability of image pixels based on similarity and projecting it onto 3D points. Subsequently, we introduce a point cloud pairs semantic fusion method to merge the point clouds, reducing the semantic probability of erroneous 3D points. Based on probability scores, we achieve 3D semantic segmentation on open vocabularies without any supervision or training. In addition, the semantic probability of 3D points can serve as pseudo-labels for 3D distillation, and the geometric features of the 3D scene can be exploited to improve the segmentation performance. Experimental results demonstrate that the proposed method exhibits competitive performance on publicly available benchmark datasets, including ScanNet, Matterport3D, and nuScenes.},
  archive      = {J_TIP},
  author       = {Lingfeng Shen and Xiaoyao Wei and Gang Pan and Qian Zheng and Yanlong Cao},
  doi          = {10.1109/TIP.2025.3607643},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cost-efficient open vocabulary 3D scene understanding based on semantic probability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a unified model for surface anomaly detection remains challenging due to significant variations across product categories. Recent feature editing methods, as a branch of image reconstruction, mitigate the over-generalization of auto-encoders that leads to accurate anomaly reconstruction. However, these methods are only suited for texture-category products and have significant limitations in being generalized to other categories. In this article, we propose a multi-category anomaly editing network with a dual-branch training approach: one branch processes defect-free images (normal branch), while the other handles synthetic anomaly images (anomaly branch). Specifically, the paired samples are first fed into the multi-category anomaly feature editing based auto-encoder (MCAFE-AE) to perform image reconstruction and inpainting. In the normal branch, we propose a dual-entropy constrained deep embedded clustering module (DEC-DECM) to promote a more compact and orderly distribution of normal latent features, while avoiding trivial clustering solutions. Based on the clustering results, we further design a patch-based adaptive thresholding (PAT) strategy to adaptively calculate the threshold representing the central boundary of the cluster center for each local patch, thereby enabling the model to detect anomalies. Then, in the anomaly branch, we propose a multi-category anomaly feature editing module (MCAFEM) to identify anomalies in synthetic images and apply a category-oriented feature editing strategy to transform detected anomaly features into normal ones, thereby suppressing the reconstruction of anomalies. After completing the image reconstruction and inpainting, the input images from both branches and their respective output images are concatenated and fed into the correlation exploration and voxel-level attention based prediction network (CEVA-Net) for anomaly segmentation. The network is integrated with our proposed correlation-dependency exploration and voxel-level attention refinement module (CDE-VARM) and generates precise anomaly maps under the guidance of the bidirectional-path feature fusion (BPFF) and deep supervised learning (DSL). Extensive experiments on three datasets show that our method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruifan Zhang and Hai-Miao Hu},
  doi          = {10.1109/TIP.2025.3607638},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral texture metrology based on distance measures in an information-theoretic framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work sought to instil metrology in existing hyperspectral texture feature extraction methods. Specifically, we propose distance-based expressions of graylevel cooccurrence matrix (GLCM), local binary pattern (LBP), and Gabor filtering directly computable for hyperspectral images without any pre- or post-processing. At the core of our proposition is Radical of Extended Mean Information for Discrimination (REID), a novel spectral distance with information-theoretic roots. Respecting the physics of spectrum as continuous function of wavelengths, REID is mathematically decomposable into spectral direction and spectral magnitude distances. The resulted feature calculations are fullband (utilizing all wavelengths), yet lightweight and fully interpretable. A similarity measure based on information theory is also justified. Their efficiency is demonstrated in the context of texture classification, content-based image retrieval, and cancer detection in which they consistently outperform existing computations based on dimensionally reduced space using PCA, ICA, and NMF. The propositions could be potentially integrated into machine/deep learning systems towards explainable AI (XAI).},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Jie Chen and Susanto Rahardja},
  doi          = {10.1109/TIP.2025.3608667},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on distance measures in an information-theoretic framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised domain adaptation methods rely on explicitly or implicitly aligning the features of source and target domains to construct a domain-invariant space, often using entropy minimization to reduce uncertainty and confusion. However, this approach faces two challenges: 1) Explicit alignment reduces discriminability, while implicit alignment risks pseudo-label noise, making it hard to balance structure preservation and alignment. 2) Sole reliance on entropy minimization can lead to trivial solutions in UDA, where all samples collapse into a single class. To address these issues, we propose Dual-Space Topological Isomorphism and Maximization of Predictive Diversity (DTI-MPD). Topological isomorphism is a continuous, bijective mapping that preserves the topological properties of two spaces, ensuring the global structure and relationships of data remain intact during alignment. Our method aligns source and target domain data in two independent spaces while balancing the effects of entropy minimization through predictive diversity maximization. The core of dual-space topological isomorphism lies in establishing a reversible correspondence between the source and target domains, avoiding information loss during alignment and preserving the global structural and topological characteristics of the data. Meanwhile, predictive diversity maximization mitigates the class collapse caused by entropy minimization, ensuring a more balanced predictive distribution across categories. This approach effectively overcomes the aforementioned issues, enabling better adaptation to new data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets, validating its effectiveness.},
  archive      = {J_TIP},
  author       = {Mengru Wang and Jinglei Liu},
  doi          = {10.1109/TIP.2025.3608670},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pull pole points to text contour by magnetism: A real-time scene text detector. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text reading plays a crucial role in scene understanding. As its precondition task, scene text detection has garnered increasing interest from researchers. Segmentation-based text detection methods have gained prominence due to their adaptable pixel-level predictions. Many existing methods predict the shrink mask and utilize the Vatti clipping algorithm to reconstruct text contours. However, the shrink mask only focuses on the global geometry feature and shrinks the same distance everywhere, which neglects local contour information and disrupts the instance shape feature. In addition, the post-processing based on the Vatti clipping algorithm heavily relies on the predictions and is relatively complex, causing suboptimal performance in both detection accuracy and efficiency. To address the above problems, we propose an efficient and effective method named Magnetic Text Detector (MTD), inspired by magnetism. It is constructed by a text representation method flexible mask (FM) and a magnetic pull module (MPM). Unlike the shrink mask and concentric mask, the former concerns the local contours and shrinks unfixed distances on different positions, which avoids the truncation issue while preserving distinctiveness from the text regions. The latter generates magnetic fields and pulls pole points of FM to the text contour by magnetism. This allows accurate reconstruction of text contours, even when predictions deviate from the actual text severely, while saving 50% of the post-processing time approximately. Several ablation studies verify the effectiveness of the proposed FM and MPM. Extensive experiments show that our MTD achieves state-of-the-art (SOTA) methods on multiple datasets from different scenes. The code is available at https://github.com/fengmulin/MTD.},
  archive      = {J_TIP},
  author       = {Xu Han and Chuang Yang and Qi Wang},
  doi          = {10.1109/TIP.2025.3609196},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pull pole points to text contour by magnetism: A real-time scene text detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NanoHTNet: Nano human topology network for efficient 3D human pose estimation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models. A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging. To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task. First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features. Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence. Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly. Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology. By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements. Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano. Code and models are available at https://github.com/vefalun/NanoHTNet.},
  archive      = {J_TIP},
  author       = {Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li},
  doi          = {10.1109/TIP.2025.3608662},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NanoHTNet: Nano human topology network for efficient 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liying Wang and Xiaoli Zhang and Chuanmin Jia and Siwei Ma},
  doi          = {10.1109/TIP.2025.3611602},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT tracking draws increasing attention because of its robustness in multi-modal warranting (MMW) scenarios, such as nighttime and adverse weather conditions, where relying on a single sensing modality fails to ensure stable tracking results. However, existing benchmarks predominantly contain videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality. This weakens the representativeness of existing benchmarks in severe imaging conditions, leading to tracking failures in MMW scenarios. To bridge this gap, we present a new benchmark considering the modality validity, MV-RGBT, captured specifically from MMW scenarios where either RGB (extreme illumination) or TIR (thermal truncation) modality is invalid. Hence, it is further divided into two subsets according to the valid modality, offering a new compositional perspective for evaluation and providing valuable insights for future designs. Moreover, MV-RGBT is the most diverse benchmark of its kind, featuring 36 different object categories captured across 19 distinct scenes. Furthermore, considering severe imaging conditions in MMW scenarios, a new problem is posed in RGBT tracking, named ‘when to fuse’, to stimulate the development of fusion strategies for such scenarios. To facilitate its discussion, we propose a new solution with a mixture of experts, named MoETrack, where each expert generates independent tracking results along with a confidence score. Extensive results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios. Besides, MoETrack achieves state-of-the-art results on several benchmarks, including MV-RGBT, GTOT, and LasHeR. Source codes and benchmarks are available at https://github.com/Zhangyong-Tang/MVRGBT.},
  archive      = {J_TIP},
  author       = {Zhangyong Tang and Tianyang Xu and Xiao-Jun Wu and Xuefeng Zhu and Chunyang Cheng and Zhenhua Feng and Josef Kittler},
  doi          = {10.1109/TIP.2025.3611687},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explanation method based on interpretable linear model with four key characteristics. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the interpretability of deep neural networks (DNNs) in visual-related tasks, existing explanation methods commonly generate a saliency map based on the linear relation between output results and input features. However, when the explanation conflicts with a human visual examination, these methods do not provide further evidence to analyze the saliency explanation. Most may fail to provide feature attribution with identifiable semantics or produce misleading explanations due to their insufficient robustness. In this paper, we first propose four key characteristics (richness, adaptivity, exclusiveness, and fairness) to evaluate the existing linear relation-based explanation method, and then construct an interpretable linear model to satisfy them. We formalize the characteristics and develop a novel explanation method based on this. We extract and reconstruct key exclusive semantic features from the feature map using the Nonnegative Matrix Factorization (NMF) algorithm, utilize the information entropy model to determine the number of features adaptively and their richness, and then linearly combine each feature with fairly assigned weights using an approximate Shapley algorithm to generate the saliency map. Compared with the state-of-the-art methods, our explanations of different datasets and DNNs are more convincing and robust in terms of Average drop (AD), Average increase (AI), Deletions (Del), and Insertions (Ins). Our supplementary experiments provide sufficient evidence that the four characteristics guarantee the feasibility of feature attribution analysis and enhance the quality of the resulting explanations.},
  archive      = {J_TIP},
  author       = {Yuecan Yuan and Zhan Ao Huang and Peng Li and Ying Fu and Xuemin Zhao and Canghong Shi and Xiaojie Li and Xi Wu},
  doi          = {10.1109/TIP.2025.3611593},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An explanation method based on interpretable linear model with four key characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent assistant domains transformer for source-free domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to address the challenge of adapting to a target domain without accessing the source domain directly. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current mainstream methods primarily focus on evaluating invariant features in the target domain that closely resemble those in the source domain, subsequently aligning the target domain with the source domain. However, these methods are susceptible to hard samples and influenced by domain bias. In this paper, we propose a Consistent Assistant Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue by constructing invariable feature representations of domain consistency. Concretely, we develop an assistant domain module for CADTrans to obtain diversified representations from the intermediate aggregated global attentions, which addresses the limitation of existing methods in adequately representing diversity. Based on assistant and target domains, invariable feature representations are obtained by multiple consistent strategies, which can be used to distinguish easy and hard samples. Finally, to align the hard samples to the corresponding easy samples, we construct a conditional multi-kernel max mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same category and those of different categories. Extensive experiments are conducted on various benchmarks such as Office-31, Office-Home, VISDA-C, and DomainNet-126, proving the significant performance improvements achieved by our proposed approaches. Code is available at https://github.com/RoryShao/CADTrans.git.},
  archive      = {J_TIP},
  author       = {Renrong Shao and Wei Zhang and Kangyang Luo and Qin Li and Jun Wang},
  doi          = {10.1109/TIP.2025.3611799},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistent assistant domains transformer for source-free domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some works integrate SSL techniques into deep clustering frameworks to enhance image clustering performance. However, they all need pre-training, clustering learning, or a trained clustering model as prerequisites, limiting the flexible and out-of-box application of SSL learners in the image clustering task. This work introduces ASD, an adaptor that enables the cold-start of SSL learners for deep image clustering without any prerequisites. Specifically, we first randomly sample pseudo-labeled data from all unlabeled data, and set an instance-level classifier to learn them with semantically aligned instance-level labels. With the ability of instance-level classification, we track the class transitions of predictions on unlabeled data to extract high-level similarities of instance-level classes, which can be utilized to assign cluster-level labels to pseudo-labeled data. Finally, we use the pseudo-labeled data with assigned cluster-level labels to trigger a general SSL learner trained on the unlabeled data for image clustering. We show the superior performance of ASD across various benchmarks against the latest deep image clustering approaches and very slight accuracy gaps compared to SSL methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can also further boost the performance of existing SSL-embedded deep image clustering methods.},
  archive      = {J_TIP},
  author       = {Yue Duan and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TIP.2025.3611144},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3595408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders (AEs) have received extensive attention in hyperspectral anomaly detection (HAD) due to their capability to separate the background from the anomaly based on the reconstruction error. However, the existing AE methods routinely fail to adequately exploit spatial information and may precisely reconstruct anomalies, thereby affecting the detection accuracy. To address these issues, this study proposes a novel Multi-scale Autoencoder Suppression Strategy (MASS). The underlying principle of MASS is to prioritize the reconstruction of background information over anomalies. In the encoding stage, the Local Feature Extractor, which integrates Convolution and Omni-Dimensional Dynamic Convolution (ODConv), is combined with the Global Feature Extractor based on Transformer to effectively extract multi-scale features. Furthermore, a Self-Attention Suppression module (SAS) is devised to diminish the influence of anomalous pixels, enabling the network to focus more intently on the precise reconstruction of the background. During the process of network learning, a mask derived from the test outcomes of each iteration is integrated into the loss function computation, encompassing only the positions with low anomaly scores from the preceding detection round. Experiments on eight datasets demonstrate that the proposed method is significantly superior to several traditional methods and deep learning methods in terms of performance.},
  archive      = {J_TIP},
  author       = {Bing Tu and Tao Zhou and Bo Liu and Yan He and Jun Li and Antonio Plaza},
  doi          = {10.1109/TIP.2025.3595408},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the coordination of frequency and attention in masked image modeling. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, masked image modeling (MIM), which learns visual representations by reconstructing the masked patches of an image, has dominated self-supervised learning in computer vision. However, the pre-training of MIM always takes massive time due to the large-scale data and large-size backbones. We mainly attribute it to the random patch masking in previous MIM works, which fails to leverage the crucial semantic information for effective visual representation learning. To tackle this issue, we propose the Frequency & Attention-driven Masking and Throwing Strategy (FAMT), which can extract semantic patches and reduce the number of training patches to boost model performance and training efficiency simultaneously. Specifically, FAMT utilizes the self-attention mechanism to extract semantic information from the image for masking during training in an unsupervised manner. However, attention alone could sometimes focus on inappropriate areas regarding the semantic information. Thus, we are motivated to incorporate the information from the frequency domain into the self-attention mechanism to derive the sampling weights for masking, which captures semantic patches for visual representation learning. Furthermore, we introduce a patch throwing strategy based on the derived sampling weights to reduce the training cost. FAMT can be seamlessly integrated as a plug-and-play module and surpasses previous works, e.g. reducing the training phase time by nearly 50% and improving the linear probing accuracy of MAE by 1.3% ∼ 3.9% across various datasets, including CIFAR-10/100, Tiny ImageNet, and ImageNet-1K. FAMT also demonstrates superior performance in downstream detection and segmentation tasks.},
  archive      = {J_TIP},
  author       = {Jie Gui and Tuo Chen and Minjing Dong and Zhengqi Liu and Hao Luo and James Tin-Yau Kwok and Yuan Yan Tang},
  doi          = {10.1109/TIP.2025.3592555},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring the coordination of frequency and attention in masked image modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3599929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of prior knowledge about unknown classes during training, existing methods for cross-domain open-set image recognition typically rely on threshold-based solutions. However, such approaches often struggle to capture the complex boundary relationships between known and unknown classes, which can lead to negative transfer effects caused by feature confusion between the two. To address this issue, this paper proposes a graph isomorphic distillation diffusion model (GIDDM) that aims to learn the boundary relationships between known and unknown classes from a closed-set classifier that models predictive uncertainty. First, a diffusion classifier is designed to quantify model predictive uncertainty through a Monte Carlo sampling strategy performed on the noise distribution during the reverse denoising process. The uncertainty distribution is modeled, and the cumulative distribution function is used to compute the probability of a sample belonging to an unknown class. Second, an open-set recognition framework is constructed, treating the closed-set diffusion classifier as a teacher classifier, and guiding the student classifier to learn the complex boundary relationships between known and unknown classes through knowledge distillation. Third, the knowledge distillation process is further formalized as a graph isomorphic optimization problem, where the predictive manifolds of the student and teacher classifiers are constrained to be consistent, thereby enhancing knowledge transfer between the classifiers. Finally, the entire process is integrated into a unified open-set adversarial domain adaptation framework, reconstructing the traditional optimization objectives of closed-set adversarial domain adaptation to ensure sufficient separation between known and unknown classes while aligning the distributions of known classes in both the source and target domains. Experiments conducted on multiple hyperspectral image (HSI) datasets demonstrate that the proposed method achieves state-of-the-art performance on cross-domain open-set image recognition tasks. The code demo can be accessed on the following website: https://github.com/wzr78998/GIDDM.},
  archive      = {J_TIP},
  author       = {Haoyu Wang and Yuhu Cheng and Wei Zhang and Xiaomin Liu and Xuesong Wang},
  doi          = {10.1109/TIP.2025.3599929},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing perception of key changes in remote sensing image change captioning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3589096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, while significant progress has been made in remote sensing image change captioning, existing methods fail to filter out areas unrelated to actual changes, making models susceptible to irrelevant features. In this article, we propose a novel multimodal model for remote sensing image change captioning, guided by Key Change Features and Instruction-tuned (KCFI). This model aims to fully leverage the intrinsic knowledge of large language models through visual instructions and enhance the effectiveness and accuracy of change features using pixel-level change detection tasks. Specifically, KCFI includes a ViTs encoder for extracting bi-temporal remote sensing image features, a key feature perceiver for identifying critical change areas, a pixel-level change detection decoder to constrain key change features, and an instruction-tuned decoder based on a large language model. Moreover, to ensure that change captioning and change detection tasks are jointly optimized, we employ a dynamic weight-averaging strategy to balance the losses between the two tasks. We also explore various feature combinations for visual fine-tuning instructions and demonstrate that using only key change features to guide the large language model is the optimal choice. To validate the effectiveness of our approach, we compare it against several state-of-the-art change captioning methods on the LEVIR-CC dataset, achieving the best performance. Our code will be available at https://github.com/yangcong356/KCFI.git.},
  archive      = {J_TIP},
  author       = {Cong Yang and Zuchao Li and Hongzan Jiao and Zhi Gao and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3589096},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing perception of key changes in remote sensing image change captioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fourier-based decoupling network for joint low-light image enhancement and deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime handheld photography is often simultaneously affected by low light and blur degradations due to object motion and camera shake. Previous methods typically design specific modules to restore the degradations in the spatial domain independently. However, the interdependence of low light and blur degradations in the spatial domain makes it difficult for these approaches to effectively decouple the degradations, limiting the performance of the designed modules. In this paper, we observe that in the Fourier domain, low light and blur degradations can be represented independently in the amplitude and phase of the image. Through an in-depth analysis of the underlying physical degradation process, we discover that low light degradation exhibits distinct characteristics across different frequency bands in amplitude, while blur degradation is characterized by phase correlation. Leveraging these insights, we mathematically derive a frequency attention mechanism and a filtering mechanism for learning decoupled representations of these degradations, proposing a Fourier-based Decoupling Network for joint low-light image enhancement and deblurring. Experimental results demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets and exhibits significantly sharper edges.},
  archive      = {J_TIP},
  author       = {Luwei Tu and Jiawei Wu and Chenxi Wang and Deyu Meng and Zhi Jin},
  doi          = {10.1109/TIP.2025.3592559},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fourier-based decoupling network for joint low-light image enhancement and deblurring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-precision edge detection guided by flow fields. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3572763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is frequently employed to support downstream visual tasks. However, current edge detection methods still encounter two significant challenges: extracting complex textured targets and capturing valuable information from complex backgrounds. We propose FFED, a flow field-guided edge detection model. FFED integrates the three components of our design. FFED incorporates three designed components: the Feature Broadcast Module (FBM), the Antagonistic Bio-inspired Spatial Attention Module (ABSAM), a novel pixel difference convolution named ALS. The FBM serves as an implementation mode of the flow field, with its input pair selection strategy inspired by video processing.The FBM broadcasts high-level semantic features to high-resolution ones, preserving more meaningful texture details. Inspired by biological studies, we propose the ABSAM. ABSAM extracts valuable information from complex backgrounds by optimizing spatial modeling of data. The ALS exhibits enhanced capability in extracting gradient information and capturing subtle texture details that are easily overlooked. Experimental results demonstrate that FFED achieved competitive detection results on NYUD, BSDS500, and BIPED datasets, as well as good performance on industrial datasets. Additionally, the experiment verified the auxiliary effect of FFED on downstream visual tasks. The code is available at https://github.com/hanyuchen2022/Flow-field-guided-edge-detection-FFED-.},
  archive      = {J_TIP},
  author       = {Bing Li and Yuchen Han and Shiyin Zhang and Haowei Wang and Zhenbing Zhao and Yongjie Zhai},
  doi          = {10.1109/TIP.2025.3572763},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-precision edge detection guided by flow fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceive-IR: Learning to perceive degradation better for all-in-one image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3566300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing All-in-One image restoration methods often fail to simultaneously perceive degradation types and severity levels, overlooking the importance of fine-grained quality perception. Moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. To address these limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image restoration framework designed for fine-grained quality control across various degradation types and severity levels. Its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. Specifically, Perceive-IR operates in two key stages: (1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern threetier quality levels by optimizing the alignment between prompts and images within the CLIP perception space. This stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; (2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. This strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while simultaneously pulling it away from both low- and medium-quality samples. Furthermore, Perceive-IR incorporates a Semantic Guidance Module (SGM) and Compact Feature Extraction (CFE). The SGM leverages semantic information from pre-trained vision models to provide high-level contextual guidance, while the CFE focuses on extracting degradation-specific features, ensuring accurate handling of diverse image degradations. Extensive experiments demonstrate that Perceive-IR not only surpasses state-of-the-art methods but also generalizes reliably to zero-shot realworld and unknown degraded scenes, while adapting seamlessly to different backbone networks. This versatility underscores the framework’s robustness and backbone-agnostic design.},
  archive      = {J_TIP},
  author       = {Xu Zhang and Jiaqi Ma and Guoli Wang and Qian Zhang and Huan Zhang and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3566300},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceive-IR: Learning to perceive degradation better for all-in-one image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unrolling plug-and-play gradient graph laplacian regularizer for image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3562425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic deep learning (DL) networks for image restoration like denoising and interpolation lack mathematical interpretability, require voluminous training data to tune large parameter sets, and are fragile in the face of covariate shift. To address these shortcomings, we build interpretable networks by unrolling variants of a graph-based optimization algorithm of different complexities. Specifically, for a general linear image formation model, we first formulate a convex quadratic programming (QP) problem with a new ℓ2-norm graph smoothness prior called gradient graph Laplacian regularizer (GGLR) that promotes piecewise planar (PWP) signal reconstruction. To solve the posed unconstrained QP problem, instead of computing a linear system solution straightforwardly, we introduce a variable number of auxiliary variables and correspondingly design a family of ADMM algorithms. We then unroll them into variable-complexity feedforward networks, amenable to parameter tuning via back-propagation. More complex unrolled networks require more labeled data to train more parameters, but have better over-all performance. The unrolled networks have periodic insertions of a graph learning module, akin to a self-attention mechanism in a transformer architecture, to learn pairwise similarity structure inherent in data. Experimental results show that our unrolled networks perform competitively to generic DL networks in image restoration quality while using only a fraction of parameters, and demonstrate improved robustness to covariate shift.},
  archive      = {J_TIP},
  author       = {Jianghe Cai and Gene Cheung and Fei Chen},
  doi          = {10.1109/TIP.2025.3562425},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unrolling plug-and-play gradient graph laplacian regularizer for image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing multi-modal large language models for measuring and interpreting color differences. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3522802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate measurement of perceptual color differences (CDs) between two images plays an important role in modern smartphone photography. Although traditional CD metrics provide numerical scores to quantify color variations, they often lack the ability to offer intuitive insights or explanations that reflect the factors behind these differences in a way that aligns with human perception and reasoning. Here, we present CD-Reasoning, an innovative method designed not merely to compute numerical CD scores but also to provide a detailed rationale for the observed CDs between images. This method surpasses simple numerical quantification, delivering a more profound and explanatory analysis that bridges quantitative assessments with the qualitative reasoning characteristic of human perception. The development of the CD-Reasoning model begins with the compilation of a multi-modal CD dataset dubbed M-SPCD based on the existing SPCD, where we collect textual descriptions that detail the quantification of CDs across seven pivotal attributes: white balance, brightness contrast, color contrast, overall brightness, overall color, shadow detail, and highlight detail. Utilizing the newly curated M-SPCD dataset, we enhance the capabilities of cutting-edge Multimodal Large Language Models (MLLMs) to not only accurately assess numerical CD scores but also to provide in-depth reasoning that explains the CDs between two images. Extensive experiments demonstrate that the proposed CD-Reasoning not only achieves superior accuracy compared to state-of-the-art CD metrics but also significantly exceeds leading MLLMs in CD interpreting. Source codes will be available at https://github.com/LongYu-LY/CD-Reasoning.},
  archive      = {J_TIP},
  author       = {Zhihua Wang and Yu Long and Qiuping Jiang and Chao Huang and Xiaochun Cao},
  doi          = {10.1109/TIP.2024.3522802},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harnessing multi-modal large language models for measuring and interpreting color differences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining pre- and post-demosaicking noise removal for RAW video. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3527886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is one of the fundamental steps of the processing pipeline that converts data captured by a camera sensor into a display-ready image or video. It is generally performed early in the pipeline, usually before demosaicking, although studies swapping their order or even conducting them jointly have been proposed. With the advent of deep learning, the quality of denoising algorithms has steadily increased. Even so, modern neural networks still have a hard time adapting to new noise levels and scenes, which is indispensable for real-world applications. With those in mind, we propose a self-similarity-based denoising scheme that weights both a pre- and a post-demosaicking denoiser for Bayer-patterned CFA video data. We show that a balance between the two leads to better image quality, and we empirically find that higher noise levels benefit from a higher influence pre-demosaicking. We also integrate temporal trajectory prefiltering steps before each denoiser, which further improve texture reconstruction. The proposed method only requires an estimation of the noise model at the sensor, accurately adapts to any noise level, and is competitive with the state of the art, making it suitable for real-world videography.},
  archive      = {J_TIP},
  author       = {M. Sánchez-Beeckman and A. Buades and N. Brandonisio and B. Kanoun},
  doi          = {10.1109/TIP.2025.3527886},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining pre- and post-demosaicking noise removal for RAW video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radar pointcloud offers attractive potential for 3D sensing, thanks to its robustness in challenging conditions such as smoke and low illumination. However, existing methods failed to simultaneously address the three main challenges in mmWave radar pointcloud reconstruction: specular information lost, low angular resolution, and severe interference. In this paper, we propose DREAM-PCD, a novel framework specifically designed for real-time 3D environment sensing that combines signal processing and deep learning methods into three well-designed components to tackle all three challenges: Non-Coherent Accumulation for dense points, Synthetic Aperture Accumulation for improved angular resolution, and Real-Denoise Multiframe network for interference removal. By leveraging causal multiple viewpoints accumulation and the “real-denoise" mechanism, DREAM-PCD significantly enhances the generalization performance and real-time capability. We also introduce RadarEyes, the largest mmWave indoor dataset with over 1,000,000 frames, featuring a unique design incorporating two orthogonal single-chip radars, Lidar, and camera, enriching dataset diversity and applications. Experimental results demonstrate that DREAM-PCD surpasses existing methods in reconstruction quality, and exhibits superior generalization and real-time capabilities, enabling high-quality real-time reconstruction of radar pointcloud under various parameters and scenarios. We believe that DREAM-PCD, along with the RadarEyes dataset, will significantly advance mmWave radar perception in future real-world applications.},
  archive      = {J_TIP},
  author       = {Ruixu Geng and Yadong Li and Dongheng Zhang and Jincheng Wu and Yating Gao and Yang Hu and Yan Chen},
  doi          = {10.1109/TIP.2024.3512356},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the prevailing transition from single-task to multi-task approaches in video anomaly detection, we observe that many adopt sub-optimal frameworks for individual proxy tasks. Motivated by this, we contend that optimizing single-task frameworks can advance both single- and multi-task approaches. Accordingly, we leverage middle-frame prediction as the primary proxy task, and introduce an effective hybrid framework designed to generate accurate predictions for normal frames and flawed predictions for abnormal frames. This hybrid framework is built upon a bi-directional structure that seamlessly integrates both vision transformers and ConvLSTMs. Specifically, we utilize this bi-directional structure to fully analyze the temporal dimension by predicting frames in both forward and backward directions, significantly boosting the detection stability. Given the transformer’s capacity to model long-range contextual dependencies, we develop a convolutional temporal transformer that efficiently associates feature maps from all context frames to generate attention-based predictions for target frames. Furthermore, we devise a layer-interactive ConvLSTM bridge that facilitates the smooth flow of low-level features across layers and time-steps, thereby strengthening predictions with fine details. Anomalies are eventually identified by scrutinizing the discrepancies between target frames and their corresponding predictions. Several experiments conducted on public benchmarks affirm the efficacy of our hybrid framework, whether used as a standalone single-task approach or integrated as a branch in a multi-task approach. These experiments also underscore the advantages of merging vision transformers and ConvLSTMs for video anomaly detection. The implementation of our hybrid framework is available at https://github.com/SHENGUODONG19951126/ConvTTrans-ConvLSTM.},
  archive      = {J_TIP},
  author       = {Guodong Shen and Yuqi Ouyang and Junru Lu and Yixuan Yang and Victor Sanchez},
  doi          = {10.1109/TIP.2024.3512369},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SALENet: Structure-aware lighting estimations from a single image for indoor environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) lighting plays a pivotal role in modern augmented and mixed-reality (AR/MR) applications, facilitating immersive experiences through realistic object insertion and dynamic relighting. However, the acquisition of precise HDR environment maps remains cost-prohibitive and impractical when using standard devices. To bridge this gap, this paper introduces SALENet , a novel deep network for estimating global lighting conditions from a single image, to effectively mitigate the need for resource-intensive acquisition methods. In contrast to earlier studies, we focus on exploring the inherent structural relationships within the lighting distribution. We design a hierarchical transformer-based neural network architecture with a proposed cross-attention mechanism between different resolution lighting source representations, optimizing the spatial distribution of lighting sources simultaneously for enhanced consistency. To further improve accuracy, a structure-based contrastive learning method is proposed to select positive-negative pairs based on lighting distribution similarity. By harnessing the synergy of hierarchical transformers and structure-based contrastive learning, our framework yields a significant enhancement in lighting prediction accuracy, enabling high-fidelity augmented and mixed reality to achieve cost-effectively immersive and realistic lighting effects.},
  archive      = {J_TIP},
  author       = {Junhong Zhao and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TIP.2024.3512381},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SALENet: Structure-aware lighting estimations from a single image for indoor environments},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning frame-event fusion for motion deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion deblurring is a highly ill-posed problem due to the significant loss of motion information in the blurring process. Complementary informative features from auxiliary sensors such as event cameras can be explored for guiding motion deblurring. The event camera can capture rich motion information asynchronously with microsecond accuracy. In this paper, a novel frame-event fusion framework is proposed for event-driven motion deblurring (FEF-Deblur), which can sufficiently explore long-range cross-modal information interactions. Firstly, different modalities are usually complementary and also redundant. Cross-modal fusion is modeled as complementary-unique features separation-and-aggregation, avoiding the modality redundancy. Unique features and complementary features are first inferred with parallel intra-modal self-attention and inter-modal cross-attention respectively. After that, a correlation-based constraint is designed to act between unique and complementary features to facilitate their differentiation, which assists in cross-modal redundancy suppression. Additionally, spatio-temporal dependencies among neighboring inputs are crucial for motion deblurring. A recurrent cross attention is introduced to preserve inter-input attention information, in which the current spatial features and aggregated temporal features are attending to each other by establishing the long-range interaction between them. Extensive experiments on both synthetic and real-world motion deblurring datasets demonstrate our method outperforms state-of-the-art event-based and image/video-based methods. The code will be made publicly available.},
  archive      = {J_TIP},
  author       = {Wen Yang and Jinjian Wu and Jupo Ma and Leida Li and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TIP.2024.3512362},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning frame-event fusion for motion deblurring},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale pre-trained vision-language models ( e.g ., CLIP) have shown incredible generalization performance in downstream tasks such as video-text retrieval (VTR). Traditional approaches have leveraged CLIP’s robust multi-modal alignment ability for VTR by directly fine-tuning vision and text encoders with clean video-text data. Yet, these techniques rely on carefully annotated video-text pairs, a process that is costly and labor-intensive. In this context, we introduce a new approach, Pseudo-Supervised Selective Contrastive Learning (PS-SCL). PS-SCL minimizes the dependency on manually-labeled text annotations by generating pseudo-supervisions from unlabeled video data for training. We first exploit CLIP’s visual recognition capabilities to generate pseudo-texts automatically. These pseudo-texts contain diverse visual concepts from the video and serve as weak textual guidance. Moreover, we introduce Selective Contrastive Learning (SeLeCT), which prioritizes and selects highly correlated video-text pairs from pseudo-supervised video-text pairs. By doing so, SeLeCT enables more effective multi-modal learning under weak pairing supervision. Experimental results demonstrate that our method outperforms CLIP zero-shot performance by a large margin on multiple video-text retrieval benchmarks, e.g ., 8.2% R@1 for video-to-text on MSRVTT, 12.2% R@1 for video-to-text on DiDeMo, and 10.9% R@1 for video-to-text on ActivityNet, respectively.},
  archive      = {J_TIP},
  author       = {Yu Lu and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3514352},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning of intrinsic semantics with diffusion model for person re-identification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) aims to learn semantic representations for person retrieval without using identity labels. Most existing methods generate fine-grained patch features to reduce noise in global feature clustering. However, these methods often compromise the discriminative semantic structure and overlook the semantic consistency between the patch and global features. To address these problems, we propose a Person Intrinsic Semantic Learning (PISL) framework with diffusion model for unsupervised person Re-ID. First, we design the Spatial Diffusion Model (SDM), which performs a denoising diffusion process from noisy spatial transformer parameters to semantic parameters, enabling the sampling of patches with intrinsic semantic structure. Second, we propose the Semantic Controlled Diffusion (SCD) loss to guide the denoising direction of the diffusion model, facilitating the generation of semantic patches. Third, we propose the Patch Semantic Consistency (PSC) loss to capture semantic consistency between the patch and global features, refining the pseudo-labels of global features. Comprehensive experiments on three challenging datasets show that our method surpasses current unsupervised Re-ID methods. The source code will be publicly available at https://github.com/taoxuefong/Diffusion-reid.},
  archive      = {J_TIP},
  author       = {Xuefeng Tao and Jun Kong and Min Jiang and Ming Lu and Ajmal Mian},
  doi          = {10.1109/TIP.2024.3514360},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised learning of intrinsic semantics with diffusion model for person re-identification},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion models as strong adversaries. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated their great ability to generate high-quality images for various tasks. With such a strong performance, diffusion models can potentially pose a severe threat to both humans and deep learning models. However, their abilities as adversaries have not been well explored. Among different adversarial scenarios, the no-box adversarial attack is the most practical one, as it assumes that the attacker has no access to the training dataset or the target model. Existing works still require some data from the training dataset, which may not be feasible in real-world scenarios. In this paper, we investigate the adversarial capabilities of diffusion models by conducting no-box attacks solely using data generated by diffusion models. Specifically, our attack method generates a synthetic dataset using diffusion models to train a substitute model. We then employ a classification diffusion model to fine-tune the substitute model, considering model uncertainty and incorporating noise augmentation. Finally, we sample adversarial examples from the diffusion models using the average approximation over the diffusion substitute model with multiple inferences. Extensive experiments on the ImageNet dataset demonstrate that the proposed attack method achieves state-of-the-art performance in both no-box attack and black-box attack scenarios.},
  archive      = {J_TIP},
  author       = {Xuelong Dai and Yanjie Li and Mingxing Duan and Bin Xiao},
  doi          = {10.1109/TIP.2024.3514361},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diffusion models as strong adversaries},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3515801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pose estimation for objects with rotational symmetry, ambiguous poses may arise, and the symmetry axes of objects are crucial for eliminating such ambiguities. Currently, in pose estimation, reliance on manual settings of symmetry axes decreases the accuracy of pose estimation. To address this issue, this method proposes determining the orders of symmetry axes and angles between axes based on a given rotational symmetry type or polyhedron, reducing the need for manual settings of symmetry axes. Subsequently, two key axes with the highest orders are defined and localized, then three orthogonal axes are generated based on key axes, while each symmetry axis can be computed utilizing orthogonal axes. Compared to localizing symmetry axes one by one, the key-axis-based symmetry axis localization is more efficient. To support geometric and texture symmetry, the method utilizes the ADI metric for key axis localization in geometrically symmetric objects and proposes a novel metric, ADI-C, for objects with texture symmetry. Experimental results on the LM-O and HB datasets demonstrate a 9.80% reduction in symmetry axis localization error and a 1.64% improvement in pose estimation accuracy. Additionally, the method introduces a new dataset, DSRSTO, to illustrate its performance across seven types of geometrically and texturally symmetric objects. The GitHub link for the open-source tool based on this method is https://github.com/WangYuLin-SEU/KASAL.},
  archive      = {J_TIP},
  author       = {Yulin Wang and Chen Luo},
  doi          = {10.1109/TIP.2024.3515801},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to compress face video is a crucial problem for a series of online applications, such as video chat/conference, live broadcasting and remote education. Compared to other natural videos, these face-centric videos owning abundant structural information can be compactly represented and high-quality reconstructed via deep generative models, such that the promising compression performance can be achieved. However, the existing generative face video compression schemes are faced with the inconsistency between the 3D facial motion in the physical world and the face content evolution in the 2D view. To solve this drawback, we propose a 3D-Keypoint-and-2D-Motion based generative method for Face Video Compression, namely FVC-3K2M, which can well ensure perceptual compensation and visual consistency between motion description and face reconstruction. In particular, the temporal evolution of face video can be characterized into separate 3D keypoints from the global and local perspectives, entailing great coding flexibility and accurate motion representation. Moreover, a cascade motion conversion mechanism is further proposed to internally convert 3D keypoints to 2D dense motion, enforcing the face video reconstruction to be perceptually realistic. Finally, an adaptive reference frame selection scheme is developed to enhance the adaptation of various temporal movements. Experimental results show that the proposed scheme can realize reliable video communication in the extremely limited bandwidth, e.g., 2 kbps. Compared to the state-of-the-art video coding standards and the latest face video compression methods, extensive comparisons demonstrate that our proposed scheme achieves superior compression performance in terms of multiple quality evaluations.},
  archive      = {J_TIP},
  author       = {Zhao Wang and Bolin Chen and Shurun Wang and Shiqi Wang and Yan Ye and Siwei Ma},
  doi          = {10.1109/TIP.2024.3518100},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rebalanced vision-language retrieval considering structure-aware distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language retrieval aims to search for similar instances in one modality based on queries from another modality. The primary objective is to learn cross-modal matching representations in a latent common space. Actually, the assumption underlying cross-modal matching is modal balance, where each modality contains sufficient information to represent the others. However, noise interference and modality insufficiency often lead to modal imbalance, making it a common phenomenon in practice. The impact of imbalance on retrieval performance remains an open question. In this paper, we first demonstrate that ultimate cross-modal matching is generally sub-optimal for cross-modal retrieval when imbalanced modalities exist. The structure of instances in the common space is inherently influenced when facing imbalanced modalities, posing a challenge to cross-modal similarity measurement. To address this issue, we emphasize the importance of meaningful structure-preserved matching. Accordingly, we propose a simple yet effective method to rebalance cross-modal matching by learning structure-preserved matching representations. Specifically, we design a novel multi-granularity cross-modal matching that incorporates structure-aware distillation alongside the cross-modal matching loss. While the cross-modal matching loss constraints instance-level matching, the structure-aware distillation further regularizes the geometric consistency between learned matching representations and intra-modal representations through the developed relational matching. Extensive experiments on different datasets affirm the superior cross-modal retrieval performance of our approach, simultaneously enhancing single-modal retrieval capabilities compared to the baseline models.},
  archive      = {J_TIP},
  author       = {Yang Yang and Wenjuan Xi and Luping Zhou and Jinhui Tang},
  doi          = {10.1109/TIP.2024.3518759},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rebalanced vision-language retrieval considering structure-aware distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language models (VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. We believe our method establishes a simple yet strong baseline for future STR research with VLMs.},
  archive      = {J_TIP},
  author       = {Shuai Zhao and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3512354},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting domain-adaptive semantic segmentation via knowledge distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous methods for unsupervised domain adaptation (UDA) have been proposed in semantic segmentation, achieving remarkable improvements. These methods are categorized into an adversarial learning-based approach that utilizes an additional discriminator and image translation model, and a self-supervised approach that uses a teacher model to generate pseudo labels. Among them, the self-supervised UDA approaches based on a self-training show excellent adaptability in semantic segmentation. However, erroneous estimates of the pseudo ground truths (PGTs) used in the self-training may often lead to inaccurate updates in the teacher model. Although several attempts have been made to address this issue, the teacher model updated through exponential moving average (EMA) still has a risk of propagating inaccuracies from the PGTs. Inspired by the fact that UDA shares similar principles with knowledge distillation (KD), we revisit the self-training based UDA approach from the perspective of KD and propose a novel UDA approach that employs two different teacher models. Specifically, we utilize both an EMA-updated teacher model to generate PGTs and a frozen teacher model pretrained with source data to transfer knowledge on a feature space. Since the frozen teacher model has no constraint on the model architecture unlike the EMA updated teacher model, we can effectively leverage a better representation power from the larger frozen teacher. Extensive experiments on various backbones (DeepLab-V2 [40] and DAFormer [73]) and scenarios (GTA5 → Cityscapes and SYNTHIA → Cityscapes) show that the proposed method improves segmentation performance in the target domain with its scalability. In particular, our method achieves comparable or better performance than state-of-the-arts even with a lightweight backbone.},
  archive      = {J_TIP},
  author       = {Seongwon Jeong and Jiyeong Kim and Sungheui Kim and Dongbo Min},
  doi          = {10.1109/TIP.2024.3501076},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting domain-adaptive semantic segmentation via knowledge distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MWFormer: Multi-weather image restoration using degradation-aware transformers. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.},
  archive      = {J_TIP},
  author       = {Ruoxi Zhu and Zhengzhong Tu and Jiaming Liu and Alan C. Bovik and Yibo Fan},
  doi          = {10.1109/TIP.2024.3501855},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MWFormer: Multi-weather image restoration using degradation-aware transformers},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified video reconstruction for rolling shutter and global shutter cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3504275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the general domain of video reconstruction (VR) is fragmented into different shutters spanning global shutter and rolling shutter cameras. Despite rapid progress in the state-of-the-art, existing methods overwhelmingly follow shutter-specific paradigms and cannot conceptually generalize to other shutter types, hindering the uniformity of VR models. In this paper, we propose UniVR, a versatile framework to handle various shutters through unified modeling and shared parameters. Specifically, UniVR encodes diverse shutter types into a unified space via a tractable shutter adapter, which is parameter-free and thus can be seamlessly delivered to current well-established VR architectures for cross-shutter transfer. To demonstrate its effectiveness, we conceptualize UniVR as three shutter-generic VR methods, namely Uni-SoftSplat, Uni-SuperSloMo, and Uni-RIFE. Extensive experimental results demonstrate that the pre-trained model without any fine-tuning can achieve reasonable performance even on novel shutters. After fine-tuning, new state-of-the-art performances are established that go beyond shutter-specific methods and enjoy strong generalization. The code is available at https://github.com/GitCVfb/UniVR.},
  archive      = {J_TIP},
  author       = {Bin Fan and Zhexiong Wan and Boxin Shi and Chao Xu and Yuchao Dai},
  doi          = {10.1109/TIP.2024.3504275},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unified video reconstruction for rolling shutter and global shutter cameras},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing few-shot out-of-distribution detection with pre-trained model features. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3468874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the reliability of open-world intelligent systems heavily relies on effective out-of-distribution (OOD) detection. Despite notable successes in existing OOD detection methods, their performance in scenarios with limited training samples is still suboptimal. Therefore, we first construct a comprehensive few-shot OOD detection benchmark in this paper. Remarkably, our investigation reveals that Parameter-Efficient Fine-Tuning (PEFT) techniques, such as visual prompt tuning and visual adapter tuning, outperform traditional methods like fully fine-tuning and linear probing tuning in few-shot OOD detection. Considering that some valuable information from the pre-trained model, which is conducive to OOD detection, may be lost during the fine-tuning process, we reutilize features from the pre-trained models to mitigate this issue. Specifically, we first propose a training-free approach, termed uncertainty score ensemble (USE). This method integrates feature-matching scores to enhance existing OOD detection methods, significantly narrowing the gap between traditional fine-tuning and PEFT techniques. However, due to its training-free property, this method is unable to improve in-distribution accuracy. To this end, we further propose a method called Domain-Specific and General Knowledge Fusion (DSGF) to improve few-shot OOD detection performance and ID accuracy under different fine-tuning paradigms. Experiment results demonstrate that DSGF enhances few-shot OOD detection across different fine-tuning strategies, shot settings, and OOD detection methods. We believe our work can provide the research community with a novel path to leveraging large-scale visual pre-trained models for addressing FS-OOD detection. The code will be released.},
  archive      = {J_TIP},
  author       = {Jiuqing Dong and Yifan Yao and Wei Jin and Heng Zhou and Yongbin Gao and Zhijun Fang},
  doi          = {10.1109/TIP.2024.3468874},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing few-shot out-of-distribution detection with pre-trained model features},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised temporal correspondence learning for unified video object removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3340605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object removal aims at erasing a target object in the entire video and filling holes with plausible contents, given an object mask in the first frame as input. Existing solutions mostly break down the task into (supervised) mask tracking and (self-supervised) video completion, and then separately tackle them with tailored designs. In this paper, we introduce a new setup, coined as unified video object removal , where mask tracking and completion are addressed within a unified framework. Despite introducing more challenges, the setup is promising for future practical usage. We embrace the observation that these two sub-tasks have strong inherent connections in terms of pixel-level temporal correspondence. Making full use of the connections could be beneficial considering the complexity of both algorithm and deployment. We propose a single network linking the two sub-tasks by inferring temporal correspondences across multiple frames, i.e ., correspondences between valid-valid (V-V) pixel pairs for mask tracking and correspondences between valid-hole (V-H) pixel pairs for video completion. Thanks to the unified setup, the network can be learned end-to-end in a totally unsupervised fashion without any annotations. We demonstrate that our method can generate visually pleasing results and perform favorably against existing separate solutions in realistic test cases.},
  archive      = {J_TIP},
  author       = {Zhongdao Wang and Jinglu Wang and Xiao Li and Ya-Li Li and Yan Lu and Shengjin Wang},
  doi          = {10.1109/TIP.2023.3340605},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised temporal correspondence learning for unified video object removal},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Field-of-view IoU for object detection in 360° images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3296013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° cameras have gained popularity over the last few years. In this paper, we propose two fundamental techniques—Field-of-View IoU (FoV-IoU) and 360Augmentation for object detection in 360° images. Although most object detection neural networks designed for perspective images are applicable to 360° images in equirectangular projection (ERP) format, their performance deteriorates owing to the distortion in ERP images. Our method can be readily integrated with existing perspective object detectors and significantly improves the performance. The FoV-IoU computes the intersection-over-union of two Field-of-View bounding boxes in a spherical image which could be used for training, inference, and evaluation while 360Augmentation is a data augmentation technique specific to 360° object detection task which randomly rotates a spherical image and solves the bias due to the sphere-to-plane projection. We conduct extensive experiments on the 360° indoor dataset with different types of perspective object detectors and show the consistent effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Miao Cao and Satoshi Ikehata and Kiyoharu Aizawa},
  doi          = {10.1109/TIP.2023.3296013},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Field-of-view IoU for object detection in 360° images},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3273451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The end-to-end image fusion framework has achieved promising performance, with dedicated convolutional networks aggregating the multi-modal local appearance. However, long-range dependencies are directly neglected in existing CNN fusion approaches, impeding balancing the entire image-level perception for complex scenario fusion. In this paper, therefore, we propose an infrared and visible image fusion algorithm based on the transformer module and adversarial learning. Inspired by the global interaction power, we use the transformer technique to learn the effective global fusion relations. In particular, shallow features extracted by CNN are interacted in the proposed transformer fusion module to refine the fusion relationship within the spatial scope and across channels simultaneously. Besides, adversarial learning is designed in the training process to improve the output discrimination via imposing competitive consistency from the inputs, reflecting the specific characteristics in infrared and visible images. The experimental performance demonstrates the effectiveness of the proposed modules, with superior improvement against the state-of-the-art, generalising a novel paradigm via transformer and adversarial learning in the fusion task.},
  archive      = {J_TIP},
  author       = {Dongyu Rao and Tianyang Xu and Xiao-Jun Wu},
  doi          = {10.1109/TIP.2023.3273451},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured attention composition for temporal action localization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2022.3180925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.},
  archive      = {J_TIP},
  author       = {Le Yang and Junwei Han and Tao Zhao and Nian Liu and Dingwen Zhang},
  doi          = {10.1109/TIP.2022.3180925},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structured attention composition for temporal action localization},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational structured attention networks for deep visual representation learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2021.3137647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have enabled major progresses in addressing pixel-level prediction tasks such as semantic segmentation, depth estimation, surface normal prediction and so on, benefiting from their powerful capabilities in visual representation learning. Typically, state of the art models integrate attention mechanisms for improved deep feature representations. Recently, some works have demonstrated the significance of learning and combining both spatial- and channel-wise attentions for deep feature refinement. In this paper, we aim at effectively boosting previous approaches and propose a unified deep framework to jointly learn both spatial attention maps and channel attention vectors in a principled manner so as to structure the resulting attention tensors and model interactions between these two types of attentions. Specifically, we integrate the estimation and the interaction of the attentions within a probabilistic representation learning framework, leading to VarIational STructured Attention networks (VISTA-Net). We implement the inference rules within the neural network, thus allowing for end-to-end learning of the probabilistic and the CNN front-end parameters. As demonstrated by our extensive empirical evaluation on six large-scale datasets for dense visual prediction, VISTA-Net outperforms the state-of-the-art in multiple continuous and discrete prediction tasks, thus confirming the benefit of the proposed approach in joint structured spatial-channel attention estimation for deep representation learning. The code is available at https://github.com/ygjwd12345/VISTA-Net.},
  archive      = {J_TIP},
  author       = {Guanglei Yang and Paolo Rota and Xavier Alameda-Pineda and Dan Xu and Mingli Ding and Elisa Ricci},
  doi          = {10.1109/TIP.2021.3137647},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational structured attention networks for deep visual representation learning},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). Stacked deconvolutional network for semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2019.2895460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and bring the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which enhances the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-ofthe- art results on four datasets, including PASCAL VOC 2012, CamVid, GATECH, COCO Stuff. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.},
  archive      = {J_TIP},
  author       = {Jun Fu and Jing Liu and Yuhang Wang and Jin Zhou and Changyong Wang and Hanqing Lu},
  doi          = {10.1109/TIP.2019.2895460},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stacked deconvolutional network for semantic segmentation},
  year         = {2019},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Monocular depth estimation with augmented ordinal depth relationships. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2877944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing algorithms for depth estimation from single monocular images need large quantities of metric groundtruth depths for supervised learning. We show that relative depth can be an informative cue for metric depth estimation and can be easily obtained from vast stereo videos. Acquiring metric depths from stereo videos is sometimes impracticable due to the absence of camera parameters. In this paper, we propose to improve the performance of metric depth estimation with relative depths collected from stereo movie videos using existing stereo matching algorithm.We introduce a new “Relative Depth in Stereo” (RDIS) dataset densely labelled with relative depths. We first pretrain a ResNet model on our RDIS dataset. Then we finetune the model on RGB-D datasets with metric ground-truth depths. During our finetuning, we formulate depth estimation as a classification task. This re-formulation scheme enables us to obtain the confidence of a depth prediction in the form of probability distribution. With this confidence, we propose an information gain loss to make use of the predictions that are close to ground-truth. We evaluate our approach on both indoor and outdoor benchmark RGB-D datasets and achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Yuanzhouhan Cao and Tianqi Zhao and Ke Xian and Chunhua Shen and Zhiguo Cao and Shugong Xu},
  doi          = {10.1109/TIP.2018.2877944},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Monocular depth estimation with augmented ordinal depth relationships},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Deep active learning with contaminated tags for image aesthetics assessment. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2828326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic quality assessment has becoming an indispensable technique that facilitates a variety of image applications, e.g., photo retargeting and non-realistic rendering. Conventional approaches suffer from the following limitations: 1) the inefficiency of semantically describing images due to the inherent tag noise and incompletion, 2) the difficulty of accurately reflecting how humans actively perceive various regions inside each image, and 3) the challenge of incorporating the aesthetic experiences of multiple users. To solve these problems, we propose a novel semi-supervised deep active learning (SDAL) algorithm, which discovers how humans perceive semantically important regions from a large quantity of images partially assigned with contaminated tags. More specifically, as humans usually attend to the foreground objects before understanding them, we extract a succinct set of BING (binarized normed gradients) [60]-based object patches from each image. To simulate human visual perception, we propose SDAL which hierarchically learns human gaze shifting path (GSP) by sequentially linking semantically important object patches from each scenery. Noticeably, SDLA unifies the semantically important regions discovery and deep GSP feature learning into a principled framework, wherein only a small proportion of tagged images are adopted. Moreover, based on the sparsity penalty, SDLA can optimally abandon the noisy or redundant low-level image features. Finally, by leveraging the deeply-learned GSP features, a probabilistic model is developed for image aesthetics assessment, where the experience of multiple professional photographers can be encoded. Besides, auxiliary quality-related features can be conveniently integrated into our probabilistic model. Comprehensive experiments on a series of benchmark image sets have demonstrated the superiority of our method. As a byproduct, eye tracking experiments have shown that GSPs generated by our SDAL are about 93% consistent with real human gaze shifting paths.},
  archive      = {J_TIP},
  author       = {Zhenguang Liu and Zepeng Wang and Yiyang Yao and Luming Zhang and Ling Shao},
  doi          = {10.1109/TIP.2018.2828326},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep active learning with contaminated tags for image aesthetics assessment},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2010). Exploring duplicated regions in natural images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2010.2046599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Duplication of image regions is a common method for manipulating original images, using typical software like Adobe Photoshop, 3DS MAX, etc. In this study, we propose a duplication detection approach that can adopt two robust features based on discrete wavelet transform (DWT) and kernel principal component analysis (KPCA). Both schemes provide excellent representations of the image data for robust block matching. Multiresolution wavelet coefficients and KPCA-based projected vectors corresponding to image-blocks are arranged into a matrix for lexicographic sorting. Sorted blocks are used for making a list of similar point-pairs and for computing their offset frequencies. Duplicated regions are then segmented by an automatic technique that refines the list of corresponding point-pairs and eliminates the minimum offset-frequency threshold parameter in the usual detection method. A new technique that extends the basic algorithm for detecting Flip and Rotation types of forgeries is also proposed. This method uses global geometric transformation and the labeling technique to indentify the mentioned forgeries. Experiments with a good number of natural images show very promising results, when compared with the conventional PCA-based approach. A quantitative analysis indicate that the wavelet-based feature outperforms PCA- or KPCA-based features in terms of average precision and recall in the noiseless, or uncompressed domain, while KPCA-based feature obtains excellent performance in the additive noise and lossy JPEG compression environments.},
  archive      = {J_TIP},
  author       = {M. Bashar and K. Noda and N. Ohnishi and K. Mori},
  doi          = {10.1109/TIP.2010.2046599},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring duplicated regions in natural images},
  year         = {2010},
}
</textarea>
</details></li>
</ul>

</body>
</html>
