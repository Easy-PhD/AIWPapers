<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 73</h2>
<ul>
<li><details>
<summary>
(2025). Instance-adaptive spatial-temporal enhancement for efficient video compression. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3602648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently compressing HD/UHD content has long been challenging due to high bitrate costs. Instance-adaptive enhancement methods try to tackle this issue by compressing a video at reduced resolution and enhancing it using a neural model specifically overfitted for this video. However, existing methods focus solely on spatial super-resolution (SR) and under-utilize the videos’ temporal redundancy. Their limited management of the model’s updated parameters also causes excessive overfitting overheads. Therefore, this paper introduces IASTE, the first instance-adaptive enhancement method based on spatial-temporal enhancement (STE), and incorporates low-rank adaptation (LoRA) for efficient model overfitting. Specifically, we downscale videos spatially and temporally to reduce the data volume and achieve efficient video compression. Then, we overfit a specific STE model for each video and use it to enhance the decoded video’s spatiotemporal resolution. Leveraging the video swin transformer’s strong capability in capturing spatiotemporal correlations, we design a lightweight and efficient model to implement video STE. The model is overfitted for each video using LoRA. By freezing the pre-trained model and selectively updating a few low-rank matrices, the bitrate overhead for model storage can be mitigated. Experiments prove that compared to directly compressing high-frame-rate (HFR), high-resolution (HR) videos, our method achieves around 30% BD-Rate gains on the CTC and UVG datasets, about 15% gains on the YoutubeUGC dataset, and about 10% gains on the ultra-long videos in the Xiph dataset.},
  archive      = {J_TIP},
  author       = {Yan Zhao and Zhengxue Cheng and Jiangchuan Li and Donghui Feng and Qunshan Gu and Qi Wang and Guo Lu and Li Song},
  doi          = {10.1109/TIP.2025.3602648},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Instance-adaptive spatial-temporal enhancement for efficient video compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot image recognition via learning dual prototype accordance across meta-domains. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen categories. However, existing methods often struggle with the persistent semantic gap caused by limited semantic descriptors and rigid visual feature modeling. In particular, modeling pre-defined class-level attribute descriptions as ground truth hinders effective semantic-to-visual alignment to some extent. To mitigate these issues, we propose the Bilateral-guided Prototype Refinement Network(BPRN), a novel ZSL framework designed to refine dual prototypes across meta-domains of varying scales. Specifically, we first disentangle the relationships among class-level semantics and use them to generate corresponding pseudo-visual prototypes. Then, by leveraging distribution information across dual prototypes in different meta-domains, BPRN achieves bidirectional calibration between visual-to-semantic and semantic-to-visual modalities. Finally, a synthesized class-level representation derived from the refined dual prototypes is employed for inference, instead of relying on a single prototype. Extensive experiments conducted on five widely-used ZSL benchmark datasets demonstrate that BPRN consistently achieves competitive or even superior performance. Specifically, in the GZSL scenario, BPRN shows improvements of 2.1%, 7.3%, 6.1%, and 4.8% on AWA1, AWA2, SUN, and aPY, respectively, compared to existing embedding-based ZSL methods. Ablation studies and visualization analyses further validate the effectiveness of the proposed components.},
  archive      = {J_TIP},
  author       = {Bocheng Ren and Yuanyuan Yi and Qingchen Zhang and Debin Liu},
  doi          = {10.1109/TIP.2025.3607588},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image recognition via learning dual prototype accordance across meta-domains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion and appearance decoupling representation for event cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, with high temporal resolution and high dynamic range, have shown great potential under extreme scenarios such as high-speed movement and low illumination. However, previous event representation methods typically aggregate event data into a single dense tensor, often overlooking the dynamic changes of events within a given time unit. This limitation can introduce historical artifacts and semantic inconsistencies, ultimately degrading model performance. Inspired by human visual prior, we propose a motion and appearance decoupling (MAD) event representation to disentangle the mixed spatial-temporal event tensor into two independent branches. This bio-inspired design helps the network extract discriminative temporal (i.e., motion) and spatial (i.e., appearance) information, thus reducing the network’s learning burden toward complex high-level interpretation tasks. In our method, the event motion guided attention module (EMGA) is designed to achieve temporal and spatial feature interaction and fusion sequentially. Based on EMGA, three specially designed decoder heads are proposed for several representative event-based tasks (i.e., object detection, semantic segmentation, and human pose estimation). Experimental results demonstrate that our method achieves state-of-the-art performance on the above three tasks, which reveals that our method is an easy-to-implement replacement for currently event-based methods. Our code is available at: https://github.com/ChenYichen9527/MAD-representation.},
  archive      = {J_TIP},
  author       = {Nuo Chen and Boyang Li and Yingqian Wang and Xinyi Ying and Longguang Wang and Chushu Zhang and Yulan Guo and Miao Li and Wei An},
  doi          = {10.1109/TIP.2025.3607632},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion and appearance decoupling representation for event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) shows great potential for improving generalization to unseen classes in real-world scenarios. However, most GZSL methods depend on benchmark datasets with per-class attribute annotations, which creates a large semantic gap and worsens the domain shift problem in the visual-semantic space. To address these challenges, instance-level attributes offer an intuitive solution, but they require expensive manual annotation. In this paper, we propose a simple yet effective approach called per-instance attribute synthesis (PIAS) to generate diverse semantic representations for each instance. Our method first uses the Vision Transformer (ViT) model to extract visual features and then generates per-instance attributes. The patch splitting, positional embedding, and multi-head self-attention mechanisms in ViT improve the discriminability of both visual and semantic representations. Next, we define the generated attributes of class-average images as class anchor points. These anchor points are calibrated in the semantic space by minimizing the cosine similarity between the anchor points and per-class attribute annotations. Finally, we improve the diversity of generated per-instance attributes by aligning the topological structure between per-class attribute annotations and synthesized per-instance attributes with that between class-average visual features and per-instance visual features. We conduct comprehensive experiments on three challenging ZSL datasets: AWA2, CUB, and SUN. The results show that PIAS significantly outperforms state-of-the-art methods under both ZSL and GZSL settings. We further demonstrate the generalization ability of PIAS by applying it to attribute-based zero-shot image retrieval tasks.},
  archive      = {J_TIP},
  author       = {Chenwei Tang and Ying Wang and Wei Xie and Qianjun Zhang and Rong Xiao and Zhenan He and Jiancheng Lv},
  doi          = {10.1109/TIP.2025.3607612},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSP: Multimodal self-attention prompt learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal prompt learning has emerged as an effective strategy for adapting vision-language models such as CLIP to downstream tasks. However, conventional approaches typically operate at the input level, forcing learned prompts to propagate through a sequence of frozen Transformer layers. This indirect adaptation introduces cumulative geometric distortions, a limitation that we formalize as the indirect learning dilemma (ILD), leading to overfitting of the base class and reduced generalization to novel classes. To overcome this challenge, we propose the Multimodal Self-Attention Prompt (MSP) framework, which shifts adaptation into the semantic core of the model by injecting learnable prompts directly into the key and value sequences of attention blocks. This direct modulation preserves the pretrained embedding geometry while enabling more precise downstream adaptation. MSP further incorporates distance-aware optimization to maintain semantic consistency with CLIP’s original representation space, and partial prompt learning via stochastic dimension masking to improve robustness and prevent over-specialization. Extensive evaluations across 11 benchmarks demonstrate the effectiveness of MSP. It achieves a state-of-the-art harmonic mean accuracy of 80.67%, with 77.32% accuracy on novel classes—representing a 2.18% absolute improvement over prior methods—while requiring only 0.11M learnable parameters. Notably, MSP surpasses CLIP’s zero-shot performance on 10 out of 11 datasets, establishing a new paradigm for efficient and generalizable prompt-based adaptation. Our implementation is available at https://github.com/laixinyi023/ Multimodal-Self-Attention-Prompt.},
  archive      = {J_TIP},
  author       = {Xinyi Lai and Xiao Ke and Huangbiao Xu and Shanghui Wu and Wenzhong Guo},
  doi          = {10.1109/TIP.2025.3607613},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSP: Multimodal self-attention prompt learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (and vice versa), opening new possibilities in image processing. However, the growing diversity of camera models, particularly in mobile devices, renders the development of individual ISPs unsustainable due to their limited versatility and adaptability across varied camera systems. In this paper, we introduce Uni-ISP, a novel pipeline that unifies ISP learning for diverse mobile cameras, delivering a highly accurate and adaptable processor. The core of Uni-ISP is leveraging device-aware embeddings through learning forward/inverse ISPs and its special training scheme. By doing so, Uni-ISP not only improves the performance of forward and inverse ISPs but also unlocks new applications previously inaccessible to conventional learned ISPs. To support this work, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphone cameras. Extensive experiments validate Uni-ISP’s accuracy in learning forward and inverse ISPs (with improvements of +2.4dB/1.5dB PSNR), versatility in enabling new applications, and adaptability to new camera models.},
  archive      = {J_TIP},
  author       = {Lingen Li and Mingde Yao and Xingyu Meng and Muquan Yu and Tianfan Xue and Jinwei Gu},
  doi          = {10.1109/TIP.2025.3607617},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient open vocabulary 3D scene understanding based on semantic probability. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D scene understanding methods heavily depend on 3D annotation and training, which allow for the identification of seen classes but struggle to recognize unseen classes. In this paper, we leverage the open vocabulary inference capabilities of pre-trained models, enabling the encoding of open vocabulary concepts. However, unlike existing open vocabulary 3D scene understanding methods, we propose a framework based on semantic probability. This innovation significantly reduces computational cost and is compatible with state-of-the-art two-stage 2D pre-trained models. Specifically, we align the text features from the CLIP model with the pixel features from the 2D pre-trained models, inferring semantic probability of image pixels based on similarity and projecting it onto 3D points. Subsequently, we introduce a point cloud pairs semantic fusion method to merge the point clouds, reducing the semantic probability of erroneous 3D points. Based on probability scores, we achieve 3D semantic segmentation on open vocabularies without any supervision or training. In addition, the semantic probability of 3D points can serve as pseudo-labels for 3D distillation, and the geometric features of the 3D scene can be exploited to improve the segmentation performance. Experimental results demonstrate that the proposed method exhibits competitive performance on publicly available benchmark datasets, including ScanNet, Matterport3D, and nuScenes.},
  archive      = {J_TIP},
  author       = {Lingfeng Shen and Xiaoyao Wei and Gang Pan and Qian Zheng and Yanlong Cao},
  doi          = {10.1109/TIP.2025.3607643},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cost-efficient open vocabulary 3D scene understanding based on semantic probability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D medical image segmentation is crucial for diagnosis and treatment. Diffusion models demonstrate promising performance in medical image segmentation tasks due to the progressive nature of the generation process and the explicit modeling of data distributions. However, the weak guidance of conditional information and insufficient feature extraction in diffusion models lead to the loss of fine-grained features and structural consistency in the segmentation results, thereby affecting the accuracy of medical image segmentation. To address this challenge, we propose a Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation. We extract multilevel semantic features from the original images using an encoder and tightly integrate them with the denoising process of the diffusion model through a Semantic Hierarchical Embedding (SHE) mechanism, to capture the intricate relationship between the noisy label and image data. Meanwhile, we design a Global-Slice Perception Mamba (GSPM) layer, which integrates multi-dimensional perception mechanisms to endow the model with comprehensive spatial reasoning and feature extraction capabilities. Experimental results show that our proposed MambaDiff achieves more competitive performance compared to prior arts with substantially fewer parameters on four public medical image segmentation datasets including BraTS 2021, BraTS 2024, LiTS and MSD Hippocampus. The source code of our method is available at https://github.com/yuliu316316/MambaDiff.},
  archive      = {J_TIP},
  author       = {Yu Liu and Yan Feng and Juan Cheng and Haolin Zhan and Zhiqin Zhu},
  doi          = {10.1109/TIP.2025.3607615},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DFACENet: 3D facial attractiveness computation and enhancement network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of facial editing, virtual makeup, AR/VR technologies and 3D games applications underscore the need for advanced 3D facial attractiveness research. However, due to the lack of 3D beauty face data and the complexity of handling 3D face data, 3D facial aesthetics research remains largely unexplored. To fill this gap, we propose 3DFACENet, an innovative system designed for the computation and enhancement of 3D facial attractiveness. Our approach employs a 3D facial reconstruction encoder to generate encoded vectors from images and a render module to obtain 3D face models. To minimize computational load, we innovatively propose an attractiveness computation module which leverage 3D shape and texture coefficients rather than 3D mesh models to access facial attractiveness, achieving state-of-the-art results. To balance aesthetic enhancement and identity preservation, we design a controllable beautification decoder. For the first time, we introduce the concept of “attractive centers”, demonstrating that an individual’s distance to these centers is significantly negatively correlated with their beauty scores. Our beautification decoder edits 3D facial coefficients towards these centers, achieving a significant and controllable enhancement in facial attractiveness. Extensive experiments on the SCUT-FBP5500 and MEBeauty dataset validate the effectiveness and feasibility of 3DFACENet.},
  archive      = {J_TIP},
  author       = {Yuan Xie and Tianhao Peng and Mu Li and Baoyuan Wu and David Zhang},
  doi          = {10.1109/TIP.2025.3607629},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3DFACENet: 3D facial attractiveness computation and enhancement network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase unwrapping is a critical step in fringe projection profilometry, essential for achieving accurate and efficient three-dimension (3D) imaging. Temporal phase unwrapping is the most widely utilized to improve robustness and the reconstruction quality. Unfortunately, due to abrupt phase discontinuities at boundaries, misalignment between the wrapped phases, and unreliable shadow regions, fringe order errors may occur. To address these challenges, this study presents a generalized multi-feature-guided progressive order correction algorithm (GMP-OCA) for high-quality 3D imaging. The algorithm integrates global coarse detection, incremental line-wise optimization, and regional precision scanning to progressively correct fringe orders. Static and dynamic experimental results demonstrate that GMP-OCA effectively eliminates the systematic errors inherent in various phase unwrapping methods, producing high-quality 3D imaging results.},
  archive      = {J_TIP},
  author       = {Jiayi Qin and Yansong Jiang and Yiping Cao},
  doi          = {10.1109/TIP.2025.3607619},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multimodal knowledge for image compression via large foundation models. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge is an abstraction of factual principles of the physical world. Large foundation models encapsulate extensive multimodal knowledge into the parameters and thus invoke machine intelligence on various tasks. How to invoke the knowledge in these models to facilitate image compression lacks in-depth exploration. In this work, we aim to harness multi-modal knowledge into ultra-low bitrate compression and propose Multimodal Knowledge-aware Image Compression (MKIC). Our key insight is that under the context of ultra-low bitrate compression, where the encoded representation is too sparse to represent enough information of the input signal, knowledge from the physical world is required to be incorporated into the compression. Thus, more shared patterns can be stored in the model together with sparse unique features also embedded into the bitstream. In light of two kinds of knowledge, namely natural visual knowledge and human language knowledge, we propose a novel Alternating Rate-Distortion Optimization to enhance the accuracy and compactness of global semantic text representation extraction, extract the local feature map that captures visual details, and integrate these multimodal representations into a large generative foundation model to achieve high-quality reconstruction. The proposed method relights the path of learned image coding, leveraging decoupled knowledge from large foundation models. Extensive experiments show that our proposed method achieves superior comprehensive performance compared to various methods and shows great potential for ultra-low bitrate image compression.},
  archive      = {J_TIP},
  author       = {Junlong Gao and Zhimeng Huang and Qi Mao and Siwei Ma and Chuanmin Jia},
  doi          = {10.1109/TIP.2025.3607616},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring multimodal knowledge for image compression via large foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prospective layout-guided multi-modal online hashing. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, the data usually appears in a streaming fashion. To achieve remarkable retrieval performance in such scenarios, online multi-modal hashing has drawn great research attention due to its high retrieval speed and low storage cost. However, existing online multi-modal hashing methods still fail to achieve satisfactory retrieval performance in the scenarios where the new streaming datapoints all belong to the new classes. Therefore, to further improve the retrieval performance in these scenarios, we propose a novel Prospective Layout-Guided Multi-modal Online Hashing, termed PLG-MOH. Specifically, PLG-MOH first establishes the layout of the Hamming space by generating a series of hashing centers to split the space. Each hashing center will be gradually assigned to a new appearing class, and these assigned centers correspond one-to-one with the classes. Moreover, we propose a novel prospective layout-guided loss, which leverages all the hashing centers, including those not yet assigned to the classes, to supervise the training of hashing model. As the unassigned hashing centers will be designated to the new classes emerging in the future, it signifies that during each round of training, PLG-MOH has already considered the forthcoming data from new classes in the future rounds. Consequently, PLG-MOH can effectively adapt its hashing functions to address the new arriving samples and learn semantic similarity-preserved hash codes for them, meanwhile it can effectively retain the information learned from the old data. Extensive experiments on two public datasets demonstrate that the proposed PLG-MOH achieves better retrieval performance than state-of-the-art baselines on online scenarios.},
  archive      = {J_TIP},
  author       = {Rong-Cheng Tu and Xian-Ling Mao and Jin-Yu Liu and Zi-Ao Ma and Tian Lan and Heyan Huang},
  doi          = {10.1109/TIP.2025.3607626},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prospective layout-guided multi-modal online hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptually-guided VR style transfer. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) makes it possible to provide immersive multimedia content composed of omnidirectional videos (ODVs). Towards enabling more immersive and satisfying VR content, methods are needed to manipulate VR scenes, taking into account perceptual factors related to viewers’ quality of experience (QoE). For example, style transfer methods can be applied to VR content, allowing users to create artistic or surreal effects in their immersive environments. Here, we study perceptual factors that affect the sensation of stylized immersiveness, including color dynamics and spatio-temporal consistency. To do this, we introduce an immersiveness sensitivity model of luminance and color perception, and use it to measure the color dynamics and spatio-temporal consistency of stylized VR contents. We subsequently use this model to construct a perceptually-guided VR style transfer model called VR Style Transfer GAN (VRST-GAN). VRST-GAN learns to transfer a desired style into VR to enhance immersiveness by considering color dynamics while preserving spatio-temporal consistency. We demonstrate the effectiveness of VRST-GAN via qualitative and quantitative experiments. We also develop a VR Immersiveness Predictor (VR-IP) that is able to predict the sensation of immersiveness using the perceptual model. In our experiments, VR-IP predicts immersiveness with an accuracy of 91%.},
  archive      = {J_TIP},
  author       = {Seonghwa Choi and Jungwoo Huh and Sanghoon Lee and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2025.3607611},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptually-guided VR style transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive anchor-guided representation learning for efficient multi-view subspace clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view Subspace Clustering (MVSC) effectively aggregating multiple data sources to promise clustering performance. Recently, various anchor-based variants have been introduced to effectively alleviate the computation complexity of MVSC. Although satisfactory advancement has been achieved, existing methods either independently learn anchor matrices and their anchor representations or learn a consensus anchor matrix and unified anchor representation, failing to capture both consistency and complementary information simultaneously. In addition, the time complexity of obtaining clustering results by applying Singular Value Decomposition (SVD) on the anchor representation matrix remains high. To tackle the above problems, we propose an Adaptive Anchor-guided Representation Learning for Efficient Multi-view Subspace Clustering (A2RL-EMVSC) framework, which integrates consensus anchors learning, anchor-guided representation learning and matrix factorization to enhance clustering performance and scalability. Technically, the proposed method learns view-specific anchor representation matrices by consensus anchors guidance, which simultaneously exploit consistency and complementary information. Moreover, by applying matrix decomposition to the view-specific anchor representation matrices, clustering results can be achieved with linear time complexity. Extensive experiments on ten challenging multi-view datasets show that the proposed method can improve the effectiveness and superiority of clustering compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mengjiao Zhang and Xinwang Liu and Tianhao Han and Xiaofeng Qu and Sijie Niu},
  doi          = {10.1109/TIP.2025.3607587},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive anchor-guided representation learning for efficient multi-view subspace clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOPE: Enhanced position image priors via high-order implicit representations. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Image Prior (DIP) has shown that networks with stochastic initialization and custom architectures can effectively address inverse imaging challenges. Despite its potential, DIP requires significant computational resources, whereas the lighter Implicit Neural Positional Image Prior (PIP) often yields overly smooth solutions due to exacerbated spectral bias. Research on lightweight, high-performance solutions for inverse imaging remains limited. This paper proposes a novel framework, Enhanced Positional Image Priors through High-Order Implicit Representations (HOPE), incorporating high-order interactions between layers within a conventional cascade structure. This approach reduces the spectral bias commonly seen in PIP, enhancing the model’s ability to capture both low- and high-frequency components for optimal inverse problem performance. We theoretically demonstrate that HOPE’s expanded representational space, narrower convergence range, and improved Neural Tangent Kernel (NTK) diagonal properties enable more precise frequency representations than PIP. Comprehensive experiments across tasks such as signal representation (audio, image, volume) and inverse image processing (denoising, super-resolution, CT reconstruction, inpainting) confirm that HOPE establishes new benchmarks for recovery quality and training efficiency.},
  archive      = {J_TIP},
  author       = {Yang Chen and Ruituo Wu and Junhui Hou and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TIP.2025.3607582},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOPE: Enhanced position image priors via high-order implicit representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic prompting learning for human-object interaction detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection, as a foundational task in human-centric understanding, aims to detect interactive triplets in real-world scenarios. To better distinguish diverse HOIs within an open-world context, current HOI detectors utilize pre-trained Visual-Language Models (VLMs) to extract prior knowledge through textual prompts (i.e., descriptive texts for each HOI instance). However, relying on predetermined descriptive texts, such approaches only acquire a fixed set of textual knowledge for HOI prediction, consequently resulting in inferior performance and limited generalization. To remedy this, we propose a novel VLM-based method, which jointly performs prompting learning from both visual and textual perspectives and synergizes visual-textual prompting for HOI detection. Initially, we design a hierarchical adaptation architecture to perform progressive prompting: visual prompting is facilitated through gradual token migration from VLM’s image encoder, while textual prompting is initialized with progressively leveled interaction descriptions. In addition, to synergize the visual-textual prompting learning, a text-supervising and image-tuning loop is introduced, in which the text-supervising stage guides visual prompting learning through contrastive learning and the image-tuning stage refines textual prompting by modal matching. Finally, we employ an interaction-aware knowledge merging mechanism to effectively transfer visual-textual knowledge encapsulated within synergistic prompting for HOI detection. Extensive experiments on two benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings.},
  archive      = {J_TIP},
  author       = {Jinguo Luo and Weihong Ren and Zhiyong Wang and Xi’ai Chen and Huijie Fan and Zhi Han and Honghai Liu},
  doi          = {10.1109/TIP.2025.3607614},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Synergistic prompting learning for human-object interaction detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a unified model for surface anomaly detection remains challenging due to significant variations across product categories. Recent feature editing methods, as a branch of image reconstruction, mitigate the over-generalization of auto-encoders that leads to accurate anomaly reconstruction. However, these methods are only suited for texture-category products and have significant limitations in being generalized to other categories. In this article, we propose a multi-category anomaly editing network with a dual-branch training approach: one branch processes defect-free images (normal branch), while the other handles synthetic anomaly images (anomaly branch). Specifically, the paired samples are first fed into the multi-category anomaly feature editing based auto-encoder (MCAFE-AE) to perform image reconstruction and inpainting. In the normal branch, we propose a dual-entropy constrained deep embedded clustering module (DEC-DECM) to promote a more compact and orderly distribution of normal latent features, while avoiding trivial clustering solutions. Based on the clustering results, we further design a patch-based adaptive thresholding (PAT) strategy to adaptively calculate the threshold representing the central boundary of the cluster center for each local patch, thereby enabling the model to detect anomalies. Then, in the anomaly branch, we propose a multi-category anomaly feature editing module (MCAFEM) to identify anomalies in synthetic images and apply a category-oriented feature editing strategy to transform detected anomaly features into normal ones, thereby suppressing the reconstruction of anomalies. After completing the image reconstruction and inpainting, the input images from both branches and their respective output images are concatenated and fed into the correlation exploration and voxel-level attention based prediction network (CEVA-Net) for anomaly segmentation. The network is integrated with our proposed correlation-dependency exploration and voxel-level attention refinement module (CDE-VARM) and generates precise anomaly maps under the guidance of the bidirectional-path feature fusion (BPFF) and deep supervised learning (DSL). Extensive experiments on three datasets show that our method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruifan Zhang and Hai-Miao Hu},
  doi          = {10.1109/TIP.2025.3607638},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). URFusion: Unsupervised unified degradation-robust image fusion network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dealing with low-quality source images, existing image fusion methods either fail to handle degradations or are restricted to specific degradations. This study proposes an unsupervised unified degradation-robust image fusion network, termed as URFusion, in which various types of degradations can be uniformly eliminated during the fusion process, leading to high-quality fused images. URFusion is composed of three core modules: intrinsic content extraction, intrinsic content fusion, and appearance representation learning and assignment. It first extracts degradation-free intrinsic content features from images affected by various degradations. These content features then provide feature-level rather than image-level fusion constraints for optimizing the fusion network, effectively eliminating degradation residues and reliance on ground truth. Finally, URFusion learns the appearance representation of images and assign the statistical appearance representation of high-quality images to the content-fused result, producing the final high-quality fused image. Extensive experiments on multi-exposure image fusion and multi-modal image fusion tasks demonstrate the advantages of URFusion in fusion performance and suppression of multiple types of degradations. The code is available at https://github.com/hanna-xu/URFusion.},
  archive      = {J_TIP},
  author       = {Han Xu and Xunpeng Yi and Chen Lu and Guangcan Liu and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3607628},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {URFusion: Unsupervised unified degradation-robust image fusion network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streaming view classification with noisy label. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many image processing tasks, e.g., 3D reconstruction of dynamic scenes, different types of descriptions, a.k.a., views, of an object are emerging in a streaming way. Streaming view learning provides an effective solution to this dynamic view problem. In this paradigm, existing streaming view learning methods typically assume that all labels are accurate. However, in many real-world applications, the initial views may be not good enough for characterizing, leading to noisy labels that degrade classification performance. How to learn a model for simultaneous view evolving and label ambiguity is critical yet unexplored. In this paper, we propose a novel method called Streaming View Classification with Noisy Label (SVCNL). We calibrate noisy labels according to the emerging of new views, thereby reflecting the dynamic changes in the data more accurately. Leveraging the sequential and non-revisitable nature of views, the method tunes existing models to inherit information from previous stages by utilizing current-stage data. It reconstructs noisy labels through a label transition matrix and establishes relationships between true labels and samples using a graph embedding strategy, progressively correcting noisy labels. Together with the theoretical analyses about generalization bounds, extensive experiments demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Xiao Ouyang and Ruidong Fan and Hong Tao and Chenping Hou},
  doi          = {10.1109/TIP.2025.3607610},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Streaming view classification with noisy label},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free object detection with detection transformer. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pretrained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.},
  archive      = {J_TIP},
  author       = {Huizai Yao and Sicheng Zhao and Shuo Lu and Hui Chen and Yangyang Li and Guoping Liu and Tengfei Xing and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi          = {10.1109/TIP.2025.3607621},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Source-free object detection with detection transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion has emerged as a prominent research area in computer vision. However, little attention has been paid to complex scenes fusion, leading to sub-optimal results under interference. To fill this gap, we propose a unified framework for infrared and visible images fusion in complex scenes, termed UMCFuse. Specifically, we classify the pixels of visible images from the degree of scattering of light transmission, allowing us to separate fine details from overall intensity. Maintaining a balance between interference removal and detail preservation is essential for the generalization capacity of the proposed method. Therefore, we propose an adaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse the energy features from different modalities by analyzing them from multiple directions. Extensive fusion experiments on real and synthetic complex scenes datasets cover adverse weather conditions, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection, and depth estimation, consistently indicate the superiority of the proposed method compared with the recent representative methods. Our code is available at https://github.com/ixilai/UMCFuse.},
  archive      = {J_TIP},
  author       = {Xilai Li and Xiaosong Li and Tianshu Tan and Huafeng Li and Tao Ye},
  doi          = {10.1109/TIP.2025.3607623},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image change detection (HSI-CD) benefits from HSIs with continuous spectral bands, which uniquely enables the analysis of more subtle changes. Existing methods have achieved desirable performance relying on multi-temporal homogenous HSIs over the same region, which is generally difficult to obtain in real scenes. HSI-RGB multimodal CD overcomes the constraint of limited HSI availability by incorporating another temporal RGB data, and the combination of advantages within different modalities enhances the robustness of detection results. Nevertheless, due to the different imaging mechanisms between two modalities, existing HSI CD methods cannot be directly applied. In this paper, we propose a cycle translation-based collaborative training (co-training) for HSI-RGB multimodal CD, which achieves cross-modal mutual guidance to collaboratively learn complementary difference information from diverse modalities for identifying changes. Specifically, a cross-modal guided CycleGAN-based image translation module is designed to implement bi-directional image translation, which mitigates modal difference and enables the extraction of information related to land cover changes. Then, a spatial-spectral interactive co-training CD module is proposed to achieve iterative interaction between cross-modal information, which jointly extracts the multimodal difference features to generate the final results. The proposed method outperforms several leading CD methods in extensive experiments carried out on both real and synthetic datasets. In addition, a new public HSI-RGB multimodal dataset along with our code are available at https://github.com/Jiahuiqu/CT2Net.},
  archive      = {J_TIP},
  author       = {Wenqian Dong and Junying Ren and Song Xiao and Leyuan Fang and Jiahui Qu and Yunsong Li},
  doi          = {10.1109/TIP.2025.3607609},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic functional brain network (DFBN) can flexibly describe the time-varying topological connectivity patterns of the brain, and show great potential in brain disease diagnosis. However, most of the existing DFBN analysis methods focus on capturing the dynamic interaction at the brain region level, ignoring the spatio-temporal topological evolution across time windows. Moreover, they are difficult to suppress interfering connections in DFBNs, which leads to a diminished capacity for discerning the intrinsic structures that are intimately linked to brain disorders. To address these issues, we propose a topological evolution graph learning model to capture disease-related spatio-temporal topological features in DFBNs. Specifically, we first take the hubness of adjacent DFBN as the source domain and the target domain in turn, and then use Wasserstein distance (WD) and Gromov-Wasserstein distance (GWD) to capture the brain’s evolution law at the node and edge levels, respectively. Furthermore, we introduce the principle of relevant information to guide the topology evolution graph to learn the structures that are most relevant to brain diseases yet least redundant information between adjacent DFBNs. On this basis, we develop a high-order spatio-temporal model with multi-hop graph convolution to collaboratively extract long-range spatial and temporal dependencies from the topological evolution graph. Extensive experiments show that the proposed method outperforms the current state-of-the-art methods, and can effectively reveal the information evolution mechanism between brain regions across windows.},
  archive      = {J_TIP},
  author       = {Shengrong Li and Qi Zhu and Chunwei Tian and Li Zhang and Bo Shen and Chuhang Zheng and Daoqiang Zhang and Wei Shao},
  doi          = {10.1109/TIP.2025.3607633},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast video recoloring via curve-based palettes. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color grading, as a crucial step in film post-production, plays an important role in emotional expression and artistic enhancement. Recently, a geometric palette-based approach to video recoloring has been introduced with impressive results. It offers an intuitive interface that allows users to alter the color of a video by manipulating a limited set of representative colors. However, this method has two primary limitations. Firstly, palette extraction is computationally expensive, often taking more than one hour to generate palettes even for medium-length videos, which significantly limits the practical application of color editing for longer videos. Secondly, the palette colors are less representative, and some primary colors may be omitted from the resulting palettes during topological simplification, making it less intuitive in color editing. To overcome these limitations, in this paper, we propose a novel approach to video recoloring. The core of our method is a set of Bézier curves that connect the dominant colors throughout the input video. By slicing these Bézier curves in RGBT space, per-frame palette can be naturally derived. During recoloring, users can select several frames of interest and modify their corresponding palettes to change the color of the video. Our method is simple and intuitive, enabling compelling time-varying recoloring results. Compared to existing methods, our approach is more efficient in palette extraction and can effectively capture the dominant colors of the video. Extensive experiments demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Zheng-Jun Du and Jia-Wei Zhou and Kang Li and Jian-Yu Hao and Zi-Kang Huang and Kun Xu},
  doi          = {10.1109/TIP.2025.3607584},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast video recoloring via curve-based palettes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware generative point cloud compression for visual perception. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a rapid growth in applications that rely on point clouds to represent the 3D world, driven by the increasing demand for immersive and other related scenarios. However, compressing the large and high-precision point cloud data efficiently while maintaining high perceptual quality for human vision remains a challenge. To solve the problem, we propose a new structure-aware generative point cloud compression framework for human vision. In the encoder, we focus on information that is more sensitive to the human vision and obtain this type of information from different scale. This allows us to capture structural importance information from global scale and local scale, which are more difficult to reconstruct. For the decoder, we introduce a progressive generative reconstruction approach that utilizes acquired information from the encoder to guide the generation of point cloud surfaces. Moreover, we propose a novel probability cloud-based discriminator. Instead of directly assessing the authenticity of the generated point clouds, our discriminator evaluates the probability distribution of the existence of points within the generated point cloud. This approach reduces the difficulty of discrimination while effectively improving the accuracy of the generator in generating probability distributions. According to the correct probability, we can obtain a high accuracy point cloud by pruning the points with low probability. Through comprehensive experiments, we demonstrate the effectiveness and superiority of our proposed framework in terms of encoding efficiency, high perceptual quality, and generation quality.},
  archive      = {J_TIP},
  author       = {Yichen Zhou and Xinfeng Zhang and Yingzhan Xu and Kai Zhang and Li Zhang and Qingming Huang},
  doi          = {10.1109/TIP.2025.3607630},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-aware generative point cloud compression for visual perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised text-based person search. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description. Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning. This presents a substantial practical challenge, given the difficulty in obtaining annotated texts for person images. This work undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations. We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS. The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images. Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data. Crucially, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data. The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process. PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision. NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning. Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting.},
  archive      = {J_TIP},
  author       = {Daming Gao and Yang Bai and Min Cao and Hao Dou and Mang Ye and Min Zhang},
  doi          = {10.1109/TIP.2025.3607637},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised text-based person search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harmonized domain enabled alternate search for infrared and visible image alignment. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image alignment is essential and critical to the fusion and multi-modal perception applications. It addresses discrepancies in position and scale caused by spectral properties and environmental variations, ensuring precise pixel correspondence and spatial consistency. Existing manual calibration requires regular maintenance and exhibits poor portability, challenging the adaptability of multi-modal application in dynamic environments. In this paper, we propose a harmonized representation based infrared and visible image alignment, achieving both high accuracy and scene adaptability. Specifically, with regard to the disparity between multi-modal images, we develop an invertible translation process to establish a harmonized representation domain that effectively encapsulates the feature intensity and distribution of both infrared and visible modalities. Building on this, we design a hierarchical framework to correct deformations inferred from the harmonized domain in a coarse-to-fine manner. Our framework leverages advanced perception capabilities alongside residual estimation to enable accurate regression of sparse offsets, while an alternate correlation search mechanism ensures precise correspondence matching. Furthermore, we propose the first ground truth available misaligned infrared and visible image benchmark for evaluation. Extensive experiments validate the effectiveness of the proposed method against the state-of-the-arts, advancing the subsequent applications further.},
  archive      = {J_TIP},
  author       = {Zhiying Jiang and Zengxi Zhang and Jinyuan Liu},
  doi          = {10.1109/TIP.2025.3607585},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harmonized domain enabled alternate search for infrared and visible image alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral texture metrology based on distance measures in an information-theoretic framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work sought to instil metrology in existing hyperspectral texture feature extraction methods. Specifically, we propose distance-based expressions of graylevel cooccurrence matrix (GLCM), local binary pattern (LBP), and Gabor filtering directly computable for hyperspectral images without any pre- or post-processing. At the core of our proposition is Radical of Extended Mean Information for Discrimination (REID), a novel spectral distance with information-theoretic roots. Respecting the physics of spectrum as continuous function of wavelengths, REID is mathematically decomposable into spectral direction and spectral magnitude distances. The resulted feature calculations are fullband (utilizing all wavelengths), yet lightweight and fully interpretable. A similarity measure based on information theory is also justified. Their efficiency is demonstrated in the context of texture classification, content-based image retrieval, and cancer detection in which they consistently outperform existing computations based on dimensionally reduced space using PCA, ICA, and NMF. The propositions could be potentially integrated into machine/deep learning systems towards explainable AI (XAI).},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Jie Chen and Susanto Rahardja},
  doi          = {10.1109/TIP.2025.3608667},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on distance measures in an information-theoretic framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient and structure consistency in multimodal emotion recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is a task that integrates text, visual, and audio data to holistically infer an individual’s emotional state. Existing research predominantly focuses on exploiting modality-specific cues for joint learning, often ignoring the differences between multiple modalities under common goal learning. Due to multimodal heterogeneity, common goal learning inadvertently introduces optimization biases and interaction noise. To address above challenges, we propose a novel approach named Gradient and Structure Consistency (GSCon). Our strategy operates at both overall and individual levels to consider balance optimization and effective interaction respectively. At the overall level, to avoid the optimization suppression of a modality on other modalities, we construct a balanced gradient direction that aligns each modality’s optimization direction, ensuring unbiased convergence. Simultaneously, at the individual level, to avoid the interaction noise caused by multimodal alignment, we align the spatial structure of samples in different modalities. The spatial structure of the samples will not differ due to modal heterogeneity, achieving effective inter-modal interaction. Extensive experiments on multimodal emotion recognition and multimodal intention understanding datasets demonstrate the effectiveness of the proposed method. Code is available at https://github.com/ShiQingHongYa/GSCon.},
  archive      = {J_TIP},
  author       = {QingHongYa Shi and Mang Ye and Wenke Huang and Bo Du and Xiaofen Zong},
  doi          = {10.1109/TIP.2025.3608664},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gradient and structure consistency in multimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised domain adaptation methods rely on explicitly or implicitly aligning the features of source and target domains to construct a domain-invariant space, often using entropy minimization to reduce uncertainty and confusion. However, this approach faces two challenges: 1) Explicit alignment reduces discriminability, while implicit alignment risks pseudo-label noise, making it hard to balance structure preservation and alignment. 2) Sole reliance on entropy minimization can lead to trivial solutions in UDA, where all samples collapse into a single class. To address these issues, we propose Dual-Space Topological Isomorphism and Maximization of Predictive Diversity (DTI-MPD). Topological isomorphism is a continuous, bijective mapping that preserves the topological properties of two spaces, ensuring the global structure and relationships of data remain intact during alignment. Our method aligns source and target domain data in two independent spaces while balancing the effects of entropy minimization through predictive diversity maximization. The core of dual-space topological isomorphism lies in establishing a reversible correspondence between the source and target domains, avoiding information loss during alignment and preserving the global structural and topological characteristics of the data. Meanwhile, predictive diversity maximization mitigates the class collapse caused by entropy minimization, ensuring a more balanced predictive distribution across categories. This approach effectively overcomes the aforementioned issues, enabling better adaptation to new data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets, validating its effectiveness.},
  archive      = {J_TIP},
  author       = {Mengru Wang and Jinglei Liu},
  doi          = {10.1109/TIP.2025.3608670},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pull pole points to text contour by magnetism: A real-time scene text detector. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text reading plays a crucial role in scene understanding. As its precondition task, scene text detection has garnered increasing interest from researchers. Segmentation-based text detection methods have gained prominence due to their adaptable pixel-level predictions. Many existing methods predict the shrink mask and utilize the Vatti clipping algorithm to reconstruct text contours. However, the shrink mask only focuses on the global geometry feature and shrinks the same distance everywhere, which neglects local contour information and disrupts the instance shape feature. In addition, the post-processing based on the Vatti clipping algorithm heavily relies on the predictions and is relatively complex, causing suboptimal performance in both detection accuracy and efficiency. To address the above problems, we propose an efficient and effective method named Magnetic Text Detector (MTD), inspired by magnetism. It is constructed by a text representation method flexible mask (FM) and a magnetic pull module (MPM). Unlike the shrink mask and concentric mask, the former concerns the local contours and shrinks unfixed distances on different positions, which avoids the truncation issue while preserving distinctiveness from the text regions. The latter generates magnetic fields and pulls pole points of FM to the text contour by magnetism. This allows accurate reconstruction of text contours, even when predictions deviate from the actual text severely, while saving 50% of the post-processing time approximately. Several ablation studies verify the effectiveness of the proposed FM and MPM. Extensive experiments show that our MTD achieves state-of-the-art (SOTA) methods on multiple datasets from different scenes. The code is available at https://github.com/fengmulin/MTD.},
  archive      = {J_TIP},
  author       = {Xu Han and Chuang Yang and Qi Wang},
  doi          = {10.1109/TIP.2025.3609196},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pull pole points to text contour by magnetism: A real-time scene text detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUNTNet: Homomorphic unified nexus topology for camouflaged object detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is challenging for both human and computer vision, as targets often blend into the background by sharing similar color, texture, or shape. While many feature enhancement techniques exist, single-view methods tend to overemphasize certain Recognizing that camouflaged objects exhibit different concealment strategies under varying observational perspectives, we propose HUNTNet, a network that establishes a dynamic detection mechanism to decouple target features from RGB images and perform topological decamouflage across multiple homomorphic feature spaces through a unified feature focusing architecture. We adopt PVTv2 as the backbone to extract multi-perspective spatial features. Detail representation is enhanced via a feature module that integrates Dual-Channel Recursive (DCR), Wavelet-Gabor Transform (WGT), and Anisotropic Gradient Responding (AGR), which together improve boundary discrimination and edge contour detection. To further boost performance, the Simplicial Feature Integration (SFI) module recursively fuses multi-layer features, enabling high-resolution focus on target regions. Experiments show that HUNTNet surpasses state-of-the-art methods in both accuracy and generalization, offering a robust solution for COD and improving segmentation in complex scenes. Our code is available at https://github.com/HaolinJi817/HUNTNet.},
  archive      = {J_TIP},
  author       = {Haolin Ji and Fengying Xie and Linpeng Pan and Yushan Zheng and Zhenwei Shi},
  doi          = {10.1109/TIP.2025.3607635},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HUNTNet: Homomorphic unified nexus topology for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task.},
  archive      = {J_TIP},
  author       = {Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong},
  doi          = {10.1109/TIP.2025.3609185},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NanoHTNet: Nano human topology network for efficient 3D human pose estimation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models. A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging. To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task. First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features. Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence. Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly. Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology. By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements. Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano. Code and models are available at https://github.com/vefalun/NanoHTNet.},
  archive      = {J_TIP},
  author       = {Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li},
  doi          = {10.1109/TIP.2025.3608662},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NanoHTNet: Nano human topology network for efficient 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-driven global-local fusion transformer for image super-resolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) has seen remarkable progress with the emergence of transformer-based architectures. However, due to the high computational cost, many existing transformer-based SR methods limit their attention to local windows, which hinders their ability to model long-range dependencies and global structures. To address these challenges, we propose a novel SR framework named Semantic-Driven Global-Local Fusion Transformer (SGLFT). The proposed model enhances the receptive field by combining a Hybrid Window Transformer (HWT) and a Scalable Transformer Module (STM) to jointly capture local textures and global context. To further strengthen the semantic consistency of reconstruction, we introduce a Semantic Extraction Module (SEM) that distills high-level semantic priors from the input. These semantic cues are adaptively integrated with visual features through an Adaptive Feature Fusion Semantic Integration Module (AFFSIM). Extensive experiments on standard benchmarks demonstrate the effectiveness of SGLFT in producing visually faithful and structurally consistent SR results. The code will be available at https://github.com/kbzhang0505/SGLFT.},
  archive      = {J_TIP},
  author       = {Kaibing Zhang and Zhouwei Cheng and Xin He and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3609106},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic-driven global-local fusion transformer for image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical color constancy via efficient spectral feature extraction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an empirical investigation into illuminant estimation using multi-spectral images. Our study emphasizes two key contributions: (1) the utilization of the estimated multi-spectral images and (2) the incorporation of a hierarchical structure. Firstly, exploiting multi-spectral images proves to have a positive influence on illuminant estimation, particularly in scenarios characterized by monochromatic images where conventional color constancy methods face challenges. Our experimental results vividly illustrate the effectiveness of leveraging spectral information in enhancing illuminant estimation. Secondly, the adoption of a hierarchical structure stems from the need for spatial invariance in the task of estimating a global illuminant. To further enhance the performance of the hierarchical structure, we employ a contrastive loss applied to different scaled outputs. This approach demonstrates remarkable effectiveness on our custom dataset, showcasing superior performance compared to the existing methods. In addition, we extend the evaluation to the widely recognized NUS-8 dataset, where the proposed method showcases a notable 26.7% relative improvement over the previous state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Dong-Keun Han and Dong-Hoon Kang and Jong-Ok Kim},
  doi          = {10.1109/TIP.2025.3607631},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical color constancy via efficient spectral feature extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations. Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: https://github.com/fidshu/SRSNet.},
  archive      = {J_TIP},
  author       = {Bingkun Nian and Fenghe Tang and Jianrui Ding and Jie Yang and Zhonglong Zheng and Shaohua Kevin Zhou and Wei Liu},
  doi          = {10.1109/TIP.2025.3607624},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COLA: Context-aware language-driven test-time adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing “distribution shift” issue while simultaneously protecting data privacy. However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability. In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels. This is achieved by using a pre-trained vision-language model (VLM), e.g., CLIP, that can recognize images through matching with class descriptions. While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain. To that end, we propose a novel method – Context-aware Language-driven TTA (COLA). The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively. It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency. Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance. We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks. The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.},
  archive      = {J_TIP},
  author       = {Aiming Zhang and Tianyuan Yu and Liang Bai and Jun Tang and Yanming Guo and Yirun Ruan and Yun Zhou and Zhihe Lu},
  doi          = {10.1109/TIP.2025.3607634},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {COLA: Context-aware language-driven test-time adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3595408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders (AEs) have received extensive attention in hyperspectral anomaly detection (HAD) due to their capability to separate the background from the anomaly based on the reconstruction error. However, the existing AE methods routinely fail to adequately exploit spatial information and may precisely reconstruct anomalies, thereby affecting the detection accuracy. To address these issues, this study proposes a novel Multi-scale Autoencoder Suppression Strategy (MASS). The underlying principle of MASS is to prioritize the reconstruction of background information over anomalies. In the encoding stage, the Local Feature Extractor, which integrates Convolution and Omni-Dimensional Dynamic Convolution (ODConv), is combined with the Global Feature Extractor based on Transformer to effectively extract multi-scale features. Furthermore, a Self-Attention Suppression module (SAS) is devised to diminish the influence of anomalous pixels, enabling the network to focus more intently on the precise reconstruction of the background. During the process of network learning, a mask derived from the test outcomes of each iteration is integrated into the loss function computation, encompassing only the positions with low anomaly scores from the preceding detection round. Experiments on eight datasets demonstrate that the proposed method is significantly superior to several traditional methods and deep learning methods in terms of performance.},
  archive      = {J_TIP},
  author       = {Bing Tu and Tao Zhou and Bo Liu and Yan He and Jun Li and Antonio Plaza},
  doi          = {10.1109/TIP.2025.3595408},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the coordination of frequency and attention in masked image modeling. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, masked image modeling (MIM), which learns visual representations by reconstructing the masked patches of an image, has dominated self-supervised learning in computer vision. However, the pre-training of MIM always takes massive time due to the large-scale data and large-size backbones. We mainly attribute it to the random patch masking in previous MIM works, which fails to leverage the crucial semantic information for effective visual representation learning. To tackle this issue, we propose the Frequency & Attention-driven Masking and Throwing Strategy (FAMT), which can extract semantic patches and reduce the number of training patches to boost model performance and training efficiency simultaneously. Specifically, FAMT utilizes the self-attention mechanism to extract semantic information from the image for masking during training in an unsupervised manner. However, attention alone could sometimes focus on inappropriate areas regarding the semantic information. Thus, we are motivated to incorporate the information from the frequency domain into the self-attention mechanism to derive the sampling weights for masking, which captures semantic patches for visual representation learning. Furthermore, we introduce a patch throwing strategy based on the derived sampling weights to reduce the training cost. FAMT can be seamlessly integrated as a plug-and-play module and surpasses previous works, e.g. reducing the training phase time by nearly 50% and improving the linear probing accuracy of MAE by 1.3% ∼ 3.9% across various datasets, including CIFAR-10/100, Tiny ImageNet, and ImageNet-1K. FAMT also demonstrates superior performance in downstream detection and segmentation tasks.},
  archive      = {J_TIP},
  author       = {Jie Gui and Tuo Chen and Minjing Dong and Zhengqi Liu and Hao Luo and James Tin-Yau Kwok and Yuan Yan Tang},
  doi          = {10.1109/TIP.2025.3592555},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring the coordination of frequency and attention in masked image modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OSFormer: One-step transformer for infrared video small object detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3598426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared video small object detection is pivotal in numerous security and surveillance applications. However, existing deep learning-based methods, which typically rely on a two-step paradigm of frame-by-frame detection followed by temporal refinement, struggle to effectively utilize temporal information. This is particularly challenging when detecting small objects against complex backgrounds. To address these issues, we introduce the One-Step Transformer (OSFormer), a novel method that pioneeringly integrates a small-object-friendly transformer with a one-step detection paradigm. Unlike traditional methods, OSFormer processes the video sequence only through a single inference, encoding the sequence into cube format data and tracking object motion trajectories. Additionally, we propose the Varied-Size Patch Attention (VPA) module, which generates patches of varying sizes to capture adaptive attention features, bridging the gap between transformer architectures and small object detection. To further enhance detection accuracy, OSFormer incorporates a Doppler Adaptive Filter, which integrates traditional filtering techniques into an end-to-end neural network to suppress background noise and accentuate small objects. OSFormer outperforms YOLOv8-s on both the AntiUAV dataset (+3.1% mAP50, -35.1% Params) and the InfraredUAV dataset (+4.0% mAP50−95, -51.0% FLOPs), demonstrating superior efficiency and effectiveness in small object detection.},
  archive      = {J_TIP},
  author       = {Haolin Qin and Tingfa Xu and Yuan Tang and Fengxiang Xu and Jianan Li},
  doi          = {10.1109/TIP.2025.3598426},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OSFormer: One-step transformer for infrared video small object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3599929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of prior knowledge about unknown classes during training, existing methods for cross-domain open-set image recognition typically rely on threshold-based solutions. However, such approaches often struggle to capture the complex boundary relationships between known and unknown classes, which can lead to negative transfer effects caused by feature confusion between the two. To address this issue, this paper proposes a graph isomorphic distillation diffusion model (GIDDM) that aims to learn the boundary relationships between known and unknown classes from a closed-set classifier that models predictive uncertainty. First, a diffusion classifier is designed to quantify model predictive uncertainty through a Monte Carlo sampling strategy performed on the noise distribution during the reverse denoising process. The uncertainty distribution is modeled, and the cumulative distribution function is used to compute the probability of a sample belonging to an unknown class. Second, an open-set recognition framework is constructed, treating the closed-set diffusion classifier as a teacher classifier, and guiding the student classifier to learn the complex boundary relationships between known and unknown classes through knowledge distillation. Third, the knowledge distillation process is further formalized as a graph isomorphic optimization problem, where the predictive manifolds of the student and teacher classifiers are constrained to be consistent, thereby enhancing knowledge transfer between the classifiers. Finally, the entire process is integrated into a unified open-set adversarial domain adaptation framework, reconstructing the traditional optimization objectives of closed-set adversarial domain adaptation to ensure sufficient separation between known and unknown classes while aligning the distributions of known classes in both the source and target domains. Experiments conducted on multiple hyperspectral image (HSI) datasets demonstrate that the proposed method achieves state-of-the-art performance on cross-domain open-set image recognition tasks. The code demo can be accessed on the following website: https://github.com/wzr78998/GIDDM.},
  archive      = {J_TIP},
  author       = {Haoyu Wang and Yuhu Cheng and Wei Zhang and Xiaomin Liu and Xuesong Wang},
  doi          = {10.1109/TIP.2025.3599929},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing perception of key changes in remote sensing image change captioning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3589096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, while significant progress has been made in remote sensing image change captioning, existing methods fail to filter out areas unrelated to actual changes, making models susceptible to irrelevant features. In this article, we propose a novel multimodal model for remote sensing image change captioning, guided by Key Change Features and Instruction-tuned (KCFI). This model aims to fully leverage the intrinsic knowledge of large language models through visual instructions and enhance the effectiveness and accuracy of change features using pixel-level change detection tasks. Specifically, KCFI includes a ViTs encoder for extracting bi-temporal remote sensing image features, a key feature perceiver for identifying critical change areas, a pixel-level change detection decoder to constrain key change features, and an instruction-tuned decoder based on a large language model. Moreover, to ensure that change captioning and change detection tasks are jointly optimized, we employ a dynamic weight-averaging strategy to balance the losses between the two tasks. We also explore various feature combinations for visual fine-tuning instructions and demonstrate that using only key change features to guide the large language model is the optimal choice. To validate the effectiveness of our approach, we compare it against several state-of-the-art change captioning methods on the LEVIR-CC dataset, achieving the best performance. Our code will be available at https://github.com/yangcong356/KCFI.git.},
  archive      = {J_TIP},
  author       = {Cong Yang and Zuchao Li and Hongzan Jiao and Zhi Gao and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3589096},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing perception of key changes in remote sensing image change captioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fourier-based decoupling network for joint low-light image enhancement and deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime handheld photography is often simultaneously affected by low light and blur degradations due to object motion and camera shake. Previous methods typically design specific modules to restore the degradations in the spatial domain independently. However, the interdependence of low light and blur degradations in the spatial domain makes it difficult for these approaches to effectively decouple the degradations, limiting the performance of the designed modules. In this paper, we observe that in the Fourier domain, low light and blur degradations can be represented independently in the amplitude and phase of the image. Through an in-depth analysis of the underlying physical degradation process, we discover that low light degradation exhibits distinct characteristics across different frequency bands in amplitude, while blur degradation is characterized by phase correlation. Leveraging these insights, we mathematically derive a frequency attention mechanism and a filtering mechanism for learning decoupled representations of these degradations, proposing a Fourier-based Decoupling Network for joint low-light image enhancement and deblurring. Experimental results demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets and exhibits significantly sharper edges.},
  archive      = {J_TIP},
  author       = {Luwei Tu and Jiawei Wu and Chenxi Wang and Deyu Meng and Zhi Jin},
  doi          = {10.1109/TIP.2025.3592559},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fourier-based decoupling network for joint low-light image enhancement and deblurring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-precision edge detection guided by flow fields. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3572763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is frequently employed to support downstream visual tasks. However, current edge detection methods still encounter two significant challenges: extracting complex textured targets and capturing valuable information from complex backgrounds. We propose FFED, a flow field-guided edge detection model. FFED integrates the three components of our design. FFED incorporates three designed components: the Feature Broadcast Module (FBM), the Antagonistic Bio-inspired Spatial Attention Module (ABSAM), a novel pixel difference convolution named ALS. The FBM serves as an implementation mode of the flow field, with its input pair selection strategy inspired by video processing.The FBM broadcasts high-level semantic features to high-resolution ones, preserving more meaningful texture details. Inspired by biological studies, we propose the ABSAM. ABSAM extracts valuable information from complex backgrounds by optimizing spatial modeling of data. The ALS exhibits enhanced capability in extracting gradient information and capturing subtle texture details that are easily overlooked. Experimental results demonstrate that FFED achieved competitive detection results on NYUD, BSDS500, and BIPED datasets, as well as good performance on industrial datasets. Additionally, the experiment verified the auxiliary effect of FFED on downstream visual tasks. The code is available at https://github.com/hanyuchen2022/Flow-field-guided-edge-detection-FFED-.},
  archive      = {J_TIP},
  author       = {Bing Li and Yuchen Han and Shiyin Zhang and Haowei Wang and Zhenbing Zhao and Yongjie Zhai},
  doi          = {10.1109/TIP.2025.3572763},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-precision edge detection guided by flow fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceive-IR: Learning to perceive degradation better for all-in-one image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3566300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing All-in-One image restoration methods often fail to simultaneously perceive degradation types and severity levels, overlooking the importance of fine-grained quality perception. Moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. To address these limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image restoration framework designed for fine-grained quality control across various degradation types and severity levels. Its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. Specifically, Perceive-IR operates in two key stages: (1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern threetier quality levels by optimizing the alignment between prompts and images within the CLIP perception space. This stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; (2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. This strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while simultaneously pulling it away from both low- and medium-quality samples. Furthermore, Perceive-IR incorporates a Semantic Guidance Module (SGM) and Compact Feature Extraction (CFE). The SGM leverages semantic information from pre-trained vision models to provide high-level contextual guidance, while the CFE focuses on extracting degradation-specific features, ensuring accurate handling of diverse image degradations. Extensive experiments demonstrate that Perceive-IR not only surpasses state-of-the-art methods but also generalizes reliably to zero-shot realworld and unknown degraded scenes, while adapting seamlessly to different backbone networks. This versatility underscores the framework’s robustness and backbone-agnostic design.},
  archive      = {J_TIP},
  author       = {Xu Zhang and Jiaqi Ma and Guoli Wang and Qian Zhang and Huan Zhang and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3566300},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceive-IR: Learning to perceive degradation better for all-in-one image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unrolling plug-and-play gradient graph laplacian regularizer for image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3562425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic deep learning (DL) networks for image restoration like denoising and interpolation lack mathematical interpretability, require voluminous training data to tune large parameter sets, and are fragile in the face of covariate shift. To address these shortcomings, we build interpretable networks by unrolling variants of a graph-based optimization algorithm of different complexities. Specifically, for a general linear image formation model, we first formulate a convex quadratic programming (QP) problem with a new ℓ2-norm graph smoothness prior called gradient graph Laplacian regularizer (GGLR) that promotes piecewise planar (PWP) signal reconstruction. To solve the posed unconstrained QP problem, instead of computing a linear system solution straightforwardly, we introduce a variable number of auxiliary variables and correspondingly design a family of ADMM algorithms. We then unroll them into variable-complexity feedforward networks, amenable to parameter tuning via back-propagation. More complex unrolled networks require more labeled data to train more parameters, but have better over-all performance. The unrolled networks have periodic insertions of a graph learning module, akin to a self-attention mechanism in a transformer architecture, to learn pairwise similarity structure inherent in data. Experimental results show that our unrolled networks perform competitively to generic DL networks in image restoration quality while using only a fraction of parameters, and demonstrate improved robustness to covariate shift.},
  archive      = {J_TIP},
  author       = {Jianghe Cai and Gene Cheung and Fei Chen},
  doi          = {10.1109/TIP.2025.3562425},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unrolling plug-and-play gradient graph laplacian regularizer for image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing multi-modal large language models for measuring and interpreting color differences. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3522802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate measurement of perceptual color differences (CDs) between two images plays an important role in modern smartphone photography. Although traditional CD metrics provide numerical scores to quantify color variations, they often lack the ability to offer intuitive insights or explanations that reflect the factors behind these differences in a way that aligns with human perception and reasoning. Here, we present CD-Reasoning, an innovative method designed not merely to compute numerical CD scores but also to provide a detailed rationale for the observed CDs between images. This method surpasses simple numerical quantification, delivering a more profound and explanatory analysis that bridges quantitative assessments with the qualitative reasoning characteristic of human perception. The development of the CD-Reasoning model begins with the compilation of a multi-modal CD dataset dubbed M-SPCD based on the existing SPCD, where we collect textual descriptions that detail the quantification of CDs across seven pivotal attributes: white balance, brightness contrast, color contrast, overall brightness, overall color, shadow detail, and highlight detail. Utilizing the newly curated M-SPCD dataset, we enhance the capabilities of cutting-edge Multimodal Large Language Models (MLLMs) to not only accurately assess numerical CD scores but also to provide in-depth reasoning that explains the CDs between two images. Extensive experiments demonstrate that the proposed CD-Reasoning not only achieves superior accuracy compared to state-of-the-art CD metrics but also significantly exceeds leading MLLMs in CD interpreting. Source codes will be available at https://github.com/LongYu-LY/CD-Reasoning.},
  archive      = {J_TIP},
  author       = {Zhihua Wang and Yu Long and Qiuping Jiang and Chao Huang and Xiaochun Cao},
  doi          = {10.1109/TIP.2024.3522802},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harnessing multi-modal large language models for measuring and interpreting color differences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining pre- and post-demosaicking noise removal for RAW video. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3527886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is one of the fundamental steps of the processing pipeline that converts data captured by a camera sensor into a display-ready image or video. It is generally performed early in the pipeline, usually before demosaicking, although studies swapping their order or even conducting them jointly have been proposed. With the advent of deep learning, the quality of denoising algorithms has steadily increased. Even so, modern neural networks still have a hard time adapting to new noise levels and scenes, which is indispensable for real-world applications. With those in mind, we propose a self-similarity-based denoising scheme that weights both a pre- and a post-demosaicking denoiser for Bayer-patterned CFA video data. We show that a balance between the two leads to better image quality, and we empirically find that higher noise levels benefit from a higher influence pre-demosaicking. We also integrate temporal trajectory prefiltering steps before each denoiser, which further improve texture reconstruction. The proposed method only requires an estimation of the noise model at the sensor, accurately adapts to any noise level, and is competitive with the state of the art, making it suitable for real-world videography.},
  archive      = {J_TIP},
  author       = {M. Sánchez-Beeckman and A. Buades and N. Brandonisio and B. Kanoun},
  doi          = {10.1109/TIP.2025.3527886},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining pre- and post-demosaicking noise removal for RAW video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radar pointcloud offers attractive potential for 3D sensing, thanks to its robustness in challenging conditions such as smoke and low illumination. However, existing methods failed to simultaneously address the three main challenges in mmWave radar pointcloud reconstruction: specular information lost, low angular resolution, and severe interference. In this paper, we propose DREAM-PCD, a novel framework specifically designed for real-time 3D environment sensing that combines signal processing and deep learning methods into three well-designed components to tackle all three challenges: Non-Coherent Accumulation for dense points, Synthetic Aperture Accumulation for improved angular resolution, and Real-Denoise Multiframe network for interference removal. By leveraging causal multiple viewpoints accumulation and the “real-denoise" mechanism, DREAM-PCD significantly enhances the generalization performance and real-time capability. We also introduce RadarEyes, the largest mmWave indoor dataset with over 1,000,000 frames, featuring a unique design incorporating two orthogonal single-chip radars, Lidar, and camera, enriching dataset diversity and applications. Experimental results demonstrate that DREAM-PCD surpasses existing methods in reconstruction quality, and exhibits superior generalization and real-time capabilities, enabling high-quality real-time reconstruction of radar pointcloud under various parameters and scenarios. We believe that DREAM-PCD, along with the RadarEyes dataset, will significantly advance mmWave radar perception in future real-world applications.},
  archive      = {J_TIP},
  author       = {Ruixu Geng and Yadong Li and Dongheng Zhang and Jincheng Wu and Yating Gao and Yang Hu and Yan Chen},
  doi          = {10.1109/TIP.2024.3512356},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the prevailing transition from single-task to multi-task approaches in video anomaly detection, we observe that many adopt sub-optimal frameworks for individual proxy tasks. Motivated by this, we contend that optimizing single-task frameworks can advance both single- and multi-task approaches. Accordingly, we leverage middle-frame prediction as the primary proxy task, and introduce an effective hybrid framework designed to generate accurate predictions for normal frames and flawed predictions for abnormal frames. This hybrid framework is built upon a bi-directional structure that seamlessly integrates both vision transformers and ConvLSTMs. Specifically, we utilize this bi-directional structure to fully analyze the temporal dimension by predicting frames in both forward and backward directions, significantly boosting the detection stability. Given the transformer’s capacity to model long-range contextual dependencies, we develop a convolutional temporal transformer that efficiently associates feature maps from all context frames to generate attention-based predictions for target frames. Furthermore, we devise a layer-interactive ConvLSTM bridge that facilitates the smooth flow of low-level features across layers and time-steps, thereby strengthening predictions with fine details. Anomalies are eventually identified by scrutinizing the discrepancies between target frames and their corresponding predictions. Several experiments conducted on public benchmarks affirm the efficacy of our hybrid framework, whether used as a standalone single-task approach or integrated as a branch in a multi-task approach. These experiments also underscore the advantages of merging vision transformers and ConvLSTMs for video anomaly detection. The implementation of our hybrid framework is available at https://github.com/SHENGUODONG19951126/ConvTTrans-ConvLSTM.},
  archive      = {J_TIP},
  author       = {Guodong Shen and Yuqi Ouyang and Junru Lu and Yixuan Yang and Victor Sanchez},
  doi          = {10.1109/TIP.2024.3512369},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SALENet: Structure-aware lighting estimations from a single image for indoor environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) lighting plays a pivotal role in modern augmented and mixed-reality (AR/MR) applications, facilitating immersive experiences through realistic object insertion and dynamic relighting. However, the acquisition of precise HDR environment maps remains cost-prohibitive and impractical when using standard devices. To bridge this gap, this paper introduces SALENet , a novel deep network for estimating global lighting conditions from a single image, to effectively mitigate the need for resource-intensive acquisition methods. In contrast to earlier studies, we focus on exploring the inherent structural relationships within the lighting distribution. We design a hierarchical transformer-based neural network architecture with a proposed cross-attention mechanism between different resolution lighting source representations, optimizing the spatial distribution of lighting sources simultaneously for enhanced consistency. To further improve accuracy, a structure-based contrastive learning method is proposed to select positive-negative pairs based on lighting distribution similarity. By harnessing the synergy of hierarchical transformers and structure-based contrastive learning, our framework yields a significant enhancement in lighting prediction accuracy, enabling high-fidelity augmented and mixed reality to achieve cost-effectively immersive and realistic lighting effects.},
  archive      = {J_TIP},
  author       = {Junhong Zhao and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TIP.2024.3512381},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SALENet: Structure-aware lighting estimations from a single image for indoor environments},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning frame-event fusion for motion deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion deblurring is a highly ill-posed problem due to the significant loss of motion information in the blurring process. Complementary informative features from auxiliary sensors such as event cameras can be explored for guiding motion deblurring. The event camera can capture rich motion information asynchronously with microsecond accuracy. In this paper, a novel frame-event fusion framework is proposed for event-driven motion deblurring (FEF-Deblur), which can sufficiently explore long-range cross-modal information interactions. Firstly, different modalities are usually complementary and also redundant. Cross-modal fusion is modeled as complementary-unique features separation-and-aggregation, avoiding the modality redundancy. Unique features and complementary features are first inferred with parallel intra-modal self-attention and inter-modal cross-attention respectively. After that, a correlation-based constraint is designed to act between unique and complementary features to facilitate their differentiation, which assists in cross-modal redundancy suppression. Additionally, spatio-temporal dependencies among neighboring inputs are crucial for motion deblurring. A recurrent cross attention is introduced to preserve inter-input attention information, in which the current spatial features and aggregated temporal features are attending to each other by establishing the long-range interaction between them. Extensive experiments on both synthetic and real-world motion deblurring datasets demonstrate our method outperforms state-of-the-art event-based and image/video-based methods. The code will be made publicly available.},
  archive      = {J_TIP},
  author       = {Wen Yang and Jinjian Wu and Jupo Ma and Leida Li and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TIP.2024.3512362},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning frame-event fusion for motion deblurring},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale pre-trained vision-language models ( e.g ., CLIP) have shown incredible generalization performance in downstream tasks such as video-text retrieval (VTR). Traditional approaches have leveraged CLIP’s robust multi-modal alignment ability for VTR by directly fine-tuning vision and text encoders with clean video-text data. Yet, these techniques rely on carefully annotated video-text pairs, a process that is costly and labor-intensive. In this context, we introduce a new approach, Pseudo-Supervised Selective Contrastive Learning (PS-SCL). PS-SCL minimizes the dependency on manually-labeled text annotations by generating pseudo-supervisions from unlabeled video data for training. We first exploit CLIP’s visual recognition capabilities to generate pseudo-texts automatically. These pseudo-texts contain diverse visual concepts from the video and serve as weak textual guidance. Moreover, we introduce Selective Contrastive Learning (SeLeCT), which prioritizes and selects highly correlated video-text pairs from pseudo-supervised video-text pairs. By doing so, SeLeCT enables more effective multi-modal learning under weak pairing supervision. Experimental results demonstrate that our method outperforms CLIP zero-shot performance by a large margin on multiple video-text retrieval benchmarks, e.g ., 8.2% R@1 for video-to-text on MSRVTT, 12.2% R@1 for video-to-text on DiDeMo, and 10.9% R@1 for video-to-text on ActivityNet, respectively.},
  archive      = {J_TIP},
  author       = {Yu Lu and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3514352},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning of intrinsic semantics with diffusion model for person re-identification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) aims to learn semantic representations for person retrieval without using identity labels. Most existing methods generate fine-grained patch features to reduce noise in global feature clustering. However, these methods often compromise the discriminative semantic structure and overlook the semantic consistency between the patch and global features. To address these problems, we propose a Person Intrinsic Semantic Learning (PISL) framework with diffusion model for unsupervised person Re-ID. First, we design the Spatial Diffusion Model (SDM), which performs a denoising diffusion process from noisy spatial transformer parameters to semantic parameters, enabling the sampling of patches with intrinsic semantic structure. Second, we propose the Semantic Controlled Diffusion (SCD) loss to guide the denoising direction of the diffusion model, facilitating the generation of semantic patches. Third, we propose the Patch Semantic Consistency (PSC) loss to capture semantic consistency between the patch and global features, refining the pseudo-labels of global features. Comprehensive experiments on three challenging datasets show that our method surpasses current unsupervised Re-ID methods. The source code will be publicly available at https://github.com/taoxuefong/Diffusion-reid.},
  archive      = {J_TIP},
  author       = {Xuefeng Tao and Jun Kong and Min Jiang and Ming Lu and Ajmal Mian},
  doi          = {10.1109/TIP.2024.3514360},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised learning of intrinsic semantics with diffusion model for person re-identification},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion models as strong adversaries. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated their great ability to generate high-quality images for various tasks. With such a strong performance, diffusion models can potentially pose a severe threat to both humans and deep learning models. However, their abilities as adversaries have not been well explored. Among different adversarial scenarios, the no-box adversarial attack is the most practical one, as it assumes that the attacker has no access to the training dataset or the target model. Existing works still require some data from the training dataset, which may not be feasible in real-world scenarios. In this paper, we investigate the adversarial capabilities of diffusion models by conducting no-box attacks solely using data generated by diffusion models. Specifically, our attack method generates a synthetic dataset using diffusion models to train a substitute model. We then employ a classification diffusion model to fine-tune the substitute model, considering model uncertainty and incorporating noise augmentation. Finally, we sample adversarial examples from the diffusion models using the average approximation over the diffusion substitute model with multiple inferences. Extensive experiments on the ImageNet dataset demonstrate that the proposed attack method achieves state-of-the-art performance in both no-box attack and black-box attack scenarios.},
  archive      = {J_TIP},
  author       = {Xuelong Dai and Yanjie Li and Mingxing Duan and Bin Xiao},
  doi          = {10.1109/TIP.2024.3514361},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diffusion models as strong adversaries},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3515801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pose estimation for objects with rotational symmetry, ambiguous poses may arise, and the symmetry axes of objects are crucial for eliminating such ambiguities. Currently, in pose estimation, reliance on manual settings of symmetry axes decreases the accuracy of pose estimation. To address this issue, this method proposes determining the orders of symmetry axes and angles between axes based on a given rotational symmetry type or polyhedron, reducing the need for manual settings of symmetry axes. Subsequently, two key axes with the highest orders are defined and localized, then three orthogonal axes are generated based on key axes, while each symmetry axis can be computed utilizing orthogonal axes. Compared to localizing symmetry axes one by one, the key-axis-based symmetry axis localization is more efficient. To support geometric and texture symmetry, the method utilizes the ADI metric for key axis localization in geometrically symmetric objects and proposes a novel metric, ADI-C, for objects with texture symmetry. Experimental results on the LM-O and HB datasets demonstrate a 9.80% reduction in symmetry axis localization error and a 1.64% improvement in pose estimation accuracy. Additionally, the method introduces a new dataset, DSRSTO, to illustrate its performance across seven types of geometrically and texturally symmetric objects. The GitHub link for the open-source tool based on this method is https://github.com/WangYuLin-SEU/KASAL.},
  archive      = {J_TIP},
  author       = {Yulin Wang and Chen Luo},
  doi          = {10.1109/TIP.2024.3515801},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to compress face video is a crucial problem for a series of online applications, such as video chat/conference, live broadcasting and remote education. Compared to other natural videos, these face-centric videos owning abundant structural information can be compactly represented and high-quality reconstructed via deep generative models, such that the promising compression performance can be achieved. However, the existing generative face video compression schemes are faced with the inconsistency between the 3D facial motion in the physical world and the face content evolution in the 2D view. To solve this drawback, we propose a 3D-Keypoint-and-2D-Motion based generative method for Face Video Compression, namely FVC-3K2M, which can well ensure perceptual compensation and visual consistency between motion description and face reconstruction. In particular, the temporal evolution of face video can be characterized into separate 3D keypoints from the global and local perspectives, entailing great coding flexibility and accurate motion representation. Moreover, a cascade motion conversion mechanism is further proposed to internally convert 3D keypoints to 2D dense motion, enforcing the face video reconstruction to be perceptually realistic. Finally, an adaptive reference frame selection scheme is developed to enhance the adaptation of various temporal movements. Experimental results show that the proposed scheme can realize reliable video communication in the extremely limited bandwidth, e.g., 2 kbps. Compared to the state-of-the-art video coding standards and the latest face video compression methods, extensive comparisons demonstrate that our proposed scheme achieves superior compression performance in terms of multiple quality evaluations.},
  archive      = {J_TIP},
  author       = {Zhao Wang and Bolin Chen and Shurun Wang and Shiqi Wang and Yan Ye and Siwei Ma},
  doi          = {10.1109/TIP.2024.3518100},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rebalanced vision-language retrieval considering structure-aware distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language retrieval aims to search for similar instances in one modality based on queries from another modality. The primary objective is to learn cross-modal matching representations in a latent common space. Actually, the assumption underlying cross-modal matching is modal balance, where each modality contains sufficient information to represent the others. However, noise interference and modality insufficiency often lead to modal imbalance, making it a common phenomenon in practice. The impact of imbalance on retrieval performance remains an open question. In this paper, we first demonstrate that ultimate cross-modal matching is generally sub-optimal for cross-modal retrieval when imbalanced modalities exist. The structure of instances in the common space is inherently influenced when facing imbalanced modalities, posing a challenge to cross-modal similarity measurement. To address this issue, we emphasize the importance of meaningful structure-preserved matching. Accordingly, we propose a simple yet effective method to rebalance cross-modal matching by learning structure-preserved matching representations. Specifically, we design a novel multi-granularity cross-modal matching that incorporates structure-aware distillation alongside the cross-modal matching loss. While the cross-modal matching loss constraints instance-level matching, the structure-aware distillation further regularizes the geometric consistency between learned matching representations and intra-modal representations through the developed relational matching. Extensive experiments on different datasets affirm the superior cross-modal retrieval performance of our approach, simultaneously enhancing single-modal retrieval capabilities compared to the baseline models.},
  archive      = {J_TIP},
  author       = {Yang Yang and Wenjuan Xi and Luping Zhou and Jinhui Tang},
  doi          = {10.1109/TIP.2024.3518759},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rebalanced vision-language retrieval considering structure-aware distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language models (VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. We believe our method establishes a simple yet strong baseline for future STR research with VLMs.},
  archive      = {J_TIP},
  author       = {Shuai Zhao and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3512354},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting domain-adaptive semantic segmentation via knowledge distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous methods for unsupervised domain adaptation (UDA) have been proposed in semantic segmentation, achieving remarkable improvements. These methods are categorized into an adversarial learning-based approach that utilizes an additional discriminator and image translation model, and a self-supervised approach that uses a teacher model to generate pseudo labels. Among them, the self-supervised UDA approaches based on a self-training show excellent adaptability in semantic segmentation. However, erroneous estimates of the pseudo ground truths (PGTs) used in the self-training may often lead to inaccurate updates in the teacher model. Although several attempts have been made to address this issue, the teacher model updated through exponential moving average (EMA) still has a risk of propagating inaccuracies from the PGTs. Inspired by the fact that UDA shares similar principles with knowledge distillation (KD), we revisit the self-training based UDA approach from the perspective of KD and propose a novel UDA approach that employs two different teacher models. Specifically, we utilize both an EMA-updated teacher model to generate PGTs and a frozen teacher model pretrained with source data to transfer knowledge on a feature space. Since the frozen teacher model has no constraint on the model architecture unlike the EMA updated teacher model, we can effectively leverage a better representation power from the larger frozen teacher. Extensive experiments on various backbones (DeepLab-V2 [40] and DAFormer [73]) and scenarios (GTA5 → Cityscapes and SYNTHIA → Cityscapes) show that the proposed method improves segmentation performance in the target domain with its scalability. In particular, our method achieves comparable or better performance than state-of-the-arts even with a lightweight backbone.},
  archive      = {J_TIP},
  author       = {Seongwon Jeong and Jiyeong Kim and Sungheui Kim and Dongbo Min},
  doi          = {10.1109/TIP.2024.3501076},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting domain-adaptive semantic segmentation via knowledge distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MWFormer: Multi-weather image restoration using degradation-aware transformers. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.},
  archive      = {J_TIP},
  author       = {Ruoxi Zhu and Zhengzhong Tu and Jiaming Liu and Alan C. Bovik and Yibo Fan},
  doi          = {10.1109/TIP.2024.3501855},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MWFormer: Multi-weather image restoration using degradation-aware transformers},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified video reconstruction for rolling shutter and global shutter cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3504275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the general domain of video reconstruction (VR) is fragmented into different shutters spanning global shutter and rolling shutter cameras. Despite rapid progress in the state-of-the-art, existing methods overwhelmingly follow shutter-specific paradigms and cannot conceptually generalize to other shutter types, hindering the uniformity of VR models. In this paper, we propose UniVR, a versatile framework to handle various shutters through unified modeling and shared parameters. Specifically, UniVR encodes diverse shutter types into a unified space via a tractable shutter adapter, which is parameter-free and thus can be seamlessly delivered to current well-established VR architectures for cross-shutter transfer. To demonstrate its effectiveness, we conceptualize UniVR as three shutter-generic VR methods, namely Uni-SoftSplat, Uni-SuperSloMo, and Uni-RIFE. Extensive experimental results demonstrate that the pre-trained model without any fine-tuning can achieve reasonable performance even on novel shutters. After fine-tuning, new state-of-the-art performances are established that go beyond shutter-specific methods and enjoy strong generalization. The code is available at https://github.com/GitCVfb/UniVR.},
  archive      = {J_TIP},
  author       = {Bin Fan and Zhexiong Wan and Boxin Shi and Chao Xu and Yuchao Dai},
  doi          = {10.1109/TIP.2024.3504275},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unified video reconstruction for rolling shutter and global shutter cameras},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing few-shot out-of-distribution detection with pre-trained model features. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3468874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the reliability of open-world intelligent systems heavily relies on effective out-of-distribution (OOD) detection. Despite notable successes in existing OOD detection methods, their performance in scenarios with limited training samples is still suboptimal. Therefore, we first construct a comprehensive few-shot OOD detection benchmark in this paper. Remarkably, our investigation reveals that Parameter-Efficient Fine-Tuning (PEFT) techniques, such as visual prompt tuning and visual adapter tuning, outperform traditional methods like fully fine-tuning and linear probing tuning in few-shot OOD detection. Considering that some valuable information from the pre-trained model, which is conducive to OOD detection, may be lost during the fine-tuning process, we reutilize features from the pre-trained models to mitigate this issue. Specifically, we first propose a training-free approach, termed uncertainty score ensemble (USE). This method integrates feature-matching scores to enhance existing OOD detection methods, significantly narrowing the gap between traditional fine-tuning and PEFT techniques. However, due to its training-free property, this method is unable to improve in-distribution accuracy. To this end, we further propose a method called Domain-Specific and General Knowledge Fusion (DSGF) to improve few-shot OOD detection performance and ID accuracy under different fine-tuning paradigms. Experiment results demonstrate that DSGF enhances few-shot OOD detection across different fine-tuning strategies, shot settings, and OOD detection methods. We believe our work can provide the research community with a novel path to leveraging large-scale visual pre-trained models for addressing FS-OOD detection. The code will be released.},
  archive      = {J_TIP},
  author       = {Jiuqing Dong and Yifan Yao and Wei Jin and Heng Zhou and Yongbin Gao and Zhijun Fang},
  doi          = {10.1109/TIP.2024.3468874},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing few-shot out-of-distribution detection with pre-trained model features},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised temporal correspondence learning for unified video object removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3340605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object removal aims at erasing a target object in the entire video and filling holes with plausible contents, given an object mask in the first frame as input. Existing solutions mostly break down the task into (supervised) mask tracking and (self-supervised) video completion, and then separately tackle them with tailored designs. In this paper, we introduce a new setup, coined as unified video object removal , where mask tracking and completion are addressed within a unified framework. Despite introducing more challenges, the setup is promising for future practical usage. We embrace the observation that these two sub-tasks have strong inherent connections in terms of pixel-level temporal correspondence. Making full use of the connections could be beneficial considering the complexity of both algorithm and deployment. We propose a single network linking the two sub-tasks by inferring temporal correspondences across multiple frames, i.e ., correspondences between valid-valid (V-V) pixel pairs for mask tracking and correspondences between valid-hole (V-H) pixel pairs for video completion. Thanks to the unified setup, the network can be learned end-to-end in a totally unsupervised fashion without any annotations. We demonstrate that our method can generate visually pleasing results and perform favorably against existing separate solutions in realistic test cases.},
  archive      = {J_TIP},
  author       = {Zhongdao Wang and Jinglu Wang and Xiao Li and Ya-Li Li and Yan Lu and Shengjin Wang},
  doi          = {10.1109/TIP.2023.3340605},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised temporal correspondence learning for unified video object removal},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Field-of-view IoU for object detection in 360° images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3296013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° cameras have gained popularity over the last few years. In this paper, we propose two fundamental techniques—Field-of-View IoU (FoV-IoU) and 360Augmentation for object detection in 360° images. Although most object detection neural networks designed for perspective images are applicable to 360° images in equirectangular projection (ERP) format, their performance deteriorates owing to the distortion in ERP images. Our method can be readily integrated with existing perspective object detectors and significantly improves the performance. The FoV-IoU computes the intersection-over-union of two Field-of-View bounding boxes in a spherical image which could be used for training, inference, and evaluation while 360Augmentation is a data augmentation technique specific to 360° object detection task which randomly rotates a spherical image and solves the bias due to the sphere-to-plane projection. We conduct extensive experiments on the 360° indoor dataset with different types of perspective object detectors and show the consistent effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Miao Cao and Satoshi Ikehata and Kiyoharu Aizawa},
  doi          = {10.1109/TIP.2023.3296013},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Field-of-view IoU for object detection in 360° images},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3273451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The end-to-end image fusion framework has achieved promising performance, with dedicated convolutional networks aggregating the multi-modal local appearance. However, long-range dependencies are directly neglected in existing CNN fusion approaches, impeding balancing the entire image-level perception for complex scenario fusion. In this paper, therefore, we propose an infrared and visible image fusion algorithm based on the transformer module and adversarial learning. Inspired by the global interaction power, we use the transformer technique to learn the effective global fusion relations. In particular, shallow features extracted by CNN are interacted in the proposed transformer fusion module to refine the fusion relationship within the spatial scope and across channels simultaneously. Besides, adversarial learning is designed in the training process to improve the output discrimination via imposing competitive consistency from the inputs, reflecting the specific characteristics in infrared and visible images. The experimental performance demonstrates the effectiveness of the proposed modules, with superior improvement against the state-of-the-art, generalising a novel paradigm via transformer and adversarial learning in the fusion task.},
  archive      = {J_TIP},
  author       = {Dongyu Rao and Tianyang Xu and Xiao-Jun Wu},
  doi          = {10.1109/TIP.2023.3273451},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured attention composition for temporal action localization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2022.3180925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.},
  archive      = {J_TIP},
  author       = {Le Yang and Junwei Han and Tao Zhao and Nian Liu and Dingwen Zhang},
  doi          = {10.1109/TIP.2022.3180925},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structured attention composition for temporal action localization},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational structured attention networks for deep visual representation learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2021.3137647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have enabled major progresses in addressing pixel-level prediction tasks such as semantic segmentation, depth estimation, surface normal prediction and so on, benefiting from their powerful capabilities in visual representation learning. Typically, state of the art models integrate attention mechanisms for improved deep feature representations. Recently, some works have demonstrated the significance of learning and combining both spatial- and channel-wise attentions for deep feature refinement. In this paper, we aim at effectively boosting previous approaches and propose a unified deep framework to jointly learn both spatial attention maps and channel attention vectors in a principled manner so as to structure the resulting attention tensors and model interactions between these two types of attentions. Specifically, we integrate the estimation and the interaction of the attentions within a probabilistic representation learning framework, leading to VarIational STructured Attention networks (VISTA-Net). We implement the inference rules within the neural network, thus allowing for end-to-end learning of the probabilistic and the CNN front-end parameters. As demonstrated by our extensive empirical evaluation on six large-scale datasets for dense visual prediction, VISTA-Net outperforms the state-of-the-art in multiple continuous and discrete prediction tasks, thus confirming the benefit of the proposed approach in joint structured spatial-channel attention estimation for deep representation learning. The code is available at https://github.com/ygjwd12345/VISTA-Net.},
  archive      = {J_TIP},
  author       = {Guanglei Yang and Paolo Rota and Xavier Alameda-Pineda and Dan Xu and Mingli Ding and Elisa Ricci},
  doi          = {10.1109/TIP.2021.3137647},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational structured attention networks for deep visual representation learning},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). Stacked deconvolutional network for semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2019.2895460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and bring the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which enhances the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-ofthe- art results on four datasets, including PASCAL VOC 2012, CamVid, GATECH, COCO Stuff. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.},
  archive      = {J_TIP},
  author       = {Jun Fu and Jing Liu and Yuhang Wang and Jin Zhou and Changyong Wang and Hanqing Lu},
  doi          = {10.1109/TIP.2019.2895460},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stacked deconvolutional network for semantic segmentation},
  year         = {2019},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Monocular depth estimation with augmented ordinal depth relationships. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2877944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing algorithms for depth estimation from single monocular images need large quantities of metric groundtruth depths for supervised learning. We show that relative depth can be an informative cue for metric depth estimation and can be easily obtained from vast stereo videos. Acquiring metric depths from stereo videos is sometimes impracticable due to the absence of camera parameters. In this paper, we propose to improve the performance of metric depth estimation with relative depths collected from stereo movie videos using existing stereo matching algorithm.We introduce a new “Relative Depth in Stereo” (RDIS) dataset densely labelled with relative depths. We first pretrain a ResNet model on our RDIS dataset. Then we finetune the model on RGB-D datasets with metric ground-truth depths. During our finetuning, we formulate depth estimation as a classification task. This re-formulation scheme enables us to obtain the confidence of a depth prediction in the form of probability distribution. With this confidence, we propose an information gain loss to make use of the predictions that are close to ground-truth. We evaluate our approach on both indoor and outdoor benchmark RGB-D datasets and achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Yuanzhouhan Cao and Tianqi Zhao and Ke Xian and Chunhua Shen and Zhiguo Cao and Shugong Xu},
  doi          = {10.1109/TIP.2018.2877944},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Monocular depth estimation with augmented ordinal depth relationships},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Deep active learning with contaminated tags for image aesthetics assessment. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2828326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic quality assessment has becoming an indispensable technique that facilitates a variety of image applications, e.g., photo retargeting and non-realistic rendering. Conventional approaches suffer from the following limitations: 1) the inefficiency of semantically describing images due to the inherent tag noise and incompletion, 2) the difficulty of accurately reflecting how humans actively perceive various regions inside each image, and 3) the challenge of incorporating the aesthetic experiences of multiple users. To solve these problems, we propose a novel semi-supervised deep active learning (SDAL) algorithm, which discovers how humans perceive semantically important regions from a large quantity of images partially assigned with contaminated tags. More specifically, as humans usually attend to the foreground objects before understanding them, we extract a succinct set of BING (binarized normed gradients) [60]-based object patches from each image. To simulate human visual perception, we propose SDAL which hierarchically learns human gaze shifting path (GSP) by sequentially linking semantically important object patches from each scenery. Noticeably, SDLA unifies the semantically important regions discovery and deep GSP feature learning into a principled framework, wherein only a small proportion of tagged images are adopted. Moreover, based on the sparsity penalty, SDLA can optimally abandon the noisy or redundant low-level image features. Finally, by leveraging the deeply-learned GSP features, a probabilistic model is developed for image aesthetics assessment, where the experience of multiple professional photographers can be encoded. Besides, auxiliary quality-related features can be conveniently integrated into our probabilistic model. Comprehensive experiments on a series of benchmark image sets have demonstrated the superiority of our method. As a byproduct, eye tracking experiments have shown that GSPs generated by our SDAL are about 93% consistent with real human gaze shifting paths.},
  archive      = {J_TIP},
  author       = {Zhenguang Liu and Zepeng Wang and Yiyang Yao and Luming Zhang and Ling Shao},
  doi          = {10.1109/TIP.2018.2828326},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep active learning with contaminated tags for image aesthetics assessment},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2010). Exploring duplicated regions in natural images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2010.2046599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Duplication of image regions is a common method for manipulating original images, using typical software like Adobe Photoshop, 3DS MAX, etc. In this study, we propose a duplication detection approach that can adopt two robust features based on discrete wavelet transform (DWT) and kernel principal component analysis (KPCA). Both schemes provide excellent representations of the image data for robust block matching. Multiresolution wavelet coefficients and KPCA-based projected vectors corresponding to image-blocks are arranged into a matrix for lexicographic sorting. Sorted blocks are used for making a list of similar point-pairs and for computing their offset frequencies. Duplicated regions are then segmented by an automatic technique that refines the list of corresponding point-pairs and eliminates the minimum offset-frequency threshold parameter in the usual detection method. A new technique that extends the basic algorithm for detecting Flip and Rotation types of forgeries is also proposed. This method uses global geometric transformation and the labeling technique to indentify the mentioned forgeries. Experiments with a good number of natural images show very promising results, when compared with the conventional PCA-based approach. A quantitative analysis indicate that the wavelet-based feature outperforms PCA- or KPCA-based features in terms of average precision and recall in the noiseless, or uncompressed domain, while KPCA-based feature obtains excellent performance in the additive noise and lossy JPEG compression environments.},
  archive      = {J_TIP},
  author       = {M. Bashar and K. Noda and N. Ohnishi and K. Mori},
  doi          = {10.1109/TIP.2010.2046599},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring duplicated regions in natural images},
  year         = {2010},
}
</textarea>
</details></li>
</ul>

</body>
</html>
