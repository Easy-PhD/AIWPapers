<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 65</h2>
<ul>
<li><details>
<summary>
(2025). Zero-shot image recognition via learning dual prototype accordance across meta-domains. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen categories. However, existing methods often struggle with the persistent semantic gap caused by limited semantic descriptors and rigid visual feature modeling. In particular, modeling pre-defined class-level attribute descriptions as ground truth hinders effective semantic-to-visual alignment to some extent. To mitigate these issues, we propose the Bilateral-guided Prototype Refinement Network(BPRN), a novel ZSL framework designed to refine dual prototypes across meta-domains of varying scales. Specifically, we first disentangle the relationships among class-level semantics and use them to generate corresponding pseudo-visual prototypes. Then, by leveraging distribution information across dual prototypes in different meta-domains, BPRN achieves bidirectional calibration between visual-to-semantic and semantic-to-visual modalities. Finally, a synthesized class-level representation derived from the refined dual prototypes is employed for inference, instead of relying on a single prototype. Extensive experiments conducted on five widely-used ZSL benchmark datasets demonstrate that BPRN consistently achieves competitive or even superior performance. Specifically, in the GZSL scenario, BPRN shows improvements of 2.1%, 7.3%, 6.1%, and 4.8% on AWA1, AWA2, SUN, and aPY, respectively, compared to existing embedding-based ZSL methods. Ablation studies and visualization analyses further validate the effectiveness of the proposed components.},
  archive      = {J_TIP},
  author       = {Bocheng Ren and Yuanyuan Yi and Qingchen Zhang and Debin Liu},
  doi          = {10.1109/TIP.2025.3607588},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image recognition via learning dual prototype accordance across meta-domains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (and vice versa), opening new possibilities in image processing. However, the growing diversity of camera models, particularly in mobile devices, renders the development of individual ISPs unsustainable due to their limited versatility and adaptability across varied camera systems. In this paper, we introduce Uni-ISP, a novel pipeline that unifies ISP learning for diverse mobile cameras, delivering a highly accurate and adaptable processor. The core of Uni-ISP is leveraging device-aware embeddings through learning forward/inverse ISPs and its special training scheme. By doing so, Uni-ISP not only improves the performance of forward and inverse ISPs but also unlocks new applications previously inaccessible to conventional learned ISPs. To support this work, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphone cameras. Extensive experiments validate Uni-ISP’s accuracy in learning forward and inverse ISPs (with improvements of +2.4dB/1.5dB PSNR), versatility in enabling new applications, and adaptability to new camera models.},
  archive      = {J_TIP},
  author       = {Lingen Li and Mingde Yao and Xingyu Meng and Muquan Yu and Tianfan Xue and Jinwei Gu},
  doi          = {10.1109/TIP.2025.3607617},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient open vocabulary 3D scene understanding based on semantic probability. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D scene understanding methods heavily depend on 3D annotation and training, which allow for the identification of seen classes but struggle to recognize unseen classes. In this paper, we leverage the open vocabulary inference capabilities of pre-trained models, enabling the encoding of open vocabulary concepts. However, unlike existing open vocabulary 3D scene understanding methods, we propose a framework based on semantic probability. This innovation significantly reduces computational cost and is compatible with state-of-the-art two-stage 2D pre-trained models. Specifically, we align the text features from the CLIP model with the pixel features from the 2D pre-trained models, inferring semantic probability of image pixels based on similarity and projecting it onto 3D points. Subsequently, we introduce a point cloud pairs semantic fusion method to merge the point clouds, reducing the semantic probability of erroneous 3D points. Based on probability scores, we achieve 3D semantic segmentation on open vocabularies without any supervision or training. In addition, the semantic probability of 3D points can serve as pseudo-labels for 3D distillation, and the geometric features of the 3D scene can be exploited to improve the segmentation performance. Experimental results demonstrate that the proposed method exhibits competitive performance on publicly available benchmark datasets, including ScanNet, Matterport3D, and nuScenes.},
  archive      = {J_TIP},
  author       = {Lingfeng Shen and Xiaoyao Wei and Gang Pan and Qian Zheng and Yanlong Cao},
  doi          = {10.1109/TIP.2025.3607643},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cost-efficient open vocabulary 3D scene understanding based on semantic probability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive anchor-guided representation learning for efficient multi-view subspace clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view Subspace Clustering (MVSC) effectively aggregating multiple data sources to promise clustering performance. Recently, various anchor-based variants have been introduced to effectively alleviate the computation complexity of MVSC. Although satisfactory advancement has been achieved, existing methods either independently learn anchor matrices and their anchor representations or learn a consensus anchor matrix and unified anchor representation, failing to capture both consistency and complementary information simultaneously. In addition, the time complexity of obtaining clustering results by applying Singular Value Decomposition (SVD) on the anchor representation matrix remains high. To tackle the above problems, we propose an Adaptive Anchor-guided Representation Learning for Efficient Multi-view Subspace Clustering (A2RL-EMVSC) framework, which integrates consensus anchors learning, anchor-guided representation learning and matrix factorization to enhance clustering performance and scalability. Technically, the proposed method learns view-specific anchor representation matrices by consensus anchors guidance, which simultaneously exploit consistency and complementary information. Moreover, by applying matrix decomposition to the view-specific anchor representation matrices, clustering results can be achieved with linear time complexity. Extensive experiments on ten challenging multi-view datasets show that the proposed method can improve the effectiveness and superiority of clustering compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mengjiao Zhang and Xinwang Liu and Tianhao Han and Xiaofeng Qu and Sijie Niu},
  doi          = {10.1109/TIP.2025.3607587},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive anchor-guided representation learning for efficient multi-view subspace clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a unified model for surface anomaly detection remains challenging due to significant variations across product categories. Recent feature editing methods, as a branch of image reconstruction, mitigate the over-generalization of auto-encoders that leads to accurate anomaly reconstruction. However, these methods are only suited for texture-category products and have significant limitations in being generalized to other categories. In this article, we propose a multi-category anomaly editing network with a dual-branch training approach: one branch processes defect-free images (normal branch), while the other handles synthetic anomaly images (anomaly branch). Specifically, the paired samples are first fed into the multi-category anomaly feature editing based auto-encoder (MCAFE-AE) to perform image reconstruction and inpainting. In the normal branch, we propose a dual-entropy constrained deep embedded clustering module (DEC-DECM) to promote a more compact and orderly distribution of normal latent features, while avoiding trivial clustering solutions. Based on the clustering results, we further design a patch-based adaptive thresholding (PAT) strategy to adaptively calculate the threshold representing the central boundary of the cluster center for each local patch, thereby enabling the model to detect anomalies. Then, in the anomaly branch, we propose a multi-category anomaly feature editing module (MCAFEM) to identify anomalies in synthetic images and apply a category-oriented feature editing strategy to transform detected anomaly features into normal ones, thereby suppressing the reconstruction of anomalies. After completing the image reconstruction and inpainting, the input images from both branches and their respective output images are concatenated and fed into the correlation exploration and voxel-level attention based prediction network (CEVA-Net) for anomaly segmentation. The network is integrated with our proposed correlation-dependency exploration and voxel-level attention refinement module (CDE-VARM) and generates precise anomaly maps under the guidance of the bidirectional-path feature fusion (BPFF) and deep supervised learning (DSL). Extensive experiments on three datasets show that our method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruifan Zhang and Hai-Miao Hu},
  doi          = {10.1109/TIP.2025.3607638},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion has emerged as a prominent research area in computer vision. However, little attention has been paid to complex scenes fusion, leading to sub-optimal results under interference. To fill this gap, we propose a unified framework for infrared and visible images fusion in complex scenes, termed UMCFuse. Specifically, we classify the pixels of visible images from the degree of scattering of light transmission, allowing us to separate fine details from overall intensity. Maintaining a balance between interference removal and detail preservation is essential for the generalization capacity of the proposed method. Therefore, we propose an adaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse the energy features from different modalities by analyzing them from multiple directions. Extensive fusion experiments on real and synthetic complex scenes datasets cover adverse weather conditions, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection, and depth estimation, consistently indicate the superiority of the proposed method compared with the recent representative methods. Our code is available at https://github.com/ixilai/UMCFuse.},
  archive      = {J_TIP},
  author       = {Xilai Li and Xiaosong Li and Tianshu Tan and Huafeng Li and Tao Ye},
  doi          = {10.1109/TIP.2025.3607623},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image change detection (HSI-CD) benefits from HSIs with continuous spectral bands, which uniquely enables the analysis of more subtle changes. Existing methods have achieved desirable performance relying on multi-temporal homogenous HSIs over the same region, which is generally difficult to obtain in real scenes. HSI-RGB multimodal CD overcomes the constraint of limited HSI availability by incorporating another temporal RGB data, and the combination of advantages within different modalities enhances the robustness of detection results. Nevertheless, due to the different imaging mechanisms between two modalities, existing HSI CD methods cannot be directly applied. In this paper, we propose a cycle translation-based collaborative training (co-training) for HSI-RGB multimodal CD, which achieves cross-modal mutual guidance to collaboratively learn complementary difference information from diverse modalities for identifying changes. Specifically, a cross-modal guided CycleGAN-based image translation module is designed to implement bi-directional image translation, which mitigates modal difference and enables the extraction of information related to land cover changes. Then, a spatial-spectral interactive co-training CD module is proposed to achieve iterative interaction between cross-modal information, which jointly extracts the multimodal difference features to generate the final results. The proposed method outperforms several leading CD methods in extensive experiments carried out on both real and synthetic datasets. In addition, a new public HSI-RGB multimodal dataset along with our code are available at https://github.com/Jiahuiqu/CT2Net.},
  archive      = {J_TIP},
  author       = {Wenqian Dong and Junying Ren and Song Xiao and Leyuan Fang and Jiahui Qu and Yunsong Li},
  doi          = {10.1109/TIP.2025.3607609},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral texture metrology based on distance measures in an information-theoretic framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work sought to instil metrology in existing hyperspectral texture feature extraction methods. Specifically, we propose distance-based expressions of graylevel cooccurrence matrix (GLCM), local binary pattern (LBP), and Gabor filtering directly computable for hyperspectral images without any pre- or post-processing. At the core of our proposition is Radical of Extended Mean Information for Discrimination (REID), a novel spectral distance with information-theoretic roots. Respecting the physics of spectrum as continuous function of wavelengths, REID is mathematically decomposable into spectral direction and spectral magnitude distances. The resulted feature calculations are fullband (utilizing all wavelengths), yet lightweight and fully interpretable. A similarity measure based on information theory is also justified. Their efficiency is demonstrated in the context of texture classification, content-based image retrieval, and cancer detection in which they consistently outperform existing computations based on dimensionally reduced space using PCA, ICA, and NMF. The propositions could be potentially integrated into machine/deep learning systems towards explainable AI (XAI).},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Jie Chen and Susanto Rahardja},
  doi          = {10.1109/TIP.2025.3608667},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on distance measures in an information-theoretic framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient and structure consistency in multimodal emotion recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is a task that integrates text, visual, and audio data to holistically infer an individual’s emotional state. Existing research predominantly focuses on exploiting modality-specific cues for joint learning, often ignoring the differences between multiple modalities under common goal learning. Due to multimodal heterogeneity, common goal learning inadvertently introduces optimization biases and interaction noise. To address above challenges, we propose a novel approach named Gradient and Structure Consistency (GSCon). Our strategy operates at both overall and individual levels to consider balance optimization and effective interaction respectively. At the overall level, to avoid the optimization suppression of a modality on other modalities, we construct a balanced gradient direction that aligns each modality’s optimization direction, ensuring unbiased convergence. Simultaneously, at the individual level, to avoid the interaction noise caused by multimodal alignment, we align the spatial structure of samples in different modalities. The spatial structure of the samples will not differ due to modal heterogeneity, achieving effective inter-modal interaction. Extensive experiments on multimodal emotion recognition and multimodal intention understanding datasets demonstrate the effectiveness of the proposed method. Code is available at https://github.com/ShiQingHongYa/GSCon.},
  archive      = {J_TIP},
  author       = {QingHongYa Shi and Mang Ye and Wenke Huang and Bo Du and Xiaofen Zong},
  doi          = {10.1109/TIP.2025.3608664},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gradient and structure consistency in multimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised domain adaptation methods rely on explicitly or implicitly aligning the features of source and target domains to construct a domain-invariant space, often using entropy minimization to reduce uncertainty and confusion. However, this approach faces two challenges: 1) Explicit alignment reduces discriminability, while implicit alignment risks pseudo-label noise, making it hard to balance structure preservation and alignment. 2) Sole reliance on entropy minimization can lead to trivial solutions in UDA, where all samples collapse into a single class. To address these issues, we propose Dual-Space Topological Isomorphism and Maximization of Predictive Diversity (DTI-MPD). Topological isomorphism is a continuous, bijective mapping that preserves the topological properties of two spaces, ensuring the global structure and relationships of data remain intact during alignment. Our method aligns source and target domain data in two independent spaces while balancing the effects of entropy minimization through predictive diversity maximization. The core of dual-space topological isomorphism lies in establishing a reversible correspondence between the source and target domains, avoiding information loss during alignment and preserving the global structural and topological characteristics of the data. Meanwhile, predictive diversity maximization mitigates the class collapse caused by entropy minimization, ensuring a more balanced predictive distribution across categories. This approach effectively overcomes the aforementioned issues, enabling better adaptation to new data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets, validating its effectiveness.},
  archive      = {J_TIP},
  author       = {Mengru Wang and Jinglei Liu},
  doi          = {10.1109/TIP.2025.3608670},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pull pole points to text contour by magnetism: A real-time scene text detector. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text reading plays a crucial role in scene understanding. As its precondition task, scene text detection has garnered increasing interest from researchers. Segmentation-based text detection methods have gained prominence due to their adaptable pixel-level predictions. Many existing methods predict the shrink mask and utilize the Vatti clipping algorithm to reconstruct text contours. However, the shrink mask only focuses on the global geometry feature and shrinks the same distance everywhere, which neglects local contour information and disrupts the instance shape feature. In addition, the post-processing based on the Vatti clipping algorithm heavily relies on the predictions and is relatively complex, causing suboptimal performance in both detection accuracy and efficiency. To address the above problems, we propose an efficient and effective method named Magnetic Text Detector (MTD), inspired by magnetism. It is constructed by a text representation method flexible mask (FM) and a magnetic pull module (MPM). Unlike the shrink mask and concentric mask, the former concerns the local contours and shrinks unfixed distances on different positions, which avoids the truncation issue while preserving distinctiveness from the text regions. The latter generates magnetic fields and pulls pole points of FM to the text contour by magnetism. This allows accurate reconstruction of text contours, even when predictions deviate from the actual text severely, while saving 50% of the post-processing time approximately. Several ablation studies verify the effectiveness of the proposed FM and MPM. Extensive experiments show that our MTD achieves state-of-the-art (SOTA) methods on multiple datasets from different scenes. The code is available at https://github.com/fengmulin/MTD.},
  archive      = {J_TIP},
  author       = {Xu Han and Chuang Yang and Qi Wang},
  doi          = {10.1109/TIP.2025.3609196},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pull pole points to text contour by magnetism: A real-time scene text detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task.},
  archive      = {J_TIP},
  author       = {Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong},
  doi          = {10.1109/TIP.2025.3609185},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NanoHTNet: Nano human topology network for efficient 3D human pose estimation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models. A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging. To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task. First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features. Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence. Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly. Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology. By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements. Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano. Code and models are available at https://github.com/vefalun/NanoHTNet.},
  archive      = {J_TIP},
  author       = {Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li},
  doi          = {10.1109/TIP.2025.3608662},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NanoHTNet: Nano human topology network for efficient 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-driven global-local fusion transformer for image super-resolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) has seen remarkable progress with the emergence of transformer-based architectures. However, due to the high computational cost, many existing transformer-based SR methods limit their attention to local windows, which hinders their ability to model long-range dependencies and global structures. To address these challenges, we propose a novel SR framework named Semantic-Driven Global-Local Fusion Transformer (SGLFT). The proposed model enhances the receptive field by combining a Hybrid Window Transformer (HWT) and a Scalable Transformer Module (STM) to jointly capture local textures and global context. To further strengthen the semantic consistency of reconstruction, we introduce a Semantic Extraction Module (SEM) that distills high-level semantic priors from the input. These semantic cues are adaptively integrated with visual features through an Adaptive Feature Fusion Semantic Integration Module (AFFSIM). Extensive experiments on standard benchmarks demonstrate the effectiveness of SGLFT in producing visually faithful and structurally consistent SR results. The code will be available at https://github.com/kbzhang0505/SGLFT.},
  archive      = {J_TIP},
  author       = {Kaibing Zhang and Zhouwei Cheng and Xin He and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3609106},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic-driven global-local fusion transformer for image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical color constancy via efficient spectral feature extraction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an empirical investigation into illuminant estimation using multi-spectral images. Our study emphasizes two key contributions: (1) the utilization of the estimated multi-spectral images and (2) the incorporation of a hierarchical structure. Firstly, exploiting multi-spectral images proves to have a positive influence on illuminant estimation, particularly in scenarios characterized by monochromatic images where conventional color constancy methods face challenges. Our experimental results vividly illustrate the effectiveness of leveraging spectral information in enhancing illuminant estimation. Secondly, the adoption of a hierarchical structure stems from the need for spatial invariance in the task of estimating a global illuminant. To further enhance the performance of the hierarchical structure, we employ a contrastive loss applied to different scaled outputs. This approach demonstrates remarkable effectiveness on our custom dataset, showcasing superior performance compared to the existing methods. In addition, we extend the evaluation to the widely recognized NUS-8 dataset, where the proposed method showcases a notable 26.7% relative improvement over the previous state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Dong-Keun Han and Dong-Hoon Kang and Jong-Ok Kim},
  doi          = {10.1109/TIP.2025.3607631},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical color constancy via efficient spectral feature extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations. Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: https://github.com/fidshu/SRSNet.},
  archive      = {J_TIP},
  author       = {Bingkun Nian and Fenghe Tang and Jianrui Ding and Jie Yang and Zhonglong Zheng and Shaohua Kevin Zhou and Wei Liu},
  doi          = {10.1109/TIP.2025.3607624},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No-reference image quality assessment leveraging GenAI images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3610238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based methods have made significant progress on the image quality assessment problem; however, challenges remain arising from the lack of annotated, real-world training data and consequent poor generalization ability. Towards addressing these challenges, we propose a no-reference image quality assessment (NR-IQA) method based on generative AI (GenAI) images. Specifically, we use GenAI images as reference images, employing a cold diffusion model to generate distorted images of four different distortion types, and we label these distorted images using a full-reference model, thereby making it possible to construct a large-scale pre-training dataset. We use this resource generation method to facilitate NR-IQA model building. We deploy a Multi-scale Cross Attention Block (MCAB) and a Scale Simple Attention Module (SSAM) to enhance feature representation by extracting multi-scale feature information from both the channel and spatial dimensions that are predictive of image quality. Extensive experiments on eight public databases demonstrate that the proposed method achieves state-of-the-art (SOTA) performance. A public release of all the codes associated with this work will be made available on GitHub.},
  archive      = {J_TIP},
  author       = {Qingbing Sang and Qian Li and Lixiong Liu and Zhaohong Deng and Xiaojun Wu and Alan C. Bovik},
  doi          = {10.1109/TIP.2025.3610238},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference image quality assessment leveraging GenAI images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Volume fusion-based self-supervised pretraining for 3D medical image segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3610249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning models for medical image segmentation is often limited in scenarios where training data or annotations are limited. Self-Supervised Learning (SSL) is an appealing solution for this dilemma due to its feature learning ability from a large amount of unannotated images. Existing SSL methods have focused on pretraining either an encoder for global feature representation or an encoder-decoder structure for image restoration, where the gap between pretext and downstream tasks limits the usefulness of pretrained decoders in downstream segmentation. In this work, we propose a novel SSL strategy named Volume Fusion (VolF) for pretraining 3D segmentation models. It minimizes the gap between pretext and downstream tasks by introducing a pseudo-segmentation pretext task, where two sub-volumes are fused by a discretized block-wise fusion coefficient map. The model takes the fused result as input and predicts the category of fusion coefficient for each voxel, which can be trained with standard supervised segmentation loss functions without manual annotations. Experiments with an abdominal CT dataset for pretraining and both in-domain and out-domain downstream datasets showed that VolF led to large performance gain from training from scratch with faster convergence speed, and outperformed several state-of-the-art SSL methods. In addition, it is general to different network structures, and the learned features have high generalizability to different body parts and modalities.},
  archive      = {J_TIP},
  author       = {Guotai Wang and Jia Fu and Jianghao Wu and Xiangde Luo and Yubo Zhou and Xinglong Liu and Kang Li and Jingsheng Lin and Baiyong Shen and Shaoting Zhang},
  doi          = {10.1109/TIP.2025.3610249},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Volume fusion-based self-supervised pretraining for 3D medical image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving visual localization with event cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of client-server localization, where edge device users communicate visual data with the service provider for locating oneself against a pre-built 3D map. This localization paradigm is a crucial component for location-based services in AR/VR or mobile applications, as it is not trivial to store large-scale 3D maps and process fast localization on resource-limited edge devices. Nevertheless, conventional client-server localization systems possess numerous challenges in computational efficiency, robustness, and privacy-preservation during data transmission. Our work aims to jointly solve these challenges with a localization pipeline based on event cameras. By using event cameras, our system consumes low energy and maintains small memory bandwidth. Then during localization, we propose applying event-to-image conversion and leverage mature image-based localization, which achieves robustness even in low-light or fast-moving scenes. To further enhance privacy protection, we introduce privacy protection techniques at two levels. Network level protection aims to hide the entire user’s view in private scenes using a novel split inference approach, while sensor level protection aims to hide sensitive user details such as faces with light-weight filtering. Both methods involve small client-side computation and localization performance loss, while significantly mitigating the feeling of insecurity as revealed in our user study. We thus project our method to serve as a building block for practical location-based services using event cameras.},
  archive      = {J_TIP},
  author       = {Junho Kim and Young Min Kim and Ramzi Zahreddine and Weston A. Welge and Gurunandan Krishnan and Sizhuo Ma and Jian Wang},
  doi          = {10.1109/TIP.2025.3607640},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Privacy-preserving visual localization with event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale segmentation-guided fusion network for hyperspectral image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution Neural Networks (CNNs) have demonstrated strong feature extraction capabilities in Euclidean spaces, achieving remarkable success in hyperspectral image (HSI) classification tasks. Meanwhile, Graph convolution networks (GCNs) effectively capture spatial-contextual characteristics by leveraging correlations in non-Euclidean spaces, uncovering hidden relationships to enhance the performance of HSI classification (HSIC). Methods combining GCNs with CNNs have achieved excellent results. However, existing GCN methods primarily rely on single-scale graph structures, limiting their ability to extract features across different spatial ranges. To address this issue, this paper proposes a multiscale segmentation-guided fusion network (MS2FN) for HSIC. This method constructs pixel-level graph structures based on multiscale segmentation data, enabling the GCN to extract features across various spatial ranges. Moreover, effectively utilizing features extracted from different spatial scales is crucial for improving classification performance. This paper adopts distinct processing strategies for different feature types to enhance feature representation. Comparative experiments demonstrate that the proposed method outperforms several state-of-the-art (SOTA) approaches in accuracy. The source code will be released at https://github.com/shengrunhua/MS2FN.},
  archive      = {J_TIP},
  author       = {Hongmin Gao and Runhua Sheng and Yuanchao Su and Zhonghao Chen and Shufang Xu and Lianru Gao},
  doi          = {10.1109/TIP.2025.3611146},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiscale segmentation-guided fusion network for hyperspectral image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward better than pseudo-reference in underwater image enhancement. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since degraded underwater images are not always accompanied with distortion-free counterparts in real-world situations, existing underwater image enhancement (UIE) methods are mostly learned on a paired set consisting of raw underwater images and their corresponding pseudo-reference labels. Although the existing UIE datasets manually select the best model-generated results as pseudo-references, such pseudo-reference labels do not always exhibit perfect visual quality. Therefore, it would be interesting to investigate whether it is possible to break through the performance bottleneck of UIE networks trained with imperfect pseudo-references. Motivated by these facts, this paper focuses on innovating more advanced loss functions rather than designing more complex network architectures. Specifically, a plug-and-play hybrid Performance SurPassing Loss (PSPL), consisting of a Quality Score Comparison Loss (QSCL) and a scene Depth-aware Unpaired Contrastive Loss (DUCL), is formulated to guide the training of UIE network. Functionally, QSCL aims to guide the UIE network to generate enhanced results with better visual quality than pseudo-references by constructing image quality score comparison losses from both image-level and region-level. Nevertheless, only using QSCL cannot guarantee obtaining desired results for those severely degraded distant regions. Therefore, we also design a tailored DUCL to handle this challenging issue from the scene depth perspective, i.e., DUCL encourages the distant regions of the enhanced results to be closer to the high-quality nearby regions (pull) and far away from the low-quality distant regions (push) of the pseudo-references. Extensive experimental results demonstrate the advantage of using PSPL over the state-of-the-arts even with an extremely simple and lightweight UIE network. The source code will be released at https://github.com/lewis081/PSPL.},
  archive      = {J_TIP},
  author       = {Yi Liu and Qiuping Jiang and Xingbo Li and Ting Luo and Wenqi Ren},
  doi          = {10.1109/TIP.2025.3611138},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward better than pseudo-reference in underwater image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOVER: Hyperbolic video-text retrieval. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-text retrieval is a crucial task in numerous computer vision applications. In this paper, we focus on video-text retrieval involving complex action compositions, where a single video encompasses multiple primitive actions such as “sitting up”, “opening door”, “cooking food”, and “eating.” Despite the common occurrences in real-world scenarios, such action-compositional videos have received limited research attention, often leading to significant performance degradations in existing retrieval methods. To address this challenge, we present Hyperbolic Video-tExt Retrieval (HOVER), which models the hierarchical semantic relationships between videos and texts by embedding them in a low-dimensional hyperbolic space. Since hyperbolic space provides a geometric prior that naturally aligns with hierarchical data, it allows for more efficient and generalizable representations of video-text semantic hierarchies. HOVER first longitudinally decomposes each video into a hierarchical action tree, where primitive mono-actions are represented as leaf nodes and increasingly complex action compositions as parent nodes. The semantic structures and temporal dependencies of videos/texts are then encoded in hyperbolic space by exploiting hyperbolic distance, norm, and relative cosine similarity. Experimental results show that HOVER significantly outperforms traditional Euclidean-based methods, particularly in scenarios with limited training labels, achieving a notable performance improvement of 28.83%. Additionally, the hyperbolic video-text embeddings learned by HOVER demonstrate strong generalization across new datasets containing videos with varying levels of action complexity. The source code is available at https://github.com/shi-rq/HOVER.},
  archive      = {J_TIP},
  author       = {Jun Wen and Yufeng Chen and Ruiqi Shi and Wei Ji and Menglin Yang and Difei Gao and Junsong Yuan and Roger Zimmermann},
  doi          = {10.1109/TIP.2025.3611174},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOVER: Hyperbolic video-text retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCE-GAN: A generative adversarial network for point cloud attribute quality enhancement based on optimal transport. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud compression significantly reduces data volume but sacrifices reconstruction quality, highlighting the need for advanced quality enhancement techniques. Most existing approaches focus primarily on point-to-point fidelity, often neglecting the importance of perceptual quality as interpreted by the human visual system. To address this issue, we propose a generative adversarial network for point cloud quality enhancement (PCE-GAN), grounded in optimal transport theory, with the goal of simultaneously optimizing both data fidelity and perceptual quality. The generator consists of a local feature extraction (LFE) unit, a global spatial correlation (GSC) unit and a feature squeeze unit. The LFE unit uses dynamic graph construction and a graph attention mechanism to efficiently extract local features, placing greater emphasis on points with severe distortion. The GSC unit uses the geometry information of neighboring patches to construct an extended local neighborhood and introduces a transformer-style structure to capture long-range global correlations. The discriminator computes the deviation between the probability distributions of the enhanced point cloud and the original point cloud, guiding the generator to achieve high quality reconstruction. Experimental results show that the proposed method achieves state-of-the-art performance. Specifically, when applying PCE-GAN to the latest geometry-based point cloud compression (G-PCC) test model, it achieves an average BD-rate of -19.2% compared with the PredLift coding configuration and -18.3% compared with the RAHT coding configuration. Subjective comparisons show a significant improvement in texture clarity and color transitions, revealing finer details and more natural color gradients.},
  archive      = {J_TIP},
  author       = {Tian Guo and Hui Yuan and Qi Liu and Honglei Su and Raouf Hamzaoui and Sam Kwong},
  doi          = {10.1109/TIP.2025.3611178},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PCE-GAN: A generative adversarial network for point cloud attribute quality enhancement based on optimal transport},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VisionHub: Learning task-plugins for efficient universal vision model. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on the success of universal language models in natural language processing (NLP), researchers have recently sought to develop methods capable of tackling a broad spectrum of visual tasks within a unified foundation framework. However, existing universal vision models face significant challenges when adapting to the rapidly expanding scope of downstream tasks. These challenges stem not only from the prohibitive computational and storage expenses associated with training such models but also from the complexity of their workflows, which makes efficient adaptations difficult. Moreover, these models often fail to deliver the required performance and versatility for a broad spectrum of applications, largely due to their incomplete visual generation and perception capabilities, limiting their generalizability and effectiveness in diverse settings. In this paper, we present VisionHub, a novel universal vision model designed to concurrently manage multiple visual restoration and perception tasks, while offering streamlined transferability to downstream tasks. Our model leverages the frozen denoising U-Net architecture from Stable Diffusion as the backbone, fully exploiting its inherent potential for both visual restoration and perception. To further enhance the model’s flexibility, we propose the incorporation of lightweight task-plugins and the task router, which are seamlessly integrated onto the U-Net backbone. This architecture enables VisionHub to efficiently handle various vision tasks according to user-provided natural language instructions, all while maintaining minimal storage costs and operational overhead. Extensive experiments across 11 different vision tasks showcase both the efficiency and effectiveness of our approach. Remarkably, VisionHub achieves competitive performance across a variety of benchmarks, including 53.3% mIoU on ADE20K semantic segmentation, 0.253 RMSE on NYUv2 depth estimation, and 74.2 AP on MS-COCO pose estimation.},
  archive      = {J_TIP},
  author       = {Haolin Wang and Yixuan Zhu and Wenliang Zhao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TIP.2025.3611645},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VisionHub: Learning task-plugins for efficient universal vision model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liying Wang and Xiaoli Zhang and Chuanmin Jia and Siwei Ma},
  doi          = {10.1109/TIP.2025.3611602},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT tracking draws increasing attention because of its robustness in multi-modal warranting (MMW) scenarios, such as nighttime and adverse weather conditions, where relying on a single sensing modality fails to ensure stable tracking results. However, existing benchmarks predominantly contain videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality. This weakens the representativeness of existing benchmarks in severe imaging conditions, leading to tracking failures in MMW scenarios. To bridge this gap, we present a new benchmark considering the modality validity, MV-RGBT, captured specifically from MMW scenarios where either RGB (extreme illumination) or TIR (thermal truncation) modality is invalid. Hence, it is further divided into two subsets according to the valid modality, offering a new compositional perspective for evaluation and providing valuable insights for future designs. Moreover, MV-RGBT is the most diverse benchmark of its kind, featuring 36 different object categories captured across 19 distinct scenes. Furthermore, considering severe imaging conditions in MMW scenarios, a new problem is posed in RGBT tracking, named ‘when to fuse’, to stimulate the development of fusion strategies for such scenarios. To facilitate its discussion, we propose a new solution with a mixture of experts, named MoETrack, where each expert generates independent tracking results along with a confidence score. Extensive results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios. Besides, MoETrack achieves state-of-the-art results on several benchmarks, including MV-RGBT, GTOT, and LasHeR. Source codes and benchmarks are available at https://github.com/Zhangyong-Tang/MVRGBT.},
  archive      = {J_TIP},
  author       = {Zhangyong Tang and Tianyang Xu and Xiao-Jun Wu and Xuefeng Zhu and Chunyang Cheng and Zhenhua Feng and Josef Kittler},
  doi          = {10.1109/TIP.2025.3611687},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explanation method based on interpretable linear model with four key characteristics. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the interpretability of deep neural networks (DNNs) in visual-related tasks, existing explanation methods commonly generate a saliency map based on the linear relation between output results and input features. However, when the explanation conflicts with a human visual examination, these methods do not provide further evidence to analyze the saliency explanation. Most may fail to provide feature attribution with identifiable semantics or produce misleading explanations due to their insufficient robustness. In this paper, we first propose four key characteristics (richness, adaptivity, exclusiveness, and fairness) to evaluate the existing linear relation-based explanation method, and then construct an interpretable linear model to satisfy them. We formalize the characteristics and develop a novel explanation method based on this. We extract and reconstruct key exclusive semantic features from the feature map using the Nonnegative Matrix Factorization (NMF) algorithm, utilize the information entropy model to determine the number of features adaptively and their richness, and then linearly combine each feature with fairly assigned weights using an approximate Shapley algorithm to generate the saliency map. Compared with the state-of-the-art methods, our explanations of different datasets and DNNs are more convincing and robust in terms of Average drop (AD), Average increase (AI), Deletions (Del), and Insertions (Ins). Our supplementary experiments provide sufficient evidence that the four characteristics guarantee the feasibility of feature attribution analysis and enhance the quality of the resulting explanations.},
  archive      = {J_TIP},
  author       = {Yuecan Yuan and Zhan Ao Huang and Peng Li and Ying Fu and Xuemin Zhao and Canghong Shi and Xiaojie Li and Xi Wu},
  doi          = {10.1109/TIP.2025.3611593},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An explanation method based on interpretable linear model with four key characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent assistant domains transformer for source-free domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to address the challenge of adapting to a target domain without accessing the source domain directly. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current mainstream methods primarily focus on evaluating invariant features in the target domain that closely resemble those in the source domain, subsequently aligning the target domain with the source domain. However, these methods are susceptible to hard samples and influenced by domain bias. In this paper, we propose a Consistent Assistant Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue by constructing invariable feature representations of domain consistency. Concretely, we develop an assistant domain module for CADTrans to obtain diversified representations from the intermediate aggregated global attentions, which addresses the limitation of existing methods in adequately representing diversity. Based on assistant and target domains, invariable feature representations are obtained by multiple consistent strategies, which can be used to distinguish easy and hard samples. Finally, to align the hard samples to the corresponding easy samples, we construct a conditional multi-kernel max mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same category and those of different categories. Extensive experiments are conducted on various benchmarks such as Office-31, Office-Home, VISDA-C, and DomainNet-126, proving the significant performance improvements achieved by our proposed approaches. Code is available at https://github.com/RoryShao/CADTrans.git.},
  archive      = {J_TIP},
  author       = {Renrong Shao and Wei Zhang and Kangyang Luo and Qin Li and Jun Wang},
  doi          = {10.1109/TIP.2025.3611799},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistent assistant domains transformer for source-free domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some works integrate SSL techniques into deep clustering frameworks to enhance image clustering performance. However, they all need pre-training, clustering learning, or a trained clustering model as prerequisites, limiting the flexible and out-of-box application of SSL learners in the image clustering task. This work introduces ASD, an adaptor that enables the cold-start of SSL learners for deep image clustering without any prerequisites. Specifically, we first randomly sample pseudo-labeled data from all unlabeled data, and set an instance-level classifier to learn them with semantically aligned instance-level labels. With the ability of instance-level classification, we track the class transitions of predictions on unlabeled data to extract high-level similarities of instance-level classes, which can be utilized to assign cluster-level labels to pseudo-labeled data. Finally, we use the pseudo-labeled data with assigned cluster-level labels to trigger a general SSL learner trained on the unlabeled data for image clustering. We show the superior performance of ASD across various benchmarks against the latest deep image clustering approaches and very slight accuracy gaps compared to SSL methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can also further boost the performance of existing SSL-embedded deep image clustering methods.},
  archive      = {J_TIP},
  author       = {Yue Duan and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TIP.2025.3611144},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic self-paced multi-expert network for cross-domain few-shot facial expression recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3612281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, cross-domain few-shot facial expression recognition (CF-FER), which identifies novel compound expressions with a few images in the target domain by using the model trained only on basic expressions in the source domain, has attracted increasing attention. Generally, existing CF-FER methods leverage the multi-dataset to increase the diversity of the source domain and alleviate the discrepancy between the source and target domains. However, these methods learn feature embeddings in the Euclidean space without considering imbalanced expression categories and imbalanced sample difficulty in the multi-dataset. Such a way makes the model difficult to capture hierarchical relationships of facial expressions, resulting in inferior transferable representations. To address these issues, we propose a hyperbolic self-paced multi-expert network (HSM-Net), which contains multiple mixture-of-experts (MoE) layers located in the hyperbolic space, for CF-FER. Specifically, HSM-Net collaboratively trains multiple experts in a self-distillation manner, where each expert focuses on learning a subset of expression categories from the multi-dataset. Based on this, we introduce a hyperbolic self-paced learning (HSL) strategy that exploits sample difficulty to adaptively train the model from easy-to-hard samples, greatly reducing the influence of imbalanced expression categories and imbalanced sample difficulty. Our HSM-Net can effectively model rich hierarchical relationships of facial expressions and obtain a highly transferable feature space. Extensive experiments on both in-the-lab and in-the-wild compound expression datasets demonstrate the superiority of our proposed method over several state-of-the-art methods. Code will be released at https://github.com/cxtjl/HSM-Net.},
  archive      = {J_TIP},
  author       = {Xueting Chen and Yan Yan and Jing-Hao Xue and Chang Shu and Hanzi Wang},
  doi          = {10.1109/TIP.2025.3612281},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperbolic self-paced multi-expert network for cross-domain few-shot facial expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep sparse-to-dense inbetweening for multi-view light fields. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3612257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) imaging, which captures both intensity and directional information of light rays, extends the capabilities of traditional imaging techniques. In this paper, we introduce a task in the field of LF imaging, sparse-to-dense inbetweening, which focuses on generating dense novel views from sparse multi-view LFs. By synthesizing intermediate views from sparse inputs, this task enhances LF view synthesis through filling in interperspective gaps within an expanded field of view and increasing data robustness by leveraging complementary information between light rays from different perspectives, which are limited by non-robust single-view synthesis and the inability to handle sparse inputs effectively. To address these challenges, we construct a high-quality multi-view LF dataset, consisting of 60 indoor scenes and 59 outdoor scenes. Building upon this dataset, we propose a baseline method. Specifically, we introduce an adaptive alignment module to dynamically align information by capturing relative displacements. Next, we explore angular consistency and hierarchical information using a multi-level feature decoupling module. Finally, a multi-level feature refinement module is applied to enhance features and facilitate reconstruction. Additionally, we introduce a universally applicable artifact-aware loss function to effectively suppress visual artifacts. Experimental results demonstrate that our method outperforms existing approaches, establishing a benchmark for sparse-to-dense inbetweening. The code is available at https://github.com/Starmao1/MutiLF.},
  archive      = {J_TIP},
  author       = {Yifan Mao and Zeyu Xiao and Ping An and Deyang Liu and Caifeng Shan},
  doi          = {10.1109/TIP.2025.3612257},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep sparse-to-dense inbetweening for multi-view light fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3595408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders (AEs) have received extensive attention in hyperspectral anomaly detection (HAD) due to their capability to separate the background from the anomaly based on the reconstruction error. However, the existing AE methods routinely fail to adequately exploit spatial information and may precisely reconstruct anomalies, thereby affecting the detection accuracy. To address these issues, this study proposes a novel Multi-scale Autoencoder Suppression Strategy (MASS). The underlying principle of MASS is to prioritize the reconstruction of background information over anomalies. In the encoding stage, the Local Feature Extractor, which integrates Convolution and Omni-Dimensional Dynamic Convolution (ODConv), is combined with the Global Feature Extractor based on Transformer to effectively extract multi-scale features. Furthermore, a Self-Attention Suppression module (SAS) is devised to diminish the influence of anomalous pixels, enabling the network to focus more intently on the precise reconstruction of the background. During the process of network learning, a mask derived from the test outcomes of each iteration is integrated into the loss function computation, encompassing only the positions with low anomaly scores from the preceding detection round. Experiments on eight datasets demonstrate that the proposed method is significantly superior to several traditional methods and deep learning methods in terms of performance.},
  archive      = {J_TIP},
  author       = {Bing Tu and Tao Zhou and Bo Liu and Yan He and Jun Li and Antonio Plaza},
  doi          = {10.1109/TIP.2025.3595408},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale autoencoder suppression strategy for hyperspectral image anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the coordination of frequency and attention in masked image modeling. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, masked image modeling (MIM), which learns visual representations by reconstructing the masked patches of an image, has dominated self-supervised learning in computer vision. However, the pre-training of MIM always takes massive time due to the large-scale data and large-size backbones. We mainly attribute it to the random patch masking in previous MIM works, which fails to leverage the crucial semantic information for effective visual representation learning. To tackle this issue, we propose the Frequency & Attention-driven Masking and Throwing Strategy (FAMT), which can extract semantic patches and reduce the number of training patches to boost model performance and training efficiency simultaneously. Specifically, FAMT utilizes the self-attention mechanism to extract semantic information from the image for masking during training in an unsupervised manner. However, attention alone could sometimes focus on inappropriate areas regarding the semantic information. Thus, we are motivated to incorporate the information from the frequency domain into the self-attention mechanism to derive the sampling weights for masking, which captures semantic patches for visual representation learning. Furthermore, we introduce a patch throwing strategy based on the derived sampling weights to reduce the training cost. FAMT can be seamlessly integrated as a plug-and-play module and surpasses previous works, e.g. reducing the training phase time by nearly 50% and improving the linear probing accuracy of MAE by 1.3% ∼ 3.9% across various datasets, including CIFAR-10/100, Tiny ImageNet, and ImageNet-1K. FAMT also demonstrates superior performance in downstream detection and segmentation tasks.},
  archive      = {J_TIP},
  author       = {Jie Gui and Tuo Chen and Minjing Dong and Zhengqi Liu and Hao Luo and James Tin-Yau Kwok and Yuan Yan Tang},
  doi          = {10.1109/TIP.2025.3592555},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring the coordination of frequency and attention in masked image modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3599929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of prior knowledge about unknown classes during training, existing methods for cross-domain open-set image recognition typically rely on threshold-based solutions. However, such approaches often struggle to capture the complex boundary relationships between known and unknown classes, which can lead to negative transfer effects caused by feature confusion between the two. To address this issue, this paper proposes a graph isomorphic distillation diffusion model (GIDDM) that aims to learn the boundary relationships between known and unknown classes from a closed-set classifier that models predictive uncertainty. First, a diffusion classifier is designed to quantify model predictive uncertainty through a Monte Carlo sampling strategy performed on the noise distribution during the reverse denoising process. The uncertainty distribution is modeled, and the cumulative distribution function is used to compute the probability of a sample belonging to an unknown class. Second, an open-set recognition framework is constructed, treating the closed-set diffusion classifier as a teacher classifier, and guiding the student classifier to learn the complex boundary relationships between known and unknown classes through knowledge distillation. Third, the knowledge distillation process is further formalized as a graph isomorphic optimization problem, where the predictive manifolds of the student and teacher classifiers are constrained to be consistent, thereby enhancing knowledge transfer between the classifiers. Finally, the entire process is integrated into a unified open-set adversarial domain adaptation framework, reconstructing the traditional optimization objectives of closed-set adversarial domain adaptation to ensure sufficient separation between known and unknown classes while aligning the distributions of known classes in both the source and target domains. Experiments conducted on multiple hyperspectral image (HSI) datasets demonstrate that the proposed method achieves state-of-the-art performance on cross-domain open-set image recognition tasks. The code demo can be accessed on the following website: https://github.com/wzr78998/GIDDM.},
  archive      = {J_TIP},
  author       = {Haoyu Wang and Yuhu Cheng and Wei Zhang and Xiaomin Liu and Xuesong Wang},
  doi          = {10.1109/TIP.2025.3599929},
  journal      = {IEEE Transactions on Image Processing},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GIDDM: Generating labels with diffusion model to promote cross-domain open-set image recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing perception of key changes in remote sensing image change captioning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3589096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, while significant progress has been made in remote sensing image change captioning, existing methods fail to filter out areas unrelated to actual changes, making models susceptible to irrelevant features. In this article, we propose a novel multimodal model for remote sensing image change captioning, guided by Key Change Features and Instruction-tuned (KCFI). This model aims to fully leverage the intrinsic knowledge of large language models through visual instructions and enhance the effectiveness and accuracy of change features using pixel-level change detection tasks. Specifically, KCFI includes a ViTs encoder for extracting bi-temporal remote sensing image features, a key feature perceiver for identifying critical change areas, a pixel-level change detection decoder to constrain key change features, and an instruction-tuned decoder based on a large language model. Moreover, to ensure that change captioning and change detection tasks are jointly optimized, we employ a dynamic weight-averaging strategy to balance the losses between the two tasks. We also explore various feature combinations for visual fine-tuning instructions and demonstrate that using only key change features to guide the large language model is the optimal choice. To validate the effectiveness of our approach, we compare it against several state-of-the-art change captioning methods on the LEVIR-CC dataset, achieving the best performance. Our code will be available at https://github.com/yangcong356/KCFI.git.},
  archive      = {J_TIP},
  author       = {Cong Yang and Zuchao Li and Hongzan Jiao and Zhi Gao and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3589096},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing perception of key changes in remote sensing image change captioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fourier-based decoupling network for joint low-light image enhancement and deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3592559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime handheld photography is often simultaneously affected by low light and blur degradations due to object motion and camera shake. Previous methods typically design specific modules to restore the degradations in the spatial domain independently. However, the interdependence of low light and blur degradations in the spatial domain makes it difficult for these approaches to effectively decouple the degradations, limiting the performance of the designed modules. In this paper, we observe that in the Fourier domain, low light and blur degradations can be represented independently in the amplitude and phase of the image. Through an in-depth analysis of the underlying physical degradation process, we discover that low light degradation exhibits distinct characteristics across different frequency bands in amplitude, while blur degradation is characterized by phase correlation. Leveraging these insights, we mathematically derive a frequency attention mechanism and a filtering mechanism for learning decoupled representations of these degradations, proposing a Fourier-based Decoupling Network for joint low-light image enhancement and deblurring. Experimental results demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets and exhibits significantly sharper edges.},
  archive      = {J_TIP},
  author       = {Luwei Tu and Jiawei Wu and Chenxi Wang and Deyu Meng and Zhi Jin},
  doi          = {10.1109/TIP.2025.3592559},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fourier-based decoupling network for joint low-light image enhancement and deblurring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-precision edge detection guided by flow fields. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3572763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is frequently employed to support downstream visual tasks. However, current edge detection methods still encounter two significant challenges: extracting complex textured targets and capturing valuable information from complex backgrounds. We propose FFED, a flow field-guided edge detection model. FFED integrates the three components of our design. FFED incorporates three designed components: the Feature Broadcast Module (FBM), the Antagonistic Bio-inspired Spatial Attention Module (ABSAM), a novel pixel difference convolution named ALS. The FBM serves as an implementation mode of the flow field, with its input pair selection strategy inspired by video processing.The FBM broadcasts high-level semantic features to high-resolution ones, preserving more meaningful texture details. Inspired by biological studies, we propose the ABSAM. ABSAM extracts valuable information from complex backgrounds by optimizing spatial modeling of data. The ALS exhibits enhanced capability in extracting gradient information and capturing subtle texture details that are easily overlooked. Experimental results demonstrate that FFED achieved competitive detection results on NYUD, BSDS500, and BIPED datasets, as well as good performance on industrial datasets. Additionally, the experiment verified the auxiliary effect of FFED on downstream visual tasks. The code is available at https://github.com/hanyuchen2022/Flow-field-guided-edge-detection-FFED-.},
  archive      = {J_TIP},
  author       = {Bing Li and Yuchen Han and Shiyin Zhang and Haowei Wang and Zhenbing Zhao and Yongjie Zhai},
  doi          = {10.1109/TIP.2025.3572763},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-precision edge detection guided by flow fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceive-IR: Learning to perceive degradation better for all-in-one image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3566300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing All-in-One image restoration methods often fail to simultaneously perceive degradation types and severity levels, overlooking the importance of fine-grained quality perception. Moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. To address these limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image restoration framework designed for fine-grained quality control across various degradation types and severity levels. Its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. Specifically, Perceive-IR operates in two key stages: (1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern threetier quality levels by optimizing the alignment between prompts and images within the CLIP perception space. This stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; (2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. This strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while simultaneously pulling it away from both low- and medium-quality samples. Furthermore, Perceive-IR incorporates a Semantic Guidance Module (SGM) and Compact Feature Extraction (CFE). The SGM leverages semantic information from pre-trained vision models to provide high-level contextual guidance, while the CFE focuses on extracting degradation-specific features, ensuring accurate handling of diverse image degradations. Extensive experiments demonstrate that Perceive-IR not only surpasses state-of-the-art methods but also generalizes reliably to zero-shot realworld and unknown degraded scenes, while adapting seamlessly to different backbone networks. This versatility underscores the framework’s robustness and backbone-agnostic design.},
  archive      = {J_TIP},
  author       = {Xu Zhang and Jiaqi Ma and Guoli Wang and Qian Zhang and Huan Zhang and Lefei Zhang},
  doi          = {10.1109/TIP.2025.3566300},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceive-IR: Learning to perceive degradation better for all-in-one image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unrolling plug-and-play gradient graph laplacian regularizer for image restoration. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3562425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic deep learning (DL) networks for image restoration like denoising and interpolation lack mathematical interpretability, require voluminous training data to tune large parameter sets, and are fragile in the face of covariate shift. To address these shortcomings, we build interpretable networks by unrolling variants of a graph-based optimization algorithm of different complexities. Specifically, for a general linear image formation model, we first formulate a convex quadratic programming (QP) problem with a new ℓ2-norm graph smoothness prior called gradient graph Laplacian regularizer (GGLR) that promotes piecewise planar (PWP) signal reconstruction. To solve the posed unconstrained QP problem, instead of computing a linear system solution straightforwardly, we introduce a variable number of auxiliary variables and correspondingly design a family of ADMM algorithms. We then unroll them into variable-complexity feedforward networks, amenable to parameter tuning via back-propagation. More complex unrolled networks require more labeled data to train more parameters, but have better over-all performance. The unrolled networks have periodic insertions of a graph learning module, akin to a self-attention mechanism in a transformer architecture, to learn pairwise similarity structure inherent in data. Experimental results show that our unrolled networks perform competitively to generic DL networks in image restoration quality while using only a fraction of parameters, and demonstrate improved robustness to covariate shift.},
  archive      = {J_TIP},
  author       = {Jianghe Cai and Gene Cheung and Fei Chen},
  doi          = {10.1109/TIP.2025.3562425},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unrolling plug-and-play gradient graph laplacian regularizer for image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing multi-modal large language models for measuring and interpreting color differences. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3522802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate measurement of perceptual color differences (CDs) between two images plays an important role in modern smartphone photography. Although traditional CD metrics provide numerical scores to quantify color variations, they often lack the ability to offer intuitive insights or explanations that reflect the factors behind these differences in a way that aligns with human perception and reasoning. Here, we present CD-Reasoning, an innovative method designed not merely to compute numerical CD scores but also to provide a detailed rationale for the observed CDs between images. This method surpasses simple numerical quantification, delivering a more profound and explanatory analysis that bridges quantitative assessments with the qualitative reasoning characteristic of human perception. The development of the CD-Reasoning model begins with the compilation of a multi-modal CD dataset dubbed M-SPCD based on the existing SPCD, where we collect textual descriptions that detail the quantification of CDs across seven pivotal attributes: white balance, brightness contrast, color contrast, overall brightness, overall color, shadow detail, and highlight detail. Utilizing the newly curated M-SPCD dataset, we enhance the capabilities of cutting-edge Multimodal Large Language Models (MLLMs) to not only accurately assess numerical CD scores but also to provide in-depth reasoning that explains the CDs between two images. Extensive experiments demonstrate that the proposed CD-Reasoning not only achieves superior accuracy compared to state-of-the-art CD metrics but also significantly exceeds leading MLLMs in CD interpreting. Source codes will be available at https://github.com/LongYu-LY/CD-Reasoning.},
  archive      = {J_TIP},
  author       = {Zhihua Wang and Yu Long and Qiuping Jiang and Chao Huang and Xiaochun Cao},
  doi          = {10.1109/TIP.2024.3522802},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harnessing multi-modal large language models for measuring and interpreting color differences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining pre- and post-demosaicking noise removal for RAW video. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3527886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is one of the fundamental steps of the processing pipeline that converts data captured by a camera sensor into a display-ready image or video. It is generally performed early in the pipeline, usually before demosaicking, although studies swapping their order or even conducting them jointly have been proposed. With the advent of deep learning, the quality of denoising algorithms has steadily increased. Even so, modern neural networks still have a hard time adapting to new noise levels and scenes, which is indispensable for real-world applications. With those in mind, we propose a self-similarity-based denoising scheme that weights both a pre- and a post-demosaicking denoiser for Bayer-patterned CFA video data. We show that a balance between the two leads to better image quality, and we empirically find that higher noise levels benefit from a higher influence pre-demosaicking. We also integrate temporal trajectory prefiltering steps before each denoiser, which further improve texture reconstruction. The proposed method only requires an estimation of the noise model at the sensor, accurately adapts to any noise level, and is competitive with the state of the art, making it suitable for real-world videography.},
  archive      = {J_TIP},
  author       = {M. Sánchez-Beeckman and A. Buades and N. Brandonisio and B. Kanoun},
  doi          = {10.1109/TIP.2025.3527886},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining pre- and post-demosaicking noise removal for RAW video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radar pointcloud offers attractive potential for 3D sensing, thanks to its robustness in challenging conditions such as smoke and low illumination. However, existing methods failed to simultaneously address the three main challenges in mmWave radar pointcloud reconstruction: specular information lost, low angular resolution, and severe interference. In this paper, we propose DREAM-PCD, a novel framework specifically designed for real-time 3D environment sensing that combines signal processing and deep learning methods into three well-designed components to tackle all three challenges: Non-Coherent Accumulation for dense points, Synthetic Aperture Accumulation for improved angular resolution, and Real-Denoise Multiframe network for interference removal. By leveraging causal multiple viewpoints accumulation and the “real-denoise" mechanism, DREAM-PCD significantly enhances the generalization performance and real-time capability. We also introduce RadarEyes, the largest mmWave indoor dataset with over 1,000,000 frames, featuring a unique design incorporating two orthogonal single-chip radars, Lidar, and camera, enriching dataset diversity and applications. Experimental results demonstrate that DREAM-PCD surpasses existing methods in reconstruction quality, and exhibits superior generalization and real-time capabilities, enabling high-quality real-time reconstruction of radar pointcloud under various parameters and scenarios. We believe that DREAM-PCD, along with the RadarEyes dataset, will significantly advance mmWave radar perception in future real-world applications.},
  archive      = {J_TIP},
  author       = {Ruixu Geng and Yadong Li and Dongheng Zhang and Jincheng Wu and Yating Gao and Yang Hu and Yan Chen},
  doi          = {10.1109/TIP.2024.3512356},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DREAM-PCD: Deep reconstruction and enhancement of mmWave radar pointcloud},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the prevailing transition from single-task to multi-task approaches in video anomaly detection, we observe that many adopt sub-optimal frameworks for individual proxy tasks. Motivated by this, we contend that optimizing single-task frameworks can advance both single- and multi-task approaches. Accordingly, we leverage middle-frame prediction as the primary proxy task, and introduce an effective hybrid framework designed to generate accurate predictions for normal frames and flawed predictions for abnormal frames. This hybrid framework is built upon a bi-directional structure that seamlessly integrates both vision transformers and ConvLSTMs. Specifically, we utilize this bi-directional structure to fully analyze the temporal dimension by predicting frames in both forward and backward directions, significantly boosting the detection stability. Given the transformer’s capacity to model long-range contextual dependencies, we develop a convolutional temporal transformer that efficiently associates feature maps from all context frames to generate attention-based predictions for target frames. Furthermore, we devise a layer-interactive ConvLSTM bridge that facilitates the smooth flow of low-level features across layers and time-steps, thereby strengthening predictions with fine details. Anomalies are eventually identified by scrutinizing the discrepancies between target frames and their corresponding predictions. Several experiments conducted on public benchmarks affirm the efficacy of our hybrid framework, whether used as a standalone single-task approach or integrated as a branch in a multi-task approach. These experiments also underscore the advantages of merging vision transformers and ConvLSTMs for video anomaly detection. The implementation of our hybrid framework is available at https://github.com/SHENGUODONG19951126/ConvTTrans-ConvLSTM.},
  archive      = {J_TIP},
  author       = {Guodong Shen and Yuqi Ouyang and Junru Lu and Yixuan Yang and Victor Sanchez},
  doi          = {10.1109/TIP.2024.3512369},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing video anomaly detection: A bi-directional hybrid framework for enhanced single- and multi-task approaches},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SALENet: Structure-aware lighting estimations from a single image for indoor environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) lighting plays a pivotal role in modern augmented and mixed-reality (AR/MR) applications, facilitating immersive experiences through realistic object insertion and dynamic relighting. However, the acquisition of precise HDR environment maps remains cost-prohibitive and impractical when using standard devices. To bridge this gap, this paper introduces SALENet , a novel deep network for estimating global lighting conditions from a single image, to effectively mitigate the need for resource-intensive acquisition methods. In contrast to earlier studies, we focus on exploring the inherent structural relationships within the lighting distribution. We design a hierarchical transformer-based neural network architecture with a proposed cross-attention mechanism between different resolution lighting source representations, optimizing the spatial distribution of lighting sources simultaneously for enhanced consistency. To further improve accuracy, a structure-based contrastive learning method is proposed to select positive-negative pairs based on lighting distribution similarity. By harnessing the synergy of hierarchical transformers and structure-based contrastive learning, our framework yields a significant enhancement in lighting prediction accuracy, enabling high-fidelity augmented and mixed reality to achieve cost-effectively immersive and realistic lighting effects.},
  archive      = {J_TIP},
  author       = {Junhong Zhao and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TIP.2024.3512381},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SALENet: Structure-aware lighting estimations from a single image for indoor environments},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning frame-event fusion for motion deblurring. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion deblurring is a highly ill-posed problem due to the significant loss of motion information in the blurring process. Complementary informative features from auxiliary sensors such as event cameras can be explored for guiding motion deblurring. The event camera can capture rich motion information asynchronously with microsecond accuracy. In this paper, a novel frame-event fusion framework is proposed for event-driven motion deblurring (FEF-Deblur), which can sufficiently explore long-range cross-modal information interactions. Firstly, different modalities are usually complementary and also redundant. Cross-modal fusion is modeled as complementary-unique features separation-and-aggregation, avoiding the modality redundancy. Unique features and complementary features are first inferred with parallel intra-modal self-attention and inter-modal cross-attention respectively. After that, a correlation-based constraint is designed to act between unique and complementary features to facilitate their differentiation, which assists in cross-modal redundancy suppression. Additionally, spatio-temporal dependencies among neighboring inputs are crucial for motion deblurring. A recurrent cross attention is introduced to preserve inter-input attention information, in which the current spatial features and aggregated temporal features are attending to each other by establishing the long-range interaction between them. Extensive experiments on both synthetic and real-world motion deblurring datasets demonstrate our method outperforms state-of-the-art event-based and image/video-based methods. The code will be made publicly available.},
  archive      = {J_TIP},
  author       = {Wen Yang and Jinjian Wu and Jupo Ma and Leida Li and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TIP.2024.3512362},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning frame-event fusion for motion deblurring},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale pre-trained vision-language models ( e.g ., CLIP) have shown incredible generalization performance in downstream tasks such as video-text retrieval (VTR). Traditional approaches have leveraged CLIP’s robust multi-modal alignment ability for VTR by directly fine-tuning vision and text encoders with clean video-text data. Yet, these techniques rely on carefully annotated video-text pairs, a process that is costly and labor-intensive. In this context, we introduce a new approach, Pseudo-Supervised Selective Contrastive Learning (PS-SCL). PS-SCL minimizes the dependency on manually-labeled text annotations by generating pseudo-supervisions from unlabeled video data for training. We first exploit CLIP’s visual recognition capabilities to generate pseudo-texts automatically. These pseudo-texts contain diverse visual concepts from the video and serve as weak textual guidance. Moreover, we introduce Selective Contrastive Learning (SeLeCT), which prioritizes and selects highly correlated video-text pairs from pseudo-supervised video-text pairs. By doing so, SeLeCT enables more effective multi-modal learning under weak pairing supervision. Experimental results demonstrate that our method outperforms CLIP zero-shot performance by a large margin on multiple video-text retrieval benchmarks, e.g ., 8.2% R@1 for video-to-text on MSRVTT, 12.2% R@1 for video-to-text on DiDeMo, and 10.9% R@1 for video-to-text on ActivityNet, respectively.},
  archive      = {J_TIP},
  author       = {Yu Lu and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3514352},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting unlabeled videos for video-text retrieval via pseudo-supervised learning},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning of intrinsic semantics with diffusion model for person re-identification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) aims to learn semantic representations for person retrieval without using identity labels. Most existing methods generate fine-grained patch features to reduce noise in global feature clustering. However, these methods often compromise the discriminative semantic structure and overlook the semantic consistency between the patch and global features. To address these problems, we propose a Person Intrinsic Semantic Learning (PISL) framework with diffusion model for unsupervised person Re-ID. First, we design the Spatial Diffusion Model (SDM), which performs a denoising diffusion process from noisy spatial transformer parameters to semantic parameters, enabling the sampling of patches with intrinsic semantic structure. Second, we propose the Semantic Controlled Diffusion (SCD) loss to guide the denoising direction of the diffusion model, facilitating the generation of semantic patches. Third, we propose the Patch Semantic Consistency (PSC) loss to capture semantic consistency between the patch and global features, refining the pseudo-labels of global features. Comprehensive experiments on three challenging datasets show that our method surpasses current unsupervised Re-ID methods. The source code will be publicly available at https://github.com/taoxuefong/Diffusion-reid.},
  archive      = {J_TIP},
  author       = {Xuefeng Tao and Jun Kong and Min Jiang and Ming Lu and Ajmal Mian},
  doi          = {10.1109/TIP.2024.3514360},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised learning of intrinsic semantics with diffusion model for person re-identification},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion models as strong adversaries. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3514361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated their great ability to generate high-quality images for various tasks. With such a strong performance, diffusion models can potentially pose a severe threat to both humans and deep learning models. However, their abilities as adversaries have not been well explored. Among different adversarial scenarios, the no-box adversarial attack is the most practical one, as it assumes that the attacker has no access to the training dataset or the target model. Existing works still require some data from the training dataset, which may not be feasible in real-world scenarios. In this paper, we investigate the adversarial capabilities of diffusion models by conducting no-box attacks solely using data generated by diffusion models. Specifically, our attack method generates a synthetic dataset using diffusion models to train a substitute model. We then employ a classification diffusion model to fine-tune the substitute model, considering model uncertainty and incorporating noise augmentation. Finally, we sample adversarial examples from the diffusion models using the average approximation over the diffusion substitute model with multiple inferences. Extensive experiments on the ImageNet dataset demonstrate that the proposed attack method achieves state-of-the-art performance in both no-box attack and black-box attack scenarios.},
  archive      = {J_TIP},
  author       = {Xuelong Dai and Yanjie Li and Mingxing Duan and Bin Xiao},
  doi          = {10.1109/TIP.2024.3514361},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diffusion models as strong adversaries},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3515801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pose estimation for objects with rotational symmetry, ambiguous poses may arise, and the symmetry axes of objects are crucial for eliminating such ambiguities. Currently, in pose estimation, reliance on manual settings of symmetry axes decreases the accuracy of pose estimation. To address this issue, this method proposes determining the orders of symmetry axes and angles between axes based on a given rotational symmetry type or polyhedron, reducing the need for manual settings of symmetry axes. Subsequently, two key axes with the highest orders are defined and localized, then three orthogonal axes are generated based on key axes, while each symmetry axis can be computed utilizing orthogonal axes. Compared to localizing symmetry axes one by one, the key-axis-based symmetry axis localization is more efficient. To support geometric and texture symmetry, the method utilizes the ADI metric for key axis localization in geometrically symmetric objects and proposes a novel metric, ADI-C, for objects with texture symmetry. Experimental results on the LM-O and HB datasets demonstrate a 9.80% reduction in symmetry axis localization error and a 1.64% improvement in pose estimation accuracy. Additionally, the method introduces a new dataset, DSRSTO, to illustrate its performance across seven types of geometrically and texturally symmetric objects. The GitHub link for the open-source tool based on this method is https://github.com/WangYuLin-SEU/KASAL.},
  archive      = {J_TIP},
  author       = {Yulin Wang and Chen Luo},
  doi          = {10.1109/TIP.2024.3515801},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Key-axis-based localization of symmetry axes in 3D objects utilizing geometry and texture},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to compress face video is a crucial problem for a series of online applications, such as video chat/conference, live broadcasting and remote education. Compared to other natural videos, these face-centric videos owning abundant structural information can be compactly represented and high-quality reconstructed via deep generative models, such that the promising compression performance can be achieved. However, the existing generative face video compression schemes are faced with the inconsistency between the 3D facial motion in the physical world and the face content evolution in the 2D view. To solve this drawback, we propose a 3D-Keypoint-and-2D-Motion based generative method for Face Video Compression, namely FVC-3K2M, which can well ensure perceptual compensation and visual consistency between motion description and face reconstruction. In particular, the temporal evolution of face video can be characterized into separate 3D keypoints from the global and local perspectives, entailing great coding flexibility and accurate motion representation. Moreover, a cascade motion conversion mechanism is further proposed to internally convert 3D keypoints to 2D dense motion, enforcing the face video reconstruction to be perceptually realistic. Finally, an adaptive reference frame selection scheme is developed to enhance the adaptation of various temporal movements. Experimental results show that the proposed scheme can realize reliable video communication in the extremely limited bandwidth, e.g., 2 kbps. Compared to the state-of-the-art video coding standards and the latest face video compression methods, extensive comparisons demonstrate that our proposed scheme achieves superior compression performance in terms of multiple quality evaluations.},
  archive      = {J_TIP},
  author       = {Zhao Wang and Bolin Chen and Shurun Wang and Shiqi Wang and Yan Ye and Siwei Ma},
  doi          = {10.1109/TIP.2024.3518100},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ultra-low bitrate face video compression based on conversions from 3D keypoints to 2D motion map},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rebalanced vision-language retrieval considering structure-aware distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3518759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language retrieval aims to search for similar instances in one modality based on queries from another modality. The primary objective is to learn cross-modal matching representations in a latent common space. Actually, the assumption underlying cross-modal matching is modal balance, where each modality contains sufficient information to represent the others. However, noise interference and modality insufficiency often lead to modal imbalance, making it a common phenomenon in practice. The impact of imbalance on retrieval performance remains an open question. In this paper, we first demonstrate that ultimate cross-modal matching is generally sub-optimal for cross-modal retrieval when imbalanced modalities exist. The structure of instances in the common space is inherently influenced when facing imbalanced modalities, posing a challenge to cross-modal similarity measurement. To address this issue, we emphasize the importance of meaningful structure-preserved matching. Accordingly, we propose a simple yet effective method to rebalance cross-modal matching by learning structure-preserved matching representations. Specifically, we design a novel multi-granularity cross-modal matching that incorporates structure-aware distillation alongside the cross-modal matching loss. While the cross-modal matching loss constraints instance-level matching, the structure-aware distillation further regularizes the geometric consistency between learned matching representations and intra-modal representations through the developed relational matching. Extensive experiments on different datasets affirm the superior cross-modal retrieval performance of our approach, simultaneously enhancing single-modal retrieval capabilities compared to the baseline models.},
  archive      = {J_TIP},
  author       = {Yang Yang and Wenjuan Xi and Luping Zhou and Jinhui Tang},
  doi          = {10.1109/TIP.2024.3518759},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rebalanced vision-language retrieval considering structure-aware distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3512354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language models (VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. We believe our method establishes a simple yet strong baseline for future STR research with VLMs.},
  archive      = {J_TIP},
  author       = {Shuai Zhao and Ruijie Quan and Linchao Zhu and Yi Yang},
  doi          = {10.1109/TIP.2024.3512354},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLIP4STR: A simple baseline for scene text recognition with pre-trained vision-language model},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting domain-adaptive semantic segmentation via knowledge distillation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous methods for unsupervised domain adaptation (UDA) have been proposed in semantic segmentation, achieving remarkable improvements. These methods are categorized into an adversarial learning-based approach that utilizes an additional discriminator and image translation model, and a self-supervised approach that uses a teacher model to generate pseudo labels. Among them, the self-supervised UDA approaches based on a self-training show excellent adaptability in semantic segmentation. However, erroneous estimates of the pseudo ground truths (PGTs) used in the self-training may often lead to inaccurate updates in the teacher model. Although several attempts have been made to address this issue, the teacher model updated through exponential moving average (EMA) still has a risk of propagating inaccuracies from the PGTs. Inspired by the fact that UDA shares similar principles with knowledge distillation (KD), we revisit the self-training based UDA approach from the perspective of KD and propose a novel UDA approach that employs two different teacher models. Specifically, we utilize both an EMA-updated teacher model to generate PGTs and a frozen teacher model pretrained with source data to transfer knowledge on a feature space. Since the frozen teacher model has no constraint on the model architecture unlike the EMA updated teacher model, we can effectively leverage a better representation power from the larger frozen teacher. Extensive experiments on various backbones (DeepLab-V2 [40] and DAFormer [73]) and scenarios (GTA5 → Cityscapes and SYNTHIA → Cityscapes) show that the proposed method improves segmentation performance in the target domain with its scalability. In particular, our method achieves comparable or better performance than state-of-the-arts even with a lightweight backbone.},
  archive      = {J_TIP},
  author       = {Seongwon Jeong and Jiyeong Kim and Sungheui Kim and Dongbo Min},
  doi          = {10.1109/TIP.2024.3501076},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting domain-adaptive semantic segmentation via knowledge distillation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MWFormer: Multi-weather image restoration using degradation-aware transformers. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3501855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.},
  archive      = {J_TIP},
  author       = {Ruoxi Zhu and Zhengzhong Tu and Jiaming Liu and Alan C. Bovik and Yibo Fan},
  doi          = {10.1109/TIP.2024.3501855},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MWFormer: Multi-weather image restoration using degradation-aware transformers},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified video reconstruction for rolling shutter and global shutter cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3504275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the general domain of video reconstruction (VR) is fragmented into different shutters spanning global shutter and rolling shutter cameras. Despite rapid progress in the state-of-the-art, existing methods overwhelmingly follow shutter-specific paradigms and cannot conceptually generalize to other shutter types, hindering the uniformity of VR models. In this paper, we propose UniVR, a versatile framework to handle various shutters through unified modeling and shared parameters. Specifically, UniVR encodes diverse shutter types into a unified space via a tractable shutter adapter, which is parameter-free and thus can be seamlessly delivered to current well-established VR architectures for cross-shutter transfer. To demonstrate its effectiveness, we conceptualize UniVR as three shutter-generic VR methods, namely Uni-SoftSplat, Uni-SuperSloMo, and Uni-RIFE. Extensive experimental results demonstrate that the pre-trained model without any fine-tuning can achieve reasonable performance even on novel shutters. After fine-tuning, new state-of-the-art performances are established that go beyond shutter-specific methods and enjoy strong generalization. The code is available at https://github.com/GitCVfb/UniVR.},
  archive      = {J_TIP},
  author       = {Bin Fan and Zhexiong Wan and Boxin Shi and Chao Xu and Yuchao Dai},
  doi          = {10.1109/TIP.2024.3504275},
  journal      = {IEEE Transactions on Image Processing},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unified video reconstruction for rolling shutter and global shutter cameras},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing few-shot out-of-distribution detection with pre-trained model features. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2024.3468874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the reliability of open-world intelligent systems heavily relies on effective out-of-distribution (OOD) detection. Despite notable successes in existing OOD detection methods, their performance in scenarios with limited training samples is still suboptimal. Therefore, we first construct a comprehensive few-shot OOD detection benchmark in this paper. Remarkably, our investigation reveals that Parameter-Efficient Fine-Tuning (PEFT) techniques, such as visual prompt tuning and visual adapter tuning, outperform traditional methods like fully fine-tuning and linear probing tuning in few-shot OOD detection. Considering that some valuable information from the pre-trained model, which is conducive to OOD detection, may be lost during the fine-tuning process, we reutilize features from the pre-trained models to mitigate this issue. Specifically, we first propose a training-free approach, termed uncertainty score ensemble (USE). This method integrates feature-matching scores to enhance existing OOD detection methods, significantly narrowing the gap between traditional fine-tuning and PEFT techniques. However, due to its training-free property, this method is unable to improve in-distribution accuracy. To this end, we further propose a method called Domain-Specific and General Knowledge Fusion (DSGF) to improve few-shot OOD detection performance and ID accuracy under different fine-tuning paradigms. Experiment results demonstrate that DSGF enhances few-shot OOD detection across different fine-tuning strategies, shot settings, and OOD detection methods. We believe our work can provide the research community with a novel path to leveraging large-scale visual pre-trained models for addressing FS-OOD detection. The code will be released.},
  archive      = {J_TIP},
  author       = {Jiuqing Dong and Yifan Yao and Wei Jin and Heng Zhou and Yongbin Gao and Zhijun Fang},
  doi          = {10.1109/TIP.2024.3468874},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing few-shot out-of-distribution detection with pre-trained model features},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised temporal correspondence learning for unified video object removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3340605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object removal aims at erasing a target object in the entire video and filling holes with plausible contents, given an object mask in the first frame as input. Existing solutions mostly break down the task into (supervised) mask tracking and (self-supervised) video completion, and then separately tackle them with tailored designs. In this paper, we introduce a new setup, coined as unified video object removal , where mask tracking and completion are addressed within a unified framework. Despite introducing more challenges, the setup is promising for future practical usage. We embrace the observation that these two sub-tasks have strong inherent connections in terms of pixel-level temporal correspondence. Making full use of the connections could be beneficial considering the complexity of both algorithm and deployment. We propose a single network linking the two sub-tasks by inferring temporal correspondences across multiple frames, i.e ., correspondences between valid-valid (V-V) pixel pairs for mask tracking and correspondences between valid-hole (V-H) pixel pairs for video completion. Thanks to the unified setup, the network can be learned end-to-end in a totally unsupervised fashion without any annotations. We demonstrate that our method can generate visually pleasing results and perform favorably against existing separate solutions in realistic test cases.},
  archive      = {J_TIP},
  author       = {Zhongdao Wang and Jinglu Wang and Xiao Li and Ya-Li Li and Yan Lu and Shengjin Wang},
  doi          = {10.1109/TIP.2023.3340605},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised temporal correspondence learning for unified video object removal},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Field-of-view IoU for object detection in 360° images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3296013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° cameras have gained popularity over the last few years. In this paper, we propose two fundamental techniques—Field-of-View IoU (FoV-IoU) and 360Augmentation for object detection in 360° images. Although most object detection neural networks designed for perspective images are applicable to 360° images in equirectangular projection (ERP) format, their performance deteriorates owing to the distortion in ERP images. Our method can be readily integrated with existing perspective object detectors and significantly improves the performance. The FoV-IoU computes the intersection-over-union of two Field-of-View bounding boxes in a spherical image which could be used for training, inference, and evaluation while 360Augmentation is a data augmentation technique specific to 360° object detection task which randomly rotates a spherical image and solves the bias due to the sphere-to-plane projection. We conduct extensive experiments on the 360° indoor dataset with different types of perspective object detectors and show the consistent effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Miao Cao and Satoshi Ikehata and Kiyoharu Aizawa},
  doi          = {10.1109/TIP.2023.3296013},
  journal      = {IEEE Transactions on Image Processing},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Field-of-view IoU for object detection in 360° images},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2023.3273451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The end-to-end image fusion framework has achieved promising performance, with dedicated convolutional networks aggregating the multi-modal local appearance. However, long-range dependencies are directly neglected in existing CNN fusion approaches, impeding balancing the entire image-level perception for complex scenario fusion. In this paper, therefore, we propose an infrared and visible image fusion algorithm based on the transformer module and adversarial learning. Inspired by the global interaction power, we use the transformer technique to learn the effective global fusion relations. In particular, shallow features extracted by CNN are interacted in the proposed transformer fusion module to refine the fusion relationship within the spatial scope and across channels simultaneously. Besides, adversarial learning is designed in the training process to improve the output discrimination via imposing competitive consistency from the inputs, reflecting the specific characteristics in infrared and visible images. The experimental performance demonstrates the effectiveness of the proposed modules, with superior improvement against the state-of-the-art, generalising a novel paradigm via transformer and adversarial learning in the fusion task.},
  archive      = {J_TIP},
  author       = {Dongyu Rao and Tianyang Xu and Xiao-Jun Wu},
  doi          = {10.1109/TIP.2023.3273451},
  journal      = {IEEE Transactions on Image Processing},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TGFuse: An infrared and visible image fusion approach based on transformer and generative adversarial network},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured attention composition for temporal action localization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2022.3180925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.},
  archive      = {J_TIP},
  author       = {Le Yang and Junwei Han and Tao Zhao and Nian Liu and Dingwen Zhang},
  doi          = {10.1109/TIP.2022.3180925},
  journal      = {IEEE Transactions on Image Processing},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structured attention composition for temporal action localization},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational structured attention networks for deep visual representation learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2021.3137647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have enabled major progresses in addressing pixel-level prediction tasks such as semantic segmentation, depth estimation, surface normal prediction and so on, benefiting from their powerful capabilities in visual representation learning. Typically, state of the art models integrate attention mechanisms for improved deep feature representations. Recently, some works have demonstrated the significance of learning and combining both spatial- and channel-wise attentions for deep feature refinement. In this paper, we aim at effectively boosting previous approaches and propose a unified deep framework to jointly learn both spatial attention maps and channel attention vectors in a principled manner so as to structure the resulting attention tensors and model interactions between these two types of attentions. Specifically, we integrate the estimation and the interaction of the attentions within a probabilistic representation learning framework, leading to VarIational STructured Attention networks (VISTA-Net). We implement the inference rules within the neural network, thus allowing for end-to-end learning of the probabilistic and the CNN front-end parameters. As demonstrated by our extensive empirical evaluation on six large-scale datasets for dense visual prediction, VISTA-Net outperforms the state-of-the-art in multiple continuous and discrete prediction tasks, thus confirming the benefit of the proposed approach in joint structured spatial-channel attention estimation for deep representation learning. The code is available at https://github.com/ygjwd12345/VISTA-Net.},
  archive      = {J_TIP},
  author       = {Guanglei Yang and Paolo Rota and Xavier Alameda-Pineda and Dan Xu and Mingli Ding and Elisa Ricci},
  doi          = {10.1109/TIP.2021.3137647},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational structured attention networks for deep visual representation learning},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). Stacked deconvolutional network for semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2019.2895460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and bring the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which enhances the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-ofthe- art results on four datasets, including PASCAL VOC 2012, CamVid, GATECH, COCO Stuff. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.},
  archive      = {J_TIP},
  author       = {Jun Fu and Jing Liu and Yuhang Wang and Jin Zhou and Changyong Wang and Hanqing Lu},
  doi          = {10.1109/TIP.2019.2895460},
  journal      = {IEEE Transactions on Image Processing},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stacked deconvolutional network for semantic segmentation},
  year         = {2019},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Monocular depth estimation with augmented ordinal depth relationships. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2877944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing algorithms for depth estimation from single monocular images need large quantities of metric groundtruth depths for supervised learning. We show that relative depth can be an informative cue for metric depth estimation and can be easily obtained from vast stereo videos. Acquiring metric depths from stereo videos is sometimes impracticable due to the absence of camera parameters. In this paper, we propose to improve the performance of metric depth estimation with relative depths collected from stereo movie videos using existing stereo matching algorithm.We introduce a new “Relative Depth in Stereo” (RDIS) dataset densely labelled with relative depths. We first pretrain a ResNet model on our RDIS dataset. Then we finetune the model on RGB-D datasets with metric ground-truth depths. During our finetuning, we formulate depth estimation as a classification task. This re-formulation scheme enables us to obtain the confidence of a depth prediction in the form of probability distribution. With this confidence, we propose an information gain loss to make use of the predictions that are close to ground-truth. We evaluate our approach on both indoor and outdoor benchmark RGB-D datasets and achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Yuanzhouhan Cao and Tianqi Zhao and Ke Xian and Chunhua Shen and Zhiguo Cao and Shugong Xu},
  doi          = {10.1109/TIP.2018.2877944},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Monocular depth estimation with augmented ordinal depth relationships},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Deep active learning with contaminated tags for image aesthetics assessment. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2018.2828326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic quality assessment has becoming an indispensable technique that facilitates a variety of image applications, e.g., photo retargeting and non-realistic rendering. Conventional approaches suffer from the following limitations: 1) the inefficiency of semantically describing images due to the inherent tag noise and incompletion, 2) the difficulty of accurately reflecting how humans actively perceive various regions inside each image, and 3) the challenge of incorporating the aesthetic experiences of multiple users. To solve these problems, we propose a novel semi-supervised deep active learning (SDAL) algorithm, which discovers how humans perceive semantically important regions from a large quantity of images partially assigned with contaminated tags. More specifically, as humans usually attend to the foreground objects before understanding them, we extract a succinct set of BING (binarized normed gradients) [60]-based object patches from each image. To simulate human visual perception, we propose SDAL which hierarchically learns human gaze shifting path (GSP) by sequentially linking semantically important object patches from each scenery. Noticeably, SDLA unifies the semantically important regions discovery and deep GSP feature learning into a principled framework, wherein only a small proportion of tagged images are adopted. Moreover, based on the sparsity penalty, SDLA can optimally abandon the noisy or redundant low-level image features. Finally, by leveraging the deeply-learned GSP features, a probabilistic model is developed for image aesthetics assessment, where the experience of multiple professional photographers can be encoded. Besides, auxiliary quality-related features can be conveniently integrated into our probabilistic model. Comprehensive experiments on a series of benchmark image sets have demonstrated the superiority of our method. As a byproduct, eye tracking experiments have shown that GSPs generated by our SDAL are about 93% consistent with real human gaze shifting paths.},
  archive      = {J_TIP},
  author       = {Zhenguang Liu and Zepeng Wang and Yiyang Yao and Luming Zhang and Ling Shao},
  doi          = {10.1109/TIP.2018.2828326},
  journal      = {IEEE Transactions on Image Processing},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep active learning with contaminated tags for image aesthetics assessment},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2010). Exploring duplicated regions in natural images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2010.2046599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Duplication of image regions is a common method for manipulating original images, using typical software like Adobe Photoshop, 3DS MAX, etc. In this study, we propose a duplication detection approach that can adopt two robust features based on discrete wavelet transform (DWT) and kernel principal component analysis (KPCA). Both schemes provide excellent representations of the image data for robust block matching. Multiresolution wavelet coefficients and KPCA-based projected vectors corresponding to image-blocks are arranged into a matrix for lexicographic sorting. Sorted blocks are used for making a list of similar point-pairs and for computing their offset frequencies. Duplicated regions are then segmented by an automatic technique that refines the list of corresponding point-pairs and eliminates the minimum offset-frequency threshold parameter in the usual detection method. A new technique that extends the basic algorithm for detecting Flip and Rotation types of forgeries is also proposed. This method uses global geometric transformation and the labeling technique to indentify the mentioned forgeries. Experiments with a good number of natural images show very promising results, when compared with the conventional PCA-based approach. A quantitative analysis indicate that the wavelet-based feature outperforms PCA- or KPCA-based features in terms of average precision and recall in the noiseless, or uncompressed domain, while KPCA-based feature obtains excellent performance in the additive noise and lossy JPEG compression environments.},
  archive      = {J_TIP},
  author       = {M. Bashar and K. Noda and N. Ohnishi and K. Mori},
  doi          = {10.1109/TIP.2010.2046599},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring duplicated regions in natural images},
  year         = {2010},
}
</textarea>
</details></li>
</ul>

</body>
</html>
