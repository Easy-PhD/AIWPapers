<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 31</h2>
<ul>
<li><details>
<summary>
(2025). ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3607961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based gesture recognition tasks, existing approaches based on graph convolutional networks (GCNs) struggle to capture the synergistic actions of nonadjacent graph nodes and the information conveyed by their long-range dependencies. Combining spatial and temporal transformers is a promising solution to address the limitation, inspired by the advantage of transformer in assessing nonadjacent long-range dependencies, but there lacks an effective strategy to integrate the spatial and temporal information extracted by these transformers. Therefore, this article proposes the spatial-temporal alternating graph convolution transformer (ST-GCN-AltFormer), which connects the spatial-temporal graph convolutional network (ST-GCN) with the spatial-temporal alternating transformer (AltFormer) architecture. In the AltFormer architecture, the spatial-temporal transformer branch employs a spatial transformer to capture information from specific frames, and uses a temporal transformer to analyze its evolution over the entire temporal range. Meanwhile, the temporal-spatial transformer branch extracts temporal information from specific nodes using a temporal transformer, and integrates it with a spatial transformer. The fusion enhances accurate spatial-temporal information extraction. Our method achieves superior performance compared to state-of-the-art methods, achieving accuracies of 97.5%, 95.8%, 94.3%, 92.8%, and 98.31% on the large-scale 3D hand gesture recognition (SHREC’17 Track), Dynamic Hand Gesture 14-28 (DHG-14/28), and leap motion dynamic hand gesture (LMDHG) dynamic gesture datasets, respectively.},
  archive      = {J_THMS},
  author       = {Qing Pan and Jintao Zhu and Lingwei Zhang and Gangmin Ning and Luping Fang},
  doi          = {10.1109/THMS.2025.3607961},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding and predicting temporal visual attention influenced by dynamic highlights in monitoring task. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3614364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring interfaces are crucial for dynamic, high-stakes tasks where effective user attention is essential. Visual highlights can guide attention effectively, but may also introduce unintended disruptions. To investigate this, we examined how visual highlights affect users’ gaze behavior in a drone monitoring task, focusing on when, how long, and how much attention they draw. We found that highlighted areas exhibit distinct temporal characteristics compared to nonhighlighted ones, quantified using normalized saliency (NS) metrics. We found that highlights elicited immediate responses, with NS peaking quickly, but this shift came at the cost of reduced search efforts elsewhere, potentially impacting situational awareness. To predict these dynamic changes and support interface design, we developed the Highlight-Informed Saliency Model, which provides granular predictions of NS over time. These predictions enable evaluations of highlight effectiveness and inform the optimal timing and deployment of highlights in future monitoring interface designs, particularly for time-sensitive tasks.},
  archive      = {J_THMS},
  author       = {Zekun Wu and Anna Maria Feit},
  doi          = {10.1109/THMS.2025.3614364},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Understanding and predicting temporal visual attention influenced by dynamic highlights in monitoring task},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model with fine-tuning for generalized few-shot activity recognition. <em>THMS</em>, 1-13. (<a href='https://doi.org/10.1109/THMS.2025.3613773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem we focused on in this article is sensor-based generalized few-shot activity recognition. In this problem, each of the predefined activity classes (i.e., base classes) has substantial training instances, while each of the new activity classes (i.e., novel classes) just has a few training instances. Both the base and the novel classes need to be recognized. Currently, just a few works focus on this problem, and no formal statement of the problem is provided. In this article, we provide a formal definition of the problem, and propose a method to address it. In the proposed method, adopting the strategy of fine-tuning deep learning models, a deep learning model is first learned with the base-class training instances, and then fine-tuned with resampled training instances from both the base and the novel classes. We evaluate our method with three publicly available datasets on 1-shot, 5-shot, and 10-shot learning tasks. The results on the evaluation metric of harmonic mean of the average per-class accuracy for the base classes and that for the novel classes show that, our method could outperform state-of-the-art methods. In addition, the time and resource cost of our method is moderate.},
  archive      = {J_THMS},
  author       = {Wei Wang and Qingzhong Li},
  doi          = {10.1109/THMS.2025.3613773},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Deep learning model with fine-tuning for generalized few-shot activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing exploration and cybersickness: Investigating curiosity-driven behavior in virtual environments. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3602125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality offers the opportunity for immersive exploration, yet it is often undermined by cybersickness. However, how individuals strike a balance between exploration and discomfort remains unclear. Existing method (e.g., reinforcement learning (RL)) often fail to fully capture the complexities of navigation and decision-making patterns. This study investigates how curiosity influences users’ navigation behavior, particularly how users strike a balance between exploration and discomfort. We propose curiosity as a key factor driving irrational decision-making and apply the free energy principle to model the relationship between curiosity and user behavior quantitatively. Our findings indicate that users generally adopt conservative strategies when navigating. Also, curiosity levels tend to rise when the virtual environment changes. These results illustrate the dynamic interplay between exploration and discomfort. In addition, it offers a new perspective on how curiosity drives behavior in immersive environments, providing a foundation for designing adaptive VR environments. Future research will further refine this model by incorporating additional psychological and environmental factors to improve prediction accuracy.},
  archive      = {J_THMS},
  author       = {Tangyao Li and Yuyang Wang},
  doi          = {10.1109/THMS.2025.3602125},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Balancing exploration and cybersickness: Investigating curiosity-driven behavior in virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHA-SCP: A UI element spatial hierarchy aware smartphone user click behavior prediction method. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3601578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting user click behavior and making relevant recommendations based on the user’s historical click behavior are critical to simplifying operations and improving user experience. Modeling User Interface (UI) elements is essential to user click behavior prediction, while the complexity and variety of the UI make it difficult to adequately capture the information of different scales. In addition, the lack of relevant datasets also presents difficulties for such studies. In response to these challenges, we construct a fine-grained smartphone usage behavior dataset containing 3 664 325 clicks of 100 users and propose a UI element Spatial Hierarchy Aware Smartphone user Click behavior Prediction method (SHA-SCP). SHA-SCP builds element groups by clustering the elements according to their spatial positions and uses attention mechanisms to perceive the UI at the element level and the element group level to fully capture the information of different scales. Experiments are conducted on the fine-grained smartphone usage behavior dataset, and the results show that our method outperforms the best baseline by an average of 18.35$\%$, 13.86$\%$, and 11.97$\%$ in Top-1 Accuracy, Top-3 Accuracy, and Top-5 Accuracy, respectively.},
  archive      = {J_THMS},
  author       = {Ling Chen and Qian Chen and Yiyi Peng and Kai Qian and Hongyu Shi and Xiaofan Zhang},
  doi          = {10.1109/THMS.2025.3601578},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SHA-SCP: A UI element spatial hierarchy aware smartphone user click behavior prediction method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3601386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has attracted significant attention. However, the skeleton spatial invariance and temporal context modeling are recent challenges for most existing methods. This work proposes a view-reset network, which integrates two important branches: spatial view-reset module (SVRM) and temporal attention module (TAM). In the SVRM, the skeleton model from different perspectives is reset in a unified coordinate system, eliminating the influence of viewpoint changes. In the TAM, the perception of temporal features is jointly enhanced by weighting each frame’s importance in the context. Furthermore, the pretrained residual network (ResNet) is used for prediction. The sample size is increased through data augmentation to improve the robustness of the model. The SVRM, TAM, and ResNet form an end-to-end learning network. The ablation study proved that the model could record the key skeletons and frames in the sequence and then reset the human body to a new position, making it easy for learning. The proposed model is evaluated on four challenging benchmarks based on the performance of the cross-view evaluation metrics. Experiments prove that the proposed model has superior performance and surpasses many state-of-the-art algorithms, with an increase of 1.91% over the top ten on the NTU RGB+D 60 dataset.},
  archive      = {J_THMS},
  author       = {Tianyu Ma and Xuna Wang and Hongwei Gao and Zide Liu and Jiahui Yu and Zhaojie Ju},
  doi          = {10.1109/THMS.2025.3601386},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context. <em>THMS</em>, 1-12. (<a href='https://doi.org/10.1109/THMS.2025.3601222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to high workload and often excessive working hours, team members in operating rooms perform surgical procedures under difficult conditions and often without sufficient breaks. Despite this, the team must deal with incomplete information and unexpected distractions. This requires a suitable level of attention and the ability to balance the demands of the task with available cognitive resources. Advances in measurement technology and data analysis in neurotechnology open up new possibilities for the assessment of attention processes. Increasingly complex and demanding surgeries, especially, could benefit from the application of neuroergonomic automated assistants to minimize distraction, stress, and fatigue, and to facilitate interactions between team members. Such assistants could improve performance via monitoring of cognitive and affective states as well as the implementation of suitable interventions strategies. Understanding the impact of distractions on performance, enhancing individuals’ resilience to distractions, and potentially employing artificial assistants to mitigate their effects are critical future goals. In order to support such future developments, a standard taxonomy of attention in the operating theater is needed, as is a broader consensus regarding the nature of distraction. Ideally, such a model would serve as a basis for comparison between studies conducted in different laboratories, and in principle could also be used to bridge the gap between the laboratory and the real scenario. Here, we propose the adoption of a model of attention previously shown to be effective for modeling levels of attention in immersion and describe its application in the surgical context.},
  archive      = {J_THMS},
  author       = {David Thinnes and Alexander L. Francis and Volkan Sayman and Daniel Guagnin and Matthias W. Laschke and Michael D. Menger and Jonas Roller and Daniel J. Strauss},
  doi          = {10.1109/THMS.2025.3601222},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG neurofeedback-based gait motor imagery training in lokomat enhances motor rhythms in complete spinal cord injury. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3603548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic interventions combining neurofeedback (NFB) and motor imagery (MI) are emerging strategies to promote cortical reorganization and functional training in individuals with complete spinal cord injury (SCI). This study proposes an electroencephalogram-based NFB approach for MI training, designed to teach the MI-related brain rhythmics modulation in Lokomat. For the purposes of this study, NFB is defined as a visual feedback training scheme. The proposed system introduces a formulation to minimize the default cortical effects that Lokomat produces on the individual’s activity during passive walking. Two individuals with complete SCI tested the proposed NFB system, in order to relearn the modulation of Mu ($\mu$ : 8–12 Hz) and Beta ($\beta$ : 13–30 Hz) rhythms over Cz, while receiving gait training with full weight support across 12 sessions. Each session consisted of the following three stages: 1) 2 min walking without MI (baseline); 2) 5 min walking with MI and True NFB; and 3) 5 min walking with MI and Sham NFB. The latter two stages were randomized session-by-session. The findings suggest that the proposed NFB approach may promote cortical reorganization and support the restoration of sensorimotor functions. Significant differences were observed between cortical patterns during True NFB and Sham NFB, particularly in the last intervention sessions. These results confirm the positive impact of the NFB system on gait motor training by enabling individuals with complete SCI to learn how to modulate their motor rhythms in specific cortical areas.},
  archive      = {J_THMS},
  author       = {Ericka R. da Silva Serafini and Cristian D. Guerrero-Mendez and Douglas M. Dunga and Teodiano F. Bastos-Filho and Anibal Cotrina Atencio and André F. O. de Azevedo Dantas and Caroline C. do Espírito Santo and Denis Delisle-Rodriguez},
  doi          = {10.1109/THMS.2025.3603548},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG neurofeedback-based gait motor imagery training in lokomat enhances motor rhythms in complete spinal cord injury},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ergodic imitation with corrections: Learning from implicit information in human feedback. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3603434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the prevalence of collaborative robots increases, physical interactions between humans and robots are inevitable—presenting an opportunity for robots to not only maintain safe working parameters with humans but also learn from these interactions. To develop adaptive robots, we first aim to analyze human responses to different errors through a study in which users are asked to correct any errors that the robot makes in various tasks. With this characterization of corrections, we can treat physical human–robot interactions as informative instead of ignoring physical interactions or leaving robots to return to the originally planned behaviors when interactions end. We incorporate physical corrections into existing learning from demonstration (LfD) frameworks, which allow robots to learn new skills by observing human demonstrations. We demonstrate that learning from physical interactions can improve task-specific performance metrics. The results reveal that including information about the behavior being corrected in the update improves task performance significantly compared to adding corrected trajectories alone. In a user study with an optimal control-based LfD framework, we also find that users are able to provide less feedback to the robot after each interaction update to the robot’s behavior. Utilizing corrections could enable advanced LfD techniques to be integrated into commercial applications for collaborative robots by enabling end-users to customize the robot’s behavior through intuitive interactions rather than by modifying the behavior in software.},
  archive      = {J_THMS},
  author       = {Junru Pang and Quentin Anderson-Watson and Kathleen Fitzsimons},
  doi          = {10.1109/THMS.2025.3603434},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Ergodic imitation with corrections: Learning from implicit information in human feedback},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bimanual manipulation of steady-hand eye robots with adaptive sclera force control: Cooperative versus teleoperation strategies. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3605011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing retinal vein cannulation (RVC) as a potential treatment for retinal vein occlusion without the assistance of a surgical robotic system is very challenging to do safely. The main limitation is the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The steady-hand eye robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not safely minimize the contact force between the surgical instrument and the sclera to prevent tissue damage. In addition, features such as haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. This work presents, for the first time in the field of robot-assisted retinal microsurgery research, a registration-free bimanual adaptive teleoperation (BMAT) control framework using SHER 2.0 and SHER 2.1 robotic systems. Both SHERs are integrated with an adaptive force control algorithm that dynamically and automatically minimizes the tool–sclera interaction forces, enforcing them within a safe limit. The scleral forces are measured using two fiber Bragg grating-based force-sensing tools. The performance of the proposed BMAT control framework is evaluated by comparison with a bimanual adaptive cooperative framework in a vessel-following experiment conducted under a surgical microscope. Experimental results demonstrate the effectiveness of the BMAT control framework in performing a safe bimanual telemanipulation of the eye without overstretching it, even in the absence of registration between the two robots.},
  archive      = {J_THMS},
  author       = {Mojtaba Esfandiari and Peter Gehlbach and Russell H. Taylor and Iulian I. Iordachita},
  doi          = {10.1109/THMS.2025.3605011},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Bimanual manipulation of steady-hand eye robots with adaptive sclera force control: Cooperative versus teleoperation strategies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A recent review on subjective and objective assessment of trust in human autonomy teaming. <em>THMS</em>, 1-15. (<a href='https://doi.org/10.1109/THMS.2025.3589981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing sophistication of autonomous systems and robotics has spurred research into the unique dynamics of human-autonomy teaming (HAT). These advanced technologies aim to enhance decision-making, situational awareness, mutual understanding, and interpersonal relationships, thereby minimizing risks in collaborative endeavors. However, achieving effective HAT requires careful attention to team trust, particularly in scenarios that move beyond simple human–machine dyads to involve complex, multiagent systems and distributed teams. This review undertakes a comprehensive exploration and evaluation of trust measurement techniques within the domain of HAT, focusing on methods that are sensitive to the nuances of these complex team dynamics. Emphasising the significance of trust measurement in optimising team performance, the review categorizes existing empirical works into subjective (e.g., self-report, questionnaires, surveys) and objective (e.g., behavioral, physiological) indices. Drawing insights from recent literature (2019–2024), the article explores the complexities of trust measurement, addressing methodologies employed by researchers and synthesizing their findings. The study suggests directions for further investigation into improving trust assessment techniques and developing practical models to better suit the evolving context of human-autonomy collaboration. The review highlights gaps in current methods and suggests avenues for future research, particularly in refining trust calibration models for dynamic, evolving contexts in human-autonomy collaboration and for understanding how trust is distributed and managed across complex team structures.},
  archive      = {J_THMS},
  author       = {Julakha Jahan Jui and Imali T. Hettiarachchi and Asim Bhatti and Mohamed Ragab Mahmoud Farghaly and Douglas Creighton},
  doi          = {10.1109/THMS.2025.3589981},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A recent review on subjective and objective assessment of trust in human autonomy teaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why highly reliable decision support systems often lead to suboptimal performance and what we can do about it. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3584662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a growing number of application domains, human decision-making is being supported by automated systems. While previous research has focused extensively on the negative consequences of automation support in terms of an overuse of such systems, we argue that this focus has largely overlooked another crucial issue: Humans often deteriorate the performance of automation. Specifically, human–automation dyads commonly perform worse than the system alone because humans, in an attempt to improve decisions, unfortunately interfere with correct system recommendations. This problem will only grow as systems based on artificial intelligence (AI) become more reliable and the gap between human-only and system-only performance continues to widen. We therefore outline the need for research that addresses this persisting and increasingly relevant issue. One approach to counteract this problem is to make systems more transparent and give humans more information on the system. However, while numerous explainability approaches have been brought forward, only very few show convincing effects. To be truly useful, we argue that systems need to be explainable in terms of effective behavioral guidance. Furthermore, beyond just thinking about how to enable humans to better adapt to the system (as is the case with explainability approaches), systems should be more human-centric, taking into account human strengths and weaknesses, and ultimately adapting to humans to enable synergy between humans and AI.},
  archive      = {J_THMS},
  author       = {Tobias Rieger and Linda Onnasch and Eileen Roesler and Dietrich Manzey},
  doi          = {10.1109/THMS.2025.3584662},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Why highly reliable decision support systems often lead to suboptimal performance and what we can do about it},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cinematographic-aware coherent shot assembly for how-to vlog generation. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3588768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The how-to vlog has gained popularity as a medium for learning practical skills, such as cooking and handcrafting. The sequencing process of these videos should meticulously integrate storytelling and cinematographic elements to ensure audience comprehension and engagement, posing a significant challenge for novice creators. Pioneer efforts have adhered to predefined editing rules or assembled clips based on textual logic, limiting their applicability across diverse educational scenarios and causing visual discontinuities. In contrast, we aim to extract rich cinematographic patterns from well-edited professional how-to vlogs to enhance shot assembly. To this end, we identify two crucial aspects influencing educational value: narrative continuity and scale transition. We model the shot assembly process as a task of selecting the next shot and devise a cinematographic-aware contrastive model to learn representations that distinguish between the good next shots against the bad ones. This method incorporates a two-stream cinematographic-aware encoding module for explicit factor encoding and a situation-adaptive attention-based integration module to accommodate varying assembly scenarios. Quantitative results from our novel professional user-generated vlogs dataset (proVlog-HowTo) clearly demonstrate the proposed method’s effectiveness. User study results further indicate its superiority in generating videos with narrative continuity and smooth transitions.},
  archive      = {J_THMS},
  author       = {Yuqi Zhang and Bin Guo and Ying Zhang and Nuo Li and Qianru Wang and Zhiwen Yu and Qing Li},
  doi          = {10.1109/THMS.2025.3588768},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cinematographic-aware coherent shot assembly for how-to vlog generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving tactical decision-making through multiobjective contrastive explanations. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3591163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the effectiveness of multiobjective counterfactual explanations (MOCEs) in helping individuals learn tactics, or rules of thumb, to apply when required to select a course of action in a specific context. In this setting, a counterfactual explanation compares one course of action against another. A MOCE presents this comparison by highlighting how the two options differ across a range of objectives or metrics. We conduct a study in which participants are presented with various scenarios alongside courses of action that could be implemented in those scenarios. Counterfactual explanations, including those involving multiple objectives, are used to identify the positive and negative aspects of the provided options. Participants were then required to identify the best course of action in various contexts. Participants trained with MOCE outperformed those given no explanations in seven of eight scenarios and those given single-objective counterfactual explanations (SOCEs) in four. SOCEs gave participants an aggregated outcome (expected rewards) without breaking these into specific objectives. MOCE improved tactic learning, but participants provided with SOCE or no explanation performed better in multitactic scenarios. These findings suggest that MOCE enhances tactical decision-making, but further research is needed for multitactic integration.},
  archive      = {J_THMS},
  author       = {Michelle Blom and Ronal Singh and Tim Miller and Liz Sonenberg and Kerry Trentelman and Adam Saulwick and Steven Wark},
  doi          = {10.1109/THMS.2025.3591163},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Improving tactical decision-making through multiobjective contrastive explanations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the validity of multiparticipant distributed simulation for understanding and modeling road user interaction. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3591506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding driver–pedestrian interactions at unsignalized locations has gained additional importance due to recent advancements in vehicle automation. Naturalistic observations can only provide correlational data of limited value for understanding and modeling the mechanisms underlying road user interaction. Therefore, controlled studies in virtual reality (VR) are an important complement, but conventional methods can only accommodate a single human participant. Recently, there has been some interest in studying interactions in VR, by means of distributed simulation, involving multiple human participants. However, there is a lack of validation of this method. Here, we provide a validation study, focusing on a distributed vehicle–pedestrian interaction setup, where pairs of one driver and one pedestrian interacted under various kinematic conditions in a connected virtual environment. To test the validity of the distributed simulation, we used a naturalistic dataset collected in the same U.K. city, at similar locations, and compared the observed behavior between the two settings. Our results indicate a good relative validity of the simulator study, where road users showed similar nonverbal communication behavior in both datasets. As an additional means of validation, we also leveraged a set of game theoretic models that were developed based on the simulator studies, and found that when applied to the naturalistic dataset, we obtained similar (although not identical) model selection results. The findings suggest that distributed simulation can also be useful for development of computational models of interaction. Overall, the findings suggest that distributed simulation can be a highly valuable tool for studying and modeling road user interactions.},
  archive      = {J_THMS},
  author       = {Amir Hossein Kalantari and Yi-Shin Lin and Ali Mohammadi and Natasha Merat and Gustav Markkula},
  doi          = {10.1109/THMS.2025.3591506},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Testing the validity of multiparticipant distributed simulation for understanding and modeling road user interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design graph guided element importance-aware layout generation with multimodality cascade transformer. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3590785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphic designs are pervasive in our daily lives and widely used to communicate information hierarchically to humans. To achieve this, the layout plays an essential role in guiding readers to understand the importance of different elements and comprehend the content. To deal with the rapidly increasing demands of graphic designs, recent studies attempt to automatically generate layouts based on category information and spatial relations, often resulting in layouts with poor communication quality. In this article, we make the first attempt to explore element importance-aware layout generation under the guidance of a novel design graph, which attracts readers’ attention to a layout by formulating aesthetic relations implicitly involved in graphic designs between element pairs. The core of our approach is a learning-based framework with a new multimodality cascade transformer (MCT) in a coarse-to-fine manner. A hierarchical multimodality fusion (HMF) mechanism and two new losses are introduced to guide the training process progressively. We further collect a new fine-grained advertisement poster layout dataset containing more than 30 K layouts labeled with 91 element labels. Both qualitative and quantitative experiments demonstrate the effectiveness of our approach against existing works. We also conduct user studies and cognitive experiments to evaluate the direct adaptability and attractiveness of generated layouts.},
  archive      = {J_THMS},
  author       = {Qiuyun Zhang and Bin Guo and Lina Yao and Xiaotian Qiao and Hao Wang and Ying Zhang and Zhiwen Yu},
  doi          = {10.1109/THMS.2025.3590785},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design graph guided element importance-aware layout generation with multimodality cascade transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural Human–Robot interaction for vascular interventional surgery: Design and evaluation of a leader device with haptic feedback. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3591542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted vascular interventional surgery is a hot topic in the interdisciplinary field of medical science and engineering, it has important research significance. Compared with traditional manual operation, it shows obvious advantages, such as avoiding the radiation exposure to the interventionist, reducing the workload of the interventionist, improving the precision of the operation, and guaranteeing the safety of the operation. However, the human–robot interaction ability has always been a major challenge that cannot be ignored. In this article, we designed and developed a leader device that enables the interventionists to experience the state of surgical instruments naturally and realistically when operating it, as if they were operating actual surgical instruments. The innovations can be summarized as follows: the structure of the leader device is highly consistent with the interventionist’s operating habit, which greatly reduces the interventionist’s operating difficulty. Haptic feedback in both translational direction and circumferential direction is achieved to provide haptic guidance to the interventionists and improve their surgical presence. The leader device is optimized in both structural design and functional realization compared to existing leader devices. This study has great potential in improving human-robot interaction ability, and could provide a reference for the design and evaluation of the leader device.},
  archive      = {J_THMS},
  author       = {Xiaoliang Jin and Aiguo Song and Lifeng Zhu and Cheng Wang},
  doi          = {10.1109/THMS.2025.3591542},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Natural Human–Robot interaction for vascular interventional surgery: Design and evaluation of a leader device with haptic feedback},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RingFormer: A neural vocoder with ring attention and convolution-augmented transformer. <em>THMS</em>, 1-9. (<a href='https://doi.org/10.1109/THMS.2025.3591502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging. Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution. This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information. In addition, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical. To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer. Ring attention effectively captures local details while integrating global information, making it well suited for processing long sequences and enabling real-time audio generation. RingFormer is trained using adversarial training with two discriminators. It is integrated as a replacement for the decoder module in the variational inference with adversarial learning for end-to-end text-to-speech (VITS) text-to-speech framework, enabling fair comparisons with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics. Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.},
  archive      = {J_THMS},
  author       = {Seongho Hong and Yong-Hoon Choi},
  doi          = {10.1109/THMS.2025.3591502},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {RingFormer: A neural vocoder with ring attention and convolution-augmented transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic estimation of mental workload and operator accuracy for time-constrained binary classification tasks. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3591550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human cognitive states, such as mental workload, play a pivotal role in decision making processes within human automation teams. Although subjective measures of mental workload can be obtained using standard questionnaires, such as the NASA-TLX, their administration is often impractical as it interferes with the primary tasks of the human operator. Therefore, it is of interest to estimate these subjective measures from less intrusive observations. Evidence suggests that mental workload is a dynamic process so incorporating historical measurements could reduce its estimation error. In addition, the estimation of operator performance in human automation teams is essential in optimizing task effectiveness and facilitating efficient resource allocation. In this work, we consider a scenario where a human and an automation solve binary classification tasks under time constraints. We present and compare different dynamic schemes to estimate the operator’s performance, i.e., classification accuracy, and its subjective ratings on subscales of the NASA-TLX questionnaire, which measure mental workload across multiple dimensions. These schemes differ in the information available for estimation. We test these schemes on data collected from a scenario, where a human and an automation perform a series of classification tasks for simulated mobile objects. Our analysis of the interaction data and the estimation schemes indicates that employing dynamic estimation for certain NASA-TLX subscale ratings leads to decreased estimation errors.},
  archive      = {J_THMS},
  author       = {Raihan Seraj and Aditya Mahajan and Jerome Le Ny},
  doi          = {10.1109/THMS.2025.3591550},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Dynamic estimation of mental workload and operator accuracy for time-constrained binary classification tasks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle routing incorporating implicit preferences: An omnidimensional Human–Algorithm collaboration approach. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3594577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity and dynamism of real-world needs, traditional vehicle routing solutions usually do not sufficiently consider implicit user preferences, leading to low acceptance and reduced practical performance. To address this issue, we propose an omnidimensional human–algorithm collaboration framework that features a novel “task assignment first, route construction second” algorithm and omnidimensional human–algorithm interaction methods. This framework supports users in expressing complex preference information across four dimensions: decision variables, constraints, objectives, and solutions. The algorithm is designed to learn user preferences and utilize computational search to provide efficient, tailored feedback. A user-friendly graphical interface system was developed to facilitate intuitive human–algorithm interaction. We conducted user experiments with 20 participants based on a real-world multiobjective road cleaning problem. The results demonstrated the system’s effectiveness in incorporating users’ implicit preferences—beyond explicit preferences—and its usability in providing positive user experience. This study represents a significant step toward making vehicle routing algorithms more practical and user-centric.},
  archive      = {J_THMS},
  author       = {Mingwei Chen and Liang Ma and Xiaofang Wang},
  doi          = {10.1109/THMS.2025.3594577},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Vehicle routing incorporating implicit preferences: An omnidimensional Human–Algorithm collaboration approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-centered fully adaptive radar for gesture recognition in smart environments. <em>THMS</em>, 1-12. (<a href='https://doi.org/10.1109/THMS.2025.3591369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, radio frequency (RF) sensing or radar has garnered great interest as an emerging modality to enable human–computer interaction via gesture recognition. Current approaches involve utilization of a radar system that transmits a fixed signal with predetermined frequency, bandwidth, and other waveform parameters. However, gesture recognition accuracy can be greatly impacted by radar transmission parameters, which affect computational load and performance. In this work, we introduce a human-centered, fully adaptive radar (HC-FAR) system for ambient gesture recognition in which a programmable, software-defined radar system dynamically changes its RF transmission in response to human behavior. We design and switch between different transmission modes for different human-computer interaction tasks—human presence detection, trigger detection, and command translation—as well as alter processing so as to minimize computational load. In this way, the proposed HC-FAR paradigm enables dynamic management of the tradeoffs between dimensionality of RF data representations and their resulting computational load with real-time classification accuracy. Our results show that HC-FAR significantly reduces the allocation of computational and spectral resources, while enhancing fine-grain gesture recognition via a joint domain multi-input deep neural network, which takes as input the RF micro-Doppler signature, range, and angle profile.},
  archive      = {J_THMS},
  author       = {Emre Kurtoğlu and Sevgi Z. Gurbuz},
  doi          = {10.1109/THMS.2025.3591369},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-centered fully adaptive radar for gesture recognition in smart environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StoryExplorer: A visualization framework for storyline generation of textual narratives. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3592357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the exponentially increasing volume of narrative texts such as novels and news, readers struggle to extract and consistently remember storylines from these intricate texts due to the constraints of human working memory and attention span. To tackle this issue, we propose a visualization approach StoryExplorer, which facilitates the process of knowledge externalization of narrative texts and further makes the form of mental models more coherent. Through the formative study and close collaboration with two domain experts, we identified key challenges for the extraction of the storyline. Guided by the distilled requirements, we then propose a set of workflow (i.e., insight finding-scripting-storytelling) to enable users to interactively generate fragments of narrative structures. We then propose a visualization system StoryExplorer that combines stroke annotation and GPT-based visual hints to quickly extract story fragments and interactively construct storylines. To evaluate the effectiveness and usefulness of StoryExplorer, we conducted two case studies and in-depth user interviews with 16 target users. The result shows that users can conveniently and effectively extract the storyline by using StoryExplorer along with the proposed workflow.},
  archive      = {J_THMS},
  author       = {Li Ye and Lei Wang and Shaolun Ruan and Heyu Wang and Yuwei Meng and Yigang Wang and Wei Chen and Zhiguang Zhou},
  doi          = {10.1109/THMS.2025.3592357},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {StoryExplorer: A visualization framework for storyline generation of textual narratives},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palpation characteristics of an instrumented virtual cricothyroidotomy simulator. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3592791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cricothyroidotomy (CCT) is a critical, life-saving procedure requiring the identification of key neck landmarks through palpation. Interactive virtual simulation offers a promising, cost-effective approach to CCT training with high visual realism. However, developing the palpation skills necessary for CCT requires a haptic interface with tactile sensitivity comparable to human fingers. Such interfaces are often represented by plastic partial mannequins, which require further adaptation to integrate into virtual environments. This study introduces an instrumented physical palpation interface for CCT, integrated into a virtual surgical simulator, and tested on 10 surgeons who practiced the procedure over a training period. Data on haptic interactions collected during the training was analyzed to evaluate participants’ palpation skills and explore their force modulation strategies about landmark identification scores. Our findings suggest that trainees become more precise in their exploration over time, apply greater normal forces around target areas. Initial landmark identification performance influences adjustments in the overall applied pressure.},
  archive      = {J_THMS},
  author       = {Melih Turkseven and Trudi Di Qi and Ganesh Sankaranarayanan and Suvranu De},
  doi          = {10.1109/THMS.2025.3592791},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Palpation characteristics of an instrumented virtual cricothyroidotomy simulator},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Role-based human-machine collaboration task-allocation strategy in multiagent environment. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3591603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human–machine collaboration task-allocation problem involves three major challenges: role diversity, capability heterogeneity, and task dynamics. Most existing studies treat humans and machines as parallel units through a static “Human + Machine” additive paradigm, which neglects the evolution of capability during collaboration. Some works “deeply couple” relatively low-autonomy machines with humans, thereby limiting the system’s flexibility in resource scheduling and dynamic reconfiguration. This study analyzes the problem from a multiagent system perspective and classifies execution units into three types: human agents, machine agents, and human–machine collaborative agents, and distinguishes between their independent and collaborative capabilities. Next, we propose the dynamic short-board balance synergy assessment method, which integrates the “short-board” concept to quantify collaboration performance and leverages agents that have low independent but high collaborative capabilities. By incorporating multiple constraints, we establish the role-based human–machine collaboration (RBHMC) model, prove its NP-hardness, and design a multi-level solving approach to handle small-scale and medium-to-large-scale data separately. The experimental results indicate that, compared with “Human + Machine” and “Deep Coupling” models, RBHMC outperforms in task completion rate, resource utilization, and system robustness. An industrial case study further validates its applicability and superiority in real-world settings. Finally, RBHMC’s transferability is validated through vertical technology adaptation and horizontal scenario migration, providing a scalable solution for multidomain human–machine collaboration in complex scenarios.},
  archive      = {J_THMS},
  author       = {Xinlei Zhang and Zhaoquan Zhu and Yuanbai Li and Haibin Zhu and Yuxiang Sun and Xianzhong Zhou},
  doi          = {10.1109/THMS.2025.3591603},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Role-based human-machine collaboration task-allocation strategy in multiagent environment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). View-embedding GCN for skeleton-based cross-view gait recognition. <em>THMS</em>, 1-12. (<a href='https://doi.org/10.1109/THMS.2025.3595213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait has emerged as a promising biometric modality due to its noninvasive nature and the ability to capture samples from a distance. Model-based gait recognition using skeleton data conveys rich information that remains invariant to carried objects and clothing variations. However, viewing a person from different angles alters their gait posture, resulting in increased intrasubject variability compared to intersubject variability. Therefore, we propose a novel framework, view-embedding modified residual graph convolutional network (VeMResGCN), for cross-view gait recognition (CVGR) by exploiting two modules: modified residual graph convolutional network (MResGCN) and view-embedding feature extraction (VeFE) for view-invariant features. A state-of-the-art pose estimation algorithm extracts skeleton key points from raw video input, from which multiple features (e.g., relative joint positions, motion velocities, and bone structures) are computed. The final feature vector for gait recognition is computed by consolidating the features from the MResGCN and VeFE modules. To the best of authors’ knowledge, this work is the first to extract view-invariant features in a unified graph convolutional network (GCN) for skeleton-based CVGR. We evaluate our proposed framework on two of the largest publicly available skeleton datasets, CASIA-B and OUMVLP-Pose, under challenging covariates of clothing variation and carried objects. Results demonstrate that VeMResGCN significantly outperforms state-of-the-art methods with average rank-1 accuracies of 90.3%, 80.7%, and 73.4% for normal, carried object, and clothing variations on CASIA-B, and 71.0% on OU-MVLP in terms of skeleton-based CVGR. These results demonstrate the ability of our proposed framework to maintain superior CVGR performance despite the presence of carried objects and clothing variations. The proposed framework holds strong implications for real-world biometric applications, including robust person reidentification and surveillance systems, where maintaining consistent recognition across varying views and covariates is crucial.},
  archive      = {J_THMS},
  author       = {Md. Zasim Uddin and Ausrukona Ray and Borsha Das and Md. Atiqur Rahman Ahad},
  doi          = {10.1109/THMS.2025.3595213},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {View-embedding GCN for skeleton-based cross-view gait recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social whole-body movement aids the objective classification of children with autism and typical development by implementing a machine learning approach. <em>THMS</em>, 1-9. (<a href='https://doi.org/10.1109/THMS.2025.3595901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is characterized by high prevalence and a time-consuming diagnostic procedure that greatly depends on clinicians’ experience. An objective and efficient approach to identifying ASD is urgently called upon. In current clinical practice, although the diagnosis of ASD is primarily based on the individual’s behavioral presentation, few studies have examined the feasibility of using behavioral indices to identify ASD, and none has investigated whether behavioral features extracted from social whole-body motor behavior during a face-to-face interaction could aid an accurate detection of ASD. This study aimed to address this question by applying machine learning (ML) algorithms to classify children with ASD and typical development (TD). Twenty children with ASD and twenty-one children with TD were videotaped in a face-to-face conversation. A motion energy analysis technique extracted the whole-body movement time series. Features computed were the power of eighty-six frequencies obtained by fast-Fourier transform. Support vector machine and leave-one-out cross validation were implemented. Results showed that a maximum classification accuracy of 97.56% (specificity = 100%, sensitivity = 95.00% ) could be achieved with three features. The contactless and calibration-free approach proposed in this study may help facilitate an objective, effective and efficient diagnosis of ASD.},
  archive      = {J_THMS},
  author       = {Zhong Zhao and Ziyan Qiu and Xiaobin Zhang and Xingda Qu and Xinyao Hu and Jianping Lu},
  doi          = {10.1109/THMS.2025.3595901},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Social whole-body movement aids the objective classification of children with autism and typical development by implementing a machine learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating HMIs to foster communications between conventional vehicles and autonomous vehicles at intersections. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3595746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mixed traffic environments that involve conventional vehicles (CVs) and autonomous vehicles (AVs), it is essential for CV drivers to maintain an appropriate level of situation awareness (SA) to ensure safe and efficient interactions with AVs. While previous research has established the benefits of external human–machine interfaces (HMIs) for communicating AV intent, this study extended this knowledge by focusing on the vital but underexplored interaction with CV drivers. Specifically, we investigated how AV communication through HMIs affected CV drivers by systematically comparing internal (iHMI) and external (eHMI) interfaces, and examined their impact on CV driver awareness, cognitive load, and behavior. Initially, we designed eight HMI concepts through a human-centered design process. The two highest-rated concepts were selected for implementation as eHMIs and iHMIs. Subsequently, we designed a within-subjects experiment with three conditions: a control condition without any communication HMI, and two treatment conditions using eHMIs and iHMIs as communication means. We investigated the effects of these conditions on 50 participants in a virtual environment (VR) driving simulator. Self-reported assessments and eye-tracking measures were employed to evaluate participants’ SA, trust, acceptance, and mental workload. Results indicated that the iHMI condition resulted in superior SA among participants and improved trust in the AV compared to the control and eHMI conditions. Additionally, iHMI led to a comparatively lower increase in mental workload compared to the other two conditions. Our study contributes to the development of effective AV-CV communications and has the potential to inform the design of future AV systems.},
  archive      = {J_THMS},
  author       = {Lilit Avetisyan and Aditya Deshmukh and X. Jessie Yang and Feng Zhou},
  doi          = {10.1109/THMS.2025.3595746},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Investigating HMIs to foster communications between conventional vehicles and autonomous vehicles at intersections},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable viewpoint gesture recognition based on a hybrid graph neural network. <em>THMS</em>, 1-9. (<a href='https://doi.org/10.1109/THMS.2025.3597964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The variations in camera view and hand spatial pose are the main reasons for the low accuracy and poor robustness of gesture recognition systems. In order to achieve accurate and stable gesture recognition with variable viewpoint, this article carries out research based on 3-D non-Euclidean vector graph features and graph neural networks. First, the 3-D information of hand joints is collected to construct a graph-structured gesture feature dataset, and a joint-based 3-D non-Euclidean vector graph method is proposed to solve the problem that similar gesture features are overly sensitive to spatial position and angle changes. Then, a Multi-Head graph attention network is designed and combined with graph convolutional neural network to explore the optimal hybrid graph neural network gesture recognition model. The experimental results show that, on the dataset processed by the joint-based 3-D non-Euclidean vector graph method, the training, testing, and validation accuracies of the optimal model reach 97.07%, 96.95%, and 87.06%, which are increased by 18.46%, 18.88%, and 44.23% compared to the original dataset, respectively. In conclusion, the method in this article is not only more robust to the variable viewpoint gesture recognition problem, but also has the advantages of low computational resource requirement and high real-time performance.},
  archive      = {J_THMS},
  author       = {Shaoxin Sun and Guanghui Chen and Xiaojie Su and Zhenshan Bing and Alois Knoll},
  doi          = {10.1109/THMS.2025.3597964},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Variable viewpoint gesture recognition based on a hybrid graph neural network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validation of a formal method for human error rate prediction with negative transfer. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3593085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human error is often associated with system failures. The complexity of human-automation interaction can make it difficult to anticipate what errors can occur and how they contribute to failures. Previous research has shown that task analytic behavior modeling with the enhanced operator function model and the cognitive reliability analysis method (CREAM) can be combined with statistical model checking to make predictions about human error rates, their stochastic impact on system failures, and the effect of negative transfer of design changes on these predictions. These efforts were successful, but the validation studies used artificial examples with limited data. Predictions also slightly overestimated error rates. This article addresses these deficiencies by conducting a validation study based on the prescription order entry interface of the OpenEMR electronic medical record. As part of this, we explored how prediction accuracy for the OpenEMR application changed based on the inclusion/exclusion of planning errors: errors based on people’s ability to formulate task plans, which we hypothesized contributed to error rate overestimation. Results found that our method’s predictions aligned with those observed in the experiment, especially when planning errors were excluded. Negative transfer conditions did not manifest significant differences in error rates experimentally or in model predictions. These results suggest that negative transfer’s impact on human–computer interaction may be overstated in the literature. Finally, higher error rates were observed between the original OpenEMR prescription order entry interface compared to an alternative that we tested. We highly suggest that OpenEMR adopt the alternative.},
  archive      = {J_THMS},
  author       = {Yeonbin Son and Matthew L. Bolton and Emma Crooks and Hannah Palmer and Eunsuk Kang and Christopher Daly},
  doi          = {10.1109/THMS.2025.3593085},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Validation of a formal method for human error rate prediction with negative transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised face deocclusion via 3-D face reconstruction with outlier segmentation. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3585780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face occlusion poses a challenge for many human–machine systems, such as facial expression perception, social signal analysis, and human identity verification. Accurate face deocclusion is essential for improving the performance of identity recognition, expression recognition, and the robustness of human–machine systems. As a result, this area has attracted significant attention from researchers in recent years. However, most existing methods rely heavily on synthetic occluded face datasets and predefined occlusion masks labels, which limits their applicability in real-world scenarios. To this end, we propose a novel self-supervised generative adversarial networks (GANs)-based framework for face deocclusion in this study, which integrates 3-D facial reconstruction with outlier segmentation guidance. To achieve reliable self-supervised occlusion guidance, we introduce an outlier segmentation module that utilizes statistical priors to generate accurate occlusion masks, facilitating the deocclusion process. Furthermore, we design a GAN-based dual-branch module, which is capable of simultaneously generating the occlusion mask and the deoccluded face. Extensive experiments on the widely used datasets demonstrate the superior performance of our approach on existing methods. Our method achieves 35.71 in peak signal to noise ratio (PSNR) and 0.891 in structural similarity index measure (SSIM) for occluded face restoration, outperforming state-of-the-art techniques.},
  archive      = {J_THMS},
  author       = {Haodong Jin and Muwei Jian and Derui Ding and Hui Yu},
  doi          = {10.1109/THMS.2025.3585780},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Self-supervised face deocclusion via 3-D face reconstruction with outlier segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeing bias through eye movements: Investigating empathy and engagement with authentic and altered refugee imagery. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3585452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates perceptual biases in responses to Syrian and Ukrainian refugee images using human–computer interaction (HCI) techniques. We hypothesized that media portrayals might shape empathy and influence both biases and donation behavior. Participants viewed both authentic and face-swapped images, with eye-tracking data capturing pupil size and decision-making patterns. In Experiment 1, participants’ eye-tracking data and donation behavior indicated higher engagement and donations for authentic images, suggesting a preference for unaltered images without ethnicity-based bias. Experiment 2 explored this further by presenting cropped refugee faces alongside neutral or intellectual art, asking participants to choose the art that best matched each face. Results showed no ethnic bias. Findings suggest that face-swapping reduces emotional engagement, and that facial authenticity is critical to empathy (Experiment 1), while no bias toward specific ethnic features emerged in the cropped face task (Experiment 2). This research underscores the value of HCI methods in uncovering implicit biases and supports the use of authentic imagery in media to foster empathy in humanitarian efforts.},
  archive      = {J_THMS},
  author       = {Reem Albaghli and Akshay Sunil Gurnaney and Nada Attar},
  doi          = {10.1109/THMS.2025.3585452},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Seeing bias through eye movements: Investigating empathy and engagement with authentic and altered refugee imagery},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
