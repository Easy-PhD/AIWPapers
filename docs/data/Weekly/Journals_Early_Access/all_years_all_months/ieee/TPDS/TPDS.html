<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds">TPDS - 29</h2>
<ul>
<li><details>
<summary>
(2025). SSpMM: Efficiently scalable SpMM kernels across multiple generations of tensor cores. <em>TPDS</em>, 1-17. (<a href='https://doi.org/10.1109/TPDS.2025.3616981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse-Dense Matrix-Matrix Multiplication (SpMM) has emerged as a foundational primitive in HPC and AI. Recent advancements have aimed to accelerate SpMM by harnessing the powerful Tensor Cores found in modern GPUs. However, despite these efforts, existing methods frequently encounter performance degradation when ported across different Tensor Core architectures. Recognizing that scalable SpMM across multiple generations of Tensor Cores relies on the effective use of general-purpose instructions, we have meticulously developed a SpMM library named SSpMM. However, a significant conflict exists between granularity and performance in current Tensor Core instructions. To resolve this, we introduce the innovative Transpose Mapping Scheme, which elegantly implements fine-grained kernels using coarse-grained instructions. Additionally, we propose the Register Shuffle Method to further enhance performance. Finally, we introduce Sparse Vector Compression, a technique that ensures our kernels are scalable with both structured and unstructured sparsity. Our experimental results, conducted on four generations of Tensor Core GPUs using over 3,000 sparse matrices from well established matrix collections, demonstrate that SSpMM achieves an average speedup of 2.04×, 2.81×, 2.07×, and 1.87×, respectively, over the state-of-the-art SpMM solution. Furthermore, we have integrated SSpMM into PyTorch, achieving a 1.81× speedup in end-to-end Transformer inference compared to cuDNN.},
  archive      = {J_TPDS},
  author       = {Zeyu Xue and Mei Wen and Jianchao Yang and Minjin Tang and Zhongdi Luo and Jing Feng and Yang Shi and Zhaoyun Chen and Junzhong Shen and Johannes Langguth},
  doi          = {10.1109/TPDS.2025.3616981},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SSpMM: Efficiently scalable SpMM kernels across multiple generations of tensor cores},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximation algorithms for scheduling with/without deadline constraints where rejection costs are proportional to processing times. <em>TPDS</em>, 1-13. (<a href='https://doi.org/10.1109/TPDS.2025.3605674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study two offline job scheduling problems where tasks can be processed on a limited number of energy-efficient edge machines or offloaded to an unlimited supply of energy-inefficient cloud machines (called rejected). The objective is to minimize total energy consumption. First, we consider scheduling without deadlines, formulating it as a scheduling problem with rejection, where rejection costs are proportional to processing times. We propose a novel $\frac{5}{4}(1+\epsilon )$-approximation algorithm, $\mathcal{BEKP}$, by associating it to a Multiple Subset Sum problem, improving upon the existing $(\frac{3}{2} - \frac{1}{2m})$-approximation for arbitrary rejection costs. Next, we address scheduling with deadlines, aiming to minimize the weighted number of rejected jobs. We position this problem within the literature and introduce a new $(1-\frac{(m-1)^{m}}{m^{m}})$-approximation algorithm, $\mathcal{MDP}$, inspired by an interval selection algorithm with a $(1-\frac{m^{m}}{(m+1)^{m}})$-approximation for arbitrary rejection costs. Experimental results demonstrate that $\mathcal{BEKP}$ and $\mathcal{MDP}$ obtain better results (lower costs or higher profits) than other state-of-the-art algorithms while maintaining a competitive or better time complexity.},
  archive      = {J_TPDS},
  author       = {Olivier Beaumont and Rémi Bouzel and Lionel Eyraud-Dubois and Esragul Korkmaz and Laércio Lima Pilla and Alexandre Van Kempen},
  doi          = {10.1109/TPDS.2025.3605674},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Approximation algorithms for scheduling with/without deadline constraints where rejection costs are proportional to processing times},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ToT: Triangle counting on tensor cores. <em>TPDS</em>, 1-14. (<a href='https://doi.org/10.1109/TPDS.2025.3606878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle counting is a fundamental graph algorithm used to identify the number of triangles within a graph. This algorithm can be reformulated into linear algebraic operations, including sparse matrix multiplication, intersection and reduction. Modern GPUs, equipped with Tensor Cores, offer massive parallelism that can significantly accelerate graph algorithms. However, leveraging Tensor Cores, originally designed for dense matrix multiplication, to handle sparse workloads for triangle counting presents non-trivial challenges. In this paper, we conduct an in-depth analysis of the state-of-the-art techniques that utilize Tensor Cores for matrix operations, identifying critical performance shortfalls. Based on these insights, we introduce ToT, which enhances the utilization of Tensor Cores and expands their functionalities for diverse sparse matrix operations. In experiments, ToT is evaluated against state-of-the-art methods. ToT outperforms the second-fastest method with a 3.81× speedup in end-to-end execution. Also, it achieves up to 17.00× memory savings. This work represents a pioneering exploration into utilizing Tensor Cores for accelerating the triangle counting algorithm.},
  archive      = {J_TPDS},
  author       = {YuAng Chen and Jeffrey Xu Yu},
  doi          = {10.1109/TPDS.2025.3606878},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ToT: Triangle counting on tensor cores},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedBiF: Communication-efficient federated learning via bits freezing. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3610224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging distributed machine learning paradigm that enables collaborative model training without sharing local data. Despite its advantages, FL suffers from substantial communication overhead, which can affect training efficiency. Recent efforts have mitigated this issue by quantizing model updates to reduce communication costs. However, most existing methods apply quantization only after local training, introducing quantization errors into the trained parameters and potentially degrading model accuracy. In this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework that directly learns quantized model parameters during local training. In each communication round, the server first quantizes the model parameters and transmits them to the clients. FedBiF then allows each client to update only a single bit of the multi-bit parameter representation, freezing the remaining bits. This bit-by-bit update strategy reduces each parameter update to one bit while maintaining high precision in parameter representation. Extensive experiments are conducted on five widely used datasets under both IID and Non-IID settings. The results demonstrate that FedBiF not only achieves superior communication compression but also promotes sparsity in the resulting models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication. The code is available at https://github.com/Leopold1423/fedbif-tpds25.},
  archive      = {J_TPDS},
  author       = {Shiwei Li and Qunwei Li and Haozhao Wang and Ruixuan Li and Jianbin Lin and Wenliang Zhong},
  doi          = {10.1109/TPDS.2025.3610224},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedBiF: Communication-efficient federated learning via bits freezing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedSR: A semi-decentralized federated learning framework for non-IID data based on incremental subgradient optimization. <em>TPDS</em>, 1-14. (<a href='https://doi.org/10.1109/TPDS.2025.3611304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Industrial Internet of Things (IoT), data heterogeneity across different devices poses a huge challenge to federated learning techniques, significantly reducing the performance of federated learning models. Additionally, the large number of devices participating in IoT federated learning and training imposes a substantial computational burden on cloud servers. Current federated learning research primarily adopts centralized or discentralized learning architectures, which cannot fundamentally solve these issues. To address this, we propose a novel semi-centralized cloud-edge-device hierarchical federate learning framework that integrated both centralized and decentralized federated learning approaches. Specifically, only a subset of adjacent devices forms small-scale ring clusters, and the cloud server aggregates the ring models to construct a global model. To mitigate the impact of data heterogeneity across devices, we use an incremental subgradient optimization algorithm within each ring cluster to enhance the generalization ability of the ring cluster models. Extensive experiments demonstrate that our approach effectively reduces the impact of data heterogeneity, improves model performance, and significantly alleviates the communication burden on cloud servers compared to centralized and discentralized federated learning frameworks. Indeed, the framework proposed in this paper aims to balance the strengths of centralized federated learning and ring federated learning. It achieves superior performance in addressing the data non-IID problem compared to centralized federated learning architectures while also mitigating issues associated with excessively large rings in ring architectures.},
  archive      = {J_TPDS},
  author       = {Jianjun Huang and Hao Huang and Li Kang and Lixin Ye},
  doi          = {10.1109/TPDS.2025.3611304},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedSR: A semi-decentralized federated learning framework for non-IID data based on incremental subgradient optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New scheduling algorithm and analysis for partitioned periodic DAG tasks on multiprocessors. <em>TPDS</em>, 1-15. (<a href='https://doi.org/10.1109/TPDS.2025.3611446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time systems are increasingly shifting from single processors to multiprocessors, where software must be parallelized to fully exploit the additional computational power. While the scheduling of real-time parallel tasks modeled as directed acyclic graphs (DAGs) has been extensively studied in the context of global scheduling, the scheduling and analysis of real-time DAG tasks under partitioned scheduling remain far less developed compared to the traditional scheduling of sequential tasks. Existing approaches primarily target plain fixed-priority partitioned scheduling and often rely on self-suspension–based analysis, which limits opportunities for further optimization. In particular, such methods fail to fully leverage fine-grained scheduling management that could improve schedulability. In this paper, we propose a novel approach for scheduling periodic DAG tasks, in which each DAG task is transformed into a set of real-time transactions by incorporating mechanisms for enforcing release offsets and intra-task priority assignments. We further develop corresponding analysis techniques and partitioning algorithms. Through comprehensive experiments, we evaluate the real-time performance of the proposed methods against state-of-the-art scheduling and analysis techniques. The results demonstrate that our approach consistently outperforms existing methods for scheduling periodic DAG tasks across a wide range of parameter settings.},
  archive      = {J_TPDS},
  author       = {Haochun Liang and Xu Jiang and Junyi Liu and Xiantong Luo and Songran Liu and Nan Guan and Wang Yi},
  doi          = {10.1109/TPDS.2025.3611446},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {New scheduling algorithm and analysis for partitioned periodic DAG tasks on multiprocessors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atomic smart contract interoperability with high efficiency via cross-chain integrated execution. <em>TPDS</em>, 1-17. (<a href='https://doi.org/10.1109/TPDS.2025.3614374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Ethereum, numerous blockchains compatible with Ethereum's execution environment (i.e., Ethereum Virtual Machine, EVM) have emerged. Developers can leverage smart contracts to run various complex decentralized applications on top of blockchains. However, the increasing number of EVM-compatible blockchains has introduced significant challenges in cross-chain interoperability, particularly in ensuring efficiency and atomicity for the whole cross-chain application. Existing solutions are either limited in guaranteeing overall atomicity for the cross-chain application, or inefficient due to the need for multiple rounds of cross-chain smart contract execution. To address this gap, we propose IntegrateX, an efficient cross-chain interoperability system that ensures the overall atomicity of cross-chain smart contract invocations. The core idea is to deploy the logic required for cross-chain execution onto a single blockchain, where it can be executed in an integrated manner. This allows cross-chain applications to perform all cross-chain logic efficiently within the same blockchain. IntegrateX consists of a cross-chain smart contract deployment protocol and a cross-chain smart contract integrated execution protocol. The former achieves efficient and secure cross-chain deployment by decoupling smart contract logic from state, and employing an off-chain cross-chain deployment mechanism combined with on-chain cross-chain verification. The latter ensures atomicity of cross-chain invocations through a 2PC-based mechanism, and enhances performance through transaction aggregation and fine-grained state lock. We implement a prototype of IntegrateX. Extensive experiments demonstrate that it reduces up to 61.2% latency compared to the state-of-the-art baseline while maintaining low gas consumption.},
  archive      = {J_TPDS},
  author       = {Chaoyue Yin and Mingzhe Li and Jin Zhang and You Lin and Qingsong Wei and Siow Mong Rick Goh},
  doi          = {10.1109/TPDS.2025.3614374},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Atomic smart contract interoperability with high efficiency via cross-chain integrated execution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAP-DCCS: A generic acceleration paradigm for data-intensive applications with efficient data compression and caching strategy over CPU-GPU clusters. <em>TPDS</em>, 1-16. (<a href='https://doi.org/10.1109/TPDS.2025.3615283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seismic exploration is a geophysical method used for imaging subsurface structures, capable of providing high-resolution images of the underground. In seismic data processing, Kirchhoff Pre-Stack Depth Migration (KPSDM) serves as one of the key techniques, playing a critical role in significantly enhancing the lateral resolution of imaging and providing accurate characterization of subsurface media. However, with the continuous growth in high-density seismic data volumes, the computational efficiency of KPSDM is primarily constrained by substantial computational loads, end-to-end I/O bottlenecks, and data storage pressures. To address the performance optimization challenges of computation-intensive applications that require frequent large-scale data transfers between the host and accelerator devices, this paper proposes GAP-DCCS, a GPU-based Generic Acceleration Paradigm with efficient Data Compression and Caching Strategy, which includes the following core strategies: (1) For compute-intensive modules, a GPU-based three-dimensional parallel acceleration is implemented, combined with memory access optimization techniques and overlapping strategies for data transfer and computation, to improve GPU resource utilization; (2) To alleviate the storage pressure of large-scale datasets, the BitComp compression algorithm is introduced to efficiently compress task data while maintaining output stability, significantly reducing storage requirements and end-to-end data transfer volume; (3) To tackle the I/O bottleneck caused by frequent large-scale data transfers between the host and devices, an adaptive dynamic caching data management mechanism is designed to effectively increase data reuse rates and markedly reduce end-to-end transfer frequency. Experimental results demonstrate that the proposed optimization method significantly enhances the computational performance of KPSDM, achieving a speedup of 123.51× on a single NVIDIA Tesla A800 GPU compared to a 16-core CPU. This optimization paradigm has not only been effectively validated in KPSDM but also offers a referable high-performance computing solution for other large-scale data processing tasks.},
  archive      = {J_TPDS},
  author       = {Jiangwei Xiao and Yingzhe Bai and Hanfei Diao and Guofeng Liu and Yuzhu Wang},
  doi          = {10.1109/TPDS.2025.3615283},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GAP-DCCS: A generic acceleration paradigm for data-intensive applications with efficient data compression and caching strategy over CPU-GPU clusters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-intrusive multi-objective task scheduling method for JointCloud environment. <em>TPDS</em>, 1-17. (<a href='https://doi.org/10.1109/TPDS.2025.3596012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of advanced technologies, such as large models, has precipitated a surging demand for computational resources, thereby driving the evolution of cloud computing services from single-cloud architectures to multi-cloud paradigms. The accompanying question is how to enable resources from different cloud service providers to collaborate efficiently. Although existing works have explored multi-cloud scheduling methods, most of these works are centralized scheduling, where the decision-maker can schedule computing resources across all clouds. However, this is nearly impossible given the current situation in which computing resources in different clouds come from various cloud service providers. In response to this challenge, JointCloud, a multi-cloud cooperation architecture, has been proposed, which aims at enhancing the cooperation among multiple clouds to provide efficient multi-cluster services. Following the idea of JointCloud, proposes a multi-objective evolutionary algorithm (MOEA) based method for task scheduling in multi-cluster cloud computing environments, without intervening in the intra-cluster scheduling scheme. In the proposed method, we construct a mathematical model with the optimization objectives of minimizing overall waiting time and load imbalance between clusters based on the actual operation data in China Computing NET (C $^{2}$ NET). In addition, we also develop an MOEA specifically tailored to address this problem. The performance of the proposed MOEA and existing state-of-the-art MOEAs is examined on the proposed problems. Comparison results highlight the promising performance of the proposed MOEA, the specifically tailored algorithm in effectively addressing the multi-cluster task scheduling problem. In addition, we also compared the results of the MOEAs with the results of three classical scheduling methods, the results proved the effectiveness of the MOEA-based method on this problem.},
  archive      = {J_TPDS},
  author       = {Lianghao Li and Haibo Mi and Bo Ding and Huaimin Wang},
  doi          = {10.1109/TPDS.2025.3596012},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A non-intrusive multi-objective task scheduling method for JointCloud environment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online container caching for IoT data processing in serverless edge computing. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3595965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless edge computing is an efficient way to execute event-driven, short-duration, and bursty IoT data processing tasks on resource-limited edge servers, using on-demand resource allocation and dynamic auto-scaling. In this paradigm, function requests are handled in virtualized environments, e.g., containers. When a function request arrives online, if there is no container in memory to execute it, the serverless platform will initialize such a container with non-negligible latency, known as cold start. Otherwise, it results in a warm start with no latency in previous studies. However, based on our experiments, we find there is a remarkable third case called Late-Warm, i.e., when a request arrives during the container initializing, its latency is less than a cold start but not zero. In this paper, we study online container caching in serverless edge computing to minimize the total latency with Late-Warm and other practical issues considered. We propose OnCoLa, a novel $O(T_{c}K)$-competitive algorithm supporting request relaying on multiple edge servers. Here, $T_{c}$ and $K$ are the maximum container cold start latency and the memory size, respectively. Extensive simulations on two real-world traces demonstrate that OnCoLa consistently outperforms the state-of-the-art container caching algorithms and reduces the latency by $23.33\%$. Experiments on Raspberry Pi and Jetson Nano show that OnCoLa reduces latency by up to $21.38\%$ compared with the representative lightweight policy.},
  archive      = {J_TPDS},
  author       = {Guopeng Li and Haisheng Tan and Chi Zhang and Xuan Zhang and Zhenhua Han and Guoliang Chen},
  doi          = {10.1109/TPDS.2025.3595965},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online container caching for IoT data processing in serverless edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling with lightweight predictions in power-constrained HPC platforms. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3586723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of demand for computing resources and the struggle to provide the necessary energy, power-aware resource management is becoming a major issue for the High-performance computing (HPC) community. Including reliable energy management to a supercomputer's resource and job management system (RJMS) is not an easy task. The energy consumption of jobs is rarely known in advance and the workload of every machine is unique and different from the others. We argue that the first step towards properly managing power is to deeply understand the power consumption of the workload, which involves predicting the workload power consumption and exploiting it by using smart power-aware scheduling algorithms. Crucial questions are (i) how sophisticated a prediction method needs to be to provide accurate workload power predictions, and (ii) to what point an accurate workload's power prediction translates into efficient power management. In this work, we proposed a method to predict and exploit HPC workloads power consumption with the objective of reducing the supercomputers power consumption, while maintaining the management (scheduling) performance of the RJMS. Our method exploits workload submission logs with power monitoring data, and relies on a mix of lightweight power prediction methods and a classical EASY Backfillling inspired heuristic. Then, we model and solve the power capping scheduling as a greedy knapsack algorithm. This algorithm improves the Quality of Service and avoids starvation while keeping the solution lightweight. We base this study on logs of Marconi 100, a 980-node supercomputer. We show using simulation that a lightweight history-based prediction method can provide accurate enough power prediction to improve the energy management of a large scale supercomputer compared to energy-unaware scheduling algorithms. These improvements have no significant negative impact on performance.},
  archive      = {J_TPDS},
  author       = {Danilo Carastan-Santos and Georges Da Costa and Igor Fontana de Nardin and Millian Poquet and Krzysztof Rzadca and Patricia Stolf and Denis Trystram},
  doi          = {10.1109/TPDS.2025.3586723},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scheduling with lightweight predictions in power-constrained HPC platforms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEMSO: A secure and efficient multi-data source blockchain oracle. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3586450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, blockchain oracle, as the key link between blockchain and real-world data interaction, has greatly expanded the application scope of blockchain. In particular, the emergence of the Multi-Data Source (MDS) oracle has greatly improved the reliability of the oracle in the case of untrustworthy data sources. However, the current MDS oracle scheme requires nodes to obtain data redundantly from multiple data sources to guarantee data reliability, which greatly increases the resource overhead and response time of the system. Therefore, in this paper, we propose a Secure and Efficient Multi-data Source Oracle framework (SEMSO), where nodes only need to access one data source to ensure the reliability of final data. First, we design a new off-chain data aggregation protocol TBLS, to guarantee data source diversity and reliability at low cost. Second, according to the rational man assumption, the data source selection task of nodes is modeled and solved based on the Bayesian game under incomplete information to maximize the node's revenue while improving the success rate of TBLS aggregation and system response speed. Security analysis verifies the reliability of the proposed scheme, and experiments show that under the same environmental assumptions, SEMSO takes into account data diversity while reducing the response time by 23.5%.},
  archive      = {J_TPDS},
  author       = {Youquan Xian and Xueying Zeng and Chunpei Li and Peng Wang and Dongcheng Li and Peng Liu and Xianxian Li},
  doi          = {10.1109/TPDS.2025.3586450},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SEMSO: A secure and efficient multi-data source blockchain oracle},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streamline: Accelerating deployment and assessment of real-time big data systems. <em>TPDS</em>, 1-15. (<a href='https://doi.org/10.1109/TPDS.2025.3587641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time stream processing applications (e.g., IoT data analytics and fraud detection) are becoming integral to everyday life. A robust and efficient Big Data system, especially a streaming pipeline composed of producers, brokers, and consumers, is at the heart of the successful deployment of these applications. However, their deployment and assessment can be complex and costly due to the intricate interactions between pipeline components and the reliance on expensive hardware or cloud environments. Thus, we propose streamline, an agile, efficient, and dependable framework as an alternative to assess streaming applications without requiring a hardware testbed or cloud setup. To simplify the deployment, prototyping, and benchmarking of end-to-end stream processing applications involving distributed platforms (e.g., Apache Kafka, Spark, Flink), the framework provides a lightweight environment with a developer-friendly, high-level API for dynamically selecting and configuring pipeline components. Moreover, the modular architecture of streamline enables developers to integrate any required platform into their systems. The performance and robustness of a deployed pipeline can be assessed with varying network conditions and injected faults. Furthermore, it facilitates benchmarking event streaming platforms like Apache Kafka and RabbitMQ. Extensive evaluations of various streaming applications confirm the effectiveness and dependability of streamline.},
  archive      = {J_TPDS},
  author       = {Md. Monzurul Amin Ifath and Tommaso Melodia and Israat Haque},
  doi          = {10.1109/TPDS.2025.3587641},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Streamline: Accelerating deployment and assessment of real-time big data systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elastic relaxation of concurrent data structures. <em>TPDS</em>, 1-17. (<a href='https://doi.org/10.1109/TPDS.2025.3587888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequential semantics of many concurrent data structures, such as stacks and queues, inevitably lead to memory contention in parallel environments, thus limiting scalability. Semantic relaxation has the potential to address this issue, increasing the parallelism at the expense of weakened semantics. Although prior research has shown that improved performance can be attained by relaxing concurrent data structure semantics, there is no one-size-fits-all relaxation that adequately addresses the varying needs of dynamic executions. In this paper, we first introduce the concept of elastic relaxation and consequently present the Lateral structure, which is an algorithmic component capable of supporting the design of elastically relaxed concurrent data structures. Using the Lateral, we design novel elastically relaxed, lock-free queues, stacks, a counter, and a deque, capable of reconfiguring relaxation during run-time. We establish linearizability and define worst-case bounds for relaxation errors in our designs. Experimental evaluations show that our elastic designs match the performance of state-of-the-art statically relaxed structures when no elastic changes are utilized. We develop a lightweight, contention-aware controller for adjusting relaxation in real time, and demonstrate its benefits both in a dynamic producer-consumer micro-benchmark and in a parallel BFS traversal, where it improves throughput and work-efficiency compared to static designs.},
  archive      = {J_TPDS},
  author       = {Kåre von Geijer and Philippas Tsigas},
  doi          = {10.1109/TPDS.2025.3587888},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic relaxation of concurrent data structures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedEFsz: Fair cross-silo federated learning system with error-bounded lossy compression. <em>TPDS</em>, 1-15. (<a href='https://doi.org/10.1109/TPDS.2025.3593896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Silo federated learning systems have been identified as an efficient approach to scaling DNN training across geographically-distributed data silos to preserve the privacy of the training data. Communication efficiency and fairness are two major issues that need to be both satisfied when federated learning systems are deployed in practice. Simultaneously guaranteeing both of them, however, is exceptionally difficult because simply combining communication reduction and fairness optimization approaches often causes non-converged training or drastic accuracy degradation. To bridge this gap, we propose FedEFsz. On the one hand, it integrates the state-of-the-art error-bounded lossy compressor SZ3 into cross-silo federated learning systems to significantly reduce communication traffic during the training. On the other hand, it achieves a high fairness (i.e., rather consistent model accuracy and performance across different clients) through a carefully designed heuristic algorithm that can tune the error-bound of SZ3 for different clients during the training. Extensive experimental results based on a GPU cluster with 65 GPU cards show that FedEFsz improves the fairness across different benchmarks by up to $60.88\%$ and meanwhile reduces the communication traffic by up to $315\times$.},
  archive      = {J_TPDS},
  author       = {Zhaorui Zhang and Sheng Di and Benben Liu and Zhuoran Ji and Guanpeng Li and Xiaoyi Lu and Amelie Chi Zhou and Khalid Ayed Alharthi and Jiannong Cao},
  doi          = {10.1109/TPDS.2025.3593896},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedEFsz: Fair cross-silo federated learning system with error-bounded lossy compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A performance-balanced scheduling algorithm for diverse real-world TSN scenarios. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3580402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-Sensitive Networking (TSN) achieves low-delay and low-jitter traffic transmission through different traffic scheduling mechanisms. However, despite numerous algorithms developed based on these mechanisms, most fail to concurrently support multipath, hybrid, and multicast traffic, which are prevalent in real-world scenarios. Moreover, balancing performance metrics such as success rate, bandwidth utilization, and computation overhead remains challenging for these algorithms, significantly limiting their application in diverse TSN scenarios. To solve this problem, this paper proposes a universal ultra-low-delay and zero-jitter traffic scheduling model. Based on this model, this paper further designs a performance-balanced algorithm. The algorithm improves traffic scheduling success rate through joint routing and scheduling, increases network bandwidth utilization through hybrid traffic scheduling, and achieves low computation overhead through policy-based searching. Finally, extensive experiments demonstrate that the algorithm effectively balances performance metrics across diverse real-world scenarios. It achieves high scheduling success rate under real-world traffic loads ($\gt $20% improvement over non-joint routing), increased bandwidth utilization in the presence of hybrid traffic (18.3% enhancement over non-hybrid traffic scheduling), and low computation overhead ($\lt $ 2 minutes).},
  archive      = {J_TPDS},
  author       = {Qian Yang and Xuyan Jiang and Rulin Liu and Tao Li and Hui Yang and Wei Quan and Zhigang Sun},
  doi          = {10.1109/TPDS.2025.3580402},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A performance-balanced scheduling algorithm for diverse real-world TSN scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A privacy-preserving IoT data access control scheme for cloud-edge computing. <em>TPDS</em>, 1-17. (<a href='https://doi.org/10.1109/TPDS.2025.3580407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Internet of Things(IoT), the combination of cloud computing and edge computing becomes a new computing paradigm to provide users with low-latency data services. However, for the limited resource, high dynamic, and wide distributed characteristics of IoT devices, it becomes a great challenge to realize the universal application of edge servers and the cloud-edge computing allocation. Meanwhile, most of the schemes ignore the leakage of data access pattern privacy when accessing data. Therefore, in this paper, we propose a privacy-preserving access control scheme for IoT data. Based on the cloud-edge-end framework, we design a pervasive edge computing protocol, which allows well-resourced devices to become edge servers at suitable geographic locations and users to outsource and access IoT data through the nearest edge server. It increases the flexibility of the cloud-edge collaborative system as well as the efficiency of data processing. Users do not need to interact beyond the network edge to enjoy the data services. Furthermore, a novel attribute-based encryption scheme is designed based on a modified Lightweight Secret Sharing Scheme to optimize computing task allocation and reduce the computation burden on end devices, without attribute information leakage. In addition, we also design a Transform algorithm and a Cloud-edge Interaction protocol to hide access pattern privacy efficiently. We analyze the feasibility of the scheme and demonstrate that the scheme is semantically secure and conceals access pattern privacy. Simulation experiments based on real IoT data show that the scheme is efficient and suitable for IoT scenarios.},
  archive      = {J_TPDS},
  author       = {Jingjing Wang and Na Wang and Wen Zhou and Jianwei Liu and Junsong Fu and Lunzhi Deng},
  doi          = {10.1109/TPDS.2025.3580407},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A privacy-preserving IoT data access control scheme for cloud-edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-optimization of partial offloading and resource allocation for multi-user tasks in vehicular edge networks. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3571470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) effectively alleviates the pressure on limited in-vehicle computing resources and energy supply caused by computation-intensive vehicular applications. However, the uneven spatial distribution of users leads to load imbalance among adjacent MEC servers, significantly increase the latency and energy consumption costs for vehicles. Therefore, achieving optimal configuration of available computing resources in MEC servers to accomplish the goal of low-latency and low-energy task offloading has become a critical issue to address. To tackle this problem, this study proposes a Multi-RSU Load Balancing (MRLB) strategy based on multi-hop network technology. This strategy dynamically allocates computing tasks to neighboring RSU server clusters with available computing resources through task segmentation and computation offloading mechanisms. Meanwhile, adaptive resource allocation strategies are implemented based on task quantity and task scale characteristics. Specifically, this study designs a multi-RSU collaborative offloading algorithm based on Deep Deterministic Policy Gradient (DDPG) to solve the optimal offloading decision. Additionally, by integrating the Lagrange multiplier method and Sequential Quadratic Programming (SQP) algorithm, the joint optimization of imbalanced task segmentation decisions and optimal CPU frequency allocation decisions for RSU servers is achieved. Experimental results demonstrate that the proposed method can achieve efficient multi-RSU resource allocation and ensure coordinated optimization of both system latency and energy consumption costs across diverse device conditions and varying network scenarios, particularly in load-imbalanced situations.},
  archive      = {J_TPDS},
  author       = {Dun Cao and Shirui Huang and Ning Gu and Fayez Alqahtani and R. Simon Sherratt and Jin Wang},
  doi          = {10.1109/TPDS.2025.3571470},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Co-optimization of partial offloading and resource allocation for multi-user tasks in vehicular edge networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ocelot: An interactive, efficient distributed compression-as-a-service platform with optimized data compression techniques. <em>TPDS</em>, 1-15. (<a href='https://doi.org/10.1109/TPDS.2025.3568221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large volumes of data generated by scientific simulations, genome sequencing, and other applications need to be moved among clusters for data collection/analysis. Data compression techniques have effectively reduced data storage and transfer costs. However, users' requirements on interactively controlling both data quality and compression ratios are non-trivial to fulfill. We propose a novel Compression-as-a-Service (CaaS) platform called Ocelot with four important contributions: (1) It offers real-time visualization, interactive compression, and transfer of scientific datasets. (2) It incorporates new strategies for compressing diverse types of datasets more effectively than traditional methods. (3) It provides an effective method for estimating the compression ratio and execution time of compression tasks. (4) Experiments on multiple real-world datasets on geographically distributed computers show that Ocelot can significantly improve data transfer efficiency with a performance gain of more than 10x in computing clusters with relatively slow networks.},
  archive      = {J_TPDS},
  author       = {Yuanjian Liu and Sheng Di and Jiajun Huang and Zhaorui Zhang and Kyle Chard and Ian Foster},
  doi          = {10.1109/TPDS.2025.3568221},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Ocelot: An interactive, efficient distributed compression-as-a-service platform with optimized data compression techniques},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LQ-GNN: A graph neural network model for response time prediction of microservice-based applications in the computing continuum. <em>TPDS</em>, 1-12. (<a href='https://doi.org/10.1109/TPDS.2025.3564214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges posed by the deployment of microservices of future end-user applications in the cloud continuum, a performance prediction model working together with a network elasticity controller will be needed. With that aim, this work introduces Layered Queuing-Graph Neural Networks (LQ-GNN), a novel Machine Learning (ML) approach to develop a generalized performance prediction model for microservicebased applications. Unlike previous works focused on individual applications, our proposal aims for a versatile model applicable to any microservice-based application, integrating the Layered Queueing Network (LQN) modeling with Graph Neural Networks (GNN). LQ-GNN allows to efficiently estimate the response time of applications under different resource allocations and placements on the computing continuum. The obtained evaluation results indicate that the proposed model achieves a prediction error below 10% when considering different evaluation scenarios. Compared to existing methodologies, our approach balances prediction accuracy and computational efficiency, making it viable for real-time deployments. Consequently, ML-based performance prediction can significantly enhance the resource management and elasticity control of microservice-based architectures, leading to more resilient and efficient systems.},
  archive      = {J_TPDS},
  author       = {Matías Richart and Juan-Luis Gorricho and Javier Baliosian and Luis M. Contreras and Alejandro Muñiz and Joan Serrat},
  doi          = {10.1109/TPDS.2025.3564214},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LQ-GNN: A graph neural network model for response time prediction of microservice-based applications in the computing continuum},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical reinforcement learning with partner modeling for distributed multi-agent cooperation. <em>TPDS</em>, 1-13. (<a href='https://doi.org/10.1109/TPDS.2024.3457153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world applications can be formulated as multi-agent cooperation problems, such as network packet routing and coordination of autonomous vehicles. The emergence of deep reinforcement learning (DRL) provides a promising approach for multi-agent cooperation through the interaction of the agents and environments. However, traditional DRL solutions suffer from the high dimensions of multiple agents with continuous action space during policy search. Besides, the dynamicity of agents' policies makes the training non-stationary. To tackle these issues, we propose HERO , a distributed hierarchical reinforcement learning approach that reduces training time and improves model performance by decomposing the overall policy into multiple layers of sub-policies in a hierarchical manner. On the one hand, the cooperation of multiple agents can be learned efficiently in high-layer discrete action space. On the other hand, the low-layer individual control can be reduced to single-agent reinforcement learning. Furthermore, we introduce a partner modeling network to effectively capture and model other agents' policies during learning. Finally, We develop a real-world multi-vehicle testbed and evaluate HERO in a series of cooperative lane change scenarios. Both the simulation and real-world experiments show that our approach can achieve a lower average collision rate of 0.09 and a faster convergence speed than other baselines.},
  archive      = {J_TPDS},
  author       = {Zhixuan Liang and Jiannong Cao and Shan Jiang and Huafeng Xu},
  doi          = {10.1109/TPDS.2024.3457153},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hierarchical reinforcement learning with partner modeling for distributed multi-agent cooperation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A state-of-the-art review with code about connected components labeling on GPUs. <em>TPDS</em>, 1-20. (<a href='https://doi.org/10.1109/TPDS.2024.3434357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is about Connected Components Labeling (CCL) algorithms developed for GPU accelerators. The task itself is employed in many modern image-processing pipelines and represents a fundamental step in different scenarios, whenever object recognition is required. For this reason, a strong effort in the development of many different proposals devoted to improving algorithm performance using different kinds of hardware accelerators has been made. This paper focuses on GPU-based algorithmic solutions published in the last two decades, highlighting their distinctive traits and the improvements they leverage. The state-of-the-art review proposed is equipped with the source code, which allows to straightforwardly reproduce all the algorithms in different experimental settings. A comprehensive evaluation on multiple environments is also provided, including different operating systems, compilers, and GPUs. Our assessments are performed by means of several tests, including real-case images and synthetically generated ones, highlighting the strengths and weaknesses of each proposal. Overall, the experimental results revealed that block-based oriented algorithms outperform all the other algorithmic solutions on both 2D images and 3D volumes, regardless of the selected environment.},
  archive      = {J_TPDS},
  author       = {Federico Bolelli and Stefano Allegretti and Luca Lumetti and Costantino Grana},
  doi          = {10.1109/TPDS.2024.3434357},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A state-of-the-art review with code about connected components labeling on GPUs},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy hardware and workload aware job scheduling towards interconnected HPC environments. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2021.3090334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New HPC machines are getting close to the exascale. Power consumption for those machines has been increasing, and researchers are studying ways to reduce it. A second trend is HPC machines' growing complexity, with increasing heterogeneous hardware components and different clusters architectures cooperating in the same machine. We refer to these environments with the term heterogeneous multi-cluster environments. With the aim of optimizing performance and energy consumption in these environments, this paper proposes an Energy-Aware-Multi-Cluster (EAMC) job scheduling policy. EAMC-policy is able to optimize the scheduling and placement of jobs by predicting performance and energy consumption of arriving jobs for different hardware architectures and processor frequencies, reducing workload's energy consumption, makespan, and response time. The policy assigns a different priority to each job-resource combination so that the most efficient ones are favored, while less efficient ones are still considered on a variable degree, reducing response time and increasing cluster utilization. We implemented EAMC-policy in Slurm, and we evaluated a scenario in which two CPU clusters collaborate in the same machine. Simulations of workloads running applications modeled from real-world show a reduction of response time and makespan by up to 25% and 6% while saving up to 20% of total energy consumed when compared to policies minimizing runtime, and by 49%, 26%, and 6% compared to policies minimizing energy.},
  archive      = {J_TPDS},
  author       = {Marco D'Amico and Julita Corbalan Gonzalez},
  doi          = {10.1109/TPDS.2021.3090334},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy hardware and workload aware job scheduling towards interconnected HPC environments},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2019). COOPER-SCHED: A cooperative scheduling framework for mobile edge computing with expected deadline guarantee. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2019.2921761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While mobile edge computing (MEC) holds promise to enhance users' mobile experiences, building a scheduling framework to make full use of MEC capabilities is challenging. When involving quality of service (QoS) in MEC, the problem becomes even harder. In this work, we focus on QoS guaranteed scheduling in MEC with a cloudlet, which is a small cloud center deployed at the wireless access point (AP) to serve nearby mobile devices. There are multiple mobile devices (MDs) and each one is associated with a job to be offloaded to the AP and executed in the cloudlet. Each job is associated with a block of input data, an execution workload, and a QoS requirement, i.e., a time deadline that the job is expected to be completed before it. Our goal is to find an efficient schedule, which involves radio access network (RAN) allocation and job mapping on multiple heterogeneous servers, such that the number of jobs whose deadlines are satisfied is maximized. The problem is proved to be NP-hard. To solve the problem, we propose an extended marriage algorithm (EMA) by adapting the stable marriage game, for job mapping in the cloudlet. Based on this algorithm, we further implement a cooperative game based scheduling method COOPER-SCHED, which also involves RAN allocation. We perform extensive random experiments and compare it with three common heuristics in scheduling literature. The results show that COOPER-SCHED can find better schedules and shows more stability, i.e., suffering less impacts from RAN allocation, than others in MEC.},
  archive      = {J_TPDS},
  author       = {Chubo Liu and Kenli Li and Jie Liang and Keqin Li},
  doi          = {10.1109/TPDS.2019.2921761},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {COOPER-SCHED: A cooperative scheduling framework for mobile edge computing with expected deadline guarantee},
  year         = {2019},
}
</textarea>
</details></li>
<li><details>
<summary>
(2017). Achieving high performance on supercomputers with a sequential task-based programming model. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2017.2766064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of accelerators as standard computing resources on supercomputers and the subsequent architectural complexity increase revived the need for high-level parallel programming paradigms. Sequential task-based programming model has been shown to efficiently meet this challenge on a single multicore node possibly enhanced with accelerators, which motivated its support in the OpenMP 4.0 standard. In this paper, we show that this paradigm can also be employed to achieve high performance on modern supercomputers composed of multiple such nodes, with extremely limited changes in the user code. To prove this claim, we have extended the StarPU runtime system with an advanced inter-node data management layer that supports this model by posting communications automatically. We illustrate our discussion with the task-based tile Cholesky algorithm that we implemented on top of this new runtime system layer. We show that it allows for very high productivity while achieving a performance competitive with both the pure Message Passing Interface (MPI)-based ScaLAPACK Cholesky reference implementation and the DPLASMA Cholesky code, which implements another (non sequential) task-based programming paradigm.},
  archive      = {J_TPDS},
  author       = {Emmanuel Agullo and Olivier Aumage and Mathieu Faverge and Nathalie Furmento and Florent Pruvost and Marc Sergent and Samuel Paul Thibault},
  doi          = {10.1109/TPDS.2017.2766064},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Achieving high performance on supercomputers with a sequential task-based programming model},
  year         = {2017},
}
</textarea>
</details></li>
<li><details>
<summary>
(2017). Improving the performance and endurance of persistent memory with loose-ordering consistency. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2017.2701364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent memory provides high-performance data persistence at main memory. Memory writes need to be performed in strict order to satisfy storage consistency requirements and enable correct recovery from system crashes. Unfortunately, adhering to such a strict order significantly degrades system performance and persistent memory endurance. This paper introduces a new mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering requirements at significantly lower performance and endurance loss. LOC consists of two key techniques. First, Eager Commit eliminates the need to perform a persistent commit record write within a transaction. We do so by ensuring that we can determine the status of all committed transactions during recovery by storing necessary metadata information statically with blocks of data written to memory. Second, Speculative Persistence relaxes the write ordering between transactions by allowing writes to be speculatively written to persistent memory. A speculative write is made visible to software only after its associated transaction commits. To enable this, our mechanism supports the tracking of committed transaction ID and multi-versioning in the CPU cache. Our evaluations show that LOC reduces the average performance overhead of memory persistence from 66.9% to 34.9% and the memory write traffic overhead from 17.1% to 3.4% on a variety of workloads.},
  archive      = {J_TPDS},
  author       = {Youyou Lu and Jiwu Shu and Long Sun and Onur Mutlu},
  doi          = {10.1109/TPDS.2017.2701364},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving the performance and endurance of persistent memory with loose-ordering consistency},
  year         = {2017},
}
</textarea>
</details></li>
<li><details>
<summary>
(2015). A relative coordinate based distributed sparse-preserving matrix factorization approach towards self-stabilizing network location service - Withdrawn. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2015.2419667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Withdrawn.},
  archive      = {J_TPDS},
  author       = {Yongquan Fu and Yijie Wang and Xiaoqiang Pei and Xiaoyong Li},
  doi          = {10.1109/TPDS.2015.2419667},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A relative coordinate based distributed sparse-preserving matrix factorization approach towards self-stabilizing network location service - Withdrawn},
  year         = {2015},
}
</textarea>
</details></li>
<li><details>
<summary>
(2013). Internet traffic privacy enhancement with masking: Optimization and trade-offs. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2013.72'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of recent experimental works have demonstrated that the supposedly secure channels in the Internet are prone to privacy breaking under many respects, due to packet traffic features leaking information on the user activity and traffic content. We aim at understanding if and how complex it is to obfuscate the information leaked by packet traffic features, namely packet lengths, directions, times: we call this technique traffic masking. We define a security model that points out what the ideal target of masking is, and then define the optimized traffic masking algorithm that removes any leaking (full masking). Further, we investigate the trade-off between traffic privacy protection and masking cost, namely required amount of overhead and realization complexity/feasibility. Numerical results are based on measured Internet traffic traces. Major findings are that: i) optimized full masking achieves similar overhead values with padding only and in case fragmentation is allowed; ii) if practical realizability is accounted for, optimized statistical masking attains only moderately better overhead than simple fixed pattern masking does, while still leaking correlation information that can be exploited by the adversary.},
  archive      = {J_TPDS},
  author       = {Alfonso Iacovazzi and Andrea Baiocchi},
  doi          = {10.1109/TPDS.2013.72},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Internet traffic privacy enhancement with masking: Optimization and trade-offs},
  year         = {2013},
}
</textarea>
</details></li>
<li><details>
<summary>
(2010). Secure time synchronization for wireless sensor networks based on bilinear pairing functions. <em>TPDS</em>, 1. (<a href='https://doi.org/10.1109/TPDS.2010.94'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time synchronization is crucial for wireless sensor networks (WSNs) and secure time synchronization is a key requirement for many sophisticated applications running on these networks. Most of the existing secure time synchronization protocols incur high communication and storage costs, and are subject to a few known security attacks. In this paper, we propose a novel secure time synchronization protocol for both homogeneous and heterogeneous WSN models; the protocol uses pairing-based cryptography to secure the time synchronization and to reduce the communication and storage requirements of each node. Security analysis of the protocol shows that it is highly robust against different attacks, namely selective forwarding attack, wormhole attack, masquerade attack, message manipulation attack, replay attack and delay attack.},
  archive      = {J_TPDS},
  author       = {Mizanur Rahman and Khalil El-Khatib},
  doi          = {10.1109/TPDS.2010.94},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Secure time synchronization for wireless sensor networks based on bilinear pairing functions},
  year         = {2010},
}
</textarea>
</details></li>
</ul>

</body>
</html>
