<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc">TAFFC - 112</h2>
<ul>
<li><details>
<summary>
(2025). A multi-scale feature refinement and dual-attention enhanced dynamic convolutional network for speech-based depression and ADHD assessment. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3604562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of affective computing, speech has been identified as a promising biomarker for assessing depression and attention deficit hyperactivity disorder (ADHD). These disorders manifest as abnormalities in speech across various frequency bands and exhibit temporal variations. Most existing work on speech features relies on the magnitude spectrogram, which discards phase information and also does not consider the impact of different frequency bands on depression and ADHD detection. Inspired by these, we propose a novel multi-scale complex feature refinement and dynamic convolution attention-aware network to enhance speech-based assessment of depression and ADHD. Our approach incorporates three key components: multi-scale complex feature refinement (MSFR), dynamic convolutional neural network (Dy-CNN), and dual-attention feature enhancement (DAFE) module. The MSFR module utilizes depth-wise convolutional networks to process both magnitude and phase input, selectively emphasizing frequency bands associated with depression and ADHD. Importantly, the Dy-CNN module employs an attention mechanism to autonomously generate multiple convolution kernels that adapt to input features and capture relevant temporal dynamics linked to depression and ADHD. Additionally, the DAFE module enhances feature representation and detection performance by incorporating channel shuffle attention (CSA) and spatial axial attention (SAA) mechanisms, which leverage both inter- and intra-channel relationships and examine time-frequency characteristics of the feature map. Extensive experiments conducted on four publicly available datasets, i.e., AVEC2013, AVEC2014, E-DAIC, and a self-collected authentic ADHD dataset demonstrated that the proposed method outperforms previous approaches and exhibits superior generalization capabilities across different language settings (i.e., English, German) for speech-based depression and ADHD assessment.},
  archive      = {J_TAFFC},
  author       = {Shuanglin Li and Siyang Song and Syed Mohsen Naqvi},
  doi          = {10.1109/TAFFC.2025.3604562},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A multi-scale feature refinement and dual-attention enhanced dynamic convolutional network for speech-based depression and ADHD assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypercomplex neural network and cross-modal attention for multi-modal emotion recognition using physiological signals. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3605133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal emotion recognition plays a crucial role in human-computer interaction. Nowadays, many studies have developed fusion algorithms for this purpose. However, two challenges are still present, i.e., insufficient cross-modal information sharing and weak fusion feature representations. To this end, we develop a novel framework, namely CH-Net, for multi-modal emotion recognition with physiological signals. It is based on cross-modal attention and hypercomplex domain fusion. First, our learnable cross-modal attention mechanism adaptively aligns features across modalities, enhancing both complementarity and modality-specific discrepancies. Second, a hypercomplex fusion module encodes these features, yielding more robust representations while reducing parameter overhead. Two benchmark datasets, i.e., MAHNOB-HCI and DEAP, are utilized to train and test our model. Extensive experiments demonstrate that CH-Net is effective and outperforms state-of-the-art (SOTA) methods. Our code will be available at https://github.com/xuxusky/CH-Net.},
  archive      = {J_TAFFC},
  author       = {Xu Xu and Junxin Chen and Chong Fu and Zhihan Lyu},
  doi          = {10.1109/TAFFC.2025.3605133},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hypercomplex neural network and cross-modal attention for multi-modal emotion recognition using physiological signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SarcasmBench: Towards evaluating large language models on sarcasm understanding. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3604806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of large language models (LLMs), tasks associated with “System I” cognition—those that are fast, automatic, and intuitive, such as sentiment analysis and text classification—are often considered effectively solved. However, sarcasm remains a persistent challenge. As a subtle and complex linguistic phenomenon, sarcasm frequently involves rhetorical devices such as hyperbole and figurative language to express implicit sentiments and intentions, demanding a higher level of abstraction and pragmatic reasoning than standard sentiment analysis. This raises concerns about whether current claims of LLM success extend robustly to the domain of sarcasm understanding. To systematically investigate this issue, we introduce a new high-quality multi-modal sarcasm detection dataset, termed AMSD, and construct a comprehensive evaluation benchmark, SarcasmBench. Our benchmark encompasses 16 state-of-the-art (SOTA) LLMs and 8 strong pretrained language models (PLMs), evaluated across six widely-used textual sarcasm datasets and three multi-modal sarcasm benchmarks. We adopt three popular prompting paradigms: zero-shot input/output (IO) prompting, few-shot IO prompting, and chain-of-thought (CoT) prompting. Our extensive experiments yield three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 and Gemini 2.0 consistently and significantly outperforms other LLMs across various prompting methods. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. We hope this benchmark will serve as a valuable resource for the research community and inspire future work toward more robust and human-aligned sarcasm understanding.},
  archive      = {J_TAFFC},
  author       = {Yazhou Zhang and Chunwang Zou and Zheng Lian and Prayag Tiwari and Jing Qin},
  doi          = {10.1109/TAFFC.2025.3604806},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SarcasmBench: Towards evaluating large language models on sarcasm understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PainFormer: A vision foundation model for automatic pain assessment. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3605475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities–including RGB, synthetic thermal, and estimated depth videos–and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 75 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment. The foundation model's architecture (code) and weights are available at https://github.com/GkikasStefanos/PainFormer.},
  archive      = {J_TAFFC},
  author       = {Stefanos Gkikas and Raul Fernandez Rojas and Manolis Tsiknakis},
  doi          = {10.1109/TAFFC.2025.3605475},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PainFormer: A vision foundation model for automatic pain assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LibEER: A comprehensive benchmark and algorithm library for EEG-based emotion recognition. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3605833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based emotion recognition (EER) has gained significant attention due to its potential for understanding and analyzing human emotions. While recent advancements in deep learning techniques have substantially improved EER, the field lacks a convincing benchmark and comprehensive open-source libraries. This absence complicates fair comparisons between models and creates reproducibility challenges for practitioners, which collectively hinder progress. To address these issues, we introduce LibEER, a comprehensive benchmark and algorithm library designed to facilitate fair comparisons in EER. LibEER carefully selects popular and powerful baselines, harmonizes key implementation details across methods, and provides a standardized codebase in PyTorch. By offering a consistent evaluation framework with standardized experimental settings, LibEER enables unbiased assessments of seventeen representative deep learning models for EER across the six most widely used datasets. Additionally, we conduct a thorough, reproducible comparison of model performance and efficiency, providing valuable insights to guide researchers in the selection and design of EER models. Moreover, we make observations and in-depth analysis on the experiment results and identify current challenges in this community. We hope that our work will not only lower entry barriers for newcomers to EEG-based emotion recognition but also contribute to the standardization of research in this domain, fostering steady development. The library and source code are publicly available at https://github.com/XJTU-EEG/LibEER.},
  archive      = {J_TAFFC},
  author       = {Huan Liu and Shusen Yang and Yuzhe Zhang and Mengze Wang and Fanyu Gong and Chengxi Xie and Guanjian Liu and Zejun Liu and Yong-Jin Liu and Bao-Liang Lu and Dalin Zhang},
  doi          = {10.1109/TAFFC.2025.3605833},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {LibEER: A comprehensive benchmark and algorithm library for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distinguishing depression and bipolar disorder from social media data utilizing intensity of emotions and interpretable deep learning models. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3606887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression and bipolar disorder (BD) are mental disorders that are often misdiagnosed as each other because their symptoms often overlap. Depression is characterized by prolonged negative emotion, which is persistent feelings of sadness. In contrast, BD is characterized by repeated extreme changes of emotions, consisting of manic (very happy) and depression (very sad) episodes. Because of overlapping symptoms, changes of emotions indicating depression or BD are difficult to recognize from individual posts. We propose two deep learning models to detect depression and BD from user posts, considering individual posts and multiple posts. A novel method to extract emotion features is designed by fine-tuning a transformer model to extract the intensity of emotions of the posts. Text features are extracted using word embedding models that have been fine tuned on mental health data, and we further consider topic features by using a topic modeling method. Multiple posts are considered by grouping them based on a time interval between the posts. The features of each group are concatenated, input into a convolution neural network (CNN), and go through a long short-term memory (LSTM). The attention mechanism is used to pay attention to important groups. The most important groups can be observed further to explain why the model classified depression and BD users. Our proposed model outperforms other models, achieving an F1-Score of 0.9589. Based on our experiment, the changes of emotions within four days are necessary for distinguishing depression and BD, which aligns with considerations in diagnosing symptoms of depression and BD.},
  archive      = {J_TAFFC},
  author       = {Syauki Aulia Thamrin and Arbee L.P. Chen},
  doi          = {10.1109/TAFFC.2025.3606887},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Distinguishing depression and bipolar disorder from social media data utilizing intensity of emotions and interpretable deep learning models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCMIB-PNS: Balancing sufficiency and necessity with probabilistic causality and cross-modal uncertainty in multimodal sentiment analysis. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3606964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims to accurately identify sentiment orientations by integrating information from multiple modalities such as text, audio, and video. However, a key challenge in multimodal fusion is effectively balancing the sufficiency and necessity of information across modalities. Traditional models often fail to qualify and capture this balance due to the presence of noise and redundant information in multimodal data, leading to suboptimal performance in sentiment analysis. To address this issue, we propose a novel multimodal sentiment analysis method called UCMIB-PNS, which is guided by information bottleneck and probabilistic causality. The method employs an Uncertain Cross-Modal Information Bottleneck (UCMIB) module to reduce redundant information within modalities and maximize discriminative information. The UCMIB utilizes codebooks to dynamically record the distributions of samples and employs random sampling to conduct uncertain modeling across different modalities. It integrates uncertainty-aware contrastive learning and KL divergence for dynamic comparison and compression of information from different modalities. Moreover, UCMIB-PNS uses differentiable Probability of Necessity and Sufficiency (PNS) estimators to estimate and re-weight the sufficiency and necessity of modalities by constructing several counterfactual scenarios through end-to-end learning. Experiments conducted on four publicly available multimodal sentiment analysis datasets demonstrate that UCMIB-PNS achieves optimal performance on both clean and noisy data. Extended experiments further validate the method's robustness under different types of noise.},
  archive      = {J_TAFFC},
  author       = {Jili Chen and Yihua Zhong and Qionghao Huang and Changqin Huang and Fan Jiang and Xiaodi Huang and Xun Wang},
  doi          = {10.1109/TAFFC.2025.3606964},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {UCMIB-PNS: Balancing sufficiency and necessity with probabilistic causality and cross-modal uncertainty in multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating symptom heterogeneity in multimodal depression estimation via level separation and deviation regression. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3606949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Depression Estimation (MDE) aims to infer individual depression scores by analyzing various signals, such as visual, auditory, and language signals etc. Compared to Multimodal Depression Detection (MDD) methods that only provide discrete labels, MDE can provide a more refined score evaluation. However, symptom heterogeneity leads to differences in external behaviors among patients with similar depressive states, which limits the performance of direct regression MDE methods. To address this issue, we propose a Combined Depression Level and Deviation (CDLD) method for MDE, which separates samples at different depression levels and further analyzes subtle deviations within same level to improve estimation performance. Specifically, the Multilevel Depression Separation module constructs depression levels with inherent commonalities based on psychological theories and models the ordinality of these levels, thereby separating samples with different levels of depression. Building on this, the Level-specific Deviation Regression module contrasts sample features relative to level-specific anchors, regressing the subtle depression deviation. Finally, the depression level and deviation are integrated to infer the depression score more accurately. Experiments on the DAIC-WOZ, CMDC, SEARCH, and AVEC 2014 datasets demonstrate that the proposed coarse-to-fine method effectively mitigates the impact of symptom heterogeneity on depression estimation performance, showing significant advantages in MDE tasks. The code is publicly available at https://github.com/LIU70KG/CDLD.},
  archive      = {J_TAFFC},
  author       = {Chengguang Liu and Shanmin Wang and Qingshan Liu and Yang Wang and Fei Wang},
  doi          = {10.1109/TAFFC.2025.3606949},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mitigating symptom heterogeneity in multimodal depression estimation via level separation and deviation regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-machine enhanced intelligence for semi-supervised facial emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3607025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning, particularly deep learning, typically achieves high facial emotion image recognition accuracy benefiting from multiple labeled data. However, the datasets usually contain insufficient labeled samples and numerous unlabeled data since human labeling is a costly endeavor. For semi-supervised learning of these datasets, self-training procedure solely based on the visual features of images fails to comprehensively understand the intricate high-level semantic features. Since EEG signals contain not only visual information related to the visual stimulus but also emotional information related to brain activity, they are highly suitable as supervisory signals for labeling unlabeled facial emotion images. In this study, we specifically employ EEG signals evoked by visual image stimuli in conjunction with EEGNet3D to learn a discriminative EEG class representation manifold of brain activity. The one-hot class label is replaced with the EEG class representation as the supervisory to train the base model. Then, better pseudo-labeling is achieved using the base model in the EEG class representation manifold. Based on pseudo-labeling results, the utilization of unlabeled data is further improved. Interestingly, our findings reveal that when utilizing EEG class representations as supervisory information for the base model, the base model demonstrates a learning pattern that involves focusing more on the eye area when making judgments about emotions. This behavior closely resembles how the human brain decodes emotions. Experiments show that the performance of the proposed method can be effectively enhanced by combining labeled and pseudo-labeled images. Further experiments demonstrate that our method exhibits strong generalization abilities when applied to new image datasets and other visual networks.},
  archive      = {J_TAFFC},
  author       = {Dongjun Liu and Weichen Dai and Hangjie Yi and Honggang Liu and Jianting Cao and Qibin Zhao and Fabio Babiloni and Wanzeng Kong},
  doi          = {10.1109/TAFFC.2025.3607025},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Brain-machine enhanced intelligence for semi-supervised facial emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIND-EEG: Multi-granularity integration network with discrete codebook for EEG-based emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3608571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains. EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge. Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features. To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity. Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization. The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition. Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available at https://github.com/XJTU-EEG/MIND_EEG.},
  archive      = {J_TAFFC},
  author       = {Yuzhe Zhang and Chengxi Xie and Huan Liu and Yuhan Shi and Guanjian Liu and Dalin Zhang},
  doi          = {10.1109/TAFFC.2025.3608571},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MIND-EEG: Multi-granularity integration network with discrete codebook for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal affect perception with large language model enhancement network. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3607727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) plays a vital role in understanding emotional content from social media and multimedia data. However, existing methods often rely on large-scale labeled datasets, leading to high annotation costs and poor adaptability. They also suffer from modality imbalance and suboptimal feature fusion. To address these issues, we propose MapleNet—a Multimodal Affect Perception framework enhanced by Large Language Models. MapleNet integrates a prototype-guided fusion strategy and a dynamic modality balancing mechanism to improve alignment and collaboration between text and image features. Specifically, a shared-space encoder combined with prompt optimization ensures semantic consistency across modalities. Within the prototype learning framework, the model dynamically adjusts modalityspecific learning by aligning features with class prototypes, thus mitigating imbalance and uncovering complementary affective cues. In addition, MapleNet employs a similaritybased sample retrieval module to construct contextual prompts, enriching sentiment understanding in few-shot settings. Experiments on six benchmark datasets show that MapleNet consistently outperforms state-of-the-art methods, especially under few-shot conditions, achieving superior accuracy and generalization. The relevant code is available at https://github.com/YFanLuo/MapleNet.},
  archive      = {J_TAFFC},
  author       = {Kaixiang Yang and Yifan Luo and Zongyan Zhang and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3607727},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal affect perception with large language model enhancement network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCGC-net: Multi-scale controllable graph convolutional network on music emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3608830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel dynamic graph learning approach for frequency graphs, underpinned by a suite of baseline methodologies and the Multi-scale Controllable Graph Convolutional Network (MCGC-net), a multi-channel dynamic graph learning framework. This approach addresses the limitations of temporal graphs in capturing global and local correlations in music signals for music emotion recognition tasks. Frequency graphs integrate timbre and chroma audio features to capture both local and global correlations in music, enhancing emotion classification and segmentation. The graph feature extractor module improves the dissemination of hierarchical feature information through control units and enhances the depiction of remote node attributes via shortcut connections. Additionally, the multi-channel feature extractor module expands the receptive field, enhancing inter-channel feature extraction. The integration of these modules enables the model to proficiently extract musical frequency features, uncovering latent correlations among various frequencies and their impact on emotion recognition. Empirical validation across the 1000songs, PMEmo, and Memo2496 datasets demonstrates the strategy's efficacy, with MCGC-net achieving classification accuracies of 79.03% for Arousal and 70.05% for Valence on 1000songs; 85.81% and 75.42% on PMEmo; and 79.52% and 78.61% on Memo2496, respectively. Complementary ablation studies and T-order robustness assessments further validate the unique contributions of model components and the performance enhancements provided by shortcut connections.},
  archive      = {J_TAFFC},
  author       = {Tong Zhang and Qilin Li and Xueyue Yang and C. L. Philip Chen},
  doi          = {10.1109/TAFFC.2025.3608830},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MCGC-net: Multi-scale controllable graph convolutional network on music emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing emotions and engagement during cognitive stimulation group training with the pepper robot. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3609855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive Stimulation Therapy (CST) is an evidence-based intervention that involves group or individual sessions demonstrating cognitive benefits in Mild Cognitive Impairment. Recent research suggests that social robots can effectively assist therapists during cognitive stimulation sessions for people with MCI. Building on these findings, we conducted an experimental study to evaluate the impact of a Socially Assistive Robot, on engagement and emotional responses during cognitive interventions for a group of elderly people. Each session was video-recorded for subsequent analysis. The findings suggest that the use of a Social Robot as a mediating tool in CST interventions is associated with high levels of participant engagement and predominantly positive-valenced emotional responses, as detected by both human and automatic evaluations.},
  archive      = {J_TAFFC},
  author       = {Berardina De Carolis and Nicola Macchiarulo and Giuseppe Palestra and Olimpia Pino},
  doi          = {10.1109/TAFFC.2025.3609855},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Analyzing emotions and engagement during cognitive stimulation group training with the pepper robot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal group constrained transformer with deformable landmark attention for video dimensional emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3610185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video dimensional emotion recognition aims to map human affect into the dimensional emotion space based on visual signals. Recent works notice that it is beneficial to locate key facial regions related to human emotion perception, as well as establish long-term temporal dependencies. While preliminary attempts have been made, there still exists much space for further improvements. In this paper, to better exploit key facial regions, we propose the Temporal cue guided Deformable Landmark Spatial (TDLS) transformer which attends to key facial regions in a data-dependent manner. We also propose the temporal cue guided frame representation learning to learn the spatial representation of each frame by considering features of other frames together. To better model temporal dependencies, we propose the Multi-layer Group Constrained Temporal (MGCT) transformer to summarize features of frames to multi-layer groups, perform group-to-group communications, and let group-level features guide the frame-level emotion recognition. We also introduce cross-clip representation learning to generate consistent results across different clips and videos. Extensive experiments are conducted on two benchmark datasets and superior results are achieved by our method compared to state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Weixin Li and Xiangjing Meng and Linmei Hu and Xuan Dong},
  doi          = {10.1109/TAFFC.2025.3610185},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Temporal group constrained transformer with deformable landmark attention for video dimensional emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STRFLNet: Spatio-temporal representation fusion learning network for EEG-based emotion recognition. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3611173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG)-based emotion recognition is essential for medical assistance and human-computer interaction. Although deep learning-based emotion recognition methods have demonstrated high performance, several challenges remain: 1) How to effectively utilize the complex dynamic-static spatial patterns inherent in emotion-related EEG signals. 2) How to hierarchically learn the latent correlations among multi-domain features. To address these challenges, a spatio-temporal representation fusion learning network (STRFLNet) is proposed to improve both the accuracy and robustness of emotion recognition using EEG signals. Specifically, dynamic-static graph topologies are constructed to capture comprehensive brain functional connectivity states, and a continuous dynamic-static graph ordinary differential equations is introduce to reveal continuous spatial patterns within EEG signals. Additionally, a hierarchical transformer fusion module is developed to fully leverage the latent correlations among multi-domain features to obtain the fused spatio-temporal representation. The experimental results on the SEED, SEED-IV, and DREAMER public EEG emotion datasets, under both subject-independent and subject-dependent settings, demonstrate that STRFLNet outperforms state-of-the-art methods in emotion recognition tasks. We further validate the effectiveness of the proposed model through interpretability analysis, which reveals the associations between the activated brain regions and corresponding emotional states. Our work highlights the significance of continuous spatial pattern learning and spatio-temporal feature fusion in emotion recognition, providing new insights for EEG-based emotion modeling.},
  archive      = {J_TAFFC},
  author       = {Fo Hu and Kailun He and Can Wang and Qinxu Zheng and Bin Zhou and Gang Li and Yu Sun},
  doi          = {10.1109/TAFFC.2025.3611173},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {STRFLNet: Spatio-temporal representation fusion learning network for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual feature disentanglement and fusion network for automatic depression severity prediction. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3611238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to achieve early screening and assist clinical decision-making, automatic depression assessment based on multimodal data are highly anticipated. However, the existed methods often suffer from semantic gap and information redundancy due to heterogeneity among modalities. To address this challenge, this paper investigates a novel Feature Disentanglement and Fusion Network (FDFNet) for predicting depression severity from audio-visual cues. Firstly, we design the shared and private encoders to disentangle modality-shared and modalityprivate representations. The former representation that acquires joint information is subjected by similarity constraints between modalities to ensure their distributions as close as possible. The latter that can capture unique features of each modality is restrained by independence constraints for keeping their distributions distinct. The decoder is then developed to reconstruct unimodal representation with constraints to minimize information loss. Finally, an efficient fusion strategy through addition and concatenation is ultilized for aggregating information. Experimental results on four benchmark datasets demonstrate that the proposed FDFNet consistently outperforms several stateof-the-art methods, with the competitive MAE/RMSE values of 6.22/7.58 on AVEC2013, 5.21/6.49 on AVEC2014, 4.25/5.34 on DAIC-WOZ, and 4.41/5.10 on E-DAIC, indicating that multimodal deep learning based on audio-visual is an attractive solution for objectively evaluating the depression severity.},
  archive      = {J_TAFFC},
  author       = {Shihao Li and Zhuhong Shao and Rongyin Qin and Yongzhen Huang and Peipeng Liang and Xiaobai Li and Yinan Jiang and Yanhe Deng and Tie Liu and Xiaohui Tan},
  doi          = {10.1109/TAFFC.2025.3611238},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Audio-visual feature disentanglement and fusion network for automatic depression severity prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building and using state-anxiety-oriented graph for student state anxiety assessment in online classroom scenarios. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3611369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State anxiety is a temporary reaction to external stressors. In online classroom scenarios, severe state anxiety over a long period significantly affects the physical and mental health of high school students. Researchers often use students' facial videos to assess their state anxiety. However, videobased assessment methods face challenges with data and clues, limiting their effectiveness in assessing student state anxiety. First, previous datasets mainly focused on the general population and there was a lack of a domain-specific dataset. Second, a single video provides limited clues, making it challenging for assessment methods to make accurate judgments when the student consistently displays a neutral expression. To address the data challenge, we collected the first state anxiety dataset containing 3,701 video clips from 106 high school students. To solve the clue challenge, we constructed a student-level stateanxiety-oriented graph and proposed a graph-based assessment method for student state anxiety at the video-level. This method incorporates course information, event information, students' academic and mental health statuses, and the earlier video information. Three well-designed attention modules are used to fuse these clues for better performance. Experimental results on the collected dataset demonstrate that our method is highly accurate in assessing students' state anxiety, with minimal errors (MSE = 0.1380, MAE = 0.2768).},
  archive      = {J_TAFFC},
  author       = {Lei Cao and Qi Li and Yaming Hang and Huijun Zhang and Zihan Wei and Fang Luo and Zhihong Qiao},
  doi          = {10.1109/TAFFC.2025.3611369},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Building and using state-anxiety-oriented graph for student state anxiety assessment in online classroom scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phy-FusionNet: A memory-augmented transformer for multimodal emotion recognition with periodicity and contextual attention. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3609046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate emotion recognition from physiological signals is critical for applications in healthcare, autonomous systems, and human-computer interaction. However, prevailing methods often fail to model long-term dependencies and overlook periodic patterns inherent in physiological data. To address these challenges, we propose Phy-FusionNet, a novel memory-augmented transformer architecture for multimodal emotion recognition. Phy-FusionNet introduces a Memory Stream Module with FIFO-queue and decay-based updates to preserve long-term contextual information. It further integrates Fourier-based positional encoding and frequency-aware attention, enabling robust detection of periodic emotional cues. An Adaptive Temporal Attention Module enhances computational efficiency and enables dynamic relevance in temporal feature extraction. For cross-modal fusion, we employ a transformer-based Multimodal Binding Learning framework that balances modality-specific and shared features. Extensive experiments on five public datasets—WESAD, CL-Drive, PPB-Emo, PhyMER, and EEG-VUI—demonstrate that Phy-FusionNet outperforms state-of-the-art models, achieving up to 16.3% improvement in accuracy and superior robustness across diverse emotional states and noisy environments. Notably, the model maintains low performance variance across emotion classes, with F1-Score differences under 2.5%, indicating stable recognition even for subtle or overlapping emotions. Our results underscore the importance of integrating memory, frequency, and adaptive attention for effective affective computing. The code will be publicly available on GitHub.},
  archive      = {J_TAFFC},
  author       = {Tianyi Wu and Erick Purwanto and Yongrun Huang and Su Yang},
  doi          = {10.1109/TAFFC.2025.3609046},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Phy-FusionNet: A memory-augmented transformer for multimodal emotion recognition with periodicity and contextual attention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spoken in jest, detected in earnest: A systematic review of sarcasm recognition - Multimodal fusion, challenges, and future prospects. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3612205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm, a common feature of human communication, poses challenges in interpersonal interactions and human-machine interactions. Linguistic research has highlighted the importance of prosodic cues, such as variations in pitch, speaking rate, and intonation, in conveying sarcastic intent. Although previous work has focused on text-based sarcasm detection, the role of speech data in recognizing sarcasm has been underexplored. Recent advancements in speech technology emphasize the growing importance of leveraging speech data for automatic sarcasm recognition, which can enhance social interactions for individuals with neurodegenerative conditions and improve machine understanding of complex human language use, leading to more nuanced interactions. This systematic review is the first to focus on speech-based sarcasm recognition, charting the evolution from unimodal to multimodal approaches. It covers datasets, feature extraction, and classification methods, and aims to bridge gaps across diverse research domains. The findings include limitations in datasets for sarcasm recognition in speech, the evolution of feature extraction techniques from traditional acoustic features to deep learning-based representations, and the progression of classification methods from unimodal approaches to multimodal fusion techniques. In so doing, we identify the need for greater emphasis on cross-cultural and multilingual sarcasm recognition, as well as the importance of addressing sarcasm as a multimodal phenomenon, rather than a text-based challenge.},
  archive      = {J_TAFFC},
  author       = {Xiyuan Gao and Shekhar Nayak and Matt Coler},
  doi          = {10.1109/TAFFC.2025.3612205},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Spoken in jest, detected in earnest: A systematic review of sarcasm recognition - Multimodal fusion, challenges, and future prospects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTADA: A multi-task adversarial domain adaptation network for EEG-based cross-subject emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3595137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In electroencephalogram (EEG)-based emotion recognition, the applicability of most current models is limited by inter-subject variability and emotion complexity. This study proposes a multi-task adversarial domain adaptation (MTADA) network to enhance cross-subject emotion recognition performance. The model first employs a domain matching strategy to select the source domain that best matches the target domain. Then, adversarial domain adaptation is used to learn the difference between source and target domains, and a fine-grained joint domain discriminator is constructed to align them by incorporating category information. At the same time, a multi-task learning mechanism is utilized to learn the intrinsic relationships between different emotions and predict multiple emotions simultaneously. We conducted comprehensive experiments on two public datasets, DEAP and FACED. On DEAP, the average accuracies for valence, arousal and dominance are 76.39%, 69.74% and 68.26%, respectively. On FACED, the average accuracies for valence and arousal are 78.90% and 77.95%. When using the subject from DEAP as the source domain to predict the subjects in FACED, the accuracies for valence and arousal are 61.07% and 60.82%. These results show that our MTADA model improves cross-subject emotion recognition and outperforms most state-of-the-art methods, which may provide new approach for EEG-based emotion brain-computer interface systems.},
  archive      = {J_TAFFC},
  author       = {Lina Qiu and Zuorui Ying and Xianyue Song and Weisen Feng and Chengju Zhou and Jiahui Pan},
  doi          = {10.1109/TAFFC.2025.3595137},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MTADA: A multi-task adversarial domain adaptation network for EEG-based cross-subject emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion alignment in human-robot interaction: Effects on communication styles and persuasion. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3596461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an experiment on the effects of inter-agents emotional alignment, a prerequisite for empathic communication, in Human-Robot Interaction (HRI). We describe a pipeline built around the Pepper robot with the idea of verifying the effect of emotionally-aligned communication toward a user. In particular, our goal is twofold, in that we investigate if and to what extent an emotionally-aligned, empathic dialogue impacts on (i) the communication style of the user, and (ii) the persuasive effectiveness of the robot, intended as its ability to alter or reinforce its interlocutors' attitudes and beliefs about the conversation topic. Both these aspects have been assessed in a controlled experiment with 46 participants, comparing a condition where the robot addresses participants with emotionally neutral sentences with a condition where the robot provides answers tailored to the emotions expressed in participants' input utterances. Results show how emotion alignment acts as an effective trigger for the elicitation of different communication styles of the users but also that, contrary to what we expected, it does not play any persuasive effect.},
  archive      = {J_TAFFC},
  author       = {Giorgia Buracchio and Ariele Callegari and Massimo Donini and Cristina Gena and Antonio Lieto and Alberto Lillo and Claudio Mattutino and Alessandro Mazzei and Linda Pigureddu and Manuel Striani and Fabiana Vernero},
  doi          = {10.1109/TAFFC.2025.3596461},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Emotion alignment in human-robot interaction: Effects on communication styles and persuasion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning classifier performance in an ensemble of classifiers for personality prediction using laughter. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3596695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper conducts a study on the acoustic features of laughter signals and performs experiments to identify the most relevant features. Different acoustic features are extracted from laughter signals, and studies are performed to identify the features that are more representative of the personality traits. An ensemble learning algorithm is proposed in this paper that gets the laughter signals from the speakers as input and predicts their personality traits. A Wagging algorithm is presented in this work that generates a set of diverse learning algorithms. A pruning algorithm is then proposed that finds the optimal subset of learning algorithms for the ensemble. In the proposed ensemble learning algorithm, a weighted averaging scheme is proposed to aggregate the output of the learning algorithms. In this scheme, during the training phase, a mechanism is adopted that measures the performance of each classifier in different areas in the feature space. These data are then used to generate a model of the performance of the basic classifiers at any given point in the feature space. In the classification phase, for any given test data record, this model is used to estimate the accuracy of the base learners. Then, a vote is performed among the classifiers, and this estimate is used to adjust the weight of the votes. The experiments are performed over a corpus of 1157 laughter bouts produced by 120 individuals to study whether laughter could be used to predict whether a person is above or below the median with respect to personality traits.},
  archive      = {J_TAFFC},
  author       = {M. -H. Tayarani-N. and Alessandro Vinciarelli},
  doi          = {10.1109/TAFFC.2025.3596695},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Learning classifier performance in an ensemble of classifiers for personality prediction using laughter},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective priming in emotional annotations and its effect on speech emotion recognition. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3597034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional annotation of data is important in affective computing for the analysis, recognition, and synthesis of emotions. As raters perceive emotion, they make relative comparisons with what they previously experienced, creating “anchors” that influence the annotations. This unconscious influence of the emotional content of previous stimuli in the perception of emotions is referred to as the affective priming effect. This phenomenon is also expected in annotations conducted with out-of-order segments, a common approach for annotating emotional databases. Can the affective priming effect introduce bias in the labels? If yes, how does this bias affect emotion recognition systems trained with these labels? This study presents a detailed analysis of the affective priming effect and its influence on speech emotion recognition (SER). The analysis shows that the affective priming effect affects emotional attributes and categorical emotion annotations. We observe that if annotators assign an extreme score to previous sentences for an emotional attribute (valence, arousal, or dominance), they will tend to annotate the next sentence closer to that extreme. We conduct SER experiments using the most biased sentences. We observe that models trained on the biased sentences perform the best and have the lowest prediction uncertainty.},
  archive      = {J_TAFFC},
  author       = {Luz Martinez-Lucas and Ali Salman and Seong-Gyun Leem and Woan-Shiuan Chien and Shreya G. Upadhyay and Chi-Chun Lee and Carlos Busso},
  doi          = {10.1109/TAFFC.2025.3597034},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affective priming in emotional annotations and its effect on speech emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion recognition using affective touch: A survey. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3592197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Touch Emotion Recognition (TER) is an emerging field in affective computing that focuses on interpreting human emotions through touch interactions. This survey provides a comprehensive review of the haptic technologies, methodologies, and applications of TER, emphasizing its role as a rich but underexplored modality for emotion recognition. A new taxonomy for TER is proposed, categorizing research based on recognition entities, recognition approaches, and touch gesture types, offering a structured perspective for advancing TER studies. The survey highlights the importance of affective touch gesture datasets, evaluates the hardware technologies underpinning TER systems, and reviews key methods for data processing and classification. Applications across domains such as emotional AI, social robotics, virtual reality, and healthcare are examined, showcasing the potential of TER in enhancing the emotional intelligence of artificial agents and robotic systems. Finally, challenges including dataset scarcity, device standardization, and cultural variability are discussed, alongside future directions for creating more robust and inclusive TER systems.},
  archive      = {J_TAFFC},
  author       = {Emma Yann Zhang and Zhigeng Pan and Adrian David Cheok},
  doi          = {10.1109/TAFFC.2025.3592197},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Emotion recognition using affective touch: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The body in affective robotics: A survey and conceptual positioning using the performing arts as a scaffold for understanding bodily expressed emotion. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3598454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective robotics centers on recognizing emotional states and generating artificial emotions through embodied robotic systems. This paper surveys the current state of the field, with a particular focus on bodily expressed emotion—both in recognizing affect through body movements and postures, and in generating movement that is parsed as affect by human observers. Framed through the lens of the performing arts, this examination provides insights into the expressive potential of robots, motivates key open questions, and highlights challenging problems, as presented through an art-inspired case study and foundational background material. A close engagement with the performing arts suggests intense malleability and diversity of bodily expression, challenging some of the field's prevailing goals—such as designing generally “happy” robotic movement—and emphasizing the importance of variables such as context and interactional intent. The paper concludes by proposing future directions for bodily expressed affective robotics that integrate advances from both robotics and the performing arts.},
  archive      = {J_TAFFC},
  author       = {Damith Herath and Amy LaViers and Sitao Zhang and Nipuni Hansika Wijesinghe and Sharni Konrad and Stelarc and Janie Busby Grant and James Z. Wang},
  doi          = {10.1109/TAFFC.2025.3598454},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The body in affective robotics: A survey and conceptual positioning using the performing arts as a scaffold for understanding bodily expressed emotion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text prompt region decomposition for effective facial expression recognition. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3599063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are conveyed through semantically distinct visual cues distributed across different facial regions, making region-aware feature modeling essential for accurate Facial Expression Recognition (FER). However, existing methods typically rely on implicit attention mechanisms or manually defined region cues without grounding in semantically aligned supervision, which often leads to suboptimal representation of regional expression cues, increased risk of overfitting, and limited interpretability. To address these limitations, we propose a Text Prompt Region Decomposition (TPRD) network that explicitly disentangles expression-relevant features across key facial regions via text prompt guidance. Specifically, TPRD comprises a visual-language pretrained encoder (e.g., CLIP), a Region Decomposition Module (RDM), and a Regional Integration Module (RIM). The visual encoder extracts global visual features from full-face images, while the text encoder embeds region-specific prompts (e.g., “mouth”, “eye”) into semantic vectors within a shared visual-language embedding space. The RDM employs a multi-branch architecture to project global visual features onto the semantic directions of region-specific text embeddings, enabling explicit extraction of local visual features. Subsequently, the RIM models the interaction between regional and global features, adaptively generating distinct regional contributions that modulate the global representation for different facial expression samples. Experimental results demonstrate that our proposed TPRD achieves leading performance in both withinand cross-dataset evaluations, as well as in scenarios involving occlusion and large head pose variations.},
  archive      = {J_TAFFC},
  author       = {Wei Nie and Hanlin Zhang and Xiangdong Zhang and Zhiyong Wang and Honghai Liu},
  doi          = {10.1109/TAFFC.2025.3599063},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Text prompt region decomposition for effective facial expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging eye movement for instructing robust video-based facial expression recognition. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3599859'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based facial expression recognition (VFER) is challenging due to variations caused by cultural background and expression camouflage. To tackle these problems, researchers introduced eye movement signals to complement visual information. However, existing methods either require expensive devices to capture high-quality eye movements or can only extract low-quality eye movements visually, making them ineffective in the real world. To address this, we propose an eye movement-instructed VFER (EM-VFER) that leverages high-quality eye movements to instruct the visual learning, obtaining robust performance without requiring costly devices during inference. Specifically, our EM-VFER operates in two stages: the high-quality eye movement pre-training stage and the eye movement-instructed video fine-tuning stage. In the pre-training, we compile an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset and use it to train a multimodal Transformer. During the fine-tuning, we propose a novel progressive eye movement-instructed learning to take better advantage of the prior knowledge about high-quality eye movement signals from EMER. The instructed fine-tuning model could then make more robust predictions on downstream facial expression datasets. We evaluate our approach on three macroexpression datasets (DFEW, MAFW and Aff-wild2) and two micro-expression datasets (CASME III and CASME II). The results demonstrate that EM-VFER significantly outperforms existing methods. The code will be available.},
  archive      = {J_TAFFC},
  author       = {Yuanyuan Liu and Lin Wei and Kejun Liu and Zijing Chen and Zhe Chen and Chang Tang and Jingying Chen and Shiguang Shan},
  doi          = {10.1109/TAFFC.2025.3599859},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Leveraging eye movement for instructing robust video-based facial expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empathetic response generation through multi-modality. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3599869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite remarkable advancements in empathetic response generation (ERG) area, existing research has centered on achieving affective and cognitive empathy by perceiving users' emotions and deducing contextual information from knowledge databases. Human communication combines textual, visual, and audio cues to interpret other's intentions. However, previous ERG works have focused on text-based methods and neglected contextual information within audiovisual data. To bridge the gap, we propose fostering empathy with users by integrating audiovisual and text modalities. First, the proposed method uses a cross-modal attention mechanism to perceive users' emotions from the multi-modal conversation. It integrates multi-modal data with the perceived emotions during response generation process, so that the generated responses resonate with users at the affective level by mirroring their emotions. Second, we import guidance text that focuses on visual context or user experiences and provides contextual information, thus enhancing cognitive empathy. The proposed method aligns multi-modal dialogue history and guidance text through the multi-source attention mechanism. Finally, the proposed method produces empathetic responses by understanding users' backgrounds and emotions. Experiments on three multi-modal datasets, e.g., MELD, IEMOCAP, and MEDIC, demonstrate that the proposed method outperforms state-of-the-art works.},
  archive      = {J_TAFFC},
  author       = {Jiaqiang Wu and Shangfei Wang and Yanan Chang and Zhouan Zhu},
  doi          = {10.1109/TAFFC.2025.3599869},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Empathetic response generation through multi-modality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label feature co-learning for facial and EEG emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3600267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing human emotions through behavioral and physiological signals is fundamental to overall health. However, since emotion occurs transiently, a semantics mismatch exists between the uniformly annotated label and multimodal temporal signals, leading to emotional ambiguity. Previous studies used the annotated label to guide feature learning, which makes it intractable to accurately identify emotional elicitation moments within each signal. Moreover, the inconsistency of specific elicitation moments across different signals complicates emotion recognition. The model hardly learns discriminative features due to emotional ambiguity, which weakens its ability to differentiate between emotions. To tackle the above challenges, this paper proposes a novel label feature co-learning model (LFCL) for emotion recognition through multimodal signals. Specifically, the LFCL leverages unimodal and multimodal information and adaptively generates instance-level emotion labels, thus precisely locating emotion elicitation moments within each signal. To promote emotion consistency across different signals, the LFCL incorporates a dynamic label calibration mechanism to balance the label generation process with historical information. Furthermore, to enhance the deep interaction between signals, the LFCL conducts multimodal interactive fusion to integrate multi-level multimodal features and extract global emotional information. The LFCL performs precise label-to-feature alignment to capture discriminative features of each signal, effectively alleviating emotional ambiguity and improving the ability to distinguish different emotions. Extensive experiments on three publicly available datasets demonstrate the effectiveness and generalization of the proposed model.},
  archive      = {J_TAFFC},
  author       = {Mingchen Cai and C. L. Philip Chen and Shuzhen Li and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3600267},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Label feature co-learning for facial and EEG emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PARSEL: A multimodal dataset for modeling decision-making processes involved in selecting partners for joint tasks. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3600687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How people evaluate, select, and engage with others in cooperative settings significantly impacts their well-being, happiness, and success. However, navigating these processes is complex. Equipping systems with the ability to recognize, interpret, and even engage during such socio-cognitive processes can increase their potential to support humans in these socio-cognitive processes and be more successful in adjusting to the social environment they are embedded in (e.g., understanding human preferences and attitudes), leading to better quality interactions and decision-making for future partners. Yet, the developments of such systems depend on available datasets. However, based on our knowledge, no dataset exists that can be used to model partner selection for joint tasks. To support research focused on creating such intelligent systems, we introduce the PARSEL dataset – a comprehensive corpus of dyadic interactions designed for computational modeling of PARtner SELection processes and collaborative behavior. In total, 297 participants took part in the datasets. The dataset contains measurements of partner selection decisions over three different stages, as well as factors that may influence partner selection in the context of (online) social interactions. It includes audiovisual recordings that offer fine-grained behavioral cues used during these interactions, self-reported traits, and reported perceptions of person-, situation- and team-specific phenomena. By providing this resource, we aim to foster advancements in computational methods that can effectively model and augment socio-cognitive processes, contributing to socially aware intelligent systems and enhanced human-system interactions.},
  archive      = {J_TAFFC},
  author       = {Tiffany Matej Hrkalovic and Bernd Dudzik and Daniel Balliet and Hayley Hung},
  doi          = {10.1109/TAFFC.2025.3600687},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PARSEL: A multimodal dataset for modeling decision-making processes involved in selecting partners for joint tasks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced emotion recognition in conversations through hybrid context encoding and latent dependency mining. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3601115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversations (ERC) is a pivotal component of affective computing, involving a common two-stage paradigm where pre-trained language models first extract context-independent features, followed by the encoding of contextual information and the modeling of emotional dependencies. This paradigm faces two challenges: (1) Existing methods struggle to capture both the intra-dialogue emotional continuity and the inter-dialogue semantic similarity. (2) The complexity of emotional elicitation processes gives rise to entangled dependencies, termed “latent dependencies”, which are difficult for current methods to detect and analyze. To overcome these challenges, we propose a Hybrid-Context Encoder with an Automated Latent Dependency Mining model for ERC. Specifically, we examine the emotional continuity and the semantic similarity from the standpoint of context encoders. We experimentally find that context encoders with different architectures exhibit distinct benefits. Based on these findings, we design a hybrid contextual encoding module that effectively combines the strengths of various encoders. Additionally, we design a lightweight generative module for latent dependency mining that autonomously generates a context mask, enabling the effective discovery of latent dependencies. We conduct extensive experiments on three datasets in the text modality. Our model achieves the best performance, which validates the superiority of our approach.},
  archive      = {J_TAFFC},
  author       = {Zheng Hu and Jiawen Deng and Satoshi Nakagawa and Yan Zhuang and Xiaoyue Zhang and Shimin Cai and Fuji Ren},
  doi          = {10.1109/TAFFC.2025.3601115},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhanced emotion recognition in conversations through hybrid context encoding and latent dependency mining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPRNet: A temporal-aware cross-modal encoding framework for personality recognition. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3601134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in personality recognition have improved trait inference from multimodal data, yet many existing methods rely on short-term video segments or static images, limiting the modeling of temporal dynamics due to short video durations, sparse frame-level annotations, and inconsistent modality coverage across audio, text, and visual channels. These limitations make it difficult to model how personality traits manifest over time and across modalities in naturalistic settings. To address these challenges, we introduce the Northeast University Personality Recognition (NEUPR) dataset, comprising 654 self-reported and discussion-based videos collected through MBTI assessments. NEUPR offers naturally expressed multimodal dataincluding audio, facial expressions, eye movements, and speech transcripts-captured across diverse participants and real-world settings. Building on this dataset, we propose MPRNet, a unified framework for dynamic personality recognition featuring two core innovations: (1) a multimodal encoder that leverages LSTM to capture temporal dependencies across longer sequences and integrates latent personality embeddings extracted from BERT representations of text to enrich semantic context, fused through adaptive weighting and enhanced by Gram encoding to preserve local feature patterns; and (2) a feature enhancement module that incorporates learnable positional encoding and channel attention to address modality imbalance and improve sensitivity to spatially salient features across modalities. Experimental results demonstrate that MPRNet outperforms state-of-the-art methods across multiple datasets, while ablation studies confirm the effectiveness of its components. By explicitly modeling temporal variation and enhancing cross-modal fusion, MPRNet enables more robust personality inference. This work establishes both a benchmark dataset and an adaptive modeling framework for multimodal personality analysis, advancing dynamic trait recognition.},
  archive      = {J_TAFFC},
  author       = {Jian Li and Huawei Zhang and Junyi Chen and Junhui Gong and Yufan Wang and Shifeng Wang and Yuliang Zhao},
  doi          = {10.1109/TAFFC.2025.3601134},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MPRNet: A temporal-aware cross-modal encoding framework for personality recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG coupled scale-invariant dynamics for emotion recognition: A domain adaptation approach. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3601809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In electroencephalogram (EEG)-based emotion recognition, traditional static univariate models often struggle to capture the complex, scale-free dynamics inherent in multivariate neural signals, which hampers generalization across subjects. To address this, we introduce a novel stochastic framework based on operator multifractional Lévy stable motion (omLsm) and stochastic differential equations (SDE). This framework effectively captures the dynamic scale-free properties of EEG signals and assesses their local cross-scaling characteristics, revealing dynamic fractal connectivity that correlates with various emotional states. The rationale behind our approach lies in the shared scale-free properties and affective cognitive attributes observed across different subjects within the same emotion categories. Local cross-scaling characteristics expose commonalities in the spatio-temporal and spectral domains, facilitating more robust emotion recognition through a multivariate lens. Furthermore, our framework incorporates domain adaptation strategies that enhance model performance across diverse subject populations. Our results indicate significant differences in scale-free connectivity associated with emotional states, reflecting clear advantages over static univariate approaches. Notably, our detection method achieves maximum accuracy of 98.00% for dominance and 98.41% for arousal recognition, respectively, using the DREMER and DEAP datasets and cross-dataset experiments, demonstrates impressive generalization capabilities of the proposed model. This signifies our method's effectiveness for practical applications in emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Mahnoosh Tajmirriahi and Hossein Rabbani},
  doi          = {10.1109/TAFFC.2025.3601809},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EEG coupled scale-invariant dynamics for emotion recognition: A domain adaptation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The diagnosis method of major depressive disorder using wavelet coherence and state-pathology separation network. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3602185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD) is a serious psychiatric disorder characterized by persistent feelings of sadness, hopelessness, and lack of interest or pleasure in daily activities. Yet, reliable diagnostic tools for this brain disorder remain lacking. Functional near-infrared spectroscopy (fNIRS), an optical brain imaging technique, offers a promising approach for monitoring cerebral hemodynamic activity associated with MDD. In this study, we propose a novel algorithm based on wavelet coherence and a state-pathology separation network (WCSN) to automatically detect MDD using a dual-channel fNIRS system. The fNIRS signals were first preprocessed and transformed into two-dimensional feature maps using a wavelet coherence method. Following this, a wrapped exhaustive search was applied to select the optimal subset of feature maps, which was then utilized to reconstruct the dataset. Finally, samples were classified using the state-pathology separation network that employed a dual-encoder convolutional autoencoder (DCoAE) module to separate the feature maps into state features and pathology features, while a Transformer module distinguished MDD patients from healthy controls based solely on pathology features. The WCSN algorithm achieved exceptional performance with an accuracy of 0.923 ± 0.068 and a subject accuracy of 0.918 ± 0.076. Our result highlights the WCSN algorithm's ability to isolate pure pathology features, enhancing classification robustness and generalizability under dual-channel data conditions. Taken together, the proposed WCSN algorithm is well-suited for home-based MDD screening applications.},
  archive      = {J_TAFFC},
  author       = {Guangming Wang and Yi Tao and Ning Wu and Rihui Li and Won Hee Lee and Zehong Cao and Xiangguo Yan and Badong Chen and Gang Wang},
  doi          = {10.1109/TAFFC.2025.3602185},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The diagnosis method of major depressive disorder using wavelet coherence and state-pathology separation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGC-link: Dual-gate chebyshev linkage network on EEG emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3602823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG emotion recognition presents several challenges, including region correlation, local and long-range node connectivity, and multi-channel patterns, necessitating advanced methods capable of effectively capturing and utilising complex EEG signal information. This paper introduces a novel method, the dual-gate Chebyshev Linkage network (DGC-Link), which comprises three main components: the Chebyshev Linkage (CL) module for extracting regional correlation features, the dual-gate module for regulating the flow of different-order information, and the deep network for extracting multi-channel features and enhancing representation capabilities. Validated on three datasets (SEED, DREAMER, and MPED) with ablation experiments demonstrating each component's effectiveness, DGC-Link achieves superior recognition performance compared to state-of-the-art methods. Notably, it achieves 96.43% accuracy on differential entropy on the SEED dataset, and 98.58%, 97.62%, and 98.01% for valence, arousal, and dominance classifications on the DREAMER dataset, along with 78.48% and 44.93% for 3-class and 7-class classifications on the MPED dataset. These results highlight DGC-Link's potential for improved performance in EEG emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Qilin Li and Tong Zhang and C. L. Philip Chen and Xiaowei Zhang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3602823},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DGC-link: Dual-gate chebyshev linkage network on EEG emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MER-CLIP: AU-guided vision-language alignment for micro-expression recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3584918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a critical psychological stress response, micro-expressions (MEs) are fleeting and subtle facial movements revealing genuine emotions. Automatic ME recognition (MER) holds valuable applications in fields such as criminal investigation and psychological diagnosis. The Facial Action Coding System (FACS) encodes expressions by identifying activations of specific facial action units (AUs), serving as a key reference for ME analysis. However, current MER methods typically limit AU utilization to defining regions of interest (ROIs) or relying on specific prior knowledge, often resulting in limited performance and poor generalization. To address this, we integrate the CLIP model's powerful cross-modal semantic alignment capability into MER and propose a novel approach namely MER-CLIP. Specifically, we convert AU labels into detailed textual descriptions of facial muscle movements, guiding fine-grained spatiotemporal ME learning by aligning visual dynamics and textual AU-based representations. Additionally, we introduce an Emotion Inference Module to capture the nuanced relationships between ME patterns and emotions with higher-level semantic understanding. To mitigate overfitting caused by the scarcity of ME data, we put forward LocalStaticFaceMix, an effective data augmentation strategy blending facial images to enhance facial diversity while preserving critical ME features. Finally, comprehensive experiments on four benchmark ME datasets confirm the superiority of MER-CLIP. Notably, UF1 scores on CAS(ME)$^{3}$ reach 0.7832, 0.6544, and 0.4997 for 3-, 4-, and 7-class classification tasks, significantly outperforming previous methods.},
  archive      = {J_TAFFC},
  author       = {Shifeng Liu and Xinglong Mao and Sirui Zhao and Peiming Li and Tong Xu and Enhong Chen},
  doi          = {10.1109/TAFFC.2025.3584918},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MER-CLIP: AU-guided vision-language alignment for micro-expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of mindfulness meditation on sensory perception and emotional evaluation of mid-air touch on the forearm. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3584989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory perception and emotional evaluation of a stimulus are deeply interconnected with sensory awareness and attention. Mindfulness meditation is a well-established tool for enhancing attention and awareness, but can this heightened awareness influence how we perceive and emotionally evaluate tactile stimuli? In this work, we investigate how mindfulness meditation influences sensory perception and emotional evaluation of gentle stroking stimuli delivered to the forearm using ultrasound-based mid-air haptics, providing tactile sensations without direct skin contact. Two studies were conducted to assess the effects of mindfulness on (1) perceived intensity, confidence, valence, and arousal in response to simple linear strokes and (2) pattern recognition and emotional evaluation of complex mid-air haptic patterns. Our findings suggest that mindfulness enhances perceived intensity, confidence, and valence ratings, but does not influence arousal ratings. Pattern recognition was challenging for the participants on the forearm leading to poor accuracy but mindfulness still enhances the recognition. These findings highlight the potential of mindfulness-enhanced haptic interactions in digital wellbeing and virtual environments. Future research should explore long-term mindfulness training, multi-sensory integration, and adaptive haptic systems to further optimize affective haptic experiences.},
  archive      = {J_TAFFC},
  author       = {Madhan Kumar Vasudevan and Dario Pittera and Antonio Cataldo and Marianna Obrist},
  doi          = {10.1109/TAFFC.2025.3584989},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Effect of mindfulness meditation on sensory perception and emotional evaluation of mid-air touch on the forearm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLM-EOE: Automatic depression detection via sentimental annotation and multi-expert ensemble. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3585599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Depression Detection (ADD) advances rapidly, though challenges persist. First, medical studies show distinct emotional changes between individuals with depression and those without, yet few studies effectively leverage emotions for ADD. Many existing models use a single processing module across various emotions, but the data characteristics differ under different emotions. A single module may not be able to grasp these data characteristics simultaneously, leading to the loss of key depressive clues. Second, fixed scales and perspectives are commonly applied in feature modeling, though depressive states and physiological signals change dynamically. Fixed modeling approaches may truncate or misconnect depressive signals. This study proposes a Multimodal Large Model-driven Ensemble of Expert Networks (MLM-EOE) for ADD, consisting of three components: (1) Multimodal Large Model Sentiment Annotation (MLM-SA) to annotate emotions in raw video data; (2) Ensemble of Experts (EOE) to capture data features under various emotional states; and (3) Modeling Inter and Intra Multiscale Patches (MIIMP) to apply multiscale, multi-perspective modeling to expert-processed data. Extensive experiments on four datasets (AVEC2013, AVEC2014, AVEC2019, and CMDep) validate the effectiveness of MLM-EOE. The self-constructed CMDep dataset is available upon request via the provided link. The code is publicly available at https://github.com/ZulongLin/MLM-EOE.},
  archive      = {J_TAFFC},
  author       = {Zulong Lin and Yaowei Wang and Yujue Zhou and Fei Du and Yun Yang},
  doi          = {10.1109/TAFFC.2025.3585599},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MLM-EOE: Automatic depression detection via sentimental annotation and multi-expert ensemble},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study of emotion concept formation by integrating vision, physiology, and word information using multilayered multimodal latent dirichlet allocation. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3585882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How are emotions formed? Through extensive debate and the promulgation of diverse theories, the theory of constructed emotion has become prevalent in recent research on emotions. According to this theory, an emotion concept refers to a category comprising interoceptive and exteroceptive information associated with a specific emotion. An emotion concept stores past experiences as knowledge and can predict unobserved information from acquired information. Therefore, in this study, we attempted to model the formation of emotion concepts using a constructionist approach from the perspective of the constructed emotion theory. Particularly, we constructed a model using multilayered multimodal latent Dirichlet allocation, which is a probabilistic generative model. We then trained the model for each subject using vision, physiology, and word information obtained from multiple people who experienced different visual emotion-evoking stimuli. To evaluate the model, we verified whether the formed categories matched human subjectivity and determined whether unobserved information could be predicted via categories. The verification results exceeded the level of chance, suggesting that emotion concept formation can be explained using the proposed model.},
  archive      = {J_TAFFC},
  author       = {Kazuki Tsurumaki and Chie Hieida and Kazuki Miyazawa},
  doi          = {10.1109/TAFFC.2025.3585882},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Study of emotion concept formation by integrating vision, physiology, and word information using multilayered multimodal latent dirichlet allocation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEED-MYA: A novel myanmar multimodal dataset for enhancing emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3586444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Myanmar multimodal dataset called SEED-MYA, which is the first culturally and linguistically tailored multimodal emotion recognition dataset for the Burmese speakers. The SEED-MYA dataset consists of EEG and eye movement data collected using Myanmar video stimuli, addressing the underrepresentation of minority cultures in emotion recognition research. To investigate the fundamental characteristics of emotion recognition based on EEG and eye movement data from Myanmar participants, and to validate the quality and effectiveness of the SEED-MYA dataset, we implement the Multimodal Adaptive Emotion Transformer with Cross-Modal Attention (MAET-CMA) as a benchmarking tool. From our experiments on SEED-MYA, we have three main findings: (a) beta and gamma bands play a critical role in distinguishing positive, neutral, and negative emotional states; (b) combining EEG and eye movement data significantly enhances emotion recognition accuracy, with MAET-CMA achieving a maximum accuracy of 91.78%; and (c) EEG signals excel in recognizing negative and positive emotional states, while eye movement data are particularly effective at differentiating neutral emotion in our experimental setup. Our neural activity analysis reveals distinct patterns of activation in temporal, parietal, and prefrontal regions, providing insights into potential culture-related neural responses. We further compare our findings with established benchmarks from Chinese participants (SEED dataset) to explore cultural similarities and differences in emotion recognition. This analysis is structured into three components: band-level EEG comparisons, modality-specific performance analysis, and neural activity differences, providing a multi-level analysis of cultural effects. While some similarities in emotion recognition are observed across two cultures, our cross-cultural performance comparison between SEED and SEED-MYA further indicates that the Chinese dataset generalizes better as a test set. These findings underscore the importance of incorporating culturally diverse datasets in the development of globally applicable emotion recognition systems.},
  archive      = {J_TAFFC},
  author       = {Khin Pa Pa Aung and Hao-Long Yin and Tian-Fang Ma and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TAFFC.2025.3586444},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SEED-MYA: A novel myanmar multimodal dataset for enhancing emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MiMamba: EEG-based emotion recognition with multi-scale inverted mamba models. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3587443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based emotion recognition holds significant potential in the field of brain-computer interfaces. A key challenge is extracting discriminative spatiotemporal features from electroencephalogram (EEG) signals. Existing studies often rely on domain-specific time-frequency features and analyze temporal dependencies and spatial characteristics separately, neglecting the local-global relationships and the interaction in spatiotemporal dynamics. To address this, we propose a novel network called Multi-scale Inverted Mamba (miMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to capture both local details and global temporal dependencies across different scale subsequences. The TSFBs, implemented with an inverted Mamba structure, focus on the interaction between dynamic temporal dependencies and spatial characteristics. The primary advantage of miMamba lies in its ability to leverage transformed multi-scale EEG sequences, exploiting the interaction between temporal and spatial features without the need for domain-specific time-frequency feature extraction. Experiments show that using only four EEG channels, miMamba achieves remarkable average recognition accuracies for Valence and Arousal classification: 94.86% on the DEAP dataset, 94.94% on the DREAMER dataset, and 91.36% on the SEED dataset. These results underscore the model's superior performance in multidimensional emotion recognition tasks and its potential for practical applications in resource-constrained affective computing scenarios.},
  archive      = {J_TAFFC},
  author       = {Xin Zhou and Dawei Huang and Xiaojiang Peng and Lijun Yin},
  doi          = {10.1109/TAFFC.2025.3587443},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MiMamba: EEG-based emotion recognition with multi-scale inverted mamba models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel conditional adversarial domain adaptation network for EEG cross-subject emotion recognition. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3588873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-subject emotion recognition based on electroencephalography (EEG) is currently a major development direction for affective brain-computer interfaces (aBCI). Currently, researchers are focusing on using domain adversarial neural networks (DANN) to capture domain-invariant features and enhance the cross-subject generalization of models. However, current DANN in the aBCI field cannot align features across different domains by directly estimating the differences between the source and target domains, and may struggle to effectively align feature distributions of different domains. Moreover, the current mainstream cross-subject evaluation protocols can result in inflated offline performance. To address the shortcomings of DANN, we develop a novel conditional adversarial domain adaptation network, which brings about a 10% performance improvement for the model. Specifically, by introducing a domain adapter, we gradually align the distributions of different domains during training to reduce domain differences. Additionally, we incorporate a conditioning strategy in the domain discriminator to effectively align distributions of different domains. We also develop a novel evaluation method that simulates an online scenario to address the issue of inflated offline performance. Extensive comparisons with existing methods demonstrate that the proposed approach achieves state-of-the-art cross-subject emotion recognition performance, attaining 93.62% accuracy on the SEED dataset and 82.16% on SEED-IV.},
  archive      = {J_TAFFC},
  author       = {He Huang and Xiaopeng Si and Yumeng Han and Dong Ming},
  doi          = {10.1109/TAFFC.2025.3588873},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A novel conditional adversarial domain adaptation network for EEG cross-subject emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empathy detection from text, audiovisual, audio or physiological signals: A systematic review of task formulations and machine learning methods. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3590107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science, and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 849 papers from 10 well-known academic databases, systematically screened them and analysed the final 82 papers. Our analyses reveal several prominent task formulations – including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion – in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities – text, audiovisual, audio and physiological signals – thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. This paper, therefore, provides a structured overview of recent advancements and remaining challenges towards developing a robust empathy detection system that could meaningfully contribute to enhancing human well-being.},
  archive      = {J_TAFFC},
  author       = {Md Rakibul Hasan and Md Zakir Hossain and Shreya Ghosh and Aneesh Krishna and Tom Gedeon},
  doi          = {10.1109/TAFFC.2025.3590107},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Empathy detection from text, audiovisual, audio or physiological signals: A systematic review of task formulations and machine learning methods},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAETFN: Context adaptively enhanced text-guided fusion network for multimodal sentiment analysis. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3590246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis(MSA) is an active research area in recent years with the exponential development of the internet and social media, which aims to recognize the speaker's sentiment in the video consisted of text, acoustic and visual cues, and has attracted attention from many applications such as smart education, intelligent medication and social security. The predominant approaches have devoted to developing more complicated fusion strategy to learn efficient multimodal representations. However, information from these modalities usually have different contributions to MSA task. More specifically, the text modality outperforms the non-verbal modalities since its highly condensed semantic information and the maturity of the pre-trained language models. Taking full advantage of the text modality while integrating the non-verbal sentiment-relevant contextual information becomes a substantial challenge. Thus, in this paper, we propose a Context Adaptively Enhanced Text-guided Fusion Network, which is embedded in the pre-trained language model and utilizes the text modality as the guide to reduce the redundancy and exploit the sentiment-relevant information and in turn uses these information to complement itself with the non-verbal sentiment contexts. Moreover, a novelly designed non-verbal feature enhancement module is introduced to capture long-range dependencies in two directions, with the substantial removal of the redundancy and the noise. Extensive experiments on two benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the competitive performance over the state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Jiabao Li and Ruyi Liu and Qiguang Miao and Di Wang and Xiangzeng Liu},
  doi          = {10.1109/TAFFC.2025.3590246},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CAETFN: Context adaptively enhanced text-guided fusion network for multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subtyping autism spectrum disorder using multimodal multilayer hypergraphs. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3590247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneity has been recognized as a large obstacle to the treatment of autism spectrum disorder (ASD). Recent studies have identified several subgroups of ASD that exhibited heterogeneous alterations in brain. However, most of them primarily depicted the pairwise similarity between individuals, relying solely on a single imaging modality. This leads to an underestimation of the complexity in inter-individual relationships and the rich information provided by multimodal images. To capture the high-order relationships among individuals, we utilized multi-task method to construct multilayer hypergraph based on brain structure and function. We then developed a novel co-optimized community detection algorithm, which jointly optimizes the modular structure across hypergraph layer, with the aim of categorizing subtypes of ASD by fusing multimodal information. By applying the proposed method on the Autism Brain Imaging Data Exchange repository data (n = 287/303, ASD/typical development [TD]), we identified two ASD subtypes with distinct alteration patterns in both brain structure and function. Distinct clinical manifestations in social and communication were observed between the two subtypes. Furthermore, subtyping significantly enhanced the diagnostic accuracy of ASD by over 10%. In addition, our method exhibited superior clustering performance that outperformed traditional community detection algorithms on graphs. Taken together, our study demonstrated the effectiveness of subtyping ASD through a multimodal multilayer hypergraph, highlighting its potential in elucidating the heterogeneity of autism and improving clinical diagnosis.},
  archive      = {J_TAFFC},
  author       = {Weihao Zheng and Jialong Li and Songyu Yang and Xiang Fu and Yalin Wang and Zhijun Yao and Minqiang Yang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3590247},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Subtyping autism spectrum disorder using multimodal multilayer hypergraphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking players' expectations: The role of non-player characters' coherence and consistency. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3590486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video games, non-player characters (NPCs) have an essential role in shaping players' experiences. The design of their appearance and their behaviors can be manipulated in coherence and consistency to maintain players' expectations or on the contrary to induce surprise. The influence of NPCs' coherence and consistency on players' evaluation of them remains to be unveiled. To fill this gap, two experiments were conducted in the context of a military shooter game. Players' evaluation of NPCs' perceived intelligence and believability, were measured, as these two dimensions are fundamental for their adoption and engagement toward them. The first experiment investigated the impact of breaking players' initial expectations on the evaluation of NPCs. The second experiment focused on the influence of NPCs' coherence and consistency on both players' expectations and evaluation of NPCs, by means of a combination of questionnaires, behavioral, and physiological measures. Our results reveal that breaking players' expectations influence their evaluation of NPCs, with coherent and consistent design reinforcing expectations and incoherent design challenging them.},
  archive      = {J_TAFFC},
  author       = {Remi Poivet and Catherine Pelachaud and Malika Auvray},
  doi          = {10.1109/TAFFC.2025.3590486},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Breaking players' expectations: The role of non-player characters' coherence and consistency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach to characterize dynamics of ECG-derived skin nerve activity via time-varying spectral analysis. <em>TAFFC</em>, 1-11. (<a href='https://doi.org/10.1109/TAFFC.2025.3590517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of the sympathetic nervous system (SNS) is one of the major approaches for studying affective states. Skin nerve activity (SKNA) derived from high-frequency components of electrocardiogram (ECG) signals has been a promising surrogate for assessing the SNS. However, current SKNA analysis tools have shown high variability across study protocols and experiments. Hence, we propose a time-varying spectral approach based on SKNA to assess the SNS with higher sensitivity and reliability. We collected ECG signals at a sampling frequency of 10 kHz from sixteen participants who underwent various SNS stimulations. Our spectral analysis revealed that frequency bands between 150 – 1,000 Hz showed significant increases in power during SNS stimulations. Using this information, we developed a time-varying index of sympathetic function measurement based on SKNA, termed Time-Varying Skin Nerve Activity (TVSKNA). TVSKNA is calculated in three steps: time-frequency decomposition, reconstruction using selected frequency bands, and smoothing. TVSKNA indices exhibited generally higher Youden's J, balanced accuracy, and area under the receiver operating characteristic curve, indicating higher sensitivity. The coefficient of variance was lower with TVSKNA indices for most SNS tasks. TVSKNA can serve as a highly sensitive and reliable marker of quantitative assessment of sympathetic function, especially during emotion and stress.},
  archive      = {J_TAFFC},
  author       = {Youngsun Kong and Farnoush Baghestani and William D'Angelo and I-Ping Chen and Ki H. Chon},
  doi          = {10.1109/TAFFC.2025.3590517},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A new approach to characterize dynamics of ECG-derived skin nerve activity via time-varying spectral analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-robot interaction conversational user enjoyment scale (HRI CUES). <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3590359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding user enjoyment is crucial in human-robot interaction (HRI), as it can impact interaction quality and influence user acceptance and long-term engagement with robots, particularly in the context of conversations with social robots. However, current assessment methods rely solely on self-reported questionnaires, failing to capture interaction dynamics. This work introduces the Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES), a novel 5-point scale to assess user enjoyment from an external perspective (e.g. by an annotator) for conversations with a robot. The scale was developed through rigorous evaluations and discussions among three annotators with relevant expertise, using open-domain conversations with a companion robot that was powered by a large language model, and was applied to each conversation exchange (i.e. a robot-participant turn pair) alongside overall interaction. It was evaluated on 25 older adults' interactions with the companion robot, corresponding to 174 minutes of data, showing moderate to good alignment between annotators. Although the scale was developed and tested in the context of older adult interactions with a robot, its basis in general and non-task-specific indicators of enjoyment supports its broader applicability. The study further offers insights into understanding the nuances and challenges of assessing user enjoyment in robot interactions, and provides guidelines on applying the scale to other domains and populations. The dataset is available online.},
  archive      = {J_TAFFC},
  author       = {Bahar Irfan and Jura Miniota and Sofia Thunberg and Erik Lagerstedt and Sanna Kuoppamäki and Gabriel Skantze and André Pereira},
  doi          = {10.1109/TAFFC.2025.3590359},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Human-robot interaction conversational user enjoyment scale (HRI CUES)},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual emotion classification using reinforcement learning-enhanced particle swarm optimisation. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3591356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraction of fine-grained spatial-temporal characteristics for emotion classification is a challenging task owing to the subtlety and ambiguity of emotional expressions through video and audio channels. In this research, we propose an audio-visual ensemble model, comprising a two-stream 3D Convolutional Neural Network (CNN) architecture with RGB and optical flow as inputs for video emotion classification, as well as a variant of Wav2Vec2 for audio emotion recognition. The Wav2Vec2 variant integrates additional recurrent and attention layers with each transformer block to extract long- and short-term dependencies. A new Particle Swarm Optimisation (PSO) algorithm is proposed to fine-tune hyper-parameters of 3D CNNs and the enhanced Wav2Vec2, and formulate audio-visual ensemble models with the smallest sizes. It integrates a reinforcement learning (RL) algorithm, i.e. Asynchronous Advantage Actor-Critic (A3C), for search parameter and hybrid leader construction, and another RL algorithm, Proximal Policy Optimisation (PPO), for search action selection, as well as hypotrochoid and super formula-based search operations. Evaluated using audio-visual emotion datasets, our evolving ensemble model outperforms those devised by other search methods and existing state-of-the-art deep networks, significantly.},
  archive      = {J_TAFFC},
  author       = {Karolis Kondrotas and Li Zhang and Chee Peng Lim and Houshyar Asadi and Yonghong Yu},
  doi          = {10.1109/TAFFC.2025.3591356},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Audio-visual emotion classification using reinforcement learning-enhanced particle swarm optimisation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DECEPTIcON: Bridging gaps in in-the-wild deception research. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3591205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DECEPTIcON, a new large-scale dataset for automatic deception detection. It contains video clips from 100 public figures, mostly politicians, along with manually aligned text transcripts and extracted audio-visual features. Each video is labeled with one of six truth levels from the PolitiFact fact-checking platform, allowing both fine-grained and binary classification tasks. Unlike earlier datasets, DECEPTIcON is designed to study deception in the wild, meaning it includes real-life, unscripted speech from a wide range of people and topics. We test and compare several baseline models for text, audio, and visual input separately, using state of the art pretrained architectures such as MPNet (text), Wav2Vec2 (audio), and VideoMAE (vision). Each model is trained for deception classification using 5-fold subject-independent cross-validation. We report CCR, F1-score, and MAE to evaluate performance. Our results show that text performs best overall, while fusion of multiple inputs leads to small but meaningful improvements. We also analyze the effect of different truth-level grouping strategies and show how attention-based interpretability tools help explain which parts of the input influence model predictions. DECEPTIcON aims to support fair, generalizable, and reproducible research in multimodal deception detection, and the dataset will be made available for research purposes.},
  archive      = {J_TAFFC},
  author       = {Berat Biçer and Bahadlr Durmaz and Serhat Aras and Hamdi Dibeklioğlu},
  doi          = {10.1109/TAFFC.2025.3591205},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DECEPTIcON: Bridging gaps in in-the-wild deception research},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ATTSF-net: Attention-based similarity fusion network for audio-visual emotion recognition. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3591567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional factors play a pivotal role in fields such as autonomous driving and intelligent emotional robotics. The accurate extraction of emotional factors is instrumental in reducing error rates within these domains. With the continuous deepening of exploration in the emotional domain, rich multimodal data has progressively supplanted unimodal data. Nevertheless, current multimodal approaches still grapple with the following challenges: 1.Partial loss of information both within individual modalities and across different modalities. 2.Incorrect extraction of modality-invariant features. To facilitate multimodal interaction and address the aforementioned issues, this paper proposes an Attention-based Similarity Fusion Network (ATTSF-Net) for audio-visual emotion recognition. The network is based on multimodal data and comprises the proposed Cross-Multimodal Block (CMB), Similarity Adjustment Block (SAB), and Audio-Visual Auxiliary Modules (AVAM). CMB employs a cross-modal attention mechanism and model-level fusion to facilitate interactions between modalities. SAB is designed to learn modality-invariant features. AVAM utilizes additional audio-visual auxiliary networks to provide supplementary emotional information, enabling the full extraction of intra-modal information. A similarity loss function based on Kullback-Leibler (KL) divergence is designed to ensure the consistency of the learned audio-visual emotional information. The proposed model achieves an accuracy of 88.67% on the RAVDESS dataset (8.67% higher than human) and an unweighted accuracy (UA) of 81.93% and a weighted accuracy (WA) of 79.77% on the IEMOCAP dataset (4.01% and 5.86% higher than transfer learning, respectively). A series of visualization experiments are conducted to demonstrate the effectiveness of the proposed model.},
  archive      = {J_TAFFC},
  author       = {Jiaming Zhang and Zhijia Zhang and Zhaojie Ju},
  doi          = {10.1109/TAFFC.2025.3591567},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ATTSF-net: Attention-based similarity fusion network for audio-visual emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic-attention-based EEG state transition modeling for emotion recognition. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3593630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based emotion decoding can objectively quantify people's emotional state and has broad application prospects in human-computer interaction and early detection of emotional disorders. Recently emerging deep learning architectures have significantly improved the performance of EEG emotion decoding. However, existing methods still fall short of fully capturing the complex spatiotemporal dynamics of neural signals, which are crucial for representing emotion processing. This study proposes a Dynamic-Attention-based EEG State Transition (DAEST) modeling method to characterize EEG spatiotemporal dynamics. The model extracts spatiotemporal components of EEG that represent multiple parallel neural processes and estimates dynamic attention weights on these components to capture transitions in brain states. The model is optimized within a contrastive learning framework for cross-subject emotion recognition. The proposed method achieved state-of-the-art performance on three publicly available datasets: FACED, SEED, and SEED-V. It achieved $81.7\pm 4.3\%$ accuracy in the binary classification of positive and negative emotions and $67.9\pm 7.3\%$ in nine-class discrete emotion classification on the FACED dataset, $88.1\pm 3.6\%$ in the three-class classification of positive, negative, and neutral emotions on the SEED dataset, and $73.6\pm 12.7\%$ in five-class discrete emotion classification on the SEED-V dataset. The learned EEG spatiotemporal patterns and dynamic transition properties offer valuable insights into neural dynamics underlying emotion processing.},
  archive      = {J_TAFFC},
  author       = {Xinke Shen and Runmin Gan and Kaixuan Wang and Shuyi Yang and Qingzhu Zhang and Quanying Liu and Dan Zhang and Sen Song},
  doi          = {10.1109/TAFFC.2025.3593630},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Dynamic-attention-based EEG state transition modeling for emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PME-MER: Efficient position and motion encoder for micro-expression recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3593666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial micro-expressions (MEs) are caused by brief and subtle movements of facial muscles, revealing a person's genuine emotions and offering valuable insights into lie detection, criminal analysis, and numerous human-computer interaction systems. Although deep learning-based micro-expression recognition (MER) approaches have achieved significant success, these methods often incorporate identity information that adversely impacts facial expression recognition and ignore capturing the synergistic interactions among different motion regions. To overcome these limitations, this paper proposes PME-MER, a novel MER method capable of effectively encoding facial position information and capturing muscle motion patterns. Specifically, we introduce Adaptive Positional and Local Motion Encoding (APME), a single-stream framework using cross-attention to align motion features with facial structure, facilitating accurate position calibration and minimizing the impact of identity information. Furthermore, we develop Collaborative Motion Encoding (CME), which utilizes self-attention and the top-k mechanism to explore the synergistic relationships among various facial regions, capturing comprehensive facial muscle movement information while reducing the impact of irrelevant regions on model predictions. Experimental results on four benchmark datasets demonstrate that our method significantly outperforms state-of-the-art methods, and ablation studies further demonstrate the effectiveness of our method.},
  archive      = {J_TAFFC},
  author       = {Aina Wang and Zili Zhang and Jining Feng and Xiaochuan Wang},
  doi          = {10.1109/TAFFC.2025.3593666},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PME-MER: Efficient position and motion encoder for micro-expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Could micro-expressions be quantified? electromyography gives affirmative evidence. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3575127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are brief, subtle facial expressions that reveal concealed emotions, offering key behavioral cues for social interaction. Characterized by short duration, low intensity, and spontaneity, MEs have been mostly studied through subjective coding, lacking objective, quantitative indicators. This paper explores ME characteristics using facial electromyography (EMG), analyzing data from 147 macro-expressions (MaEs) and 233 MEs collected from 35 participants. First, regarding external characteristics, we demonstrate that MEs are short in duration and low in intensity. Precisely, we proposed an EMG-based indicator, the percentage of maximum voluntary contraction (MVC%), to measure ME intensity. Moreover, we provided precise interval estimations of ME intensity and duration, with MVC% ranging from 7% to 9.2% and the duration ranging from 307 ms to 327 ms. This research facilitates fine-grained ME quantification. Second, regarding the internal characteristics, we confirm that MEs are less controllable and consciously recognized compared to MaEs, as shown by participants' responses and self-reports. This study provides a theoretical basis for research on ME mechanisms and real-life applications. Third, building on our previous work, we present CASMEMG, the first public ME database including EMG signals, providing a robust foundation for studying micro-expression mechanisms and movement dynamics through physiological signals.},
  archive      = {J_TAFFC},
  author       = {Jingting Li and Shaoyuan Lu and Yan Wang and Zizhao Dong and Su-Jing Wang and Xiaolan Fu},
  doi          = {10.1109/TAFFC.2025.3575127},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Could micro-expressions be quantified? electromyography gives affirmative evidence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seismocardiography for emotion recognition: A study on EmoWear with insights from DEAP. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3575281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions have a profound impact on our daily lives, influencing our thoughts, behaviors, and interactions, but also our physiological reactions. Recent advances in wearable technology have facilitated studying emotions through cardio-respiratory signals. Accelerometers offer a non-invasive, convenient, and costeffective method for capturing heart- and pulmonary-induced vibrations on the chest wall, specifically Seismocardiography (SCG) and Accelerometry-Derived Respiration (ADR). Their affordability, wide availability, and ability to provide rich contextual data make accelerometers ideal for everyday use. While accelerometers have been used as part of broader modality fusions for Emotion Recognition (ER), their stand-alone potential via SCG and ADR remains unexplored. Bridging this gap could significantly help the embedding of ER into real-world applications, minimizing the hardware, and increasing contextual integration potentials. To address this gap, we introduce SCG and ADR as novel modalities for ER and evaluate their performance using the EmoWear dataset. First, we replicate the single-trial emotion classification pipeline from the DEAP dataset study, achieving similar results. Then we use our validated pipeline to train models that predict affective valence-arousal states using SCG and compare them against established cardiac signals, Electrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG is a viable modality for ER, achieving similar performance to ECG and BVP. By combining ADR with SCG, we achieved a working ER framework that only requires a single chest-worn accelerometer. These findings pave the way for integrating ER into real-world, enabling seamless affective computing in everyday life.},
  archive      = {J_TAFFC},
  author       = {Mohammad Hasan Rahmani and Rafael Berkvens and Maarten Weyn},
  doi          = {10.1109/TAFFC.2025.3575281},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Seismocardiography for emotion recognition: A study on EmoWear with insights from DEAP},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting signs of depression using social media texts through an ensemble of ensemble classifiers. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3571749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence-based machine learning models have been widely used to explore and address various mental health-related problems in recent years, including depression. In this study, we present an ensemble approach to complement the 90 unique input features that we proposed in a previous study on depression detection using social media texts. Our proposed Ensemble of Ensemble Classifiers (EECs) combines many ensemble models, including Bagging Predictors, Random Forests, Adaptive Boosting and Gradient Boosting, as inner ensembles. These inner ensembles are arranged in a parallel fashion, where each of them is trained using different subsets of data sampled from the training data via bootstrap sampling. After the models are trained, during the testing phase, the results of all inner ensembles are processed using two methods— majority vote or class priority threshold—to get the final result as an output. From the experiments, we find that EECs are accurate in detecting signs of depression in social media users by analysing their posts in social media platforms such as Twitter. Our approach outperforms other ensemble methods on the public datasets we used. Moreover, if set correctly, the parameters of EECs can further improve the performance of the proposed ensemble in detecting signs of depression.},
  archive      = {J_TAFFC},
  author       = {Raymond Chionga and Gregorius Satia Budhib and Erik Cambriac},
  doi          = {10.1109/TAFFC.2025.3571749},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Detecting signs of depression using social media texts through an ensemble of ensemble classifiers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Public opinion crisis management via social media mining. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3576134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprises suffer tremendous economic and reputational losses in public opinion crises. However, providing specific guidance for enterprises to respond effectively to public opinion crises is challenging. This study proposes a netizen-centered approach to identify crisis response opportunities for enterprises by mining social media data. First, we identify the topics discussed by netizens with the Biterm Topic Model, thereby evaluating their importance levels. Then, the negativeness levels of crisis topics are quantified by sentiment analysis. Lastly, crisis response opportunities are quantitatively identified and prioritized by applying an opportunity algorithm that simultaneously considers each crisis topic's importance and negativeness levels. Experimental results demonstrate the validity and superiority of our approach. Moreover, we demonstrate that a very negative and less important topic in the growth stage will likely become very negative and important at the maturity of the crisis. This finding supports decision-making in response to critical topics at the early stage of a crisis, preventing the deterioration of the public opinion crisis.},
  archive      = {J_TAFFC},
  author       = {Yu Ma and Rui Mao and Peng Wu and Erik Cambria},
  doi          = {10.1109/TAFFC.2025.3576134},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Public opinion crisis management via social media mining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view self-supervised domain adaptation for EEG-based emotion recognition. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3574868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on emotion recognition based on EEG signals has made significant progress. Most of the existing studies have focused on supervised learning methods, but real-life data cannot meet the requirement of high quality with labels. In addition, EEG signals have individual variability and instability, which requires transfer learning to enhance the model generalization. In this paper, we propose a multi-view self-supervised domain adaptation model that combines self-supervised learning techniques with domain-adaptive transfer learning algorithms, which can solve the last two problems mentioned above. Specifically, we add a multi-class domain discriminator to construct the adversarial relationship between the sub-networks so that distribution discrepancy of different subjects can be reduced effectively. We conduct both subject-dependent and subject-independent experiments on the SEED and SEED-IV datasets to thoroughly evaluate the performance of our model. The results show that our model achieves outstanding emotion recognition performance even with limited labeled data. In the subject-dependent experiments on both datasets, our model achieves accuracy rates of 85.91% and 87.19% respectively, surpassing the original self-supervised masked autoencoder model by about 3%. In subject-independent experiments, our model demonstrates strong data distribution adaptation capabilities, achieving an accuracy of 69.72% and 62.87%, respectively on the SEED and SEED-IV datasets using only 90 samples for subject-independent experiments. This effectively mitigates the accuracy degradation caused by differences in data distribution across subjects. Furthermore, our model is capable of extracting meaningful features from corrupted EEG data, highlighting its robustness and effectiveness.},
  archive      = {J_TAFFC},
  author       = {Lu Zhang and Hanwen Shi and Ziyi Li and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TAFFC.2025.3574868},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-view self-supervised domain adaptation for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A discourse structure- and interlocutor-guided network for dialogue act recognition and sentiment classification. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3580818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue act recognition and sentiment classification (DAR and DSC) are closely related tasks in dialogue systems, both benefiting from joint modeling of their interdependencies. Recent advancements have improved performance on these tasks by integrating them, yet many approaches oversimplify dialogues by treating them as monologues and assuming uniform influence across all utterances. This neglects the inherent structural and interactive nature of dialogues. To address these issues, we propose a novel Discourse Structure- and Interlocutor-Guided (DSIG) network that fuses dialogue act recognition and sentiment classification. Our network synergizes structural dialogue relationships with interlocutor identity information, enabling effective modeling of utterance flow and cross-task interactions. Specifically, we utilize a shared encoder that functions as a dialogue discourse parser to dynamically construct utterance connections, thereby integrating dialogue structural relationships. In addition, we embed interlocutor affiliations into the fusion process during encoding, semantic modeling, and decoding, enhancing dialogue understanding and dual-task reasoning. The core component is a collaborative updating graph interaction layer, which filters redundant connections and introduces interlocutor nodes for exclusive information exchange. Experimental results on two benchmark datasets demonstrate DSIG achieves state-of-the-art performance by effectively fusing dialogue structure and interlocutor information, improving F1 scores by 6.9% and 2.1% for DSC and DAR, respectively, on Mastodon, and by 13.7% and 4.7% on DailyDialog. Model variants trained independently show promise for extension to other dialogue-related tasks.},
  archive      = {J_TAFFC},
  author       = {Yunhe Xie and Rui Mao and Wei Li and Atika Qazi and Erik Cambria},
  doi          = {10.1109/TAFFC.2025.3580818},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A discourse structure- and interlocutor-guided network for dialogue act recognition and sentiment classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-selectable global information and discrepancy learning network for multimodal sentiment analysis. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3580779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis and depression detection are pivotal for advancing human-computer interaction, yet significant challenges remain. First, the limited extraction of global contextual information within individual modalities risks the loss of modal-specific features. Second, existing methods often prioritize unaligned textual interactions, neglecting critical inter-modal discrepancies. To address these issues, we propose the Scale-Selectable Global and Discrepancy Learning Network (SSGDL), an innovative framework that integrates two core modules: the Cross-Shaped Dynamic Scale Attention Module (CSDSA) and the Primary-Secondary modal Discrepancy Learning Module (PS-MDL). The CS-DSA dynamically selects scales and employs cross-shaped attention to capture comprehensive global context and intricate internal correlations, effectively producing a fused modal representation. Meanwhile, the PS-MDL designates the fused modal as primary and utilizes cross-attention mechanisms to learn discrepancy representations between it and other modalities (textual, acoustic, and visual). By leveraging intermodal discrepancies, SSGDL achieves a more nuanced and holistic understanding of emotional content. Extensive experiments on three benchmark multimodal sentiment analysis datasets (MOSI, MOSEI, SIMS) and a depression detection dataset (AVEC2019) demonstrate that SSGDL consistently outperforms state-of-theart approaches, setting a new benchmark for multimodal affective computing.},
  archive      = {J_TAFFC},
  author       = {Xiaojiang He and Yushan Pan and Xinfei Guo and Zhijie Xu and Chenguang Yang},
  doi          = {10.1109/TAFFC.2025.3580779},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Scale-selectable global information and discrepancy learning network for multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAHAG: Two-stage domain adaptation with hybrid adaptive graph learning for EEG emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3580787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based emotion recognition is crucial for understanding human affective states, offering valuable insights into diverse fields like mental health monitoring and humancomputer interaction. Recent advancements in graph learning have significantly impacted EEG emotion recognition due to their ability to model the complex, dynamic relationships within brain networks. However, current methods often neglect the interplay between shared and individual correlations among EEG channels. Furthermore, individual variations in EEG patterns lead to distributional shifts that hinder the generalization of existing approaches. This paper proposes a novel Two-stage domain Adaptation with Hybrid Adaptive Graph learning (TAHAG) for EEG emotion recognition. TAHAG first employs hybrid adaptive graph learning to capture both shared and individual spatial characteristics of the EEG signals, dynamically integrating their contributions. Feature attention mechanisms are then incorporated to refine node features and enhance the model's discriminability. To address distributional variations, TAHAG utilizes a two-stage domain adaptation strategy. This strategy involves aligning the refined node features across different domains through discrepancy alignment. Subsequently, adversarial training captures domain-invariant summarized features of the entire graph. Extensive experiments on three public datasets demonstrate the superiority of TAHAG compared to existing methods. Furthermore, visualization of neuronal activity reveals significant brain regions and inter-channel relationships relevant to EEG emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Peiliang Gong and Yueying Zhou and Shuo Huang and Pengpai Wang and Daoqiang Zhang},
  doi          = {10.1109/TAFFC.2025.3580787},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {TAHAG: Two-stage domain adaptation with hybrid adaptive graph learning for EEG emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to enhance causal discrimination of emotional utterances: A case on LLMs. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3580755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods, including large language models (LLMs), excel at capturing semantic correlations between utterances, but often struggle to accurately distinguish specific causal relationships. This limitation poses a significant challenge for reasoning-intensive tasks in affective computing, where precise identification of emotional triggers and their effects is crucial. Our preliminary work demonstrated the potential of introducing i.i.d. noise terms within Structural Causal Models (SCMs) for the Emotion-Cause Pair Extraction (ECPE) task. However, this approach relied on end-to-end learning of high-dimensional latent representations, which hindered both scalability to LLMs and model interpretability. To address these issues, we conceptualize i.i.d. noise terms as token-level implicit causes—natural language expressions that reflect a speaker's underlying emotions, intentions, or situational context. Building on this insight, we introduce ICE (Implicit-Cause-Enhanced), an instruction-based framework that leverages implicit causes to enhance causal reasoning in LLMs. First, we design prompts that heuristically guide LLMs to generate implicit causes, which are then iteratively refined via an external evaluation mechanism. Second, by incorporating these implicit causes as intermediate reasoning steps, ICE improves the accuracy of emotion-cause pair prediction. Moreover, we distill the rationales produced by ICE into lightweight generative models, demonstrating that even small models can benefit from implicit-cause-driven reasoning. Extensive experiments in both instruction-based and distillation-based settings confirm the effectiveness, robustness, and interpretability of our approach.},
  archive      = {J_TAFFC},
  author       = {Xinyu Yang and Daiying Zhao and Hang Chen and Keqing Du},
  doi          = {10.1109/TAFFC.2025.3580755},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {How to enhance causal discrimination of emotional utterances: A case on LLMs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmotionMIL: An end-to-end multiple instance learning framework for emotion recognition from EEG signals. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3581388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from EEG signals offers significant advantages in affective computing, as EEG more accurately reflects internal emotional states than other modalities, such as facial expressions or peripheral physiological signals. Modeling and capturing subtle affective changes over time is crucial for real-world applications to achieve better human-computer interaction. However, training such models usually requires segment-level emotion labels, which are costly and may not be feasible. Assigning the overall label to all EEG segments within a trial can lead to inaccurate model training and degraded performance, as emotions evolve continuously. This highlights the need for models capable of learning from trial-wise emotion labels while capturing temporal dynamics of emotional responses within each segment because trial-wise post-stimulus labels are more accessible. To this end, we propose EmotionMIL, an end-to-end EEG-based emotion recognition framework that leverages recent advances in deep multiple instance learning (MIL). This framework enables robust emotion recognition from weakly labeled EEG signals and identifies the most prominent emotional responses. EmotionMIL captures the temporal dynamics of emotions using a retentive self-attention mechanism, which adaptively assigns weights to EEG segments based on their relevance in predicting the overall emotion label. A pseudo-bag augmentation strategy is also introduced to enhance the model's generalization ability by generating additional pseudo-bags from the original ones. Evaluated on three benchmark datasets—DEAP, DREAMER, and SEED—EmotionMIL outperforms state-of-the-art non-MIL and MIL models in both subject-dependent and subject-independent tasks, achieving superior accuracy and F1-score. Ablation study further validates the model design, while visualization results demonstrate that EmotionMIL effectively identifies both spatial EEG patterns and temporal emotional dynamics. These findings underscore EmotionMIL's potential for robust, interpretable emotion recognition, paving the way for real-world applications in emotion-aware systems. The code is available at https://github.com/yuty2009/emotionmil.},
  archive      = {J_TAFFC},
  author       = {Jun Xiao and Feifei Qi and Lingli Wang and Yanbin He and Jingang Yu and Wei Wu and Zhuliang Yu and Yuanqing Li and Zhenghui Gu and Tianyou Yu},
  doi          = {10.1109/TAFFC.2025.3581388},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EmotionMIL: An end-to-end multiple instance learning framework for emotion recognition from EEG signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aware yet biased: Investigating emotional reasoning and appraisal bias in large language models. <em>TAFFC</em>, 1-11. (<a href='https://doi.org/10.1109/TAFFC.2025.3581461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports two studies investigating the emotional reasoning of Large Language Models (LLM). Previous research has suggested that LLMs are surprisingly accurate at predicting human emotions from text descriptions of situations and reason in a way that is consistent with appraisal theory—a leading theory of emotion. Study 1 tests this claim with a large multilingual corpus (English, French, and German) of autobiographical descriptions of emotionally charged events. We confirm that GPT-4, one of the most advanced and widely studied LLMs, shows a remarkable ability to predict emotion and appraisals. We further show this ability is language-independent, with accuracy being consistent across languages and unaffected by the language of the prompt. However, GPT-4 struggles to accurately predict certain emotions (shame, fear, and irritation) and fails to understand appraisal dimensions related to control and power. We repeat the experiments with Gemini-2.0-Flash and find a remarkably similar pattern of strengths and weaknesses, although it consistently outperforms GPT-4. Study 2 examines a possible mechanism for these failures based on the idea of cognitive appraisal bias. In psychological appraisal theory, appraisal bias is the idea that people evaluate situations in biased, often unrealistic ways. By testing both models on a set of situations designed to identify appraisal bias, we find they exhibit strong—but similar—appraisal bias; for example, evaluating situations as if they were a person high in agreeableness and low in power. We further offer evidence suggesting that LLMs could be debiased by incorporating a person's personality in the prompt. This research underscores LLMs' capabilities and limitations in emotional reasoning, though highlights one mechanism underlying this limitation and suggests an approach for addressing these limits.},
  archive      = {J_TAFFC},
  author       = {Ala N. Tak and Jonathan Gratch and Klaus R. Scherer},
  doi          = {10.1109/TAFFC.2025.3581461},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Aware yet biased: Investigating emotional reasoning and appraisal bias in large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multisensory music listening in affective virtual environments. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3580703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research has shown that the vibrotactile enhancement of music signals can be effective in creating intense musical experiences. Virtual Reality is emerging as a new medium with which experiencing music and inducing emotions, but the study of the effects of tactile enrichments of music in virtual environments (VEs) has been largely overlooked thus far. It is unknown whether the same results found for real environments hold for VEs too, and what is the role played by vibrations in altering the emotion, perception and behavior of listeners in such spaces. To bridge these gaps, we created 12 affective musical VEs accompanying as many musical pieces and assessed the impact of adding synchronous vibrations. Results showed that the VEs involving the vibrations received significantly higher ratings of valence, arousal and presence compared to their audio-visual counterparts. The majority of participants preferred the condition with the vibrations. Cybersickness was not reduced in presence of vibrations, but it was found to be significantly higher for females than males. Although most participants reported that they were induced to move to a greater extent in presence of vibrations, the recorded motion of their head and hands did not increase when vibrations were provided.},
  archive      = {J_TAFFC},
  author       = {Luca Turchet and Carlo Marotta and Alberto Boem},
  doi          = {10.1109/TAFFC.2025.3580703},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multisensory music listening in affective virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small but fair! fairness for multimodal human-human and robot-human mental wellbeing coaching. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3582074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the affective computing (AC) and human-robot interaction (HRI) research communities have put fairness at the centre of their research agenda. However, none of the existing work has addressed the problem of machine learning (ML) bias in HRI settings. In addition, many of the current datasets for AC and HRI are ‘small’, making ML bias and debias analysis challenging. This paper presents the first work to explore ML bias analysis and mitigation of three small multimodal datasets collected within both a human-human and robot-human wellbeing coaching settings. The contributions of this work includes: i) being the first to explore the problem of ML bias within HRI settings; and ii) providing a multimodal analysis evaluated via modelling performance and fairness metrics across both high and low-level features and proposing a simple and effective data augmentation strategy (MixFeat) to debias the small datasets presented within this paper; and iii) conducting extensive experimentation and analyses to reveal ML fairness insights unique to AC and HRI research in order to distill a set of recommendations to aid AC and HRI researchers to be more engaged with fairness-aware ML-based research.},
  archive      = {J_TAFFC},
  author       = {Jiaee Cheong and Micol Spitale and Hatice Gunes},
  doi          = {10.1109/TAFFC.2025.3582074},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Small but fair! fairness for multimodal human-human and robot-human mental wellbeing coaching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal framework for therapeutic consultations. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3582198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Therapeutic engagement between client and clinician is a key indicator in determining treatment outcomes for clients with mental health disorders. Quantifying this type of engagement provides an opportunity for the development of an engagement quantification framework for therapeutic efficacy, based on a number of data streams including, body movement and synchronicity, speech, and gestures to determine an individual's level of engagement. In this paper, we present a subset of such a framework through the quantification of engagement based on Facial Affect Recognition, Head Motion, and Natural Language Processing. We propose the use of semantic analysis, emotion dynamics and transitions, and head motion to describe a participant's attention over the consultation. For emotion dynamics and transitions we employ seven standard categorical emotions; for head motion we use acute and chronic head movement; and for semantic analysis we employ Robustly Optimized BERT Pretraining Approach. These features derive two engagement levels: low and high. We performed experiments on the AnnoMI dataset, which contains 133 therapeutic consultation videos for low and high quality motivational interviews, and compared the resulting engagement to the level of motivational interviewing. We achieved an 89.1% average accuracy for the Clinician model and an 81.1% average accuracy for the Client model using Gradient Boost as a classifier.},
  archive      = {J_TAFFC},
  author       = {Martin Ivanov and Alice Rueda and Venkat Bhat and Sridhar Krishnan},
  doi          = {10.1109/TAFFC.2025.3582198},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal framework for therapeutic consultations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamical causal graph neural network for EEG emotion recognition. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3582740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, topological graphs based on structural or functional connectivity of brain network have been utilized to construct graph neural networks (GNN) for Electroencephalogram (EEG) emotion recognition. In this paper, we propose a novel dynamical causal graph neural network (DCGNN) based on the effective causal connectivity of brain function network for EEG emotion recognition, in which Greedy Equivalence Search (GES) is used to find the optimal causal graph topology associated with the adjacent matrix of DCGNN. To this end, we firstly construct a skeleton graph using canonical correlation analysis (CCA) and then use GES to optimize the directional graph topology of DCGNN. Then, learnable weight parameters associated with the adjacent matrix are learnt during the model training of DCGNN. Additionally, a sparse graphic constraint is employed to enhance the efficacy of emotion recognition, while a Conditional Domain Adversarial Network (CDAN) is used to integrate features with emotion labels for improving subjectindependent validation. Extensive experiments and ablation studies are conducted on five public datasets, i.e., SEED, SEED-IV, SEED-V, MPED, and FACED, demonstrating that the proposed model surpasses recent causal based (Granger causality) and domain adaptation based GNN models across all experimental settings.},
  archive      = {J_TAFFC},
  author       = {Yushun Xiao and Wenming Zheng and Guoying Zhao},
  doi          = {10.1109/TAFFC.2025.3582740},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Dynamical causal graph neural network for EEG emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical control of emotion rendering in speech synthesis. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3582715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional speech from input text. However, quantitatively controlling multi-level emotion rendering remains challenging. In this paper, we propose a flow-matching based emotional TTS framework with a novel approach for emotion intensity modeling to facilitate fine-grained control over emotion rendering at the phoneme, word, and utterance levels. We introduce a hierarchical emotion distribution (ED) extractor that captures a quantifiable ED embedding across different speech segment levels. Additionally, we explore various acoustic features and assess their impact on emotion intensity modeling. During TTS training, the hierarchical ED embedding effectively captures the variance in emotion intensity from the reference audio and correlates it with linguistic and speaker information. The TTS model not only generates emotional speech during inference, but also quantitatively controls the emotion rendering over the speech constituents. Both objective and subjective evaluations demonstrate the effectiveness of our framework in terms of speech quality, emotional expressiveness, and hierarchical emotion control.},
  archive      = {J_TAFFC},
  author       = {Sho Inoue and Kun Zhou and Shuai Wang and Haizhou Li},
  doi          = {10.1109/TAFFC.2025.3582715},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hierarchical control of emotion rendering in speech synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3583505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models for music have advanced drastically in recent years, but how good are machine learning models at capturing emotion, and what challenges are researchers facing? In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field. We also offer a brief overview of various types of music emotion prediction models that have been built over the years, providing insights into the diverse approaches within the field. Through this examination, we highlight the challenges that persist in accurately capturing emotion in music, including issues related to dataset quality, annotation consistency, and model generalization. Additionally, we explore the impact of different modalities, such as audio, MIDI, and physiological signals, on the effectiveness of emotion prediction models. Through this examination, we identify persistent challenges in music emotion recognition (MER), including issues related to dataset quality, the ambiguity in emotion labels, and the difficulties of cross-dataset generalization. We argue that future advancements in MER require standardized benchmarks, larger and more diverse datasets, and improved model interpretability. Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository. This repository contains a comprehensive list of music emotion datasets and recent predictive models.},
  archive      = {J_TAFFC},
  author       = {Jaeyong Kang and Dorien Herremans},
  doi          = {10.1109/TAFFC.2025.3583505},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal knowledge distillation for enhanced unimodal emotion recognition. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3583594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal physiological signal-based emotion recognition technology represents an emerging branch in the field of affective computing. By combining various physiological signals, its performance significantly surpasses traditional unimodal methods. Notably, the joint application of electroencephalography (EEG) and galvanic skin response (GSR) show high effectiveness in emotion recognition tasks. However, the difficulty and high cost of EEG signal acquisition limit its widespread use in practical applications. To address this challenge, we propose a Cross-Modal Emotion Knowledge Distillation (CMEKD) framework. This framework not only extracts the heterogeneity and interactivity between GSR and EEG signals but also conveys a comprehensive fusion of multimodal features into an unimodal GSR model through knowledge distillation techniques, thereby enhancing unimodal model performance. We employ cosine similarity-based knowledge representation to reduce the gap between the multimodal model and the unimodal model. Additionally, an adaptive feedback mechanism is introduced to dynamically adjust the distillation process based on the performance of the unimodal model, further improving the classification performance of emotion recognition. Experiment results demonstrate that this framework achieves state-of-the-art performance on two public datasets. By reducing reliance on multimodal data in the application of the emotion recognition technique, we significantly improve the practicality and feasibility of emotion recognition technology. The code for this paper will be made publicly available on GitHub: https://anonymous.4open.science/r/CMEKD-2440/.},
  archive      = {J_TAFFC},
  author       = {Ziyu Jia and Yucheng Liu and Haichao Wang and Tianzi Jiang},
  doi          = {10.1109/TAFFC.2025.3583594},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Cross-modal knowledge distillation for enhanced unimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoupleFER: Dynamic cross-modal fusion via prompt learning for improved 2D+3D FER. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3584264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of 2D texture information and 3D geometric data has shown great promise in advancing the accuracy and robustness of 2D+3D facial expression recognition (FER) systems. Traditional methods in this domain often rely on projecting 3D data onto 2D maps, which limits the effective utilization of critical 3D features. To address this, we introduce CoupleFER, a novel approach that utilizes a cross-modal fusion strategy by combining image-based and point cloud-based networks. Unlike conventional multi-modal fusion methods, CoupleFER introduces the Cross-Modal Prompt Fusion (CouPle) module, enabling dynamic and interactive fusion between the two branches at every layer. This allows 2D texture information to serve as a guiding prompt, thereby enhancing the performance of the 3D FER branch. To further boost robustness and generalization, we propose a dual-level supervision mechanism, which imposes constraints at both the cluster and sample levels during training. Extensive experiments on the widely used BU-3DFE and Bosphorus datasets demonstrate that CoupleFER outperforms state-of-the-art methods, achieving superior recognition accuracy. Ablation studies validate the importance of each key component of the framework, underscoring its potential to significantly improve the performance of 2D + 3D FER systems, and robustness tests demonstrate its stability.},
  archive      = {J_TAFFC},
  author       = {Hebeizi Li and Hongyu Yang and Di Huang},
  doi          = {10.1109/TAFFC.2025.3584264},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CoupleFER: Dynamic cross-modal fusion via prompt learning for improved 2D+3D FER},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heart rate and facial expression data influence the ease of communication in a remote work set-up. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3584315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote communication is challenging compared with face-to-face communication because of the limited social cues. Computer systems built to address this problem could enable individuals to share facial expressions, which are available in face-to-face communication, or biosignals such as heart rate. Currently, many people are unfamiliar with the advantages of sharing such signals online, and clarification of the associated benefits is necessary. To compare the effects of heart rate and facial expression data on the ease of communication, we exposed 20 pairs of participants to a remote work simulation. In this experiment, one participant made contact with their remote partner on the basis of heart rate or facial expression data. Although participants evaluated the heart rate and facial expression data as being similarly useful on average, we found the following differences. Recipients of the contact evaluated the heart rate data as more effective, likely because the recipient's status was more accurately discriminated by the sender. Senders evaluated the facial expression data as more effective, likely because the many parameters were easily interpreted and quickly updated. Thus, heart rate and facial expression data may increase the ease of remote communication, particularly for the recipient and the sender, respectively.},
  archive      = {J_TAFFC},
  author       = {Kaito Kojima and Yuna Takamori and Wataru Sasaki and Aki Kimura and Kensuke Ueda and Kazunori Iwata and Kazushi Mimura and Takeaki Shimokawa},
  doi          = {10.1109/TAFFC.2025.3584315},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Heart rate and facial expression data influence the ease of communication in a remote work set-up},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking emotion annotations in the era of large language models. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3584775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern affective computing systems rely heavily on datasets with human-annotated emotion labels for both training and evaluation. However, human annotations are expensive to obtain, sensitive to study design, and difficult to quality control, because of the subjective nature of emotions. Meanwhile, Large Language Models (LLMs) have shown remarkable performance on many Natural Language Understanding tasks, emerging as a promising tool for text annotation. In this work, we analyze the complexities of emotion annotation in the context of LLMs, focusing on GPT-4 as a leading model. In our experiments, GPT-4 achieves high ratings in a human evaluation study, painting a more positive picture than previous work, in which human labels served as the only ground truth. On the other hand, we observe differences between human and GPT-4 emotion perception, underscoring the importance of human input in annotation studies. To harness GPT-4's strength while preserving human perspective, we explore two ways of integrating GPT-4 into emotion annotation pipelines, showing its potential to flag low-quality labels, reduce the workload of human annotators, and improve downstream model learning performance and efficiency. Together, our findings highlight opportunities for new emotion labeling practices and suggest the use of LLMs as a promising tool to aid human annotation.},
  archive      = {J_TAFFC},
  author       = {Minxue Niu and Yara El-Tawil and Amrit Romana and Emily Mower Provost},
  doi          = {10.1109/TAFFC.2025.3584775},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Rethinking emotion annotations in the era of large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizing to unseen speakers: Multimodal emotion recognition in conversations with speaker generalization. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3566059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Emotion Recognition in Conversations (MERC) aims to identify the emotion expressed in each utterance within conversational videos. Current efforts are directed toward modeling speaker-sensitive context dependencies and multimodal fusion. However, they still struggle to handle utterances from unseen speakers, hampering the model's generalizability. To tackle this challenge, we propose a Speaker Generalization Framework for MERC. Specifically, we build a prototype graph to learn Speaker-based Utterance Representations (SUR), leveraging prototypes as the bridge between seen and unseen speakers. Speaker-aware Contrastive Learning (CL) is then applied to refine SUR, pulling utterances (or prototypes) from the same speaker together while pushing those from different speakers apart. Further, we introduce a prototypical graph CL to generalize SUR to unseen speakers, ensuring that the same speakers exhibit similar graph structures, while dissimilar ones differ. To further enhance model generalization, we introduce Uncertainty-based Generalization for Speakers, randomly sampling SUR statistics from the estimated Gaussian distribution and probabilistically replacing the original SUR. Experimental findings on two datasets highlight that our framework substantially improves the generalization of various MERC models, surpassing state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Geng Tu and Ran Jing and Bin Liang and Yue Yu and Min Yang and Bing Qin and Ruifeng Xu},
  doi          = {10.1109/TAFFC.2025.3566059},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Generalizing to unseen speakers: Multimodal emotion recognition in conversations with speaker generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to rank onset-occurring-offset representations for micro-expression recognition. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3566113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the research of micro-expression recognition (MER) and proposes a flexible and reliable deep learning method called learning to rank onset-occurring-offset representations (LTR3O). The LTR3O method introduces a dynamic and reduced-size sequence structure known as 3O, which consists of onset, occurring, and offset frames, for representing micro-expressions (MEs). This structure facilitates the subsequent learning of ME-discriminative features. A noteworthy advantage of the 3O structure is its flexibility, as the occurring frame is randomly extracted from the original ME sequence without the need for accurate frame spotting methods. Based on the 3O structures, LTR3O generates multiple 3O representation candidates for each ME sample and incorporates well-designed modules based on learning to rank (LTR) to measure and calibrate their emotional expressiveness. This calibration process implicitly enhances the visibility of MEs by amplifying the originally narrow emotional expressiveness gap among ME frames caused by their low-intensity characteristics, thereby facilitating the reliable learning of more discriminative features for MER. Extensive experiments were conducted to evaluate the performance of LTR3O using four widely-used ME databases: CASME II, SMIC, SAMM, and MEVIEW. The experimental results demonstrate the effectiveness and superior performance of LTR3O, particularly in terms of its flexibility and reliability, when compared to recent state-of-the-art MER methods.},
  archive      = {J_TAFFC},
  author       = {Jie Zhu and Yuan Zong and Jingang Shi and Cheng Lu and Hongli Chang and Wenming Zheng},
  doi          = {10.1109/TAFFC.2025.3566113},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Learning to rank onset-occurring-offset representations for micro-expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AM-ConvBLS: Adaptive manifold convolutional broad learning system for cross-session and cross-subject emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3565570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition based on electroencephalography (EEG) data has gained rapid development because of its believability and accuracy. However, most existing EEG emotion recognition methods suffer from three issues: 1) underutilization of multi-scale emotion representations, 2) underexploitation of emotion labels and intrinsic geometric structures, and 3) inherent non-stationarity characteristic and individual variability of EEG signals. To this end, we propose an Adaptive Manifold Convolutional Broad Learning System (AM-ConvBLS) to capture the multi-scale distribution aligned emotion patterns in the geometric structure preserved emotion submanifold space. To begin with, a Multi-Scale Representation Learning (MSRL) module is developed to learn diverse multi-scale emotion representations. The Domain Discrepancy Elimination (DDE) module is then developed to further align feature distributions in the source and target domains. To further utilize EEG emotion labels, we devise a Label Manifold Information Exploration (LMIE) module to retain the sample label consistency. In addition, a global feature importance analysis method for AM-ConvBLS based on Shapley Additive Global importancE (SAGE) is designed to investigate the EEG frequency band importance and brain neural activation patterns. Experiments on SEED, SEED-IV, and SEED-V datasets demonstrate the effectiveness and superiority of our AM-ConvBLS.},
  archive      = {J_TAFFC},
  author       = {Chunyu Lei and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3565570},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {AM-ConvBLS: Adaptive manifold convolutional broad learning system for cross-session and cross-subject emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight spatio-temporal convolutional neural network for audio-visual emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3566773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition (ER) enhances human-computer interaction in customer service and healthcare. However, the high computational complexity of existing convolutional neural network (CNN)-based approaches limits their real-time applicability in compact audio-visual emotion recognition (AVER) systems. This study introduces a lightweight deep learning-based AVER framework, employing a 2-layer, 1D CNN for audio analysis and a 3-layer, 2D CNN for facial image processing. The spatial model (2D CNN) reduces input complexity by using grayscale images, downsizing to 64×64 pixels, and performing three convolutions to extract facial patches. For audio, 1D convolution on Mel-Frequency Cepstral Coefficients (MFCCs) helps preserve essential features while lowering computational demand. With small kernel size and set of optimized parameters, the proposed framework balances performance and complexity. Benchmarking on SAVEE, RAVDESS, and MEAD datasets show accuracy of 97.57%, 95.89%, and 98.57%, respectively, demonstrating its potential for resource-constrained devices. Integrated gradient analysis further reveals key dependencies on eyebrows, eyes, and mouth for facial ER, and 40 MFCCs for audio ER.},
  archive      = {J_TAFFC},
  author       = {Su Yen Ding and Tong Boon Tang and Cheng-Kai Lu},
  doi          = {10.1109/TAFFC.2025.3566773},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Lightweight spatio-temporal convolutional neural network for audio-visual emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with an efficient mix transformer for affective human-robot interaction. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3567966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition can significantly enhance interactions between humans and robots, particularly in shared tasks and collaborative processes. Facial Expression Recognition (FER) allows affective robots to adapt their behavior in a socially appropriate manner. However, the potential of efficient Transformers for FER remains underexplored. Additionally, leveraging self-attention mechanisms to create segmentation masks that accentuate facial landmarks for improved accuracy has not been fully investigated. Furthermore, current FER methods lack computational efficiency and scalability, limiting their applicability in real-time scenarios. Therefore, we developed the robust, scalable, and generalizable EmoFormer model, incorporating an efficient Mix Transformer block along with a novel fusion block. Our approach scales across a range of models from EmoFormer-B0 to EmoFormer-B2. The main innovation lies in the fusion block, which uses element-wise multiplication of facial landmarks to emphasize their role in the feature map. This integration of local and global attention creates powerful representations. The efficient self-attention mechanism within the Mix Transformer establishes connections among various facial regions. It enhances efficiency while maintaining accuracy in emotion classification from facial landmarks. We evaluated our approach for both categorical and dimensional facial expression recognition on four datasets: FER2013, AffectNet-7, AffectNet-8, and DEAP. Our ensemble method achieved state-of-the-art results, with accuracies of 77.35% on FER2013, 67.71% on AffectNet-7, and 65.14% on AffectNet-8. For the DEAP dataset, our method achieved 98.07% accuracy for arousal and 97.86% for valence, demonstrating the robustness and generalizability of our models. As an application of our method, we implemented EmoFormer in an affective robotic arm, enabling the human-robot interaction system to adjust its speed based on the user's facial expressions. This was validated through a user experiment with six subjects, demonstrating the feasibility and effectiveness of our approach in creating emotionally intelligent human-robot interactions. Overall, our results demonstrate that EmoFormer is a robust, efficient, and scalable solution for FER, with significant potential for advancing human-robot interaction through emotion-aware robotics.},
  archive      = {J_TAFFC},
  author       = {Farshad Safavi and Kulin Patel and Ramana Vinjamuri},
  doi          = {10.1109/TAFFC.2025.3567966},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial expression recognition with an efficient mix transformer for affective human-robot interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Past, present, and future: A survey of the evolution of affective robotics for well-being. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3567740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research in affective robots has recognized their potential in supporting human well-being. Due to rapidly developing affective and artificial intelligence technologies, this field of research has undergone explosive expansion and advancement in recent years. In order to develop a deeper understanding of recent advancements, we present a systematic review of the past 10 years of research in affective robotics for wellbeing. In this review, we identify the domains of well-being that have been studied, the methods used to investigate affective robots for well-being, and how these have evolved over time. We also examine the evolution of the multifaceted research topic from three lenses: technical, design, and ethical. Finally, we discuss future opportunities for research based on the gaps we have identified in our review – proposing pathways to take affective robotics from the past and present to the future. The results of our review are of interest to human-robot interaction and affective computing researchers, as well as clinicians and well-being professionals who may wish to examine and incorporate affective robotics in their practices.},
  archive      = {J_TAFFC},
  author       = {Micol Spitale and Minja Axelsson and Sooyeon Jeong and Paige Tuttösí and Caitlin A. Stamatis and Guy Laban and Angelica Lim and Hatice Gunes},
  doi          = {10.1109/TAFFC.2025.3567740},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Past, present, and future: A survey of the evolution of affective robotics for well-being},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SetPeER: Set-based personalized emotion recognition with weak supervision. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3568024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual variability of expressive behaviors is a major challenge for emotion recognition systems. Personalized emotion recognition strives to adapt machine learning models to individual behaviors, thereby enhancing emotion recognition performance and overcoming the limitations of generalized emotion recognition systems. However, existing datasets for audiovisual emotion recognition either have a very low number of data points per speaker or include a limited number of speakers. The scarcity of data significantly limits the development and assessment of personalized models, hindering their ability to effectively learn and adapt to individual expressive styles. This paper introduces EmoCeleb: a large-scale, weakly labeled emotion dataset generated via cross-modal labeling. EmoCeleb comprises over 150 hours of audiovisual content from approximately 1,500 speakers, with a median of 50 utterances per speaker. This rich dataset provides a rich resource for developing and benchmarking personalized emotion recognition methods, including those requiring substantial data per individual, such as set learning approaches. We also propose SetPeER: a novel personalized emotion recognition architecture employing set learning. SetPeER effectively captures individual expressive styles by learning representative speaker features from limited data, achieving strong performance with as few as eight utterances per speaker. By leveraging set learning, SetPeER overcomes the limitations of previous approaches that struggle to learn effectively from limited data per individual. Through extensive experiments on EmoCeleb and established benchmarks, i.e, MSP-Podcast and MSP-Improv, we demonstrate the effectiveness of our dataset and the superior performance of SetPeER compared to existing methods for emotion recognition. Our work paves the way for more robust and accurate personalized emotion recognition systems.},
  archive      = {J_TAFFC},
  author       = {Minh Tran and Yufeng Yin and Mohammad Soleymani},
  doi          = {10.1109/TAFFC.2025.3568024},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SetPeER: Set-based personalized emotion recognition with weak supervision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revealing emotional insights from mental health discussions on instagram and TikTok using BERT models. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3568074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research addresses challenges related to mental health issues in social media by integrating natural language processing. First, the study extends a previous corpus labelled with emotions and polarity by including new Instagram and TikToK posts related to celebrity and influencer disclosures about mental health. This corpus is the first Spanish corpus designed to analyse the impact of social responses to mental health narratives on two of the most widely used social networks. Secondly, the research integrates BERT (Bidirectional Encoder Representations) classification models to improve emotion and polarity detection. One of the modelled algorithms, MenTaiBERT, leveraging a specialised classification layer demonstrates superiority over the other BERT algorithms, achieving 99% accuracy in emotion detection and 98% accuracy in polarity. Indeed, MenTaiBERT significantly outperforms the accuracy of the other algorithms by up to 13 percentage points. Third, A user-friendly graphical tool has been designed, based on the previous corpus and classification models, to help practitioners identify emotional patterns in social media posts related to mental health. In summary, analysing through innovative artificial intelligence strategies the emotional impact of celebrity posts on social networks is crucial, especially among young people, as these platforms significantly influence their self-esteem, perception of reality and emotional well-being.},
  archive      = {J_TAFFC},
  author       = {N. Merayo and A. Ayuso-Lanchares and C. González-Sanguino},
  doi          = {10.1109/TAFFC.2025.3568074},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Revealing emotional insights from mental health discussions on instagram and TikTok using BERT models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Datasets of smartphone modalities for depression assessment: A scoping review. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3568385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile sensing research for depression assessment is rapidly expanding, it is vital to uncover fundamental patterns and overarching limitations in the datasets available for such research, especially as many analyses are conducted on the same datasets. As such, we conduct a scoping review by identifying 80 datasets through the start of 2024 that contain smartphone modalities and depression labels. These datasets originated from 72 manuscripts and approximately 60 research groups. The most collected smartphone modalities included location and/or activity for 68.75%, communication logs for 47.5%, phone use for 41.25%, vocal utterances for 30%, and WiFi and/or Bluetooth connectivity for 28.75% of the datasets. The PHQ-8 and PHQ-9 (Patient Health Questionnaire) were the most popular screening instruments, administered for 53.75% of datasets. Of the 80 datasets, 31.25% recruited from student populations, 22.5% from patient populations, and 7.5% recruited crowdsourced workers. Excluding the nine datasets that did not report demographics, 73% reported a majority of women participants. Given the importance of datasets on analysis outcomes, this scoping review is an invaluable resource to inform the state of science and guide future mobile health research.},
  archive      = {J_TAFFC},
  author       = {ML Tlachac and Michael V. Heinz and Anastasia C. Bryan and Arielle LaPreay and Geri Louise Dimas and Tingting Zhao and Nicholas C. Jacobson and Samuel S. Ogden},
  doi          = {10.1109/TAFFC.2025.3568385},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Datasets of smartphone modalities for depression assessment: A scoping review},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel self-attention residual network: Learning micro-expression recognition features from augmented motion flow images. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3568633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are tiny muscular movements on the face that conceal an individual's genuine emotions. However, the micro-expression recognition (MER) task faces challenges like short duration, low motion intensity, and a scarcity of training data. To solve these problems and obtain a good recognition effect, a Channel Self-Attention Residual Network (CSARNet) is proposed for extracting micro-expression discriminative information from motion stream images with augmented local features. Firstly, based on the offset frames of micro-expressions, a local feature augmentation strategy is devised to augment the local feature representations of motion flow images, thus effectively suppressing the interference of motions that are not related to micro-expressions. Second, aiming to mitigate the risk of model overfitting resulting from the dataset's limited size, CSARNet with a lightweight backbone network structure is designed to streamline the model's complexity and decrease computation time, which also accurately extracts the discriminative information of micro-expressions across channel and spatial dimensions, enabling the effective recognition of emotions. The proposed method was extensively tested on three benchmark datasets (SMIC, CASME II, SAMM) and the composite 3DB dataset, with experimental results clearly showcasing its superiority.},
  archive      = {J_TAFFC},
  author       = {Shuhuan Zhao and Shen Li and Yudong Zhang and Shuaiqi Liu},
  doi          = {10.1109/TAFFC.2025.3568633},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Channel self-attention residual network: Learning micro-expression recognition features from augmented motion flow images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNMCN: Dual-stage normalization based modality-collaborative fusion network for multimodal sentiment analysis. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3569089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high-quality semantic information provided by the text modality, text-driven models have become the dominant approach for Multimodal Sentiment Analysis (MSA) in recent years. Despite notable progress in previous studies, two primary limitations remain: (i) aligning multimodal features often relies on simple matching of sequence length or feature dimension, which overlooks cross-modal heterogeneity. (ii) existing fusion techniques tend to over-rely on text, potentially diminishing the emotional data contributed by other modalities. To address these issues, in this paper, we propose a Dual-stage Normalization based Modality-Collaborative Fusion Network (DNMCN). Initially, to reduce modality discrepancies, we introduce a dual-stage normalization strategy, where features from different modalities were mapped into a common dimensional space in the first stage to facilitate effective cross-modal comparisons; sequence length inconsistencies caused by cropping and multiscale dimension reduction were addressed in the second stage. Additionally, to achieve high-quality cross-modal mapping without losing non-textual modality information, we propose an Adaptive modality-Collaborative Fusion Transformer (ACF-T) block. Specifically, in ACF-T block, textual semantics are first integrated into any non-text modality via multi-head attention. Next, a novel adaptive weighting strategy is introduced to balance the contribution of fused features and other non-textual modality features, thereby enhancing crossmodal interaction. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on the public benchmark datasets CH-SIMS, CMU-MOSI and CMU-MOSEI.},
  archive      = {J_TAFFC},
  author       = {Miao Chen and Jin Liu and Xingye Li and Yaohui Zhang and Hongze Liu and Jiajia Jiao and Huihua He},
  doi          = {10.1109/TAFFC.2025.3569089},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DNMCN: Dual-stage normalization based modality-collaborative fusion network for multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-modal multi-expert framework for pain assessment in postoperative children. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3567307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic pain assessment in postoperative children is crucial for monitoring their health and preventing potential complications. However, the automatic pain assessment still faces the following challenges. Firstly, the individual variation of painful expressions in children enhances the difficulty of mapping diverse features of expressions to pain scores accurately. Secondly, the imbalanced label distribution caused by abundant non-painful samples usually makes the model more likely to predict an unexpectedly lower pain score. To address the above challenges, we propose a novel multi-modal multi-expert framework, namely MMF, for postoperative pain assessment in children. Specifically, the samples are clustered in each modality to train multiple expert models, each focusing on a smaller feature subspace for easier regression of pain scores. Meanwhile, some expert models are allocated to rare painful samples to relieve the side effects caused by the imbalanced distribution of labels. Moreover, a confidence-based integration of multi-modal features from multiple experts is made to achieve a more accurate final prediction. Experimental results show that MMF exhibits superior accuracy of pain assessment on the multi-modal pain database collected from postoperative children by us. In particular, MMF can achieve the mean absolute error (MAE) of 1.03 and the Pearson correlation coefficient (PCC) of 0.88.},
  archive      = {J_TAFFC},
  author       = {Zequan Liang and Hao Luo and Xi Chen and Zhipeng Zhong and Cheng Fan and Xingrong Song and Bilian Li and Jianming Lv},
  doi          = {10.1109/TAFFC.2025.3567307},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A multi-modal multi-expert framework for pain assessment in postoperative children},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of schizophrenia spectrum disorder and major depression disorder using automated speech analysis. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3564531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective biomarkers for differential diagnosis in psychiatry are still scarce. Voice atypicalities characterize two prominent, often co-occurring psychiatric disorders: schizophrenia-spectrum disorders (SSD) and major depressive disorders (MDD). Given that voice recordings can be easily obtained, advanced speech analysis might facilitate the development of diagnostic biomarkers for SSD and MDD. Speech was recorded from a transdiagnostic sample comprising 47 SSD patients, 62 MDD patients, and 41 healthy controls (HC), during three different tasks: a semi-structured interview, a reading task and an empathy task. We evaluated the discriminative power of standardized speech parameters and compared the performance of the three tasks. The extended Geneva Acoustic Minimalistic Parameter Set (eGeMAPS) was extracted using openSMILE and fed into random forest (RF) algorithms with 10-fold crossvalidation. Model performances were evaluated using accuracy, F1-score, precision, and recall. Importance of specific predictors was assessed using Gini importance. In this three-class problem, a simple 1-minute video task reached best results with 57% accuracy. The acoustic parameters revealed distinct vocal profiles associated with each disorder. Considering the chance probability of 33%, our results show that automated speech analysis could predict diagnostic classes with good to high accuracy.},
  archive      = {J_TAFFC},
  author       = {Inka C. Hiß and Jarek Krajewski and Ulrich Canzler and Steffen Leonhardt and Christoph Weiss and Benjamin Clemens and Ute Habel},
  doi          = {10.1109/TAFFC.2025.3564531},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Detection of schizophrenia spectrum disorder and major depression disorder using automated speech analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective embodied agent for patient assistance in virtual rehabilitation. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3569676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents an affective embodied agent to generate empathic interactions in an upper limb virtual rehabilitation platform, enhancing the platform's usability and encouraging its use at home without close supervision by physiotherapists. The agent interacts using natural language (Spanish) and simulating affective states through facial expressions and voice intonation. An interaction component set was defined by considering the opinions of several physiotherapists, including an affective state set (neutrality, enthusiasm, happiness, surprise) and a set of phrases to be expressed by the agent. A 3D model was designed and since affective state perception is subjective, its facial expressions were chosen through a consensus-based study. The agent's affective behavior was defined by a control model that receives the patient's affective state and performance as inputs. It is a rule-based system with averaging and synchronization techniques and a selection method based on probability distributions to provide more natural interactions. A usability evaluation was performed on both versions of the platform, with and without the agent, on a group of 12 non-disabled participants and another of 12 disabled. The results suggest that the agent provides empathic and motivating user experiences, enhances usability, and encourages the use of the platform at home without close expert supervision},
  archive      = {J_TAFFC},
  author       = {Walfred Arreola and Jesus Joel Rivas and Luis Castrejon and Luis Enrique Sucar},
  doi          = {10.1109/TAFFC.2025.3569676},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affective embodied agent for patient assistance in virtual rehabilitation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electronic library for commercially usable emotional stimuli (EL-CUES): An annotated image database for emotion induction validated in a german population. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3570090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inducing specific emotional states in participants is a critical step in many behavioral experiments. Researchers often rely on databases that provide validated stimuli, such as images, for emotional elicitation. However, many of these databases have significant limitations, such as outdated image sets, lack of generalizability across different participant demographics, and a restriction to basic research settings, which complicates their use in commercial research and applied sciences. Here, we introduce the Electronic Library for Commercially Usable Emotional Stimuli (EL-CUES) - a stimulus database comprising 210 images annotated for valence, arousal, and basic emotions by 532 German raters, resulting in an average of 116 ratings per image. Based on these annotations, we grouped the images from the EL-CUES database into eight emotional response groups using k-means clustering. We provide a tool for selecting images tailored to specific individual characteristics, such as gender and age, and the desired emotional response. EL-CUES is available for research and commercial licensing, offering a new option for emotion induction in both basic and applied research with enhanced functionality compared to existing databases.},
  archive      = {J_TAFFC},
  author       = {Moritz Engelhardt and Nina Holzer and Jaspar Pahl and Jessica Freiherr},
  doi          = {10.1109/TAFFC.2025.3570090},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Electronic library for commercially usable emotional stimuli (EL-CUES): An annotated image database for emotion induction validated in a german population},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual graph reconstruction and emotional variation learning for conversational emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3568519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational Emotion Recognition (CER) significantly benefits from the integration of multiple modalities. However, real-world scenarios are often plagued by hardware malfunctions and network failures that lead to missing modalities and incomplete emotional representations. Existing methods primarily focus on modeling inter-modal relationships within isolated utterances to generate missing data, so they do not adequately capture conversational context and dynamic emotional evolution. In conversations, emotional expressions are inherently context-dependent and dynamic. Insufficient modeling of these properties can substantially degrade performance. To address these challenges, we propose Contextual Graph Reconstruction and Emotional Variation Learning (CGR-EVL). Our approach flexibly constructs an utterance graph with diverse coverage based on speaker activity and modality availability, thereby capturing more contextual information. To enhance the handling of missing modalities, we further integrate temporal relations through a relational graph convolutional network to reconstruct missing features and introduce a speaker emotion-aware constraint to ensure emotional coherence. Additionally, we propose the concept of emotional entropy to quantify variation patterns and develop a novel loss function that aligns predicted and actual variations. Experiments on the IEMOCAP and MELD datasets show that CGR-EVL outperforms state-of-the-art methods, particularly under conditions of incomplete modalities.},
  archive      = {J_TAFFC},
  author       = {Yujing Rao and Min Cao and Mang Ye},
  doi          = {10.1109/TAFFC.2025.3568519},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Contextual graph reconstruction and emotional variation learning for conversational emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robot-mediated multi-party conversation aimed at affect improvement for psychiatric patients. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3571427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a multi-party attentive listening system that interacts with two persons to familiarize each other through conversation. We are mainly targeting social implementation in hospitals to contribute to the rehabilitation of people with psychiatric disorders to promote affect improvement in terms of pleasure and arousal. We conducted an experiment in a psychiatric outpatient-daycare program. Twenty daycare attendees participated in a three-party conversation session between a pair of two humans and a humanoid robot. One of the paired participants talked about his/her favorite topic and was attentively listened to by the other and the robot. In a subsequent session, the human pairs switched each other's roles. The subjective evaluations showed that both the pleasure and arousal of the participants were significantly improved after the conversation. The participants rated the impression of the robot as easier to talk with than strangers. They also rated that they could understand and feel familiar with significantly more their human talk partner after the conversational session. The multiple linear regression analysis showed that participants became more pleasant and verbal when stimulated by backchannels and questions from both the human listener and the robot. It suggests that those who expressed themself using more words received positive impressions.},
  archive      = {J_TAFFC},
  author       = {Keiko Ochi and Divesh Lala and Koji Inoue and Tatsuya Kawahara and Hirokazu Kumazaki},
  doi          = {10.1109/TAFFC.2025.3571427},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Robot-mediated multi-party conversation aimed at affect improvement for psychiatric patients},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Catching the blackdog easily: A convenient depression diagnosis method based on audio-visual deep learning. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3571697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression has currently become a serious social problem worldwide. However, the need for experienced doctors and tedious medical examinations greatly increases the inconvenience in diagnosing the depression. A convenient depression diagnosis method can significantly improve the medical experience of depression patients, and can greatly reduce the workload of doctors. In this paper, a Convenient Depression Diagnosis method based on Audio-Visual Deep Learning (CDD-AVDL) is proposed. CDD-AVDL exploits the videos of testers reading a specially-designed text, and note that the videos contain many subconscious human reactions (e.g., micro expressions, voice variations), which are difficultly affected by the artificial interventions, thus enabling the depression diagnosis results more accurate. In CDD-AVDL, the source features are first extracted from audios and visuals, and then the time-sequential features are extracted. Finally, a full connection layer and a convolution layer fusion are responsible for fusing the audio-visual features to yield the depression probabilities. Extensive experiments and clinical tests show that CDD-AVDL outperforms the state-of-the-arts in terms of the accuracy of depression diagnosis. Moreover, the data collection manner in CDD-AVDL is convenient, and the training cost of CDD-AVDL is very low.},
  archive      = {J_TAFFC},
  author       = {Linfeng Liu and Sitan Chen and Kejia Chen and Jiajun Xu and Xiacan Chen},
  doi          = {10.1109/TAFFC.2025.3571697},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Catching the blackdog easily: A convenient depression diagnosis method based on audio-visual deep learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the effects of sleep conditions on emotion responses with EEG signals and eye movements. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3572504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies in psychology and neuroscience have extensively examined the effects of sleep deprivation on emotional responses. More recently, researchers have begun applying deep learning algorithms to further investigate this relationship, emphasizing the importance of accessible and high-quality multimodal datasets across different sleep states. To address this need, we develop SEED-SD, a multimodal dataset comprising data from 40 participants. The dataset includes electroencephalography (EEG) and eye movement signals collected under three sleep conditions: sleep deprivation (SD), sleep recovery (SR), and normal sleep (NS). Each condition contains data corresponding to four basic emotions: happiness, sadness, fear, and neutral state. Additionally, we propose a novel Region Transformer with Layer-Fusion (ReLF), to conduct comprehensive analyses on the SEED-SD dataset. ReLF incorporates a region- wise self-attention mechanism to extract localized features from EEG and eye movement signals, and supports flexible adaptation to both multimodal and unimodal inputs. Following multimodal generative pre-training, ReLF introduces learnable prompts to replace the missing modality under unimodal settings, thereby enabling effective fine-tuning of the pre-trained model. The experimental results demonstrate that ReLF outperforms the existing models. Our analysis further reveals that SD significantly impacts emotion recognition performance, while SR and NS conditions yield similar results, highlighting the importance of SR in mitigating the adverse effects of SD. Furthermore, we conduct a systematic analysis of multimodal complementarity, critical frequency bands, and neural patterns. Our findings reveal distinct EEG patterns under the SD condition compared to the SR and NS conditions. Notably, the multimodal complementarity and critical frequency bands in both the SD and SR conditions align with those observed in the NS condition. In summary, to the best of our knowledge, SEED-SD is the largest publicly available multimodal dataset for studying the relationship between emotion recognition and sleep states. This dataset lays a crucial foundation for applying deep learning methods in this area. Moreover, through extensive data-driven analysis, this work confirms the inhibitory effect of SD on emotion recognition and the restorative role of SR. The SEED-SD dataset and codes will be public upon paper acceptance.},
  archive      = {J_TAFFC},
  author       = {Ziyi Li and Le-Yan Tao and Rui-Xiao Ma and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TAFFC.2025.3572504},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Investigating the effects of sleep conditions on emotion responses with EEG signals and eye movements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age against the machine: How age relates to listeners' ability to recognize emotions in robots' semantic-free utterances. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3568595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic-Free Utterances (SFUs, sounds conveying intention without using words) are being increasingly adopted for human-robot interaction (HRI) to communicate affect. In healthcare, where older adults are overrepresented, affective robotics are becoming more common to reduce healthcare professionals' workload. Hence, understanding how older adults perceive and communicate with robots is crucial. Although previous studies have demonstrated a decline in older adults' ability to categorize emotions, it remains unclear how this impacts their comprehension of SFUs used in HRI. This paper investigates the effect of age (and other factors) on listeners' ability to categorize emotions in SFUs designed for HRI. Additionally, we explore listeners' preferences of SFUs for a healthcare robot. Listeners indicated that SFUs' similarities to natural language, the need for a distinction between human and robot, and their expectations of how a hospital robot should sound like, influenced their preferences. Furthermore, we conducted an online emotion categorization task to investigate how age, emotion category, type of SFU (with varying degrees of robot-likeness), listeners' gender, and their experience with robots relate to listeners' ability to categorize emotions. Results confirm that as age increases, there is a decline in emotion categorization performance of SFUs varying by emotion category and type of SFU.},
  archive      = {J_TAFFC},
  author       = {Hideki Garcia Goo and Laura Ermers and Esther Janse and Jan Kolkmeier and Bob Schadenberg and Vanessa Evers and Khiet P. Truong},
  doi          = {10.1109/TAFFC.2025.3568595},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Age against the machine: How age relates to listeners' ability to recognize emotions in robots' semantic-free utterances},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VISTANet: VIsual spoken textual additive net for interpretable multimodal emotion recognition. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3573180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multimodal emotion recognition system, VIsual Spoken Textual Additive Net (VISTANet), to classify emotions reflected by input containing image, speech, and text into discrete classes. A new interpretability technique, K-Average Additive exPlanation (KAAP), has been developed that identifies important visual, spoken, and textual features leading to predicting a particular emotion class. The VISTANet fuses information from image, speech, and text modalities using a hybrid of intermediate and late fusion. It automatically adjusts the weights of their intermediate outputs while computing the weighted average. The KAAP technique computes the contribution of each modality and corresponding features toward predicting a particular emotion class. To mitigate the insufficiency of multimodal emotion datasets labelled with discrete emotion classes, we have constructed the IIT-R MMEmoRec dataset consisting of images, corresponding speech and text, and emotion labels (‘angry,’ ‘happy,’ ‘hate,’ and ‘sad’). The VISTANet has resulted in an overall emotion recognition accuracy of 80.11% on the IIT-R MMEmoRec dataset using visual, spoken, and textual modalities, outperforming single or dual-modality configurations. The code and data can be accessed at github.com/MIntelligence-Group/MMEmoRec.},
  archive      = {J_TAFFC},
  author       = {Puneet Kumar and Sarthak Malik and Balasubramanian Raman and Xiaobai Li},
  doi          = {10.1109/TAFFC.2025.3573180},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {VISTANet: VIsual spoken textual additive net for interpretable multimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of human emotion synthesis based on generative technology. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3573878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.},
  archive      = {J_TAFFC},
  author       = {Fei Ma and Yifan Xie and Yukan Li and Ying He and Yi Zhang and Hongwei Ren and Zhou Liu and Wei Yao and Fuji Ren and Fei Richard Yu and Shiguang Ni},
  doi          = {10.1109/TAFFC.2025.3573878},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {5},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A review of human emotion synthesis based on generative technology},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensing robots and social odor: How relational engagement and trust change. <em>TAFFC</em>, 1-11. (<a href='https://doi.org/10.1109/TAFFC.2025.3557855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the interaction dynamics between humans and robots also entails examining the exchange of multisensory signals. In this light, this study explores the role of social odors (putative pheromones) in shaping human-robot interactions (HRI) beyond the conventional focus on visual and auditory cues. Our research examines how these odors influence relational engagement (RE; assessed as co-presence and social presence) and trust (dispositional and situational) during interactions with the humanoid social robot (HSR) NAO. The robot was also equipped with a male or female voice, to better counteract odor-related gender differences. Thirty participants engaged in storytelling sessions with NAO under different sensory conditions. Our findings indicate odors significantly affecting trust, especially when resembling female pheromones. Conversely, odors had a nuanced influence on RE, despite participants reporting greater CoPre than SoPre. Additionally, a bidirectional relationship between SoPre and trust emerged, reinforced by sensory congruence. These results highlight the significance of sensory alignment in shaping socio-relational and trust dynamics and emphasize the potential of olfactory stimuli in enhancing human-robot interactions. By deepening our understanding of the complex interplay among olfactory cues, voice gender, RE, and trust within HRIs, we provide valuable insights for crafting and deploying HSRs that are more relatable and trustworthy.},
  archive      = {J_TAFFC},
  author       = {Aquilino L. and Di Dio C. and Manzi F. and Massaro D. and Schito A. and Mazzatenta A. and Cangelosi A. and Invitto S. and Marchetti A.},
  doi          = {10.1109/TAFFC.2025.3557855},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Sensing robots and social odor: How relational engagement and trust change},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective touch via haptic interfaces: A sequential indentation approach. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3565157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving user experience of haptic interfaces is among challenges of haptics research. Affective touch describes haptic information regarding the emotional and social aspects of touch. Still, affective dimensions of touch have not been considered as frequently as its discriminative properties in the design of haptic interfaces. To this end, we designed a haptic interface that can mediate affective touch through sequential indention on the human forearm via nine actuators and investigate in a two-phased repeated-measures study ($\mathbf{n=30}$) if user experience can be improved via our interface. In the first phase, the hidden haptic interface stimulated the forearm of participants with different stimulation velocities synchronously and asynchronously inspired by the rubber hand illusion while the participants focused their vision on the experimenter touching an artificial arm to examine embodiment. The second phase focuses on the perceived pleasantness and continuity of computer-controlled stimuli with different stimulation velocities and forces. The results show that the stimuli with both velocities elicit embodiment, especially during synchronous conditions. Moreover, the results indicate that pleasantness is negatively correlated with the increasing stimulation velocity and/or force despite overall positive ratings under any conditions. These results provide insights into integrating haptic interfaces eliciting affective responses in human-robot interaction scenarios.},
  archive      = {J_TAFFC},
  author       = {Mehmet Ege Cansev and Marius Kindermann and Adna Bliek and Philipp Beckerle},
  doi          = {10.1109/TAFFC.2025.3565157},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affective touch via haptic interfaces: A sequential indentation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding affective cues in multimodal textual transcriptions. <em>TAFFC</em>, 1-8. (<a href='https://doi.org/10.1109/TAFFC.2025.3565628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal annotations provide important cues for understanding how a conversation proceeded, particularly in relation to affective factors. In this paper, we extend the automated conversation annotation system MONAH with pitch and volume annotations to encode new affective cues, making MONAHv3 the state-of-the-art automatic annotation system in terms of the number of automatically annotated aspects. MONAHv3 provides an automated solution that, while not a direct equivalent, offers performance that is competitive with the manually produced Jefferson transcription system. In automatic evaluations, the additions significantly improve supervised learning in ten out of eighteen experiments. In human evaluations of emotion recognition, the additions significantly outperformed the Jefferson transcription system. Usability studies further showed that MONAHv3 is much more user-friendly than Jefferson-style transcripts. Lastly, in human evaluations of paralinguistic cues (e.g., tone and volume), MONAHv3 achieved performance comparable to the Jefferson system, while remaining more competitive in kinesics (e.g., describing actions). This distinction between automated and manual systems is critical for appreciating the scalability and accessibility that MONAHv3 offers.},
  archive      = {J_TAFFC},
  author       = {Joshua Y. Kim and Kalina Yacef},
  doi          = {10.1109/TAFFC.2025.3565628},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Encoding affective cues in multimodal textual transcriptions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cognitive and aesthetic causality for multimodal aspect-based sentiment analysis. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3565506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal aspect-based sentiment classification (MASC) is an emerging task due to an increase in user-generated multimodal content on social platforms, aimed at predicting sentiment polarity toward specific aspect targets (i.e., entities or attributes explicitly mentioned in text-image pairs). Despite extensive efforts and significant achievements in existing MASC, substantial gaps remain in understanding fine-grained visual content and the cognitive rationales derived from semantic content and impressions (cognitive interpretations of emotions evoked by image content). In this study, we present Chimera: a cognitive and aesthetic sentiment causality understanding framework to derive fine-grained holistic features of aspects and infer the fundamental drivers of sentiment expression from both semantic perspectives and affective-cognitive resonance (the synergistic effect between emotional responses and cognitive interpretations). The framework aligns visual patches with words, extracts coarse and fine-grained visual features, translates them into textual descriptions, and uses LLM-generated sentimental causes and impressions to boost sensitivity to affective cues. Experiments on MASC datasets show the model's effectiveness and greater flexibility compared to LLMs like GPT-4o. We have publicly released the complete implementation and dataset at https://github.com/Xillv/Chimera},
  archive      = {J_TAFFC},
  author       = {Luwei Xiao and Rui Mao and Shuai Zhao and Qika Lin and Yanhao Jia and Liang He and Erik Cambria},
  doi          = {10.1109/TAFFC.2025.3565506},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring cognitive and aesthetic causality for multimodal aspect-based sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Examining the fourier spectrum of speech signal from a time-frequency perspective for automatic depression level prediction. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3565654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, many studies use Fourier amplitude spectra of speech signals to predict depression levels. However, those works often treat Fourier amplitude spectra as images or sequences to capture depression cues using convolutional neural networks or multilayer perceptrons. Therefore, they ignore the complex element composition and time-frequency attributes of Fourier spectra, which is not conducive to capturing the differences among individuals with different depression levels. For this reason, we construct a Time-Frequency Self-Embedding (TFSE) module, which not only stores the correlation relationship among real (imaginary) parts of Fourier spectra of different subjects from the time-frequency perspective, but also maintain the physical properties of data through the weight embedding process. Besides, Global Average Pooling (GAP) or linear layers are difficult to balance both temporal and frequency dimensions in the vectorization process. Therefore, we construct a Time-Frequency Tensor Vectorization (TFTV) module, which summarizes each channel along time and frequency dimensions, and then generates the vectorization result by integrating various channels. In this way, we combine TFSE and TFTV modules to form our SpectrumFormer model for predicting depression levels. Evaluation indicators on AVEC 2013 and AVEC 2014 depression databases imply the progressiveness of our model.},
  archive      = {J_TAFFC},
  author       = {Mingyue Niu and Jianhua Tao and Yongjun He and Shiqing Zhang and Ming Li},
  doi          = {10.1109/TAFFC.2025.3565654},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Examining the fourier spectrum of speech signal from a time-frequency perspective for automatic depression level prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting sympathetic discharges: Comparison of electrodermal activity and skin sympathetic nerve activity in stimulation-to-response time and recovery time to baseline. <em>TAFFC</em>, 1-9. (<a href='https://doi.org/10.1109/TAFFC.2025.3565174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sympathetic nervous system (SNS) is pivotal in cardiovascular regulation. Skin sympathetic nerve activity (SKNA), extracted from electrocardiogram (ECG), is an innovative noninvasive metric for assessing SNS, traditionally evaluated via electrodermal activity (EDA). Identifying sparse drivers—markers of neural discharge onsets in EDA or SKNA signals—enhances SNS analysis. We employed the SparsEDA algorithm to identify SKNA drivers and compared them with EDA phasic drivers derived from cvxEDA and SparsEDA algorithms. Our evaluation included burst detection performance and temporal features: stimulus-to-response time and recovery time. In a thermal grill pain experiment with sixteen subjects, each undergoing six SNS stimulations, we concurrently recorded EDA and SKNA signals. SKNA demonstrated superior performance with a 97% hit rate, 100% recovery rate, and minimal false alarms during control (0.20±0.77) and interstimulus periods (1.27±1.44). In contrast, EDA drivers had hit and recovery rates below 80% and 50%, respectively, and a higher number of false alarms during interstimulus times (SparsEDA: 11.36±5.24; cvxEDA: 10.00±5.88). SKNA drivers closely matched annotated labels, achieving a root mean square error (RMSE) of 0.42 s, outperforming SparsEDA (RMSE: 5.09 s) and cvxEDA (RMSE: 3.73 s) drivers. The SKNA recovery time averaged 5.59±2.44 seconds, closely aligning with the actual pain stimulus duration (∼5 s), whereas EDA recovery times were nearly three times longer. The superior accuracy of SKNA drivers may result from directly measuring nerve activity, unlike EDA signals influenced by sweat pore hydrodynamics. Therefore, SKNA, with its precise onset response and quicker recovery time, holds promise for diverse applications in SNS assessment.},
  archive      = {J_TAFFC},
  author       = {Farnoush Baghestani and Youngsun Kong and I-Ping Chen and William D'Angelo and Ki H. Chon},
  doi          = {10.1109/TAFFC.2025.3565174},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Detecting sympathetic discharges: Comparison of electrodermal activity and skin sympathetic nerve activity in stimulation-to-response time and recovery time to baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inner speech and damasio's theory for modelling robot's emotions. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3547756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Affective Robotics, there is a growing emphasis on endowing robots with the ability to experience emotions, not just detect and recognise human emotions. Robots can appropriately respond to emotionally relevant events, simulating emotional behaviour, thus improving social interaction. This work analyses the robot's emotional experiences by inner speech. Recent investigations demonstrated that the robot's inner speech improves people's trust and brings the robot closer to human cognition. Through inner speech, the robot overtly talks to itself and reasons about the context and inner processes. Based on Damasio's theory, emotions emerge from implementing the dynamic interplay between bodily sensations and emotional cognitive processes. Integrating such a theory with self-talking capability and deploying the resulting model on a real robot makes such a robot able to experience emotions. Experiments demonstrate that people interacting with robots equipped with the proposed model recognise the robot's emotional experiences, supporting a more empathetic and emotionally connected relationship.},
  archive      = {J_TAFFC},
  author       = {Sophia Corvaia and Arianna Pipitone and Antonio Chella},
  doi          = {10.1109/TAFFC.2025.3547756},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Inner speech and damasio's theory for modelling robot's emotions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantifying the sense of control through the hand blink reflex in human-robot interaction. <em>TAFFC</em>, 1-11. (<a href='https://doi.org/10.1109/TAFFC.2025.3548172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and robots are expected to collaborate closely sharing the same working environment. However, a quantitative measure of the perceived sense of control (SoC) in interacting with artificial systems is still an unmet need. This work introduces a ground-breaking approach that will profoundly impact the way of evaluating the SoC in human-robot interaction. We investigated the human behaviour during human-robot interactions by examining the Hand Blink Reflex (HBR). The HBR is an innate defensive reflex elicited by the electrical stimulation of the median nerve and can be measured using EMG recordings from the orbicularis oculi muscles. We recorded HBR in twenty subjects during different experimental conditions considering a robotic arm entering the defensive peripersonal space (DPPS) at near or far proximities to the face and under different control modalities: autonomous robot movement, human control limited to starting or stopping the robot, and human fully controlling the speed of the robot. According to predictions, the HBR amplitude is modulated by the proximity of the robotic arm to the DPPS. Crucially, the more the human confidence in the robot control, the lower the HBR amplitude. This novel method quantifies human confidence in robot control, potentially advancing human-robot collaboration by enhancing our understanding of the neural mechanisms underlying perceived control and safety in shared workspaces. Results can be further exploited for comparing the effectiveness of robot control interfaces and algorithms},
  archive      = {J_TAFFC},
  author       = {Tommaso Lisini Baldi and Bernardo Brogi and Alessandro Giannotta and Gionata Salvietti and Domenico Prattichizzo and Simone Rossi},
  doi          = {10.1109/TAFFC.2025.3548172},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Quantifying the sense of control through the hand blink reflex in human-robot interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curiosity and affect-driven cognitive architecture for HRI. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3551512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how humans and cognitive robots with different value systems and motivations understand each other's needs in free-form interactions. We developed a cognitive architecture that links sensing and perception to internal motivation and an intrinsic value system for determining actions. Inspired by young children's needs, this architecture includes three drives: learning, interaction, and recharging, each with varying dependence on the human partner. We aimed to assess how experimentally changing the importance of these drives within a fixed architecture affects interaction dynamics with human partners (acting as caregivers) and their understanding of the robot's needs. By adjusting the learning and interaction drives, we created two robot profiles: Playful, which prioritizes environmental exploration and playfulness to reduce boredom, and Social, which focuses on social interaction through touch and visual contact to increase comfort. Our findings show that changing the importance of these drives produces distinct behaviors and human perceptions. Robot behaviors matched their profiles, and participants adapted their responses accordingly. Participants identified and attributed distinct traits to each robot without knowing the specific profiles. Despite variability among human partners, the robots, especially the playful one, were generally well understood by most participants.},
  archive      = {J_TAFFC},
  author       = {Letícia Berto and Ana Tanevska and Azamor Cirne and Paula Costa and Alexandre Simões and Ricardo Gudwin and Francesco Rea and Esther Colombini and Alessandra Sciutti},
  doi          = {10.1109/TAFFC.2025.3551512},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {3},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Curiosity and affect-driven cognitive architecture for HRI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial action unit recognition guided by labeling rules. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3551773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial action unit (AU) recognition studies either do not leverage the relations between AU representations or do not utilize important facial cues for AU labeling fully, which has limited their performance. To address these limitations, we design a novel AU recognition framework guided by AU labeling rules. Specifically, we first leverage AU labeling rules from the Facial Action Coding System to separate facial judgment areas and define the explicit correspondence between AUs and the judgment areas. A region feature extraction component is utilized to extract representations for the judgment areas. Then, AU-specific representations are mapped from their corresponding judgment areas. AU relations are further encoded to enhance the AU representation learning based on Transformer encoder. After that, we introduce a region relation learning component to encode the relations among judgment areas to further guide the region representation learning through Transformer encoder and the proposed auxiliary task. Finally, the encoded AU and region patterns are jointly fed into the AU predicting component to perform AU recognition based on Transformer decoder. The designed Transformer encoder-decoder framework can fully leverage both relations among AU representations and facial cues. Experimental results on three public databases demonstrate the effectiveness of the proposed method compared with that of current state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Shangfei Wang and Yanan Chang and Caichao Zhang},
  doi          = {10.1109/TAFFC.2025.3551773},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial action unit recognition guided by labeling rules},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of distraction and the impact on technology acceptance of robot monitoring behaviour in older adults care. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3539015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People's successful coexistence with robots strictly depends on people's acceptance of robots' presence in their daily activities. This is particularly relevant when the robot's actions may interfere with or intrude on people's activities, creating discomfort and possible rejection. We believe that people's acceptance of a robot may vary depending on the activities they are involved in. In this study, we investigate the impact of a robot's actions on people's engagement in an activity while the robot has the task of monitoring them. We observed the behaviours of 18 older adults with respect to the robot while they were carrying out tasks that require different cognitive workloads (e.g., working at the PC, talking on the phone). We used subjective and objective metrics, such as social cues, to evaluate people's engagement in the robot and their disengagement in their own tasks. We observed that people were distracted by the robot's behaviours based on the cognitive loads required by their activity. Our results show that variation in people's engagement in the robot and the task is affected by their perception of the usefulness of and trust in the robot, and by individuals' personality traits and acceptance of the robot. People with higher trust in the robot, and a higher degree of conscientiousness and emotional stability, tend to continue with their task, paying less attention to the robot. We observed, in contrast, that a robot perceived as a social entity caught more easily their attention when people have a higher extroverted personality. Our findings also showed that variations in the affective and emotional demeanour of the participants are a predictor of their distraction to an external observer.},
  archive      = {J_TAFFC},
  author       = {Gianpaolo Maggi and Luca Raggioli and Alessandra Rossi and Silvia Rossi},
  doi          = {10.1109/TAFFC.2025.3539015},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Assessment of distraction and the impact on technology acceptance of robot monitoring behaviour in older adults care},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theory of mind abilities predict robot's gaze effects on object preference. <em>TAFFC</em>, 1-10. (<a href='https://doi.org/10.1109/TAFFC.2025.3531945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the differences between human and robot gaze in influencing preference formation, and examined the role of Theory of Mind (ToM) abilities in this process. Human eye gaze is one of the most important sources of information for social interaction and research has demonstrated its effectiveness in influencing people's preference. With increasing technological development, we will interact with robots that can exhibit gaze behavior and influence people's preference. It is unclear whether there are any differences between humans and robots in this process. The present study aimed to analyze the role of the gaze of a robot and a human in influencing the ascription of a preference to the gazer and the participants' preference. Furthermore, we examined the role of ToM abilities in preference formation. The results showed that the gaze has a greater effect on the gazer preference compared to participants' preference regardless of the agent (human or robot). In addition, ToM abilities predict both gazer and individual preferences in the robot's condition only even though different socio-cognitive mechanisms are involved. The study suggests that adults are cognitively able to process the gaze of a robot similar to a human, recognizing the underlying mental state. However, only for the robot, different cognitive mechanisms are involved in the gazer (i.e., perspective taking) and participants' preference formation (i.e., advanced ToM).},
  archive      = {J_TAFFC},
  author       = {Federico Manzi and Mitsuhiko Ishikawa and Cinzia Di Dio C and Shoji Itakura and Takayuki Kanda and Hiroshi Ishiguro and Davide Massaro and Antonella Marchetti},
  doi          = {10.1109/TAFFC.2025.3531945},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Theory of mind abilities predict robot's gaze effects on object preference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial 3D regional structural motion representation using lightweight point cloud networks for micro-expression recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3535569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-computer interaction (HCI) relies on understanding and adapting to users' emotional states. Micro-expressions (MEs), a critical component of emotional perception, are characterized by their spontaneity, rapidity, subtlety, and difficulty to control. They often reveal an individual's true emotions. A comprehensive and detailed representation of motion is necessary to capture the nuances of facial dynamics effectively. Presently, motion representation methods are predominantly confined to 2D analysis within RGB images, overlooking the critical role of facial structure and its movements in conveying emotions. To overcome this limitation, we introduce an innovative facial motion representation that encompasses 3D facial structure, regionalized RGB and structural motion features. Furthermore, we segment the face into eight distinct regions, selecting only the most significant motion points to delineate the primary motion characteristics of each area. To model the interactions among crucial facial motion regions, we employ an advanced, lightweight point cloud and graph convolution network (Lite-Point-GCN). Comprehensive testing on the $\mathrm{CAS(ME)^{3}}$ dataset, using leave-one-subject-out (LOSO), demonstrates that our method outperforms existing state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Ren Zhang and Jianqin Yin and Chao Qi and Yonghao Dang and Zehao Wang and Zhicheng Zhang and Huaping Liu},
  doi          = {10.1109/TAFFC.2025.3535569},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial 3D regional structural motion representation using lightweight point cloud networks for micro-expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machiavellian robots and their theory of mind. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2024.3494595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this work is to develop and evaluate computational cognitive models of Theory of Mind (ToM) and Machiavellian behavior embedded in a humanoid robot. Machiavellianism, together with psychopathy and narcissism, is part of the Dark Triad (DT), three constructs that correspond to socially aversive yet not necessarily pathological personalities. The motivations of the present work are both theoretical and application-oriented. In the long term, we aim to: (i) Provide researchers with new insights into the Machiavellian as well as other DT constructs through simulated and robotic setups; (ii) Provide a tool to train psychologists to deal with social and antisocial behavior in a controlled setup; (iii) Help people become aware of the behavioral mechanisms that they may expect from people with DT traits in social and affective relationships; (iv) Assist robotic engineers in developing better robots by identifying behaviors that should be avoided. To this end, we explored a computational model of ToM in the popular Planning Domain Definition Language (PDDL), and defined a domain with the necessary elements to induce Machiavellian behavior during planning and execution. Subsequently, we implemented our computational model in a software architecture controlling the behavior of a humanoid robot and recorded videos of the robot interacting with two actors. Finally, we conducted experiments with 300 participants divided into 6 conditions to verify whether the implemented framework is versatile enough to generate behaviors that participants would rate as either more Machiavellian or less Machiavellian based on their observations of the recorded videos.},
  archive      = {J_TAFFC},
  author       = {Antonio Sgorbissa and Lorenzo Morocutti and Ilenia D'Angelo and Carmine T. Recchiuto},
  doi          = {10.1109/TAFFC.2024.3494595},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {11},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Machiavellian robots and their theory of mind},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pilot study on the comparison of prefrontal cortex activities of robotic therapies on elderly with mild cognitive impairment. <em>TAFFC</em>, 1-11. (<a href='https://doi.org/10.1109/TAFFC.2024.3461409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demographic shifts have led to an increase in mild cognitive impairment (MCI), and this study investigates the effects of cognitive training (CT) and reminiscence therapy (RT) conducted by humans or socially assistive robots (SARs) on prefrontal cortex activation in elderly individuals with MCI, aiming to determine the most effective therapy-modality combination for promoting cognitive function. This pilot study employs a randomized control trial (RCT) design. Additionally, the study explores the efficacy of Reminiscence Therapy (RT) in comparison to Cognitive Training (CT). Eight MCI subjects, with a mean age of 70.125 years, were randomly assigned to “human-led” or “SAR-led” groups. Utilizing Functional Near- infrared Spectroscopy (fNIRS) to measure oxy-hemoglobin concentration changes in the dorsolateral prefrontal cortex (DLPFC), the study found no significant differences in the effects of human-led and SAR-led cognitive training on DLPFC activation. However, distinct patterns emerged in memory encoding and retrieval phases between RT and CT, suggesting the impacts of these interventions on brain activation in the context of MCI.},
  archive      = {J_TAFFC},
  author       = {King Tai Henry Au-Yeung and William Wai Lam Chan and Kwan Yin Brian Chan and Hongjie Jiang and Junpei Zhong},
  doi          = {10.1109/TAFFC.2024.3461409},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A pilot study on the comparison of prefrontal cortex activities of robotic therapies on elderly with mild cognitive impairment},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharing our emotions with robots: Why do we do it and how does it make us feel?. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2024.3470984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-disclosure and the social sharing of emotions facilitate social relationships and can positively affect people's well-being. Nevertheless, individuals might refrain from engaging in these interpersonal communication behaviours with other people, due to socio-emotional barriers, such as shame and stigma. Social robots, free from these human-centric judgements, could encourage openness and overcome these barriers. Accordingly, this paper reviews the role of self-disclosure and social sharing of emotion in human-robot interactions (HRIs), particularly its implications for emotional well-being and the dynamics of social relationship building between humans and robots. We investigate the transition of self-disclosure dynamics from traditional human-to-human interactions to HRI, revealing the potential of social robots to bridge socio-emotional barriers and provide unique forms of emotional support. This review not only highlights the therapeutic potential of social robots but also raises critical ethical considerations and potential drawbacks of these interactions, emphasising the importance of a balanced approach to integrating robots into emotional support roles. The review underscores a complex but promising frontier at the intersection of technology and emotional well-being, advocating for careful consideration of ethical standards and the intrinsic human need for connection as we advance in the development and application of social robots.},
  archive      = {J_TAFFC},
  author       = {Guy Laban and Emily S. Cross},
  doi          = {10.1109/TAFFC.2024.3470984},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Sharing our emotions with robots: Why do we do it and how does it make us feel?},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
