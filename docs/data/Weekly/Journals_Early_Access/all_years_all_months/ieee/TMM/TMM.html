<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm">TMM - 223</h2>
<ul>
<li><details>
<summary>
(2025). Model synthesis for zero-shot model attribution. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, generative models are shaping various fields such as art, design, and human-computer interaction, yet they are accompanied by copyright infringement and content management challenges. In response, existing research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute the generated images to their source models. However, existing methods are restricted to identifying models within a static set included in classifier training, incapable of adapting dynamically to newly emerging unseen models. To bridge this gap, this paper aims to develop a generalized model fingerprint extractor capable of zero-shot attribution that effectively attributes unseen models without exposure during training. Central to our method is a model synthesis technique, which generates numerous synthetic models that mimic the fingerprint patterns of real-world generative models. The design of the synthesis technique is motivated by observations on how the basic generative model's architecture building blocks and parameters influence fingerprint patterns, and it is validated through designed metrics to examine synthetic models' fidelity. Our experiments demonstrate that the fingerprint extractor, trained solely on synthetic models, achieves impressive zero-shot generalization on a wide range of real-world generative models, improving model identification and verification accuracy on unseen models by over 40% and 15%, respectively, compared to existing approaches.},
  archive      = {J_TMM},
  author       = {Tianyun Yang and Juan Cao and Danding Wang and Chang Xu},
  doi          = {10.1109/TMM.2025.3607778},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Model synthesis for zero-shot model attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instructive probabilistic transformer for complex action recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex action recognition aims to identify multiple actions over a long time. Multiple actions may occur at the same time (defined as simultaneous actions), and may occur after each other (defined as each action) Complex action recognition may suffer from two challenges. (1) Temporal repeated bias. The same action may repeat in a temporal duration. In this duration, the prediction may be biased to the majority of actions, which occur repeatedly in the past temporal frames. (2) Epistemic uncertainty of multiple actions. When there are multiple simultaneous actions in one frame, this frame's feature may result in the distribution of multiple actions overlapping each other. Without modeling proper relations between actions, the model may hinder accurately explaining certain categories in multiple actions (defined as the model's epistemic uncertainty). In this work, we propose an Instructive Probabilistic Transformer, which contains a probabilistic temporal memorizer, and a probabilistic prototype Transformer. First, to alleviate temporal repeated bias, we design a probabilistic temporal memory module, which learns probabilistic temporal gates to localize each action. The probabilistic gates instruct the selective memory of each action in long-term frames. Second, we cluster features to capture common action semantics among features (defined as action prototypes). To alleviate the epistemic uncertainty of multiple actions, we design a probabilistic prototype Transformer module. This module learns probabilistic relations depending on each prototype, which can ensure the separation between different prototypes. Third, to ensure the proper probabilistic relations depending on each prototype, we extend action loss with distribution loss to learn uncertainty-aware action loss. In uncertainty-aware action loss, the distribution loss measures the consistency between probabilistic relations and prototype relation distribution. The prediction uncertainty is learned by analyzing the entropy of multiple predictions, and helps to ensure the effect between action loss and distribution loss. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Charades, Breakfast Actions, and MultiTHUMOS.},
  archive      = {J_TMM},
  author       = {Zhao Xie and Longsheng Lu and Kewei Wu and Zhehan Kan and Xingming Yang and Dan Guo},
  doi          = {10.1109/TMM.2025.3599089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instructive probabilistic transformer for complex action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AS-GCL: Asymmetric spectral augmentation on graph contrastive learning. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.},
  archive      = {J_TMM},
  author       = {Ruyue Liu and Rong Yin and Yong Liu and Xiaoshuai Hao and Haichao Shi and Can Ma and Weiping Wang},
  doi          = {10.1109/TMM.2025.3604953},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AS-GCL: Asymmetric spectral augmentation on graph contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards semi-supervised dual-modal semantic segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D and 2D data acquisition techniques, it has become easy to obtain point clouds and images of scenes simultaneously, which further facilitates dual-modal semantic segmentation. Most existing methods for simultaneously segmenting point clouds and images rely heavily on the quantity and quality of the labeled training data. However, massive point-wise and pixel-wise labeling procedures are time-consuming and labor-intensive. To address this issue, we propose a parallel dual-stream network to handle the semi-supervised dual-modal semantic segmentation task, called PD-Net, by jointly utilizing a small number of labeled point clouds, a large number of unlabeled point clouds, and unlabeled images. The proposed PD-Net consists of two parallel streams (called original stream and pseudo-label prediction stream). The pseudo-label prediction stream predicts the pseudo labels of unlabeled point clouds and their corresponding images. Then, the unlabeled data is sent to the original stream for self-training. Each stream contains two encoder-decoder branches for 3D and 2D data respectively. In each stream, multiple dual-modal fusion modules are explored for fusing the dual-modal features. In addition, a pseudo-label optimization module is explored to optimize the pseudo labels output by the pseudo-label prediction stream. Experimental results on two public datasets demonstrate that the proposed PD-Net not only outperforms the comparative semi-supervised methods but also achieves competitive performances with some fully-supervised methods in most cases.},
  archive      = {J_TMM},
  author       = {Qiulei Dong and Jianan Li and Shuang Deng},
  doi          = {10.1109/TMM.2025.3604939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards semi-supervised dual-modal semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high resolution (UHR) image segmentation is a challenging task that requires efficient processing of large images while maintaining high accuracy. Existing approaches usually employ both shallow and deep networks to extract high-resolution details and global context from different-resolution inputs, achieving a balance between performance, memory, and speed. However, these methods still rely on preserving relatively high-resolution features within the deep network, leading to increased time and memory costs. This also indicates that the full potential of the high-resolution information from the shallow network remains underexplored. To address this, we propose a novel framework called the Hierarchical Grafting Network (HGN), wherein the shallow network is hierarchically grafted to the deep network from multiple perspectives, enabling comprehensive utilization of the features from the shallow network. Our framework involves carefully designed global structure aggregated grafting and local structure aligned grafting mechanism, which progressively integrate semantic details and spatial structure from the shallow network to the deep network. In addition, to enhance the discriminative power of the high-resolution local features extracted by the shallow network, we introduce a shallow-deep contrastive loss to encourage the shallow network to learn semantically similar features to those of the deep network. Extensive experiments on several UHR image segmentation datasets demonstrate that our approach outperforms state-of-the-art UHR methods. The results demonstrate an overall improvement in terms of memory efficiency, accuracy, and speed.},
  archive      = {J_TMM},
  author       = {Ting Liu and Jing Yang and Shikui Wei and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3604913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crafting more transferable adversarial examples via quality-aware transformation combination. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input diversity is an effective technique for crafting transferable adversarial examples that can deceive unknown AI models. Existing input-diversity-based methods typically use single input transformation, limiting targeted transferability and defense robustness. Combining different transformation types is challenging, as keeping increasing types would degrade semantic information and targeted transferability. This paper proposes a quality-aware transformation combination attack (TCA) that selects high-quality transformation combinations. The quality-aware selection enables expansion of transformation types, enhances input diversity, and hence improves targeted transferability and defense robustness. We first design a quality-evaluation framework to quantify the effectiveness of transformation combinations, which jointly considers convergence, transferability, and robustness. Only a small group (up to 10) of images are required for computation-efficient quality evaluation. Experiments validate TCA's superiority over state-of-the-art baselines in adversarial transferability and robustness. When defenses are secured, the average targeted success rate of TCA with four transformation types (i.e., TCA-t4) outperforms the best baseline by 26%$\sim$42% on ImageNet.},
  archive      = {J_TMM},
  author       = {Junlin Liu and Xinchen Lyu and Chenshan Ren and Qimei Cui},
  doi          = {10.1109/TMM.2025.3604967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Crafting more transferable adversarial examples via quality-aware transformation combination},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code is made publicly available at https://github.com/hangyu94/KE2NT.},
  archive      = {J_TMM},
  author       = {Hangyu Li and Yihan Xu and Jiangchao Yao and Nannan Wang and Xinbo Gao and Bo Han},
  doi          = {10.1109/TMM.2025.3604916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement-based equivariant learning for compositional VQA. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional visual question answering (VQA) represents a challenging yet fundamental task that requires models to comprehend novel combinations of previously learned concepts. The current methods often overlook the disentanglement of underlying concepts and are restricted in terms of their ability to effectively capture the compositional variation mechanism. Moreover, the state-of-the-art techniques depend on additional clues for training, which is not feasible in real-world VQA scenarios. To address these issues, in this paper, we introduce a novel Disentanglement-based EquivAriant Learning (DEAL) framework for compositional VQA, which is guided exclusively by ground-truth answers. In DEAL, we employ causality-inspired interventions to disentangle concepts derived from visual and textual inputs within a re-encoding framework. Based on the principle of equivariance, we subsequently perform a compositional transformation on the inference input and impose the equivariant constraint on the output to augment the compositional reasoning capacity of the model. Comprehensive experiments conducted on the benchmark CLEVR-CoGenT and GQA-SGL datasets validate the superiority of our proposed DEAL approach over the existing state-of-the-art methods for compositional VQA tasks in both visual and linguistic generalization settings.},
  archive      = {J_TMM},
  author       = {Zhou Du and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604897},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentanglement-based equivariant learning for compositional VQA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient SDRTV-to-HDRTV by learning from image formation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary display enables video content rendering with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of existing content remains in standard dynamic range (SDR) format. Therefore, the conversion of SDR content to HDRTV standards holds significant value. This paper delineates and analyzes the SDRTV-to-HDRTV conversion by modeling the formation of SDRTV/HDRTV content. The findings reveal that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this, we propose a new three-step solution called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step utilizes global statistics for image-adaptive color adjustments, followed by a local enhancement network for detail improvement. These two components are integrated as a generator, with GAN-based joint training ensuring highlight consistency. Our method, tailored for ultra-high-definition TV content, offers both effectiveness and computational efficiency in processing 4K resolution images. We also construct HDRTV1K, a dataset comprising HDR videos adhering to the HDR10 standard, featuring 1235 training and 117 testing images at 4K resolution. Furthermore, we employ five metrics to assess SDRTV-to-HDRTV performance. Our results demonstrate state-of-the-art performance both quantitatively and visually. The codes and models are available at https://github.com/xiaom233/HDRTVNet-plus.},
  archive      = {J_TMM},
  author       = {Xiangyu Chen and Zheyuan Li and Zhengwen Zhang and Jimmy S. Ren and Yihao Liu and Jingwen He and Yu Qiao and Jiantao Zhou and Chao Dong},
  doi          = {10.1109/TMM.2025.3604961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient SDRTV-to-HDRTV by learning from image formation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised hyperspectral image (HSI) clustering remains a fundamental yet challenging task due to the absence of labeled data and the inherent complexity of spatial-spectral interactions. While recent advancements have explored innovative approaches, existing methods face critical limitations in clustering accuracy, feature discriminability, computational efficiency, and robustness to noise, hindering their practical deployment. In this paper, a self-supervised efficient low-pass contrastive graph clustering (SLCGC) is introduced for HSIs. Our approach begins with homogeneous region generation, which aggregates pixels into spectrally consistent regions to preserve local spatial-spectral coherence while drastically reducing graph complexity. We then construct a structural graph using an adjacency matrix A and introduce a low-pass graph denoising mechanism to suppress high-frequency noise in the graph topology, ensuring stable feature propagation. A dual-branch graph contrastive learning module is developed, where Gaussian noise perturbations generate augmented views through two multilayer perceptrons (MLPs), and a cross-view contrastive loss enforces structural consistency between views to learn noise-invariant representations. Finally, latent embeddings optimized by this process are clustered via K-means. Extensive experiments and repeated comparative analysis have verified that our SLCGC contains high clustering accuracy, low computational complexity, and strong robustness. The code source will be available at https://github.com/DY-HYX.},
  archive      = {J_TMM},
  author       = {Yao Ding and Zhili Zhang and Aitao Yang and Yaoming Cai and Xiongwu Xiao and Danfeng Hong and Junsong Yuan},
  doi          = {10.1109/TMM.2025.3604954},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional quality assessment for text-to-3D assets: Dataset and model. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively. Specifically, the proposed method utilizes the projection videos of text-to-3D assets to extract 3D shape, texture and text-asset correspondence features, then fuses them to calculate the final three preference scores respectively. Extensive experimental results demonstrate the effectiveness of the proposed T23DAQA method in evaluating the quality of AI generated 3D asset, which is more consistent with human perception. To the best of our knowledge, this is the first work that studies the problem of text-guided 3D generation quality assessment, and our database and codes will be released to facilitate future research.},
  archive      = {J_TMM},
  author       = {Kang Fu and Huiyu Duan and Zicheng Zhang and Xiaohong Liu and Xiongkuo Min and Jia Wang and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3604905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-dimensional quality assessment for text-to-3D assets: Dataset and model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of ‘Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in $2.55\% / 5.7\%$ under different settings, compared with the existing methods.},
  archive      = {J_TMM},
  author       = {Ming-Hui Liu and Xiao-Qian Liu and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TMM.2025.3604932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a weakly supervised referring image segmentation method, named CollabLearn, that segments objects described by free-form referring expression utilizing solely image-text pairs. Existing methods suffer from incorrect localization of referring expressions due to the lack of high-level semantics in cross-modal alignment or rough segmentation of referenced objects stemming from the absence of low-level details. To address these issues, we propose an innovative framework for generating cross-modal features encompassing both high-level semantics and low-level details via two fusion modules: a semantic awareness module and a detail cognition module. Each of these modules generates an activation map, and they mutually correct each other through a collaborative learning strategy. Specifically, the semantic awareness module performs in-depth cross-modal interaction and achieves accurate localization in a top-down manner. The detail cognition module facilitates the segmentation of entire objects in a bottom-up manner. A collaborative learning strategy is designed to enable interaction between these two modules, enforcing sufficient vision-language alignment. Experiments on three benchmarks demonstrate that CollabLearn consistently outperforms state-of-the-art weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Chao Jiang and Yuqiu Kong and Mengnan Zhao and Lihe Zhang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3604944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced head: Exploring strong detection heads with vision transformer. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial component of object detectors, current detection heads often lack the capability to effectively utilize contextual information, adapt to deformable objects, and align features and tasks. However, most existing methods prioritize a single capability, lacking comprehensive approaches to introduce them simultaneously. In this paper, we propose the Enhanced Head to integrate the above three capabilities into the detectors concurrently. Specifically, we propose three attention blocks with linear complexity: Global Concentrated Attention (GCA), Local Deformable Cross-Task Attention (LDCA), and Boundary-Aware Cross-Task Attention (BACA). The GCA captures long-range dependencies efficiently by employing Spatial Information Concentration (SIC). The LDCA improves feature alignment and deformation adaptability by enabling local deformable cross-task feature interactions. The BACA aligns classification features with localization results, enhancing task alignment and further improving deformation adaptability through a region-deformable interaction scheme. We implement Enhanced Head as a plug-and-play detection head and evaluate its effectiveness through extensive experiments on the MS COCO and VisDrone datasets. For instance, on the COCO detection benchmark, our Enhanced Head achieves +3.6 AP gain for FSAF, +3.3 AP for RetinaNet, and +2.9 AP for ATSS while reducing the FLOPs.},
  archive      = {J_TMM},
  author       = {Zewen Du and Zhenjiang Hu and Guiyu Zhao and Ying Jin and Hongbin Ma},
  doi          = {10.1109/TMM.2025.3604917},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced head: Exploring strong detection heads with vision transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual high-order total variation model for underwater image restoration. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are typically characterized by color cast, haze, blurring, and uneven illumination due to the selective absorption and scattering when light propagates through the water, which limits their practical applications. Underwater image enhancement and restoration (UIER) is one crucial mode to improve the visual quality of underwater images. However, most existing UIER methods concentrate on enhancing contrast and dehazing, rarely pay attention to the local illumination differences within the image caused by illumination variations, thus introducing some undesirable artifacts and unnatural color. To address this issue, an effective variational framework is proposed based on an extended underwater image formation model (UIFM). Technically, dual high-order regularizations are successfully integrated into the variational model to acquire smoothed local ambient illuminance and structure-revealed reflectance in a unified manner. In our proposed framework, the weight factors-based color compensation is combined with the color balance to compensate for the attenuated color channels and remove the color cast. In particular, the local ambient illuminance with strong robustness is acquired by performing the local patch brightest pixel estimation and an improved gamma correction. Additionally, we design an iterative optimization algorithm relying on the alternating direction method of multipliers (ADMM) to accelerate the solution of the proposed variational model. Considerable experiments on three real-world underwater image datasets demonstrate that the proposed method outperforms several state-of-the-art methods with regard to visual quality and quantitative assessments. In the quantitative assessments, the proposed method achieves average scores of 0.205 FADE, 7.688 Entropy, 0.628 UCIQE, and 0.775 FDUM across the UIEB and UIQS datasets. Moreover, the proposed method can also be extended to outdoor image dehazing and low-light image enhancement tasks. The code is available at https://github.com/HouGuojia/UDHTV.},
  archive      = {J_TMM},
  author       = {Yuemei Li and Guojia Hou and Peixian Zhuang and Zhenkuan Pan},
  doi          = {10.1109/TMM.2025.3604900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual high-order total variation model for underwater image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FontGuard: A robust font watermarking approach leveraging deep font knowledge. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard.},
  archive      = {J_TMM},
  author       = {Kahim Wong and Jicheng Zhou and Kemou Li and Yain-Whar Si and Xiaowei Wu and Jiantao Zhou},
  doi          = {10.1109/TMM.2025.3604908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FontGuard: A robust font watermarking approach leveraging deep font knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the behavior of contrastive regularization in improving chinese text recognizer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense representation space in Chinese scene text recognition (STR) makes discriminating between categories highly challenging, because of the large candidate category set. Mainstream STR methods have achieved remarkable advancements by leveraging linguistic knowledge to implicitly address this challenge. In this paper, inspired by the correlation between recognizer performance and the distributional properties of character representations, as well as the inherent consistency between this correlation and supervised contrastive learning (SupCon), we thoroughly investigate how to integrate SupCon with an STR model to alleviate this challenge, and elucidate some dynamic behaviors underlying the performance improvements. Specifically, we analyze the SupCon-STR models instantiated with different projectors and evaluate their distributional properties through metrics, including intra-class compactness, inter-class separability, and feature redundancy, while assessing performances that involve in-domain accuracy and cross-domain recognition generalization. The main results reveal how the temperature $\tau$ and projectors affect the representation distribution, and highlight that suitable intra-class compactness and sufficient inter-class separability are key factors for delivering competitive performances in both in-domain and cross-domain STR scenarios. Moreover, these results also provide valuable insights into the design of SupCon-STR architectures for diverse resource constraints. Taking existing Chinese STR models as baselines, and combining SupCon-STR with them, the average improvements in cross-domain recognition performance are over 5% across 7 testing datasets. A new state-of-the-art accuracy of 77.19% on the Chinese Scene benchmark is also established.},
  archive      = {J_TMM},
  author       = {Dekang Liu and Tianlei Wang and Huanqiang Zeng and Jiuwen Cao},
  doi          = {10.1109/TMM.2025.3604892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {On the behavior of contrastive regularization in improving chinese text recognizer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep frequency-separable temporal network for efficient video denoising. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively explore inter-frame information is critical for video denoising. Existing methods often rely on complex architectures, such as optical flow estimation and cross-frame self-attention, which introduce high computational costs and limit their practicality in real-world scenarios. To address this limitation, we propose a simple yet efficient deep Frequency-Separable Temporal Network (FSTN) for video denoising. FSTN utilizes the multi-scale analysis capability of wavelet transform to extract high-frequency and low-frequency information at the feature level, enabling faster processing while maintaining high-quality reconstruction. To further reduce computational complexity and enhance detail preservation, we develop a learnable high-frequency processing module that adaptively filters noise and recovers edge details. Additionally, to effectively utilize information from long-range frames, we propose a low-frequency propagation method equipped with a temporal feature alignment module. This method enables the efficient transfer of structural information from distant frames, ensuring temporal consistency and enhancing denoising performance. Extensive experiments demonstrate that our method has 1.28× fewer network parameters than state-of-the-art efficient video denoising methods, such as BasicVSR++, and requires less computational cost while achieving comparable performance.},
  archive      = {J_TMM},
  author       = {Zhulin Tao and Jinjuan Wang and Lifang Yang and Jinshan Pan and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3604914},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep frequency-separable temporal network for efficient video denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BASNet: Boundary assisted network for image splicing forgery detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing is a common technique used in image forgery. With the rapid development of digital image processing technology, detecting image splicing forgery has become increasingly challenging. Existing splicing forgery localization methods lack exploration in effectively utilizing tampered region boundary information. To address this issue, we propose a novel model for detecting image splicing forgery called boundary-assisted network (BASNet). We introduce a boundary-motivated module (BMM) to explore valuable and additional boundary features related to tampered regions, enhancing representation learning for detecting tampered regions. Additionally, we present a boundary-enhanced module (BEM) to enhance boundary information using the cross-channel attention mechanism. To efficiently merge features from various levels and boundary features, we further present the feature fusion module (FFM). To optimize performance, the BASNet incorporates weighted binary cross-entropy loss, dice loss, and boundary loss, which can effectively leverage edge supervision while mitigating imbalance between positive and negative samples. Evaluation of five widely-used forgery detection datasets demonstrates the state-of-the-art performance of the BASNet. Robustness experiments verify that the BASNet is robust enough to detect image splicing forgery across various common attacks.},
  archive      = {J_TMM},
  author       = {Enji Liang and Kuiyuan Zhang and Zhongyun Hua and Xiaohua Jia},
  doi          = {10.1109/TMM.2025.3604911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BASNet: Boundary assisted network for image splicing forgery detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy protection based on hopfield cross neural network in WBANs for medical images. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable medical data collection and surveillance devices provides real-time guarantees for the whole process of a patient's medical treatment, especially medical image data plays a key role. However, existing medical images face data leakage, pollution and vulnerability to attacks during transmission over wireless body area networks(WBANs). To address these issues, a privacy protection algorithm based on Hopfield cross neural network (HCNN) for medical data is proposed. Specifically, the HCNN model is first constructed and its dynamic behavior is analyzed, which is suitable for application to image encryption. Then, a confusion method of NZ fractal curve sorting matrix (NZ-FCSM) is designed to achieve good encryption effect. Subsequently, the secret image sharing (SIS) technique based on sharing matrix is introduced to enhance the algorithm robustness. Finally, an alignment embedding of double diamond prediction (AEDDP) method is proposed to implement lossless hiding of private information. The present issues in medical image protection include ensuring the security and effectiveness of encryption algorithms while maintaining the robustness and concealment of ciphertext data, and balancing the need for preservation with the limited resources of complex work environment. Experimental results show that the proposed algorithm achieves PSNR of 53 dB for the cipher image, more than 36 dB for the reconstructed image, and the information entropy of the secret image is over 7.99, and displays good robustness. These findings highlight the validity of the algorithm in medical image data privacy preserving applications that ensure confidentiality and extend to practical applications of concealed transmission of confidential information and secure multi-party transactions.},
  archive      = {J_TMM},
  author       = {Xiuli Chai and Guoqiang Long and Yakun Ma and Changbo Li and Zhihua Gan and Yushu Zhang},
  doi          = {10.1109/TMM.2025.3604898},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy protection based on hopfield cross neural network in WBANs for medical images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification via hölder divergence for multi-view representation learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the “distance” between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpasses the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.},
  archive      = {J_TMM},
  author       = {Yan Zhang and Ming Li and Chun Li and Zhaoxia Liu and Ye Zhang and F. Yu},
  doi          = {10.1109/TMM.2025.3604966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty quantification via hölder divergence for multi-view representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral pedestrian detection has been shown to be effective in improving performance in complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To efficiently compress multispectral object detection networks, we propose a novel distillation method, the Adaptive Modal Fusion Distillation (AMFD) framework. Unlike traditional distillation methods, the AMFD framework fully leverages the original modal features from the teacher network, thereby significantly enhancing the performance of the student network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP, SUNRGB-D and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.},
  archive      = {J_TMM},
  author       = {Zizhao Chen and Yeqiang Qian and Xiaoxiao Yang and Chunxiang Wang and Ming Yang},
  doi          = {10.1109/TMM.2025.3604937},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic frame aggregation-based transformer for live video comment generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments that align with viewer interests. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with cross-attention mechanism to attend to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large-scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.},
  archive      = {J_TMM},
  author       = {Anam Fatima and Yi Yu and Janak Kapuriya and Julien Lalanne and Jainendra Shukla},
  doi          = {10.1109/TMM.2025.3604921},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic frame aggregation-based transformer for live video comment generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of repetitive actions in videos, a task commonly referred to as Video Action Counting (VAC), is a critical challenge in understanding and analyzing content in sports, fitness, and daily activities. Traditional approaches to VAC have largely overlooked the nuanced irregularities inherent in action repetitions, such as interruptions and variable lengths between cycles. Addressing this gap, our study introduces a novel perspective on VAC, focusing on Irregular Video Action Counting (IVAC), which emphasizes the importance of modeling the irregular repetition priors present in video content. We conceptualize these priors through two key aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures that the spatiotemporal representations across all cycle segments in a video remain homogeneous, thereby reflecting the uniformity of actions between different cycle segments. In contrast, Cycle-interval Inconsistency mandates a clear semantic distinction between the representations of cycle segments and intervals, acknowledging the inherent dissimilarities in content. To effectively encapsulate these priors, we introduce a novel methodology consisting of consistency and inconsistency modules, underpinned by a tailored pull-push loss ($\mathrm {P^{2}~L}$) mechanism. This approach employs a pull loss to enhance the cohesion among cycle segment features and a push loss to distinctly differentiate between cycle and interval segment features. Empirical evaluations on the RepCount dataset illustrate that our IVAC-$\mathrm {P^{2}~L}$ model sets a new benchmark in state-of-the-art performance for the VAC task. Moreover, our model demonstrates adaptability and generalization across diverse video content, achieving superior performance on two additional datasets, UCFRep and Countix, without necessitating dataset-specific fine-tuning. These findings not only validate the effectiveness of our approach in addressing the complexities of irregular repetitions in videos but also open new avenues for future research in video understanding and analysis.},
  archive      = {J_TMM},
  author       = {Hang Wang and Zhi-Qi Cheng and Youtian Du and Lei Zhang},
  doi          = {10.1109/TMM.2025.3604935},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception-distortion- tradeoff reveals the limitation of current low-level deep learning paradigms, i.e., minimizing reconstruction distortion does not guarantee improved perceptual quality. Acknowledging the lack of a reliable perception-oriented optimization function, we are motivated to explore a flexible approach for enhancing perceptual quality by steering the tradeoff to prioritize perception. To this end, we reconsider the perception-distortion function by incorporating the Just-Noticeable-Distortion (JND) mechanism. We mathematically demonstrate that in the common image restoration process, altering the optimization target from natural images to distorted images—where the distortion intensity is constrained by the JND threshold and the distortion type aligns with that arising from the restorer itself—effectively obtained improved perception indices without any changes to the restorer or optimization function. Accordingly, to facilitate various low-level learning models, we are motivated to construct the first large-scale CNN-oriented JND image dataset. Our dataset comprises 500 natural images and 4,500 degraded versions generated by a series of autoencoders, as well as the actual JND judgment results collected through rigorous subjective testing from twenty volunteers. Finally, a learning-based JND inference model is established on the proposed dataset and employed in the proposed JND-based adaptation scheme, where the inferred JND images serve as pseudo-ground truth for the training or fine-tuning processes of low-level vision models. Extensive experiments on image super-resolution and end-to-end image compression across multiple models have shown encouraging improvements in perceptual quality, demonstrating the effectiveness of the proposed scheme. Our dataset is available at: https://github.com/ohq17/CNN-Oriented-JND-Dataset.},
  archive      = {J_TMM},
  author       = {Xuelin Shen and Haoqiao Ou and Zhangkai Ni and Wenhan Yang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TMM.2025.3604973},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RandomViG: Random vision graph neural network for image classification. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Graph Neural Network (ViG) is the first graph neural network model capable of directly processing image data. The community primarily focuses on the model structures to improve ViG's performance but lacks attention to its graph construction method. To avoid quadratic computational complexity, ViG uses clustering algorithms (K-nearest neighbor) to construct graph structures. Nevertheless, clustering algorithms introduce biases, which limit ViG's ability to obtain global information. To address this problem, we propose RandomViG, which abandons clustering algorithms and uses a random manner to obtain relationships between nodes. Our RandomViG is sparse in computation and can approximate a complete graph, enabling ViG to gain global interaction capability. In order to obtain the local dependence, we design a local feature extraction module for RandomViG. In addition, to alleviate the over-smoothing problem, we propose a novel method called MRN (maintaining relationships among nodes). Considering that the increased feature diversity does not necessarily lead to better performance, MRN does not aim to maximize the feature diversity of the model but instead strives to maintain consistency between the feature similarity and the inherent similarity of the original image. We validate our proposal in three major computer visual tasks, including image classification, object detection, and instance segmentation. Without extra data, RandomViG-Ti achieves 79.4% ImageNet-1 K top-1 accuracy, outperforming the baseline (ViG) by 1.2%. Under the same model scale, our RandomViG performs better with fewer FLOPs compared with existing state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Xun Gong and Daisong Yan and Zhemin Zhang},
  doi          = {10.1109/TMM.2025.3604948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RandomViG: Random vision graph neural network for image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLE: Out-of-distribution detection with shallow layer-driven enhancement. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3604940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution detection aims to protect models against overconfidently categorizing samples from unknown categories, i.e., out-of-distribution data (OOD), into known categories, i.e., in-distribution data (ID). From the perspective of feature distribution, the difference between OOD samples and ID samples can be decomposed into semantic shifts and covariate shifts. Most DL-based methods only extract deeper features, which represent semantic shifts, to discern feature variances in the data, ignoring the exploration of covariance shifts. In this paper, we propose a Shallow Layer-driven Enhanced OOD detection method (SLE), which enhances the difference of OOD samples by exploiting covariate shifts in shallow features. Specifically, it contains three main components: Hierarchical Feature Extractor (HFE), Adaptive Dimensionality Reduction Strategy (ADR), Cross-layer Score Aggregator (CSA). HFE is responsible for extracting both deeper and shallow features from the deep network. ADR adaptively reduces all hierarchical feature dimensionality according to sample characteristics, avoiding feature redundancy. CSA defines a novel confidence score for OOD samples, that effectively prevents confusion in the feature representation space at each layer. In SLE, these three closely related components cooperate with each other to effectively enhance the representation ability of OOD samples and divide OOD data better. We conduct extensive experiments to examine the performance of SLE in four benchmarks and discuss its individual components. This method performs well on the OOD datasets.},
  archive      = {J_TMM},
  author       = {Zhenni Yang and Chengxu Liu and Xueming Qian},
  doi          = {10.1109/TMM.2025.3604940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLE: Out-of-distribution detection with shallow layer-driven enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active cross-modal domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most cross-modal methods assume that training and testing data come from the same domain, which is often not the case in real-world scenarios due to cross-modal domain shifts and potential unknown concepts. Moreover, cross-modal shifts hinder the capture of unknown concepts, and the presence of unknown concepts can in turn exacerbate the cross-modal shifts. To address these challenges, this paper proposes a new paradigm called Active Cross-Modal Domain Adaptation (ACM-DA), wherein only cross-modal data from the source domain and uni-modal data from the target domain are utilized. To concurrently mitigate the adverse effects of both cross-modal domain shifts and unknown concepts, we propose a Curiosity-Driven Active Adaptation Network (CD-A2N), selectively annotating samples to maximize performance gain. First, we present Curiosity Arousal within Cross-modal Domain Adaptation (CA-CDA) to explore the complexity and novelty characteristics of target samples, while reducing cross-modal discrepancy and aligning source and target domains. Second, Curiosity-driven Active Learning (CAL) is devised to strategically select a subset of target samples for annotation, aiming to achieve more valuable data selection at a small labeling cost. Finally, we jointly train CA-CDA and CAL with the newly labeled target domain sub-dataset to alleviate the above issues. Extensive experiments demonstrate that CD-A2N provides an effective solution for achieving ACM-DA. Code will be available at https://github.com/Feliciaxyao/ACM-DA.},
  archive      = {J_TMM},
  author       = {Xuan Yao and Xiao Peng and Junyu Gao and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604968},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Active cross-modal domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards multimodal emotional support conversation systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support. This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities. This dataset captures the intricate interplay of user emotions, system strategies, system emotions, and system responses, setting a new precedent in the field. Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. This work not only pushes the boundaries of AI's role in mental health care but also establishes a foundation for developing conversational agents that can provide more empathetic and effective emotional support.},
  archive      = {J_TMM},
  author       = {Yuqi Chu and Lizi Liao and Zhiyuan Zhou and Chong-Wah Ngo and Richang Hong},
  doi          = {10.1109/TMM.2025.3604951},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards multimodal emotional support conversation systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HARG: Hierarchical adaptive reasoning graph for activity parsing. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a video understanding task, activity parsing aims at encompassing actions into multiple levels of activity components, including activity, sub-activity and atomic action, enabling understanding of complex video scenes within multimedia systems. Existing methods form activity parsing as a multi-task learning problem to predict multi-granular activity labels simultaneously, which ignores modeling the hierarchical structure and the fine-grained transitions of activity components at different levels. In this paper, we propose a Hierarchical Adaptive Reasoning Graph (HARG) to model the hierarchical structure (i.e., object level $\rightarrow$ atomic action level $\rightarrow$ activity level) dynamically and precisely. To achieve that, an object reasoning graph (ORG) and an atomic action reasoning graph (ARG) are designed to reason fine-grained information transitions between multiple actors at different levels. In addition, an adaptive segmentation module (ASM) is investigated for bridging the gap among different levels, permitting step-by-step reasoning from the object level to the atomic action level. Experimental results show our method outperforms state-of-the-art methods on two activity parsing datasets, achieving hierarchical modeling and fine-grained reasoning for activity understanding. The code is available on GitHub: https://github.com/whuoyj/HARG.},
  archive      = {J_TMM},
  author       = {Yangjun Ou and Li Mi and Zhenzhong Chen},
  doi          = {10.1109/TMM.2025.3604927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HARG: Hierarchical adaptive reasoning graph for activity parsing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware wavelet transformer for pyramid learning object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer displays the impressive capabilities on vision tasks. The built-in self-attention retains the quadratic computation burden in respect of the spatial resolution of image features. The traditional downsampling (e.g., average pooling) can reduce the resolution. Nonetheless, it may suffer from the dropping of detailed information. In this work, we propose an Efficient Wavelet Attention (EWA), which injects the wavelet transform and a Mean GELU (MGELU) function. Firstly, the wavelet transform enables the detailed information to participate in the efficient interaction modeling. Secondly, MGELU regards the statistical mean as reference and loosely passes the high relative responses. Building upon EWA, we present an effective Semantic-aware Wavelet Transformer (SWFormer), which is then employed for pyramid learning, including CNN feature hierarchy or Region of Interest (RoI) features. For the feature hierarchy, a Pyramid SWFormer (PSWFormer) incorporates SWFormer at each level to fit the bidirectional features. For RoIs, a Recognition-Localization SWFormer (RLSWFormer) is inserted into the head to fit their features from all levels. The effectiveness of our SWFormer is displayed experimentally on the MS COCO detection dataset and the Pascal VOC dataset. When exploiting Swin-small backbone, our SWFormer-based method acquires AP of 52.1 in the single-scale evaluation on the COCO test-dev set. This work will have the codes at https://github.com/TimeIsFuture/Dt2_SWFormer.},
  archive      = {J_TMM},
  author       = {Yang Li and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Puhua Chen},
  doi          = {10.1109/TMM.2025.3604963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-aware wavelet transformer for pyramid learning object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-contact respiratory monitoring, reducing motion artifact and selecting the appropriate Region of Interest (ROI) pose significant challenges. Most motion artifact removal methods rely on signal periodicity assumptions, while respiratory signals usually are non-periodic in real-world scenarios. Existing automated ROI selection approaches are mostly primarily impacted by the texture of clothing, absence of chest landmarks, and obstruction of face. To improve the quality of respiratory signals, in this study, we propose a framework for automatic respiratory ROI selection based on video, namely, Optimizing Video-based Respiration Monitoring (OVRM), which consists of peak-trough adaptive motion artifact removal and characteristic-driven adaptive ROI selection. This motion artifact removal strategy removes motion artifacts by using a dynamic ratio-based judgment mechanism, and reconstructs signals using sinusoidal interpolation. The adaptive ROI method scores signals based on periodicity, similarity, smoothness, and energy, selecting the highest-scoring blocks as the ROIs to match respiratory signals efficiently. Experimental results, validated across four datasets, demonstrate that OVRM effectively reduces signal noise caused by subject movement and outperforms state-of-the-art non-contact respiratory monitoring algorithms. The dataset and code are publicly available at: https://github.com/zxx5058/OVRM.},
  archive      = {J_TMM},
  author       = {Xinxin Zhang and Xudong Tan and Yan Zhu and Mei Zhou and Menghan Hu and Zhanzhan Cheng and Nengfeng Qian and Changyin Wu and Guangtao Zhai and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3604970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action-responsive contrastive network for fine-grained skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, fine-grained skeleton action recognition based on graph convolutional networks (GCNs) has become an important research focus. Fine-grained action recognition refers to the accurate recognition of subtle, complex or detailed actions. This task is particularly challenging due to the limited appearance information in skeleton data and the limitations of predefined single-topology skeleton structures. To address these challenges, we propose an action-responsive contrastive network (ARCN). The network consists of two main components: an action-responsive graph convolutional network (ARGCN) with enhanced skeleton topology and a fine-grained action comparator (FAC) that uses feature contrastive learning to explore the latent space of motion features. The ARGCN contains two specialized modules: the action-responsive topology (ART) module, which captures important motion features through the learned action-specific topology structure matrix and multiscale temporal features; and the action-responsive attention (ARA) module, which learns complex spatiotemporal skeleton attention information. These modules jointly generate a multichannel cross-temporal dynamic skeleton joint attention topology map tailored for the specific action being analysed. To further clarify the fine-grained action feature differences, the FAC is integrated in some stages of the ARGCN. The FAC performs spatiotemporal decoupling of feature maps, classifies and contrasts similar and different fine-grained motion features, and builds a learnable latent space for fine-grained motion, thereby improving classification performance. Our model is evaluated on six public datasets: NTU RGB+D, NTU RGB+D 120, NW-UCLA, UAV-Human, Finegym, and Diving48. It achieves 91.2% accuracy on the NTU RGB+D 120 dataset X-Set, 97.2% accuracy on the NW-UCLA dataset, 44.6% accuracy on the UAV-Human dataset CSv1, 72.0% accuracy on the UAV-Human dataset CSv2, 95.3% accuracy on the Finegym dataset, and 54.3% accuracy on the Diving48 dataset, which are competitive results compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hongjun Li and Tian Bai},
  doi          = {10.1109/TMM.2025.3604906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Action-responsive contrastive network for fine-grained skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed AutoRegressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed AutoRegressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can achieve over a magnitude acceleration in decoding time without industrial level optimization or achieve state-of-the-art reconstruction quality compared with other INR codecs. To the best of our knowledge, our method is the first INR-based codec comparable with Ballé et al. [1] in both decoding speed and quality while maintaining low complexity.},
  archive      = {J_TMM},
  author       = {Xiang Liu and Jiahong Chen and Bin Chen and Zimo Liu and Baoyi An and Shu-Tao Xia and Zhi Wang},
  doi          = {10.1109/TMM.2025.3604982},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To counter the security threats posed by the realism of image inpainting generated through diffusion models and GANs, in this paper, we propose a texture enhancement and progressive refinement network (TEPR-Net) for image inpainting localization (IIL). The IIL task is divided into two phases: coarse and fine locating. In the coarse locating phase, we utilize an anomaly texture encoder to capture tampering traces in textures, employ a texture–context feature interaction strategy to effectively integrate texture features with contextual features, and utilize a pixel-level contrastive learning strategy to enhance feature clustering and model generalization. In the fine locating phase, we first enhance the receptive field features in the frequency domain by transforming the features and separately enhancing the low- and high-frequency components. Then, we utilize the coarse localization result to augment the model's sensitivity to tampered regions. Additionally, we introduce a progressive edge distribution guidance and reconstruction strategy that progressively refines the edges of the tampered regions at each level, ultimately generating refined localization results. To support the research and evaluation of the IIL task, we create the Inpaint32K dataset, which is characterized by its large scale, diversity, comprehensiveness, high quality, and authenticity. Finally, extensive experiments demonstrate that TEPR-Net has significant advantages in terms of localization performance, generalizability, extensibility, and robustness.},
  archive      = {J_TMM},
  author       = {Qixian Hao and Kai Wang and Haoliang Cui and Jiwei Zhang and Shaozhang Niu},
  doi          = {10.1109/TMM.2025.3604965},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and robust 3D panoptic segmentation is crucial for scene perception in autonomous driving. Modern methods widely adopt multi-modal fusion based simple feature concatenation to enhance 3D scene understanding, resulting in generated multi-modal representations typically lack comprehensive semantic and geometry information. These methods focused on panoptic prediction in a single step also limit the capability to progressively refine panoptic predictions under varying noise levels, which is essential for enhancing model robustness. To address these limitations, we first utilize BEV space to unify semantic-geometry perceptual representation, allowing for a more effective integration of LiDAR and camera data. Then, we propose PrimePSegter, a progressively combined diffusion 3D panoptic segmentation model that is conditioned on BEV maps to iteratively refine predictions by denoising samples generated from Gaussian distribution. PrimePSegter adopts a conditional encoder-decoder architecture for fine-grained panoptic predictions. Specifically, a multi-modal conditional encoder is equipped with BEV fusion network to integrate semantic and geometric information from LiDAR and camera streams into unified BEV space. Additionally, a diffusion transformer decoder operates on multi-modal BEV features with varying noise levels to guide the training of diffusion model, refining the BEV panoptic representations enriched with semantics and geometry in a progressive way. PrimePSegter achieves state-of-the-art performance on the nuScenes and competitive results on the SemanticKITTI, respectively. Moreover, PrimePSegter demonstrates superior robustness towards various scenarios, outperforming leading methods.},
  archive      = {J_TMM},
  author       = {Hongqi Yu and Sixian Chan and Xiaolong Zhou and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3604903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided adversarial attack in the low-frequency space. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3604964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples can assess the robustness of machine learning models, which has attracted the attention of many researchers to adversarial example generation methods. Transferability and imperceptibility stand out as two crucial metrics for evaluating the quality of adversarial examples. However, achieving a balance between these two indicators poses a formidable challenge. In this paper, we propose a low-frequency guided adversarial attack method (LGA) to generate adversarial examples with strong transferability and good imperceptibility. Specifically, we enhance the transferability of adversarial examples by increasing the diversity of attack algorithms, and introduce the guiding principle and the triplet loss constraint to ensure that the generated adversarial examples are optimized away from the class regions of the clean examples. We find that the low-frequency component in the frequency domain of the image contains the vast majority of the semantic information of the image. Therefore, we constrain the attack perturbations to low-frequency component space to enhance the covert nature while maintaining visual coherence, rendering the adversarial examples more difficult to perceive. We conduct extensive experiments on various models with different network structures and multiple defense strategies, and the experimental results demonstrate that our method outperforms existing methods in the tradeoff between transferability and imperceptibility, achieving the SOTA performance.},
  archive      = {J_TMM},
  author       = {Jiang Zhu and Lingping Tan and Yanchun Li and Shujuan Tian and Jianqi Li and Yaonan Wang},
  doi          = {10.1109/TMM.2025.3604964},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guided adversarial attack in the low-frequency space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view clustering via mutual information. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering focus on mining useful information from low-quality multiple sources, such as missing and distorted data that are prevalent in real life. However, after representation learning and the processing of incomplete information, existing methods often leave representations containing information task-irrelevant information. In addition, the separation between missing data imputation and clustering tasks leads to sub-optimal multi-view clustering performance. To address these issues, we propose an incomplete multi-view clustering method based on mutual information. For the problem of task-irrelevant information, we use incomplete view prediction to extract sufficient and minimal task-relevant information and provide theoretical proof from the perspective of mutual information. For the problem of separation between missing data imputation and clustering tasks, we integrate incomplete-view prediction with contrastive clustering, collaboratively enhancing the clustering performance. Comparative experiments on five public datasets, under both complete and incomplete scenarios, reveal that our method outperforms nine other competing approaches, demonstrating its effectiveness and robustness in handling multi-view data.},
  archive      = {J_TMM},
  author       = {Xuejiao Yu and Guoqing Chao and Yi Jiang and Guanzhou Ke and Dianhui Chu},
  doi          = {10.1109/TMM.2025.3604942},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incomplete multi-view clustering via mutual information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Like humans to few-shot learning through knowledge permeation of visual and language. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario. With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes. However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge. In this paper, we set out to fill this gap via a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in human intuition: a class name description offers a more general representation, whereas an image captures the specificity of individuals. BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation. On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information. Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop, particularly outperforming previous methods by a substantial margin in the 1-shot setting (improving the accuracy by 7.58% on miniImageNet).},
  archive      = {J_TMM},
  author       = {Yuyu Jia and Qing Zhou and Junyu Gao and Qiang Li and Qi Wang},
  doi          = {10.1109/TMM.2025.3604977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Like humans to few-shot learning through knowledge permeation of visual and language},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCPoint: Contrasting corrupted point clouds for self-supervised representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised Learning (SSL), including mainstream contrastive learning, has achieved significant success in learning visual representations without the need for data annotations in 3D vision. While most contrastive learning methods focus on instance-level information through random affine transformations, they pay limited attention to the intrinsic structures within point clouds. In this work, we propose a novel SSL paradigm for point cloud representation learning, called CCPoint, which incorporates a novel form of data corruption as a negative augmentation strategy. Specifically, we degrade the input point cloud with various corruptions and conduct contrastive learning among the augmented, raw, and corrupted points to learn robust and discriminative representations. To preserve the semantic structure of the point cloud even under heavy degradation, an auxiliary reconstruction decoder is introduced into the corruption branch to provide an additional supervision signal. We explore four families of corruptions—affine, noise, masking, and combined transformations. Different from previous methods that rely on multi-modal data or complex network architectures, CCPoint achieves state-of-the-art performance on three widely used datasets (ModelNet40, ScanObjectNN, and ShapeNetPart) with a lightweight and efficient structure, reaching top linear accuracies of 92.4% and 86.2% on ModelNet40 and ScanObjectNN, respectively.},
  archive      = {J_TMM},
  author       = {Xiaoyang Xiao and Shaoyi Du and Zhiqiang Tian and Meiqin Liu and Xinhu Zheng},
  doi          = {10.1109/TMM.2025.3604890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCPoint: Contrasting corrupted point clouds for self-supervised representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive gradient-guided self-distillation keypoint detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of autonomous driving and 3D reconstruction, keypoint detection, as a key link in visual localization, has become a hot topic in current research. However, existing keypoint detection methods rarely pay attention to the difficulty differences of samples and lack a progressive learning mechanism, which often leads to overfitting for simple samples and underfitting for complex samples, limiting the overall performance of the model. To address these issues, we propose a novel progressive gradient-guided self-distillation method (PG2 SD) for keypoint detection, which possesses self-evolutionary learning capabilities. Specifically, we propose a progressive gradient constraint strategy (PGCS) that dynamically adjusts the gradient contributions of different samples, enabling the model to adapt to the evolving learning capability during training. On this basis, we propose a gradient-guided self-distillation strategy (G2 SDS), which integrates seamlessly with PGCS to alleviate the insufficient feature representation of hard samples in the early training stage. We further design a novel loss function to achieve dynamic collaboration between PGCS and G2 SDS, allowing G2 SDS to adaptively adjust the self-distillation parameters through the PGCS. Experimental results on multiple benchmark datasets show that our method achieves state-of-theart performance on image matching, visual localization, and 3D reconstruction tasks without designing a proprietary network, indicating broad application prospects.},
  archive      = {J_TMM},
  author       = {Zhaoyang Li and Jie Cao and Qun Hao and Haifeng Yao and Yingbo Wang},
  doi          = {10.1109/TMM.2025.3607731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive gradient-guided self-distillation keypoint detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-Based Signature (ABS) provides a critical solution for ensuring data integrity, fine-grained access control, and anonymous authentication in security-sensitive systems such as the Multimedia Internet of Things (MIoT) and multimedia streaming platforms. However, practical adoption of ABS faces three fundamental challenges: vulnerability to key exposure and escrow risks, linear growth of computational cost, and insufficient robustness in multi-authority environments. To address these issues, we propose a forward secure and threshold authorized multi-authority ABS scheme called FORT in this paper. By employing a binary tree structure to divide multiple time periods, historical signatures remain valid even in the event of key exposure. Furthermore, to balance robustness and resistance to corruption while mitigating the key escrow problem, we construct a threshold authorized multi-authority structure based on Lagrange interpolation. This structure effectively reduces the impact of a single authority on the MIoT. Additionally, through the adoption of outsourced computation technology, which offloads complex computations in the signature and verification phases to the edge server, the computational burden for both the signer and verifier is significantly reduced to a small constant. Rigorous security analysis demonstrates that the FORT scheme achieves forward security, collusion attack resistance, corrupt authority resistance and anonymity. Theoretical comparisons and simulation experiments demonstrate the lightweight nature of the FORT scheme in terms of computation and communication.},
  archive      = {J_TMM},
  author       = {Chong Guo and Bei Gong and Zhe Li and Mowei Gong and Haotian Zhu},
  doi          = {10.1109/TMM.2025.3607696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foodfusion: A novel approach for food image composition via diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image composition requires the use of existing dish images and background images to synthesize a natural new image, while diffusion models have made significant advancements in image generation, enabling the construction of end-to-end architectures that yield promising results. However, existing diffusion models face challenges in processing and fusing information from multiple images and lack access to high-quality publicly available datasets, which prevents the application of diffusion models in food image composition. In this paper, we introduce a large-scale, high-quality food image composite dataset, FC22k, which comprises 22,000 foreground, background, and ground truth ternary image pairs. Additionally, we propose a novel food image composition method, Foodfusion, which leverages the capabilities of the pre-trained diffusion models and incorporates a Fusion Module for processing and integrating foreground and background information. This fused information aligns the foreground features with the background structure by merging the global structural information at the cross-attention layer of the denoising UNet. To further enhance the content and structure of the background, we also integrate a Content-Structure Control Module. Extensive experiments demonstrate the effectiveness and scalability of our proposed method.},
  archive      = {J_TMM},
  author       = {Chaohua Shi and Xuan Wang and Si Shi and Xule Wang and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3607683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Foodfusion: A novel approach for food image composition via diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have demonstrated remarkable abilities in image-language reasoning. However, they deal with Video Question Answering (VideoQA) insufficiently, especially for questions demanding causal-temporal reasoning. Typically, they directly concatenate features of uniformly sampled frames as visual inputs for VideoQA. This gives rise to two challenges. For one thing, uniformly sampled frames are discrete and separately distributed across different timestamps, disrupting the coherence of question-critical events or actions. For another, it considers every scene within videos equally and introduces redundant frames that may distract the model from discovering the truth. Towards this, we highlight the importance of identifying continuous frames that are crucial for answering the questions, and propose a lightweight and differentiable Coherence Recognizer (CoRe) to achieve this. Guided by the semantics of questions, CoRe computes scores recording the relevance between each frame and the question, and selects a set of continuous frames with the highest scores for answer prediction. Additionally, CoRe encodes the unselected frames into a short and coarse-grained representation as a completion of the general context. Equipped with CoRe, we can efficiently fine-tune the current MLLMs for VideoQA in an end-to-end manner, without suffering from the problems of incoherence or distraction. Extensive experiments demonstrate that our method achieves substantial improvements on several VideoQA benchmarks.},
  archive      = {J_TMM},
  author       = {Haibo Wang and Chenghang Lai and Weifeng Ge},
  doi          = {10.1109/TMM.2025.3607780},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage causal intervention framework for long-tailed SAR target recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of SAR targets generally conforms to a long-tailed distribution. Due to the existence of sample distribution bias and sample selection bias, training classifiers on this distribution of data often introduces spurious correlations between samples and classes. To address this issue, we propose a two-stage causal intervention framework. The core is that structural causality allows for independent interventions on multiple biases, thereby ensuring high-quality tail class predictions while maintaining unbiased performance for head classes. Firstly, we construct a structural causal graph for the long-tailed recognition task from causal perspective. Based on this graph, the causal paths underlying the two types of biases are identified. Secondly, we design a data augmentation method named DiagPatch-M, which identifies causal features within samples. In this process, these generated patches randomly integrate causal and non-causal features from two different samples, disrupting the original recognition process and effectively eliminating biases induced by sample selection. Thirdly, we design an unbiased structural risk minimization (USRM) optimization strategy, which eliminates the “head preference” of conventional models and the “tail preference” of modified models. This strategy reduces the bias introduced by the model's dependence on the original sample distribution, and achieves stable recognition under different sample distributions. Experimental results on two long-tailed and two balanced datasets demonstrate that the effectiveness of our model surpasses the state-of-the-art (SOTA) methods, indicating the efficacy of our proposed framework in tackling the challenges posed by the long-tailed distribution in SAR target recognition.},
  archive      = {J_TMM},
  author       = {Jiaxiang Liu and Zhunga Liu and Longfei Wang and Zuowei Zhang},
  doi          = {10.1109/TMM.2025.3607804},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A two-stage causal intervention framework for long-tailed SAR target recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTSNet: Dynamic transformer slimming for efficient vision recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have recently adopted increasingly complex structure (e.g., deeper or wider stacked network) to promote the representation learning capabilities of vision recognition. However, progressively deeper or wider stacked network cause the expensive computation cost, which hinders their effective deployment in resource-constrained edge clouds or end devices. In this paper, we propose DTSNet, a dynamic transformer slimming model, which scales vision transformers (ViTs) down across layers from both of the model depth and input width. This is the first time to explore the joint reduction of input tokens and model parameters for ViTs under maintaining performance. Specifically, DTSNet adopts a diversity-enhanced weight sharing module to reduce network parameters, where the weight knowledge of multiple adjacent blocks is effectively integrated into one block. Furthermore, DTSNet designs a unified and massively scalable token pruning mechanism that dynamically discarding less important tokens with a model-driven manner, by introducing a series of discriminant parameters, which is a simple change to the common architecture of vision transformers. Extensive experiments are conducted to verify that DTSNet is able to yield high efficacy in compressing parameter space and accelerating model inference. DTSNet-T/-S/-B on ImageNet achieves 3.0M/11.1M/42.9M parameters and 0.8/2.9/13.7 GFLOPs, where number of parameters are reduced by 48%$\sim$51% and inference speed are improved by 1.3$\times \sim 1.5\times$. Experiments results on semantic segmentation and object detection dataset further demonstrate the potential of DTSNet on complex dense prediction tasks. Code will be available upon publication.},
  archive      = {J_TMM},
  author       = {Wenjing Xiao and Xianzhi Li and Long Hu and Yixue Hao and Min Chen},
  doi          = {10.1109/TMM.2025.3607796},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTSNet: Dynamic transformer slimming for efficient vision recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.},
  archive      = {J_TMM},
  author       = {Jianwu Fang and Lei-Lei Li and Zhedong Zheng and Hongkai Yu and Jianru Xue and Zhengguo Li and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3607808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comp-diff: A unified pruning and distillation framework for compressing diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, generative models such as diffusion models (DMs) have gained prominence in various applications, and there is a growing demand for their deployment on resourceconstrained devices. Model pruning provides an effective solution by reducing the model redundancy without significantly impacting performance. However, most existing model pruning methods are designed for classification models and often lead to substantial performance degradation when applied to generative models. To address this issue, we propose Comp-Diff, a novel two stage framework of pruning and knowledge distillation tailored for diffusion models. In the pruning stage, we propose a new structured content-aware pruning (CaP) method within CompDiff to identify and preserve informative units (filters/channels) that actually contribute to the generative capability of the model. Specifically, we introduce input perturbations to the pre-trained model and measure each unit's importance score using gradients induced by these perturbations. Units with higher importance scores are considered more informative and are retained to maintain the model's generative power. In the fine-tuning stage of Comp-Diff, we propose the distribution-aware knowledge distillation (DaKD) method, which effectively transfers finegrained knowledge from the original model to the pruned one on both attention and noise distribution levels. In addition, DaKD includes an adversarial loss to improve the quality and diversity of generated outputs. To verify and evaluate our method, we apply the proposed Comp-Diff on three representative tasks: unconditional image generation, conditional image generation, and text-to-image generation. Extensive experiments on both multi-step and one-step diffusion models demonstrate that the proposed framework consistently yields compact models and outperforms existing pruning techniques by a large margin.},
  archive      = {J_TMM},
  author       = {Lu Yu and Wei Xiang and Kang Han and Gaowen Liu and Ramana Kompella},
  doi          = {10.1109/TMM.2025.3607799},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comp-diff: A unified pruning and distillation framework for compressing diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online video super-resolution methods utilize implicit memories of previous frames to provide reference information, which have a single memory stream path and are highly dependent on the continuous memory stream. However, video capture in real-world scenes is typically affected by abnormal exposures resulting in sudden changes of lightness thus interrupting the memory stream, while long-term memories suffer from memory vanishing problems during transmission. To address this problem, we propose a novel multi-memory streams based online video super-resolution paradigm that adaptively corrects for abnormal exposures and creates multi-memory streams to accurately converge long-term memories. Specifically, we first propose an exposure detection-correction module, which utilizes optical flow overfitting property and temporal lightness information to detect and correct abnormal exposures to avoid interruption of memory streams. In addition, we propose a dynamic-static decoupled alignment strategy, which can adaptively select the alignment method based on pixel displacement, thus accurately aggregating past long-term memories to create multiple memory streams. Further, we propose an adaptive memory fusion module to mine complementary information between multiple memory streams to solve the memory vanishing problem. Extensive experimental results show that our method outperforms existing video super-resolution methods on complex exposure datasets. We also conduct detailed ablation experiments to analyze and validate our contributions. The implementation code is available at https://github.com/GZ-T/MMVSR.},
  archive      = {J_TMM},
  author       = {Guozhi Tang and Hongwei Ge and Yong Luo and Bo Li and Chunguo Wu},
  doi          = {10.1109/TMM.2025.3607758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clean image may be dangerous: Data poisoning attacks against deep hashing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image retrieval using deep hashing has become increasingly popular due to the exponential growth of image data and the remarkable feature extraction capabilities of deep neural networks (DNNs). However, deep hashing methods are vulnerable to malicious attacks, including adversarial and backdoor attacks. It is worth noting that these attacks typically involve altering the query images, which is not a practical concern in real-world scenarios. In this paper, we point out that even clean query images can be dangerous, inducing malicious target retrieval results, like undesired or illegal images. To the best of our knowledge, we are the first to study data poisoning attacks against deep hashing (PADHASH). Specifically, we first train a surrogate model to simulate the behavior of the target deep hashing model. Then, a strict gradient matching strategy is proposed to generate the poisoned images. Extensive experiments on different models, datasets, hash methods, and hash code lengths demonstrate the effectiveness and generality of our attack method.},
  archive      = {J_TMM},
  author       = {Shuai Li and Jie Zhang and Yuang Qi and Kejiang Chen and Tianwei Zhang and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2025.3607774},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clean image may be dangerous: Data poisoning attacks against deep hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale invertible neural network for wide-range variable-rate learned image compression. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder-based structures have dominated recent learned image compression methods. However, the inherent information loss associated with autoencoders limits their rate-distortion performance at high bit rates and restricts their flexibility of rate adaptation. In this paper, we present a variable-rate image compression model based on invertible transform to overcome these limitations. Specifically, we design a lightweight multi-scale invertible neural network, which bijectively maps the input image into multi-scale latent representations. To improve the compression efficiency, a multi-scale spatial-channel context model with extended gain units is devised to estimate the entropy of the latent representation from high to low levels. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared to existing variable-rate methods, and remains competitive with recent multi-model approaches. Notably, our method is the first learned image compression solution that outperforms VVC across a very wide range of bit rates using a single model, especially at high bit rates.},
  archive      = {J_TMM},
  author       = {Hanyue Tu and Siqi Wu and Li Li and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2025.3607748},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale invertible neural network for wide-range variable-rate learned image compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIFormer+: Rethinking rotation-invariant feature learning in transformer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved remarkable success in the field of computer vision due to their advantage in capturing the global information of images. However, they fail to model the variance of rotation, resulting in significant performance loss in target detection in remote sensing imagery. In this paper, a rotation-invariant transformer plus model, namely RIFormer+ is proposed to enhance the capabilities of transformers in rotation-invariant feature learning at both long-overlooked local-level and the acknowledged global-level. At the local-level, a rotation-invariant cross-patch embedding (RICPE) module is designed to generate dense patches, which handles encoding inconsistency of tokens with similar semantic information before and after rotation. Moreover, response-enhanced attention (REA) is proposed to extract more rotation-robust global features, which highlights overly dispersed responses ensure sustained attention on discriminative regions. Extensive experiments on three datasets demonstrate the effectiveness of RIFormer+. Without bells and whistles, RIFormer+ increases the classification accuracy by an average of 10% and improves the accuracy on rotated datasets by 20% compared with some state-of-the-art transformers. The code of this paper is available at: https://github.com/psychAo/RIFormerPlus.},
  archive      = {J_TMM},
  author       = {Chao Song and Yifan Zhang and Mingyang Ma and Shaohui Mei},
  doi          = {10.1109/TMM.2025.3607728},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RIFormer+: Rethinking rotation-invariant feature learning in transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Unmanned Aerial Vehicles (UAVs) and their unique vantage points present both new opportunities and challenges for person Re-Identification (ReID). Uncertain rotations and scale variations of targets in UAV images, coupled with complex environmental factors, hinder existing methods from extracting robust feature representations. Some methods either make minor modifications to the traditional model architecture or apply simple image rotations but still fail to effectively address the challenges of UAV person ReID. To overcome these limitations, we propose a novel Data Augmentation and Rotation Invariance (DARI) algorithm. First, rotation-invariant convolution is introduced to adaptively extract features, mitigating the uncertainty caused by target rotation. Second, a refined data augmentation correction strategy is employed to reduce noise interference by increasing the richness of global features at different stages. Additionally, considering that multiple features of the same identity should yield consistent recognition result, invariant constraints are designed to enhance the clustering effect. We conducted extensive experiments on both UAV and fixed-camera datasets. The results on PRAI-1581 demonstrate a 5.6% and 6.1% improvement in mAP and Rank-1, respectively, compared to baseline. These findings highlight the model's effectiveness in addressing the challenges of UAV ReID, demonstrating its robustness and superiority. The source code will be released at https://github.com/ZFZ314/DARI.},
  archive      = {J_TMM},
  author       = {Fuzeng Zhang and Eksan Firkat and Hongbing Ma and Jihong Zhu and Bin Zhu and Askar Hamdulla},
  doi          = {10.1109/TMM.2025.3607835},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential of diffusion-generated data on salient object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, salient object detection (SOD) has made significant progress. However, this advancement is often constrained by the requirement for extensive training data and expensive manual annotation. To eliminate the laborious cost of dataset collection and pixel-level annotation, in this work, we employ Stable Diffusion to synthesize data and subsequently automate annotation for the SOD task. Firstly, we design a unified prompt and ChatGPT4 driven diverse prompts, which guide generating images with simple and complex scenes using Stable Diffusion. Secondly, the reliable pseudo-labels of these synthetic images are generated. For simple images, we propose the simple pseudo-label generation (SPLG) strategy which combines SAM segmentation and CLIP classifier, then train the initial SOD model. For complex images, we utilize the inference capability of the initial SOD model to generate pseudo-labels using the complex pseudo-label generation (CPLG) strategy, and employ iterative training to dynamically update the pseudo-labels. Finally, we design a simple yet effective SOD model which combines a feature fusion module (FFM) and an edge enhancement module (EEM), the former is employed to extract saliency via fusing high-level features, and the latter extracts spatial positional information from low-level features to enhance the edges of saliency results. Experiments on five benchmarks show that our method outperforms the unannotated methods, and also demonstrates better or comparable performance than weak annotation based methods. Our code will be published at https://github.com/FangWenRE/PotentialOfSDSOD.},
  archive      = {J_TMM},
  author       = {Xiangquan Liu and Xianlong Luo and Ying Ye and Xiaoming Huang},
  doi          = {10.1109/TMM.2025.3607734},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Potential of diffusion-generated data on salient object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced subspace clustering network with information bottleneck. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data mining, subspace clustering is a crucial technique which determines the union of the underlying subspace to cluster data points in an unsupervised manner. Although deep-learning-based subspace clustering, typically referred to as deep subspace clustering (DSC), has significantly improved clustering accuracy, existing DSC models still struggle to capture a comprehensive and compact latent representation as they generally explore the spatial domain to extract useful information and face difficulty in balancing the high mutual and low redundant information between the original input space and latent subspace. This leads to the performance of the model being dependent on initialization, resulting in a lack of stability. In this study, a novel network is proposed to extract features in both the frequency domain and spatial domain. We introduce three types of ResBlocks in the discrete Fourier transform (DFT), discrete cosine transform (DCT), or discrete wavelet transform (DWT) frequency domains separately to learn both the low-frequency and high-frequency information in the proposed networks. Additionally, to extract concise and rich latent representations, IB loss is employed by deriving a variational lower bound on the IB objective. Extensive experiments on several benchmark datasets verify the effectiveness of our networks compared to state-of-the-art models. In addition, detailed ablation studies are performed to demonstrate the advantages of the two introduced components.},
  archive      = {J_TMM},
  author       = {Mengran Hou and Mengyao Li and Chengli Tan and Junmin Liu and Jinhai Li and Huirong Li},
  doi          = {10.1109/TMM.2025.3607797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-enhanced subspace clustering network with information bottleneck},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local prompts distillation for domain generalization. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has been proven effective for Domain Generalization (DG) by enhancing the generalization capability of visual-language models with fewer learnable tokens. Existing methods adopt mostly inferring global-level individual prompts for the whole dataset to capture domain-invariant knowledge across different domains. However, since domain shifts exist, a single global-level individual prompt is easily overfitted to source domain datasets, thus lacking generalizability to the whole dataset's feature distribution. Moreover, fluctuations in the generalization performance during the training process in DG problems often pose significant challenges to model selection strategies. To address the aforementioned problems, inspired by the Mixture-of-Expert (MOE) and knowledge distillation, we propose a novel Multiple Local Prompts Distillation (MLPD) method to inject the knowledge of multiple local prompts into a unique global prompt, improving both the generalization and discriminative ability. To ensure the diversity of local prompts, we split the whole dataset into several subsets to infer the discriminative local prompts for each subset, which is further applied to generate the generability global prompt. Formally, for each subset, Meta Prompt Tuning (MPT) is proposed to constrain each local prompt to capture both the domain-specific and domain-shared generalization knowledge on the basis of the domain label and meta-learning mechanism. After that, Prompt Knowledge Distillation (PKD) is proposed to distill the knowledge captured in the local-level prompts into the global-level prompt with prompt-level and feature-level knowledge distillations. The final evaluation on multiple benchmarks underscores the effectiveness of the proposed MLPD, e.g., achieving mAPs of 97.3%, 84.8%, 85.2%, 57.3%, and 60.7% on PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, respectively.},
  archive      = {J_TMM},
  author       = {Huaihai Lyu and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607719},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple local prompts distillation for domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuInNet: Dual-modality feature interaction for point cloud completion. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further promote the development of multimodal point cloud completion, we contribute a large-scale multimodal point cloud completion benchmark ModelNet-MPC with richer shape categories and more diverse test data, which contains nearly 400,000 pairs of high-quality point clouds and rendered images of 40 categories. Besides the fully supervised point cloud completion task, two additional tasks including denoising completion and zero-shot learning completion are proposed in ModelNet-MPC, to simulate real-world scenarios and verify the robustness to noise and the transfer ability across categories of current methods. Meanwhile, considering that existing multimodal completion pipelines usually adopt a unidirectional fusion mechanism and ignore the shape prior contained in the image modality, we propose a Dual-Modality Feature Interaction Network (DuInNet) in this paper. DuInNet iteratively interacts features between point clouds and images to learn both geometric and texture characteristics of shapes with the dual feature interactor. To adapt to specific tasks such as fully supervised, denoising, and zero-shot learning point cloud completions, an adaptive point generator is proposed to generate complete point clouds in blocks with different weights for these two modalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC benchmarks demonstrate that DuInNet exhibits superiority, robustness and transfer ability in all completion tasks over state-of-the-art methods. The code and dataset will be available at https://github.com/xinpuliu/DuInNet.},
  archive      = {J_TMM},
  author       = {Xinpu Liu and Baolin Hou and Hanyun Wang and Ke Xu and Jianwei Wan and Yulan Guo},
  doi          = {10.1109/TMM.2025.3607739},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DuInNet: Dual-modality feature interaction for point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module-a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: (i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and (ii) Attribute binding, ensuring that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models like Stable Diffusion and Gligen, markedly enhancing models' performance in addressing these key challenges. We assess our technique on the well-established CompBench and TIFA score benchmarks, and HRS dataset where B2B not only surpasses methods specialized in either attribute binding or layout guidance but also uniquely excels by integrating these capabilities to deliver enhanced overall performance.},
  archive      = {J_TMM},
  author       = {Ashkan Taghipour and Morteza Ghahremani and Mohammed Bennamoun and Aref Miri Rekavandi and Hamid Laga and Farid Boussaid},
  doi          = {10.1109/TMM.2025.3607759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in point cloud compression (PCC) is how to evaluate the perceived distortion so that the codec can be optimized for perceptual quality. Current standard practices in PCC highlight a primary issue: while single-feature metrics are widely used to assess compression distortion, the classic method of searching point-to-point nearest neighbors frequently fails to adequately build precise correspondences between point clouds, resulting in an ineffective capture of human perceptual features. To overcome the related limitations, we propose a novel assessment method called RBFIM, utilizing radial basis function (RBF) interpolation to convert discrete point features into a continuous feature function for the distorted point cloud. By substituting the geometry coordinates of the original point cloud into the feature function, we obtain the bijective sets of point features. This enables an establishment of precise corresponding features between distorted and original point clouds and significantly improves the accuracy of quality assessments. Moreover, this method avoids the complexity caused by bidirectional searches. Extensive experiments on multiple subjective quality datasets of compressed point clouds demonstrate that our RBFIM excels in addressing human perception tasks, thereby providing robust support for PCC optimization efforts.},
  archive      = {J_TMM},
  author       = {Zhang Chen and Shuai Wan and Siyu Ren and Fuzheng Yang and Mengting Yu and Junhui Hou},
  doi          = {10.1109/TMM.2025.3607782},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRFCNet: Feature refinement and flexible concatenation for object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art YOLO detection algorithms still suffer from the issue of redundant extraction of similar features during feature propagation, and the simplistic stacking approach of connecting different features limits the flexibility of feature fusion. We propose a new feature recombination mechanism involving refining feature extraction and flexible concatenation. It includes the HFConv (Hybrid Flexibility Convolution) module, the MFD (Multivariate Flexibility Downsampling) module, and the DFSPP (Deformable and Flexible Spatial Pyramid Pooling) module. Specifically, the HFConv module employs feature refinement and flexible connection strategies to optimize feature representation and reduce redundancy in a dynamic way, acquiring diverse feature information from local and surrounding regions. The MFD module leverages multiple downsampling methods to address the issue of feature redundancy that may arise from a single downsampling method, thereby enhancing feature diversity. The DFSPP module learns an offset corresponding to the pooling kernel size, allowing for the extraction of the most critical information in a dynamic manner. By incorporating these modules into the YOLO architecture, we develop a more robust network called FRFCNet, and the experimental results show a notable 4.1% and 2.8% improvement in AP values on the VOC2012 and COCO2017 datasets, respectively, compared to the baseline (YOLOV7-Tiny-SiLu), outperforming current one-stage detectors.},
  archive      = {J_TMM},
  author       = {Tao Zhang and Zhiheng Wu and Xiangjian He and Qiang Wu},
  doi          = {10.1109/TMM.2025.3607701},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FRFCNet: Feature refinement and flexible concatenation for object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrackletGait: A robust framework for gait recognition in the wild. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8% and 80.4% rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.},
  archive      = {J_TMM},
  author       = {Shaoxiong Zhang and Jinkai Zheng and Shangdong Zhu and Chenggang Yan},
  doi          = {10.1109/TMM.2025.3607705},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TrackletGait: A robust framework for gait recognition in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriPHiT: Privacy-preserving hierarchical training of deep neural networks. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training phase of deep neural networks requires substantial resources and as such is often performed on cloud servers. However, this raises privacy concerns when the training dataset contains sensitive content, e.g., facial or medical images. In this work, we propose a method to perform the training phase of a deep learning model on both an edge device and a cloud server that prevents sensitive content being transmitted to the cloud while retaining the desired information. The proposed privacy-preserving method uses adversarial early exits to suppress the sensitive content at the edge and transmits the task-relevant information to the cloud. This approach incorporates noise addition during the training phase to provide a differential privacy guarantee. We extensively test our method on different facial and medical datasets with diverse attributes using various deep learning architectures, showcasing its outstanding performance. We also demonstrate the effectiveness of privacy preservation through successful defenses against different white-box, deep and GAN-based reconstruction attacks. This approach is designed for resource-constrained edge devices, ensuring minimal memory usage and computational overhead.},
  archive      = {J_TMM},
  author       = {Yamin Sepehri and Pedram Pad and Pascal Frossard and L. Andrea Dunbar},
  doi          = {10.1109/TMM.2025.3607801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PriPHiT: Privacy-preserving hierarchical training of deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screenshot, which is a common tool in office work, has become a significant threat to organizations like companies and research institutions. Malicious users can easily leak sensitive information like business secrets and research data by taking a screenshot and spreading onto the Internet. While existing watermarking schemes serve as useful tools for leakage tracing, they fall short in the scenario of arbitrary screenshot. Most current methods are file-targeted, focusing on embedding watermark for a single file of one type at a time, making it hard to handle arbitrary content on screen. To address the issues above and better satisfy the need of the scenario, we propose ScreenGuard, a novel watermarking scheme targeted for the screen itself to protect arbitrary screen content shown on it. Unlike previous watermarking schemes, ScreenGuard does not modify the content itself. Instead, we generate a transparent mask template based on the watermark, tile it to the size of the screen to form a complete transparent mask, and overlay this mask onto the screen. This ensures that any screenshots taken will contain our watermark. We then train a locator and a decoder to extract watermarks from suspected leaked screenshots to trace leaks to their source. We summarized five properties that needs to be satisfied in the scenario of arbitrary screenshot (Generalizable, Unseeable, Adaptable, Robust, Dynamic) and evaluate our method on these criteria. Extensive experiments demonstrate that ScreenGuard meets these five properties effectively, showcasing its superiority and broad practical applications.},
  archive      = {J_TMM},
  author       = {Gaozhi Liu and Xiujian Liang and Xiaoxiao Hu and Yichao Si and Xinpeng Zhang and Zhenxing Qian},
  doi          = {10.1109/TMM.2025.3607779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical semantic distillation framework for open-vocabulary object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, e.g., CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin.},
  archive      = {J_TMM},
  author       = {Shenghao Fu and Junkai Yan and Qize Yang and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3607729},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hierarchical semantic distillation framework for open-vocabulary object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-attention transformers for class-incremental learning: A tale of two memories. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (Class-IL) aims to continuously learn a model from a sequence of tasks, which suffers from the issue of catastrophic forgetting. Recently, a few transformer based methods are proposed to address this issue by transferring self-attention into task-specific attention. However, these methods utilize shared task-specific attention modules across the whole incremental learning process, and are unable to achieve the balance between consolidation and plasticity, i.e., to remember the knowledge learned from previous tasks and absorb the knowledge from the current task simultaneously. Motivated by the mechanism of LSTM and hippocampus memory, we point out that dual attention on long and short-term memories can handle the consolidation-plasticity dilemma of Class-IL. Typically, we propose Dual-Attention Transformers (DAFormer) to learn external attention and internal attention. The former utilizes sample-dependent keys which exclusively focused on the new tasks, while the latter consolidates the knowledge from previous tasks by using sample-agnostic keys. We present two editions of DAFormer: DAFormer-S and DAFormer-M: the former utilizes shared external keys and maintains a small parameter size, while the latter utilizes multiple external keys and enhances the long-term memory. Furthermore, we propose the $K$-nearest neighbor invariant based distillation scheme, which distills knowledge from previous tasks to current task by maintaining the same neighborhood relationship of each sample over old and new models. Experimental results on CIFAR-100, ImageNet-subset and ImageNet-full demonstrate that DAFormer significantly outperforms all the state-of-the-art parameter-static and parameter-growing methods.},
  archive      = {J_TMM},
  author       = {Shaofan Wang and Weixing Wang and Yanfeng Sun and Zhiyong Wang and Boyue Wang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3607800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-attention transformers for class-incremental learning: A tale of two memories},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-aware dynamic label assignment with high-quality proposals for oriented object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented objects in images are typically embedded in complex backgrounds and exhibit arbitrary orientations. When using oriented bounding boxes (OBBs) to represent these objects, the periodicity of the angles and associated variations in side lengths lead to discontinuities in the angle loss. This paper fundamentally addresses this problem by proposing a trigonometric loss function in the complex plane. Moreover, a conformer RPN head is designed with convolution and multi-head self-attention, which can dynamically capture angular and classification information. The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals. A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for oriented proposal label assignment. This method makes negative sample selection more representative, ensuring consistency between classification and regression features. Experiments were conducted on five realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs. Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, 46.45%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, STAR, and HRSC2016 datasets, respectively.},
  archive      = {J_TMM},
  author       = {Mingkui Feng and Hancheng Yu and Xiaoyu Dang and Ming Zhou},
  doi          = {10.1109/TMM.2025.3607785},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-aware dynamic label assignment with high-quality proposals for oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively representing and transferring user preferences across various domains presents a significant challenge in cross-domain recommendation (CDR). Some approaches utilize graph neural networks that use interaction behavior to establish relationships between entities, providing a comprehensive understanding of user interests. However, the impact of consistent semantics across various types, fields, and perspectives of social media information on user preferences is overlooked, i.e. the multidimensional consistency of user preferences. This oversight results in graph node representations that inadequately reflect user preferences. To address these limitations, we propose a multi-layer transfer learning network (MTLG) for CDR based on graph node representation enhancement via multi-dimensional consistent user preferences. Firstly, the model introduces a set of globally shared semantic units to perform different-grained semantic alignment of multiple media information without clear alignment boundaries, thereby modeling multi-dimensional consistent user preference features. These features are then seamlessly integrated with the initial high-order graph structure embedding features, thus significantly improving the quality of graph node representation. Secondly, the model innovatively designs a multi-layer transfer learning network that hierarchically aligns the domain distribution differences. It calculates the similarity between domains to derive layer weights for more precise transfer learning, thereby mitigating the possibility of information error accumulation resulting from inaccurate feature aggregation processes. We conducted numerous experiments on 3 scenarios, including 7,954,943 rating information from the Amazon dataset. The results indicate that MTLG's recommendation accuracy surpasses those of state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xin Ni and Jie Nie and Niantai Jing and Jianliang Xu and Xiaodong Wang and Xuesong Gao and MingXing Jiang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3607706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth map super-resolution via deep cross-modality and cross-scale guidance. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided depth super-resolution is essential in many applications, which enhances low-resolution (LR) depth maps using high-resolution (HR) RGB images from the same scene. However, the challenge lies in avoiding the texture-copy artifacts issue caused by structural inconsistencies between two modalities. To mitigate, we propose a cross-modality and cross-scale guided depth super-resolution network (D2CNet). We first design a novel two-stage feature integration module to effectively fuse multi-modal RGB and depth while minimizing texture-copy artifacts. That is, a cross-modality fusion stage transfers consistent structures from RGB to depth in a multi-scale manner, and a cross-scale refinement stage mitigates inconsistent structures across modalities. In addition, we design a convolution group as the basic module to well extract high-frequency features and an LR and HR domain projection strategy to enrich features between the fusion and refinement stages. We then develop a new network architecture by progressively repeating the feature integration module and the convolution group, which is flexibly controllable to strike a balance between accuracy and cost for easy implementation in real world. Extensive experiments on multiple benchmarks demonstrate that our D2CNet consistently achieves superior accuracy and generalization ability across sampling scales in both qualitative and quantitative evaluations, when compared to state-of-the-art baselines. The code is at https://github.com/suzdl/d2cnet},
  archive      = {J_TMM},
  author       = {Shuzhe Liu and Delong Suzhang and Meng Yang and Xinhu Zheng and Ce Zhu},
  doi          = {10.1109/TMM.2025.3607763},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Depth map super-resolution via deep cross-modality and cross-scale guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to identify novel compositions formed by known primitives (attributes and objects). Motivated by recent advancements in pre-trained vision-language models such as CLIP, many methods attempt to fine-tune CLIP for CZSL and achieve remarkable performance. However, the existing CLIP-based CZSL methods focus mainly on text prompt tuning, which lacks the flexibility to dynamically adapt both modalities. To solve this issue, an intuitive solution is to additionally introduce visual prompt tuning. This insight is not trivial to achieve because effectively learning prompts for CZSL involves the challenge of entanglement between visual primitives as well as appearance shifts in different compositions. In this paper, we propose a novel Synergetic Prompts as Disentanglement Queries (SPDQ) framework for CZSL. It can disentangle primitive features based on synergetic prompts to jointly alleviate these challenges. Specifically, we first design a low-rank primitive modulator to produce synergetic adaptive attribute and object prompts based on prior knowledge of each instance for model adaptation. Then, we additionally utilize text prefix prompts to construct synergetic prompt queries, which are used to resample corresponding visual features from local visual patches. Comprehensive experiments conducted on three benchmarks demonstrate that our SPDQ approach achieves state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Han Jiang and Xiaoshan Yang and Chaofan Chen and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607726},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning trimaps via clicks for image matting. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant advancements achieved in image matting, the existing models heavily depend on manually drawn trimaps to produce accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming and lacks user-friendliness and device compatibility. This greatly limits the practical applicability of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model that is capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. By analyzing real users' behavioral logic and the characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated simulation function, making Click2Trimap exhibit versatility across various scenarios. Compared with all existing trimap-free matting methods, Click2Trimap achieves superior performance in quantitative and qualitative assessments conducted on synthetic and real-world matting datasets. In particular, in a user study, Click2Trimap yields high-quality trimap and matting predictions in just 5 seconds per image on average, demonstrating its substantial practical value for use in real-world applications.},
  archive      = {J_TMM},
  author       = {Chenyi Zhang and Yihan Hu and Henghui Ding and Humphrey Shi and Yao Zhao and Yunchao Wei},
  doi          = {10.1109/TMM.2025.3607710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning trimaps via clicks for image matting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although screen-camera resilient watermarking addresses issues such as privacy leakage and copyright infringement in digital images to some extent during screen-camera communication. However, in screen-camera scenarios, uncontrolled shooting environments, various display devices, and different lens types introduce more complex noise into the watermarked images. Because some noise generated during the screen-camera process cannot be quantitatively analyzed, the integrity of the embedded watermark is compromised, making copyright verification and information acquisition still difficult. To solve this problem, we establish a large-scale screen-camera image dataset (SCISet) and propose a noise simulation network (NoS-Net). Specifically, we obtain 36,000 screen-camera images under various shooting environments with multiple types of screens and cameras. Then, we use SCISet to train the proposed NoS-Net based on the U-Net architecture, which can learn multi-level and complementary feature information of screen-camera images, enhancing its ability to simulate complex noise. Experimental results show that integrating the proposed NoS-Net into mainstream screen-camera resilient watermarking methods significantly improves their ability to resist screen-camera noise attacks. Furthermore, the diversity of SCISet plays an important role in advancing robust watermarking research.},
  archive      = {J_TMM},
  author       = {Daidou Guo and Chuan Qin and Fengyong Li and Heng Yao and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3607783},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD). However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture. To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and total Floating Point Operations (FLOPs). Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework. Simulations show that the proposed model is robust against noise and negative samples. Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest parameters and FLOPs. Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities.},
  archive      = {J_TMM},
  author       = {Ruituo Wu and Yang Chen and Jian Xiao and Bing Li and Jicong Fan and Frédéric Dufaux and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TMM.2025.3607708},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oversampling with GAN via meta-learning for imbalanced data. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing generative adversarial networks (GANs) for oversampling imbalanced data has demonstrated its effectiveness. However, many GAN-based oversampling methods are confronted with a significant challenge, namely, mode collapse, especially when dealing with tabular imbalanced data. In this paper, two unique penalty terms are respectively incorporated into the loss functions of the discriminator and the generator of GAN to promote the generated samples to exhibit not just statistical but also spatial information consistency with the minority samples, thereby alleviating the issue of mode collapse. In contrast to other studies that fix the coefficient of the penalty terms, the optimal coefficients of the penalty terms are adaptively searched using a meta-learning approach, where Bayesian optimization is firstly employed to effectively handle situations involving small size of minority samples in the imbalanced data. We call the proposed model as META_GAN. Experimental results demonstrate that META_GAN outperforms alternative oversampling methods on general tabular and image imbalanced datasets and long-tailed datasets in terms of different metrics.},
  archive      = {J_TMM},
  author       = {Yueqi Chen and Witold Pedrycz and Chao Zhang and Jian Wang and Jie Yang},
  doi          = {10.1109/TMM.2025.3607712},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oversampling with GAN via meta-learning for imbalanced data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting long-tailed recognition with label descriptor and beyond. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Tailed Recognition (LTR) poses significant challenges due to the heavily imbalanced nature of real-world data, which severely skews data-driven deep neural networks. Despite the rapid progress of Vision-Language Models (VLMs), they still face challenges in effectively learning from long-tailed visual data. In this paper, we present a comprehensive analysis of the reasons behind the underperformance of VLMs and propose a hierarchical inference framework to address this issue. Specifically, we prompt the large language models to generate sentencelevel descriptors for class labels and conduct the open vocabulary classification by computing the average similarity between the image and each descriptor. A reweighting mechanism is further proposed to filter out uninformative descriptors. To mitigate model bias incurred by the long-tail distribution, we propose a feature adapter with the logit adjustment technique and finetune the CLIP model via visual prompt tokens. We introduce the Shared Feature space Mixup (SFM) to enhance the interaction between modalities to address tail visual feature insufficiency. Finally, we propose a hierarchical inference manner to combine the aforementioned proposals. Extensive evaluations demonstrate that our approach achieves state-of-the-art performance by finetuning only a few parameters on the Places-LT, ImageNet-LT, and iNaturalist 2018 benchmarks.},
  archive      = {J_TMM},
  author       = {Zhengzhuo Xu and Ruikang Liu and Zenghao Chai and Yiyan Qi and Lei Li and Haiqin Yang and Chun Yuan},
  doi          = {10.1109/TMM.2025.3607812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting long-tailed recognition with label descriptor and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive action quality assessment through multi-branch modeling. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA) aims to evaluate and score human actions in videos accurately. Existing approaches involve extracting features from the input video and implementing regression based on those features. However, representations derived from a single branch often lack the necessary diversity and flexibility to capture the complexity of human actions effectively. This work addresses these limitations by introducing a multi-branch architecture designed to capture a broad spectrum of video dynamics at varying levels of granularity. Specifically, we enhance video representation in the flow-guided branch by integrating optical flow with video features. This combination of multimodal features offers a more comprehensive context of global motion. Meanwhile, the momentfocused branch is tailored to extract frame-specific features, constructing two distinct quality-based representations with different focuses on moments, which achieves adaptive clues aggregation. Furthermore, the detail-aware branch leverages multiscale deep embeddings from a hierarchy convolutional neural network to capture fine-grained spatial information, which is useful when objects have complex spatial changes. Finally, a post-fusion strategy is employed to merge outputs from all branches, contributing to the comprehensive action quality assessment. Experimental evaluations on three benchmark datasets, FineDiving, MTLAQA, and AQA-7, demonstrate the superiority of our model in providing reliable assessments of action quality.},
  archive      = {J_TMM},
  author       = {Siyuan Xu and Peilin Chen and Yue Liu and Meng Wang and Shiqi Wang and Hong Yan and Sam Kwong},
  doi          = {10.1109/TMM.2025.3607713},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comprehensive action quality assessment through multi-branch modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the optimization of quality of experience (QoE) through learning adaptive bitrate (ABR) algorithms from internet video streams. However, the complex nature of the real-world Internet, characterized by heavy-tailed behavior, diversity, and unpredictability, hinder the effective learning of off-the-shelf reinforcement learning (RL)-based ABR algorithms. As a result, existing methods inevitably fail to achieve optimal performance under various network conditions and user QoE objectives. We propose Fortuna, a novel offline meta RL ABR algorithm that can effectively learn from these heavy-tailed internet data features and become more practical. Fortuna is primarily divided into two phases. In the offline phase, Fortuna utilizes diverse offline data for learning to reduce the costly online RL interaction expense, while in the online phase, we gradually increase video streaming sessions complexity through curriculum learning to quickly adapt to specific network conditions. Fortuna then utilizes meta-learning to optimize ABR policies and enhance generalization. Additionally, to better learn network features, Fortuna further optimizes QoE by learning low-level TCP congestion control information. Experimental results from trace-driven and real-world scenarios demonstrate that Fortuna enhances learning efficiency by more than 7.5%-4 ×, reduces stall time by 4.6%-14.2%, and generalizes to different network conditions and video streams.},
  archive      = {J_TMM},
  author       = {Ling Yi and Yongbin Qin and Ruizhang Huang},
  doi          = {10.1109/TMM.2025.3604930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal transformer for cross-modal long document classification. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3608295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.},
  archive      = {J_TMM},
  author       = {Tengfei Liu and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1109/TMM.2025.3608295},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-modal transformer for cross-modal long document classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USTC-TD: A test dataset and benchmark for image and video coding in 2020s. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3608643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image/video coding has been a remarkable research area for both academia and industry for many years. Testing datasets, especially high-quality image/video datasets, are desirable for the justified evaluation of coding-related research, practical applications, and standardization activities. We put forward a test dataset, namely USTC-TD, which has been successfully adopted in the practical end-to-end image/video coding challenge of IEEE International Conference on Visual Communications and Image Processing (VCIP) in 2022 and 2023. USTC-TD contains 40 images at 4K spatial resolution and 10 video sequences at 1080p spatial resolution, featuring various content due to the diverse environmental factors (e.g., scene type, texture, motion, view) and the designed imaging factors (e.g., illumination, lens, shadow). We quantitatively evaluate USTC-TD on different image/video features (spatial, temporal, color, lightness), and compare it with the previous image/video test datasets, which verifies its excellent compensation for the shortcomings of existing datasets. We also evaluate both classic standardized and recently learned image/video coding schemes on USTC-TD using objective quality metrics (PSNR, MS-SSIM, VMAF) and subjective quality metric (MOS), providing an extensive benchmark for these evaluated schemes. Based on the characteristics and specific design of the proposed test dataset, we analyze the benchmark performance and shed light on the future research and development of image/video coding. All the data are released online: https://esakak.github.io/USTC-TD.},
  archive      = {J_TMM},
  author       = {Zhuoyuan Li and Junqi Liao and Chuanbo Tang and Haotian Zhang and Yuqi Li and Yifan Bian and Xihua Sheng and Xinmin Feng and Yao Li and Changsheng Gao and Li Li and Dong Liu and Feng Wu},
  doi          = {10.1109/TMM.2025.3608643},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {USTC-TD: A test dataset and benchmark for image and video coding in 2020s},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JPEG AI compressed domain face detection: A multi-scale bridging perspective. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3609179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image coding is showing improved compression efficiency, while also offering a novel advantage in enabling computer vision tasks directly within the compressed domain. The latent representation created by deep learning methods inherently contains all visual features, without a computationally expensive synthesis process at the decoder. This paper is an invited extension of a previous solution for JPEG AI compressed domain face detection that adapts a RetinaFace-based detector to operate directly on the latent tensor. In addition to a former single-scale bridging solution, this work provides a novel multi-scale bridging architecture to enable a more effective multi-scale compressed domain face detection. The results show a significant performance gain, improving accuracy up to 20% for detection of tiny faces on the WIDER FACE dataset compared to single-scale bridging, and further narrowing the gap when compared to detection on uncompressed or JPEG AI decoded images. Furthermore, since the computationally expensive decoding step is bypassed and since the bridges consist of lower-complexity networks, the overall processing cost is significantly reduced. Single and multi-scale bridging, respectively, have about 10% and 32% the complexity of applying pixel domain face detection on decoded images. The proposed architecture is expected to be extended to other multiscale sensitive vision tasks, as JPEG AI is not specifically designed for any single downstream application.},
  archive      = {J_TMM},
  author       = {Ayman Alkhateeb and Alessandro Gnutti and Fabrizio Guerrini and Riccardo Leonardi and João Ascenso and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3609179},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {JPEG AI compressed domain face detection: A multi-scale bridging perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-led prompting learning caption generating model and benchmark. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) is an important intelligent system application, but most current research views it as a coarse binary classification task that lacks a fine-grained understanding of abnormal video sequences. We explore a new task for video anomaly analysis called Comprehensive Video Anomaly Caption (CVAC), which aims to generate comprehensive textual captions (containing scene information such as time, location, anomalous subject, anomalous behavior, etc.) for surveillance videos. CVAC is more consistent with human understanding than VAD, but it has not been well explored. We constructed a large-scale benchmark CVACBench to lead this research. For each video clip, we provide 6 fine-grained annotations, including scene information and abnormal keywords. A new evaluation metric Abnormal-F1 (A-F1) is also proposed to more accurately evaluate the caption generation performance of the model. We also designed a method called Anomaly-Led Generating Prompting Transformer (AGPFormer) as a baseline. In AGPFormer, we introduce an anomaly-led language modeling mechanism (Anomaly-Led MLM, AMLM) to focus on anomalous events in videos. To achieve more efficient cross-modal semantic understanding, we design the Interactive Generating Prompting (IGP) module and Scene Alignment Prompting (SAP) module to explore the divide between video and text modalities from multiple perspectives, and to improve the model's performance in understanding and reasoning about the complex semantics of videos. We conducted experiments on CVACBench by using traditional caption metrics and the proposed metrics, and the experimental results demonstrate the effectiveness of AGPFormer in the field of anomaly caption.},
  archive      = {J_TMM},
  author       = {Qianyue Bao and Fang Liu and Licheng Jiao and Yang Liu and Shuo Li and Lingling Li and Xu Liu and Xinyi Wang and Baoliang Chen},
  doi          = {10.1109/TMM.2025.3607837},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anomaly-led prompting learning caption generating model and benchmark},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3608947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised text-based person re-identification (Text-ReID) confronts the challenge of matching target person images with textual descriptions, hindered by the absence of identity annotations during training. Traditional approaches, which rely solely on global features, overlook the rich, fine-grained information within both text and image modalities. Besides, merely aligning features at the semantic level is insufficient due to the significant differences in feature representation spaces between the two modalities. Existing methods also neglect the information inequality caused by person-irrelevant factors in images. In this paper, we introduce a novel framework called Attribute-Centric Cross-modal Alignment (ACCA), specifically designed to overcome these issues. Our approach concentrates on two main aspects: visual-text attribute alignment and prediction distribution alignment. To effectively capture fine-grained information without identity labels, we implement a visual-text attribute alignment method based on momentum contrastive learning to synchronize visual and textual attribute features within a unified embedding space. We also propose a unique strategy for negative sample filtering and enrichment, creating robust and comprehensive negative attribute sample spaces to support the attribute alignment. Additionally, we establish two methods of label-free prediction distribution alignment to encourage the learning of invariant feature representations across modalities. The first method, bias-reduction distribution alignment, aligns features and predictions within each text-image pair by utilizing semantic information from the text and reduces the impact of person-irrelevant factors in images. The second method, global-attribute distribution alignment, enhances the interaction between global and local prediction distributions across visual and textual modalities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets validate our superior performances across all standard benchmarks.},
  archive      = {J_TMM},
  author       = {Jiajia Xu and Weiwei Cai and Xuemiao Xu and Yi Xie and Huaidong Zhang and Shengfeng He},
  doi          = {10.1109/TMM.2025.3608947},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSG: Multi-scale speaker graph network for active speaker detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3608949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The active speaker detection task is to determine whether a person is speaking or not across a series of video frames. Existing methods heavily rely on facial information within the annotated face bounding boxes for cross-modal learning with audio. This leads to a substantial decline in detection performance when facial cues are unclear, such as in cases of face occlusion or low-resolution facial appearances. In this paper, we extend the perception scale using only face bounding box annotations to model both facial and gestural cues, addressing the over-reliance on facial cues in active speaker detection. We propose a novel graph neural network that models inter-speaker interactions and integrates various cues from individual speakers. The final detection results are obtained through a binary graph node classification task. Our method achieves state-of-the-art performance on the AVA-ActiveSpeaker dataset (mAP: 95.6%) and the ASW dataset (mAP: 99.4%), with a model size only 21% that of the second-best method. Additionally, when facial cues are of poor quality, our method demonstrates a significant performance advantage over existing approaches. The code and model weights will be available at https://github.com/sdqdlgj/MSSG.},
  archive      = {J_TMM},
  author       = {Guanjun Li and Jiangyan Yi and Zhengqi Wen and Ruibo Fu and Yuwang Wang and Jianhua Tao},
  doi          = {10.1109/TMM.2025.3608949},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSSG: Multi-scale speaker graph network for active speaker detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning orthogonal latent representations for multi-view clustering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view clustering, latent representations are often employed to address the challenge posed by low-quality data. Traditional approaches typically assume that multiple views are fully dependent, directly learning a common latent representation from the observed data. However, this assumption is overly restrictive in real-world scenarios and may overlook valuable information, as the independence of different views can reveal critical view-specific characteristics. To overcome this limitation, we propose learning Orthogonal Latent Representations for Multi-View Clustering (OLR-MVC), which jointly captures both cross-view dependence and independence. Specifically, our model maps multi-view data into shared and private latent spaces using distinct projection bases. To accurately capture both dependence and independence, we enforce orthogonality between the shared and private latent representations while also encouraging pairwise orthogonality among private representations. Furthermore, we leverage the self-expressive property of these latent representations to capture global data structures. Extensive experimental evaluations demonstrate that OLR-MVC outperforms state-of-the-art multi-view clustering methods.},
  archive      = {J_TMM},
  author       = {Xiaolin Xiao and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TMM.2025.3607704},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning orthogonal latent representations for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dehazing approach: Recovery of color and polarization information using polarized characteristics. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarization provides valuable physical information, making it beneficial for various computer vision tasks. However, haze reduces both the color and polarization information of a scene. While existing single-image dehazing methods can restore color information, they are poor at recovering polarization information. Furthermore, current polarization-based dehazing approaches neglect the physical mechanisms of polarization degradation, resulting in inaccurate reconstruction of polarization information. In this paper, we propose a novel polarization dehazing algorithm, along with a polarization degradation model, to accurately recover both polarization and color information. First, we combine two key characteristics (the polarization achromatism prior and polarization attenuation prior) with the polarization degradation model to precisely reconstruct the scene's polarization. Then, we utilize the reconstructed polarization information to recover the color information of the scene. Finally, a multi-scale fusion optimization framework is introduced to further enhance the image quality. Our method shows excellent performance on both real-world indoor and outdoor polarized images, outperforming existing dehazing algorithms in both objective evaluation metrics and subjective visual assessment.},
  archive      = {J_TMM},
  author       = {Zhenshuo Yang and Chunhui Hao and Yang Lu and Yiming Su and Yukuan Zhang and Junchao Zhang and Jiandong Tian},
  doi          = {10.1109/TMM.2025.3607694},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel dehazing approach: Recovery of color and polarization information using polarized characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained domain generalization with feature structuralization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained domain generalization (FGDG) is a more challenging task than traditional DG tasks due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the vulnerability of subtle features leads to a severe deterioration in model performance. Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning the commonality and specificity within categories. Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is accomplished through joint optimization of five constraints: a decorrelation function applied to disentangled segments, three constraints ensuring common feature consistency and specific feature distinctiveness, and a prediction calibration term. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.2% in FGDG performance. Beyond that, the explainability analysis on explicit concept matching intensity between the shared concepts among categories and the model channels, along with experiments on various mainstream model architectures, substantiates the validity of FS.},
  archive      = {J_TMM},
  author       = {Wenlong Yu and Dongyue Chen and Qilong Wang and Qinghua Hu},
  doi          = {10.1109/TMM.2025.3607716},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained domain generalization with feature structuralization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detailed object description with controllable dimensions. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object description plays an important role for visually impaired individuals to understand and compare the differences between objects. Recent multimodal large language models (MLLMs) exhibit powerful perceptual abilities and demonstrate impressive potential for generating object-centric descriptions. However, the descriptions generated by such models may still usually contain a lot of content that is not relevant to the user intent or miss some important object dimension details. Under special scenarios, users may only need the details of certain dimensions of an object. In this paper, we propose a training-free object description refinement pipeline, Dimension Tailor, designed to enhance user-specified details in object descriptions. This pipeline includes three steps: dimension extracting, erasing, and supplementing, which decompose the description into user-specified dimensions. Dimension Tailor can not only improve the quality of object details but also offer flexibility in including or excluding specific dimensions based on user preferences. We conducted extensive experiments to demonstrate the effectiveness of Dimension Tailor on controllable object descriptions. Notably, the proposed pipeline can consistently improve the performance of the recent MLLMs. The code is currently accessible at https://github.com/PRIS-CV/ControllableObjectDescription.},
  archive      = {J_TMM},
  author       = {Xinran Wang and Haiwen Zhang and Baoteng Li and Kongming Liang and Hao Sun and Zhongjiang He and Zhanyu Ma and Jun Guo},
  doi          = {10.1109/TMM.2025.3607747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detailed object description with controllable dimensions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StableIdentity: Inserting anybody into anywhere at first sight. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in large pretrained text-to-image generation models have shown unprecedented capabilities for high-quality human-centric generation, however, customizing face identity is still an intractable problem. Existing methods cannot ensure stable identity preservation and flexible editability, even with several images for each subject during training. In this work, we propose StableIdentity, which allows identity-consistent recontextualization with just one face image from a person seen for the first time. More specifically, we employ a face encoder with the identity prior to encode the input face, and then calibrate the face representation to align the distribution of a space with the editability prior, which is constructed from celeb names. By incorporating identity prior and editability prior, the learned identity can be injected anywhere with various contexts. In addition, we design a masked two-phase diffusion loss to boost the pixel-level perception of the input face and maintain the diversity of generation. Extensive experiments demonstrate our method outperforms previous customization methods. In addition, the learned identity can be flexibly combined with the off-theshelf modules such as ControlNet. Notably, to the best of our knowledge, we are the first to directly inject the identity learned from a single image into video/3D generation without finetuning. We believe that the proposed StableIdentity is an important step to unify image, video, and 3D customized generation models. The code is available: https://github.com/qinghew/StableIdentity.},
  archive      = {J_TMM},
  author       = {Qinghe Wang and Xu Jia and Xiaomin Li and Taiqing Li and Liqian Ma and Yunzhi Zhuge and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3613113},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StableIdentity: Inserting anybody into anywhere at first sight},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised contrastive learning for indoor point cloud oversegmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud oversegmentation method can obtain a series of superpoints by grouping points that are semantically and geometrically consistent. The generated superpoints can be treated as the basic processing units in various downstream tasks to improve task performance and processing efficiency. However, due to the high semantic and geometric complexity of point cloud scenes, obtaining high-quality superpoints is still challenging. Aiming to generate high-quality indoor superpoints, we propose an end-to-end supervised contrastive learning framework SCL-OverSeg for indoor point cloud oversegmentation. Firstly, to solve the challenge of balancing the importance of geometric similarity and spatial proximity constraint between points and superpoints in indoor scenes, we integrate the geometric similarity and spatial proximity constraint into the supervision signal by generating the superpoint ground truth. To solve the challenge of superpoints crossing objects, we propose to utilize instance labels rather than semantic labels to generate the ideal superpoint ground truth as the object-level supervision signal. Secondly, to construct the distinguishable embedding space facilitating to the assignments of points to superpoints, we propose point-superpoint contrastive learning to compel the network to project each point to be closer to the reasonable superpoint in embedding space. Besides, with the instance labels, to improve the superpoint performance on object boundaries, we propose the object boundary contrastive learning to enhance the feature distinguishability between tough points across the object boundaries. Extensive experiments demonstrate that SCL-OverSeg can effectively improve indoor oversegmentation performance, especially on object boundaries. The relevant codes will be available on https://github.com/sssssyf/SCL-OverSeg.},
  archive      = {J_TMM},
  author       = {Yifan Sun and Chenguang Dai and Wenke Li and Yongsheng Zhang and Song Ji and Anzhu Yu and Yiping Chen and Hanyun Wang},
  doi          = {10.1109/TMM.2025.3613177},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Supervised contrastive learning for indoor point cloud oversegmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-driven sampling for efficient pairwise comparison subjective assessment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing image quality is crucial in image processing tasks such as compression, super-resolution, and denoising. While subjective assessments involving human evaluators provide the most accurate quality scores, they are impractical for large-scale or continuous evaluations due to their high cost and time requirements. Pairwise comparison subjective assessment tests, which rank image pairs instead of assigning scores, offer more reliability and accuracy but require numerous comparisons, leading to high costs. Although objective quality metrics are more efficient, they lack the precision of subjective tests, which are essential for benchmarking and training learning-based quality metrics. This paper proposes an uncertainty-based sampling method to optimize the pairwise comparison subjective assessment process. By utilizing deep learning models to estimate human preferences and identify pairs that need human labeling, the approach reduces the number of required comparisons while maintaining high accuracy. The key contributions include modeling uncertainty for accurate preference predictions and for pairwise sampling. The experimental results demonstrate superior performance of the proposed approach compared to traditional active sampling methods. An implementation of the pairwise sampling method is publicly available at https://github.com/shimamohammadi/LBPS-EIC},
  archive      = {J_TMM},
  author       = {Shima Mohammadi and João Ascenso},
  doi          = {10.1109/TMM.2025.3613160},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-driven sampling for efficient pairwise comparison subjective assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PADNet: Progressive-difference-aware feature reconstruction mechanism for anomaly detection. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection generally aims to identify irregularities using only normal samples, where feature reconstruction-based methods demonstrate greater robustness to noise by comparing reconstructed results with original data. However, they encounter issues with detailed information loss and insufficient anomaly discriminability. To address these challenges, we propose a progressive-difference-aware feature reconstruction network for image anomaly detection, named PADNet. To enhance context interaction, we develop a harmonic symmetric reconstruction framework integrated with a progressive feature harmonizer (PFH). The PFH mitigates detailed information loss to reduce undesired reconstruction errors through the progressive fusion of information flows. To enhance anomaly discriminability, we introduce the neighbor-aided residual feature representation module (NRFR) to strengthen difference-aware feature representations. The NRFR innovatively captures discriminative cues by interacting with neighboring reference samples in the feature cache pool. Experimental results on the MVTec, Visa, and BTAD datasets demonstrate that our method achieves superior performance while requiring only 25.3% of the parameters compared to the state-of-the-art baseline. Code is available at: https://github.com/haaloowo/PADNet.},
  archive      = {J_TMM},
  author       = {Fan Yang and Peiguang Jing and Weiming Wang and Fu Lee Wang and Yuting Su},
  doi          = {10.1109/TMM.2025.3613127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PADNet: Progressive-difference-aware feature reconstruction mechanism for anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GrabDAE: An innovative framework for unsupervised domain adaptation utilizing grab-mask and denoise auto-encoder. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Unsupervised Domain Adaptation (UDA) methods often fall short in fully leveraging contextual information from the target domain, leading to suboptimal decision boundary separation during source and target domain alignment. To address this, we introduce GrabDAE, an innovative UDA framework designed to tackle domain shift in visual classification tasks. GrabDAE incorporates two key innovations: the Grab-Mask module, which blurs background information in target domain images, enabling the model to focus on essential, domain-relevant features through contrastive learning; and the Denoising Auto-Encoder (DAE), which enhances feature alignment by reconstructing features and filtering noise, ensuring a more robust adaptation to the target domain. These components empower GrabDAE to effectively handle unlabeled target domain data, significantly improving both classification accuracy and robustness. Extensive experiments on benchmark datasets, including VisDA-2017, Office-Home, and Office31, demonstrate that GrabDAE consistently surpasses state-of-the-art UDA methods, setting new performance benchmarks. By tackling UDA's critical challenges with its novel feature masking and denoising approach, GrabDAE offers both significant theoretical and practical advancements in domain adaptation.},
  archive      = {J_TMM},
  author       = {Junzhou Chen and Xuan Wen and Ronghui Zhang and Bingtao Ren and Di Wu and Zhigang Xu and Danwei Wang},
  doi          = {10.1109/TMM.2025.3613149},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GrabDAE: An innovative framework for unsupervised domain adaptation utilizing grab-mask and denoise auto-encoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIRE: Enhancing facial expression recognition through domain-invariant representation learning for robust generalization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose DIRE (Domain-Invariant Representation Learning for Expression), a novel approach to enhance the generalizability of facial expression recognition (FER) models in unseen domains. Traditional FER models often struggle with distribution shifts between training and test datasets, leading to significant performance drops. Based on the concept of Single-Source Domain Generalization, we introduce a novel domain augmentation technique that applies pixel-level and feature-level perturbations to domain-variant regions while preserving semantic consistency. Additionally, we incorporate semantic alignment regularization and domain information minimization loss so that domain-invariant features effectively represent facial expressions. Extensive experiments on multiple FER datasets demonstrate that our method significantly improves generalization across diverse target domains, even when trained on a single source domain. The proposed DIRE approach offers a robust solution to real-world FER tasks, where unseen domain generalizability is crucial.},
  archive      = {J_TMM},
  author       = {Heeje Kim and Yoojin Jung and Byung Cheol Song},
  doi          = {10.1109/TMM.2025.3613175},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DIRE: Enhancing facial expression recognition through domain-invariant representation learning for robust generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VQM4HAS: A real-time quality metric for HEVC videos in HTTP adaptive streaming. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In HTTP Adaptive Streaming (HAS), a video is encoded at various bitrate-resolution pairs, collectively known as the bitrate ladder, allowing users to select the most suitable representation based on their network conditions. Optimizing this set of pairs to enhance the Quality of Experience (QoE) requires accurately measuring the quality of these representations. VMAF and ITU-T's P.1204.3 are highly reliable metrics for assessing the quality of representations in HAS. However, in practice, using these metrics for optimization is often impractical for live streaming applications due to their high computational costs and the large number of bitrate-resolution pairs in the bitrate ladder that need to be evaluated. To address their high complexity, our paper introduces a new method called VQM4HAS, which extracts low-complexity features, including ($i$) video complexity features, ($ii$) frame-level encoding statistics logged during the encoding process, and ($iii$) lightweight video quality metrics. These extracted features are then fed into a regression model to predict VMAF or P.1204.3. The VQM4HAS model is designed to operate on a per bitrate-resolution pair, per-resolution, and cross-representation basis, optimizing quality predictions across different scenarios. Our experimental results demonstrate that VQM4HAS achieves a high correlation with VMAF and P.1204.3, with Pearson correlation coefficients (PCC) ranging from 0.95 to 0.96 for VMAF and 0.97 to 0.99 for P.1204.3, depending on the resolution. Despite achieving a high correlation with VMAF and P.1204.3, VQM4HAS exhibits significantly less complexity than both metrics, with 98% and 99% less complexity for VMAF and P.1204.3, respectively, making it suitable for live streaming scenarios. We also conduct a feature importance analysis to further reduce the complexity of the proposed method. Furthermore, we evaluate the effectiveness of our method by using it to predict subjective quality scores. The results show that VQM4HAS achieves a higher correlation with subjective scores at various resolutions despite its minimal complexity. The source code is available at https://github.com/cd-athena/VQM4HAS.},
  archive      = {J_TMM},
  author       = {Hadi Amirpour and Jingwen Zhu and Wei Zhou and Patrick Le Callet and Christian Timmerer},
  doi          = {10.1109/TMM.2025.3613110},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VQM4HAS: A real-time quality metric for HEVC videos in HTTP adaptive streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the model's feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused image's brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at https://github.com/HeDan-11/DM-FNet.},
  archive      = {J_TMM},
  author       = {Dan He and Weisheng Li and Guofen Wang and Yuping Huang and Shiqiang Liu},
  doi          = {10.1109/TMM.2025.3613156},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUGS-net: A lightweight and unified network for adverse weather image denoising. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising under adverse weather conditions aims to eliminate multiple weather-related noises and restore bright and clear images. Until now, most methods are task-specific while all-in-one algorithms often require a large number of parameters, limiting their model efficiency. Our theoretical analysis and statistical experiments reveal that adverse weather images in Hue channel contain rich contextual information for further processing. With this observation, we propose a novel lightweight HUe-Guided Synergistic Network (HUGS-Net) with multi-scale detail refinement. First, we design a Fourier interaction and evolution module to capture global information from Hue channel without introducing excessive network parameters. Second, we develop a lightweight residue group convolution block to model local texture features, incorporating them with global information to guide noises removal. Third, we introduce a multi-scale fusion module to enhance high-frequency details at a small feature resolution in RGB color space. With the above design, HUGS-Net further supervises and supplements refined background information. Comprehensive experiments showcase the superiority of HUGS-Net across various adverse weather datasets (e.g., image deraining, desnowing, dehazing) with the least parameter size and fast running speed.The source code will be made public after peer review process.},
  archive      = {J_TMM},
  author       = {Ting Zhang and Runjie Wang and Yuzhen Niu and Zuoyong Li and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3613104},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HUGS-net: A lightweight and unified network for adverse weather image denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted polygonal faces using vertex index similarity. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding techniques serve as a cornerstone in the protection of embedded information across diverse media. Traditionally, methods applied to 3D models have primarily focused on modifying vertex coordinates. However, this approach neglects the untapped potential of polygonal faces, which, being more abundant than vertices, offer a scalable and efficient avenue for data embedding. By leveraging polygon indices for reversible data hiding—particularly when integrated with encryption—it becomes possible to randomize the model's structure, facilitating secure modifications while preserving geometric integrity. This study introduces an innovative reversible data hiding algorithm that embeds messages within the polygon indices of encrypted 3D models. The algorithm harnesses the inherent similarities among vertex indices to conceal additional information. To maintain the consistency of polygon normal vectors, we implement a right circular shifting mechanism that systematically reorganizes the indices, ensuring that the smallest value consistently occupies the initial position. Additionally, we incorporate techniques such as leading zero count and multi-MSB prediction to enhance embedding capacity while keeping index values within permissible ranges. Experimental results demonstrate that our approach significantly outperforms conventional vertex-based methods, yielding substantial improvements in embedding efficiency. Crucially, the reversible nature of the proposed technique ensures the exact restoration of the original 3D model upon data extraction, guaranteeing zero information loss and no compromise in quality. Moreover, the algorithm is designed to integrate with vertex-based reversible data hiding techniques for encrypted 3D models, potentially enhancing data embedding capacity under compatible conditions.},
  archive      = {J_TMM},
  author       = {Yuan-Yu Tsai},
  doi          = {10.1109/TMM.2025.3613172},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reversible data hiding in encrypted polygonal faces using vertex index similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric continuity and consistency learning for self-supervised point cloud completion. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to infer the complete point clouds from incomplete ones. In real-world scenarios, where the paired data is absent, self-supervised methods have emerged as a promising solution. Although existing self-supervised methods perform well at relatively low resolutions, they suffer significant performance degradation at higher resolution primarily because they focus on point cloud reconstruction at patch-level or point-level. In this paper, we propose a self-supervised method based on Geometric Continuity and Consistency Learning (GCCL) at multi-scale level to improve the accuracy of predicting local details and global shapes of point clouds. Specifically, to capture local details, we employ a patch-topoint strategy and a coarse-fine manner for geometric continuity learning. To constrain the global shapes, we construct multiple branches for mutual supervision and utilize class priors to build a memory queue for contrasting current features, enhancing the network focus on geometric consistency learning. We evaluate GCCL on multiple datasets, and the results show that our method outperforms existing self-supervised methods by a 4.4 improvement in CD-$\ell_2$ on the synthetic PCN dataset and can generate more uniformly distributed completion results on realworld datasets},
  archive      = {J_TMM},
  author       = {Junkang Ma and Shuoyao Wang and Qi Zheng and Xiaochun Mai},
  doi          = {10.1109/TMM.2025.3613154},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geometric continuity and consistency learning for self-supervised point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RUL: Region uncertainty learning for robust face recognition. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data uncertainty refers to the degree of uncertainty in model predictions caused by data variability, challenging the model robustness in multimedia applications. The increased variability of uncontrolled face images, such as mask occlusion and image blur, increases the intra-class differences and inter-class similarities. As a result, the ambiguity in the learned features exacerbates the uncertainty of sample-to-class membership. Traditional face recognition models are deterministic point embedding models that fail to measure data uncertainty. Probabilistic embedding models, such as advanced Data Uncertainty Learning (DUL), represent each face image as a Gaussian distribution to measure data uncertainty. However, these models perform random sampling from the distribution once per training, and the sampled points may fall into different class regions, leading to training oscillation. Therefore, we propose a robust Region Uncertainty Learning (RUL) method, which adopts the entire Gaussian distribution of each sample during each training epoch, and estimates the region relations between the sample distribution region and the class region to measure the sample-to-class membership. In fact, DUL is a special case of the proposed RUL. Specifically, DUL estimates point-with-region relations and only represents absolute membership and non-membership. In contrast, RUL estimates region-with-region relations, enabling it to additionally represent incomplete membership. This more comprehensive membership measurement fully represents the uncertainty of membership, enhancing the model performance and robustness in uncontrolled scenes. Furthermore, for robust face recognition, we propose two RUL-based angular margin losses, AngleFace and RegionFace, to adaptively adjust the learning weights according to the uncertainty of membership. Finally, we comprehensively evaluate the effectiveness of RUL on various face datasets, and profoundly analyze the role of region relations. In future, we will explore the applicability of RUL in other tasks.},
  archive      = {J_TMM},
  author       = {Weiming Xiong and Mingyang Zhong and Shenglin Li and Guojun Huang and Libo Zhang},
  doi          = {10.1109/TMM.2025.3613164},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RUL: Region uncertainty learning for robust face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mix-based training strategies for learning implicit neural representations. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With coordinates as the input and RGB pixel values as the output, a neural network can be used to represent an image, which is widely known as Implicit neural representations (INRs). Previous works on INR have mainly focused on learning an invariant image target without exploring the impact of learning strategies on learning INR. It is observed that there is a substantial variation in PSNR among different images, and our preliminary investigation shows that, in the early training stage, learning complex image content yields significantly better performance than simple image content. Inspired by this finding, we conjecture that increasing INR task complexity in the early stage of training might boost INR performance and thus propose to intentionally contaminate the target image with another complex image. Our proposed method is called Mix-INR, which adopts a two-stage training to first learn a pseudo-target image (contaminated target) and then learn the real-target image (uncontaminated target). To generate the pseudo-target image, we experiment with two contamination methods (blending and replacement), both of which show superior performance and verify our conjecture. INRs have gained popularity as a promising approach for representing a variety of data types, including images of the task complexity of the pseudo-target image, we set the contamination image from a complex natural image to a random-noise image. Moreover, we propose a dynamic contamination method to smoothly transition from the pseudo-target image to the real-target image. Experimental results demonstrate that our proposed method achieves competitive performance, which suggests that INR can be improved by manipulating the task complexity in the early stage of training.},
  archive      = {J_TMM},
  author       = {Dongshen Han and Chaoning Zhang and Sheng Zheng and Fachrina Dewi Puspitasari and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3613162},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mix-based training strategies for learning implicit neural representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local and global structure-guided no-reference point cloud quality assessment. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial representation of 3D data, a point cloud (PC) can accurately capture the geometry, structure, and color information of objects. However, various quality problems arise owing to device noise, data acquisition errors, and compression algorithms, limiting the application of PCs. Therefore, assessing PC quality to determine its suitability for applications is a challenging task. In this work, a local and global structure-guided feature extraction and attention network (LGS-Net) is introduced for no-reference PC quality assessment (PCQA). This approach incorporates cluster construction (CC), local structure-guided cluster feature extraction (LSFE), and global structure-guided attention (GSA) modules. First, owing to the heightened sensitivity of the human visual system (HVS) to structural information, a graph filter is employed to identify high-frequency clusters. Within the LSFE module, a multiscale strategy is employed to ensure that structural information effectively influences both the geometry and color information. Simultaneously, the multiscale features within the cluster are dynamically fine-tuned using feature channel weight reassignment. To account for the impact of interclusters on overall quality, a GSA module is introduced to establish global dependencies between local clusters. This approach enables the extraction of final geometry, color, and structure information, which are ultimately used for accurate quality assessment. Extensive experimental results show that the proposed method outperforms the existing state-of-the-art PCQA methods using two publicly available subjective datasets.},
  archive      = {J_TMM},
  author       = {Zhouyan He and Qihao Liang and Gangyi Jiang and Mei Yu and Yeyao Chen and Ting Luo and Wujie Zhou},
  doi          = {10.1109/TMM.2025.3613114},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Local and global structure-guided no-reference point cloud quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and robust video virtual try-on via enhanced multi-garment alignment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video virtual try-on aims to generate realistic sequences where garments maintain their identity and adapt accurately to a person's pose and body shape in source video. This task can be regarded as video inpainting, whereas previous methods focus primarily on the specific try-on region while simply “copying” the remaining parts of the person. However, this approach limits the degrees of freedom and heavily relies on precise human parsing. In complex in-the-wild scenarios, dynamic blurring and limb occlusions can introduce errors and discontinuities in the inpainting regions, adversely affecting the video try-on results. Our solution, VidClothEditor, adopts a relaxed editing approach that allows for full-body inpainting and treats non-edited regions as a reconstruction task. It utilizes multiple garment alignment with a proposed region guidance to enhance the naturalness of video try-on results. Additionally, we employ garment-augmented video consistency learning, which significantly reduces the inference time and increases the practical potential for video editing. Comprehensive experiments on the VITON-HD and TikTok datasets confirm VidClothEditor's ability to generate high-quality images and smooth videos. The project website is at video-tryon.github.io.},
  archive      = {J_TMM},
  author       = {Zijian He and Peixin Chen and Guolin Zheng and Guangrun Wang and Xiaonan Luo and Liang Lin and Guanbin Li},
  doi          = {10.1109/TMM.2025.3613169},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient and robust video virtual try-on via enhanced multi-garment alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic dual-adversarial network for blended-target domain adaptation. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of blended-target domain adaptation (BTDA) is growing since target data in the real world often come from multiple domains with different data distributions. Most BTDA studies adapt directly from the source domain to the target domains without considering which kinds of semantic information embedded in images should be explored. Therefore, some irrelevant semantic information is inevitably used, which leads to negative transfer. To address these issues, we propose a semantic dual-adversarial network (SDN) method for BTDA. Specifically, to suppress irrelevant semantic information, we adopt a min-max game strategy between the classifier and the feature extractor. The classifier tries to maximize the prediction distribution discrepancy, whereas the extractor endeavors to minimize this discrepancy. In this process, irrelevant semantic information is suppressed and the principal semantic information is emphasized. To align the categorical distributions, we train a category-aware domain discriminator and a feature extractor with category labels. In addition, we introduce a random ratio-based feature fusion scheme to augment the source domain, which can decrease domain gaps. At last, we propose a weighted negative self-supervised learning method to enhance the model's generalization. Extensive experiments on multiple benchmarks showcase that our method significantly outperforms the prior state-of-the-art methods in BTDA.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Xue Hu and Haoyu Huang and Zhihui Lai and Xuelong Li},
  doi          = {10.1109/TMM.2025.3613144},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic dual-adversarial network for blended-target domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-guided diffusion transformer for low-light image enhancement. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the diffusion transformer (DiT) has become a focal point of interest in recent years, its application in low-light image enhancement remains a blank area for exploration. Current methods recover the details from low-light images while inevitably amplifying the noise in images, resulting in poor visual quality. In this paper, we firstly introduce DiT into the low-light enhancement task and design a novel Structure-guided Diffusion Transformer based Low-light image enhancement (SDTL) framework. We compress the feature through wavelet transform to improve the inference efficiency of the model and capture the multi-directional frequency band. Then we propose a Structure Enhancement Module (SEM) that uses structural prior to enhance the texture and leverages an adaptive fusion strategy to achieve more accurate enhancement effect. In Addition, we propose a Structure-guided Attention Block (SAB) to pay more attention to texture-riched tokens and avoid interference from noisy areas in noise prediction. Extensive qualitative and quantitative experiments demonstrate that our method achieves SOTA performance on several popular datasets, validating the effectiveness of SDTL in improving image quality and the potential of DiT in low-light enhancement tasks.},
  archive      = {J_TMM},
  author       = {Xiangchen Yin and Zhenda Yu and Longtao Jiang and Xin Gao and Xiao Sun and Zhi Liu and Xun Yang},
  doi          = {10.1109/TMM.2025.3613117},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Structure-guided diffusion transformer for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALCER3D: Adaptive learning constraints for enhanced retrieval of complex indoor 3D scenarios. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse is growing rapidly, resulting in thousands of rich virtual universes. This results in a difficult search process for the user, making advanced search tools a necessity. Existing methods leverage contrastive learning to obtain a function mapping a 3D scene and its textual descriptions into similar representations. However, Metaverse scenarios are complex, multimedia-rich 3D scenes containing many elements, making cross-modal alignment difficult. For instance, a museum dedicated to Van Gogh is unrelated to Warhol, yet it shares similarities with Matisse or Monet. To make the mapping functions aware of these nuances, we propose a novel learning strategy to integrate Adaptive Optimization Constraints, computing data-dependent distances using a language-based method we design and enforcing them between the representations at training time. This novelty sets our approach apart from standard procedures enforcing the same distance. We validate the effectiveness of two datasets, one including 6000 apartments, and a novel dataset of 3000 museums that we collect. We observe consistent improvements compared to existing methods. Moreover, we obtain better generalization when with very complex scenarios, e.g. on the museums dataset it obtains an average R@1 of 5.2% compared to 1.2% obtained by existing methods. Finally, the source code is available at https://github.com/aliabdari/ALCER3D.},
  archive      = {J_TMM},
  author       = {Alex Falcon and Ali Abdari and Giuseppe Serra},
  doi          = {10.1109/TMM.2025.3613176},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ALCER3D: Adaptive learning constraints for enhanced retrieval of complex indoor 3D scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust noisy label learning via two-stream sample distillation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy label learning aims to learn robust networks under the supervision of noisy labels, which plays a critical role in deep learning. Existing work either conducts sample selection or label correction to deal with noisy labels during the model training process. In this paper, we design a simple yet effective sample selection framework, termed Two-Stream Sample Distillation (TSSD), for noisy label learning, which can extract more high-quality samples with clean labels to improve the robustness of network training. Firstly, a novel Parallel Sample Division (PSD) module is designed to generate a certain training set with sufficient reliable positive and negative samples by jointly considering the sample structure in feature space and the human prior in loss space. Secondly, a novel Meta Sample Purification (MSP) module is further designed to mine adequate semi-hard samples from the remaining uncertain training set by learning a strong meta classifier with extra golden data. As a result, more and more high-quality samples will be distilled from the noisy training set to train networks robustly in every iteration. Extensive experiments on four benchmark datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet and Clothing-1M, show that our method has achieved state-of-the-art results over its competitors.},
  archive      = {J_TMM},
  author       = {Sihan Bai and Sanping Zhou and Zheng Qin and Le Wang and Nanning Zheng},
  doi          = {10.1109/TMM.2025.3613115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust noisy label learning via two-stream sample distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-reflection neural network for class-incremental object counting. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowded scenarios, achieving the counting task of dynamically evolving categories is extremely challenging. In addition to grappling with challenges such as scale variations, severe occlusion and complex backgrounds, it is imperative to mitigate the issue of catastrophic forgetting. Previous approaches have heavily relied on leveraging historical data for knowledge distillation to tackle these difficulties. However, this strategy encounters two prominent obstacles: 1) Employing the teacher network from the previous stage for distillation incurs additional computational overhead during the training stage. 2) Although knowledge distillation can facilitate effective knowledge transfer, some inaccurate predictions from the teacher network may affect the knowledge acquisition in the current stage. To overcome these issues, we introduce a novel solution: a self-reflection neural network for class-incremental object counting. First, we construct a global-aware incremental regression branch that uses stacked transformer layers as backends to capture global information, while the final regression layers dynamically expand as categories increase. Furthermore, we introduce an uncertain estimation branch that selectively isolates certain feature maps to avoid some neurons updated with excessive gradient information, thereby enhancing the network plasticity while preserving stability. The output of this branch functions as a regularization signal, steering the learning process of the incremental regression branch. To foster a more robust retention of past knowledge, we propose a self-reflection loss. It employs the rectified outputs of global-aware incremental regression branch to encourage the network to reflect upon and refine its grasp of historical knowledge, effectively averting the pitfalls of inaccurate information. Our extensive experiments validate the effectiveness of our proposed method, achieving state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Shengqin Jiang and Linfei Li and Fengna Cheng and Yuankai Qi and Qingshan Liu},
  doi          = {10.1109/TMM.2025.3613145},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-reflection neural network for class-incremental object counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEVC video steganalysis based on centralized error and attention mechanism. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With high embedding capacity and security, transform coefficient-based video steganography has become an important branch of video steganography. However, existing steganalysis methods against transform coefficient-based steganography provide insufficient consideration to the prediction process of HEVC compression, which results in steganalysis that is not straightforward and fail to effectively detect adaptive steganography methods in low embedding rate scenarios. In this paper, an HEVC video steganalysis method based on centralized error and attention mechanism against transform coefficient-based steganography is proposed. Firstly, the centralized error phenomenon brought by distortion compensation-based steganography is analyzed, and prediction error maps is constructed for steganalysis to achieve higher SNR(signal-to-noise ratio). Secondly, a video steganalysis network called CESNet (Centralized Error Steganalysis Network) is proposed. The network takes the prediction error maps as input and four types of convolutional modules are designed to adapt to different stages of feature extraction. To address the intra-frame sparsity of adaptive steganography, CEA (Centralized Error Attention) modules based on spatial and channel attention mechanisms are proposed to adaptively enhance the steganographic region. Finally, after extracting the feature vectors of each frame, the detection of steganographic video is completed using the self-attention mechanism. Experimental results show that compared with the existing transform coefficient-based video steganalysis methods, the proposed method can effectively detect multiple transform coefficient-based steganography algorithms and achieve higher detection performance in low payload scenarios.},
  archive      = {J_TMM},
  author       = {Haojun Dai and Dawen Xu and Lin Yang and Rangding Wang},
  doi          = {10.1109/TMM.2025.3613171},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HEVC video steganalysis based on centralized error and attention mechanism},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StegFlow: Flow-based high-frequency distribution mapping network for multi-image steganography. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-image steganography refers to the technique of embedding multiple secret images into a single cover image while ensuring that the secret images remain imperceptible and can be perfectly recovered by the recipient. Traditional single-image-based steganography often leads to noticeable contour shadows or color distortions in the cover image, making the hidden image more detectable. In contrast, cascaded invertible neural network-based steganography introduces a large number of parameters, complicating the network structure and resulting in a time-consuming learning and training process. To address the above problems, this paper proposes a novel flow-based, end-to-end multi-image invertible steganography framework (StegFlow), which effectively integrates forward and backward data flows for image hiding and recovery. The framework employs cascading operations to enable deep hiding of multiple secret images. To enhance the coupling capabilities, we introduce an invertible permutation layer that disrupts the channel arrangement order, allowing the coupling layer to more accurately guide the embedding of secret information into regions of the image that are easy to hide and recover. In addition, a high-frequency distribution mapping (HFDM) is designed to model the lost high-frequency information during image hiding process, significantly improving the recovery performance of the secret images. Extensive experiments are performed over multiple classical datasets, and the results demonstrate that compared to state-of-the-art (SOTA) models, the proposed framework can achieve a superior overall performance in terms of visual quality and anti-steganalysis capability. Specifically, our scheme can improve the hiding accuracy (measured by PSNR) by over 3 dB and the recovery accuracy by over 1 dB when hiding two secret images.},
  archive      = {J_TMM},
  author       = {Fengyong Li and Hao Liu and Xinpeng Zhang and Chuan Qin},
  doi          = {10.1109/TMM.2025.3613124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StegFlow: Flow-based high-frequency distribution mapping network for multi-image steganography},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor completion framework by graph refinement for incomplete multi-view clustering. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-view Clustering (IMVC) endeavors to harness information from multiple incomplete views to partition multi-view data into their respective clusters. How to recover missing information with lossless fidelity is the core of IMVC, which is of vital importance but challenging. Most of the existing methods include a feature recovery step to mitigate the negative impact of missing samples on the feature graph, however, these IMVC algorithms simply utilize the correlation between samples to recover the relationship between the unmissing instances and the missing instances while ignoring the consistency between views, which leads to often unsatisfactory recovery results. In addition, previous IMVC algorithms focus more on the recovery of incomplete data, ignoring the effect of the error term on incomplete graphs. This can mislead the recovery process of IMVC algorithm and the feature graph can be affected by anomalous information, which leads to degradation of clustering performance. To address this gap, this paper introduces the Tensor Completion Framework by Graph Refinement for Incomplete Multi-view Clustering (IMVC-TGR). IMVC-TGR separates the redundant information in each affine graph by graph refinement operation, aiming to mitigate the negative impact of error terms and redundant information on the feature graph during the recovery process. Meanwhile, IMVC-TGR stacks the feature graphs into tensors to explore intra-view correlation and inter-view consistency, so as to recover the relationship between missing samples and non-missing samples, and improve the quality of the feature graphs. Finally, IMVC-TGR introduces semantic consistency constraints and self-weighted fusion strategies into the high-quality feature graphs, aiming at preserving the complementary information between different views while balancing the contributions of the refined representation matrices of different views. The experimental results on multiple different datasets indicate that IMVC-TGR can achieve state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Huibing Wang and Yawei Chen and Mingze Yao and Wenzhe Liu and Jinjia Peng and Xianping Fu},
  doi          = {10.1109/TMM.2025.3613125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tensor completion framework by graph refinement for incomplete multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Watch where you move: Region-aware dynamic aggregation and excitation for gait recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based gait recognition has achieved great success in various applications. The key to accurate gait recognition lies in considering the unique and diverse behavior patterns in different motion regions, especially when covariates affect visual appearance. However, existing methods typically use predefined regions for temporal modeling, with fixed or equivalent temporal scales assigned to different types of regions, which makes it difficult to model motion regions that change dynamically over time and adapt to their specific patterns. To tackle this problem, we introduce a Region-aware Dynamic Aggregation and Excitation framework (GaitRDAE) that automatically searches for motion regions, assigns adaptive temporal scales and applies corresponding attention. Specifically, the framework includes two core modules: the Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the optimal temporal receptive field for each region, and the Region-aware Dynamic Excitation (RDE) module, which emphasizes the learning of motion regions containing more stable behavior patterns while suppressing attention to static regions that are more susceptible to covariates. Experimental results show that GaitRDAE achieves state-of-the-art performance on several benchmark datasets. The source code will be published at https://github.com/HUAFOR/GaitRDAE.},
  archive      = {J_TMM},
  author       = {Binyuan Huang and Yongdong Luo and Xianda Guo and Xiawu Zheng and Zheng Zhu and Jiahui Pan and Chengju Zhou},
  doi          = {10.1109/TMM.2025.3613158},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Watch where you move: Region-aware dynamic aggregation and excitation for gait recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeskTransfer: Predicting multi-scenario video stream throughput in cloud desktop based on transfer autoencoder. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of cloud services has provided a new medium for video streams transmission. Cloud desktops, as a representative multimedia application, facilitate interaction between users and cloud via video streams, garnering widespread adoption in various fields. The network condition directly affects the transmission. Therefore, accurate throughput prediction helps guide the allocation of network resources, avoiding a decline in user experience due to insufficient resources and waste caused by excessive resources. Recent works focus more on the temporal characteristics of throughput. However, we believe that throughput of video streaming is significantly influenced by usage scenario. In this paper, we propose a transfer-based autoencoder framework DeskTransfer for throughput prediction in frequent switching cloud desktop scenarios. Specifically, we construct the Scenario Autoencoder and Throughput Autoencoder to respectively learn the scenario and throughput features from historical usage records. By adopting an adversarial mechanism, we design transfer algorithm using latent vectors, enabling the model suitable for multiple scenarios. We collect real-world data from a project cooperated with Lenovo Research for experiment and compare our solution with leading methods on public datasets to validate its effectiveness.},
  archive      = {J_TMM},
  author       = {Zuodong Jin and Peng Qi and Ruipeng Gao and Yanzhe Jing and Dan Tao},
  doi          = {10.1109/TMM.2025.3613109},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeskTransfer: Predicting multi-scenario video stream throughput in cloud desktop based on transfer autoencoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving infrared small target detection with GAN-driven data augmentation. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection (IRSTD) based on deep learning has received extensive research and application. However, deep learning models require a large amount of data to perform well, and the collection and standardization of infrared small target data is challenging, limiting the applicability of such models. To address this issue, this study proposes a data augmentation scheme for infrared small targets based on Generative Adversarial Networks (GANs). The proposed method is a two-step approach: the first step is the generation of clean backgrounds, and the second is the adaptive fusion of targets and backgrounds. In the background generation stage, we first use the Fast Marching Method (FMM) to fill background targets and obtain clean backgrounds. Then, we design a multi-generator and multi-discriminator GAN model (MGD-GAN) to generate high-quality and diverse background images. In the adaptive target-background fusion stage, we propose a dual-discriminator GAN network (FusionGAN), which allows the target mask to be adaptively fused with the background pixels. By combining real targets with generated backgrounds, new infrared small target images are generated, achieving the goal of data augmentation. Experiments conducted across three different scenarios demonstrate that the proposed data augmentation scheme effectively enhances the performance of both traditional and advanced detection models.},
  archive      = {J_TMM},
  author       = {Hongwei Ding and Nana Huang and Yaoxin Wu and Xiaohui Cui},
  doi          = {10.1109/TMM.2025.3613079},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving infrared small target detection with GAN-driven data augmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked text pre-training for scene text detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training has greatly boosted scene text detection methods by learning the representation of text. However, they still suffer from two drawbacks: 1) The learned representation for text is not discriminative due to the insufficient annotated real data and the domain gap between synthetic data. 2) Existing methods perform poorly on text lacking of visual information (e.g. occluded text). To address them, this paper explores the potential of the CLIP model and proposes a novel self-supervised pre-training network with masked text modeling (MTM) and text knowledge distillation (TKD), which aims at obtaining discriminative representation for text. First, a Text Perception Module is proposed to perceive coarse text area under an unsupervised manner. Second, we design a Text-aware Masking Strategy to mask the text area with a certain ratio and reconstruct the masked texts by the MTM Module. Compared to randomly pixel-level masking in classic masked image modeling, we perform a targeted text-aware masking and reconstruction. MTM obtains linguistic reasoning ability of text occlusion with reconstruction of masked text. Besides, to better utilize the multimodal knowledge of text in CLIP model, this paper devises a TKD Module to guide the representation learning of masked texts in semantic level. This robust feature extraction learned by reconstructing masked text and knowledge distillation ensures a more discriminative representation for text. Extensive experiments on four challenging datasets verify the effectiveness and superiority of our pre-training method. Specifically, our method achieves F-measure of $\bf{86.5\%}$, $\bf{87.1\%}$ and $\bf{88.5\%}$ for DBNet++ on CTW1500, Total-Text and MSRA-TD500 respectively. Dataset and code are available at https://github.com/rangek/MTP.},
  archive      = {J_TMM},
  author       = {Hongtao Xie and Keran Wang and Bangbang Zhou and Yuxin Wang and Weigang Qi and Jie Wu and Yadong Qu and Zuan Gao and Dongming Zhang and Yizhi Liu},
  doi          = {10.1109/TMM.2025.3613181},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Masked text pre-training for scene text detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware diffusion-enhanced multimedia recommendation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia recommendations aim to use rich multimedia content to enhance historical user-item interaction information, which can not only indicate the content relatedness among items but also reveal finer-grained preferences of users. In this paper, we propose a Knowledge-aware Diffusion-Enhanced architecture using contrastive learning paradigms (KDiffE) for multimedia recommendations. Specifically, we first utilize original user-item graphs to build an attention-aware matrix into graph neural networks, which can learn the importance between users and items for main view construction. The attention-aware matrix is constructed by adopting a random walk with a restart strategy, which can preserve the importance between users and items to generate aggregation of attention-aware node features. Then, we propose a guided diffusion model to generate strongly task-relevant knowledge graphs with less noise for constructing a knowledge-aware contrastive view, which utilizes user embeddings with an edge connected to an item to guide the generation of strongly task-relevant knowledge graphs for enhancing the item's semantic information. We perform comprehensive experiments on three multimedia datasets that reveal the effectiveness of our KDiffE and its components on various state-of-the-art methods. Our source codes are available.},
  archive      = {J_TMM},
  author       = {Xian Mo and Fei Liu and Rui Tang and Jintao Gao and Hao Liu},
  doi          = {10.1109/TMM.2025.3613108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-aware diffusion-enhanced multimedia recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tackling ambiguity from perspectives of uncertainty inference and affinity diversification for weakly supervised semantic segmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve dense predictions without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, particularly during the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, is widely hindered by ambiguity. Despite this, this issue has received little attention in previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspectives of uncertainty inference and affinity diversification. When activating class objects, we argue that the false activation stems from the bias to ambiguous regions during the feature extraction. Therefore, we formulate a robust feature representation with a Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be overly similar among ambiguities. To this end, we design an affinity diversification module to promote diversity among semantics. A mutual complementing refinement is first proposed to statically rectify the ambiguous affinity with multiple inferred pseudo labels. Then a contrastive affinity loss is further designed to dynamically diversify the relations among unrelated semantics. It stably propagates the diversity into the feature representation and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and its superiority over recent single-staged or even most multi-staged competitors. Code is publicly available at https://github.com/zwyang6/UniA.},
  archive      = {J_TMM},
  author       = {Zhiwei Yang and Yucong Meng and Kexue Fu and Shuo Wang and Zhijian Song},
  doi          = {10.1109/TMM.2025.3613165},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tackling ambiguity from perspectives of uncertainty inference and affinity diversification for weakly supervised semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A UNet-like transformer network for camouflaged object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of Camouflaged Object Detection (COD) is to identify the objects that integrate seamlessly with the surrounding environment. Due to the high intrinsic similarity between the objects and their background, this task presents greater challenges than traditional object detection. Most existing COD methods often have a large number of parameters and high computational complexity in the pursuit of detection accuracy, which hinders the application of COD in practical scenarios. To address this issue, we propose a UNet-like Transformer Network for COD, termed UTNet, which achieves competitive detection accuracy with a smaller parameter set. Specifically, we propose a Camouflaged Region Awareness Module (CRAM) consisting of a Hierarchical Attention Mechanism (HAM) that groups features to reveal intrinsic consistency between sub-features. This CRAM can be embedded into the backbone network, giving it powerful modeling capabilities. And, we present a Contextual Knowledge Collector (CKC) that exploits a cross-aggregation approach for neighboring feature layers, promoting the flow of semantic information from high-level to low-level features, and ensuring the integrity of camouflaged objects at each level of features. Furthermore, we introduce a progressive decoder that utilizes a cascade of attention units to filter noise and explores knowledge aggregation to emphasize features from different levels, ensuring that camouflaged objects have complete spatial details at the local level. Extensive experimental results show that UTNet achieves competitive results compared to 20 state-of-the-art methods. Codes and results are released on https://github.com/hjy0518/UTNet.},
  archive      = {J_TMM},
  author       = {Fuming Sun and Jinyu Han and Weiyi Wu and Jing Sun and Mengyin Wang},
  doi          = {10.1109/TMM.2025.3613076},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A UNet-like transformer network for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed two-tier cache optimization in metaverse scenarios combining MADDPG and GCN. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid emergence of the Metaverse requires higher network throughput and lower latency to deliver immersive and responsive virtual experiences. Traditional centralized data processing approaches are constrained by limited computational and bandwidth resources when handling large-scale user data. A Cloud-Edge-End transmission architecture is proposed in this study, tailored for Metaverse scenarios to optimize resource allocation, minimize latency, and enhance rendering efficiency. A real-time trajectory segment prediction scheme (FDK) was developed, which combines FastDTW with K-means by leveraging user behavior trajectories to determine subscene popularity and store them on GPU servers, thereby reducing user wait time. A two-tier cache optimization scheme (MAE2C) is also proposed, incorporating GCN for subscene feature identification. GPU servers employ the MADDPG strategy to cache popular subscenes, while edge servers utilize DDPG to cache missed scenes. This approach effectively reduces cloud access and cache replacement frequency. Simulation results demonstrate that the subscene cache hit rate of the MAE2C scheme significantly outperforms existing methods across various cache capacities, with a 6.9% reduction in cache replacement frequency. This research provides effective technical support for Metaverse scene rendering and offers insights into the development of generative Metaverse systems.},
  archive      = {J_TMM},
  author       = {Zheng Wan and Shenglu Zhao and Xiaogang Dong and Xuelin Liu and Yifeng Tan and Yuming Fang},
  doi          = {10.1109/TMM.2025.3613168},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distributed two-tier cache optimization in metaverse scenarios combining MADDPG and GCN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep no-reference quality assessment for underwater enhanced images. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of underwater image enhancement (UIE) is to boost the acquired underwater image quality, which increases the value of the underwater image significantly. However, without effective underwater enhanced image quality assessment (UEIQA) measures that benchmark the UIE, the process of UIE becomes driftless and the enhanced results of different UIE algorithms cannot be fairly compared. Toward this end, we in this work construct a dedicated UEIQA scheme on the basis of deep investigation of the underwater enhanced image characteristics. Specifically, in our proposed method, we respectively design deep neural networks to represent the unique attributes of the underwater enhanced image, such as color cast, local distortions, naturalness degree, sharpness, contrast, fog density, etc., that are highly correlated with the image quality. Then we introduce the Vision Transformer (ViT) to capture the dependencies among different image attributes and infer the image quality level. Extensive experiments conducted on three typical UEIQA databases, i.e., SOTA, UID2021 and SAUD, show that the proposed UEIQA model yields noteworthy higher prediction accuracy than the representative IQA and UEIQA metrics, e.g., achieving SRCC values of 0.891 ( vs. 0.749 in SAUD) and 0.933 ( vs. 0.798 in UID2021). The proposed UEIQA model will be released at https://github.com/YT2015?tab=repositories.},
  archive      = {J_TMM},
  author       = {Yutao Liu and Baochao Zhang and Runze Hu and Ke Gu and Guangtao Zhai and Junyu Dong},
  doi          = {10.1109/TMM.2025.3613105},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep no-reference quality assessment for underwater enhanced images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unrevealed threats: Adversarial robustness analysis of underwater image enhancement models. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based methods for underwater image enhancement (UWIE) have undergone extensive exploration. However, learning-based models are usually vulnerable to adversarial examples so as the UWIE models. To the best of our knowledge, there is no comprehensive study on the adversarial robustness of UWIE models, which indicates that UWIE models are potentially under the threat of adversarial attacks. In this paper, we propose a general adversarial attack protocol. We make a first attempt to conduct adversarial attacks on five well-designed UWIE models on three common underwater image benchmark datasets. Considering the scattering and absorption of light in the underwater environment, there exists a strong correlation between color correction and underwater image enhancement. On the basis of that, we also design two effective UWIE-oriented adversarial attack methods, Pixel Attack and Color Shift Attack targeting different color spaces. The results show that five models exhibit varying degrees of vulnerability to adversarial attacks and well-designed small perturbations on degraded images are capable of preventing UWIE models from generating enhanced results. In addition, we conduct adversarial training on these models and successfully mitigated the effectiveness of adversarial attacks. In summary, we reveal the adversarial vulnerability of UWIE models and propose a new evaluation dimension of UWIE models.},
  archive      = {J_TMM},
  author       = {Siyu Zhai and Zhibo He and Xiaofeng Cong and Junming Hou and Jie Gui and Jian Wei You and Xin Gong and James Tin-Yau Kwok and Yuan Yan Tang},
  doi          = {10.1109/TMM.2025.3613152},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unrevealed threats: Adversarial robustness analysis of underwater image enhancement models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-supervised asymmetric co-training for semi-supervised medical domain generalization. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudo-labels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.},
  archive      = {J_TMM},
  author       = {Jincai Song and Haipeng Chen and Jun Qin and Na Zhao},
  doi          = {10.1109/TMM.2025.3613080},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-supervised asymmetric co-training for semi-supervised medical domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BTDGNet: A dual-guided camouflaged object detection network leveraging boundary and texture information. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection aims to identify objects that blend seamlessly with their background, posing a greater challenge compared to general object detection tasks. Due to its ability to recognize camouflaged objects, such detection models hold significant practical value across various fields. To accurately identify camouflaged targets in various complex environments, we designed a dual-guided camouflaged object detection network based on boundary and texture information(BTDGNet). The process consists of two main stages. The first stage is the localization stage, which leverages a convolutional neural network (CNN) to capture boundary and texture information of objects. These features are then fused to achieve coarse localization of the camouflaged objects. In the second stage, the recognition stage, we employ a Transformer to extract global information from the image, enhancing the differentiation between foreground and background. An interactive fusion module is designed to fully exploit and integrate both global and local features, producing precise prediction images. By leveraging boundary and texture information, the model's adaptability to different camouflaged objects is improved. The integration of local and global features enhances the model's detection accuracy from various perspectives, ultimately building a camouflaged object detection model suitable for a wide range of complex scenarios. The proposed method was extensively compared with other state-of-the-art methods across four public datasets, and the results demonstrated superior performance. Furthermore, benefiting from our dual-guidance strategy that leverages both texture and boundary information, our model demonstrates robust performance. We conducted tests on detection tasks across four different domains, and the results confirm that our model can accurately segment camouflaged objects in complex scenes.},
  archive      = {J_TMM},
  author       = {Xiaogang Song and Pengfei Zhang and Xiaochang Li and Xinhong Hei and Rongrong Liu},
  doi          = {10.1109/TMM.2025.3613150},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BTDGNet: A dual-guided camouflaged object detection network leveraging boundary and texture information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled denoising and counterfactual balance for multimodal recommendation. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional network-based dual-view multimodal recommendation methods have achieved great success. They extract multimodal and behavior features based on item-item and user-item graphs, respectively. However, they still have two- fold limitations. First, the relevance between multimodal semantics and user preferences is ignored, resulting in the propagation and coupling of preference-irrelevant noise. Second, the direct use of uneven factual user-item graphs is suboptimal, as both redundant noisy edges and missing positive interaction edges impair recommendations. To solve the above issues, we propose a DisentAngled deNoising and Counterfactual balancE method for multimodal recommendation, dubbed as DANCE. Specifically, for multimodal features, we explicitly disentangle them into preference-relevant and preference-irrelevant representations, to absorb and discard irrelevant noise via the latter. An orthogonal regularization and a contrastive learning task on preference relevance score prediction are proposed as the dual safeguard to prevent preference-relevant representations from encoding irrelevant noise. For behavior feature extraction, we construct a balanced user-item graph by integrating factual and counterfactual graphs. In this process, we pre-train a behavior simulator to build the counterfactual graph with full interactions. Top-$K$ sampling is adopted to omit noisy edges and add missing edges in the graph. The final recommendation is performed upon the fused representation of preference-relevant multimodal and behavior representations. Extensive experiments on three public datasets verify the power of our DANCE.},
  archive      = {J_TMM},
  author       = {Xin Wen and Weizhi Nie and Jing Liu and Ning Zhang and Yuting Su and An-An Liu},
  doi          = {10.1109/TMM.2025.3613112},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangled denoising and counterfactual balance for multimodal recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeRegNet: Edge feature-based multimodal registration network between images and LiDAR point clouds. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal data registration has long been a critical task in computer vision, with extensive applications in autonomous driving and robotics. Accurate and robust registration methods are essential for aligning data from different modalities, forming the foundation for multimodal sensor data fusion and enhancing perception systems' accuracy and reliability. The registration task between 2D images captured by cameras and 3D point clouds captured by Light Detection and Ranging (LiDAR) sensors is usually treated as a visual pose estimation problem. High-dimensional feature similarities from different modalities are leveraged to identify pixel-point correspondences, followed by pose estimation techniques using least squares methods. However, existing approaches often resort to downsampling the original point cloud and image data due to computational constraints, inevitably leading to a loss in precision. Additionally, high-dimensional features extracted using different feature extractors from various modalities require specific techniques to mitigate cross-modal differences for effective matching. To address these challenges, we propose a method that uses edge information from the original point clouds and images for cross-modal registration. We retain crucial information from the original data by extracting edge points and pixels, enhancing registration accuracy while maintaining computational efficiency. The use of edge points and edge pixels allows us to introduce an attention-based feature exchange block to eliminate cross-modal disparities. Furthermore, we incorporate an optimal matching layer to improve correspondence identification. We validate the accuracy of our method on the KITTI and nuScenes datasets, demonstrating its state-of-the-art performance. Our code is publicly available on GitHub at https://github.com/ESRSchao/EdgeRegNet.},
  archive      = {J_TMM},
  author       = {Yuanchao Yue and Hui Yuan and Qinglong Miao and Xiaolong Mao and Raouf Hamzaoui and Peter Eisert},
  doi          = {10.1109/TMM.2025.3613107},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EdgeRegNet: Edge feature-based multimodal registration network between images and LiDAR point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiftFormer: Lifting and frame theory based monocular depth estimation using depth and edge oriented subspace representation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation (MDE) has attracted increasing interest in the past few years, owing to its important role in 3D vision. MDE is the estimation of a depth map from a monocular image/video to represent the 3D structure of a scene, which is a highly ill-posed problem. To solve this problem, in this paper, we propose a LiftFormer based on lifting theory topology, for constructing an intermediate subspace that bridges the image color features and depth values, and a subspace that enhances the depth prediction around edges. MDE is formulated by transforming the depth value prediction problem into depth-oriented geometric representation (DGR) subspace feature representation, thus bridging the learning from color values to geometric depth values. A DGR subspace is constructed based on frame theory by using linearly dependent vectors in accordance with depth bins to provide a redundant and robust representation. The image spatial features are transformed into the DGR subspace, where these features correspond directly to the depth values. Moreover, considering that edges usually present sharp changes in a depth map and tend to be erroneously predicted, an edge-aware representation (ER) subspace is constructed, where depth features are transformed and further used to enhance the local features around edges. The experimental results demonstrate that our LiftFormer achieves state-of-the-art performance on widely used datasets, and an ablation study validates the effectiveness of both proposed lifting modules in our LiftFormer.},
  archive      = {J_TMM},
  author       = {Shuai Li and Huibin Bai and Yanbo Gao and Chong Lv and Hui Yuan and Chuankun Li and Wei Hua and Tian Xie},
  doi          = {10.1109/TMM.2025.3613146},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LiftFormer: Lifting and frame theory based monocular depth estimation using depth and edge oriented subspace representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLOSS: Global-local matching network towards outfit recommendation for diverse body shapes and scenes. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of individuals' living standards has transformed clothing preferences, elevating fashion beyond mere utility to a potent means of self-expression. However, the intricate task of outfit selection persists as a challenge, marked by traditional methods facing challenges such as the oversight of combined factors of scene and body shape, insufficient emphasis on detail-oriented matching, and overreliance on rigid hierarchical structures. To tackle these challenges, this article introduces a novel model, termed Global-Local matching network towards Outfit recommendation for diverse body Shapes and Scenes (GLOSS). Specifically, we first introduce a newly compiled fashion dataset, StreetFashion, to capture the combined factors of body shapes and scene characteristics. Additionally, we develop innovative multi-level globality- and locality-aware matching methods to enhance the accuracy of outfit recommendations by comprehensively considering both global and local relationships among clothing items, outfits, users, and scenes. Furthermore, we develop a personalized outfit heterogeneous graph that incorporates historical interactions among fashion entities, enabling effective modeling of nonstrict hierarchical relationships. Evaluation conducted on both our collected dataset and an adapted existing dataset demonstrates the effectiveness of our proposed approach in outfit recommendation. Our codes are released at: https://github.com/ChenrMa/GLOSS.},
  archive      = {J_TMM},
  author       = {Chenrui Ma and Huiyue Sun and Jianghong Ma and Haijun Zhang and Yuxin Ding and Zhao Zhang},
  doi          = {10.1109/TMM.2025.3613163},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GLOSS: Global-local matching network towards outfit recommendation for diverse body shapes and scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Propagation based recycling contrastive learning for coupled noisy visible-infrared person re-identification. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a crucial role in round-the-clock security surveillance systems, aiming to detect consistent identity recognition across transitions from day to night. A significant challenge in this field is the variation in the appearance of the same identity across visible and infrared modalities, which often leads to coupled noisy labels, referring to both Noisy Annotation (NA) and Noisy Correspondence (NC). Therefore, learning noisy-tolerant and discriminative representations is the primary objective in VI-ReID. However, existing research typically faces two principal limitations: (1) Learning strategies for noisy labeled scenarios usually rely on analyzing the distribution of loss response while ignoring the rich semantic information from neighboring samples. (2) When dealing with identified noisy samples, most previous approaches usually employ filtering strategies to mitigate the impact of noisy samples but fail to consider the valuable information in the noisy samples. To address these challenges, we propose a Propagation based Recycling Contrastive Learning (PRCL) approach. This method utilizes a label propagation strategy to distinguish clean annotations to learn identity-wise semantic information and recycles filtered noisy samples to capture the geometric-wise representation. Thus, even in the presence of noisy labels, the method can help learn robust representations across visible and infrared modalities. Specifically, we design a Noisy-aware Heterogeneous Graph Propagation module, which identifies noisy samples by aggregating the effects of neighboring labels using a graph propagation strategy. In addition, we develop a Cross Modality Recycling Debiased Contrastive Learning algorithm, which leverages the identity-wise information from clean samples and geometry-wise information from noisy samples. This approach utilizes identity-wise and geometric-wise information to mitigate the effect of noisy labels and retain as much valuable information as possible. Extensive experiments on two VI-ReID benchmark datasets demonstrate that our proposed method achieves highly competitive performance.},
  archive      = {J_TMM},
  author       = {Yongxi Li and Wenzhong Tang and Shuai Wang and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3613106},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Propagation based recycling contrastive learning for coupled noisy visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level contrastive learning for multimodal sentiment analysis. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis has garnered increasing attention. The bulk of existing work in multimodal sentiment analysis primarily focuses on designing various networks to align and subsequently fuse representations from individual modalities. Contrastive learning, recognized for its intrinsic alignment capabilities, has also been extensively applied in multimodal sentiment analysis. However, current contrastive learning methods are often limited to pairwise modalities and typically perform contrastive learning prior to modality fusion, neglecting the consistency of interactions across multiple modalities. Moreover, they overlook the overall consistency within samples. To address these issues, we introduce a novel Multi-Level Contrastive Learning (MLCL) framework for multimodal sentiment analysis, composed of Uni-Modal Contrastive Learning (UMCL), Bi-Modal Contrastive Learning (BMCL) and Tri-Modal Contrastive Learning (TMCL). UMCL enhances intra-modal representations by creating positive pairs using modality-specific random dropout, while BMCL leverages the asymmetry of attention mechanisms, using two directional attentions as positive samples. TMCL aligns non-overlapping uni-modal and bi-modal representations, underscoring the complementarity of tri-modal information. The effectiveness of MLCL is demonstrated through its performance on multiple datasets. Our comprehensive experiments across multiple datasets demonstrate the superiority of the MLCL framework, which achieves new state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Yan Zhuang and Wei Bai and Yanru Zhang and Jiawen Deng and Zheng Hu and Xiaoyue Zhang and Fuji Ren},
  doi          = {10.1109/TMM.2025.3613116},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-level contrastive learning for multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Critical contour prior-guided graph learning with pose calibration for identity-aware deepfake detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake has recently raised severe public concerns about security issues, such as creating fake news of celebrities. As countermeasures, identity-aware detection methods leverage identity information to expose forged videos by measuring identity consistency between the suspicious input and its reference samples. However, the performance of existing methods suffers from notable degradation due to undesired variations of head poses and capturing environments. In this work, we first conduct a statistical analysis to illustrate the influence of different facial regions for forensic purposes, which infers more reliable identity information is located in critical face regions. Motivated by this analysis, we propose a graph learning-based identity-aware deepfake detection framework considering critical contour prior as guidance. First, feature sampling based on contour landmarks is applied to construct the graph data as the input of our critical contour prior-guided graph attention network (CP-GAT), where a node position prediction task is constructed as auxiliary supervision to explore rich relationships between nodes. To enhance pose-invariant ability, a rotation compensation block is integrated into CP-GAT and trained using a pose-calibrated contrastive learning to extract identity features, which takes high-quality front faces as the calibration goal with a progressively updating selection. Besides, an adversarial node masking-based training strategy is proposed as feature augmentation to further enhance the reliability. During the inference stage, the similarity between identity features of the input sample and its reference samples extracted by the trained CP-GAT is used to obtain the detection result. Extensive experiments are conducted on various face forgery datasets and state-of-the-art methods are compared to verify the superiority of the proposed method in terms of detection capability and robustness.},
  archive      = {J_TMM},
  author       = {Liyue Ming and Peisong He and Haoliang Li and Shiqi Wang and Xinghao Jiang},
  doi          = {10.1109/TMM.2025.3613159},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Critical contour prior-guided graph learning with pose calibration for identity-aware deepfake detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSDP: Real-time asymmetric dual-stream instance segmentation embedding depth-predictive architecture for enhanced scene understanding. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation can help vehicles or robots enhance their understanding of a scene through the pixel-level segmentation of different objects. However, occlusion and boundary blur, especially in cases with similar colors or textures, are still challenges encountered in real-time robust segmentation tasks. To segment a complete instance boundary, the existing 2D approaches fuse local and abstract semantic features derived from the color domain, which leads to homogeneous semantic information, and efficiently separating different objects is difficult in some cases. To address these complicated scenes, inspired by a human prediction processing strategy, where “the brain fills in missing information in advance to help make better decisions”, this study proposes a real-time asymmetric dual-stream instance segmentation algorithm embedding a depth-predictive architecture that provides the covisible depth information of objects. Furthermore, a cross-domain data fusion method and an enhancement-decoupling loss are designed to complement RGB data by utilizing the rich foreground and boundary details of the predicted depth map. In addition, our model can be fine-tuned to integrate it with real depth domain data provided by different input devices. Extensive experiments conducted on the COCO, OCHuman and CityScapes datasets demonstrate the effectiveness of our method. We further deployed our DSDP method on a UAV platform for validation purposes and qualitatively confirmed its validity.},
  archive      = {J_TMM},
  author       = {Mingyu Chen and Qiang Li and Weizhi Nie and Jing Liu and Jingjing Geng and Yongtao Ma and Xin Guan},
  doi          = {10.1109/TMM.2025.3613161},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSDP: Real-time asymmetric dual-stream instance segmentation embedding depth-predictive architecture for enhanced scene understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCTFormer: A dual-branch transformer with cloze tests for video anomaly detection. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is of critical importance in safety-critical scenarios. The key challenge is to effectively capture the spatio-temporal features of videos and learn normal patterns from the training data. However, existing methods often fall short in modelling intra-channel and inter-channel correlations as well as dynamic dependencies between video frames, leading to challenges in model robustness and generalization. To address these issues, we propose DCTFormer, a dual-branch framework that integrates both RGB and optical flow branches to handle Video Anomaly Detection. Firstly, we design a novel module TRAECT (Transformer-based Residual Autoencoder with Cloze Tests), which incorporates high-level semantics and temporal context information to improve the spatio-temporal relationships learning ability by capturing intra-channel and inter-channel correlations. More importantly, conditioned on the RGB branch, we propose a new optical flow completion approach incorporating richer motion dynamics to learn dynamic dependencies between video frames and optical flows through a conditional variational autoencoder. At last, we introduce an ensemble strategy to compute anomaly scores for both branches, and thus fully exploit the branches modality information. The experimentation on three challenging benchmark datasets evinces the efficacy of our framework, which outperforms current state-of-the-art approaches with regard to anomaly detection performance.},
  archive      = {J_TMM},
  author       = {Pengzhan Chen and Shengdong Du and Xiaole Zhao and Jie Hu and Jingjing Li and Tianrui Li},
  doi          = {10.1109/TMM.2025.3613082},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCTFormer: A dual-branch transformer with cloze tests for video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive diversity augmentation for single domain generalization. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single domain generalization aims to train a model on a single source domain that generalizes to unseen target domains, which is critical in multimedia applications. Current methods typically use adversarial data augmentation to enrich the source domain distribution with novel samples. However, these methods typically rely on labeled data and require adversarial training between generators and classifiers, which may limit sample diversity and introduce spurious correlations. To tackle these problems, we propose a method that integrates Contrastive clustering regularization with an Unsupervised Diversity Augmentation (UDA), termed C-UDA. Specifically, UDA is a flexible and general framework in which two customized models iteratively optimize a novel adversarial loss to enable fully unsupervised data augmentation. Within UDA, we design a lightweight generator that diversifies each input image along three distinct visual attributes. Based on both original and augmented images, we further introduce contrastive clustering regularization to encourage the model to learn domain-invariant representations, resulting in robust decision boundaries. Extensive experiments on four challenging benchmarks demonstrate that C-UDA significantly outperforms 22 state-of-the-art methods. The source code is available at https://github.com/Ruiding1/C-UDA.},
  archive      = {J_TMM},
  author       = {Rui Ding and Kehua Guo and Huiling Chen and Xiangyuan Zhu},
  doi          = {10.1109/TMM.2025.3613123},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive diversity augmentation for single domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust temporal action localization with meta boundary refinement. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Action Localization (TAL) aims to localize the start and end timestamps of actions with specific categories in untrimmed videos. Despite great success, noisy action boundary labels may be included due to the inherent subjectivity of manual annotations. This can lead TAL models to learn inaccurate action boundaries during training, potentially impairing their localization performance. To systematically analyze and enhance the TAL models' robustness against noisy action boundary labels, we introduce a new task termed TAL with Noisy Label. We demonstrate that introducing even minimal random noise to action boundary labels in training data can substantially degrade the performance of leading TAL methods, thereby underscoring their vulnerability to noisy action boundary labels. To be specific, we propose a novel plug-and-play method called Energy-based Meta Boundary Refinement (EMBR), where a meta-learning pipeline is employed to rectify noisy action boundary labels, ameliorating the misguidance of noisy labels on model training. Under this meta-learning pipeline, EMBR utilizes an energy function to calculate the magnitude of label noise and re-weights samples, assigning lower weights to samples with higher noise, alleviating the impact of noisy samples on model training. In addition, considering the energy difference between action and background segments, an energy-based loss function is proposed to achieve larger energy differences across the boundary, assisting in the boundary refinement. Experimental results on the THUMOS14, ActivityNet1.3, and HACS datasets demonstrate the effectiveness of EMBR in enhancing the robustness of TAL models.},
  archive      = {J_TMM},
  author       = {Jiahua Li and Kun Wei and Zhe Xu and Liejun Wang and Cheng Deng},
  doi          = {10.1109/TMM.2025.3613078},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust temporal action localization with meta boundary refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revealing directions for text-guided 3D face editing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face editing is a significant task in multimedia, aimed at the manipulation of 3D face models across various control signals. The success of 3D-aware GAN provides expressive 3D models learned from 2D single-view images only, encouraging researchers to discover semantic editing directions in its latent space. However, previous methods face challenges in balancing quality, efficiency, and generalization. To solve the problem, we explore the possibility of introducing the strength of diffusion model into 3D-aware GANs. In this paper, we present Face Clan, a fast and text-general approach for generating and manipulating 3D faces based on arbitrary attribute descriptions. To achieve disentangled editing, we propose to diffuse on the latent space under a pair of opposite prompts to estimate the mask indicating the region of interest on latent codes. Based on the mask, we then apply denoising to the masked latent codes to reveal the editing direction. Our method offers a precisely controllable manipulation method, allowing users to intuitively customize regions of interest with the text description. Experiments demonstrate the effectiveness and generalization of our Face Clan for various pre-trained GANs. It offers an intuitive and wide application for text-guided face editing that contributes to the landscape of multimedia content creation. Our project page: https://windlikestone.github.io/Face_clan_website/.},
  archive      = {J_TMM},
  author       = {Zhuo Chen and Yichao Yan and Sehngqi Liu and Yuhao Cheng and Weiming Zhao and Lincheng Li and Mengxiao Bi and Xiaokang Yang},
  doi          = {10.1109/TMM.2025.3604978},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Revealing directions for text-guided 3D face editing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferential and commonsense visual question generation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Visual Question Generation (VQG) task generally aims to produce questions based on images in natural language. Existing studies often handle VQG as a reverse Visual Question Answering (VQA), training data-driven generators on VQA datasets. However, this solution pipeline struggles to generate high-quality questions that effectively challenge robots and humans, even by leveraging the most advanced large-scale foundational models. There are also some other VQG methods depending on elaborate and costly manual preprocessing heavily. To address these limitations, we propose a novel method with a two-module framework for automatically generating inferential visual questions that also follow commonsense. The “Scene Graph Generation” module constructs specialized scene graphs by progressively expanding connections from high-confidence nodes. This module ensures semantic consistency by aligning visual, textual, and salient features. Additionally, we incorporate external knowledge to extend abstract semantic concepts and associated facts, enriching the content of generated questions and facilitating the generated question to better follow the commonsense of human. Another module “Question Generation” utilizes the above scene graph as a foundation to search and instantiate for the question. The generated questions will match with the program templates and have diverse inferential paths. Experimental results demonstrate that our method is both effective and highly scalable. The generated questions are controllable in terms of semantic richness and difficulty, exhibiting clear inferential and commonsense properties. Furthermore, we automatically utilize our method to create a large-scale dataset, ICVQA, which includes approximately 160,000 images and 800,000 questionanswer pairs, thereby facilitating further research in VQA and visual dialogue.},
  archive      = {J_TMM},
  author       = {Chao Bi and Shuhui Wang and Na Li and Qingming Huang},
  doi          = {10.1109/TMM.2025.3604975},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Inferential and commonsense visual question generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection difference-guided geometry quality enhancement for video-based point cloud compression. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based point cloud compression (V-PCC) developed by MPEG has achieved remarkable compression efficiency for dynamic point clouds. However, the point clouds compressed by V-PCC still suffer from serious artifacts due to the lossy compression and lose a large number of points. In this paper, we propose a new geometry quality enhancement method for the V-PCC compressed point clouds and it can effectively recover the lost points. Our method is applied to the 2D projected near and far frames rather than 3D point clouds. It is designed to enhance the quality of 2D frames, guided by the predicted difference information between them. More specifically, we firstly construct a gradient-based difference prediction network (G-DPnet) to predict the difference between near and far frames. This difference is introduced in the enhancement of 2D frames, for the recovery of the lost 3D points. Meanwhile, we propose the single-frame quality enhancement network (SFQEnet) to separately enhance near and far frames. The enhanced frames are then used to produce the near-far frame difference with G-DPnet. After obtaining the difference, we feed it into a dualframe quality enhancement network (DFQEnet) to guide the further enhancement of near and far frames. Experimental results demonstrate that our method can effectively recover a large number of lost points and improve the quality of point clouds compressed by V-PCC.},
  archive      = {J_TMM},
  author       = {Yu Liu and Jingwei Bao and Zeliang Li and Qiang Zhu and Shuyuan Zhu and Siu-Kei Au Yeung and Bing Zeng},
  doi          = {10.1109/TMM.2025.3613122},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Projection difference-guided geometry quality enhancement for video-based point cloud compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTRec: Cross-modal learning for multimodal recommendation via optimal transport. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in multimodal recommendation systems due to the rapid growth of multimedia and the explosion of information. Despite notable advancements, current models often fuse multimodal embeddings with ID (name or concept) embeddings in a weighted or concatenated manner for items. Under this circumstance, they may overlook the heterogeneity problem between different modalities, and lack theoretical guarantees, potentially leading to suboptimal item representations. To overcome this challenge, we introduce a novel model named OTRec, which employs optimal transport (OT) to align heterogeneous multimodal embeddings with ID embeddings. Specifically, OTRec captures co-occurrence features across modalities and distinctive features within modalities, enabling the formation of the unified representation from both modalinvariant and modal-specific perspectives. This dual strategy ensures a comprehensive alignment of heterogeneous multimodal data, significantly improving the accuracy of capturing user preferences. Additionally, traditional recommendation models typically match an item's ID with its multimodal data as positive samples for contrastive learning, neglecting the potential complementary information from other items' multimodal data. To address this issue, we introduce a semanticenhanced contrastive learning module, which can learn latent semantic correlations across items by a semantic-similarity weighting matrix. It can be integrated as a plug-in for other models to effectively explore latent semantics. On top of this, we provide theoretical guarantees that demonstrate the effectiveness of OTRec in aligning multimodal and ID information and in enhancing the mutual information between them. Extensive evaluations on three public datasets illustrate OTRec's effectiveness and achieve state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Zongsheng Cao and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TMM.2025.3607735},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OTRec: Cross-modal learning for multimodal recommendation via optimal transport},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint deep-unfolding optimization learning for depth map arbitrary-scale super-resolution. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color-guided Depth map Super-Resolution (DSR) based on Convolutional Neural Networks (CNN) is a crucial technology to remedy the defects of mainstream commercial depth cameras and has made significant progress in recent years. Nevertheless, this technology is inevitably facing some huge challenges. Firstly, existing CNN-based DSR methods are designed as black-box network architectures. Secondly, few approaches study single model to achieve arbitrary-scale DSR. Thirdly, due to structural inconsistency between dual-modality, color-guided DSR methods always face texture-copying issue. To this end, we propose a novel joint DSR and high-low frequency decomposition optimization model and this model is unfolded into Deep Arbitrary-Scale Unfolding Network (DASU-Net). DASUNet can achieve robust continuous representation ability by alternately-iterative updating of high-low frequencies and depth features. More importantly, Arbitrary-scale Up-sampling Fusion (AUF) module is proposed to achieve arbitrary-scale up-sampling and dual-modality feature fusion. Specifically, two essential components make up the cores of AUF module including arbitraryscale up-sampling block as well as Feature Enhancement and Multiple Strategies Fusion (FEMSF) blocks. In FEMSF block, color features are first enhanced to highlight its inherentlycorrelated structure with the guidance of depth features, and then the enhanced features are modulated according to different fusion strategies. Furthermore, a fast version of DASU-Net is proposed to fit real-time scenes, named FDASU-Net, which can diminish the runtime by several times for a depth map with a size of 640 × 480 during inference. A large number of experiments can demonstrate that our DASU-Net and FDASUNet can transcend many state-of-the-art DSR methods in terms of several quantitative and qualitative indexes.},
  archive      = {J_TMM},
  author       = {Jialong Zhang and Lijun Zhao and Jinjing Zhang and Anhong Wang and Huihui Bai},
  doi          = {10.1109/TMM.2025.3613083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint deep-unfolding optimization learning for depth map arbitrary-scale super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaptiveFusion: Adaptive multi-modal multi-view fusion for 3D human body reconstruction. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in sensor technology and deep learning have led to significant progress in 3D human body reconstruction. However, most existing approaches rely on data from a specific sensor, which can be unreliable due to the inherent limitations of individual sensing modalities. Additionally, existing multi-modal fusion methods generally require customized designs based on the specific sensor combinations or setups, which limits the flexibility and generality of these methods. Furthermore, conventional point-image projection-based and Transformer-based fusion networks are susceptible to the influence of noisy modalities and sensor poses. To address these limitations and achieve robust 3D human body reconstruction in various conditions, we propose AdaptiveFusion, a generic adaptive multi-modal multi-view fusion framework that can effectively incorporate arbitrary combinations of uncalibrated sensor inputs. By treating different modalities from various viewpoints as equal tokens, and our handcrafted modality sampling module by leveraging the inherent flexibility of Transformer models, AdaptiveFusion is able to cope with arbitrary numbers of inputs and accommodate noisy modalities with only a single training network. Extensive experiments on large-scale human datasets demonstrate the effectiveness of AdaptiveFusion in achieving high-quality 3D human body reconstruction in various environments. In addition, our method achieves superior accuracy compared to state-of-the-art fusion methods.},
  archive      = {J_TMM},
  author       = {Anjun Chen and Xiangyu Wang and Zhi Xu and Kun Shi and Yan Qin and Yuchi Huo and Jiming Chen and Qi Ye},
  doi          = {10.1109/TMM.2025.3613111},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AdaptiveFusion: Adaptive multi-modal multi-view fusion for 3D human body reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning generalization from various unaware degradations for blind hyperspectral image super-resolution via transparent diffusion model. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) super-resolution through the fusion of low-resolution HSI (LrHSI) and high-resolution multispectral image (HrMSI) has emerged as a critical technique for enhancing the quality of HSIs. The recent progress in this field predominantly assume a known mapping relationships between high-resolution HSI (HrHSI) and low-resolution version, relying on networks to learn this mapping to generate HrHSI. However, this assumption is often unrealistic in practical applications. To address this limitation, we propose the Spatial-Spectral-Integrated Transparent Diffusion Model (S2TD) for blind HSI-SR, which is more adaptive to scene-variant degradations with a universal framework for both spatial and spectral reconstruction. Specifically, we design a multi-order degradation pool to generate diverse samples, thereby reducing the distribution gap between low-resolution images in real scenes. Additionally, we develop a spatial-spectral consistent degradation model, which is iteratively solved using an optimization algorithm and unrolled into neural networks for separate restoration in spatial and spectral aspects. Furthermore, the capabilty of progressive reconstruction in the diffusion model is involved to fit various degradations in different dimensions using similar network architectures, thereby enhancing the overall robustness of the network to various and complex scenarios. Comprehensive experiments conducted on three publicly synthetic datasets and one real-world dataset validate the superior performance of the proposed method under the condition that the degradation remains unknown.},
  archive      = {J_TMM},
  author       = {Sen Liu and Jiahui Qu and Song Xiao and Wenqian Dong and Yunsong Li},
  doi          = {10.1109/TMM.2025.3613121},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning generalization from various unaware degradations for blind hyperspectral image super-resolution via transparent diffusion model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truncate diffusion: Efficient video editing with low-rank truncate. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated remarkable capabilities for text-to-video (T2V) editing tasks, relying on fine-tuning for pretrained text-to-image (T2I) diffusion models with only one video-prompt pair. However, conventional fine-tuning approaches require tuning and storing numerous parameters for each video, leading to substantial parameter and memory costs. To mitigate these issues, we propose Truncating Diffusion, an efficient fine-tuning method for video editing that optimizes both parameter and memory usage. Specifically, we propose the Truncating Diffusion module, which is designed with a focus on module architecture and initialization, specifically targeting optimization with a small training set, such as a single video-prompt pair. Theoretical analysis using the Johnson-Lindenstrauss lemma and the Eckart-Young-Mirsky theorem shows that Truncating Diffusion can achieve a minimal Frobenius norm distance to the original attention algorithm with appropriate initialization, which enhances the ease of optimization and improves video editing performance. During fine-tuning, the Truncating Diffusion module integrates seamlessly with the original diffusion model. We freeze the weights of the denoising network within the original pretrained diffusion model, updating only the introduced low-rank alternatives to ensure parameter and memory efficiency. Additionally, we propose Latent Flow Loss and Bidirectional Inter-Frame Attention (BIFA) to improve temporal consistency in synthesized videos. The Latent Flow Loss leverages global temporal information from the input video during training, while BIFA utilizes local temporal information from adjacent frames during inference. These enhancements do not incur additional memory or parameter costs during fine-tuning. Comparisons with state-of-the-art approaches demonstrate that Truncating Diffusion provides superior text alignment, video quality, and inter-frame temporal consistency in video editing. Importantly, Truncating Diffusion requires fine-tuning only 3.2% of the parameters and uses just 62% of the memory compared to the baseline model.},
  archive      = {J_TMM},
  author       = {Bosheng Qin and Wentao Ye and Chi Zhang and Qifan Yu and Wenqiao Zhang and Silang Tang and Yueting Zhuang},
  doi          = {10.1109/TMM.2025.3590901},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Truncate diffusion: Efficient video editing with low-rank truncate},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable graph-guided transformer for point cloud geometry coding. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3598605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention models, particularly Transformers, have significantly advanced deep learning in fields like natural language processing and computer vision by capturing contextual relationships in both sequential and spatial data. This ability is valuable for Point Clouds (PC), which are unstructured sets of points in 3D space. Transformers can effectively identify correlations between distant points, allowing them to focus on the most critical regions of the data. To demonstrate this capability, this paper proposes a novel, scalable Graph-Guided Transformer model, labeled 2GFormer, for static PC geometry. This model is built using a scalable architecture that leverages Graph Convolutions to enhance a Relational Neighborhood SelfAttention (RNSA) base layer model. Both models are integrated into the JPEG Pleno Learning-based Point Cloud Coding (JPEG PCC) standard, resulting in the creation of two attention-enabled codecs for static PC coding: JPEG RNSA and JPEG 2GFormer. While JPEG RNSA codec delivers significant compression improvements for solid and dense PCs compared to the baseline JPEG PCC standard, JPEG 2GFormer extends these gains to solid, dense, and sparse PCs with only a marginal increase in model parameters. Additionally, JPEG 2GFormer outperforms both conventional and learning-based state-of-the-art PC codecs. These results position JPEG 2GFormer as a highly efficient solution for versatile PC coding.},
  archive      = {J_TMM},
  author       = {Mohammadreza Ghafari and André F. R. Guarda and Nuno M. M. Rodrigues and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3598605},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scalable graph-guided transformer for point cloud geometry coding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion-mamba for cross-modality object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality object detection aims to fuse complementary information from different modalities to improve model performance, which achieves a wider range of applications. However, traditional cross-modality fusion methods, based on CNN or Transformer, inadequately address the issue of pseudo-target information, which causes model attention dispersion to degrade object detection performance. In this paper, we investigate a novel cross-modality fusion approach by associating cross-modal features in a hidden state space based on an improved Mamba with a gating attention mechanism. We propose the Fusion-Mamba Block (FMB), designed to map cross-modal features into a hidden state space for interaction, thereby refining the model's attention on true target areas and enhancing overall performance. The FMB comprises two key modules: State Space Channel Swapping (SSCS) module, which facilitates the fusion of shallow features, and Dual State Space Fusion (DSSF) module, which enables deep fusion and effectively suppresses pseudo-target information within the hidden state space. Our proposed method outperforms state-of-the-art approaches, achieving improvements of 5.9%, 3.5% and 2.1% mAP on $M^{3}$FD, DroneVehicle and FLIR-Aligned, respectively. To the best of our knowledge, this work establishes a new baseline for cross-modality object detection, providing a robust foundation for future research in this area.},
  archive      = {J_TMM},
  author       = {Wenhao Dong and Haodong Zhu and Shaohui Lin and Xiaoyan Luo and Yunhang Shen and Guodong Guo and Baochang Zhang},
  doi          = {10.1109/TMM.2025.3599020},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fusion-mamba for cross-modality object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-frame spatiotemporal feature and hierarchical learning approach for no-reference screen content video quality assessment. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid adoption of remote work, online conferencing, and shared-screen collaboration has significantly increased the usage of screen content videos (SCVs), creating a growing need for reliable quality assessment to maintain excellent quality of service. While several full-reference SCV quality assessment (SCVQA) methods have been proposed, their practical application is often limited by the unavailability of reference videos. Existing no-reference SCVQA (NR-SCVQA) methods rely on handcrafted features and focus solely on specific distortions and features, potentially limiting their generalization ability. Moreover, they fail to explore the underlying spatiotemporal information of SCVs, which could hinder their performance. In this work, we propose a novel deep learning-based NR-SCVQA model specifically tailored to capture the comprehensive spatiotemporal features of SCVs to overcome these issues and challenges posed by the SCVQA task. Our approach incorporates a dual-channel spatiotemporal convolutional neural network (DCST-CNN) module to extract both content-aware and edge-aware spatiotemporal quality features, which enables an effective spatiotemporal quality feature representation learning for the downstream SCVQA task. Building upon the DCST-CNN, we further propose a Temporal Pyramid Transformer (TPT) module to fuse spatiotemporal features across multiple temporal scales, enabling the model to capture both short-term and long-term temporal dependencies within an SCV for hierarchical learning. The proposed DCST-CNN and TPT modules work together to provide a robust and accurate NR-SCVQA framework. We conduct experiments on SCVQA databases to validate the effectiveness of our model, which outperforms existing state-of-the-art NR-SCVQA method. The results demonstrate the strength and applicability of our approach in real-world SCVQA tasks.},
  archive      = {J_TMM},
  author       = {Ngai-Wing Kwong and Yui-Lam Chan and Sik-Ho Tsang and Ziyin Huang and Kin-Man Lam},
  doi          = {10.1109/TMM.2025.3599071},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-frame spatiotemporal feature and hierarchical learning approach for no-reference screen content video quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-guided vision-language learning for long-term action quality assessment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term action quality assessment poses a challenging visual task since it requires assessing technical actions at different skill levels in a long video. Recent state-of-the-art methods incorporate additional modality information to aid in understanding action semantics, which incurs extra annotation costs and imposes higher constraints on action scenes and datasets. To address this issue, we propose a Quality-Guided Vision-Language Learning (QGVL) method to map visual features into appropriate fine-grained intervals of quality scores. Specifically, we use a set of quality-related textual prompts as quality prototypes to guide the discrimination and aggregation of specific visual actions. To avoid fuzzy rule mapping, we further propose a progressive semantic learning strategy with a Granularity-Adaptive Semantic Learning Module (GSLM) that refines accurate score intervals from coarse to fine at clip, grade, and score levels. The quality-related semantics we designed are universal to all types of action scenarios without any additional annotations. Extensive experiments show that our approach outperforms previous work by a significant margin and establishes new state-of-the-art on four public AQA benchmarks: Rhythmic Gymnastics, Fis-V, FS1000, and FineFS.},
  archive      = {J_TMM},
  author       = {Huangbiao Xu and Huanqi Wu and Xiao Ke and Yuezhou Li and Rui Xu and Wenzhong Guo},
  doi          = {10.1109/TMM.2025.3599078},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality-guided vision-language learning for long-term action quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coupling and decoupling: Towards temporal feedback for 3D object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection has garnered significant attention within the academic community, primarily due to its broad utility in domains such as autonomous driving and robotics. Prior research efforts have predominantly concentrated on leveraging temporal contextual information embedded within sequential data to enhance the current feature representations. However, a notable limitation of these endeavors lies in their inadequate treatment of the inherent noise present within historical sequences, thereby constraining the efficiency of fusion methods. In this paper, we propose a new temporal feedback network, named TFNet, to model and correct the temporal noise by designing a coupling-decoupling mechanism. Central to our approach are two distinct modules: (i) Foreground Feature Enhancement, which amplifies sparse instance details across temporal frames, thereby furnishing essential local information priors for subsequent fusion; and (ii) Coupling-Decoupling Feature Interaction, designed to first aggregate temporal contextual information and then disentangle fusion features into frame-specific representations. Leveraging a feedback strategy, this module can adaptively enhance useful information and eliminate noise within individual frame features. Empirical evaluations conducted on the nuScenes benchmark demonstrate the effectiveness of TFNet, achieving the new state-of-the-art performance without any bells and whistles.},
  archive      = {J_TMM},
  author       = {Yubo Cui and Zhikang Zou and Xiaoqing Ye and Xiao Tan and Zhiheng Li and Zheng Fang},
  doi          = {10.1109/TMM.2025.3599031},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Coupling and decoupling: Towards temporal feedback for 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oriented-derivative representation for boundary-aware polyp segmentation. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3599039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of colon polyps is important for the prevention of colorectal cancer. Polyp segmentation, however, is still a challenging problem given that recent medical computer-aided equipment suffers from situations of polyp variations in terms of size, color, texture, and poor illuminations in endoscopy videos. These obstacles hinder the prediction of polyp boundaries. Inspired by the observation that the values of pixels on the border region change more sharply than others, we propose the oriented-derivative (OD) representation to capture the relationship between pixels and the boundary region given distance and orientation. To adaptively use the proposed representation in arbitrary frameworks, we design plug-in modules to learn the representation and aggregate features to improve the accuracy of boundary predictions in the polyp segmentation task, which can be implemented in frameworks including the encoder-decoder and top-down architectures. Extensive experimental results show the improvement from the proposed oriented-derivative representation for the polyp segmentation task and the extendibility of our proposed modules in different architectures. Our methods achieved an improvement ranging from 0.3% to 2.5% (mDice) compared with the baseline on five publicly available datasets, including Kvasir, CVC-ClinicDB, EndoScene, CVC-ColonDB, and ETIS.},
  archive      = {J_TMM},
  author       = {Yuke Li and Mengjun Cheng and Xiawu Zheng and Rongrong Ji and Jie Chen},
  doi          = {10.1109/TMM.2025.3599039},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oriented-derivative representation for boundary-aware polyp segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degradation-equivariant representations for robust feature detection and description in low-light environments. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint detection and matching have garnered significant attention, yet remain challenging in low-light environments. Most current studies follow an enhance-then-detect pipeline, which consists of independent enhancers and detectors. While the enhancer focuses on improving the visual quality of low-light images to satisfy human perception standards, the detector prioritizes detection accuracy for machine vision tasks. The unaligned optimization objectives of the enhancer and detector overlook the gap between human and machine vision and lead to sub-optimal performance in low-light keypoint detection. To tackle this problem, a joint enhance-and-detect pipeline is proposed to unify the optimization objectives of enhancement and detection by regarding the improvement of keypoint detection accuracy with enhanced features under machine vision standards. Specifically, we propose a low-light keypoint detection network named DeRFeat, which learns a degradation-equivariant representation between normal and dark domains using AutoEncoding transformation and domain descriptor similarity constraints to indirectly enhance the features from the encoder in the training stage. Then, DeRFeat guides the shared encoder to obtain the degradation-equivariant representations from dark images in the inference stage. With the dark degradation predictions, the encoder is capable of generating equivariant representations between normal and dark domains. The proposed domain descriptor similarity module further aids the encoder in mitigating the impact of dark degradation factors, enabling local descriptors to acquire undisturbed representations. Moreover, a coarse-to-fine point selection strategy is proposed to provide reliable prior keypoints for a globally optimal descriptor construction. Experimental results on four benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art methods under varying low-light conditions.},
  archive      = {J_TMM},
  author       = {Fan Wang and Hengye Lyu and Guanyu Xing and Yanci Zhang and Yanli Liu},
  doi          = {10.1109/TMM.2025.3599075},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Degradation-equivariant representations for robust feature detection and description in low-light environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion-induced saliency shifts in video. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual saliency modelling is of fundamental importance in modern video processing and its applications. Our previous eye-tracking study revealed that signal distortions caused by editing, compression, or transmission alter gaze patterns and consequently induce saliency shifts in both spatial and temporal domains. Saliency shifts provide crucial insights into viewers' behavioural responses to video distortions, facilitating the perception-based optimisation of video algorithms. However, the spatio-temporal saliency shifts and their measurable effects on perception related applications remain largely unexplored. In this paper, we first investigate the measurement of distortion-induced saliency shifts (DSS) in videos and analyse DSS behaviours as functions of video content, time order and critical distortion disruption. Second, based on our findings, we construct three vision models to quantitatively simulate distinct DSS behaviours and integrate them into a comprehensive DSS behaviour model. Finally, we demonstrate that the computational DSS model can enhance emerging video technologies.},
  archive      = {J_TMM},
  author       = {Xinbo Wu and Jianxun Lou and Zhengyan Dong and Fan Zhang and Paul Rosin and Hantao Liu},
  doi          = {10.1109/TMM.2025.3599087},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distortion-induced saliency shifts in video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COFNet: Contrastive object-aware fusion using box-level masks for multispectral object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral object detection, which combines RGB visible light and thermal infrared spectral information, has broad applications in complex environments and varying illumination conditions. However, existing methods face challenges in processing multispectral data, such as inconspicuous object features in spectral images and significant discrepancies between input modality spaces and output detection spaces. To address these issues, we propose an innovative multispectral object detection method that combines contrastive learning and a new cross-modal feature fusion module. We introduce a mask feature contrastive loss that maximizes the similarity between the box-level mask features and modal features while suppressing background responses, enabling effective representative alignment between the input and output spaces. Additionally, we propose a mask-guided attention fusion module that uses a predicted pseudo mask to guide the fusion of different modal features, enhancing object responses and reducing background noise interference. Our extensive experiments on several challenging multispectral datasets demonstrate that our proposed COFNet achieves state-of-the-art performance. Our code is publicly available at https://github.com/li554/COFNet.},
  archive      = {J_TMM},
  author       = {Mingliang Zhou and Yunyao Li and Guangchao Yang and Xuekai Wei and Huayan Pu and Jun Luo and Weijia Jia},
  doi          = {10.1109/TMM.2025.3599097},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {COFNet: Contrastive object-aware fusion using box-level masks for multispectral object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dictionary based generative adversarial network for multi-collection style transfer. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most collection-based style transfer methods require training a separate model for each individual collection of styles, making the extension to multiple collections of styles less flexible. Besides, the existing collection-based methods are also less flexible in extending to new style collections in a continual manner. To address these issues, we propose a novel MultI-Dictionary Generative Adversarial Network framework (MID-GAN) for multi-collection style transfer. Specifically, we design a multi-dictionary architecture within a GAN, with each dictionary consisting of a set of local style codes for a specific style collection. Benefiting from the local style codes used in the dictionary, a stylization module with aligned skip connections is further proposed, which can better preserve both the local details and the overall image structure. The dictionary design allows a flexible extension to new style collections by readily adding new dictionaries and we propose a continual training strategy that can both preserve the style transfer ability of old styles and achieve good transfer results for newly added styles. Extensive experiments are performed to show that the proposed method is better than existing collection-based style transfer methods. We also demonstrate the proposed method can generate diverse meaningful style transfer results of the same style collection.},
  archive      = {J_TMM},
  author       = {Jing Huo and Shiyin Jin and Jiashen Li and Pingzhuo Tian and Wenbin Li and Jing Wu and Yu-Kun Lai and Yang Gao},
  doi          = {10.1109/TMM.2025.3599024},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dictionary based generative adversarial network for multi-collection style transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DALFace: Dynamic association learning for face recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition owes its success to the availability of large-scale training data. Recent adaptive margin-based loss functions pay more attention to hard (misclassified) samples, resulting in more discriminative face embeddings. However, large-scale datasets inevitably include open-set noise samples, which are usually mistaken for hard samples by mining-based methods and thus mislead the training of the model. In this work, we redefine hard samples and further design a dynamic association learning strategy for mining hard samples while ignoring noise. We argue that the difficulty of recognizing a sample depends on both identity-related and objective factors. On one hand, intrinsic attributes such as facial structure and face shape inherently influence the ease of identity recognition. On the other hand, external factors, including pose, occlusion, and resolution, directly affect the recognizability of a sample. Particularly in the case of noise samples, although they pose challenges for the deep network similar to hard samples, should not be regarded as hard samples. To this end, we propose an associated prototype learning method to achieve an approximation of face identity difficulty by exploring the fitting trends of identity prototype. Furthermore, we design a dynamic sample learning method to distinguish noise samples from hard samples by observing the distance fluctuation from the class center during sample learning. All observations are integrated into the loss function through adaptive margins and sample weights. Extensive experiments and visualizations on several datasets demonstrate that our method significantly outperforms state-of-the-art counterparts.},
  archive      = {J_TMM},
  author       = {Baojin Huang and Guangcheng Wang and Kui Jiang and Zhongyuan Wang},
  doi          = {10.1109/TMM.2025.3599040},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DALFace: Dynamic association learning for face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual-language multi-task blind image quality assessment with local quality weighting. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of blind image quality assessment (BIQA) is to develop a model capable of automatically evaluating image quality without requiring any reference knowledge. While multi-task learning has been widely utilized in BIQA, it has predominantly remained unimodal. This paper delves into the Visual-Language multi-task BIQA model, where distortion knowledge can be captured through image-text contrastive learning. Specifically, Visual-Language auxiliary tasks targeting distortion type and quality level are introduced, respectively, where both positive and negative image-text pairs are constructed for the target distorted image. Subsequently, image-text correspondences are learned in the embedding space while simultaneously evaluating image quality. Notably, in the auxiliary task learning, the proposed method not only brings the image and its corresponding positive text prompt closer but also pushes away the image from its negative text prompts, thereby facilitating the extraction of pertinent distortion features. In the quality assessment task, a patch-wise strategy is employed during the training phase. Differing from conventional BIQA methods, a novel NSS-guided quality weighting is introduced to gauge the correlation between patch quality and global quality, thereby enabling precise quality prediction. Extensive experiments are conducted on six IQA datasets, and the experimental results verify the superiority of the proposed method.},
  archive      = {J_TMM},
  author       = {Jili Xia and Lihuo He and Bo Hu and Leida Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3599072},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual-language multi-task blind image quality assessment with local quality weighting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised 3D visual grounding based on visual language alignment. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Language Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.},
  archive      = {J_TMM},
  author       = {Xiaoxu Xu and Yitian Yuan and Qiudan Zhang and Wenhui Wu and Zequn Jie and Lin Ma and Xu Wang},
  doi          = {10.1109/TMM.2025.3599032},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly-supervised 3D visual grounding based on visual language alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating hallucinations in large vision-language models via reasoning uncertainty-guided refinement. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite demonstrating impressive capabilities in comprehending multi-modal contexts, large vision-language models (LVLMs) are invariably prone to generate unreliable answers, i.e., hallucinations. Existing methods mainly mitigate this hallucination by introducing specific designed datasets or employing contrastive decoding techniques. However, these methods heavily rely on the quality of constructed datasets and negative samples, overlooking the inherent ambiguity in reasoning caused by over-reliance on linguistic priors and data complexity, termed reasoning uncertainty. This oversight hinders the models from effectively identifying the causal relationships behind each token, increasing their susceptibility to hallucinations. To address this issue, we propose a novel framework named Reasoning Uncertainty-guided Refinement (RUR) for mitigating hallucinations in LVLMs from an uncertainty perspective. Specifically, unlike conventional uncertainty quantification methods, we first extract the causal reasoning relationships between tokens by exploiting the link between structural causal models and the Transformer architecture. Based on this relationship, we then employ the Subjective Logic principle to model the reasoning uncertainty at both token and sentence levels, which reflects the unreliability degree of generated tokens and sentences. Finally, guided by reasoning uncertainty, we develop multi-level uncertainty-based adjustment to eliminate deceptive tokens exhibiting severe uncertainty and mitigate potential hallucinations in sentences. Extensive experiments demonstrate that our RUR method consistently achieves state-of-the-art performance on five benchmarks. Our code is available at https://github.com/Mrshenshen/RUR.},
  archive      = {J_TMM},
  author       = {Shenshen Li and Xing Xu and Wenxin Meng and Jingkuan Song and Chong Peng and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3599076},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mitigating hallucinations in large vision-language models via reasoning uncertainty-guided refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OAFTracker: One-stage associative multiple object tracking with fine-grained orthogonal representation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking based on the tracking-by-detection paradigm relies on appearance information and motion information for trajectory association. Employing global re-identification features and two-stage association strategies can improve the utilization of both types of information for detections with different confidence scores. However, when targets are occluded, coarse-grained global representations can lead to false positive detections. Additionally, two-stage association strategies tend to prioritize matching high-confidence detections over more accurate low-confidence detections, leading to identity switch problems. To address these issues, we propose the OAFTracker framework, which focuses on local representations and a one-stage association strategy. Firstly, a Fine-grained Representation Orthogonal Fusion (FROF) network is designed to adaptively integrate local and global representations. Secondly, we propose a One-stage Association Matching (OAM) strategy. This strategy combines multiple distance constraints to ensure fairness in matching detections with different confidence scores to predicted trajectories. Additionally, we propose an Adaptive Variable Noise (AVN) Kalman filtering algorithm to dynamically update the state of predicted trajectories. Finally, extensive experiments conducted on two public datasets demonstrate the effectiveness of the OAFTracker method.},
  archive      = {J_TMM},
  author       = {Jialin Liu and Jun Kong and Min Jiang and Xuefeng Tao},
  doi          = {10.1109/TMM.2025.3599069},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OAFTracker: One-stage associative multiple object tracking with fine-grained orthogonal representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artistic style transfer via fine-grained text guidance and contrastive semantics similarity. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the development of text-image multimodal methods, text is used to guide the style transfer of images, which has attracted growing attention. Notably, The existing text-guided image style methods are limited to expressing specific artistic style through simple text. It can only accept coarse-grained text input such as “Van Gogh” and “White Cloud”, and cannot understand fine-grained text input such as “The Night Café by Vincent van Gogh”. To this end, this paper proposes a novel artistic style transfer network based on the fine-grained text guidance and the contrastive semantics similarity, named as TCStyler. It can accept images or texts as style guidance, which is more suitable for fine-grained content understanding stylization. In this network, to address the issue of text-image cross-modal discrepancy, the residual attention feature mapper (RAFM) is introduced to constrain the differences between different modalities in feature space. Then, the global cascading style-sharing module (GCSM) is proposed for performing content-style feature fusion and image-text modality fusion by adopting a global feature-sharing strategy. Furthermore, the contrastive semantics similarity loss is designed to address the problem of multimodal universality. Quantitative and visualization experiments demonstrate that our TCStyler can handle fine-grained artistic text inputs and maintain consistency in the style transfer results guided by different modalities.},
  archive      = {J_TMM},
  author       = {Linfeng Li and Chunmei Qing and Junpeng Tan and Jianxiu Jin and Xiangmin Xu},
  doi          = {10.1109/TMM.2025.3599050},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Artistic style transfer via fine-grained text guidance and contrastive semantics similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised photographic image layout representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image layout representation learning, which converts layouts into compact vectors, is essential for tasks such as image retrieval, editing, and generation. However, existing methods—especially those applied to photographic images—face several challenges: supervised methods rely on expensive labeled datasets, weakly-supervised methods struggle with generalization, and self-supervised methods are limited in handling the diversity of photographic layouts. To address these issues, we propose a novel heterogeneous layout graph that efficiently captures the layout information in images. The vertices of this graph represent the compositional primitives of the image, capturing their attributes, while the edges encode the relationships between these primitives. We also design effective pretext tasks to guide a layout encoder-decoder in self-supervised training, ultimately generating the layout graph embedding vector. Additionally, we introduce a new layout evaluation dataset—LODB—which features a richer variety of layout categories, significantly better label quality than existing datasets, and a more balanced distribution of semantic scenes across layout categories, providing a comprehensive benchmark for evaluation. Experiments on the LODB dataset demonstrate that our method outperforms existing approaches in representing photographic image layouts.},
  archive      = {J_TMM},
  author       = {Zhaoran Zhao and Peng Lu and Xujun Peng and Wenhao Guo},
  doi          = {10.1109/TMM.2025.3599102},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised photographic image layout representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning generalizable contrastive representations for graph zero-shot learning. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of graph zero-shot learning, which aims at recognizing novel classes of nodes on the graph that are never seen during training. The key to graph zero-shot learning is establishing the mathematical relationship to transfer the prior knowledge of nodes from seen classes to unseen classes. However, the problem is largely under-explored and existing methods typically focus on acquiring supervision signals from seen classes or simply establishing connections between classes based solely on a semantic description matrix, such that the learned representations lack generalizable properties to unseen classes. To address this issue, this paper proposes GraphGCR that learns generalizable contrastive representations from the perspective of uniformity and alignment. Technically, GraphGCR leverages graph diffusion to extend supervised contrastive learning, encouraging the representations of semantics from different classes to be distributed uniformly and meanwhile achieve the alignment of node features and class semantics with the assistance of graph structural information. Moreover, to effectively enhance model generalizability, we further develop a class generator to synthesize features of unseen classes by embedding propagation and interpolation, thereby enriching the diversity of classes. Theoretical analysis also shows that our proposed framework exhibits strong discriminative property, which significantly enhances graph zero-shot learning. Experimental findings reveal that our GraphGCR achieves significant performance improvements over state-of-the-art methods across various benchmark datasets.},
  archive      = {J_TMM},
  author       = {Siyu Yi and Zhengyang Mao and Kangjie Zheng and Zhiping Xiao and Ziyue Qiao and Chong Chen and Xian-Sheng Hua and Yongdao Zhou and Ming Zhang and Wei Ju},
  doi          = {10.1109/TMM.2025.3599043},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning generalizable contrastive representations for graph zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation learning and aggregate-attention for multi-person motion prediction. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods achieve impressive predictions using various networks but often overlook the distinct representations of joint relations within individuals (intra-relations) and interactions among groups (inter-relations), inevitably leading to undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations: a GCN-based network for intra-relations and a novel reasoning network for inter-relations. Specifically, we propose a distance-aware cross-attention that incorporates physical distance constraints into inter-relation learning through a learnable distance weighting coefficient. Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9∼15 persons), demonstrate that our method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Kehua Qu and Rui Ding and Jin Tang},
  doi          = {10.1109/TMM.2025.3599049},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relation learning and aggregate-attention for multi-person motion prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SecureDA: Privacy-preserving source-free domain adaptation for person re-identification. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional domain adaptation (DA) for person reidentification (ReID) aims to bridge the domain gap but often requires direct use of fully labeled source and target domains, raising significant data privacy concerns due to the inclusion of personal identity information (PII) in raw data. Source-free domain adaptation (SFDA) for person ReID effectively preserves PII within the authorized source model. Nevertheless, these methods are vulnerable to data privacy (e.g., portrait rights) of the target domain during retrieval, where attackers can exploit pedestrian images for malicious generation, leading to damage to an individual's reputation. Beyond these limitations, we propose a novel framework called SecureDA to address privacy-preserving SFDA for person ReID, which can generate a privacy key to defend against potential attacks on PII. Technically, we introduce domain-specific adversarial attacks into DA, where the protected query and gallery images are encrypted to ensure secure image retrieval. Furthermore, we employ two simultaneous processes: 1) The global–local adversarial pathway (GLAP) leverages encrypted and original images as adversarial pairs, thereby fostering the development of robust ReID models; 2) The global–local collaborative pathway (GLCP) is mastered through positive pairs collected from the same domain, effectively mitigating the pernicious catastrophic forgetting phenomenon. Extensive experiments show that SecureDA achieves state-ofthe-art performance on multiple DA benchmarks and even outperforms the conventional DA and SFDA methods, which inherently compromise data privacy.},
  archive      = {J_TMM},
  author       = {Xiaofeng Qu and Li Liu and Huaxiang Zhang and Lei Zhu and Liqiang Nie and Xiaojun Chang and Fengling Li},
  doi          = {10.1109/TMM.2025.3599094},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SecureDA: Privacy-preserving source-free domain adaptation for person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privileged information-guided multitask mutualistic transformer for gaze prediction. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting where people look is a crucial for understanding human intentions. Gaze prediction, as a research hotspot, has evolved from predicting the gaze of a single person to simultaneously predicting the positions of all individuals and their corresponding gaze targets. However, the study of the correlation between humans and gaze as two interdependent tasks has largely been neglected. In this paper, inspired by the concept of “mutualistic symbiosis” in ecology, we propose a novel multitask mutualistic transformer (MMTR). MMTR captures paired dependencies by establishing information communication between different branches, thereby enabling comprehensive and interpretable gaze analysis for all individuals and gaze targets. Specifically, we first utilize a transformer encoder to capture the common features of all the tasks. Then, we design a mutualistic attention mechanism (MAM) in the dual-branch Transformer decoder to establish cross-task information interaction. The MAM can learn privileged information from other tasks that is helpful for the current task, thereby guiding the current branch to learn the most valuable and distinctive features. To the best of our knowledge, this is the first time that privileged information has been introduced into the gaze estimation task. Furthermore, to more flexibly learn pixel locality and long-range semantic dependencies for different tasks, we construct and embed a learnable global-local position encoding (GLPE) in different branches of MMTR. Experiments demonstrate that our proposed MMTR can guide the two branches to communicate through privileged information, effectively solve the information asymmetry problem between human detection and gaze prediction, and significantly outperform state-of-the-art gaze prediction methods on two standard benchmark datasets GazeFollowing and VideoAttentionTarget.},
  archive      = {J_TMM},
  author       = {Wenhe Chen and Yuan Chai and Xiao-Jun Wu and Hongjin Zhu and Qian Yu and Zhuo-Ming Du and Feilong Han and Wei Gao and Caixia Zheng and Honghui Fan},
  doi          = {10.1109/TMM.2025.3599030},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privileged information-guided multitask mutualistic transformer for gaze prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PalmMamba: Palm intrinsic features learning selective state space model for palmprint image denoising. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint-based biometric recognition has gained widespread attention due to its rich features, contactless acquisition, and low invasiveness. However, most existing methods neglect image quality, making them less effective for low-quality, noisy palmprint images. In this paper, we propose a palm intrinsic features learning selective state space model (PalmMamba) for palmprint image denoising, which consists of shallow feature representation, noise-insensitive palmprint-specific feature learning, and sharp palmprint image restoration modules. First, we convert the degraded noisy palmprint image into a high-dimensional shallow feature representation through a single-layer convolution backbone. Then, we develop parallel learning branches, including a second-order attention-based selective state space model and a mixed difference convolution module, to exploit diverse palmprint-specific features with both global and local details. Finally, we map the fine-grained palmprint-intrinsic feature map into the identity-preserved sharp palmprint image via a commonly used convolution layer. Extensive experimental results on five public palmprint databases demonstrate the encouraging performance of the proposed PalmMamba in palmprint image denoising.},
  archive      = {J_TMM},
  author       = {Zhu Wang and Lunke Fei and Shuping Zhao and Bob Zhang and Qi Zhu and Imad Rida},
  doi          = {10.1109/TMM.2025.3599093},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PalmMamba: Palm intrinsic features learning selective state space model for palmprint image denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial-temporal progressive fusion network for breast lesion segmentation in ultrasound videos. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound video-based breast lesion segmentation provides valuable assistance in early breast lesion detection and discrimination. However, this field faces two key challenges: the first is how to simultaneously utilize both intra-frame and inter-frame lesion cues to accurately segment breast lesions, and the second is that the availability of breast ultrasound video datasets is quite limited. In this paper, we propose a novel Spatial-Temporal Progressive Fusion Network (STPFNet) for video-based breast lesion segmentation problem. The proposed STPFNet comprises three main components. First, we propose to adopt a unified network architecture to capture spatial dependencies within each ultrasound frame and temporal correlations between different frames together for feature representation of ultrasound video. Second, we propose a new fusion module called Multi-Granularity Feature Fusion (MGFF) to fuse the extracted information with different granularities for lesion segmentation. MGFF can help improve the issue of lesion boundary blurring. Third, we propose to take the segmentation result of the previous frame as prior knowledge to suppress the noisy background and learn a more robust representation. To further promote the research in this field, we construct a new ultrasound video breast lesion segmentation dataset, called UVBLS200, comprising 200 videos (80 benign and 120 malignant lesions). Experiments on the proposed dataset demonstrate that the proposed STPFNet achieves a better breast lesion detection performance than state-of-the-art methods. The code is available at https://github.com/zzgzzgz/STPF-Net.},
  archive      = {J_TMM},
  author       = {Zhengzheng Tu and Zigang Zhu and Yayang Duan and Bo Jiang and Qishun Wang and Chaoxue Zhang},
  doi          = {10.1109/TMM.2025.3599028},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A spatial-temporal progressive fusion network for breast lesion segmentation in ultrasound videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RA-BLIP: Multimodal adaptive retrieval-augmented bootstrapping language-image pre-training. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have recently received substantial interest, which shows their emerging potential as general-purpose models for various vision-language tasks. MLLMs involve significant external knowledge within their parameters; however, it is challenging to continually update these models with the latest knowledge, which involves huge computational costs and poor interpretability. Retrieval augmentation techniques have proven to be effective plugins for both LLMs and MLLMs. In this study, we propose multimodal adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training (RA-BLIP), a novel retrieval-augmented framework for various MLLMs. We first leverage the question to instruct the extraction of visual information through interactions with one set of learnable queries, minimizing irrelevant interference and redundancy during retrieval and generation. Besides, we introduce a pre-trained multimodal adaptive fusion module to achieve question text-to-multimodal retrieval and integration of multimodal knowledge by projecting visual and language modalities into a unified semantic space. Furthermore, we present an Adaptive Selection Knowledge Generation (ASKG) strategy to train the generator to autonomously discern the relevance of retrieved knowledge, which realizes excellent denoising performance. Extensive experiments on open multimodal question-answering datasets demonstrate that RA-BLIP achieves significant performance and surpasses the state-of-the-art retrieval-augmented models.},
  archive      = {J_TMM},
  author       = {Muhe Ding and Yang Ma and Pengda Qin and Jianlong Wu and Yuhong Li and Liqiang Nie},
  doi          = {10.1109/TMM.2025.3599070},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RA-BLIP: Multimodal adaptive retrieval-augmented bootstrapping language-image pre-training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CASIA-PR-v1: A multi-ethnic, multi-device and cross-spectral dataset and a multiscale disentangled model for periocular recognition. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periocular recognition is regarded as an alternative trait for biometric recognition that can effectively solve the identification problem under large occlusions. However, few datasets are tailored for periocular recognition. For most compromises, iris datasets at near-infrared wavelengths, miss information about the eyebrows or eyelids. In this paper, a challenging dataset for real scenarios named CASIA-PR-V1 with evaluation protocols is released for periocular recognition. It is collected from multiple types of mobile devices with different resolutions or wavelengths. A rich set of attributes, e.g., ethnicities, is tagged to support fine-grained classification tasks. Moreover, we consider a wide range of noisy data in unconstrained environment, especially for glasses. Superior to its counterparts, this periocular dataset is highly valuable for studying cross-device and cross-spectral periocular recognition with occlusions, as well as fine-grained attribute classification. Additionally, a multiscale disentangled model is proposed to extract discriminating representations for periocular recognition with severe occlusions. Extensive experiments are conducted on CASIA-PR-V1, and the results indicate the superiority of our model for unconstraint periocular recognition. Please visit http://biometrics.idealtest.org for more details about our dataset.},
  archive      = {J_TMM},
  author       = {Wanting Zhou and Yiwei Ru and Yushan Han and Longteng Kong and Zijian Wang and Yong He and Zhenan Sun},
  doi          = {10.1109/TMM.2025.3599084},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CASIA-PR-v1: A multi-ethnic, multi-device and cross-spectral dataset and a multiscale disentangled model for periocular recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of prompt learning via multi-knowledge representation for vision-language models. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage the potential of VLMs in adapting to downstream tasks, context optimization methods such as prompt tuning are essential. However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules. This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks. To address this challenge, we propose context optimization with multi-knowledge representation (CoKnow), a framework that enhances prompt learning for VLMs with rich contextual knowledge. To facilitate CoKnow during inference, we train lightweight semantic knowledge mappers, which are capable of generating multi-knowledge representations for an input image without requiring additional priors. Experimentally, we conduct extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods.},
  archive      = {J_TMM},
  author       = {Enming Zhang and Bingke Zhu and Yingying Chen and Qinghai Miao and Ming Tang and Jinqiao Wang},
  doi          = {10.1109/TMM.2025.3599096},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimization of prompt learning via multi-knowledge representation for vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology learning for two-view correspondence filtering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel neural network called Topology Learning Network (TL-Net), that exploits local and global geometric relation by topology graphs to handle the problem of correspondence filtering in complex scenes. Specifically, we first design a Multi-level Topology Encoder (MLTE), which fuses local and global topology graphs by a channel attention, to sufficiently extract the geometric relation among correspondences. MLTE not only includes local topology graphs by gathering the information of relative motion and multi-resolution group convolution, but also includes a global topology graph by aggregating the information of the similarity and the Graph Laplacian. In addition, inspired by Transformer, we design the backbone of TL-Net to generate enriched feature maps for correspondence filtering. Meanwhile, by simplifying the global context aggregation, we maintain the lightweight of the backbone, introducing the superiority of Transformer while avoiding extra parameters and calculations. Empirical experiments on several computer vision tasks show that the performance and generalization ability of TL-Net are significantly superior to the state of the art methods. Notably, on relative pose estimation, we achieve $5.63\%$ and $5.03\%$ mAP improvements under an error threshold of $5^{\circ }$ outdoors and indoors, respectively. The code is available at https://github.com/guobaoxiao/TLNet.},
  archive      = {J_TMM},
  author       = {Ziwei Shi and Xiangyang Miao and Guobao Xiao and Songlin Du and Zheng Wang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3599033},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Topology learning for two-view correspondence filtering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving back-projection accuracy for the semantic segmentation of indoor point clouds with fewer & sparse image annotations. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing semantic segmentation on point clouds is the primary method by which machines perceive 3D scenes in a fine-grained manner. Deep learning algorithms usually require many pointwise annotations obtained with specialized tools, which is a laborious and inefficient process. To this end, we develop two frameworks for training point cloud semantic segmentation networks, one that utilizes fewer projected image annotations and another that employs sparse scribble image annotations, making the process more flexible and user friendly. However, back-projecting 2D-pixel labels to 3D points during loss calculations always introduces errors. To increase the back-projection accuracy of our approach, we first identify and record potential pixel-point correspondence errors and then develop strategies for constructing an accurate back-projection mapping matrix. Specifically, we filter out occluded and noisy points to avoid incorrect label allocations and permit multiclass assignments to adjust the ambiguity of boundary points. By incorporating an accurate back-projection mechanism into the loss functions of the proposed training frameworks, our networks can perform well with only four projected image annotations or even sparse scribble image annotations for each scene. This results in state-of-the-art performance compared with that of other weakly supervised point cloud semantic segmentation approaches, and the outcomes are even comparable to those produced by fully supervised methods on the S3DIS and ScanNet-v2 datasets.},
  archive      = {J_TMM},
  author       = {Peng Jiang and Haochen Sun and Zhiyi Pan and Jinming Cao and Roger Zimmermann and Changhe Tu},
  doi          = {10.1109/TMM.2025.3599085},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving back-projection accuracy for the semantic segmentation of indoor point clouds with fewer & sparse image annotations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted medical images based on huffman tree coding and count-encryption. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDHEI) has been recognized as an effective method for overcoming management difficulties within picture archiving and communication system (PACS). However, most existing RDHEI algorithms still encounter notable challenges when applied to the PACS, specifically in terms of their key management, embedding capacity, and security. This paper introduces a novel framework and corresponding algorithm for reversible data hiding in encrypted medical images (RDHEMI) to bridge this gap. The framework employs a unique key for each patient and maintains consistency in the key linked to patient images regardless of changes in doctor, thereby addressing key management challenges. In the proposed algorithm, Huffman tree coding (HTC) integrates Huffman coding with innovative leaf-to-leaf coding, achieving a better compression performance for medical images than move-to-front (MTF) cache and Huffman coding, as medical images contain more smooth areas. Count-encryption (CE) produces encryption keys according to the frequency of encryption occurrences for an image and ensures a peak signal-tonoise ratio under 8 dB for multiple encryptions with the same key, enhancing the algorithm's resistance to attacks. The experimental results demonstrate that the proposed algorithm achieves high security to counter various attacks and outperforms existing algorithms in terms of the time complexity and embedding capacity, with an improvement of 0.21 bpp.},
  archive      = {J_TMM},
  author       = {Yaolin Yang and Hongjie He and Fan Chen and Yuan Yuan and Ningxiong Mao and Yang Li and Jun Zhao},
  doi          = {10.1109/TMM.2025.3599036},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reversible data hiding in encrypted medical images based on huffman tree coding and count-encryption},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-type image quality assessment based on multi-region deep feature fusion under meta-learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing image quality assessment methods need to be retrained when dealing with a new type of task. This approach wastes computing resources and time. Therefore, these methods fail to suit the application scenarios that require processing of multi-type image quality assessment tasks. In the human visual system, the eyes of human tend to pay varying degrees of attention to different regions. Inspired by this system, this paper proposes a multi-type image quality assessment method based on multi-region deep feature fusion under meta-learning (MMQA). First, we utilize the differences in the structural information to screen out salient and non-salient regions. Second, a deep multi-stream network is designed to comprehensively consider and fuse different features related to the quality in salient regions, non-salient regions and the entire image. Third, meta-learning is applied to quickly learn and update the parameters of the model when facing new types of images. By summarizing the prior knowledge in the training of one type of task, the model can be quickly fine-tuned for other types of images. The experimental results demonstrate that the proposed method has advantages over the existing methods in generalization and robustness. Furthermore, the proposed method can adapt well to different distortion types and different image types quickly and accurately.},
  archive      = {J_TMM},
  author       = {Shun Zhu and Xichen Yang and Tianshu Wang and Tianhai Chen and Nengxin Li and Xiaobo Shen and Genlin Ji},
  doi          = {10.1109/TMM.2025.3599027},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-type image quality assessment based on multi-region deep feature fusion under meta-learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual fuzzy alignment on hypergraph for open-set 3D cross-modal retrieval. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing 3D cross-modal retrieval (3CMR) methods heavily rely on prior knowledge of training categories, which leads to the problem of modality shift and unseen center deviation when encountering unseen categories under the open-set environment. Aiming at the open-set 3CMR, this paper introduces the Hypergraph-Based Residual Fuzzy Alignment (ReFA) framework, which revisits the open-set retrieval task and navigates uncertainty of it through the lens of Fuzzy Theory. Facing the challenges of boundaryless space caused by uncertain unseen categories, we explore the representation and measurement in the fuzzy membership space as an alternative to fixed close-set category space. Specifically, to address the problem of modality shift caused by unseen categories, we utilize the Residual Sampling Generation (RSG) module to generate modality sampling embeddings that are independent of seen categories under the guidance of fuzzy representation, which residually decouples the entangled interactions of seen categories and modalities. To overcome the problem of unseen center deviation, we propose the Center Fuzzy Alignment (CFA) module to leverage the high-order fuzzy correlations for generalized metric, by constructing a fuzzy hypergraph based on the inherent and fuzzy correlations among both modalities and categories. The comprehensive evaluations of comparison and ablation studies on the four benchmarks demonstrate the superiority of our proposed framework compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yang Xu and Yifan Feng and Xu Zhuang and Jason Wang and Zongze Wu and Yue Gao},
  doi          = {10.1109/TMM.2025.3599081},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Residual fuzzy alignment on hypergraph for open-set 3D cross-modal retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPSR-CLIP: Conditional prompt-induced style reconstruction for zero-shot domain adaptation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the requirement of target domain data in existing unsupervised domain adaptation (UDA) techniques, researchers have shifted their focus to a more practical and challenging scenario, i.e., zero-shot domain adaptation (ZSDA). However, ZSDA remains a significant challenge, with existing approaches in ZSDA often relying heavily on a carefully crafted and highly compatible auxiliary domain. This is impractical in real-world applications. To address the mentioned problems, we propose conditional prompt-induced style reconstruction with contrastive language-image pre-training (CPSR-CLIP), which leverages the rich semantic embedding of CLIP to synthesize target-like features, effectively bypassing the need for auxiliary dual-domain samples. CPSR-CLIP adopts a multi-phase optimization strategy and every optimization phase is a prerequisite for the next phase. Firstly, we propose dynamic prompt disentanglement to facilitate the model in differentiating the discrepancy between the source and target prompts, thus paving the way for conditional prompt-induced style reconstruction phase. This phase meticulously strips away domain-specific styles to reserve domain-invariant features and injects target style characteristics through target domain prompts. Finally, with the target-like features in hand, we adaptively adjust the learnable part of target prompts for further fitting. Extensive experiments have been conducted on several datasets and the results demonstrate the superiority of CPSR-CLIP over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jiayu Qian and Yuwu Lu and Wuyuan Xie and Zhihui Lai and Miaohui Wang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3599044},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CPSR-CLIP: Conditional prompt-induced style reconstruction for zero-shot domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OV-BIS: Open-vocabulary boundary guide zero-shot 3D instance segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open vocabulary 3D instance segmentation aims to align 3D instance segmentation results with natural language text, thereby achieving semantic prediction without relying on predefined class labels for specific scenes, which has been widely used in the field of multimedia. Current open vocabulary 3D instance segmentation methods mainly rely on 2D masks provided by various 2D segmentation foundation models. However, in complex scenes, the calculation of 2D masks often struggles to balance over-segmentation of large objects and under-segmentation of small objects. In this paper, we introduce OV-BIS, a novel zero-shot open vocabulary 3D instance segmentation method that leverages instance boundary information to improve 3D semantic segmentation performance. The key insight of our method is that the edge map as 3D boundary projection is suitable for multi-scale tasks and capable of compensating for the weakness of 2D masks in multi-scale adaptability for complex scenes. Our method aggregates multiview edge maps and 2D masks, iteratively guiding the merging of over-segmented point clouds with regions growing to cluster 3D primitives into distinct 3D instances. By projecting 3D instances onto images and using CLIP to calculate semantic features from multiple perspectives with an outliers filter, 3D semantic instance segmentation has been achieved. Experiments on multiple datasets demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Tinghao Yi and Shaohu Wang and Zhengtao Zhang and Changwei Wang and Dongming Yan and Rongtao Xu and Enhong Chen},
  doi          = {10.1109/TMM.2025.3599047},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OV-BIS: Open-vocabulary boundary guide zero-shot 3D instance segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal complementary learning and template-based reasoning chains for future event prediction in videos. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multi-modal large language models (MLLMs) have impressive cross-modal reasoning and prediction capabilities, a unified and rigorous evaluation standard is still lacking. In this paper, we propose a future event prediction task to evaluate their cross-modal temporal prediction capability. This task requires the model to generate descriptions of events that may occur in future based on the input premise video. We build a dataset on the existing datasets for model evaluation. This task faces many challenges, including the complexity of processing video data, such as understanding changes in objects, actions, and time dimensions within the video and the interference of redundant information. To address these challenges, we propose a novel cross-modal prediction framework that introduces cross-modal supplementary learning and template-based reasoning chains based on MLLMs. Cross-modal supplementary learning aims to promote visual and text information to supplement and mine their respective information, primarily to capture critical information in videos, relying on the adaptive temporal filter and casual Q-Former. The template-based reasoning chain drives GPT-4 to generate a series of template question pairs through design prompts, gradually guiding the model to perform hierarchical reasoning to support the final prediction. Through experimental evaluation, the performance of the current MLLMs may not meet the requirements, and our model outperforms all existing models in predicting future events. It shows that the capabilities of MLLMs can be further explored.},
  archive      = {J_TMM},
  author       = {Chenghang Lai and Weifeng Ge and Xiangyang Xue},
  doi          = {10.1109/TMM.2025.3599038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal complementary learning and template-based reasoning chains for future event prediction in videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multi-feature relationship in retinex decomposition for low-light image enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent advancements in deep learning techniques, existing unsupervised low-light image enhancement methods fail to improve global brightness and restore colour due to the lack of high-quality training targets. Moreover, real-world low-light images inevitably contain noise, which significantly reduces image visibility and quality, further complicating the enhancement process. However, current unsupervised approaches tend to oversimplify or ignore the noise in low-light images. To address these issues, we first revise the traditional Retinex decomposition to better integrate with unsupervised deep learning frameworks. Then, we design a Local and Global Illumination-Guided Network for removing corruption from the reflectance component, which improves enhancement quality by not only investigating multi-feature similarity and attention mechanism based on the Retinex theory but also leveraging local details and long-range dependencies. Furthermore, by analysing the attributes of corruption within the reflectance component, we introduce a novel reflectance enhancement loss to effectively remove noise without using ground truth. The code is available at: https://github.com/RuoyuGuo/ErcRetinex.},
  archive      = {J_TMM},
  author       = {Ruoyu Guo and Maurice Pagnucco and Yang Song},
  doi          = {10.1109/TMM.2025.3599099},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring multi-feature relationship in retinex decomposition for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature quality assessment: A database and a lightweight objective method. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Artificial Intelligence, visual data gathered by edge devices could be primarily utilized for machine vision tasks. The prominent coding frameworks accomplish this by extracting and compressing features extracted from input data. As such, the quality of these features is vital, as they reflect the performance of the coding framework. However, much less work has been dedicated to quality assessment on features, impeding the optimization of the coding system. In this work, we pioneer to explore the feature quality assessment by creating a novel database tailored for features, with the quality ground-truth for each feature. Then, we propose a lightweight feature quality assessment method, called Lightweight Feature Quality Assessment (LFQA). We analyze the feature characteristics from the perspective of spatial and channel thoroughly, and the framework of LFQA is designed based on the analysis results. Experimental results demonstrate that LFQA accurately evaluates the quality of features, reaching a notable Spearman Rank-Order Correlation Coefficient of 85.38%, and exhibits competitive performance in improving the performance of video coding for machine system. Furthermore, LFQA has fewer model parameters and faster inference speed, ensuring a wide range of promising applications.},
  archive      = {J_TMM},
  author       = {Shipei Wang and Ping An and Chao Yang and Gongyang Li and Xinpeng Huang and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3599090},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Feature quality assessment: A database and a lightweight objective method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCInet: Fusing low-light visible-infrared image via max-merge complementary information. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing complementary information in the visible-infrared image offers a promising approach to enhance the performance of downstream computer vision tasks (e.g., object detection, segmentation etc) in complicated imaging conditions (e.g., low-illumination). However, due to the robust imaging capacity of the infrared sensor in complicated imaging conditions, most existing methods primarily rely on the salient object intensity information in the infrared modality for fusion, while the visible information (e.g., color, texture etc) is not adequately utilized, and thus limit their generalization capacity in downstream computer vision tasks. In this study, we present a novel image fusion framework, i.e., MCInet, which attempts to Maximize and merge the Complementary Information across visible-infrared modalities for more informative image fusion. To this end, we first introduce the modality-specific processing module into the fusion framework to improve the information representation of each modality image. For visible images, a pre-trained low-light enhance module is adopted to enhance its color and texture information. In addition, for infrared images, a nonlinear mapping module is constructed to suppress the excessive salient object intensity information of infrared modality. Then we establish a reusable MCI block that embeds a cross-image mutual information minimization scheme into an input-aware fusion module. This empowers us to dynamically maximize and merge the complementary information between two input images according to their feature representation. In addition, we introduce a cycle reconstruction loss to self-supervised regularize the fusion results for further enhancement. Experiments on image fusion, object detection, and segmentation tasks demonstrate that the proposed framework can produce more informative fusion results and exhibit better performance in downstream computer vision tasks.},
  archive      = {J_TMM},
  author       = {Jiangtao Nie and Boxiong Wu and Wei Wei and Lei Zhang and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3599042},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MCInet: Fusing low-light visible-infrared image via max-merge complementary information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-guided video frame interpolation with spatial-temporal global attention. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation technology improves visual experience with the development of deep learning. However, capturing large motions while synthesizing fine texture details remains a challenging task. Regarding large motion scenarios, some pioneering Transformer-based methods primarily rely on local attention, which does not fully leverage the global receptive field advantage. To address this issue, this paper proposes to further broaden the receptive field of the Transformer to capture more correlations in the video frame interpolation task. Specifically, we propose a global self-attention mechanism in the form of spatial-temporal separation. Regarding texture details, since roughly enlarging the receptive field results in the loss of details, we propose to use large motion information in both feature and pixel spaces as a dual-guided prior to enhance detail synthesis. The separable attention mechanism and the straightforward frame synthesis design significantly enhance the resource efficiency of our model. Extensive experiments show that our method achieves state-of-the-art performance, effectively capturing large motions and preserving texture details.},
  archive      = {J_TMM},
  author       = {Baojun Zhou and Xinpeng Huang and Gongyang Li and Chao Yang and Liquan Shen and Ping An},
  doi          = {10.1109/TMM.2025.3599095},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-guided video frame interpolation with spatial-temporal global attention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale up composed image retrieval learning via modification text generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed Image Retrieval (CIR) aims to search an image of interest using a combination of a reference image and modification text as the query. Despite recent advancements, this task remains challenging due to limited training data and laborious triplet annotation processes. To address this issue, this paper proposes to synthesize the training triplets to augment the training resource for the CIR problem. Specifically, we commence by training a modification text generator exploiting large-scale multimodal models and scale up the CIR learning throughout both the pretraining and fine-tuning stages. During pretraining, we leverage the trained generator to directly create Modification Text-oriented Synthetic Triplets (MTST) conditioned on pairs of images. For fine-tuning, we first synthesize reverse modification text to connect the target image back to the reference image. Subsequently, we devise a two-hop alignment strategy to incrementally close the semantic gap between the multimodal pair and the target image. We initially learn an implicit prototype utilizing both the original triplet and its reversed version in a cycle manner, followed by combining the implicit prototype feature with the modification text to facilitate accurate alignment with the target image. Extensive experiments validate the efficacy of the generated triplets and confirm that our proposed methodology attains competitive recall on both the CIRR and FashionIQ benchmarks. Codes and datasets will be made publicly accessible.},
  archive      = {J_TMM},
  author       = {Yinan Zhou and Yaxiong Wang and Haokun Lin and Chen Ma and Li Zhu and Zhedong Zheng},
  doi          = {10.1109/TMM.2025.3599088},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scale up composed image retrieval learning via modification text generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilevel direction preserving for few-shot open-set recognition. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3599083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot open-set recognition (FSOSR) poses a significant challenge as it requires identifying unknown classes while maintaining the classification performance of known classes, despite having limited access to labeled training samples. Current methods often employ non-directional metric-based losses to encapsulate feature attributes within the embedding space, inadvertently disregarding the potential influence of spatial distribution deviations of feature representations on open-set recognition performance. To address this, we present a novel directional metric-based method termed Bilevel Direction Preserving (BiDirP). This method incorporates two direction-preserving regularizers operating at distinct levels, specifically at the instance and prototype levels. The combined application of these two direction-preserving regularizers effectively enhances the spatial separation between prototypes of different classes and refines the classification decision boundaries, which results in an improved discriminative ability to differentiate unknown classes within a broader open space. Comprehensive experiments on public benchmarks show that BiDirP can significantly improve the detection ability of unknown classes while correctly classifying known classes.},
  archive      = {J_TMM},
  author       = {Zihui Zhang and Chenghao Xu and Jiexi Yan and Cheng Deng},
  doi          = {10.1109/TMM.2025.3599083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bilevel direction preserving for few-shot open-set recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuV-SLAM: Fast neural multiresolution voxel optimization for RGBD dense SLAM. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping pipeline based on neural multiresolution voxels, characterized by ultra-fast convergence and incremental expansion capabilities. This pipeline utilizes RGBD images as input to construct multiresolution neural voxels, achieving rapid convergence while maintaining robust incremental scene reconstruction and camera tracking. Central to our methodology is to propose a novel implicit representation, termed VDF that combines the implementation of neural signed distance field (SDF) voxels with an SDF activation strategy. This approach entails the direct optimization of color features and SDF values anchored within the voxels, substantially enhancing the rate of scene convergence. To ensure the acquisition of clear edge delineation, SDF activation is designed, which maintains exemplary scene representation fidelity even under constraints of voxel resolution. Furthermore, in pursuit of advancing rapid incremental expansion with low computational overhead, we developed hashMV, a novel hash-based multiresolution voxel management structure. This architecture is complemented by a strategically designed voxel generation technique that synergizes with a two-dimensional scene prior. Our empirical evaluations, conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's exceptional efficacy in terms of convergence speed, tracking accuracy, scene reconstruction, and rendering quality.},
  archive      = {J_TMM},
  author       = {Wenzhi Guo and Bing Wang and Lijun Chen},
  doi          = {10.1109/TMM.2025.3599100},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {NeuV-SLAM: Fast neural multiresolution voxel optimization for RGBD dense SLAM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-spatial attention for refined object placement in text-to-image synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solely based on given prompts, text-guided diffusion models have enjoyed a unique capability in generating diverse and creative images. Nevertheless, the conveyance of image information through text presents a series of challenges, particularly in controlling the positioning of objects in synthesized images. Despite attempts of recent efforts in exploring alternative conditions, such as bounding box/mask-image pairs, the requirement of a substantial amount of paired data and time-consuming fine-tuning emerge as new issues. Given the observations that not only prompt-related cross-attention maps reveal the spatial arrangement and centroid positions of the objects, but also out-of-prompt markers enjoy rich semantic information, we thus engineer a weighted optimization loss. Specifically, three spatial sub-losses, namely inner box reinforcement loss, outer box attenuation loss, and centroid loss, are devised and seamlessly integrated into the sampling step of current vanilla diffusion models. Without any annotations of layout data required, the final approach runs in a training-free fashion. Extensive experiments with new performance scores demonstrate that our proposal not only successfully addresses the issue of object positioning but also boosts the capabilities of most current models, such as Stable Diffusion and GLIGEN, in high-quality synthesis and coverage of various concepts. Moreover, the proposed mechanism plays a plug-and-play role.},
  archive      = {J_TMM},
  author       = {Jianwei Zheng and Ni Xu and Wei Li and Jiawei Jiang and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3599077},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-spatial attention for refined object placement in text-to-image synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-frequency collaborative learning for camouflaged object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a challenging task that struggles to accurately detect the objects concealed in the surrounding environment. This is largely attributed to the intrinsic similarity of the camouflaged objects with the surrounding environment. To address this challenge, we propose a Spatial-Frequency Collaborative Learning network for COD (SFCNet). Specifically, we propose a Domain Transformation Fusion (DTF) module to handle the similarity between the camouflaged objects and the background, because when processed in the frequency domain, the features of the camouflaged object and the background become easy to discriminate. Then, we design a Cross-domain Integration Unit (CIU) to integrate the high-level features progressively through a Spatial-Frequency Coordinated Fusion (SFCF) module and a Multi-scale Feature Enhancement (MFE) module. Finally, the low-level features are combined with the high-level features from different decoding stages to correct the camouflaged objects in detail. In addition, an Edge Amplification (EA) module is designed to enable the model to pay attention to the global contour of the camouflaged object. It can facilitate the generation of prediction maps with accurate object boundaries. Extensive experiments on four benchmark COD datasets show that SFCNet outperforms state-of-the-art (SOTA) COD models. Meanwhile, it also has the characteristics of low parameters (21.01 M) and low computational complexity (24.14 G). Codes and results are released on https://github.com/Zhaorui328/SFCNet.},
  archive      = {J_TMM},
  author       = {Rui Zhao and Mengyin Wang and Fasheng Wang and Fuming Sun and Haojie Li},
  doi          = {10.1109/TMM.2025.3599041},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-frequency collaborative learning for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative learning framework with coupling graph transformers for 3D tooth segmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of 3D dental models into individual teeth is an important step in orthodontic computer-aided design (CAD) systems. However, most existing methods rely on single-view dental models and ignore the intrinsic relationships between upper and lower dental models, hindering the handling of complex tooth structures. In this paper, a collaborative learning framework with coupling graph Transformers (CGT-CLF) is proposed for automatic tooth segmentation on 3D dental models. The framework collaboratively learns geometric features of both upper and lower dental models, capturing their interactivity and complementarity by facilitating interaction between graph-Transformer encoders to improve segmentation of complex and diverse teeth. Specifically, CGT-CLF consists of three key components as follows: First, a graph embedding-based boundary perception module (GEBPM) is developed to aggregate fine-grained geometric features within the neighborhood graph domain, enhancing the network's ability to perceive and distinguish intricate tooth boundaries. Then, coupling geometric Transformers are designed to capture the intrinsic relationships of pair-wise dental models by promoting the exchange of relevant information to gain a comprehensive understanding of the overall tooth structure, allowing for better identification of adjacent teeth with similar appearances. Finally, a collaborative cross-scale feature fusion (CCFF) strategy is utilized to obtain interactive and complementary information by modeling the inter-relationships between dual-stream features. Experimental results on a clinical dental model dataset demonstrate that the proposed CGT-CLF framework outperforms state-of-the-art methods, delivering superior segmentation performance.},
  archive      = {J_TMM},
  author       = {Zhijie Lin and Zhaoshui He and Chang Liu and Hao Liang and Wenqing Su and Ji Tan and Jing Guo},
  doi          = {10.1109/TMM.2025.3599046},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A collaborative learning framework with coupling graph transformers for 3D tooth segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-DINO: Cross the deep MLP and transformer for small object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small Object Detection (SOD) poses significant challenges due to limited information and the model's low class prediction score. While Transformer-based detectors have shown promising performance, their potential for SOD remains largely unexplored. In typical DETR-like frameworks, the CNN backbone network, specialized in aggregating local information, struggles to capture the necessary contextual information for SOD. The multiple attention layers in the Transformer Encoder face difficulties in effectively attending to small objects and can also lead to blurring of features. Furthermore, the model's lower class prediction score of small objects compared to large objects further increases the difficulty of SOD. To address these challenges, we introduce a novel approach called Cross-DINO. This approach incorporates the deep MLP network to aggregate initial feature representations with both short and long range information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to integrate these initial representations to the Transformer Encoder feature, enhancing the details of small objects. Additionally, we introduce a new kind of soft label named Category-Size (CS), integrating the Category and Size of objects. By treating CS as new ground truth, we propose a new loss function called Boost Loss to improve the class prediction score of the model. Extensive experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D datasets demonstrate that Cross-DINO efficiently improves the performance of DETR-like models on SOD. Specifically, our model achieves 36.4% AP$_{S}$ on COCO for SOD with only 45M parameters, outperforming the DINO by +4.4% AP$_{S}$ (36.4% vs. 32.0%) with fewer parameters and FLOPs, under 12 epochs training setting.},
  archive      = {J_TMM},
  author       = {Guiping Cao and Wenjian Huang and Xiangyuan Lan and Jianguo Zhang and Dongmei Jiang and Yaowei Wang},
  doi          = {10.1109/TMM.2025.3599074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-DINO: Cross the deep MLP and transformer for small object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complementary and contrastive learning for audio-visual segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps that correlate with the auditory signals of objects. This field has seen significant progress with numerous CNN and Transformer-based methods enhancing the segmentation accuracy and robustness. Traditional CNN approaches manage audio-visual interactions through basic operations like padding and multiplications but are restricted by CNNs' limited local receptive field. More recently, Transformer-based methods treat auditory cues as queries, utilizing attention mechanisms to enhance audio-visual cooperation within frames. Nevertheless, they typically struggle to extract multimodal coefficients and temporal dynamics adequately. To overcome these limitations, we present the Complementary and Contrastive Transformer (CCFormer), a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively. Our CCFormer initiates with the Early Integration Module (EIM) that employs a parallel bilateral architecture, merging multi-scale visual features with audio data to boost cross-modal complementarity. To extract the intra-frame spatial features and facilitate the perception of temporal coherence, we introduce the Multi-query Transformer Module (MTM), which dynamically endows audio queries with learning capabilities and models the frame and video-level relations simultaneously. Furthermore, we propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across both modalities in the unified feature space. Through the effective combination of those designs, our method sets new state-of-the-art benchmarks across the S4, MS3 and AVSS datasets.},
  archive      = {J_TMM},
  author       = {Sitong Gong and Yunzhi Zhuge and Lu Zhang and Pingping Zhang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3599048},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Complementary and contrastive learning for audio-visual segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SkeFormer: Skeletal cues-aware bone point relationship learning for efficient FBIC via transformers. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3603431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to identify endangered bird species in complex outdoor environments has attracted significant attention in the fields of computer vision and machine learning. Previous studies on fine-grained bird image classification (FBIC) face numerous challenges, such as environmental occlusions and arbitrary postures, which limit the accuracy and robustness of existing methods. To address these challenges and enable more reliable bird species identification in extreme outdoor conditions, we propose a novel skeletal cues-aware bone point relationship learning for efficient FBIC via Transformers (SkeFormer). To the best of our knowledge, this is the first time skeletal relationships have been introduced to the FBIC task. Our model introduces three key modules: the skeletal relationship mining (SRM) module, the multilevel feature generation (MFG) module, and the key feature selection (KFS) module. Specifically, in SRM, the model mines the skeletal relationships among different bird species. In MFG, multiscale information is aggregated by connecting features across multiple layers. The KFS module selects key immutable regions of birds based on the learned skeletal relationships. Extensive experiments on two benchmark datasets, CUB-200-2011 and NABirds, show that SkeFormer outperforms existing state-ofthe- art models. The code for SkeFormer will be publicly available.},
  archive      = {J_TMM},
  author       = {Hai Liu and Qiang Chen and Zhibing Liu and Tingting Liu and Li Zhao and Zhaoli Zhang and You-Fu Li},
  doi          = {10.1109/TMM.2025.3603431},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SkeFormer: Skeletal cues-aware bone point relationship learning for efficient FBIC via transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rivisting source-free domain adaptation object detection in thresholds. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3590905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptive object detection (SFOD) aims to transfer models pre-trained on the source domain to the unlabeled target domain without requiring access to the source data. Most existing SFOD methods leverage pseudo-labels for self-supervised training in the target domain. We investigate the limitations of threshold techniques to obtain high-quality pseudo-labels. In response, we design the Sequential SourceFree domain adaptive Object Detection (S-SFOD) algorithm, which enhances the quality of pseudo-labels at both the image and instance levels. At the image level, we reconstruct the training dataset, prioritizing the training of images that yield more reliable pseudo-labels to help the model acquire valuable target domain knowledge in the initial training stages. At the instance level, we introduce an adaptive local-global threshold method to balance the quality and quantity of pseudo-labels by dynamically adjusting the thresholds based on the model's learning progress. By improving the quality of pseudo-labels through these complementary techniques at both the image and instance levels, we effectively transfer knowledge from the source domain to the target domain. Extensive experiments on multiple cross-domain object detection datasets demonstrate that our proposed method outperforms current state-of-the-art SFOD algorithms. The code and model will be released.},
  archive      = {J_TMM},
  author       = {Yuchen Dong and Chengeyang Li and Yongqiang Xie and Zhongbo Li},
  doi          = {10.1109/TMM.2025.3590905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rivisting source-free domain adaptation object detection in thresholds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointMax: Self-boosted local sampling for 3D point cloud analysis. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local sampling plays a key role in modeling 3D point clouds. Due to the disordered and unstructured nature of point cloud data, conventional 3D deep models such as PointNet++ and its variants usually employ random or fixed rules to sample local neighborhoods, leading to considerable redundancy in the feature aggregation process. In this paper, we propose a self-supervised method for learning to adaptively select effective neighbors. Firstly, we observe that only a part of sampled points contributes to the aggregated features after the max-pooling operation in existing point cloud models. Then, based on this observation, we propose a simple and task-oriented metric to evaluate the sampling efficiency by measuring the effective neighbors in the feature aggregation process. The metric is also used to supervise a lightweight neighborhood scoring module (NSM), which is designed to efficiently select effective neighboring points from a wider range of neighbors to reduce the computational cost and keep the performance superior. To further improve the performance, we introduce Neighborhood Attention in the feature aggregation process according to the importance score of neighborhood points predicted by NSM. Experimental results show that our method is simple and efficient, and can be applied to most tasks and models to reduce the computational cost and keep the performance superiority. Our code is available at https://github.com/sunshuofeng/PointMax_Code},
  archive      = {J_TMM},
  author       = {Shuofeng Sun and Yongming Rao and Jiwen Lu and Haibin Yan},
  doi          = {10.1109/TMM.2025.3590932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointMax: Self-boosted local sampling for 3D point cloud analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt-guided prototype-aware commonality and discrimination learning for zero-shot skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Skeleton-Based Action Recognition (ZSSAR) is an emerging research field focused on developing alignment models that connect skeleton movements with action definitions, thus enabling generalization to unobserved actions. Current methods often employ generative models to reconstruct cross-modal features or enhance mutual information across modalities for alignment. However, when applied to unseen action categories, these models often neglect the inherent consistency among basic actions, thereby diminishing their generalization capabilities. Furthermore, imprecise annotations fail to capture the rich semantic details of actions, resulting in misalignment. Inspired by human cognitive processes and chain of thought, we argue that integrating prior information about human actions with intrinsic commonality knowledge of basic actions is essential for ZSSAR. To actualize this, we propose a novel method termed Prompt-guided Prototype-aware Commonality and Discrimination Learning (PP-CDL). This method utilize the comprehensive world knowledge contained in LLMs, employing tailored prompts to partition seen action categories into distinct, non-overlapping prototype spaces that embody the commonality knowledge of basic actions. Subsequently, we introduce the Inter- and Intra-Prototype Discriminating (I2PD) module and the Intra-Prototype Commonality Mining (IPCM) module. The I2PD amplifies the distinctiveness of knowledge within prototypes, furnishing a personalized search space for the recognition of unseen actions. In contrast, the IPCM models the shared commonality concept within prototypes, bolstering the consistency between skeleton action representations and corresponding text knowledge representations. Experiments on different skeleton action benchmarks demonstrate the significant improvement of our method over existing alternatives.},
  archive      = {J_TMM},
  author       = {Xingyu Zhu and Xiangbo Shu and Peng Huang and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3590904},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prompt-guided prototype-aware commonality and discrimination learning for zero-shot skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel secure and robust recoverable cryptographic mosaic technique. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image mosaic is a prevalent technique to conceal critical content in images. However, conventional mosaic techniques cannot be recovered using a small-sized key, as they require retransmission of the original images for perfect recovery. In this work, we propose a novel, computationally efficient, and effective recoverable image-mosaic technique. A key advantage of our proposed image-mosaic scheme is its robust performance across a range of adjustable key lengths. Our technique effectively conceals original information even with a small-sized key of only a few bits. To evaluate its performance, we introduce a new image-similarity metric based on the magnitude of the discrete cosine transform (DCT). This metric exhibits several advantageous mathematical properties, including the ability to quantify the perceptibility of major content in mosaicked images, invariance under image reflections and 180-degree rotations, and insensitivity to small translations. Finally, numerical experiments demonstrate that our method outperforms existing recoverable image-mosaic techniques and performs consistent across varying key lengths. We also compare the run-times required by our proposed new scheme with those required by other existing recoverable image-mosaic methods and the state-of-the-art image-encryption methods to exhibit the computational efficiency of our proposed new scheme.},
  archive      = {J_TMM},
  author       = {Chi Yung and Scott C.-H. Huang and Hsiao-Chun Wu and Che-Hua Li},
  doi          = {10.1109/TMM.2025.3590922},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Novel secure and robust recoverable cryptographic mosaic technique},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manifold embedding for fast and accurate 3D reconstruction. <em>TMM</em>, 1-20. (<a href='https://doi.org/10.1109/TMM.2025.3590908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the fusion process in RGB-D reconstruction systems is to verify and update the 3D model while ensuring both completeness and accuracy. However, achieving precise dense correspondences in a point-to-point or pixel model during this process is challenging and computationally intensive. To address this challenge, we propose a Manifold Embedding framework that facilitates rapid point-to-surface fusion, removing the need for direct point-to-point or pixel correspondences. Our approach consists of three main steps: 1) Manifold Voxel: We transform discrete point sets into smooth surfaces using the Implicit Moving Least Squares (IMLS) method; 2) Two-Step Filtering: We enhance reconstruction accuracy through a two-step filtering technique that evaluates sampling points based on probabilistic measures; 3) Embedding for Smooth Surface: Lastly, we embed points into a smooth manifold surface represented via IMLS, ensuring high-quality reconstructed surfaces. Extensive experiments on both real and synthetic 3D scenes demonstrate the effectiveness of our Manifold Embedding framework. For instance, on the public Replica dataset, our method surpasses state-of-the-art fusion techniques regarding both completeness and accuracy. Our average accuracy is 2.11 cm and completeness is 2.80 cm, while NICE-SLAM achieves 2.85 cm and 3.00 cm, respectively (with lower values indicating better performance). Overall, our proposed method provides superior reconstruction quality and enhanced computational efficiency (See Fig. 1).},
  archive      = {J_TMM},
  author       = {Duo Chen and Zixin Tang and Ke Song and Xingyu Peng and Wuque Cai and Hongze Sun and Dezhong Yao and Daqing Guo},
  doi          = {10.1109/TMM.2025.3590908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Manifold embedding for fast and accurate 3D reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointCloud-text matching: Benchmark dataset and baseline. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3590931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching (PTM), which aims to identify the exact cross-modal instance that matches a given point-cloud query or text query. PTM has potential applications in various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there is a lack of suitable and targeted datasets for PTM in practice. To address this issue, we present a new PTM benchmark dataset, namely SceneDepict-3D2T. We observe that the data poses significant challenges due to its inherent characteristics, such as the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which render existing cross-modal matching methods ineffective for PTM. To overcome these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two key modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention mechanisms to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL enhances robustness against mismatching by dividing negative pairs into clean and noisy subsets and assigning them forward and reverse optimization directions, respectively. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa.},
  archive      = {J_TMM},
  author       = {Yanglin Feng and Yang Qin and Dezhong Peng and Hongyuan Zhu and Xi Peng and Peng Hu},
  doi          = {10.1109/TMM.2025.3590931},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointCloud-text matching: Benchmark dataset and baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Micro-image domain view synthesizer for free navigation with focused plenoptic cameras. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel, first-of-its-kind view synthesis method for plenoptic images, which enables the direct manipulation of images in the micro-images array format, thereby bypassing intermediate transformation steps. Current plenoptic imaging approaches typically rely on an initial conversion to dense multiview images, also known as subaperture images extraction. However, the use of subaperture images presents two main limitations that ultimately impact further processing. First, existing subaperture view extraction methods offer limited control over camera parameters, resolutions, and poses of the subaperture views, which are also constrained to a small area around the main lens, thus restricting free navigation. Second, subaperture images are susceptible to artifacts which can propagate to subsequent processes such as calibration, depth estimation and view synthesis. In this paper, we propose a camera model that enables depth image-based rendering with plenoptic cameras, in a way that allows for the direct synthesis of any target viewpoint. In our evaluation, we show that our method expands view synthesis extrapolation to a range that is two to three times greater than that of pipelines requiring a conversion to subaperture images, including generally accepted tools such as depth image-based rendering and learning-based rendering approaches.},
  archive      = {J_TMM},
  author       = {Sarah Fachada and Daniele Bonatto and Gauthier Lafruit and Mehrdad Teratani},
  doi          = {10.1109/TMM.2025.3590906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Micro-image domain view synthesizer for free navigation with focused plenoptic cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging fuzzy manifold intra-class correlation and inter-class separability for online multilabel streaming features analysis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional multilabel feature selection (MFS) typically relies on pre-computing global information within the feature space. However, in real-world applications, features are dynamically generated and continuously arrive over time, known as streaming features, rendering many existing approaches ineffective. Some MFS methods for streaming features have been developed, several challenges persist: (1) Previous research often uses certain strategies to model streaming feature evaluation, failing to process fuzzy information effectively; (2) The maximum correlation between features and class is emphasized, while inter-class separability is ignored, leading to inaccurate feature evaluation; (3) The continuous influx of streaming features brings the dynamics and unknowns to data distribution, has been largely overlooked in previous work; (4) Streaming feature selection requires immediate feedback on newly arriving features, posing challenges to the algorithm's real-time responsiveness. Motivated by these observations, this paper introduces a novel online MFS strategy for streaming features. First, the weighted manifold distance is designed, and the fuzzy manifold similarity learning strategy is formalized to analyze the instance relationships of unknown distribution. Second, the fuzzy manifold intra-class correlation and inter-class separability are devised to quantify feature discriminability. Finally, a novel multilabel streaming feature analysis framework is established, with feature discriminability as the guiding factor. Incoming features are categorized as weakly relevant, strongly relevant, or redundant, culminating in generating a reliable feature selection subset. Extensive experiments on fifteen public datasets demonstrate that our algorithm achieves competitive performance compared to nine state-of-the-art offline and online algorithms.},
  archive      = {J_TMM},
  author       = {Tengyu Yin and Hongmei Chen and Jihong Wan and Keyu Liu and Zhong Yuan and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1109/TMM.2025.3590919},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging fuzzy manifold intra-class correlation and inter-class separability for online multilabel streaming features analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSLL-face: Distributed supervision-integrated framework for low-light face detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-light environments, human vision is severely limited by weak light sources, leading to significantly reduced visual capabilities. Similarly, in machine vision, low-light recognition tasks such as nighttime autonomous driving and surveillance tasks involving the detection of small faces in low-light conditions are more challenging than tasks in normal lighting. Current low-light face detection models lack adaptability to different low-light conditions, and the accuracy of face detection remains unsatisfactory. In this paper, we propose a novel face detection framework DSLL-Face, specifically designed to tackle the challenges of face detection in low-light environments. Our proposed DarkHead, featuring a specialized branch designed to predict the distribution of bounding boxes, thereby substantially enhances the supervision of bounding box localization. This innovative approach effectively resolves the issue of blurry bounding boxes and significantly increases the accuracy of predicted positions. We employ a novel loss function tailored for detecting small faces, enhancing the sensitivity and effectively addressing the blurriness issues in small face detection. Furthermore, we leverage the Channel Grouping and Partial Convolution block (CGP) to enhance multi-scale expression capabilities. We develop the EMNet-pro model with the aim of further enhancing images to improve their adaptability under various low-light conditions. Extensive experiments demonstrate that our model exhibits outstanding capability in low-light face detection on the DARK FACE dataset and achieves significantly better performance compared to existing state-of-the-art frameworks.},
  archive      = {J_TMM},
  author       = {Shuhong Chen and Kairen Chen and Guojun Wang and Sheng Wen and Zhili Zhou},
  doi          = {10.1109/TMM.2025.3590911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSLL-face: Distributed supervision-integrated framework for low-light face detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal depression detection in interview via exploring emotional distribution information. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, automatic depression detection (ADD) technology has been rapidly developed to boost an objective and assistive diagnosis for major depressive disorder (MDD) with the help of artificial intelligence technology and various physiological and psychological data. Despite emotion being an important reflection of mental status and frequently related to depression symptoms, few recent multi-modal ADD methods take emotional information into account. To address the above issue, we propose to explore emotional distribution information in interviews to assist multi-modal ADD model. On one hand, we use large language models (LLMs) to automatically recognize emotion of text data, and re-organize the data guided by the valence attribute of emotion, which facilitates our model being aware of difference in emotion distribution. On the other hand, we design the emotion encoding which enhances the proposed model to consider the emotional distribution information in its decision-making process. Extensive experiments are conducted by comparing with state-of-the-art ADD methods as well as the ablation study on different modules of the proposed method. More importantly, our experimental results can confirm the research findings in the psychology field, where more attention on negative emotion information is demanded in distinguishing different depressive status.},
  archive      = {J_TMM},
  author       = {Zhiyuan Zhou and Yanrong Guo and Shijie Hao and Richang Hong},
  doi          = {10.1109/TMM.2025.3590939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal depression detection in interview via exploring emotional distribution information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-division based progressive learning for source-free domain adaptation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing privacy and portability concerns, source-free domain adaptation requires only a source pre-trained model and an unlabeled target domain, allowing for effective adaptation to the target data. Most existing self-training methods focus on selecting and exploiting samples with reliable predictions, often neglecting others. Inspired by the finding that deep models learn clean samples faster than noisy ones, we propose a domain-division based progressive learning method named DPL. Specifically, our approach consists of two alternating stages, each beginning with the division of the target domain into easy-to-adapt and hard-to-adapt subdomains based on adaptation difficulty, followed by neighborhood-based pseudo label assignment. In stage one, we enhance classification accuracy through uncertainty-aware self-training and alignment of corresponding classes between subdomains. Stage two then applies tailored learning strategies to each subdomain, starting with consistency learning on the easy-to-adapt samples and progressing to utilizing local structural information for the more challenging ones, thereby mining the intrinsic properties of the target data. Extensive experiments on several widely used benchmarks validate the effectiveness of our approach, demonstrating superior performance compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Pan Liu and Jing Li and Meng Zhao and Wanli Xue and Qinghua Hu and Shengyong Chen},
  doi          = {10.1109/TMM.2025.3590903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain-division based progressive learning for source-free domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced reasoning via multimodal LLMs and collaborative inference. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a prevalent task that can facilitate the perception of the real world by the visually impaired. However, many VQA models tend to rely on superficial correlations in datasets for predictions rather than genuine reasoning, limiting their real-world applicability. While existing methods address this issue by incorporating debiasing strategies during training, they typically assume prior knowledge of out-of-distribution (OOD) test sets and then tailor debiasing strategies and select optimal models on the basis of the OOD samples. This reliance on OOD test data, however, is unrealistic in practical applications. To address this, some works introduce test-time adaptation techniques to mitigate dataset shifts during model deployment. Despite their potential, these methods risk catastrophic forgetting as they update models at test time without access to the ground-truth answers or the source data. An emerging solution involves leveraging the extensive knowledge embedded in Large Language Models (LLMs) to support reasoning tasks, yet their language-only input restricts flexibility in multimodal tasks. To bridge this gap, we propose leveraging the zero-shot capability of Multimodal Large Language Models (MLLMs). To optimise computational efficiency, we introduce a novel VQA Collaborative Inference framework (VQA-CI) that integrates MLLMs (e.g. BLIP-2 Flan T5) with VQA specialists (e.g. UpDn). This framework initially processes samples through VQA specialists and subsequently determines the necessity for re-evaluation with MLLMs based on predefined bias and reliability indicators. Experiments on the GQA-OOD and VQA-CP v2 datasets show that our VQA-CI achieves significant performance gains, with accuracy improvements of around 6% over state-of-the-art methods, underscoring the effectiveness of our VQA-CI.},
  archive      = {J_TMM},
  author       = {Zhiquan Wen and Mingkui Tan and Yaowei Wang and Qingyao Wu and Qi Wu},
  doi          = {10.1109/TMM.2025.3590940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced reasoning via multimodal LLMs and collaborative inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive clustering and weighted regularization contrastive learning framework for unsupervised person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (ReID) has recently gained significant attention from researchers. ReID matches images of the same person from different camera views in various scenes without any labels. Existing clustering methods primarily rely on a fixed threshold (the maximum distance between sample points and clustering centroids) and overlook the importance of adjusting this threshold during continuous model optimization. This mismatch between clustering thresholds and inter- or intra-class spacing reduces clustering accuracy. To address this issue, this study proposes an Adaptive Clustering and Weighted Regularization Contrastive Learning (ACWRCL) framework for unsupervised person ReID. The ACWRCL framework comprises two main components: (1) the Clustering Threshold Adaptive Adjustment (CTAA) module, and (2) the Weighted Regularization Contrastive Learning (WRCL) module. The CTAA module dynamically adjusts the clustering threshold to align with model optimization, ensuring that the threshold remains within an appropriate range to prevent under- or over-robustness in the clustering model. The WRCL module uses the similarity ratio between the query sample and the clustering centroid relative to the overall similarity of all samples with the same labels as the query sample. This ratio is used as the weight in the loss function to penalize incorrect clustering and improve pseudo-label generation accuracy. Extensive experiments on public ReID datasets—Market-1501, MSMT17, Veri776, CUHK03, and PersonX—demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Mingfu Xiong and Kaikang Hu and Zhongyuan Wang and Ruimin Hu and Khan Muhammad and Javier Del Ser and Xiaokang Yang and anbd Bin Sheng},
  doi          = {10.1109/TMM.2025.3590938},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive clustering and weighted regularization contrastive learning framework for unsupervised person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical token-aware cross-modality reconstruction for visible-infrared person re-identification. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) aims to query the same pedestrian's visible (infrared) images in the gallery set from the infrared (visible) images. VI-ReID not only needs to deal with the challenging factors like pose variation and occlusion, but also requires handling the large modality discrepancy. Previous methods mainly focus on learning single-scale modality-shared features and do not effectively explore the multi-scale features of two modalities from both short-range and long-range perspectives. In order to solve these problems, this paper proposes a novel Hierarchical Token-Aware Cross-Modality Reconstruction (HTCR) network to significantly mitigate the modality discrepancy for effective VI-ReID. The HTCR network consists of two main components, i.e., Hierarchical Token-aware Fusion (HTF) and Cross-modality Feature Reconstruction (CFR). The HTF module first bidirectionally exchanges the short-range and long-range multi-scale modality-shared features with a few learnable tokens to achieve discriminative pedestrian features by making full use of the advantages of both Convolutional Neural Network (CNN) and Transformer. Moreover, the CFR module reconstructs global and local pedestrian features of one modality by using the token sequence of the other modality with multi-scale cues to further explore the relationship between the two distinct modalities and alleviate the modality discrepancy. In addition, the Modality-shared feature Reconstruction (MR) loss is leveraged to reduce the noises between the reconstructed and the target features. Experimental results indicate that the proposed HTCR can significantly improve the VI-ReID performance and outperform the state-of-the-art methods on the cross-modality SYSU-MM01, RegDB, and LLCM datasets.},
  archive      = {J_TMM},
  author       = {Si Chen and Liuxiang Qiu and Da-Han Wang and Wentao Zhu and Yang Hua and Yan Yan},
  doi          = {10.1109/TMM.2025.3590933},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical token-aware cross-modality reconstruction for visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid network for extended reality environments. <em>TMM</em>, 1-17. (<a href='https://doi.org/10.1109/TMM.2025.3590913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly evolving realm of Extended Reality (XR) demands high bandwidth and low-latency communication to support immersive experiences such as high-resolution 360-degree videos and real-time interactions in virtual reality gaming. In this study, “resources” are defined as digital assets essential for XR applications, divided into “static resources” (immutable media files such as textures and video segments) and “dynamic resources” (real-time user data and interactive elements crucial for user interactions). A primary challenge in XR environments is optimizing the delivery and caching of these resources within existing network infrastructures to enhance the Quality of Experience (QoE) for users. We introduce a novel hybrid network architecture that integrates resource caching, user-to-user communication, and central server oversight. This architecture not only ensures reliable delivery but also significantly reduces communication latency. Preliminary experiments, conducted under conditions where each node in the network has a 10% chance of failing at any given time, demonstrate that our approach enhances the delivery efficiency of static resources by 68%, affecting 38% of communications, with an increase in latency observed in 4% of cases by 22%. For dynamic resources, it reduces latency in 89% of the cases by an average of 30%, though 8% of cases experienced a 36% increase in latency. These results affirm the effectiveness of our architecture in enhancing user experience in XR environments under challenging network conditions.},
  archive      = {J_TMM},
  author       = {Hexu Xing and Torsten Braun},
  doi          = {10.1109/TMM.2025.3590913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hybrid network for extended reality environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dual video summarization: From dynamic keyframes to captions. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization and captioning condense content by selecting keyframes and generating language descriptions, integrating both visual and textual perspectives. Existing video-and-language learning models typically select multiple frames as proxies rather than analyzing all frames, which improves computational efficiency but may not adequately represent the original content without redundancy. In this paper, we propose an adaptive dual video summarization framework and demonstrate its effectiveness within the context of video captioning. Given the video frames, we extract visual representations using a video-domain fine-tuned ViT model to narrow the domain shift. The keyframes are summarized based on the frame-level scores. To minimize the number of keyframes while ensuring captioning quality, we introduce a cross-modal video summarizer that selects the most semantically consistent frames according to pseudo score labels. Furthermore, we incorporate an adaptive keyframe selector that determines the optimal number of keyframes based on the video's complexity and content, enhancing the framework's adaptability and generalization. The proposed adaptive keyframe selector enables the framework to handle diverse video content, making it more generalizable and applicable to real-world scenarios.We designed a ranking scheme to assess the video's static appearance and temporal dynamics from score-based and time-based perspectives. To conclude, we use a lightweight LSTM decoder to generate descriptions. Experimental results on the MSR-VTT, MSVD and VATEX benchmarks demonstrate that our adaptive dual video summarization framework can effectively convey the same semantic information as the original video while using a significantly reduced number of keyframes, leading to improved video captioning performance.},
  archive      = {J_TMM},
  author       = {Zhenzhen Hu and Ao Sun and Zhenshan Wang and Jia Li and Zijie Song and Richang Hong and Meng Wang},
  doi          = {10.1109/TMM.2025.3590936},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive dual video summarization: From dynamic keyframes to captions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual semantic contextualization network for multi-query image retrieval. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Query Image Retrieval (MQIR) aims to establish connections between vision and language by exploring fine-grained region-query alignments. It is still a challenging task owing to its intrinsical ambiguity, where a query matches with multiple semantically similar regions and introduces misleading noises. Although researchers have made great efforts to alleviate the ambiguity in many retrieval-related tasks, there are few attempts considering this bottleneck in MQIR, which greatly limits present performance. To this end, we propose a novel Visual Semantic Contextualization Network (VSCN) to mitigate ambiguity by capturing the contextual knowledge within each image-text pair. Specifically, we first develop a Context Semantic Perception (CSP) module to capture the dual-level context, where a visual context transformer explores the intra-context within regions, and a cross-modal context transformer mines the inter-context among concatenated visual-linguistic embeddings. Then, to yield superior contextual understanding, we strengthen the connotations in context via a Context Semantic Interaction (CSI) module. Particularly, knowledge distillation is first employed to transfer the CLIP-guided semantic into the regional intra-context to complement the potential background information. Then, the intra-context & inter-context interaction is conducted via the self-attention mechanism to link the dual-level context and obtain the interacted contextual knowledge. Our method is evaluated on the Visual Genome dataset and substantially outperforms the state-of-the-art methods (30.3% improvements on Recall@1 in the first round). Our source codes will be released at https://github.com/zhli-cs/VSCN.},
  archive      = {J_TMM},
  author       = {Zhong Ji and Zhihao Li and Yan Zhang and Yanwei Pang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3590927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual semantic contextualization network for multi-query image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR-HMR: 3D human mesh recovery from LiDAR. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human mesh recovery (HMR) holds significant utility in many applications. Studying HMR involving various types of sensors is necessary, as it enables the acquisition of human meshes in diverse scenes. Unlike HMR based on RGB images, HMR based on LiDAR has received considerably less attention in previous works. The major challenge in estimating human poses and meshes from sparse point clouds lies in the sparsity, noise, and incompletion of LiDAR point clouds. To address these challenges, we propose a LiDAR-based 3D human mesh recovery algorithm, called LiDAR-HMR. This algorithm involves estimating a sparse representation of a human (3D human pose) and gradually reconstructing the body mesh. To better leverage the 3D structural information of point clouds, we propose a point-cloud-to-SMPL pipeline that uses the original point cloud features to guide the reconstruction. The experimental results on four publicly available datasets demonstrate the effectiveness of LiDAR-HMR. The codes are available at https://github.com/soullessrobot/LiDAR-HMR.},
  archive      = {J_TMM},
  author       = {Bohao Fan and Wenzhao Zheng and Jianjiang Feng and Jie Zhou},
  doi          = {10.1109/TMM.2025.3590928},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LiDAR-HMR: 3D human mesh recovery from LiDAR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural-enhanced rate adaptation and computation distribution for emerging mmwave multi-user 3D video streaming systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate multitask edge-user communication-computation resource allocation for $360^\circ$ video streaming in an edge-computing enabled millimeter wave (mmWave) multi-user virtual reality system. To balance the communication-computation trade-offs that arise herein, we formulate a video quality maximization problem that integrates interdependent multitask/multi-user action spaces and rebuffering time/quality variation constraints. We formulate a deep reinforcement learning framework for multi-task rate adaptation and computation distribution (MTRC) to solve the problem of interest. Our solution does not rely on a priori knowledge about the environment and uses only prior video streaming statistics (e.g., throughput, decoding time, and transmission delay), and content information, to adjust the assigned video bitrates and computation distribution, as it observes the induced streaming performance online. Moreover, to capture the task interdependence in the environment, we leverage neural network cascades to extend our MTRC method to two novel variants denoted as R1C2 and C1R2. We train all three methods with real-world mmWave network traces and $360^\circ$ video datasets to evaluate their performance in terms of expected quality of experience (QoE), viewport peak signal-to-noise ratio (PSNR), rebuffering time, and quality variation. We outperform state-of-the-art rate adaptation algorithms, with C1R2 showing best results and achieving $5.21-6.06$ dB PSNR gains, $2.18-2.70$x rebuffering time reduction, and $4.14-4.50$ dB quality variation reduction.},
  archive      = {J_TMM},
  author       = {Babak Badnava and Jacob Chakareski and Morteza Hashemi},
  doi          = {10.1109/TMM.2025.3590902},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural-enhanced rate adaptation and computation distribution for emerging mmwave multi-user 3D video streaming systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-city: A source-free domain adaptation method for city-scale point cloud semantic segmentation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {City-scale point cloud semantic segmentation is an important yet challenging task. Despite progress, existing methods rely heavily on point-wise annotations. An alternative solution is to apply the Unsupervised Domain Adaptation (UDA) approach. Recently, the 2D foundation model has achieved significant progress with training with internet-scale images. Therefore, adapting 2D foundation models to 3D City-scale point clouds is an attempting idea. Due to the data protection and storage issue, 2D source domain data is typically unavailable. Thus, we focus on Source-Free Domain Adaptation (SFDA) and propose a Source-Free City-scale point cloud semantic segmentation method, namely SF-City. Our method leverages knowledge from 2D pre-trained models to generate point-wise pseudo labels for training a 3D semantic segmentation network. We convert point clouds into remote-sensing-like images using Bird's-Eye-View (BEV) projection. However, directly using source models for pseudo label generation is hindered by domain gaps such as viewpoint variations, concept divergences, and geometry loss. To tackle these problems, we propose a Multi-scale Content Feature Extractor (MCFE) to extract holistic and contextual feature representations. Then, an Uncertainty-guided Inter-Model Feature Integrator (UIFI) is introduced to integrate inherent knowledge across source models. Furthermore, the Geometric-guided Pseudo Label Generator (GPLG) is leveraged to introduce geometric information to regulate pseudo labels. Through extensive experiments on two public benchmarks, SF-City demonstrates superior performance, achieving an mIoU of 28.8% on the SensatUrban dataset, outperforming recent state-of-the-art methods CLIPFO3D by about 6.3%.},
  archive      = {J_TMM},
  author       = {Yan Liu and Hongyuan Zhu and Yinjie Lei and Hao Liu and Yun Pei and Yulan Guo},
  doi          = {10.1109/TMM.2025.3590934},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SF-city: A source-free domain adaptation method for city-scale point cloud semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FoodLMM: A versatile food assistant using large multi-modal model. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Multi-modal Models (LMMs) have made impressive progress in many vision-language tasks. Nevertheless, the performance of general LMMs in specific domains is still far from satisfactory. This paper proposes FoodLMM, a versatile food assistant based on LMMs with various capabilities, including food recognition, ingredient recognition, recipe generation, nutrition estimation, food segmentation, and multi-round conversation. To facilitate FoodLMM in dealing with tasks beyond pure text output, we introduce a series of novel task-specific tokens and heads, enabling the model to predict food nutritional values and multiple segmentation masks. We adopt a two-stage training strategy. In the first stage, we utilize multiple public food benchmarks for multi-task learning by leveraging the instruct-following paradigm. In the second stage, we construct a multi-round conversation dataset and a reasoning segmentation dataset to fine-tune the model, enabling it to conduct professional dialogues and generate segmentation masks based on complex reasoning in the food domain. Our fine-tuned FoodLMM achieves state-of-the-art results across several food benchmarks. Our code, models, and datasets are available at https://github.com/YuehaoYin/FoodLMM.},
  archive      = {J_TMM},
  author       = {Yuehao Yin and Huiyan Qi and Bin Zhu and Jingjing Chen and Yu-Gang Jiang and Chong-Wah Ngo},
  doi          = {10.1109/TMM.2025.3590924},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FoodLMM: A versatile food assistant using large multi-modal model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRIF: Data-free knowledge distillation via stable regulation and input filtering. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3565963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free knowledge distillation (DFKD) enables knowledge transfer from a pre-trained teacher to a student network without accessing the real dataset. However, generator-based DFKD methods struggle to ensure that the synthetic images accurately reflect the real dataset distribution. The update of the generator network relies heavily on teacher category guidance, but varying teacher prediction accuracy across categories leads to inconsistent synthetic image quality. Such variations introduce a distribution shift between synthetic and real datasets, negatively impacting student network performance during knowledge distillation. To address this challenge, we propose the SRIF, comprising two components: Student-Driven Flexible Filtering (SDFF) and Re-weighting for Independent Regularization (RIR). SDFF filters out synthetic images affected by the category distribution shift during data generation, producing a more reliable dataset. RIR, applied during distillation, encourages the student to learn stable causal relationships through sample reweighting. Both components flexibly integrate into existing DFKD frameworks, improving performance while reducing training costs.},
  archive      = {J_TMM},
  author       = {Zherui Zhang and Rongtao Xu and Changwei Wang and Shibiao Xu and Jie Zhou and Wenhao Xu and Longxiang Gao and Wenbo Xu and Li Guo},
  doi          = {10.1109/TMM.2025.3565963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SRIF: Data-free knowledge distillation via stable regulation and input filtering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph consistency learning with relational distillation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3543068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of semi-supervised learning on graphs, which has recently aroused widespread interest in relational data mining. The focal point of exploration in this area has been the utilization of graph neural networks (GNNs), which stand out for excellent performance. Previous methods, however, typically rely on the limited labeled data while ignoring the abundant structural information in unlabeled nodes inherently on graphs, easily resulting in overfitting, especially in scenarios where only a few label nodes are available. Even worse, GNNs, despite their success, are constrained by their ability to solely capture local neighborhood information through message-passing mechanisms, thereby falling short in modeling higher-order dependencies among nodes. To circumvent the above drawbacks, we propose a simple yet effective framework called Hypergraph COnsistency LeArning (HOLA). Specifically, we employ a collaborative distillation framework consisting of a teacher network and a student network. To achieve effective interaction, we propose momentum distillation, a self-training method that enables the student network to learn from pseudo-targets generated by a momentum teacher network. Further, a novel hypergraph structure learning network is developed to model complex high-order relations among nodes with relational consistency learning, thereby transferring the knowledge to the student network. Extensive experiments conducted on a variety of benchmark datasets demonstrate the superior performance of the HOLA over various state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Siyu Yi and Zhengyang Mao and Yifan Wang and Yiyang Gu and Zhiping Xiao and Chong Chen and Xian-Sheng Hua and Ming Zhang and Wei Ju},
  doi          = {10.1109/TMM.2025.3543068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hypergraph consistency learning with relational distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual collaborative learning for weakly supervised video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3535377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised anomaly detection is to identify the time window when an anomaly event happened based on the video-level label indicating whether the video contains anomaly event or not. Recent efforts have focused on leveraging multi-modal data, specifically combining visual and audio information, to enhance detection accuracy. While some studies have explored intra-video separation techniques, the primary emphasis remains on distinguishing potentially anomalous events scoring highest from those scoring lowest as normal events. Nevertheless, challenges persist in delineating boundaries between normal and abnormal events, particularly when visual differences are subtle. Our proposed framework, called Audio-Visual Collaborative Learning (AVCL), addresses the challenge of ambiguity in weakly supervised anomaly detection. Our core idea centers around utilizing both audio track variations and the perceptual robustness of visual information to detect and differentiate challenging cases, which composed of two essential modules: the Audio-Visual collaborative Hard cases Separation (AVHS) module and the Multi-modal Mutual Learning (MML) module. The AVHS module aims to address the challenge of discerning visually ambiguous clips in anomaly videos, differentiating between normal and abnormal events. To further improve detection accuracy, we introduce the Multi-modal Mutual Learning (MML) module, and this module enables a process of mutual learning to facilitate the exchange of knowledge and expertise between the single-modal branch and the multi-modal branch. We demonstrate that the proposed approach achieves state-of-the-art detection performance on benchmarks of XD-Violence and CCTV-Fights$_{sub}$ datasets.},
  archive      = {J_TMM},
  author       = {Jingke Meng and Huilin Tian and Ge Lin and Jian-Fang Hu and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3535377},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Audio-visual collaborative learning for weakly supervised video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gifts from gallery: Advancing image retrieval via unsupervised asymmetric feature fusion. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3535354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric retrieval systems, characterized by the deployment of models with varying capacities on platforms with differing computational and storage resources, pose a challenge in balancing retrieval efficiency and accuracy. The recent introduction of the Asymmetric Feature Fusion (AFF) paradigm has shown promise by enhancing existing asymmetric retrieval systems through feature fusion on the gallery side. However, its reliance on extensive human-annotated data hinders practical applicability. To this end, we introduce an innovative unsupervised training method tailored for AFF. Leveraging multiple gallery models as feature extractors, our approach exploits similarities among images encoded by these models as pseudolabels. For each gallery model, we calculate its adaptive weight through an in-depth exploration of contextual relationships among ranking list images in its embedding spaces. Subsequently, these weights are utilized to effectively fuse the image similarities encoded by different gallery models into a more robust one. The fused image similarities provide powerful pseudo-supervision for training AFF. Our unsupervised training approach enhances the generality and utility of AFF in real-world scenarios, particularly when labeled data is limited or expensive to obtain. Exhaustive experiments on various landmark retrieval datasets demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Wengang Zhou and Hui Wu and Min Wang and Houqiang Li},
  doi          = {10.1109/TMM.2025.3535354},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Gifts from gallery: Advancing image retrieval via unsupervised asymmetric feature fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards better distortion feature learning for object detection in top-view fisheye cameras. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2024.3521808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning in recent years, the performance of object detection under conventional cameras has been significantly improved. Nevertheless, due to the distortion caused by the fisheye cameras, detecting objects in this scenario remains a significant challenge. The dominant approaches focus on modifying the shape of the bounding box to better align the boundaries of the distorted object. However, these methods neglect the learning of spatial distortion information, which prevents them from satisfactory results. In this paper, we propose a novel fisheye camera detection network to learn distortion features better, dubbed SDANet. SDANet is composed of a series of SDABlocks, which are designed to learn spatial distortion features. Each SDABlock consists of multiple convolution kernels of different sizes, and it can generate the most suitable kernel based on the current input's distortion characteristics. Moreover, to address the limitations of the scarcity and uneven spatial distribution of fisheye image datasets on performance improvement, we propose a dedicated data augmentation strategy called Prominent Fisheye Distortion Augmentation (PFDAug). PFDAug can further introduce distortions to fisheye images, effectively alleviating these problems. Experimental results on the CEPDOF, MW-R, HABBOF, LOAF, and FishEye8k fisheye image datasets demonstrate that our method achieves state-ofthe-art performance.},
  archive      = {J_TMM},
  author       = {Pengbo Guo and Chengxu Liu and Xingsong Hou and Xueming Qian},
  doi          = {10.1109/TMM.2024.3521808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards better distortion feature learning for object detection in top-view fisheye cameras},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning visual conditioning tokens to correct domain shift for fully test-time adaptation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2024.3443633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully test-time adaptation aims to adapt the network model based on sequential analysis of input samples during the inference stage to address the cross-domain performance degradation problem of deep neural networks. This work is based on the following interesting finding: in transformer-based image classification, the class token at the first transformer encoder layer can be learned to capture the domain-specific characteristics of target samples during test-time adaptation. This learned token, when combined with input image patch embeddings, is able to gradually remove the domain-specific information from the feature representations of input samples during the transformer encoding process, thereby significantly improving the test-time adaptation performance of the source model across different domains. We refer to this class token as visual conditioning token (VCT). To successfully learn the VCT, we propose a bi-level learning approach to capture the longterm variations of domain-specific characteristics while accommodating local variations of instance-specific characteristics. Experimental results on the benchmark datasets demonstrate that our proposed bi-level visual conditioning token learning method is able to achieve significantly improved test-time adaptation performance by up to 1.9%},
  archive      = {J_TMM},
  author       = {Yushun Tang and Shuoshuo Chen and Zhehan Kan and Yi Zhang and Qinghai Guo and Zhihai He},
  doi          = {10.1109/TMM.2024.3443633},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning visual conditioning tokens to correct domain shift for fully test-time adaptation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DEER: Distribution divergence-based graph contrast for partial label learning on graphs. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2024.3408038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have emerged as powerful tools for graph classification tasks. However, contemporary graph classification methods are predominantly studied in fully supervised scenarios, while there could be label ambiguity and noise in real-world applications. In this work, we explore the weakly supervised problem of partial label learning on graphs, where each graph sample is assigned a collection of candidate labels. A novel method called D istribution Div e rgence-bas e d Graph Cont r ast (DEER) is proposed to address this issue. At the heart of our DEER is to measure the divergence among the underlying semantic distributions in the hidden space and this metric enables the identification of accurate positive graph pairs for effective graph contrastive learning. Specifically, we generate graph representations of augmented graph views that retain semantics and can be regarded as samples from the underlying semantic distributions. We employ a non-parametric metric to measure distribution divergence, which is then combined with pseudo-labeling to generate unbiased and target-oriented graph pairs. Furthermore, we introduce a label-correction method to eliminate noisy candidate labels, updating target labels using posterior distributions in a soft manner. Comprehensive experiments on various benchmarks demonstrate the superiority of our DEER in different settings compared to a range of state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Yiyang Gu and Zihao Chen and Yifang Qin and Zhengyang Mao and Zhiping Xiao and Wei Ju and Chong Chen and Xian-Sheng Hua and Yifan Wang and Xiao Luo and Ming Zhang},
  doi          = {10.1109/TMM.2024.3408038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DEER: Distribution divergence-based graph contrast for partial label learning on graphs},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cross-task transferability of adversarial examples via spatial and channel attention. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2024.3349925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples are well known to pose a security risk, when attacking deep learning models. While, most of existing adversarial attacks are designed to attack a single deep learning-based task, such as image classification. In practical scenarios, it is more necessary to study adversarial examples transferring across different vision tasks. However, it is challenging to create cross-task adversarial examples that can destroy multiple vision tasks at once due to unavailable various task-specific models and loss functions for attackers. To deal with this problem, we propose a Dual Attention-Guided Method (DAGM) for crafting cross-task adversarial examples by designing a spatial attention module and a channel attention module to capture overlapping discriminative regions and features that contribute to various tasks. Then we craft cross-task adversarial examples via reducing the dispersion ( i.e. , standard deviation) of feature maps re-weighted by both attention modules, which can destroy the overlapping discriminative regions and features for various tasks. Furthermore, to present theoretical explanation, we systematically analyze our method, and rigorously prove that both attention modules can provide better effectiveness of our adversarial examples, compared with existing cross-task adversarial attacks. Extensive experiments on two datasets demonstrate that our method can significantly degrade the performance of various tasks, even online CV APIs, and consistently outperform state-of-the-art methods by a large margin.},
  archive      = {J_TMM},
  author       = {Weiwei Feng and Nanqing Xu and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TMM.2024.3349925},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing cross-task transferability of adversarial examples via spatial and channel attention},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReE3D: Boosting novel view synthesis for monocular images using residual encoders. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2023.3347642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, novel view synthesis from a monocular image has become a research hot-spot that attracts significant attention. Some recent work identifies latent vectors for high-quality view generation via iterative optimisation, which is a time-consuming process. In contrast, some others utilise an encoder learning a mapping function to approximately estimate optimal latent codes, which significantly reduces its processing time but sacrifices reconstruction quality. Consequently, how to balance synthesis quality and its generation efficiency still remains challenging. In this paper, we propose a residual-based encoder to incorporate with a 3D Generative Adversarial Networks (GAN), named ReE3D, for novel view synthesis. It applies an iterative prediction of latent codes to ensure much higher quality of novel view synthesis with an insignificant increase of processing time when compared to existing encoder-based 3D GAN inversion methods. Additionally, we enforce a novel geometric loss constraint on the encoder to predict view-invariant latent codes, thus effectively mitigating the trade-off between geometric and texture quality in 3D GAN inversion. Extensive experimental results demonstrate that our extended encoder-based method has achieved best trade-off performance in terms of novel view synthesis quality and its execution time. Our method has gained comparable synthesis quality with exponentially decreased processing time when compared to iterative optimisation methods, while improved synthesis performance of encoder-based methods significantly.},
  archive      = {J_TMM},
  author       = {KeHua Guo and Tianyu Chen and Sheng Ren and Bin Hu and Zheng Wu and Shaojun Guo and Hui Fang},
  doi          = {10.1109/TMM.2023.3347642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ReE3D: Boosting novel view synthesis for monocular images using residual encoders},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSAM: Parameter-free spatiotemporal attention mechanism for video question answering. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2023.3333192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal attention learning has always been a challenging research task in video question answering (VideoQA). It needs to consider not only the modelling of local neighbourhood dependencies between the adjacent frames in a video but also the modelling of long-term dependencies between nonadjacent frames. Although the existing methods are usually good at modelling temporal dependencies in one aspect, they cannot simultaneously and effectively model the temporal dependencies between adjacent and nonadjacent frames. To address this issue, we first derive a novel statistic-driven difference-aware generation function, which can efficiently calculate the difference between a sequence feature value and the whole mean value to identify the significance of the feature. Subsequently, we design a novel parameter-free spatiotemporal attention mechanism (PSAM), which captures the most relevant cues scattered in the context of a spatiotemporal video by generating functions and utilizes a gating mechanism to adaptively integrate and filter relevant and irrelevant information. Finally, we use the PSAM and hierarchical modelling to construct a lightweight multiscale context fusion- and reasoning-based VideoQA model. Extensive experimental research results obtained on five benchmark datasets for the VideoQA task show that our VideoQA model has high Q&A performance and lightweight characteristics. Simultaneously, comprehensive ablation experimental results show that the PSAM can not only improve the performance of the model but also significantly reduce the number of model parameters. In addition, extensive experimental findings obtained on the benchmark dataset of joint tasks (video moment retrieval and video highlight detection) further demonstrate that the PSAM is a general and effective spatiotemporal attention mechanism.},
  archive      = {J_TMM},
  author       = {Fuwei Zhang and Ruomei Wang and Fan Zhou and Yuanmao Luo and Jinyu Li},
  doi          = {10.1109/TMM.2023.3333192},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PSAM: Parameter-free spatiotemporal attention mechanism for video question answering},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic reasoning for movie QA: A character-centric approach. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2023.3322321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movie story understanding necessitates modeling of, and reasoning about characters and their relationships with the surroundings and others as the story goes. In Movie QA, this poses the challenges of effectively capturing the visual moments relevant to questions in long videos, and efficiently navigating the web of dynamic, contextual character-centric relationships over time. This paper presents a novel character-centric method that efficiently supports reasoning about relational dynamics for Movie QA. Central to the method is a T ime- E volving C onditional C H aracter-centric graph network ( ${\rm{TECH}}$ ) which models the characters, objects, and their question-conditioned relationships in space-time. ${\rm{TECH}}$ first maps the raw video data into a question-focused temporal neural graph over visual entities within and across shots and then distills the graph into a character-centric network which gives rise to the answer. At the core of this graph reasoning machine, TECH uses a two-stage feature refinement process for feature movie characters and their relationships, using their interactions with the surroundings as contextual information. ${\rm{TECH}}$ draws its efficiency over long videos from a “skim and scan” technique to rapidly localize the most query-relevant moments in the movie. Tested on the three large-scale datasets, TECH clearly shows advantages over recent state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Long Hoang Dang and Thao Minh Le and Vuong Le and Tu Minh Phuong and Truyen Tran},
  doi          = {10.1109/TMM.2023.3322321},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic reasoning for movie QA: A character-centric approach},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CHFusion: A cross-modality high-resolution representation framework for infrared and visible image fusion. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2023.3294814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our study, we proposed a novel infrared and visible image fusion framework based on cross-modality transfer and high-resolution representation, termed as CHFusion. On the one hand, the high-resolution representation backbone is devised to receive multi-scale information and maintain high-resolution representation. More specifically, the proposed method involves the pyramid cross-modality feature transfer module to achieve information interaction with different modalities and resolutions. In particular, we introduce the gradient block to obtain texture information of the source image and then supplement it with high-resolution features. We utilize the adaptive channel attention block to compress high-resolution features and then guide image reconstruction. Moreover, a cross-modality high-resolution aggregation block is used to integrate multi-scale information. On the other hand, we propose a difference-aware algorithm, which can generate a pair of weights and then use the weights to construct the difference-aware loss, and then difference-aware loss, a texture loss, and an intensity loss to guide our network to preserve abundant texture information and optimally salient target. Extensive experiments demonstrate the superiority of our method over state-of-the-art alternatives in terms of object maintenance and texture preservation.},
  archive      = {J_TMM},
  author       = {Biaojian Jin and Rencan Nie and Jinde Cao and Ying Zhang and Dongyang Li},
  doi          = {10.1109/TMM.2023.3294814},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CHFusion: A cross-modality high-resolution representation framework for infrared and visible image fusion},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning fine-grained information with capsule-wise attention for salient object detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2023.3234436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of convolutional neural networks being used for salient object detection (SOD), the performance has been significantly improved. However, how to integrate crucial features for modeling salient objects needs further exploration. In this work, we propose an effective feature selection scheme to solve this task. Firstly, we provide a Simplified Atrous Spatial Pyramid Pooling (SASPP) module to lightweight the multi-scale features. Dealing with the SASSP features, we design a pixel-level local feature selection scheme named Multi-Scale Capsule-wise Attention (MSCA). It aggregates features from multi-scales by dynamic routing and helps the network to generate fine-grained prediction maps. In addition, we exploit holistic features by the Spatial-wise Attention and Channel-wise Attention (SA/CA) mechanisms, which adaptively extracts spatial or channel information. We also propose a Multi-crossed Layer Connections (MLC) structure in the upsampling stage, to fuse features from not only different levels but also different scales. The salient object prediction is performed in a coarse-to-fine manner. By conducting comprehensive experiments on five benchmark datasets, our method achieves the best performance when compared to existing state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Sanyuan Zhao and Zongzheng Wen and Qi Qi and Kin-Man Lam and Jianbing Shen},
  doi          = {10.1109/TMM.2023.3234436},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning fine-grained information with capsule-wise attention for salient object detection},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging component learning with degradation modelling for blind image super-resolution. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2022.3216115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network (CNN)-based image super-resolution (SR) has exhibited impressive success on known degraded low-resolution (LR) images. However, this type of approach is hard to hold its performance in practical scenarios when the degradation process ( i.e. blur and downsampling) is unknown. Despite existing blind SR methods proposed to solve this problem using blur kernel estimation, the perceptual quality and reconstruction accuracy are still unsatisfactory. In this paper, we analyze the degradation of a high-resolution (HR) image from image intrinsic components according to a degradation-based formulation model. We propose a components decomposition and co-optimization network (CDCN) for blind SR. Firstly, CDCN decomposes the input LR image into structure and detail components in feature space. Then, the mutual collaboration block (MCB) is presented to exploit the relationship between both two components. In this way, the detail component can provide informative features to enrich the structural context and the structure component can carry structural context for better detail revealing via a mutual complementary manner. After that, we present a degradation-driven learning strategy to jointly supervise the HR image detail and structure restoration process. Finally, a multi-scale fusion module followed by an upsampling layer is designed to fuse the structure and detail features and perform SR reconstruction. Empowered by such degradation-based components decomposition, collaboration, and mutual optimization, we can bridge the correlation between component learning and degradation modelling for blind SR, thereby producing SR results with more accurate textures. Extensive experiments on both synthetic SR datasets and real-world images show that the proposed method achieves the state-of-the-art performance compared to existing methods.},
  archive      = {J_TMM},
  author       = {Yixuan Wu and Feng Li and Huihui Bai and Weisi Lin and Runmin Cong and Yao Zhao},
  doi          = {10.1109/TMM.2022.3216115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bridging component learning with degradation modelling for blind image super-resolution},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intention-interaction graph based hierarchical reasoning networks for human trajectory prediction. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2022.3182151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding crowd motion dynamics and forecasting the future pedestrian trajectories are critical to various applications, e.g. autonomous driving and surveillance system. This task is challenging because when pedestrians plan the future paths in real crowd scenes, they will distinguish the priorities of following their predetermined destinations and responding to the motion behaviors of neighboring pedestrians. However, most of the existing methods ignore the problem of intention-interaction trade-off. In this paper, we tackle this problem by a hierarchical network, which achieves dynamically reasoning predetermined destinations and future trajectories. A novel graph structure called Intention-Interaction Graph (IIG) is designed to jointly model the self intentions and social interactions. To aggregate information in IIG, Interaction Gated Graph Attention Networks (IGGAN) consisting of a gate mechanism and an attention mechanism is proposed, thus achieving reasoning the influence degree of neighboring pedestrians and destinations. Experimental results on multiple widely used pedestrian trajectory prediction datasets, including two datasets in ETH and three datasets in UCY, demonstrate the effectiveness of the proposed model.},
  archive      = {J_TMM},
  author       = {Cunyan Li and Hua Yang and Jun Sun},
  doi          = {10.1109/TMM.2022.3182151},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intention-interaction graph based hierarchical reasoning networks for human trajectory prediction},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
