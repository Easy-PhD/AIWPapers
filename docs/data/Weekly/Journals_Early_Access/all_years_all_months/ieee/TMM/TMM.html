<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm">TMM - 242</h2>
<ul>
<li><details>
<summary>
(2025). Instructive probabilistic transformer for complex action recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex action recognition aims to identify multiple actions over a long time. Multiple actions may occur at the same time (defined as simultaneous actions), and may occur after each other (defined as each action) Complex action recognition may suffer from two challenges. (1) Temporal repeated bias. The same action may repeat in a temporal duration. In this duration, the prediction may be biased to the majority of actions, which occur repeatedly in the past temporal frames. (2) Epistemic uncertainty of multiple actions. When there are multiple simultaneous actions in one frame, this frame's feature may result in the distribution of multiple actions overlapping each other. Without modeling proper relations between actions, the model may hinder accurately explaining certain categories in multiple actions (defined as the model's epistemic uncertainty). In this work, we propose an Instructive Probabilistic Transformer, which contains a probabilistic temporal memorizer, and a probabilistic prototype Transformer. First, to alleviate temporal repeated bias, we design a probabilistic temporal memory module, which learns probabilistic temporal gates to localize each action. The probabilistic gates instruct the selective memory of each action in long-term frames. Second, we cluster features to capture common action semantics among features (defined as action prototypes). To alleviate the epistemic uncertainty of multiple actions, we design a probabilistic prototype Transformer module. This module learns probabilistic relations depending on each prototype, which can ensure the separation between different prototypes. Third, to ensure the proper probabilistic relations depending on each prototype, we extend action loss with distribution loss to learn uncertainty-aware action loss. In uncertainty-aware action loss, the distribution loss measures the consistency between probabilistic relations and prototype relation distribution. The prediction uncertainty is learned by analyzing the entropy of multiple predictions, and helps to ensure the effect between action loss and distribution loss. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Charades, Breakfast Actions, and MultiTHUMOS.},
  archive      = {J_TMM},
  author       = {Zhao Xie and Longsheng Lu and Kewei Wu and Zhehan Kan and Xingming Yang and Dan Guo},
  doi          = {10.1109/TMM.2025.3599089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instructive probabilistic transformer for complex action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AS-GCL: Asymmetric spectral augmentation on graph contrastive learning. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.},
  archive      = {J_TMM},
  author       = {Ruyue Liu and Rong Yin and Yong Liu and Xiaoshuai Hao and Haichao Shi and Can Ma and Weiping Wang},
  doi          = {10.1109/TMM.2025.3604953},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AS-GCL: Asymmetric spectral augmentation on graph contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards semi-supervised dual-modal semantic segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D and 2D data acquisition techniques, it has become easy to obtain point clouds and images of scenes simultaneously, which further facilitates dual-modal semantic segmentation. Most existing methods for simultaneously segmenting point clouds and images rely heavily on the quantity and quality of the labeled training data. However, massive point-wise and pixel-wise labeling procedures are time-consuming and labor-intensive. To address this issue, we propose a parallel dual-stream network to handle the semi-supervised dual-modal semantic segmentation task, called PD-Net, by jointly utilizing a small number of labeled point clouds, a large number of unlabeled point clouds, and unlabeled images. The proposed PD-Net consists of two parallel streams (called original stream and pseudo-label prediction stream). The pseudo-label prediction stream predicts the pseudo labels of unlabeled point clouds and their corresponding images. Then, the unlabeled data is sent to the original stream for self-training. Each stream contains two encoder-decoder branches for 3D and 2D data respectively. In each stream, multiple dual-modal fusion modules are explored for fusing the dual-modal features. In addition, a pseudo-label optimization module is explored to optimize the pseudo labels output by the pseudo-label prediction stream. Experimental results on two public datasets demonstrate that the proposed PD-Net not only outperforms the comparative semi-supervised methods but also achieves competitive performances with some fully-supervised methods in most cases.},
  archive      = {J_TMM},
  author       = {Qiulei Dong and Jianan Li and Shuang Deng},
  doi          = {10.1109/TMM.2025.3604939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards semi-supervised dual-modal semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high resolution (UHR) image segmentation is a challenging task that requires efficient processing of large images while maintaining high accuracy. Existing approaches usually employ both shallow and deep networks to extract high-resolution details and global context from different-resolution inputs, achieving a balance between performance, memory, and speed. However, these methods still rely on preserving relatively high-resolution features within the deep network, leading to increased time and memory costs. This also indicates that the full potential of the high-resolution information from the shallow network remains underexplored. To address this, we propose a novel framework called the Hierarchical Grafting Network (HGN), wherein the shallow network is hierarchically grafted to the deep network from multiple perspectives, enabling comprehensive utilization of the features from the shallow network. Our framework involves carefully designed global structure aggregated grafting and local structure aligned grafting mechanism, which progressively integrate semantic details and spatial structure from the shallow network to the deep network. In addition, to enhance the discriminative power of the high-resolution local features extracted by the shallow network, we introduce a shallow-deep contrastive loss to encourage the shallow network to learn semantically similar features to those of the deep network. Extensive experiments on several UHR image segmentation datasets demonstrate that our approach outperforms state-of-the-art UHR methods. The results demonstrate an overall improvement in terms of memory efficiency, accuracy, and speed.},
  archive      = {J_TMM},
  author       = {Ting Liu and Jing Yang and Shikui Wei and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3604913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crafting more transferable adversarial examples via quality-aware transformation combination. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input diversity is an effective technique for crafting transferable adversarial examples that can deceive unknown AI models. Existing input-diversity-based methods typically use single input transformation, limiting targeted transferability and defense robustness. Combining different transformation types is challenging, as keeping increasing types would degrade semantic information and targeted transferability. This paper proposes a quality-aware transformation combination attack (TCA) that selects high-quality transformation combinations. The quality-aware selection enables expansion of transformation types, enhances input diversity, and hence improves targeted transferability and defense robustness. We first design a quality-evaluation framework to quantify the effectiveness of transformation combinations, which jointly considers convergence, transferability, and robustness. Only a small group (up to 10) of images are required for computation-efficient quality evaluation. Experiments validate TCA's superiority over state-of-the-art baselines in adversarial transferability and robustness. When defenses are secured, the average targeted success rate of TCA with four transformation types (i.e., TCA-t4) outperforms the best baseline by 26%$\sim$42% on ImageNet.},
  archive      = {J_TMM},
  author       = {Junlin Liu and Xinchen Lyu and Chenshan Ren and Qimei Cui},
  doi          = {10.1109/TMM.2025.3604967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Crafting more transferable adversarial examples via quality-aware transformation combination},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code is made publicly available at https://github.com/hangyu94/KE2NT.},
  archive      = {J_TMM},
  author       = {Hangyu Li and Yihan Xu and Jiangchao Yao and Nannan Wang and Xinbo Gao and Bo Han},
  doi          = {10.1109/TMM.2025.3604916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement-based equivariant learning for compositional VQA. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional visual question answering (VQA) represents a challenging yet fundamental task that requires models to comprehend novel combinations of previously learned concepts. The current methods often overlook the disentanglement of underlying concepts and are restricted in terms of their ability to effectively capture the compositional variation mechanism. Moreover, the state-of-the-art techniques depend on additional clues for training, which is not feasible in real-world VQA scenarios. To address these issues, in this paper, we introduce a novel Disentanglement-based EquivAriant Learning (DEAL) framework for compositional VQA, which is guided exclusively by ground-truth answers. In DEAL, we employ causality-inspired interventions to disentangle concepts derived from visual and textual inputs within a re-encoding framework. Based on the principle of equivariance, we subsequently perform a compositional transformation on the inference input and impose the equivariant constraint on the output to augment the compositional reasoning capacity of the model. Comprehensive experiments conducted on the benchmark CLEVR-CoGenT and GQA-SGL datasets validate the superiority of our proposed DEAL approach over the existing state-of-the-art methods for compositional VQA tasks in both visual and linguistic generalization settings.},
  archive      = {J_TMM},
  author       = {Zhou Du and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604897},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentanglement-based equivariant learning for compositional VQA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient SDRTV-to-HDRTV by learning from image formation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary display enables video content rendering with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of existing content remains in standard dynamic range (SDR) format. Therefore, the conversion of SDR content to HDRTV standards holds significant value. This paper delineates and analyzes the SDRTV-to-HDRTV conversion by modeling the formation of SDRTV/HDRTV content. The findings reveal that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this, we propose a new three-step solution called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step utilizes global statistics for image-adaptive color adjustments, followed by a local enhancement network for detail improvement. These two components are integrated as a generator, with GAN-based joint training ensuring highlight consistency. Our method, tailored for ultra-high-definition TV content, offers both effectiveness and computational efficiency in processing 4K resolution images. We also construct HDRTV1K, a dataset comprising HDR videos adhering to the HDR10 standard, featuring 1235 training and 117 testing images at 4K resolution. Furthermore, we employ five metrics to assess SDRTV-to-HDRTV performance. Our results demonstrate state-of-the-art performance both quantitatively and visually. The codes and models are available at https://github.com/xiaom233/HDRTVNet-plus.},
  archive      = {J_TMM},
  author       = {Xiangyu Chen and Zheyuan Li and Zhengwen Zhang and Jimmy S. Ren and Yihao Liu and Jingwen He and Yu Qiao and Jiantao Zhou and Chao Dong},
  doi          = {10.1109/TMM.2025.3604961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient SDRTV-to-HDRTV by learning from image formation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised hyperspectral image (HSI) clustering remains a fundamental yet challenging task due to the absence of labeled data and the inherent complexity of spatial-spectral interactions. While recent advancements have explored innovative approaches, existing methods face critical limitations in clustering accuracy, feature discriminability, computational efficiency, and robustness to noise, hindering their practical deployment. In this paper, a self-supervised efficient low-pass contrastive graph clustering (SLCGC) is introduced for HSIs. Our approach begins with homogeneous region generation, which aggregates pixels into spectrally consistent regions to preserve local spatial-spectral coherence while drastically reducing graph complexity. We then construct a structural graph using an adjacency matrix A and introduce a low-pass graph denoising mechanism to suppress high-frequency noise in the graph topology, ensuring stable feature propagation. A dual-branch graph contrastive learning module is developed, where Gaussian noise perturbations generate augmented views through two multilayer perceptrons (MLPs), and a cross-view contrastive loss enforces structural consistency between views to learn noise-invariant representations. Finally, latent embeddings optimized by this process are clustered via K-means. Extensive experiments and repeated comparative analysis have verified that our SLCGC contains high clustering accuracy, low computational complexity, and strong robustness. The code source will be available at https://github.com/DY-HYX.},
  archive      = {J_TMM},
  author       = {Yao Ding and Zhili Zhang and Aitao Yang and Yaoming Cai and Xiongwu Xiao and Danfeng Hong and Junsong Yuan},
  doi          = {10.1109/TMM.2025.3604954},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional quality assessment for text-to-3D assets: Dataset and model. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively. Specifically, the proposed method utilizes the projection videos of text-to-3D assets to extract 3D shape, texture and text-asset correspondence features, then fuses them to calculate the final three preference scores respectively. Extensive experimental results demonstrate the effectiveness of the proposed T23DAQA method in evaluating the quality of AI generated 3D asset, which is more consistent with human perception. To the best of our knowledge, this is the first work that studies the problem of text-guided 3D generation quality assessment, and our database and codes will be released to facilitate future research.},
  archive      = {J_TMM},
  author       = {Kang Fu and Huiyu Duan and Zicheng Zhang and Xiaohong Liu and Xiongkuo Min and Jia Wang and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3604905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-dimensional quality assessment for text-to-3D assets: Dataset and model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of ‘Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in $2.55\% / 5.7\%$ under different settings, compared with the existing methods.},
  archive      = {J_TMM},
  author       = {Ming-Hui Liu and Xiao-Qian Liu and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TMM.2025.3604932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a weakly supervised referring image segmentation method, named CollabLearn, that segments objects described by free-form referring expression utilizing solely image-text pairs. Existing methods suffer from incorrect localization of referring expressions due to the lack of high-level semantics in cross-modal alignment or rough segmentation of referenced objects stemming from the absence of low-level details. To address these issues, we propose an innovative framework for generating cross-modal features encompassing both high-level semantics and low-level details via two fusion modules: a semantic awareness module and a detail cognition module. Each of these modules generates an activation map, and they mutually correct each other through a collaborative learning strategy. Specifically, the semantic awareness module performs in-depth cross-modal interaction and achieves accurate localization in a top-down manner. The detail cognition module facilitates the segmentation of entire objects in a bottom-up manner. A collaborative learning strategy is designed to enable interaction between these two modules, enforcing sufficient vision-language alignment. Experiments on three benchmarks demonstrate that CollabLearn consistently outperforms state-of-the-art weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Chao Jiang and Yuqiu Kong and Mengnan Zhao and Lihe Zhang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3604944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced head: Exploring strong detection heads with vision transformer. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial component of object detectors, current detection heads often lack the capability to effectively utilize contextual information, adapt to deformable objects, and align features and tasks. However, most existing methods prioritize a single capability, lacking comprehensive approaches to introduce them simultaneously. In this paper, we propose the Enhanced Head to integrate the above three capabilities into the detectors concurrently. Specifically, we propose three attention blocks with linear complexity: Global Concentrated Attention (GCA), Local Deformable Cross-Task Attention (LDCA), and Boundary-Aware Cross-Task Attention (BACA). The GCA captures long-range dependencies efficiently by employing Spatial Information Concentration (SIC). The LDCA improves feature alignment and deformation adaptability by enabling local deformable cross-task feature interactions. The BACA aligns classification features with localization results, enhancing task alignment and further improving deformation adaptability through a region-deformable interaction scheme. We implement Enhanced Head as a plug-and-play detection head and evaluate its effectiveness through extensive experiments on the MS COCO and VisDrone datasets. For instance, on the COCO detection benchmark, our Enhanced Head achieves +3.6 AP gain for FSAF, +3.3 AP for RetinaNet, and +2.9 AP for ATSS while reducing the FLOPs.},
  archive      = {J_TMM},
  author       = {Zewen Du and Zhenjiang Hu and Guiyu Zhao and Ying Jin and Hongbin Ma},
  doi          = {10.1109/TMM.2025.3604917},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced head: Exploring strong detection heads with vision transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual high-order total variation model for underwater image restoration. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are typically characterized by color cast, haze, blurring, and uneven illumination due to the selective absorption and scattering when light propagates through the water, which limits their practical applications. Underwater image enhancement and restoration (UIER) is one crucial mode to improve the visual quality of underwater images. However, most existing UIER methods concentrate on enhancing contrast and dehazing, rarely pay attention to the local illumination differences within the image caused by illumination variations, thus introducing some undesirable artifacts and unnatural color. To address this issue, an effective variational framework is proposed based on an extended underwater image formation model (UIFM). Technically, dual high-order regularizations are successfully integrated into the variational model to acquire smoothed local ambient illuminance and structure-revealed reflectance in a unified manner. In our proposed framework, the weight factors-based color compensation is combined with the color balance to compensate for the attenuated color channels and remove the color cast. In particular, the local ambient illuminance with strong robustness is acquired by performing the local patch brightest pixel estimation and an improved gamma correction. Additionally, we design an iterative optimization algorithm relying on the alternating direction method of multipliers (ADMM) to accelerate the solution of the proposed variational model. Considerable experiments on three real-world underwater image datasets demonstrate that the proposed method outperforms several state-of-the-art methods with regard to visual quality and quantitative assessments. In the quantitative assessments, the proposed method achieves average scores of 0.205 FADE, 7.688 Entropy, 0.628 UCIQE, and 0.775 FDUM across the UIEB and UIQS datasets. Moreover, the proposed method can also be extended to outdoor image dehazing and low-light image enhancement tasks. The code is available at https://github.com/HouGuojia/UDHTV.},
  archive      = {J_TMM},
  author       = {Yuemei Li and Guojia Hou and Peixian Zhuang and Zhenkuan Pan},
  doi          = {10.1109/TMM.2025.3604900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual high-order total variation model for underwater image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FontGuard: A robust font watermarking approach leveraging deep font knowledge. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard.},
  archive      = {J_TMM},
  author       = {Kahim Wong and Jicheng Zhou and Kemou Li and Yain-Whar Si and Xiaowei Wu and Jiantao Zhou},
  doi          = {10.1109/TMM.2025.3604908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FontGuard: A robust font watermarking approach leveraging deep font knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the behavior of contrastive regularization in improving chinese text recognizer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense representation space in Chinese scene text recognition (STR) makes discriminating between categories highly challenging, because of the large candidate category set. Mainstream STR methods have achieved remarkable advancements by leveraging linguistic knowledge to implicitly address this challenge. In this paper, inspired by the correlation between recognizer performance and the distributional properties of character representations, as well as the inherent consistency between this correlation and supervised contrastive learning (SupCon), we thoroughly investigate how to integrate SupCon with an STR model to alleviate this challenge, and elucidate some dynamic behaviors underlying the performance improvements. Specifically, we analyze the SupCon-STR models instantiated with different projectors and evaluate their distributional properties through metrics, including intra-class compactness, inter-class separability, and feature redundancy, while assessing performances that involve in-domain accuracy and cross-domain recognition generalization. The main results reveal how the temperature $\tau$ and projectors affect the representation distribution, and highlight that suitable intra-class compactness and sufficient inter-class separability are key factors for delivering competitive performances in both in-domain and cross-domain STR scenarios. Moreover, these results also provide valuable insights into the design of SupCon-STR architectures for diverse resource constraints. Taking existing Chinese STR models as baselines, and combining SupCon-STR with them, the average improvements in cross-domain recognition performance are over 5% across 7 testing datasets. A new state-of-the-art accuracy of 77.19% on the Chinese Scene benchmark is also established.},
  archive      = {J_TMM},
  author       = {Dekang Liu and Tianlei Wang and Huanqiang Zeng and Jiuwen Cao},
  doi          = {10.1109/TMM.2025.3604892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {On the behavior of contrastive regularization in improving chinese text recognizer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep frequency-separable temporal network for efficient video denoising. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively explore inter-frame information is critical for video denoising. Existing methods often rely on complex architectures, such as optical flow estimation and cross-frame self-attention, which introduce high computational costs and limit their practicality in real-world scenarios. To address this limitation, we propose a simple yet efficient deep Frequency-Separable Temporal Network (FSTN) for video denoising. FSTN utilizes the multi-scale analysis capability of wavelet transform to extract high-frequency and low-frequency information at the feature level, enabling faster processing while maintaining high-quality reconstruction. To further reduce computational complexity and enhance detail preservation, we develop a learnable high-frequency processing module that adaptively filters noise and recovers edge details. Additionally, to effectively utilize information from long-range frames, we propose a low-frequency propagation method equipped with a temporal feature alignment module. This method enables the efficient transfer of structural information from distant frames, ensuring temporal consistency and enhancing denoising performance. Extensive experiments demonstrate that our method has 1.28× fewer network parameters than state-of-the-art efficient video denoising methods, such as BasicVSR++, and requires less computational cost while achieving comparable performance.},
  archive      = {J_TMM},
  author       = {Zhulin Tao and Jinjuan Wang and Lifang Yang and Jinshan Pan and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3604914},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep frequency-separable temporal network for efficient video denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BASNet: Boundary assisted network for image splicing forgery detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing is a common technique used in image forgery. With the rapid development of digital image processing technology, detecting image splicing forgery has become increasingly challenging. Existing splicing forgery localization methods lack exploration in effectively utilizing tampered region boundary information. To address this issue, we propose a novel model for detecting image splicing forgery called boundary-assisted network (BASNet). We introduce a boundary-motivated module (BMM) to explore valuable and additional boundary features related to tampered regions, enhancing representation learning for detecting tampered regions. Additionally, we present a boundary-enhanced module (BEM) to enhance boundary information using the cross-channel attention mechanism. To efficiently merge features from various levels and boundary features, we further present the feature fusion module (FFM). To optimize performance, the BASNet incorporates weighted binary cross-entropy loss, dice loss, and boundary loss, which can effectively leverage edge supervision while mitigating imbalance between positive and negative samples. Evaluation of five widely-used forgery detection datasets demonstrates the state-of-the-art performance of the BASNet. Robustness experiments verify that the BASNet is robust enough to detect image splicing forgery across various common attacks.},
  archive      = {J_TMM},
  author       = {Enji Liang and Kuiyuan Zhang and Zhongyun Hua and Xiaohua Jia},
  doi          = {10.1109/TMM.2025.3604911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BASNet: Boundary assisted network for image splicing forgery detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy protection based on hopfield cross neural network in WBANs for medical images. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable medical data collection and surveillance devices provides real-time guarantees for the whole process of a patient's medical treatment, especially medical image data plays a key role. However, existing medical images face data leakage, pollution and vulnerability to attacks during transmission over wireless body area networks(WBANs). To address these issues, a privacy protection algorithm based on Hopfield cross neural network (HCNN) for medical data is proposed. Specifically, the HCNN model is first constructed and its dynamic behavior is analyzed, which is suitable for application to image encryption. Then, a confusion method of NZ fractal curve sorting matrix (NZ-FCSM) is designed to achieve good encryption effect. Subsequently, the secret image sharing (SIS) technique based on sharing matrix is introduced to enhance the algorithm robustness. Finally, an alignment embedding of double diamond prediction (AEDDP) method is proposed to implement lossless hiding of private information. The present issues in medical image protection include ensuring the security and effectiveness of encryption algorithms while maintaining the robustness and concealment of ciphertext data, and balancing the need for preservation with the limited resources of complex work environment. Experimental results show that the proposed algorithm achieves PSNR of 53 dB for the cipher image, more than 36 dB for the reconstructed image, and the information entropy of the secret image is over 7.99, and displays good robustness. These findings highlight the validity of the algorithm in medical image data privacy preserving applications that ensure confidentiality and extend to practical applications of concealed transmission of confidential information and secure multi-party transactions.},
  archive      = {J_TMM},
  author       = {Xiuli Chai and Guoqiang Long and Yakun Ma and Changbo Li and Zhihua Gan and Yushu Zhang},
  doi          = {10.1109/TMM.2025.3604898},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy protection based on hopfield cross neural network in WBANs for medical images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification via hölder divergence for multi-view representation learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the “distance” between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpasses the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.},
  archive      = {J_TMM},
  author       = {Yan Zhang and Ming Li and Chun Li and Zhaoxia Liu and Ye Zhang and F. Yu},
  doi          = {10.1109/TMM.2025.3604966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty quantification via hölder divergence for multi-view representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral pedestrian detection has been shown to be effective in improving performance in complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To efficiently compress multispectral object detection networks, we propose a novel distillation method, the Adaptive Modal Fusion Distillation (AMFD) framework. Unlike traditional distillation methods, the AMFD framework fully leverages the original modal features from the teacher network, thereby significantly enhancing the performance of the student network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP, SUNRGB-D and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.},
  archive      = {J_TMM},
  author       = {Zizhao Chen and Yeqiang Qian and Xiaoxiao Yang and Chunxiang Wang and Ming Yang},
  doi          = {10.1109/TMM.2025.3604937},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic frame aggregation-based transformer for live video comment generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments that align with viewer interests. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with cross-attention mechanism to attend to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large-scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.},
  archive      = {J_TMM},
  author       = {Anam Fatima and Yi Yu and Janak Kapuriya and Julien Lalanne and Jainendra Shukla},
  doi          = {10.1109/TMM.2025.3604921},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic frame aggregation-based transformer for live video comment generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of repetitive actions in videos, a task commonly referred to as Video Action Counting (VAC), is a critical challenge in understanding and analyzing content in sports, fitness, and daily activities. Traditional approaches to VAC have largely overlooked the nuanced irregularities inherent in action repetitions, such as interruptions and variable lengths between cycles. Addressing this gap, our study introduces a novel perspective on VAC, focusing on Irregular Video Action Counting (IVAC), which emphasizes the importance of modeling the irregular repetition priors present in video content. We conceptualize these priors through two key aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures that the spatiotemporal representations across all cycle segments in a video remain homogeneous, thereby reflecting the uniformity of actions between different cycle segments. In contrast, Cycle-interval Inconsistency mandates a clear semantic distinction between the representations of cycle segments and intervals, acknowledging the inherent dissimilarities in content. To effectively encapsulate these priors, we introduce a novel methodology consisting of consistency and inconsistency modules, underpinned by a tailored pull-push loss ($\mathrm {P^{2}~L}$) mechanism. This approach employs a pull loss to enhance the cohesion among cycle segment features and a push loss to distinctly differentiate between cycle and interval segment features. Empirical evaluations on the RepCount dataset illustrate that our IVAC-$\mathrm {P^{2}~L}$ model sets a new benchmark in state-of-the-art performance for the VAC task. Moreover, our model demonstrates adaptability and generalization across diverse video content, achieving superior performance on two additional datasets, UCFRep and Countix, without necessitating dataset-specific fine-tuning. These findings not only validate the effectiveness of our approach in addressing the complexities of irregular repetitions in videos but also open new avenues for future research in video understanding and analysis.},
  archive      = {J_TMM},
  author       = {Hang Wang and Zhi-Qi Cheng and Youtian Du and Lei Zhang},
  doi          = {10.1109/TMM.2025.3604935},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception-distortion- tradeoff reveals the limitation of current low-level deep learning paradigms, i.e., minimizing reconstruction distortion does not guarantee improved perceptual quality. Acknowledging the lack of a reliable perception-oriented optimization function, we are motivated to explore a flexible approach for enhancing perceptual quality by steering the tradeoff to prioritize perception. To this end, we reconsider the perception-distortion function by incorporating the Just-Noticeable-Distortion (JND) mechanism. We mathematically demonstrate that in the common image restoration process, altering the optimization target from natural images to distorted images—where the distortion intensity is constrained by the JND threshold and the distortion type aligns with that arising from the restorer itself—effectively obtained improved perception indices without any changes to the restorer or optimization function. Accordingly, to facilitate various low-level learning models, we are motivated to construct the first large-scale CNN-oriented JND image dataset. Our dataset comprises 500 natural images and 4,500 degraded versions generated by a series of autoencoders, as well as the actual JND judgment results collected through rigorous subjective testing from twenty volunteers. Finally, a learning-based JND inference model is established on the proposed dataset and employed in the proposed JND-based adaptation scheme, where the inferred JND images serve as pseudo-ground truth for the training or fine-tuning processes of low-level vision models. Extensive experiments on image super-resolution and end-to-end image compression across multiple models have shown encouraging improvements in perceptual quality, demonstrating the effectiveness of the proposed scheme. Our dataset is available at: https://github.com/ohq17/CNN-Oriented-JND-Dataset.},
  archive      = {J_TMM},
  author       = {Xuelin Shen and Haoqiao Ou and Zhangkai Ni and Wenhan Yang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TMM.2025.3604973},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RandomViG: Random vision graph neural network for image classification. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Graph Neural Network (ViG) is the first graph neural network model capable of directly processing image data. The community primarily focuses on the model structures to improve ViG's performance but lacks attention to its graph construction method. To avoid quadratic computational complexity, ViG uses clustering algorithms (K-nearest neighbor) to construct graph structures. Nevertheless, clustering algorithms introduce biases, which limit ViG's ability to obtain global information. To address this problem, we propose RandomViG, which abandons clustering algorithms and uses a random manner to obtain relationships between nodes. Our RandomViG is sparse in computation and can approximate a complete graph, enabling ViG to gain global interaction capability. In order to obtain the local dependence, we design a local feature extraction module for RandomViG. In addition, to alleviate the over-smoothing problem, we propose a novel method called MRN (maintaining relationships among nodes). Considering that the increased feature diversity does not necessarily lead to better performance, MRN does not aim to maximize the feature diversity of the model but instead strives to maintain consistency between the feature similarity and the inherent similarity of the original image. We validate our proposal in three major computer visual tasks, including image classification, object detection, and instance segmentation. Without extra data, RandomViG-Ti achieves 79.4% ImageNet-1 K top-1 accuracy, outperforming the baseline (ViG) by 1.2%. Under the same model scale, our RandomViG performs better with fewer FLOPs compared with existing state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Xun Gong and Daisong Yan and Zhemin Zhang},
  doi          = {10.1109/TMM.2025.3604948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RandomViG: Random vision graph neural network for image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLE: Out-of-distribution detection with shallow layer-driven enhancement. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3604940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution detection aims to protect models against overconfidently categorizing samples from unknown categories, i.e., out-of-distribution data (OOD), into known categories, i.e., in-distribution data (ID). From the perspective of feature distribution, the difference between OOD samples and ID samples can be decomposed into semantic shifts and covariate shifts. Most DL-based methods only extract deeper features, which represent semantic shifts, to discern feature variances in the data, ignoring the exploration of covariance shifts. In this paper, we propose a Shallow Layer-driven Enhanced OOD detection method (SLE), which enhances the difference of OOD samples by exploiting covariate shifts in shallow features. Specifically, it contains three main components: Hierarchical Feature Extractor (HFE), Adaptive Dimensionality Reduction Strategy (ADR), Cross-layer Score Aggregator (CSA). HFE is responsible for extracting both deeper and shallow features from the deep network. ADR adaptively reduces all hierarchical feature dimensionality according to sample characteristics, avoiding feature redundancy. CSA defines a novel confidence score for OOD samples, that effectively prevents confusion in the feature representation space at each layer. In SLE, these three closely related components cooperate with each other to effectively enhance the representation ability of OOD samples and divide OOD data better. We conduct extensive experiments to examine the performance of SLE in four benchmarks and discuss its individual components. This method performs well on the OOD datasets.},
  archive      = {J_TMM},
  author       = {Zhenni Yang and Chengxu Liu and Xueming Qian},
  doi          = {10.1109/TMM.2025.3604940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLE: Out-of-distribution detection with shallow layer-driven enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active cross-modal domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most cross-modal methods assume that training and testing data come from the same domain, which is often not the case in real-world scenarios due to cross-modal domain shifts and potential unknown concepts. Moreover, cross-modal shifts hinder the capture of unknown concepts, and the presence of unknown concepts can in turn exacerbate the cross-modal shifts. To address these challenges, this paper proposes a new paradigm called Active Cross-Modal Domain Adaptation (ACM-DA), wherein only cross-modal data from the source domain and uni-modal data from the target domain are utilized. To concurrently mitigate the adverse effects of both cross-modal domain shifts and unknown concepts, we propose a Curiosity-Driven Active Adaptation Network (CD-A2N), selectively annotating samples to maximize performance gain. First, we present Curiosity Arousal within Cross-modal Domain Adaptation (CA-CDA) to explore the complexity and novelty characteristics of target samples, while reducing cross-modal discrepancy and aligning source and target domains. Second, Curiosity-driven Active Learning (CAL) is devised to strategically select a subset of target samples for annotation, aiming to achieve more valuable data selection at a small labeling cost. Finally, we jointly train CA-CDA and CAL with the newly labeled target domain sub-dataset to alleviate the above issues. Extensive experiments demonstrate that CD-A2N provides an effective solution for achieving ACM-DA. Code will be available at https://github.com/Feliciaxyao/ACM-DA.},
  archive      = {J_TMM},
  author       = {Xuan Yao and Xiao Peng and Junyu Gao and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604968},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Active cross-modal domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards multimodal emotional support conversation systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support. This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities. This dataset captures the intricate interplay of user emotions, system strategies, system emotions, and system responses, setting a new precedent in the field. Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. This work not only pushes the boundaries of AI's role in mental health care but also establishes a foundation for developing conversational agents that can provide more empathetic and effective emotional support.},
  archive      = {J_TMM},
  author       = {Yuqi Chu and Lizi Liao and Zhiyuan Zhou and Chong-Wah Ngo and Richang Hong},
  doi          = {10.1109/TMM.2025.3604951},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards multimodal emotional support conversation systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HARG: Hierarchical adaptive reasoning graph for activity parsing. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a video understanding task, activity parsing aims at encompassing actions into multiple levels of activity components, including activity, sub-activity and atomic action, enabling understanding of complex video scenes within multimedia systems. Existing methods form activity parsing as a multi-task learning problem to predict multi-granular activity labels simultaneously, which ignores modeling the hierarchical structure and the fine-grained transitions of activity components at different levels. In this paper, we propose a Hierarchical Adaptive Reasoning Graph (HARG) to model the hierarchical structure (i.e., object level $\rightarrow$ atomic action level $\rightarrow$ activity level) dynamically and precisely. To achieve that, an object reasoning graph (ORG) and an atomic action reasoning graph (ARG) are designed to reason fine-grained information transitions between multiple actors at different levels. In addition, an adaptive segmentation module (ASM) is investigated for bridging the gap among different levels, permitting step-by-step reasoning from the object level to the atomic action level. Experimental results show our method outperforms state-of-the-art methods on two activity parsing datasets, achieving hierarchical modeling and fine-grained reasoning for activity understanding. The code is available on GitHub: https://github.com/whuoyj/HARG.},
  archive      = {J_TMM},
  author       = {Yangjun Ou and Li Mi and Zhenzhong Chen},
  doi          = {10.1109/TMM.2025.3604927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HARG: Hierarchical adaptive reasoning graph for activity parsing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware wavelet transformer for pyramid learning object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer displays the impressive capabilities on vision tasks. The built-in self-attention retains the quadratic computation burden in respect of the spatial resolution of image features. The traditional downsampling (e.g., average pooling) can reduce the resolution. Nonetheless, it may suffer from the dropping of detailed information. In this work, we propose an Efficient Wavelet Attention (EWA), which injects the wavelet transform and a Mean GELU (MGELU) function. Firstly, the wavelet transform enables the detailed information to participate in the efficient interaction modeling. Secondly, MGELU regards the statistical mean as reference and loosely passes the high relative responses. Building upon EWA, we present an effective Semantic-aware Wavelet Transformer (SWFormer), which is then employed for pyramid learning, including CNN feature hierarchy or Region of Interest (RoI) features. For the feature hierarchy, a Pyramid SWFormer (PSWFormer) incorporates SWFormer at each level to fit the bidirectional features. For RoIs, a Recognition-Localization SWFormer (RLSWFormer) is inserted into the head to fit their features from all levels. The effectiveness of our SWFormer is displayed experimentally on the MS COCO detection dataset and the Pascal VOC dataset. When exploiting Swin-small backbone, our SWFormer-based method acquires AP of 52.1 in the single-scale evaluation on the COCO test-dev set. This work will have the codes at https://github.com/TimeIsFuture/Dt2_SWFormer.},
  archive      = {J_TMM},
  author       = {Yang Li and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Puhua Chen},
  doi          = {10.1109/TMM.2025.3604963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-aware wavelet transformer for pyramid learning object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-contact respiratory monitoring, reducing motion artifact and selecting the appropriate Region of Interest (ROI) pose significant challenges. Most motion artifact removal methods rely on signal periodicity assumptions, while respiratory signals usually are non-periodic in real-world scenarios. Existing automated ROI selection approaches are mostly primarily impacted by the texture of clothing, absence of chest landmarks, and obstruction of face. To improve the quality of respiratory signals, in this study, we propose a framework for automatic respiratory ROI selection based on video, namely, Optimizing Video-based Respiration Monitoring (OVRM), which consists of peak-trough adaptive motion artifact removal and characteristic-driven adaptive ROI selection. This motion artifact removal strategy removes motion artifacts by using a dynamic ratio-based judgment mechanism, and reconstructs signals using sinusoidal interpolation. The adaptive ROI method scores signals based on periodicity, similarity, smoothness, and energy, selecting the highest-scoring blocks as the ROIs to match respiratory signals efficiently. Experimental results, validated across four datasets, demonstrate that OVRM effectively reduces signal noise caused by subject movement and outperforms state-of-the-art non-contact respiratory monitoring algorithms. The dataset and code are publicly available at: https://github.com/zxx5058/OVRM.},
  archive      = {J_TMM},
  author       = {Xinxin Zhang and Xudong Tan and Yan Zhu and Mei Zhou and Menghan Hu and Zhanzhan Cheng and Nengfeng Qian and Changyin Wu and Guangtao Zhai and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3604970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action-responsive contrastive network for fine-grained skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, fine-grained skeleton action recognition based on graph convolutional networks (GCNs) has become an important research focus. Fine-grained action recognition refers to the accurate recognition of subtle, complex or detailed actions. This task is particularly challenging due to the limited appearance information in skeleton data and the limitations of predefined single-topology skeleton structures. To address these challenges, we propose an action-responsive contrastive network (ARCN). The network consists of two main components: an action-responsive graph convolutional network (ARGCN) with enhanced skeleton topology and a fine-grained action comparator (FAC) that uses feature contrastive learning to explore the latent space of motion features. The ARGCN contains two specialized modules: the action-responsive topology (ART) module, which captures important motion features through the learned action-specific topology structure matrix and multiscale temporal features; and the action-responsive attention (ARA) module, which learns complex spatiotemporal skeleton attention information. These modules jointly generate a multichannel cross-temporal dynamic skeleton joint attention topology map tailored for the specific action being analysed. To further clarify the fine-grained action feature differences, the FAC is integrated in some stages of the ARGCN. The FAC performs spatiotemporal decoupling of feature maps, classifies and contrasts similar and different fine-grained motion features, and builds a learnable latent space for fine-grained motion, thereby improving classification performance. Our model is evaluated on six public datasets: NTU RGB+D, NTU RGB+D 120, NW-UCLA, UAV-Human, Finegym, and Diving48. It achieves 91.2% accuracy on the NTU RGB+D 120 dataset X-Set, 97.2% accuracy on the NW-UCLA dataset, 44.6% accuracy on the UAV-Human dataset CSv1, 72.0% accuracy on the UAV-Human dataset CSv2, 95.3% accuracy on the Finegym dataset, and 54.3% accuracy on the Diving48 dataset, which are competitive results compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hongjun Li and Tian Bai},
  doi          = {10.1109/TMM.2025.3604906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Action-responsive contrastive network for fine-grained skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed AutoRegressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed AutoRegressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can achieve over a magnitude acceleration in decoding time without industrial level optimization or achieve state-of-the-art reconstruction quality compared with other INR codecs. To the best of our knowledge, our method is the first INR-based codec comparable with Ballé et al. [1] in both decoding speed and quality while maintaining low complexity.},
  archive      = {J_TMM},
  author       = {Xiang Liu and Jiahong Chen and Bin Chen and Zimo Liu and Baoyi An and Shu-Tao Xia and Zhi Wang},
  doi          = {10.1109/TMM.2025.3604982},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To counter the security threats posed by the realism of image inpainting generated through diffusion models and GANs, in this paper, we propose a texture enhancement and progressive refinement network (TEPR-Net) for image inpainting localization (IIL). The IIL task is divided into two phases: coarse and fine locating. In the coarse locating phase, we utilize an anomaly texture encoder to capture tampering traces in textures, employ a texture–context feature interaction strategy to effectively integrate texture features with contextual features, and utilize a pixel-level contrastive learning strategy to enhance feature clustering and model generalization. In the fine locating phase, we first enhance the receptive field features in the frequency domain by transforming the features and separately enhancing the low- and high-frequency components. Then, we utilize the coarse localization result to augment the model's sensitivity to tampered regions. Additionally, we introduce a progressive edge distribution guidance and reconstruction strategy that progressively refines the edges of the tampered regions at each level, ultimately generating refined localization results. To support the research and evaluation of the IIL task, we create the Inpaint32K dataset, which is characterized by its large scale, diversity, comprehensiveness, high quality, and authenticity. Finally, extensive experiments demonstrate that TEPR-Net has significant advantages in terms of localization performance, generalizability, extensibility, and robustness.},
  archive      = {J_TMM},
  author       = {Qixian Hao and Kai Wang and Haoliang Cui and Jiwei Zhang and Shaozhang Niu},
  doi          = {10.1109/TMM.2025.3604965},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and robust 3D panoptic segmentation is crucial for scene perception in autonomous driving. Modern methods widely adopt multi-modal fusion based simple feature concatenation to enhance 3D scene understanding, resulting in generated multi-modal representations typically lack comprehensive semantic and geometry information. These methods focused on panoptic prediction in a single step also limit the capability to progressively refine panoptic predictions under varying noise levels, which is essential for enhancing model robustness. To address these limitations, we first utilize BEV space to unify semantic-geometry perceptual representation, allowing for a more effective integration of LiDAR and camera data. Then, we propose PrimePSegter, a progressively combined diffusion 3D panoptic segmentation model that is conditioned on BEV maps to iteratively refine predictions by denoising samples generated from Gaussian distribution. PrimePSegter adopts a conditional encoder-decoder architecture for fine-grained panoptic predictions. Specifically, a multi-modal conditional encoder is equipped with BEV fusion network to integrate semantic and geometric information from LiDAR and camera streams into unified BEV space. Additionally, a diffusion transformer decoder operates on multi-modal BEV features with varying noise levels to guide the training of diffusion model, refining the BEV panoptic representations enriched with semantics and geometry in a progressive way. PrimePSegter achieves state-of-the-art performance on the nuScenes and competitive results on the SemanticKITTI, respectively. Moreover, PrimePSegter demonstrates superior robustness towards various scenarios, outperforming leading methods.},
  archive      = {J_TMM},
  author       = {Hongqi Yu and Sixian Chan and Xiaolong Zhou and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3604903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided adversarial attack in the low-frequency space. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3604964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples can assess the robustness of machine learning models, which has attracted the attention of many researchers to adversarial example generation methods. Transferability and imperceptibility stand out as two crucial metrics for evaluating the quality of adversarial examples. However, achieving a balance between these two indicators poses a formidable challenge. In this paper, we propose a low-frequency guided adversarial attack method (LGA) to generate adversarial examples with strong transferability and good imperceptibility. Specifically, we enhance the transferability of adversarial examples by increasing the diversity of attack algorithms, and introduce the guiding principle and the triplet loss constraint to ensure that the generated adversarial examples are optimized away from the class regions of the clean examples. We find that the low-frequency component in the frequency domain of the image contains the vast majority of the semantic information of the image. Therefore, we constrain the attack perturbations to low-frequency component space to enhance the covert nature while maintaining visual coherence, rendering the adversarial examples more difficult to perceive. We conduct extensive experiments on various models with different network structures and multiple defense strategies, and the experimental results demonstrate that our method outperforms existing methods in the tradeoff between transferability and imperceptibility, achieving the SOTA performance.},
  archive      = {J_TMM},
  author       = {Jiang Zhu and Lingping Tan and Yanchun Li and Shujuan Tian and Jianqi Li and Yaonan Wang},
  doi          = {10.1109/TMM.2025.3604964},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guided adversarial attack in the low-frequency space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view clustering via mutual information. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering focus on mining useful information from low-quality multiple sources, such as missing and distorted data that are prevalent in real life. However, after representation learning and the processing of incomplete information, existing methods often leave representations containing information task-irrelevant information. In addition, the separation between missing data imputation and clustering tasks leads to sub-optimal multi-view clustering performance. To address these issues, we propose an incomplete multi-view clustering method based on mutual information. For the problem of task-irrelevant information, we use incomplete view prediction to extract sufficient and minimal task-relevant information and provide theoretical proof from the perspective of mutual information. For the problem of separation between missing data imputation and clustering tasks, we integrate incomplete-view prediction with contrastive clustering, collaboratively enhancing the clustering performance. Comparative experiments on five public datasets, under both complete and incomplete scenarios, reveal that our method outperforms nine other competing approaches, demonstrating its effectiveness and robustness in handling multi-view data.},
  archive      = {J_TMM},
  author       = {Xuejiao Yu and Guoqing Chao and Yi Jiang and Guanzhou Ke and Dianhui Chu},
  doi          = {10.1109/TMM.2025.3604942},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incomplete multi-view clustering via mutual information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Like humans to few-shot learning through knowledge permeation of visual and language. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario. With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes. However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge. In this paper, we set out to fill this gap via a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in human intuition: a class name description offers a more general representation, whereas an image captures the specificity of individuals. BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation. On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information. Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop, particularly outperforming previous methods by a substantial margin in the 1-shot setting (improving the accuracy by 7.58% on miniImageNet).},
  archive      = {J_TMM},
  author       = {Yuyu Jia and Qing Zhou and Junyu Gao and Qiang Li and Qi Wang},
  doi          = {10.1109/TMM.2025.3604977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Like humans to few-shot learning through knowledge permeation of visual and language},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCPoint: Contrasting corrupted point clouds for self-supervised representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised Learning (SSL), including mainstream contrastive learning, has achieved significant success in learning visual representations without the need for data annotations in 3D vision. While most contrastive learning methods focus on instance-level information through random affine transformations, they pay limited attention to the intrinsic structures within point clouds. In this work, we propose a novel SSL paradigm for point cloud representation learning, called CCPoint, which incorporates a novel form of data corruption as a negative augmentation strategy. Specifically, we degrade the input point cloud with various corruptions and conduct contrastive learning among the augmented, raw, and corrupted points to learn robust and discriminative representations. To preserve the semantic structure of the point cloud even under heavy degradation, an auxiliary reconstruction decoder is introduced into the corruption branch to provide an additional supervision signal. We explore four families of corruptions—affine, noise, masking, and combined transformations. Different from previous methods that rely on multi-modal data or complex network architectures, CCPoint achieves state-of-the-art performance on three widely used datasets (ModelNet40, ScanObjectNN, and ShapeNetPart) with a lightweight and efficient structure, reaching top linear accuracies of 92.4% and 86.2% on ModelNet40 and ScanObjectNN, respectively.},
  archive      = {J_TMM},
  author       = {Xiaoyang Xiao and Shaoyi Du and Zhiqiang Tian and Meiqin Liu and Xinhu Zheng},
  doi          = {10.1109/TMM.2025.3604890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCPoint: Contrasting corrupted point clouds for self-supervised representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive gradient-guided self-distillation keypoint detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of autonomous driving and 3D reconstruction, keypoint detection, as a key link in visual localization, has become a hot topic in current research. However, existing keypoint detection methods rarely pay attention to the difficulty differences of samples and lack a progressive learning mechanism, which often leads to overfitting for simple samples and underfitting for complex samples, limiting the overall performance of the model. To address these issues, we propose a novel progressive gradient-guided self-distillation method (PG2 SD) for keypoint detection, which possesses self-evolutionary learning capabilities. Specifically, we propose a progressive gradient constraint strategy (PGCS) that dynamically adjusts the gradient contributions of different samples, enabling the model to adapt to the evolving learning capability during training. On this basis, we propose a gradient-guided self-distillation strategy (G2 SDS), which integrates seamlessly with PGCS to alleviate the insufficient feature representation of hard samples in the early training stage. We further design a novel loss function to achieve dynamic collaboration between PGCS and G2 SDS, allowing G2 SDS to adaptively adjust the self-distillation parameters through the PGCS. Experimental results on multiple benchmark datasets show that our method achieves state-of-theart performance on image matching, visual localization, and 3D reconstruction tasks without designing a proprietary network, indicating broad application prospects.},
  archive      = {J_TMM},
  author       = {Zhaoyang Li and Jie Cao and Qun Hao and Haifeng Yao and Yingbo Wang},
  doi          = {10.1109/TMM.2025.3607731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive gradient-guided self-distillation keypoint detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-Based Signature (ABS) provides a critical solution for ensuring data integrity, fine-grained access control, and anonymous authentication in security-sensitive systems such as the Multimedia Internet of Things (MIoT) and multimedia streaming platforms. However, practical adoption of ABS faces three fundamental challenges: vulnerability to key exposure and escrow risks, linear growth of computational cost, and insufficient robustness in multi-authority environments. To address these issues, we propose a forward secure and threshold authorized multi-authority ABS scheme called FORT in this paper. By employing a binary tree structure to divide multiple time periods, historical signatures remain valid even in the event of key exposure. Furthermore, to balance robustness and resistance to corruption while mitigating the key escrow problem, we construct a threshold authorized multi-authority structure based on Lagrange interpolation. This structure effectively reduces the impact of a single authority on the MIoT. Additionally, through the adoption of outsourced computation technology, which offloads complex computations in the signature and verification phases to the edge server, the computational burden for both the signer and verifier is significantly reduced to a small constant. Rigorous security analysis demonstrates that the FORT scheme achieves forward security, collusion attack resistance, corrupt authority resistance and anonymity. Theoretical comparisons and simulation experiments demonstrate the lightweight nature of the FORT scheme in terms of computation and communication.},
  archive      = {J_TMM},
  author       = {Chong Guo and Bei Gong and Zhe Li and Mowei Gong and Haotian Zhu},
  doi          = {10.1109/TMM.2025.3607696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foodfusion: A novel approach for food image composition via diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image composition requires the use of existing dish images and background images to synthesize a natural new image, while diffusion models have made significant advancements in image generation, enabling the construction of end-to-end architectures that yield promising results. However, existing diffusion models face challenges in processing and fusing information from multiple images and lack access to high-quality publicly available datasets, which prevents the application of diffusion models in food image composition. In this paper, we introduce a large-scale, high-quality food image composite dataset, FC22k, which comprises 22,000 foreground, background, and ground truth ternary image pairs. Additionally, we propose a novel food image composition method, Foodfusion, which leverages the capabilities of the pre-trained diffusion models and incorporates a Fusion Module for processing and integrating foreground and background information. This fused information aligns the foreground features with the background structure by merging the global structural information at the cross-attention layer of the denoising UNet. To further enhance the content and structure of the background, we also integrate a Content-Structure Control Module. Extensive experiments demonstrate the effectiveness and scalability of our proposed method.},
  archive      = {J_TMM},
  author       = {Chaohua Shi and Xuan Wang and Si Shi and Xule Wang and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3607683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Foodfusion: A novel approach for food image composition via diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have demonstrated remarkable abilities in image-language reasoning. However, they deal with Video Question Answering (VideoQA) insufficiently, especially for questions demanding causal-temporal reasoning. Typically, they directly concatenate features of uniformly sampled frames as visual inputs for VideoQA. This gives rise to two challenges. For one thing, uniformly sampled frames are discrete and separately distributed across different timestamps, disrupting the coherence of question-critical events or actions. For another, it considers every scene within videos equally and introduces redundant frames that may distract the model from discovering the truth. Towards this, we highlight the importance of identifying continuous frames that are crucial for answering the questions, and propose a lightweight and differentiable Coherence Recognizer (CoRe) to achieve this. Guided by the semantics of questions, CoRe computes scores recording the relevance between each frame and the question, and selects a set of continuous frames with the highest scores for answer prediction. Additionally, CoRe encodes the unselected frames into a short and coarse-grained representation as a completion of the general context. Equipped with CoRe, we can efficiently fine-tune the current MLLMs for VideoQA in an end-to-end manner, without suffering from the problems of incoherence or distraction. Extensive experiments demonstrate that our method achieves substantial improvements on several VideoQA benchmarks.},
  archive      = {J_TMM},
  author       = {Haibo Wang and Chenghang Lai and Weifeng Ge},
  doi          = {10.1109/TMM.2025.3607780},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage causal intervention framework for long-tailed SAR target recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of SAR targets generally conforms to a long-tailed distribution. Due to the existence of sample distribution bias and sample selection bias, training classifiers on this distribution of data often introduces spurious correlations between samples and classes. To address this issue, we propose a two-stage causal intervention framework. The core is that structural causality allows for independent interventions on multiple biases, thereby ensuring high-quality tail class predictions while maintaining unbiased performance for head classes. Firstly, we construct a structural causal graph for the long-tailed recognition task from causal perspective. Based on this graph, the causal paths underlying the two types of biases are identified. Secondly, we design a data augmentation method named DiagPatch-M, which identifies causal features within samples. In this process, these generated patches randomly integrate causal and non-causal features from two different samples, disrupting the original recognition process and effectively eliminating biases induced by sample selection. Thirdly, we design an unbiased structural risk minimization (USRM) optimization strategy, which eliminates the “head preference” of conventional models and the “tail preference” of modified models. This strategy reduces the bias introduced by the model's dependence on the original sample distribution, and achieves stable recognition under different sample distributions. Experimental results on two long-tailed and two balanced datasets demonstrate that the effectiveness of our model surpasses the state-of-the-art (SOTA) methods, indicating the efficacy of our proposed framework in tackling the challenges posed by the long-tailed distribution in SAR target recognition.},
  archive      = {J_TMM},
  author       = {Jiaxiang Liu and Zhunga Liu and Longfei Wang and Zuowei Zhang},
  doi          = {10.1109/TMM.2025.3607804},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A two-stage causal intervention framework for long-tailed SAR target recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTSNet: Dynamic transformer slimming for efficient vision recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have recently adopted increasingly complex structure (e.g., deeper or wider stacked network) to promote the representation learning capabilities of vision recognition. However, progressively deeper or wider stacked network cause the expensive computation cost, which hinders their effective deployment in resource-constrained edge clouds or end devices. In this paper, we propose DTSNet, a dynamic transformer slimming model, which scales vision transformers (ViTs) down across layers from both of the model depth and input width. This is the first time to explore the joint reduction of input tokens and model parameters for ViTs under maintaining performance. Specifically, DTSNet adopts a diversity-enhanced weight sharing module to reduce network parameters, where the weight knowledge of multiple adjacent blocks is effectively integrated into one block. Furthermore, DTSNet designs a unified and massively scalable token pruning mechanism that dynamically discarding less important tokens with a model-driven manner, by introducing a series of discriminant parameters, which is a simple change to the common architecture of vision transformers. Extensive experiments are conducted to verify that DTSNet is able to yield high efficacy in compressing parameter space and accelerating model inference. DTSNet-T/-S/-B on ImageNet achieves 3.0M/11.1M/42.9M parameters and 0.8/2.9/13.7 GFLOPs, where number of parameters are reduced by 48%$\sim$51% and inference speed are improved by 1.3$\times \sim 1.5\times$. Experiments results on semantic segmentation and object detection dataset further demonstrate the potential of DTSNet on complex dense prediction tasks. Code will be available upon publication.},
  archive      = {J_TMM},
  author       = {Wenjing Xiao and Xianzhi Li and Long Hu and Yixue Hao and Min Chen},
  doi          = {10.1109/TMM.2025.3607796},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTSNet: Dynamic transformer slimming for efficient vision recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.},
  archive      = {J_TMM},
  author       = {Jianwu Fang and Lei-Lei Li and Zhedong Zheng and Hongkai Yu and Jianru Xue and Zhengguo Li and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3607808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comp-diff: A unified pruning and distillation framework for compressing diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, generative models such as diffusion models (DMs) have gained prominence in various applications, and there is a growing demand for their deployment on resourceconstrained devices. Model pruning provides an effective solution by reducing the model redundancy without significantly impacting performance. However, most existing model pruning methods are designed for classification models and often lead to substantial performance degradation when applied to generative models. To address this issue, we propose Comp-Diff, a novel two stage framework of pruning and knowledge distillation tailored for diffusion models. In the pruning stage, we propose a new structured content-aware pruning (CaP) method within CompDiff to identify and preserve informative units (filters/channels) that actually contribute to the generative capability of the model. Specifically, we introduce input perturbations to the pre-trained model and measure each unit's importance score using gradients induced by these perturbations. Units with higher importance scores are considered more informative and are retained to maintain the model's generative power. In the fine-tuning stage of Comp-Diff, we propose the distribution-aware knowledge distillation (DaKD) method, which effectively transfers finegrained knowledge from the original model to the pruned one on both attention and noise distribution levels. In addition, DaKD includes an adversarial loss to improve the quality and diversity of generated outputs. To verify and evaluate our method, we apply the proposed Comp-Diff on three representative tasks: unconditional image generation, conditional image generation, and text-to-image generation. Extensive experiments on both multi-step and one-step diffusion models demonstrate that the proposed framework consistently yields compact models and outperforms existing pruning techniques by a large margin.},
  archive      = {J_TMM},
  author       = {Lu Yu and Wei Xiang and Kang Han and Gaowen Liu and Ramana Kompella},
  doi          = {10.1109/TMM.2025.3607799},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comp-diff: A unified pruning and distillation framework for compressing diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online video super-resolution methods utilize implicit memories of previous frames to provide reference information, which have a single memory stream path and are highly dependent on the continuous memory stream. However, video capture in real-world scenes is typically affected by abnormal exposures resulting in sudden changes of lightness thus interrupting the memory stream, while long-term memories suffer from memory vanishing problems during transmission. To address this problem, we propose a novel multi-memory streams based online video super-resolution paradigm that adaptively corrects for abnormal exposures and creates multi-memory streams to accurately converge long-term memories. Specifically, we first propose an exposure detection-correction module, which utilizes optical flow overfitting property and temporal lightness information to detect and correct abnormal exposures to avoid interruption of memory streams. In addition, we propose a dynamic-static decoupled alignment strategy, which can adaptively select the alignment method based on pixel displacement, thus accurately aggregating past long-term memories to create multiple memory streams. Further, we propose an adaptive memory fusion module to mine complementary information between multiple memory streams to solve the memory vanishing problem. Extensive experimental results show that our method outperforms existing video super-resolution methods on complex exposure datasets. We also conduct detailed ablation experiments to analyze and validate our contributions. The implementation code is available at https://github.com/GZ-T/MMVSR.},
  archive      = {J_TMM},
  author       = {Guozhi Tang and Hongwei Ge and Yong Luo and Bo Li and Chunguo Wu},
  doi          = {10.1109/TMM.2025.3607758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clean image may be dangerous: Data poisoning attacks against deep hashing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image retrieval using deep hashing has become increasingly popular due to the exponential growth of image data and the remarkable feature extraction capabilities of deep neural networks (DNNs). However, deep hashing methods are vulnerable to malicious attacks, including adversarial and backdoor attacks. It is worth noting that these attacks typically involve altering the query images, which is not a practical concern in real-world scenarios. In this paper, we point out that even clean query images can be dangerous, inducing malicious target retrieval results, like undesired or illegal images. To the best of our knowledge, we are the first to study data poisoning attacks against deep hashing (PADHASH). Specifically, we first train a surrogate model to simulate the behavior of the target deep hashing model. Then, a strict gradient matching strategy is proposed to generate the poisoned images. Extensive experiments on different models, datasets, hash methods, and hash code lengths demonstrate the effectiveness and generality of our attack method.},
  archive      = {J_TMM},
  author       = {Shuai Li and Jie Zhang and Yuang Qi and Kejiang Chen and Tianwei Zhang and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2025.3607774},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clean image may be dangerous: Data poisoning attacks against deep hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale invertible neural network for wide-range variable-rate learned image compression. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder-based structures have dominated recent learned image compression methods. However, the inherent information loss associated with autoencoders limits their rate-distortion performance at high bit rates and restricts their flexibility of rate adaptation. In this paper, we present a variable-rate image compression model based on invertible transform to overcome these limitations. Specifically, we design a lightweight multi-scale invertible neural network, which bijectively maps the input image into multi-scale latent representations. To improve the compression efficiency, a multi-scale spatial-channel context model with extended gain units is devised to estimate the entropy of the latent representation from high to low levels. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared to existing variable-rate methods, and remains competitive with recent multi-model approaches. Notably, our method is the first learned image compression solution that outperforms VVC across a very wide range of bit rates using a single model, especially at high bit rates.},
  archive      = {J_TMM},
  author       = {Hanyue Tu and Siqi Wu and Li Li and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2025.3607748},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale invertible neural network for wide-range variable-rate learned image compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIFormer+: Rethinking rotation-invariant feature learning in transformer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved remarkable success in the field of computer vision due to their advantage in capturing the global information of images. However, they fail to model the variance of rotation, resulting in significant performance loss in target detection in remote sensing imagery. In this paper, a rotation-invariant transformer plus model, namely RIFormer+ is proposed to enhance the capabilities of transformers in rotation-invariant feature learning at both long-overlooked local-level and the acknowledged global-level. At the local-level, a rotation-invariant cross-patch embedding (RICPE) module is designed to generate dense patches, which handles encoding inconsistency of tokens with similar semantic information before and after rotation. Moreover, response-enhanced attention (REA) is proposed to extract more rotation-robust global features, which highlights overly dispersed responses ensure sustained attention on discriminative regions. Extensive experiments on three datasets demonstrate the effectiveness of RIFormer+. Without bells and whistles, RIFormer+ increases the classification accuracy by an average of 10% and improves the accuracy on rotated datasets by 20% compared with some state-of-the-art transformers. The code of this paper is available at: https://github.com/psychAo/RIFormerPlus.},
  archive      = {J_TMM},
  author       = {Chao Song and Yifan Zhang and Mingyang Ma and Shaohui Mei},
  doi          = {10.1109/TMM.2025.3607728},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RIFormer+: Rethinking rotation-invariant feature learning in transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Unmanned Aerial Vehicles (UAVs) and their unique vantage points present both new opportunities and challenges for person Re-Identification (ReID). Uncertain rotations and scale variations of targets in UAV images, coupled with complex environmental factors, hinder existing methods from extracting robust feature representations. Some methods either make minor modifications to the traditional model architecture or apply simple image rotations but still fail to effectively address the challenges of UAV person ReID. To overcome these limitations, we propose a novel Data Augmentation and Rotation Invariance (DARI) algorithm. First, rotation-invariant convolution is introduced to adaptively extract features, mitigating the uncertainty caused by target rotation. Second, a refined data augmentation correction strategy is employed to reduce noise interference by increasing the richness of global features at different stages. Additionally, considering that multiple features of the same identity should yield consistent recognition result, invariant constraints are designed to enhance the clustering effect. We conducted extensive experiments on both UAV and fixed-camera datasets. The results on PRAI-1581 demonstrate a 5.6% and 6.1% improvement in mAP and Rank-1, respectively, compared to baseline. These findings highlight the model's effectiveness in addressing the challenges of UAV ReID, demonstrating its robustness and superiority. The source code will be released at https://github.com/ZFZ314/DARI.},
  archive      = {J_TMM},
  author       = {Fuzeng Zhang and Eksan Firkat and Hongbing Ma and Jihong Zhu and Bin Zhu and Askar Hamdulla},
  doi          = {10.1109/TMM.2025.3607835},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential of diffusion-generated data on salient object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, salient object detection (SOD) has made significant progress. However, this advancement is often constrained by the requirement for extensive training data and expensive manual annotation. To eliminate the laborious cost of dataset collection and pixel-level annotation, in this work, we employ Stable Diffusion to synthesize data and subsequently automate annotation for the SOD task. Firstly, we design a unified prompt and ChatGPT4 driven diverse prompts, which guide generating images with simple and complex scenes using Stable Diffusion. Secondly, the reliable pseudo-labels of these synthetic images are generated. For simple images, we propose the simple pseudo-label generation (SPLG) strategy which combines SAM segmentation and CLIP classifier, then train the initial SOD model. For complex images, we utilize the inference capability of the initial SOD model to generate pseudo-labels using the complex pseudo-label generation (CPLG) strategy, and employ iterative training to dynamically update the pseudo-labels. Finally, we design a simple yet effective SOD model which combines a feature fusion module (FFM) and an edge enhancement module (EEM), the former is employed to extract saliency via fusing high-level features, and the latter extracts spatial positional information from low-level features to enhance the edges of saliency results. Experiments on five benchmarks show that our method outperforms the unannotated methods, and also demonstrates better or comparable performance than weak annotation based methods. Our code will be published at https://github.com/FangWenRE/PotentialOfSDSOD.},
  archive      = {J_TMM},
  author       = {Xiangquan Liu and Xianlong Luo and Ying Ye and Xiaoming Huang},
  doi          = {10.1109/TMM.2025.3607734},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Potential of diffusion-generated data on salient object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced subspace clustering network with information bottleneck. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data mining, subspace clustering is a crucial technique which determines the union of the underlying subspace to cluster data points in an unsupervised manner. Although deep-learning-based subspace clustering, typically referred to as deep subspace clustering (DSC), has significantly improved clustering accuracy, existing DSC models still struggle to capture a comprehensive and compact latent representation as they generally explore the spatial domain to extract useful information and face difficulty in balancing the high mutual and low redundant information between the original input space and latent subspace. This leads to the performance of the model being dependent on initialization, resulting in a lack of stability. In this study, a novel network is proposed to extract features in both the frequency domain and spatial domain. We introduce three types of ResBlocks in the discrete Fourier transform (DFT), discrete cosine transform (DCT), or discrete wavelet transform (DWT) frequency domains separately to learn both the low-frequency and high-frequency information in the proposed networks. Additionally, to extract concise and rich latent representations, IB loss is employed by deriving a variational lower bound on the IB objective. Extensive experiments on several benchmark datasets verify the effectiveness of our networks compared to state-of-the-art models. In addition, detailed ablation studies are performed to demonstrate the advantages of the two introduced components.},
  archive      = {J_TMM},
  author       = {Mengran Hou and Mengyao Li and Chengli Tan and Junmin Liu and Jinhai Li and Huirong Li},
  doi          = {10.1109/TMM.2025.3607797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-enhanced subspace clustering network with information bottleneck},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local prompts distillation for domain generalization. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has been proven effective for Domain Generalization (DG) by enhancing the generalization capability of visual-language models with fewer learnable tokens. Existing methods adopt mostly inferring global-level individual prompts for the whole dataset to capture domain-invariant knowledge across different domains. However, since domain shifts exist, a single global-level individual prompt is easily overfitted to source domain datasets, thus lacking generalizability to the whole dataset's feature distribution. Moreover, fluctuations in the generalization performance during the training process in DG problems often pose significant challenges to model selection strategies. To address the aforementioned problems, inspired by the Mixture-of-Expert (MOE) and knowledge distillation, we propose a novel Multiple Local Prompts Distillation (MLPD) method to inject the knowledge of multiple local prompts into a unique global prompt, improving both the generalization and discriminative ability. To ensure the diversity of local prompts, we split the whole dataset into several subsets to infer the discriminative local prompts for each subset, which is further applied to generate the generability global prompt. Formally, for each subset, Meta Prompt Tuning (MPT) is proposed to constrain each local prompt to capture both the domain-specific and domain-shared generalization knowledge on the basis of the domain label and meta-learning mechanism. After that, Prompt Knowledge Distillation (PKD) is proposed to distill the knowledge captured in the local-level prompts into the global-level prompt with prompt-level and feature-level knowledge distillations. The final evaluation on multiple benchmarks underscores the effectiveness of the proposed MLPD, e.g., achieving mAPs of 97.3%, 84.8%, 85.2%, 57.3%, and 60.7% on PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, respectively.},
  archive      = {J_TMM},
  author       = {Huaihai Lyu and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607719},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple local prompts distillation for domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuInNet: Dual-modality feature interaction for point cloud completion. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further promote the development of multimodal point cloud completion, we contribute a large-scale multimodal point cloud completion benchmark ModelNet-MPC with richer shape categories and more diverse test data, which contains nearly 400,000 pairs of high-quality point clouds and rendered images of 40 categories. Besides the fully supervised point cloud completion task, two additional tasks including denoising completion and zero-shot learning completion are proposed in ModelNet-MPC, to simulate real-world scenarios and verify the robustness to noise and the transfer ability across categories of current methods. Meanwhile, considering that existing multimodal completion pipelines usually adopt a unidirectional fusion mechanism and ignore the shape prior contained in the image modality, we propose a Dual-Modality Feature Interaction Network (DuInNet) in this paper. DuInNet iteratively interacts features between point clouds and images to learn both geometric and texture characteristics of shapes with the dual feature interactor. To adapt to specific tasks such as fully supervised, denoising, and zero-shot learning point cloud completions, an adaptive point generator is proposed to generate complete point clouds in blocks with different weights for these two modalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC benchmarks demonstrate that DuInNet exhibits superiority, robustness and transfer ability in all completion tasks over state-of-the-art methods. The code and dataset will be available at https://github.com/xinpuliu/DuInNet.},
  archive      = {J_TMM},
  author       = {Xinpu Liu and Baolin Hou and Hanyun Wang and Ke Xu and Jianwei Wan and Yulan Guo},
  doi          = {10.1109/TMM.2025.3607739},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DuInNet: Dual-modality feature interaction for point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module-a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: (i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and (ii) Attribute binding, ensuring that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models like Stable Diffusion and Gligen, markedly enhancing models' performance in addressing these key challenges. We assess our technique on the well-established CompBench and TIFA score benchmarks, and HRS dataset where B2B not only surpasses methods specialized in either attribute binding or layout guidance but also uniquely excels by integrating these capabilities to deliver enhanced overall performance.},
  archive      = {J_TMM},
  author       = {Ashkan Taghipour and Morteza Ghahremani and Mohammed Bennamoun and Aref Miri Rekavandi and Hamid Laga and Farid Boussaid},
  doi          = {10.1109/TMM.2025.3607759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in point cloud compression (PCC) is how to evaluate the perceived distortion so that the codec can be optimized for perceptual quality. Current standard practices in PCC highlight a primary issue: while single-feature metrics are widely used to assess compression distortion, the classic method of searching point-to-point nearest neighbors frequently fails to adequately build precise correspondences between point clouds, resulting in an ineffective capture of human perceptual features. To overcome the related limitations, we propose a novel assessment method called RBFIM, utilizing radial basis function (RBF) interpolation to convert discrete point features into a continuous feature function for the distorted point cloud. By substituting the geometry coordinates of the original point cloud into the feature function, we obtain the bijective sets of point features. This enables an establishment of precise corresponding features between distorted and original point clouds and significantly improves the accuracy of quality assessments. Moreover, this method avoids the complexity caused by bidirectional searches. Extensive experiments on multiple subjective quality datasets of compressed point clouds demonstrate that our RBFIM excels in addressing human perception tasks, thereby providing robust support for PCC optimization efforts.},
  archive      = {J_TMM},
  author       = {Zhang Chen and Shuai Wan and Siyu Ren and Fuzheng Yang and Mengting Yu and Junhui Hou},
  doi          = {10.1109/TMM.2025.3607782},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRFCNet: Feature refinement and flexible concatenation for object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art YOLO detection algorithms still suffer from the issue of redundant extraction of similar features during feature propagation, and the simplistic stacking approach of connecting different features limits the flexibility of feature fusion. We propose a new feature recombination mechanism involving refining feature extraction and flexible concatenation. It includes the HFConv (Hybrid Flexibility Convolution) module, the MFD (Multivariate Flexibility Downsampling) module, and the DFSPP (Deformable and Flexible Spatial Pyramid Pooling) module. Specifically, the HFConv module employs feature refinement and flexible connection strategies to optimize feature representation and reduce redundancy in a dynamic way, acquiring diverse feature information from local and surrounding regions. The MFD module leverages multiple downsampling methods to address the issue of feature redundancy that may arise from a single downsampling method, thereby enhancing feature diversity. The DFSPP module learns an offset corresponding to the pooling kernel size, allowing for the extraction of the most critical information in a dynamic manner. By incorporating these modules into the YOLO architecture, we develop a more robust network called FRFCNet, and the experimental results show a notable 4.1% and 2.8% improvement in AP values on the VOC2012 and COCO2017 datasets, respectively, compared to the baseline (YOLOV7-Tiny-SiLu), outperforming current one-stage detectors.},
  archive      = {J_TMM},
  author       = {Tao Zhang and Zhiheng Wu and Xiangjian He and Qiang Wu},
  doi          = {10.1109/TMM.2025.3607701},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FRFCNet: Feature refinement and flexible concatenation for object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrackletGait: A robust framework for gait recognition in the wild. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8% and 80.4% rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.},
  archive      = {J_TMM},
  author       = {Shaoxiong Zhang and Jinkai Zheng and Shangdong Zhu and Chenggang Yan},
  doi          = {10.1109/TMM.2025.3607705},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TrackletGait: A robust framework for gait recognition in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriPHiT: Privacy-preserving hierarchical training of deep neural networks. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training phase of deep neural networks requires substantial resources and as such is often performed on cloud servers. However, this raises privacy concerns when the training dataset contains sensitive content, e.g., facial or medical images. In this work, we propose a method to perform the training phase of a deep learning model on both an edge device and a cloud server that prevents sensitive content being transmitted to the cloud while retaining the desired information. The proposed privacy-preserving method uses adversarial early exits to suppress the sensitive content at the edge and transmits the task-relevant information to the cloud. This approach incorporates noise addition during the training phase to provide a differential privacy guarantee. We extensively test our method on different facial and medical datasets with diverse attributes using various deep learning architectures, showcasing its outstanding performance. We also demonstrate the effectiveness of privacy preservation through successful defenses against different white-box, deep and GAN-based reconstruction attacks. This approach is designed for resource-constrained edge devices, ensuring minimal memory usage and computational overhead.},
  archive      = {J_TMM},
  author       = {Yamin Sepehri and Pedram Pad and Pascal Frossard and L. Andrea Dunbar},
  doi          = {10.1109/TMM.2025.3607801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PriPHiT: Privacy-preserving hierarchical training of deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screenshot, which is a common tool in office work, has become a significant threat to organizations like companies and research institutions. Malicious users can easily leak sensitive information like business secrets and research data by taking a screenshot and spreading onto the Internet. While existing watermarking schemes serve as useful tools for leakage tracing, they fall short in the scenario of arbitrary screenshot. Most current methods are file-targeted, focusing on embedding watermark for a single file of one type at a time, making it hard to handle arbitrary content on screen. To address the issues above and better satisfy the need of the scenario, we propose ScreenGuard, a novel watermarking scheme targeted for the screen itself to protect arbitrary screen content shown on it. Unlike previous watermarking schemes, ScreenGuard does not modify the content itself. Instead, we generate a transparent mask template based on the watermark, tile it to the size of the screen to form a complete transparent mask, and overlay this mask onto the screen. This ensures that any screenshots taken will contain our watermark. We then train a locator and a decoder to extract watermarks from suspected leaked screenshots to trace leaks to their source. We summarized five properties that needs to be satisfied in the scenario of arbitrary screenshot (Generalizable, Unseeable, Adaptable, Robust, Dynamic) and evaluate our method on these criteria. Extensive experiments demonstrate that ScreenGuard meets these five properties effectively, showcasing its superiority and broad practical applications.},
  archive      = {J_TMM},
  author       = {Gaozhi Liu and Xiujian Liang and Xiaoxiao Hu and Yichao Si and Xinpeng Zhang and Zhenxing Qian},
  doi          = {10.1109/TMM.2025.3607779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical semantic distillation framework for open-vocabulary object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, e.g., CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin.},
  archive      = {J_TMM},
  author       = {Shenghao Fu and Junkai Yan and Qize Yang and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3607729},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hierarchical semantic distillation framework for open-vocabulary object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-attention transformers for class-incremental learning: A tale of two memories. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (Class-IL) aims to continuously learn a model from a sequence of tasks, which suffers from the issue of catastrophic forgetting. Recently, a few transformer based methods are proposed to address this issue by transferring self-attention into task-specific attention. However, these methods utilize shared task-specific attention modules across the whole incremental learning process, and are unable to achieve the balance between consolidation and plasticity, i.e., to remember the knowledge learned from previous tasks and absorb the knowledge from the current task simultaneously. Motivated by the mechanism of LSTM and hippocampus memory, we point out that dual attention on long and short-term memories can handle the consolidation-plasticity dilemma of Class-IL. Typically, we propose Dual-Attention Transformers (DAFormer) to learn external attention and internal attention. The former utilizes sample-dependent keys which exclusively focused on the new tasks, while the latter consolidates the knowledge from previous tasks by using sample-agnostic keys. We present two editions of DAFormer: DAFormer-S and DAFormer-M: the former utilizes shared external keys and maintains a small parameter size, while the latter utilizes multiple external keys and enhances the long-term memory. Furthermore, we propose the $K$-nearest neighbor invariant based distillation scheme, which distills knowledge from previous tasks to current task by maintaining the same neighborhood relationship of each sample over old and new models. Experimental results on CIFAR-100, ImageNet-subset and ImageNet-full demonstrate that DAFormer significantly outperforms all the state-of-the-art parameter-static and parameter-growing methods.},
  archive      = {J_TMM},
  author       = {Shaofan Wang and Weixing Wang and Yanfeng Sun and Zhiyong Wang and Boyue Wang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3607800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-attention transformers for class-incremental learning: A tale of two memories},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-aware dynamic label assignment with high-quality proposals for oriented object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented objects in images are typically embedded in complex backgrounds and exhibit arbitrary orientations. When using oriented bounding boxes (OBBs) to represent these objects, the periodicity of the angles and associated variations in side lengths lead to discontinuities in the angle loss. This paper fundamentally addresses this problem by proposing a trigonometric loss function in the complex plane. Moreover, a conformer RPN head is designed with convolution and multi-head self-attention, which can dynamically capture angular and classification information. The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals. A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for oriented proposal label assignment. This method makes negative sample selection more representative, ensuring consistency between classification and regression features. Experiments were conducted on five realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs. Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, 46.45%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, STAR, and HRSC2016 datasets, respectively.},
  archive      = {J_TMM},
  author       = {Mingkui Feng and Hancheng Yu and Xiaoyu Dang and Ming Zhou},
  doi          = {10.1109/TMM.2025.3607785},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-aware dynamic label assignment with high-quality proposals for oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively representing and transferring user preferences across various domains presents a significant challenge in cross-domain recommendation (CDR). Some approaches utilize graph neural networks that use interaction behavior to establish relationships between entities, providing a comprehensive understanding of user interests. However, the impact of consistent semantics across various types, fields, and perspectives of social media information on user preferences is overlooked, i.e. the multidimensional consistency of user preferences. This oversight results in graph node representations that inadequately reflect user preferences. To address these limitations, we propose a multi-layer transfer learning network (MTLG) for CDR based on graph node representation enhancement via multi-dimensional consistent user preferences. Firstly, the model introduces a set of globally shared semantic units to perform different-grained semantic alignment of multiple media information without clear alignment boundaries, thereby modeling multi-dimensional consistent user preference features. These features are then seamlessly integrated with the initial high-order graph structure embedding features, thus significantly improving the quality of graph node representation. Secondly, the model innovatively designs a multi-layer transfer learning network that hierarchically aligns the domain distribution differences. It calculates the similarity between domains to derive layer weights for more precise transfer learning, thereby mitigating the possibility of information error accumulation resulting from inaccurate feature aggregation processes. We conducted numerous experiments on 3 scenarios, including 7,954,943 rating information from the Amazon dataset. The results indicate that MTLG's recommendation accuracy surpasses those of state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xin Ni and Jie Nie and Niantai Jing and Jianliang Xu and Xiaodong Wang and Xuesong Gao and MingXing Jiang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3607706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth map super-resolution via deep cross-modality and cross-scale guidance. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided depth super-resolution is essential in many applications, which enhances low-resolution (LR) depth maps using high-resolution (HR) RGB images from the same scene. However, the challenge lies in avoiding the texture-copy artifacts issue caused by structural inconsistencies between two modalities. To mitigate, we propose a cross-modality and cross-scale guided depth super-resolution network (D2CNet). We first design a novel two-stage feature integration module to effectively fuse multi-modal RGB and depth while minimizing texture-copy artifacts. That is, a cross-modality fusion stage transfers consistent structures from RGB to depth in a multi-scale manner, and a cross-scale refinement stage mitigates inconsistent structures across modalities. In addition, we design a convolution group as the basic module to well extract high-frequency features and an LR and HR domain projection strategy to enrich features between the fusion and refinement stages. We then develop a new network architecture by progressively repeating the feature integration module and the convolution group, which is flexibly controllable to strike a balance between accuracy and cost for easy implementation in real world. Extensive experiments on multiple benchmarks demonstrate that our D2CNet consistently achieves superior accuracy and generalization ability across sampling scales in both qualitative and quantitative evaluations, when compared to state-of-the-art baselines. The code is at https://github.com/suzdl/d2cnet},
  archive      = {J_TMM},
  author       = {Shuzhe Liu and Delong Suzhang and Meng Yang and Xinhu Zheng and Ce Zhu},
  doi          = {10.1109/TMM.2025.3607763},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Depth map super-resolution via deep cross-modality and cross-scale guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to identify novel compositions formed by known primitives (attributes and objects). Motivated by recent advancements in pre-trained vision-language models such as CLIP, many methods attempt to fine-tune CLIP for CZSL and achieve remarkable performance. However, the existing CLIP-based CZSL methods focus mainly on text prompt tuning, which lacks the flexibility to dynamically adapt both modalities. To solve this issue, an intuitive solution is to additionally introduce visual prompt tuning. This insight is not trivial to achieve because effectively learning prompts for CZSL involves the challenge of entanglement between visual primitives as well as appearance shifts in different compositions. In this paper, we propose a novel Synergetic Prompts as Disentanglement Queries (SPDQ) framework for CZSL. It can disentangle primitive features based on synergetic prompts to jointly alleviate these challenges. Specifically, we first design a low-rank primitive modulator to produce synergetic adaptive attribute and object prompts based on prior knowledge of each instance for model adaptation. Then, we additionally utilize text prefix prompts to construct synergetic prompt queries, which are used to resample corresponding visual features from local visual patches. Comprehensive experiments conducted on three benchmarks demonstrate that our SPDQ approach achieves state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Han Jiang and Xiaoshan Yang and Chaofan Chen and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607726},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning trimaps via clicks for image matting. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant advancements achieved in image matting, the existing models heavily depend on manually drawn trimaps to produce accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming and lacks user-friendliness and device compatibility. This greatly limits the practical applicability of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model that is capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. By analyzing real users' behavioral logic and the characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated simulation function, making Click2Trimap exhibit versatility across various scenarios. Compared with all existing trimap-free matting methods, Click2Trimap achieves superior performance in quantitative and qualitative assessments conducted on synthetic and real-world matting datasets. In particular, in a user study, Click2Trimap yields high-quality trimap and matting predictions in just 5 seconds per image on average, demonstrating its substantial practical value for use in real-world applications.},
  archive      = {J_TMM},
  author       = {Chenyi Zhang and Yihan Hu and Henghui Ding and Humphrey Shi and Yao Zhao and Yunchao Wei},
  doi          = {10.1109/TMM.2025.3607710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning trimaps via clicks for image matting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although screen-camera resilient watermarking addresses issues such as privacy leakage and copyright infringement in digital images to some extent during screen-camera communication. However, in screen-camera scenarios, uncontrolled shooting environments, various display devices, and different lens types introduce more complex noise into the watermarked images. Because some noise generated during the screen-camera process cannot be quantitatively analyzed, the integrity of the embedded watermark is compromised, making copyright verification and information acquisition still difficult. To solve this problem, we establish a large-scale screen-camera image dataset (SCISet) and propose a noise simulation network (NoS-Net). Specifically, we obtain 36,000 screen-camera images under various shooting environments with multiple types of screens and cameras. Then, we use SCISet to train the proposed NoS-Net based on the U-Net architecture, which can learn multi-level and complementary feature information of screen-camera images, enhancing its ability to simulate complex noise. Experimental results show that integrating the proposed NoS-Net into mainstream screen-camera resilient watermarking methods significantly improves their ability to resist screen-camera noise attacks. Furthermore, the diversity of SCISet plays an important role in advancing robust watermarking research.},
  archive      = {J_TMM},
  author       = {Daidou Guo and Chuan Qin and Fengyong Li and Heng Yao and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3607783},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD). However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture. To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and total Floating Point Operations (FLOPs). Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework. Simulations show that the proposed model is robust against noise and negative samples. Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest parameters and FLOPs. Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities.},
  archive      = {J_TMM},
  author       = {Ruituo Wu and Yang Chen and Jian Xiao and Bing Li and Jicong Fan and Frédéric Dufaux and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TMM.2025.3607708},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oversampling with GAN via meta-learning for imbalanced data. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing generative adversarial networks (GANs) for oversampling imbalanced data has demonstrated its effectiveness. However, many GAN-based oversampling methods are confronted with a significant challenge, namely, mode collapse, especially when dealing with tabular imbalanced data. In this paper, two unique penalty terms are respectively incorporated into the loss functions of the discriminator and the generator of GAN to promote the generated samples to exhibit not just statistical but also spatial information consistency with the minority samples, thereby alleviating the issue of mode collapse. In contrast to other studies that fix the coefficient of the penalty terms, the optimal coefficients of the penalty terms are adaptively searched using a meta-learning approach, where Bayesian optimization is firstly employed to effectively handle situations involving small size of minority samples in the imbalanced data. We call the proposed model as META_GAN. Experimental results demonstrate that META_GAN outperforms alternative oversampling methods on general tabular and image imbalanced datasets and long-tailed datasets in terms of different metrics.},
  archive      = {J_TMM},
  author       = {Yueqi Chen and Witold Pedrycz and Chao Zhang and Jian Wang and Jie Yang},
  doi          = {10.1109/TMM.2025.3607712},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oversampling with GAN via meta-learning for imbalanced data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting long-tailed recognition with label descriptor and beyond. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Tailed Recognition (LTR) poses significant challenges due to the heavily imbalanced nature of real-world data, which severely skews data-driven deep neural networks. Despite the rapid progress of Vision-Language Models (VLMs), they still face challenges in effectively learning from long-tailed visual data. In this paper, we present a comprehensive analysis of the reasons behind the underperformance of VLMs and propose a hierarchical inference framework to address this issue. Specifically, we prompt the large language models to generate sentencelevel descriptors for class labels and conduct the open vocabulary classification by computing the average similarity between the image and each descriptor. A reweighting mechanism is further proposed to filter out uninformative descriptors. To mitigate model bias incurred by the long-tail distribution, we propose a feature adapter with the logit adjustment technique and finetune the CLIP model via visual prompt tokens. We introduce the Shared Feature space Mixup (SFM) to enhance the interaction between modalities to address tail visual feature insufficiency. Finally, we propose a hierarchical inference manner to combine the aforementioned proposals. Extensive evaluations demonstrate that our approach achieves state-of-the-art performance by finetuning only a few parameters on the Places-LT, ImageNet-LT, and iNaturalist 2018 benchmarks.},
  archive      = {J_TMM},
  author       = {Zhengzhuo Xu and Ruikang Liu and Zenghao Chai and Yiyan Qi and Lei Li and Haiqin Yang and Chun Yuan},
  doi          = {10.1109/TMM.2025.3607812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting long-tailed recognition with label descriptor and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive action quality assessment through multi-branch modeling. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA) aims to evaluate and score human actions in videos accurately. Existing approaches involve extracting features from the input video and implementing regression based on those features. However, representations derived from a single branch often lack the necessary diversity and flexibility to capture the complexity of human actions effectively. This work addresses these limitations by introducing a multi-branch architecture designed to capture a broad spectrum of video dynamics at varying levels of granularity. Specifically, we enhance video representation in the flow-guided branch by integrating optical flow with video features. This combination of multimodal features offers a more comprehensive context of global motion. Meanwhile, the momentfocused branch is tailored to extract frame-specific features, constructing two distinct quality-based representations with different focuses on moments, which achieves adaptive clues aggregation. Furthermore, the detail-aware branch leverages multiscale deep embeddings from a hierarchy convolutional neural network to capture fine-grained spatial information, which is useful when objects have complex spatial changes. Finally, a post-fusion strategy is employed to merge outputs from all branches, contributing to the comprehensive action quality assessment. Experimental evaluations on three benchmark datasets, FineDiving, MTLAQA, and AQA-7, demonstrate the superiority of our model in providing reliable assessments of action quality.},
  archive      = {J_TMM},
  author       = {Siyuan Xu and Peilin Chen and Yue Liu and Meng Wang and Shiqi Wang and Hong Yan and Sam Kwong},
  doi          = {10.1109/TMM.2025.3607713},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comprehensive action quality assessment through multi-branch modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the optimization of quality of experience (QoE) through learning adaptive bitrate (ABR) algorithms from internet video streams. However, the complex nature of the real-world Internet, characterized by heavy-tailed behavior, diversity, and unpredictability, hinder the effective learning of off-the-shelf reinforcement learning (RL)-based ABR algorithms. As a result, existing methods inevitably fail to achieve optimal performance under various network conditions and user QoE objectives. We propose Fortuna, a novel offline meta RL ABR algorithm that can effectively learn from these heavy-tailed internet data features and become more practical. Fortuna is primarily divided into two phases. In the offline phase, Fortuna utilizes diverse offline data for learning to reduce the costly online RL interaction expense, while in the online phase, we gradually increase video streaming sessions complexity through curriculum learning to quickly adapt to specific network conditions. Fortuna then utilizes meta-learning to optimize ABR policies and enhance generalization. Additionally, to better learn network features, Fortuna further optimizes QoE by learning low-level TCP congestion control information. Experimental results from trace-driven and real-world scenarios demonstrate that Fortuna enhances learning efficiency by more than 7.5%-4 ×, reduces stall time by 4.6%-14.2%, and generalizes to different network conditions and video streams.},
  archive      = {J_TMM},
  author       = {Ling Yi and Yongbin Qin and Ruizhang Huang},
  doi          = {10.1109/TMM.2025.3604930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal transformer for cross-modal long document classification. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3608295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.},
  archive      = {J_TMM},
  author       = {Tengfei Liu and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1109/TMM.2025.3608295},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-modal transformer for cross-modal long document classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USTC-TD: A test dataset and benchmark for image and video coding in 2020s. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3608643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image/video coding has been a remarkable research area for both academia and industry for many years. Testing datasets, especially high-quality image/video datasets, are desirable for the justified evaluation of coding-related research, practical applications, and standardization activities. We put forward a test dataset, namely USTC-TD, which has been successfully adopted in the practical end-to-end image/video coding challenge of IEEE International Conference on Visual Communications and Image Processing (VCIP) in 2022 and 2023. USTC-TD contains 40 images at 4K spatial resolution and 10 video sequences at 1080p spatial resolution, featuring various content due to the diverse environmental factors (e.g., scene type, texture, motion, view) and the designed imaging factors (e.g., illumination, lens, shadow). We quantitatively evaluate USTC-TD on different image/video features (spatial, temporal, color, lightness), and compare it with the previous image/video test datasets, which verifies its excellent compensation for the shortcomings of existing datasets. We also evaluate both classic standardized and recently learned image/video coding schemes on USTC-TD using objective quality metrics (PSNR, MS-SSIM, VMAF) and subjective quality metric (MOS), providing an extensive benchmark for these evaluated schemes. Based on the characteristics and specific design of the proposed test dataset, we analyze the benchmark performance and shed light on the future research and development of image/video coding. All the data are released online: https://esakak.github.io/USTC-TD.},
  archive      = {J_TMM},
  author       = {Zhuoyuan Li and Junqi Liao and Chuanbo Tang and Haotian Zhang and Yuqi Li and Yifan Bian and Xihua Sheng and Xinmin Feng and Yao Li and Changsheng Gao and Li Li and Dong Liu and Feng Wu},
  doi          = {10.1109/TMM.2025.3608643},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {USTC-TD: A test dataset and benchmark for image and video coding in 2020s},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JPEG AI compressed domain face detection: A multi-scale bridging perspective. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3609179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image coding is showing improved compression efficiency, while also offering a novel advantage in enabling computer vision tasks directly within the compressed domain. The latent representation created by deep learning methods inherently contains all visual features, without a computationally expensive synthesis process at the decoder. This paper is an invited extension of a previous solution for JPEG AI compressed domain face detection that adapts a RetinaFace-based detector to operate directly on the latent tensor. In addition to a former single-scale bridging solution, this work provides a novel multi-scale bridging architecture to enable a more effective multi-scale compressed domain face detection. The results show a significant performance gain, improving accuracy up to 20% for detection of tiny faces on the WIDER FACE dataset compared to single-scale bridging, and further narrowing the gap when compared to detection on uncompressed or JPEG AI decoded images. Furthermore, since the computationally expensive decoding step is bypassed and since the bridges consist of lower-complexity networks, the overall processing cost is significantly reduced. Single and multi-scale bridging, respectively, have about 10% and 32% the complexity of applying pixel domain face detection on decoded images. The proposed architecture is expected to be extended to other multiscale sensitive vision tasks, as JPEG AI is not specifically designed for any single downstream application.},
  archive      = {J_TMM},
  author       = {Ayman Alkhateeb and Alessandro Gnutti and Fabrizio Guerrini and Riccardo Leonardi and João Ascenso and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3609179},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {JPEG AI compressed domain face detection: A multi-scale bridging perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-led prompting learning caption generating model and benchmark. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) is an important intelligent system application, but most current research views it as a coarse binary classification task that lacks a fine-grained understanding of abnormal video sequences. We explore a new task for video anomaly analysis called Comprehensive Video Anomaly Caption (CVAC), which aims to generate comprehensive textual captions (containing scene information such as time, location, anomalous subject, anomalous behavior, etc.) for surveillance videos. CVAC is more consistent with human understanding than VAD, but it has not been well explored. We constructed a large-scale benchmark CVACBench to lead this research. For each video clip, we provide 6 fine-grained annotations, including scene information and abnormal keywords. A new evaluation metric Abnormal-F1 (A-F1) is also proposed to more accurately evaluate the caption generation performance of the model. We also designed a method called Anomaly-Led Generating Prompting Transformer (AGPFormer) as a baseline. In AGPFormer, we introduce an anomaly-led language modeling mechanism (Anomaly-Led MLM, AMLM) to focus on anomalous events in videos. To achieve more efficient cross-modal semantic understanding, we design the Interactive Generating Prompting (IGP) module and Scene Alignment Prompting (SAP) module to explore the divide between video and text modalities from multiple perspectives, and to improve the model's performance in understanding and reasoning about the complex semantics of videos. We conducted experiments on CVACBench by using traditional caption metrics and the proposed metrics, and the experimental results demonstrate the effectiveness of AGPFormer in the field of anomaly caption.},
  archive      = {J_TMM},
  author       = {Qianyue Bao and Fang Liu and Licheng Jiao and Yang Liu and Shuo Li and Lingling Li and Xu Liu and Xinyi Wang and Baoliang Chen},
  doi          = {10.1109/TMM.2025.3607837},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anomaly-led prompting learning caption generating model and benchmark},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3608947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised text-based person re-identification (Text-ReID) confronts the challenge of matching target person images with textual descriptions, hindered by the absence of identity annotations during training. Traditional approaches, which rely solely on global features, overlook the rich, fine-grained information within both text and image modalities. Besides, merely aligning features at the semantic level is insufficient due to the significant differences in feature representation spaces between the two modalities. Existing methods also neglect the information inequality caused by person-irrelevant factors in images. In this paper, we introduce a novel framework called Attribute-Centric Cross-modal Alignment (ACCA), specifically designed to overcome these issues. Our approach concentrates on two main aspects: visual-text attribute alignment and prediction distribution alignment. To effectively capture fine-grained information without identity labels, we implement a visual-text attribute alignment method based on momentum contrastive learning to synchronize visual and textual attribute features within a unified embedding space. We also propose a unique strategy for negative sample filtering and enrichment, creating robust and comprehensive negative attribute sample spaces to support the attribute alignment. Additionally, we establish two methods of label-free prediction distribution alignment to encourage the learning of invariant feature representations across modalities. The first method, bias-reduction distribution alignment, aligns features and predictions within each text-image pair by utilizing semantic information from the text and reduces the impact of person-irrelevant factors in images. The second method, global-attribute distribution alignment, enhances the interaction between global and local prediction distributions across visual and textual modalities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets validate our superior performances across all standard benchmarks.},
  archive      = {J_TMM},
  author       = {Jiajia Xu and Weiwei Cai and Xuemiao Xu and Yi Xie and Huaidong Zhang and Shengfeng He},
  doi          = {10.1109/TMM.2025.3608947},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSG: Multi-scale speaker graph network for active speaker detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3608949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The active speaker detection task is to determine whether a person is speaking or not across a series of video frames. Existing methods heavily rely on facial information within the annotated face bounding boxes for cross-modal learning with audio. This leads to a substantial decline in detection performance when facial cues are unclear, such as in cases of face occlusion or low-resolution facial appearances. In this paper, we extend the perception scale using only face bounding box annotations to model both facial and gestural cues, addressing the over-reliance on facial cues in active speaker detection. We propose a novel graph neural network that models inter-speaker interactions and integrates various cues from individual speakers. The final detection results are obtained through a binary graph node classification task. Our method achieves state-of-the-art performance on the AVA-ActiveSpeaker dataset (mAP: 95.6%) and the ASW dataset (mAP: 99.4%), with a model size only 21% that of the second-best method. Additionally, when facial cues are of poor quality, our method demonstrates a significant performance advantage over existing approaches. The code and model weights will be available at https://github.com/sdqdlgj/MSSG.},
  archive      = {J_TMM},
  author       = {Guanjun Li and Jiangyan Yi and Zhengqi Wen and Ruibo Fu and Yuwang Wang and Jianhua Tao},
  doi          = {10.1109/TMM.2025.3608949},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSSG: Multi-scale speaker graph network for active speaker detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning orthogonal latent representations for multi-view clustering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view clustering, latent representations are often employed to address the challenge posed by low-quality data. Traditional approaches typically assume that multiple views are fully dependent, directly learning a common latent representation from the observed data. However, this assumption is overly restrictive in real-world scenarios and may overlook valuable information, as the independence of different views can reveal critical view-specific characteristics. To overcome this limitation, we propose learning Orthogonal Latent Representations for Multi-View Clustering (OLR-MVC), which jointly captures both cross-view dependence and independence. Specifically, our model maps multi-view data into shared and private latent spaces using distinct projection bases. To accurately capture both dependence and independence, we enforce orthogonality between the shared and private latent representations while also encouraging pairwise orthogonality among private representations. Furthermore, we leverage the self-expressive property of these latent representations to capture global data structures. Extensive experimental evaluations demonstrate that OLR-MVC outperforms state-of-the-art multi-view clustering methods.},
  archive      = {J_TMM},
  author       = {Xiaolin Xiao and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TMM.2025.3607704},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning orthogonal latent representations for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dehazing approach: Recovery of color and polarization information using polarized characteristics. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarization provides valuable physical information, making it beneficial for various computer vision tasks. However, haze reduces both the color and polarization information of a scene. While existing single-image dehazing methods can restore color information, they are poor at recovering polarization information. Furthermore, current polarization-based dehazing approaches neglect the physical mechanisms of polarization degradation, resulting in inaccurate reconstruction of polarization information. In this paper, we propose a novel polarization dehazing algorithm, along with a polarization degradation model, to accurately recover both polarization and color information. First, we combine two key characteristics (the polarization achromatism prior and polarization attenuation prior) with the polarization degradation model to precisely reconstruct the scene's polarization. Then, we utilize the reconstructed polarization information to recover the color information of the scene. Finally, a multi-scale fusion optimization framework is introduced to further enhance the image quality. Our method shows excellent performance on both real-world indoor and outdoor polarized images, outperforming existing dehazing algorithms in both objective evaluation metrics and subjective visual assessment.},
  archive      = {J_TMM},
  author       = {Zhenshuo Yang and Chunhui Hao and Yang Lu and Yiming Su and Yukuan Zhang and Junchao Zhang and Jiandong Tian},
  doi          = {10.1109/TMM.2025.3607694},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel dehazing approach: Recovery of color and polarization information using polarized characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained domain generalization with feature structuralization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained domain generalization (FGDG) is a more challenging task than traditional DG tasks due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the vulnerability of subtle features leads to a severe deterioration in model performance. Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning the commonality and specificity within categories. Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is accomplished through joint optimization of five constraints: a decorrelation function applied to disentangled segments, three constraints ensuring common feature consistency and specific feature distinctiveness, and a prediction calibration term. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.2% in FGDG performance. Beyond that, the explainability analysis on explicit concept matching intensity between the shared concepts among categories and the model channels, along with experiments on various mainstream model architectures, substantiates the validity of FS.},
  archive      = {J_TMM},
  author       = {Wenlong Yu and Dongyue Chen and Qilong Wang and Qinghua Hu},
  doi          = {10.1109/TMM.2025.3607716},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained domain generalization with feature structuralization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detailed object description with controllable dimensions. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object description plays an important role for visually impaired individuals to understand and compare the differences between objects. Recent multimodal large language models (MLLMs) exhibit powerful perceptual abilities and demonstrate impressive potential for generating object-centric descriptions. However, the descriptions generated by such models may still usually contain a lot of content that is not relevant to the user intent or miss some important object dimension details. Under special scenarios, users may only need the details of certain dimensions of an object. In this paper, we propose a training-free object description refinement pipeline, Dimension Tailor, designed to enhance user-specified details in object descriptions. This pipeline includes three steps: dimension extracting, erasing, and supplementing, which decompose the description into user-specified dimensions. Dimension Tailor can not only improve the quality of object details but also offer flexibility in including or excluding specific dimensions based on user preferences. We conducted extensive experiments to demonstrate the effectiveness of Dimension Tailor on controllable object descriptions. Notably, the proposed pipeline can consistently improve the performance of the recent MLLMs. The code is currently accessible at https://github.com/PRIS-CV/ControllableObjectDescription.},
  archive      = {J_TMM},
  author       = {Xinran Wang and Haiwen Zhang and Baoteng Li and Kongming Liang and Hao Sun and Zhongjiang He and Zhanyu Ma and Jun Guo},
  doi          = {10.1109/TMM.2025.3607747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detailed object description with controllable dimensions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truncate diffusion: Efficient video editing with low-rank truncate. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated remarkable capabilities for text-to-video (T2V) editing tasks, relying on fine-tuning for pretrained text-to-image (T2I) diffusion models with only one video-prompt pair. However, conventional fine-tuning approaches require tuning and storing numerous parameters for each video, leading to substantial parameter and memory costs. To mitigate these issues, we propose Truncating Diffusion, an efficient fine-tuning method for video editing that optimizes both parameter and memory usage. Specifically, we propose the Truncating Diffusion module, which is designed with a focus on module architecture and initialization, specifically targeting optimization with a small training set, such as a single video-prompt pair. Theoretical analysis using the Johnson-Lindenstrauss lemma and the Eckart-Young-Mirsky theorem shows that Truncating Diffusion can achieve a minimal Frobenius norm distance to the original attention algorithm with appropriate initialization, which enhances the ease of optimization and improves video editing performance. During fine-tuning, the Truncating Diffusion module integrates seamlessly with the original diffusion model. We freeze the weights of the denoising network within the original pretrained diffusion model, updating only the introduced low-rank alternatives to ensure parameter and memory efficiency. Additionally, we propose Latent Flow Loss and Bidirectional Inter-Frame Attention (BIFA) to improve temporal consistency in synthesized videos. The Latent Flow Loss leverages global temporal information from the input video during training, while BIFA utilizes local temporal information from adjacent frames during inference. These enhancements do not incur additional memory or parameter costs during fine-tuning. Comparisons with state-of-the-art approaches demonstrate that Truncating Diffusion provides superior text alignment, video quality, and inter-frame temporal consistency in video editing. Importantly, Truncating Diffusion requires fine-tuning only 3.2% of the parameters and uses just 62% of the memory compared to the baseline model.},
  archive      = {J_TMM},
  author       = {Bosheng Qin and Wentao Ye and Chi Zhang and Qifan Yu and Wenqiao Zhang and Silang Tang and Yueting Zhuang},
  doi          = {10.1109/TMM.2025.3590901},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Truncate diffusion: Efficient video editing with low-rank truncate},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRSOT: Cross-resolution object tracking using unaligned frame and event cameras. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3586135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing datasets for RGB-DVS tracking are collected with DVS346 camera and their resolution ($346 \times 260$) is low for practical applications. Actually, only visible cameras are deployed in many practical systems, and the newly designed neuromorphic cameras may have different resolutions. The latest neuromorphic sensors can output high-definition event streams, but it is very difficult to achieve strict alignment between events and frames on both spatial and temporal views. Therefore, how to achieve accurate tracking with unaligned neuromorphic and visible sensors is a valuable but unresearched problem. In this work, we formally propose the task of object tracking using unaligned neuromorphic and visible cameras. We build the first unaligned frame-event dataset CRSOT collected with a specially built data acquisition system, which contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In addition, we propose a novel unaligned object tracking framework that can realize robust tracking even using the loosely aligned RGB-Event data. This proposed method utilizes uncertainty perception techniques, which can effectively reduce the negative impact of noise (especially noise in event data) on tracking performance. Specifically, we extract the template and search regions of RGB and Event data and feed them into a unified ViT backbone for feature embedding. Next, we propose uncertainty perception modules to encode the RGB and Event features, respectively, then, we propose a modality uncertainty fusion module to aggregate the two modalities. These three branches are jointly optimized in the training phase. Extensive experiments demonstrate that our tracker can collaborate the dual modalities for high-performance tracking even without strictly temporal and spatial alignment.},
  archive      = {J_TMM},
  author       = {Yabin Zhu and Xiao Wang and Chenglong Li and Bo Jiang and Lin Zhu and Zhixiang Huang and Yonghong Tian and Jin Tang},
  doi          = {10.1109/TMM.2025.3586135},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CRSOT: Cross-resolution object tracking using unaligned frame and event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video segmentation and tokenization for model-based video scene classification. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3595019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach for segmenting and tokenizing a video scene recording into a sequence of cascade units, known as visual segment units and modeled with visual segment models (VSMs) for video scene classification (VSC). Specifically, the proposed VSM framework takes deep visual features extracted from pre-trained encoders as inputs and models the temporal interactions between segment units by hidden Markov models. Next, we use unit co-occurrence statistics to introduce relationships between VSM units within a video scene recording. Furthermore, the VSM approach is extended to an acoustic-visual variant, subsequently integrating itself into a deep learning-based multi-modal scene classification system. This combination serves to further exploit the complementary nature of audio and video data. By incorporating a set of visual segment units into modeling a video scene class, it captures both inter-class similarity and intra-class diversity, facilitating improved scene classification, especially within categories prone to confusion. Extensive experimental results on a benchmark published by the DCASE (Detection and Classification of Acoustic Scenes and Events) 2021 Challenge show that the proposed framework can effectively handle the confusion issue among similar video scenes. In addition, our multi-modal integration system achieves state-of-the-art performance in the audio-visual scene classification task in the DCASE 2021 Challenge, thereby demonstrating the effectiveness of our proposed approach.},
  archive      = {J_TMM},
  author       = {Qing Wang and Yajian Wang and Hang Chen and Shuxian Wang and Jun Du and Chin-Hui Lee},
  doi          = {10.1109/TMM.2025.3595019},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video segmentation and tokenization for model-based video scene classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable graph-guided transformer for point cloud geometry coding. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3598605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention models, particularly Transformers, have significantly advanced deep learning in fields like natural language processing and computer vision by capturing contextual relationships in both sequential and spatial data. This ability is valuable for Point Clouds (PC), which are unstructured sets of points in 3D space. Transformers can effectively identify correlations between distant points, allowing them to focus on the most critical regions of the data. To demonstrate this capability, this paper proposes a novel, scalable Graph-Guided Transformer model, labeled 2GFormer, for static PC geometry. This model is built using a scalable architecture that leverages Graph Convolutions to enhance a Relational Neighborhood SelfAttention (RNSA) base layer model. Both models are integrated into the JPEG Pleno Learning-based Point Cloud Coding (JPEG PCC) standard, resulting in the creation of two attention-enabled codecs for static PC coding: JPEG RNSA and JPEG 2GFormer. While JPEG RNSA codec delivers significant compression improvements for solid and dense PCs compared to the baseline JPEG PCC standard, JPEG 2GFormer extends these gains to solid, dense, and sparse PCs with only a marginal increase in model parameters. Additionally, JPEG 2GFormer outperforms both conventional and learning-based state-of-the-art PC codecs. These results position JPEG 2GFormer as a highly efficient solution for versatile PC coding.},
  archive      = {J_TMM},
  author       = {Mohammadreza Ghafari and André F. R. Guarda and Nuno M. M. Rodrigues and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3598605},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scalable graph-guided transformer for point cloud geometry coding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion-mamba for cross-modality object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality object detection aims to fuse complementary information from different modalities to improve model performance, which achieves a wider range of applications. However, traditional cross-modality fusion methods, based on CNN or Transformer, inadequately address the issue of pseudo-target information, which causes model attention dispersion to degrade object detection performance. In this paper, we investigate a novel cross-modality fusion approach by associating cross-modal features in a hidden state space based on an improved Mamba with a gating attention mechanism. We propose the Fusion-Mamba Block (FMB), designed to map cross-modal features into a hidden state space for interaction, thereby refining the model's attention on true target areas and enhancing overall performance. The FMB comprises two key modules: State Space Channel Swapping (SSCS) module, which facilitates the fusion of shallow features, and Dual State Space Fusion (DSSF) module, which enables deep fusion and effectively suppresses pseudo-target information within the hidden state space. Our proposed method outperforms state-of-the-art approaches, achieving improvements of 5.9%, 3.5% and 2.1% mAP on $M^{3}$FD, DroneVehicle and FLIR-Aligned, respectively. To the best of our knowledge, this work establishes a new baseline for cross-modality object detection, providing a robust foundation for future research in this area.},
  archive      = {J_TMM},
  author       = {Wenhao Dong and Haodong Zhu and Shaohui Lin and Xiaoyan Luo and Yunhang Shen and Guodong Guo and Baochang Zhang},
  doi          = {10.1109/TMM.2025.3599020},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fusion-mamba for cross-modality object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-frame spatiotemporal feature and hierarchical learning approach for no-reference screen content video quality assessment. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid adoption of remote work, online conferencing, and shared-screen collaboration has significantly increased the usage of screen content videos (SCVs), creating a growing need for reliable quality assessment to maintain excellent quality of service. While several full-reference SCV quality assessment (SCVQA) methods have been proposed, their practical application is often limited by the unavailability of reference videos. Existing no-reference SCVQA (NR-SCVQA) methods rely on handcrafted features and focus solely on specific distortions and features, potentially limiting their generalization ability. Moreover, they fail to explore the underlying spatiotemporal information of SCVs, which could hinder their performance. In this work, we propose a novel deep learning-based NR-SCVQA model specifically tailored to capture the comprehensive spatiotemporal features of SCVs to overcome these issues and challenges posed by the SCVQA task. Our approach incorporates a dual-channel spatiotemporal convolutional neural network (DCST-CNN) module to extract both content-aware and edge-aware spatiotemporal quality features, which enables an effective spatiotemporal quality feature representation learning for the downstream SCVQA task. Building upon the DCST-CNN, we further propose a Temporal Pyramid Transformer (TPT) module to fuse spatiotemporal features across multiple temporal scales, enabling the model to capture both short-term and long-term temporal dependencies within an SCV for hierarchical learning. The proposed DCST-CNN and TPT modules work together to provide a robust and accurate NR-SCVQA framework. We conduct experiments on SCVQA databases to validate the effectiveness of our model, which outperforms existing state-of-the-art NR-SCVQA method. The results demonstrate the strength and applicability of our approach in real-world SCVQA tasks.},
  archive      = {J_TMM},
  author       = {Ngai-Wing Kwong and Yui-Lam Chan and Sik-Ho Tsang and Ziyin Huang and Kin-Man Lam},
  doi          = {10.1109/TMM.2025.3599071},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-frame spatiotemporal feature and hierarchical learning approach for no-reference screen content video quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-guided vision-language learning for long-term action quality assessment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term action quality assessment poses a challenging visual task since it requires assessing technical actions at different skill levels in a long video. Recent state-of-the-art methods incorporate additional modality information to aid in understanding action semantics, which incurs extra annotation costs and imposes higher constraints on action scenes and datasets. To address this issue, we propose a Quality-Guided Vision-Language Learning (QGVL) method to map visual features into appropriate fine-grained intervals of quality scores. Specifically, we use a set of quality-related textual prompts as quality prototypes to guide the discrimination and aggregation of specific visual actions. To avoid fuzzy rule mapping, we further propose a progressive semantic learning strategy with a Granularity-Adaptive Semantic Learning Module (GSLM) that refines accurate score intervals from coarse to fine at clip, grade, and score levels. The quality-related semantics we designed are universal to all types of action scenarios without any additional annotations. Extensive experiments show that our approach outperforms previous work by a significant margin and establishes new state-of-the-art on four public AQA benchmarks: Rhythmic Gymnastics, Fis-V, FS1000, and FineFS.},
  archive      = {J_TMM},
  author       = {Huangbiao Xu and Huanqi Wu and Xiao Ke and Yuezhou Li and Rui Xu and Wenzhong Guo},
  doi          = {10.1109/TMM.2025.3599078},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality-guided vision-language learning for long-term action quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coupling and decoupling: Towards temporal feedback for 3D object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection has garnered significant attention within the academic community, primarily due to its broad utility in domains such as autonomous driving and robotics. Prior research efforts have predominantly concentrated on leveraging temporal contextual information embedded within sequential data to enhance the current feature representations. However, a notable limitation of these endeavors lies in their inadequate treatment of the inherent noise present within historical sequences, thereby constraining the efficiency of fusion methods. In this paper, we propose a new temporal feedback network, named TFNet, to model and correct the temporal noise by designing a coupling-decoupling mechanism. Central to our approach are two distinct modules: (i) Foreground Feature Enhancement, which amplifies sparse instance details across temporal frames, thereby furnishing essential local information priors for subsequent fusion; and (ii) Coupling-Decoupling Feature Interaction, designed to first aggregate temporal contextual information and then disentangle fusion features into frame-specific representations. Leveraging a feedback strategy, this module can adaptively enhance useful information and eliminate noise within individual frame features. Empirical evaluations conducted on the nuScenes benchmark demonstrate the effectiveness of TFNet, achieving the new state-of-the-art performance without any bells and whistles.},
  archive      = {J_TMM},
  author       = {Yubo Cui and Zhikang Zou and Xiaoqing Ye and Xiao Tan and Zhiheng Li and Zheng Fang},
  doi          = {10.1109/TMM.2025.3599031},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Coupling and decoupling: Towards temporal feedback for 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oriented-derivative representation for boundary-aware polyp segmentation. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3599039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of colon polyps is important for the prevention of colorectal cancer. Polyp segmentation, however, is still a challenging problem given that recent medical computer-aided equipment suffers from situations of polyp variations in terms of size, color, texture, and poor illuminations in endoscopy videos. These obstacles hinder the prediction of polyp boundaries. Inspired by the observation that the values of pixels on the border region change more sharply than others, we propose the oriented-derivative (OD) representation to capture the relationship between pixels and the boundary region given distance and orientation. To adaptively use the proposed representation in arbitrary frameworks, we design plug-in modules to learn the representation and aggregate features to improve the accuracy of boundary predictions in the polyp segmentation task, which can be implemented in frameworks including the encoder-decoder and top-down architectures. Extensive experimental results show the improvement from the proposed oriented-derivative representation for the polyp segmentation task and the extendibility of our proposed modules in different architectures. Our methods achieved an improvement ranging from 0.3% to 2.5% (mDice) compared with the baseline on five publicly available datasets, including Kvasir, CVC-ClinicDB, EndoScene, CVC-ColonDB, and ETIS.},
  archive      = {J_TMM},
  author       = {Yuke Li and Mengjun Cheng and Xiawu Zheng and Rongrong Ji and Jie Chen},
  doi          = {10.1109/TMM.2025.3599039},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oriented-derivative representation for boundary-aware polyp segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degradation-equivariant representations for robust feature detection and description in low-light environments. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint detection and matching have garnered significant attention, yet remain challenging in low-light environments. Most current studies follow an enhance-then-detect pipeline, which consists of independent enhancers and detectors. While the enhancer focuses on improving the visual quality of low-light images to satisfy human perception standards, the detector prioritizes detection accuracy for machine vision tasks. The unaligned optimization objectives of the enhancer and detector overlook the gap between human and machine vision and lead to sub-optimal performance in low-light keypoint detection. To tackle this problem, a joint enhance-and-detect pipeline is proposed to unify the optimization objectives of enhancement and detection by regarding the improvement of keypoint detection accuracy with enhanced features under machine vision standards. Specifically, we propose a low-light keypoint detection network named DeRFeat, which learns a degradation-equivariant representation between normal and dark domains using AutoEncoding transformation and domain descriptor similarity constraints to indirectly enhance the features from the encoder in the training stage. Then, DeRFeat guides the shared encoder to obtain the degradation-equivariant representations from dark images in the inference stage. With the dark degradation predictions, the encoder is capable of generating equivariant representations between normal and dark domains. The proposed domain descriptor similarity module further aids the encoder in mitigating the impact of dark degradation factors, enabling local descriptors to acquire undisturbed representations. Moreover, a coarse-to-fine point selection strategy is proposed to provide reliable prior keypoints for a globally optimal descriptor construction. Experimental results on four benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art methods under varying low-light conditions.},
  archive      = {J_TMM},
  author       = {Fan Wang and Hengye Lyu and Guanyu Xing and Yanci Zhang and Yanli Liu},
  doi          = {10.1109/TMM.2025.3599075},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Degradation-equivariant representations for robust feature detection and description in low-light environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion-induced saliency shifts in video. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual saliency modelling is of fundamental importance in modern video processing and its applications. Our previous eye-tracking study revealed that signal distortions caused by editing, compression, or transmission alter gaze patterns and consequently induce saliency shifts in both spatial and temporal domains. Saliency shifts provide crucial insights into viewers' behavioural responses to video distortions, facilitating the perception-based optimisation of video algorithms. However, the spatio-temporal saliency shifts and their measurable effects on perception related applications remain largely unexplored. In this paper, we first investigate the measurement of distortion-induced saliency shifts (DSS) in videos and analyse DSS behaviours as functions of video content, time order and critical distortion disruption. Second, based on our findings, we construct three vision models to quantitatively simulate distinct DSS behaviours and integrate them into a comprehensive DSS behaviour model. Finally, we demonstrate that the computational DSS model can enhance emerging video technologies.},
  archive      = {J_TMM},
  author       = {Xinbo Wu and Jianxun Lou and Zhengyan Dong and Fan Zhang and Paul Rosin and Hantao Liu},
  doi          = {10.1109/TMM.2025.3599087},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distortion-induced saliency shifts in video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COFNet: Contrastive object-aware fusion using box-level masks for multispectral object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral object detection, which combines RGB visible light and thermal infrared spectral information, has broad applications in complex environments and varying illumination conditions. However, existing methods face challenges in processing multispectral data, such as inconspicuous object features in spectral images and significant discrepancies between input modality spaces and output detection spaces. To address these issues, we propose an innovative multispectral object detection method that combines contrastive learning and a new cross-modal feature fusion module. We introduce a mask feature contrastive loss that maximizes the similarity between the box-level mask features and modal features while suppressing background responses, enabling effective representative alignment between the input and output spaces. Additionally, we propose a mask-guided attention fusion module that uses a predicted pseudo mask to guide the fusion of different modal features, enhancing object responses and reducing background noise interference. Our extensive experiments on several challenging multispectral datasets demonstrate that our proposed COFNet achieves state-of-the-art performance. Our code is publicly available at https://github.com/li554/COFNet.},
  archive      = {J_TMM},
  author       = {Mingliang Zhou and Yunyao Li and Guangchao Yang and Xuekai Wei and Huayan Pu and Jun Luo and Weijia Jia},
  doi          = {10.1109/TMM.2025.3599097},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {COFNet: Contrastive object-aware fusion using box-level masks for multispectral object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dictionary based generative adversarial network for multi-collection style transfer. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most collection-based style transfer methods require training a separate model for each individual collection of styles, making the extension to multiple collections of styles less flexible. Besides, the existing collection-based methods are also less flexible in extending to new style collections in a continual manner. To address these issues, we propose a novel MultI-Dictionary Generative Adversarial Network framework (MID-GAN) for multi-collection style transfer. Specifically, we design a multi-dictionary architecture within a GAN, with each dictionary consisting of a set of local style codes for a specific style collection. Benefiting from the local style codes used in the dictionary, a stylization module with aligned skip connections is further proposed, which can better preserve both the local details and the overall image structure. The dictionary design allows a flexible extension to new style collections by readily adding new dictionaries and we propose a continual training strategy that can both preserve the style transfer ability of old styles and achieve good transfer results for newly added styles. Extensive experiments are performed to show that the proposed method is better than existing collection-based style transfer methods. We also demonstrate the proposed method can generate diverse meaningful style transfer results of the same style collection.},
  archive      = {J_TMM},
  author       = {Jing Huo and Shiyin Jin and Jiashen Li and Pingzhuo Tian and Wenbin Li and Jing Wu and Yu-Kun Lai and Yang Gao},
  doi          = {10.1109/TMM.2025.3599024},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dictionary based generative adversarial network for multi-collection style transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DALFace: Dynamic association learning for face recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition owes its success to the availability of large-scale training data. Recent adaptive margin-based loss functions pay more attention to hard (misclassified) samples, resulting in more discriminative face embeddings. However, large-scale datasets inevitably include open-set noise samples, which are usually mistaken for hard samples by mining-based methods and thus mislead the training of the model. In this work, we redefine hard samples and further design a dynamic association learning strategy for mining hard samples while ignoring noise. We argue that the difficulty of recognizing a sample depends on both identity-related and objective factors. On one hand, intrinsic attributes such as facial structure and face shape inherently influence the ease of identity recognition. On the other hand, external factors, including pose, occlusion, and resolution, directly affect the recognizability of a sample. Particularly in the case of noise samples, although they pose challenges for the deep network similar to hard samples, should not be regarded as hard samples. To this end, we propose an associated prototype learning method to achieve an approximation of face identity difficulty by exploring the fitting trends of identity prototype. Furthermore, we design a dynamic sample learning method to distinguish noise samples from hard samples by observing the distance fluctuation from the class center during sample learning. All observations are integrated into the loss function through adaptive margins and sample weights. Extensive experiments and visualizations on several datasets demonstrate that our method significantly outperforms state-of-the-art counterparts.},
  archive      = {J_TMM},
  author       = {Baojin Huang and Guangcheng Wang and Kui Jiang and Zhongyuan Wang},
  doi          = {10.1109/TMM.2025.3599040},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DALFace: Dynamic association learning for face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual-language multi-task blind image quality assessment with local quality weighting. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of blind image quality assessment (BIQA) is to develop a model capable of automatically evaluating image quality without requiring any reference knowledge. While multi-task learning has been widely utilized in BIQA, it has predominantly remained unimodal. This paper delves into the Visual-Language multi-task BIQA model, where distortion knowledge can be captured through image-text contrastive learning. Specifically, Visual-Language auxiliary tasks targeting distortion type and quality level are introduced, respectively, where both positive and negative image-text pairs are constructed for the target distorted image. Subsequently, image-text correspondences are learned in the embedding space while simultaneously evaluating image quality. Notably, in the auxiliary task learning, the proposed method not only brings the image and its corresponding positive text prompt closer but also pushes away the image from its negative text prompts, thereby facilitating the extraction of pertinent distortion features. In the quality assessment task, a patch-wise strategy is employed during the training phase. Differing from conventional BIQA methods, a novel NSS-guided quality weighting is introduced to gauge the correlation between patch quality and global quality, thereby enabling precise quality prediction. Extensive experiments are conducted on six IQA datasets, and the experimental results verify the superiority of the proposed method.},
  archive      = {J_TMM},
  author       = {Jili Xia and Lihuo He and Bo Hu and Leida Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3599072},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual-language multi-task blind image quality assessment with local quality weighting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised 3D visual grounding based on visual language alignment. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Language Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.},
  archive      = {J_TMM},
  author       = {Xiaoxu Xu and Yitian Yuan and Qiudan Zhang and Wenhui Wu and Zequn Jie and Lin Ma and Xu Wang},
  doi          = {10.1109/TMM.2025.3599032},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly-supervised 3D visual grounding based on visual language alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating hallucinations in large vision-language models via reasoning uncertainty-guided refinement. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite demonstrating impressive capabilities in comprehending multi-modal contexts, large vision-language models (LVLMs) are invariably prone to generate unreliable answers, i.e., hallucinations. Existing methods mainly mitigate this hallucination by introducing specific designed datasets or employing contrastive decoding techniques. However, these methods heavily rely on the quality of constructed datasets and negative samples, overlooking the inherent ambiguity in reasoning caused by over-reliance on linguistic priors and data complexity, termed reasoning uncertainty. This oversight hinders the models from effectively identifying the causal relationships behind each token, increasing their susceptibility to hallucinations. To address this issue, we propose a novel framework named Reasoning Uncertainty-guided Refinement (RUR) for mitigating hallucinations in LVLMs from an uncertainty perspective. Specifically, unlike conventional uncertainty quantification methods, we first extract the causal reasoning relationships between tokens by exploiting the link between structural causal models and the Transformer architecture. Based on this relationship, we then employ the Subjective Logic principle to model the reasoning uncertainty at both token and sentence levels, which reflects the unreliability degree of generated tokens and sentences. Finally, guided by reasoning uncertainty, we develop multi-level uncertainty-based adjustment to eliminate deceptive tokens exhibiting severe uncertainty and mitigate potential hallucinations in sentences. Extensive experiments demonstrate that our RUR method consistently achieves state-of-the-art performance on five benchmarks. Our code is available at https://github.com/Mrshenshen/RUR.},
  archive      = {J_TMM},
  author       = {Shenshen Li and Xing Xu and Wenxin Meng and Jingkuan Song and Chong Peng and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3599076},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mitigating hallucinations in large vision-language models via reasoning uncertainty-guided refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OAFTracker: One-stage associative multiple object tracking with fine-grained orthogonal representation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking based on the tracking-by-detection paradigm relies on appearance information and motion information for trajectory association. Employing global re-identification features and two-stage association strategies can improve the utilization of both types of information for detections with different confidence scores. However, when targets are occluded, coarse-grained global representations can lead to false positive detections. Additionally, two-stage association strategies tend to prioritize matching high-confidence detections over more accurate low-confidence detections, leading to identity switch problems. To address these issues, we propose the OAFTracker framework, which focuses on local representations and a one-stage association strategy. Firstly, a Fine-grained Representation Orthogonal Fusion (FROF) network is designed to adaptively integrate local and global representations. Secondly, we propose a One-stage Association Matching (OAM) strategy. This strategy combines multiple distance constraints to ensure fairness in matching detections with different confidence scores to predicted trajectories. Additionally, we propose an Adaptive Variable Noise (AVN) Kalman filtering algorithm to dynamically update the state of predicted trajectories. Finally, extensive experiments conducted on two public datasets demonstrate the effectiveness of the OAFTracker method.},
  archive      = {J_TMM},
  author       = {Jialin Liu and Jun Kong and Min Jiang and Xuefeng Tao},
  doi          = {10.1109/TMM.2025.3599069},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OAFTracker: One-stage associative multiple object tracking with fine-grained orthogonal representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artistic style transfer via fine-grained text guidance and contrastive semantics similarity. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the development of text-image multimodal methods, text is used to guide the style transfer of images, which has attracted growing attention. Notably, The existing text-guided image style methods are limited to expressing specific artistic style through simple text. It can only accept coarse-grained text input such as “Van Gogh” and “White Cloud”, and cannot understand fine-grained text input such as “The Night Café by Vincent van Gogh”. To this end, this paper proposes a novel artistic style transfer network based on the fine-grained text guidance and the contrastive semantics similarity, named as TCStyler. It can accept images or texts as style guidance, which is more suitable for fine-grained content understanding stylization. In this network, to address the issue of text-image cross-modal discrepancy, the residual attention feature mapper (RAFM) is introduced to constrain the differences between different modalities in feature space. Then, the global cascading style-sharing module (GCSM) is proposed for performing content-style feature fusion and image-text modality fusion by adopting a global feature-sharing strategy. Furthermore, the contrastive semantics similarity loss is designed to address the problem of multimodal universality. Quantitative and visualization experiments demonstrate that our TCStyler can handle fine-grained artistic text inputs and maintain consistency in the style transfer results guided by different modalities.},
  archive      = {J_TMM},
  author       = {Linfeng Li and Chunmei Qing and Junpeng Tan and Jianxiu Jin and Xiangmin Xu},
  doi          = {10.1109/TMM.2025.3599050},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Artistic style transfer via fine-grained text guidance and contrastive semantics similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised photographic image layout representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image layout representation learning, which converts layouts into compact vectors, is essential for tasks such as image retrieval, editing, and generation. However, existing methods—especially those applied to photographic images—face several challenges: supervised methods rely on expensive labeled datasets, weakly-supervised methods struggle with generalization, and self-supervised methods are limited in handling the diversity of photographic layouts. To address these issues, we propose a novel heterogeneous layout graph that efficiently captures the layout information in images. The vertices of this graph represent the compositional primitives of the image, capturing their attributes, while the edges encode the relationships between these primitives. We also design effective pretext tasks to guide a layout encoder-decoder in self-supervised training, ultimately generating the layout graph embedding vector. Additionally, we introduce a new layout evaluation dataset—LODB—which features a richer variety of layout categories, significantly better label quality than existing datasets, and a more balanced distribution of semantic scenes across layout categories, providing a comprehensive benchmark for evaluation. Experiments on the LODB dataset demonstrate that our method outperforms existing approaches in representing photographic image layouts.},
  archive      = {J_TMM},
  author       = {Zhaoran Zhao and Peng Lu and Xujun Peng and Wenhao Guo},
  doi          = {10.1109/TMM.2025.3599102},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised photographic image layout representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning generalizable contrastive representations for graph zero-shot learning. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of graph zero-shot learning, which aims at recognizing novel classes of nodes on the graph that are never seen during training. The key to graph zero-shot learning is establishing the mathematical relationship to transfer the prior knowledge of nodes from seen classes to unseen classes. However, the problem is largely under-explored and existing methods typically focus on acquiring supervision signals from seen classes or simply establishing connections between classes based solely on a semantic description matrix, such that the learned representations lack generalizable properties to unseen classes. To address this issue, this paper proposes GraphGCR that learns generalizable contrastive representations from the perspective of uniformity and alignment. Technically, GraphGCR leverages graph diffusion to extend supervised contrastive learning, encouraging the representations of semantics from different classes to be distributed uniformly and meanwhile achieve the alignment of node features and class semantics with the assistance of graph structural information. Moreover, to effectively enhance model generalizability, we further develop a class generator to synthesize features of unseen classes by embedding propagation and interpolation, thereby enriching the diversity of classes. Theoretical analysis also shows that our proposed framework exhibits strong discriminative property, which significantly enhances graph zero-shot learning. Experimental findings reveal that our GraphGCR achieves significant performance improvements over state-of-the-art methods across various benchmark datasets.},
  archive      = {J_TMM},
  author       = {Siyu Yi and Zhengyang Mao and Kangjie Zheng and Zhiping Xiao and Ziyue Qiao and Chong Chen and Xian-Sheng Hua and Yongdao Zhou and Ming Zhang and Wei Ju},
  doi          = {10.1109/TMM.2025.3599043},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning generalizable contrastive representations for graph zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation learning and aggregate-attention for multi-person motion prediction. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods achieve impressive predictions using various networks but often overlook the distinct representations of joint relations within individuals (intra-relations) and interactions among groups (inter-relations), inevitably leading to undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations: a GCN-based network for intra-relations and a novel reasoning network for inter-relations. Specifically, we propose a distance-aware cross-attention that incorporates physical distance constraints into inter-relation learning through a learnable distance weighting coefficient. Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9∼15 persons), demonstrate that our method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Kehua Qu and Rui Ding and Jin Tang},
  doi          = {10.1109/TMM.2025.3599049},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relation learning and aggregate-attention for multi-person motion prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SecureDA: Privacy-preserving source-free domain adaptation for person re-identification. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional domain adaptation (DA) for person reidentification (ReID) aims to bridge the domain gap but often requires direct use of fully labeled source and target domains, raising significant data privacy concerns due to the inclusion of personal identity information (PII) in raw data. Source-free domain adaptation (SFDA) for person ReID effectively preserves PII within the authorized source model. Nevertheless, these methods are vulnerable to data privacy (e.g., portrait rights) of the target domain during retrieval, where attackers can exploit pedestrian images for malicious generation, leading to damage to an individual's reputation. Beyond these limitations, we propose a novel framework called SecureDA to address privacy-preserving SFDA for person ReID, which can generate a privacy key to defend against potential attacks on PII. Technically, we introduce domain-specific adversarial attacks into DA, where the protected query and gallery images are encrypted to ensure secure image retrieval. Furthermore, we employ two simultaneous processes: 1) The global–local adversarial pathway (GLAP) leverages encrypted and original images as adversarial pairs, thereby fostering the development of robust ReID models; 2) The global–local collaborative pathway (GLCP) is mastered through positive pairs collected from the same domain, effectively mitigating the pernicious catastrophic forgetting phenomenon. Extensive experiments show that SecureDA achieves state-ofthe-art performance on multiple DA benchmarks and even outperforms the conventional DA and SFDA methods, which inherently compromise data privacy.},
  archive      = {J_TMM},
  author       = {Xiaofeng Qu and Li Liu and Huaxiang Zhang and Lei Zhu and Liqiang Nie and Xiaojun Chang and Fengling Li},
  doi          = {10.1109/TMM.2025.3599094},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SecureDA: Privacy-preserving source-free domain adaptation for person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privileged information-guided multitask mutualistic transformer for gaze prediction. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting where people look is a crucial for understanding human intentions. Gaze prediction, as a research hotspot, has evolved from predicting the gaze of a single person to simultaneously predicting the positions of all individuals and their corresponding gaze targets. However, the study of the correlation between humans and gaze as two interdependent tasks has largely been neglected. In this paper, inspired by the concept of “mutualistic symbiosis” in ecology, we propose a novel multitask mutualistic transformer (MMTR). MMTR captures paired dependencies by establishing information communication between different branches, thereby enabling comprehensive and interpretable gaze analysis for all individuals and gaze targets. Specifically, we first utilize a transformer encoder to capture the common features of all the tasks. Then, we design a mutualistic attention mechanism (MAM) in the dual-branch Transformer decoder to establish cross-task information interaction. The MAM can learn privileged information from other tasks that is helpful for the current task, thereby guiding the current branch to learn the most valuable and distinctive features. To the best of our knowledge, this is the first time that privileged information has been introduced into the gaze estimation task. Furthermore, to more flexibly learn pixel locality and long-range semantic dependencies for different tasks, we construct and embed a learnable global-local position encoding (GLPE) in different branches of MMTR. Experiments demonstrate that our proposed MMTR can guide the two branches to communicate through privileged information, effectively solve the information asymmetry problem between human detection and gaze prediction, and significantly outperform state-of-the-art gaze prediction methods on two standard benchmark datasets GazeFollowing and VideoAttentionTarget.},
  archive      = {J_TMM},
  author       = {Wenhe Chen and Yuan Chai and Xiao-Jun Wu and Hongjin Zhu and Qian Yu and Zhuo-Ming Du and Feilong Han and Wei Gao and Caixia Zheng and Honghui Fan},
  doi          = {10.1109/TMM.2025.3599030},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privileged information-guided multitask mutualistic transformer for gaze prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PalmMamba: Palm intrinsic features learning selective state space model for palmprint image denoising. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint-based biometric recognition has gained widespread attention due to its rich features, contactless acquisition, and low invasiveness. However, most existing methods neglect image quality, making them less effective for low-quality, noisy palmprint images. In this paper, we propose a palm intrinsic features learning selective state space model (PalmMamba) for palmprint image denoising, which consists of shallow feature representation, noise-insensitive palmprint-specific feature learning, and sharp palmprint image restoration modules. First, we convert the degraded noisy palmprint image into a high-dimensional shallow feature representation through a single-layer convolution backbone. Then, we develop parallel learning branches, including a second-order attention-based selective state space model and a mixed difference convolution module, to exploit diverse palmprint-specific features with both global and local details. Finally, we map the fine-grained palmprint-intrinsic feature map into the identity-preserved sharp palmprint image via a commonly used convolution layer. Extensive experimental results on five public palmprint databases demonstrate the encouraging performance of the proposed PalmMamba in palmprint image denoising.},
  archive      = {J_TMM},
  author       = {Zhu Wang and Lunke Fei and Shuping Zhao and Bob Zhang and Qi Zhu and Imad Rida},
  doi          = {10.1109/TMM.2025.3599093},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PalmMamba: Palm intrinsic features learning selective state space model for palmprint image denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial-temporal progressive fusion network for breast lesion segmentation in ultrasound videos. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound video-based breast lesion segmentation provides valuable assistance in early breast lesion detection and discrimination. However, this field faces two key challenges: the first is how to simultaneously utilize both intra-frame and inter-frame lesion cues to accurately segment breast lesions, and the second is that the availability of breast ultrasound video datasets is quite limited. In this paper, we propose a novel Spatial-Temporal Progressive Fusion Network (STPFNet) for video-based breast lesion segmentation problem. The proposed STPFNet comprises three main components. First, we propose to adopt a unified network architecture to capture spatial dependencies within each ultrasound frame and temporal correlations between different frames together for feature representation of ultrasound video. Second, we propose a new fusion module called Multi-Granularity Feature Fusion (MGFF) to fuse the extracted information with different granularities for lesion segmentation. MGFF can help improve the issue of lesion boundary blurring. Third, we propose to take the segmentation result of the previous frame as prior knowledge to suppress the noisy background and learn a more robust representation. To further promote the research in this field, we construct a new ultrasound video breast lesion segmentation dataset, called UVBLS200, comprising 200 videos (80 benign and 120 malignant lesions). Experiments on the proposed dataset demonstrate that the proposed STPFNet achieves a better breast lesion detection performance than state-of-the-art methods. The code is available at https://github.com/zzgzzgz/STPF-Net.},
  archive      = {J_TMM},
  author       = {Zhengzheng Tu and Zigang Zhu and Yayang Duan and Bo Jiang and Qishun Wang and Chaoxue Zhang},
  doi          = {10.1109/TMM.2025.3599028},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A spatial-temporal progressive fusion network for breast lesion segmentation in ultrasound videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RA-BLIP: Multimodal adaptive retrieval-augmented bootstrapping language-image pre-training. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have recently received substantial interest, which shows their emerging potential as general-purpose models for various vision-language tasks. MLLMs involve significant external knowledge within their parameters; however, it is challenging to continually update these models with the latest knowledge, which involves huge computational costs and poor interpretability. Retrieval augmentation techniques have proven to be effective plugins for both LLMs and MLLMs. In this study, we propose multimodal adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training (RA-BLIP), a novel retrieval-augmented framework for various MLLMs. We first leverage the question to instruct the extraction of visual information through interactions with one set of learnable queries, minimizing irrelevant interference and redundancy during retrieval and generation. Besides, we introduce a pre-trained multimodal adaptive fusion module to achieve question text-to-multimodal retrieval and integration of multimodal knowledge by projecting visual and language modalities into a unified semantic space. Furthermore, we present an Adaptive Selection Knowledge Generation (ASKG) strategy to train the generator to autonomously discern the relevance of retrieved knowledge, which realizes excellent denoising performance. Extensive experiments on open multimodal question-answering datasets demonstrate that RA-BLIP achieves significant performance and surpasses the state-of-the-art retrieval-augmented models.},
  archive      = {J_TMM},
  author       = {Muhe Ding and Yang Ma and Pengda Qin and Jianlong Wu and Yuhong Li and Liqiang Nie},
  doi          = {10.1109/TMM.2025.3599070},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RA-BLIP: Multimodal adaptive retrieval-augmented bootstrapping language-image pre-training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CASIA-PR-v1: A multi-ethnic, multi-device and cross-spectral dataset and a multiscale disentangled model for periocular recognition. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periocular recognition is regarded as an alternative trait for biometric recognition that can effectively solve the identification problem under large occlusions. However, few datasets are tailored for periocular recognition. For most compromises, iris datasets at near-infrared wavelengths, miss information about the eyebrows or eyelids. In this paper, a challenging dataset for real scenarios named CASIA-PR-V1 with evaluation protocols is released for periocular recognition. It is collected from multiple types of mobile devices with different resolutions or wavelengths. A rich set of attributes, e.g., ethnicities, is tagged to support fine-grained classification tasks. Moreover, we consider a wide range of noisy data in unconstrained environment, especially for glasses. Superior to its counterparts, this periocular dataset is highly valuable for studying cross-device and cross-spectral periocular recognition with occlusions, as well as fine-grained attribute classification. Additionally, a multiscale disentangled model is proposed to extract discriminating representations for periocular recognition with severe occlusions. Extensive experiments are conducted on CASIA-PR-V1, and the results indicate the superiority of our model for unconstraint periocular recognition. Please visit http://biometrics.idealtest.org for more details about our dataset.},
  archive      = {J_TMM},
  author       = {Wanting Zhou and Yiwei Ru and Yushan Han and Longteng Kong and Zijian Wang and Yong He and Zhenan Sun},
  doi          = {10.1109/TMM.2025.3599084},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CASIA-PR-v1: A multi-ethnic, multi-device and cross-spectral dataset and a multiscale disentangled model for periocular recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of prompt learning via multi-knowledge representation for vision-language models. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage the potential of VLMs in adapting to downstream tasks, context optimization methods such as prompt tuning are essential. However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules. This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks. To address this challenge, we propose context optimization with multi-knowledge representation (CoKnow), a framework that enhances prompt learning for VLMs with rich contextual knowledge. To facilitate CoKnow during inference, we train lightweight semantic knowledge mappers, which are capable of generating multi-knowledge representations for an input image without requiring additional priors. Experimentally, we conduct extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods.},
  archive      = {J_TMM},
  author       = {Enming Zhang and Bingke Zhu and Yingying Chen and Qinghai Miao and Ming Tang and Jinqiao Wang},
  doi          = {10.1109/TMM.2025.3599096},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimization of prompt learning via multi-knowledge representation for vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology learning for two-view correspondence filtering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel neural network called Topology Learning Network (TL-Net), that exploits local and global geometric relation by topology graphs to handle the problem of correspondence filtering in complex scenes. Specifically, we first design a Multi-level Topology Encoder (MLTE), which fuses local and global topology graphs by a channel attention, to sufficiently extract the geometric relation among correspondences. MLTE not only includes local topology graphs by gathering the information of relative motion and multi-resolution group convolution, but also includes a global topology graph by aggregating the information of the similarity and the Graph Laplacian. In addition, inspired by Transformer, we design the backbone of TL-Net to generate enriched feature maps for correspondence filtering. Meanwhile, by simplifying the global context aggregation, we maintain the lightweight of the backbone, introducing the superiority of Transformer while avoiding extra parameters and calculations. Empirical experiments on several computer vision tasks show that the performance and generalization ability of TL-Net are significantly superior to the state of the art methods. Notably, on relative pose estimation, we achieve $5.63\%$ and $5.03\%$ mAP improvements under an error threshold of $5^{\circ }$ outdoors and indoors, respectively. The code is available at https://github.com/guobaoxiao/TLNet.},
  archive      = {J_TMM},
  author       = {Ziwei Shi and Xiangyang Miao and Guobao Xiao and Songlin Du and Zheng Wang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3599033},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Topology learning for two-view correspondence filtering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving back-projection accuracy for the semantic segmentation of indoor point clouds with fewer & sparse image annotations. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing semantic segmentation on point clouds is the primary method by which machines perceive 3D scenes in a fine-grained manner. Deep learning algorithms usually require many pointwise annotations obtained with specialized tools, which is a laborious and inefficient process. To this end, we develop two frameworks for training point cloud semantic segmentation networks, one that utilizes fewer projected image annotations and another that employs sparse scribble image annotations, making the process more flexible and user friendly. However, back-projecting 2D-pixel labels to 3D points during loss calculations always introduces errors. To increase the back-projection accuracy of our approach, we first identify and record potential pixel-point correspondence errors and then develop strategies for constructing an accurate back-projection mapping matrix. Specifically, we filter out occluded and noisy points to avoid incorrect label allocations and permit multiclass assignments to adjust the ambiguity of boundary points. By incorporating an accurate back-projection mechanism into the loss functions of the proposed training frameworks, our networks can perform well with only four projected image annotations or even sparse scribble image annotations for each scene. This results in state-of-the-art performance compared with that of other weakly supervised point cloud semantic segmentation approaches, and the outcomes are even comparable to those produced by fully supervised methods on the S3DIS and ScanNet-v2 datasets.},
  archive      = {J_TMM},
  author       = {Peng Jiang and Haochen Sun and Zhiyi Pan and Jinming Cao and Roger Zimmermann and Changhe Tu},
  doi          = {10.1109/TMM.2025.3599085},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving back-projection accuracy for the semantic segmentation of indoor point clouds with fewer & sparse image annotations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted medical images based on huffman tree coding and count-encryption. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDHEI) has been recognized as an effective method for overcoming management difficulties within picture archiving and communication system (PACS). However, most existing RDHEI algorithms still encounter notable challenges when applied to the PACS, specifically in terms of their key management, embedding capacity, and security. This paper introduces a novel framework and corresponding algorithm for reversible data hiding in encrypted medical images (RDHEMI) to bridge this gap. The framework employs a unique key for each patient and maintains consistency in the key linked to patient images regardless of changes in doctor, thereby addressing key management challenges. In the proposed algorithm, Huffman tree coding (HTC) integrates Huffman coding with innovative leaf-to-leaf coding, achieving a better compression performance for medical images than move-to-front (MTF) cache and Huffman coding, as medical images contain more smooth areas. Count-encryption (CE) produces encryption keys according to the frequency of encryption occurrences for an image and ensures a peak signal-tonoise ratio under 8 dB for multiple encryptions with the same key, enhancing the algorithm's resistance to attacks. The experimental results demonstrate that the proposed algorithm achieves high security to counter various attacks and outperforms existing algorithms in terms of the time complexity and embedding capacity, with an improvement of 0.21 bpp.},
  archive      = {J_TMM},
  author       = {Yaolin Yang and Hongjie He and Fan Chen and Yuan Yuan and Ningxiong Mao and Yang Li and Jun Zhao},
  doi          = {10.1109/TMM.2025.3599036},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reversible data hiding in encrypted medical images based on huffman tree coding and count-encryption},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-type image quality assessment based on multi-region deep feature fusion under meta-learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing image quality assessment methods need to be retrained when dealing with a new type of task. This approach wastes computing resources and time. Therefore, these methods fail to suit the application scenarios that require processing of multi-type image quality assessment tasks. In the human visual system, the eyes of human tend to pay varying degrees of attention to different regions. Inspired by this system, this paper proposes a multi-type image quality assessment method based on multi-region deep feature fusion under meta-learning (MMQA). First, we utilize the differences in the structural information to screen out salient and non-salient regions. Second, a deep multi-stream network is designed to comprehensively consider and fuse different features related to the quality in salient regions, non-salient regions and the entire image. Third, meta-learning is applied to quickly learn and update the parameters of the model when facing new types of images. By summarizing the prior knowledge in the training of one type of task, the model can be quickly fine-tuned for other types of images. The experimental results demonstrate that the proposed method has advantages over the existing methods in generalization and robustness. Furthermore, the proposed method can adapt well to different distortion types and different image types quickly and accurately.},
  archive      = {J_TMM},
  author       = {Shun Zhu and Xichen Yang and Tianshu Wang and Tianhai Chen and Nengxin Li and Xiaobo Shen and Genlin Ji},
  doi          = {10.1109/TMM.2025.3599027},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-type image quality assessment based on multi-region deep feature fusion under meta-learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual fuzzy alignment on hypergraph for open-set 3D cross-modal retrieval. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing 3D cross-modal retrieval (3CMR) methods heavily rely on prior knowledge of training categories, which leads to the problem of modality shift and unseen center deviation when encountering unseen categories under the open-set environment. Aiming at the open-set 3CMR, this paper introduces the Hypergraph-Based Residual Fuzzy Alignment (ReFA) framework, which revisits the open-set retrieval task and navigates uncertainty of it through the lens of Fuzzy Theory. Facing the challenges of boundaryless space caused by uncertain unseen categories, we explore the representation and measurement in the fuzzy membership space as an alternative to fixed close-set category space. Specifically, to address the problem of modality shift caused by unseen categories, we utilize the Residual Sampling Generation (RSG) module to generate modality sampling embeddings that are independent of seen categories under the guidance of fuzzy representation, which residually decouples the entangled interactions of seen categories and modalities. To overcome the problem of unseen center deviation, we propose the Center Fuzzy Alignment (CFA) module to leverage the high-order fuzzy correlations for generalized metric, by constructing a fuzzy hypergraph based on the inherent and fuzzy correlations among both modalities and categories. The comprehensive evaluations of comparison and ablation studies on the four benchmarks demonstrate the superiority of our proposed framework compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yang Xu and Yifan Feng and Xu Zhuang and Jason Wang and Zongze Wu and Yue Gao},
  doi          = {10.1109/TMM.2025.3599081},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Residual fuzzy alignment on hypergraph for open-set 3D cross-modal retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPSR-CLIP: Conditional prompt-induced style reconstruction for zero-shot domain adaptation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the requirement of target domain data in existing unsupervised domain adaptation (UDA) techniques, researchers have shifted their focus to a more practical and challenging scenario, i.e., zero-shot domain adaptation (ZSDA). However, ZSDA remains a significant challenge, with existing approaches in ZSDA often relying heavily on a carefully crafted and highly compatible auxiliary domain. This is impractical in real-world applications. To address the mentioned problems, we propose conditional prompt-induced style reconstruction with contrastive language-image pre-training (CPSR-CLIP), which leverages the rich semantic embedding of CLIP to synthesize target-like features, effectively bypassing the need for auxiliary dual-domain samples. CPSR-CLIP adopts a multi-phase optimization strategy and every optimization phase is a prerequisite for the next phase. Firstly, we propose dynamic prompt disentanglement to facilitate the model in differentiating the discrepancy between the source and target prompts, thus paving the way for conditional prompt-induced style reconstruction phase. This phase meticulously strips away domain-specific styles to reserve domain-invariant features and injects target style characteristics through target domain prompts. Finally, with the target-like features in hand, we adaptively adjust the learnable part of target prompts for further fitting. Extensive experiments have been conducted on several datasets and the results demonstrate the superiority of CPSR-CLIP over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jiayu Qian and Yuwu Lu and Wuyuan Xie and Zhihui Lai and Miaohui Wang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3599044},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CPSR-CLIP: Conditional prompt-induced style reconstruction for zero-shot domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OV-BIS: Open-vocabulary boundary guide zero-shot 3D instance segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open vocabulary 3D instance segmentation aims to align 3D instance segmentation results with natural language text, thereby achieving semantic prediction without relying on predefined class labels for specific scenes, which has been widely used in the field of multimedia. Current open vocabulary 3D instance segmentation methods mainly rely on 2D masks provided by various 2D segmentation foundation models. However, in complex scenes, the calculation of 2D masks often struggles to balance over-segmentation of large objects and under-segmentation of small objects. In this paper, we introduce OV-BIS, a novel zero-shot open vocabulary 3D instance segmentation method that leverages instance boundary information to improve 3D semantic segmentation performance. The key insight of our method is that the edge map as 3D boundary projection is suitable for multi-scale tasks and capable of compensating for the weakness of 2D masks in multi-scale adaptability for complex scenes. Our method aggregates multiview edge maps and 2D masks, iteratively guiding the merging of over-segmented point clouds with regions growing to cluster 3D primitives into distinct 3D instances. By projecting 3D instances onto images and using CLIP to calculate semantic features from multiple perspectives with an outliers filter, 3D semantic instance segmentation has been achieved. Experiments on multiple datasets demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Tinghao Yi and Shaohu Wang and Zhengtao Zhang and Changwei Wang and Dongming Yan and Rongtao Xu and Enhong Chen},
  doi          = {10.1109/TMM.2025.3599047},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OV-BIS: Open-vocabulary boundary guide zero-shot 3D instance segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal complementary learning and template-based reasoning chains for future event prediction in videos. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multi-modal large language models (MLLMs) have impressive cross-modal reasoning and prediction capabilities, a unified and rigorous evaluation standard is still lacking. In this paper, we propose a future event prediction task to evaluate their cross-modal temporal prediction capability. This task requires the model to generate descriptions of events that may occur in future based on the input premise video. We build a dataset on the existing datasets for model evaluation. This task faces many challenges, including the complexity of processing video data, such as understanding changes in objects, actions, and time dimensions within the video and the interference of redundant information. To address these challenges, we propose a novel cross-modal prediction framework that introduces cross-modal supplementary learning and template-based reasoning chains based on MLLMs. Cross-modal supplementary learning aims to promote visual and text information to supplement and mine their respective information, primarily to capture critical information in videos, relying on the adaptive temporal filter and casual Q-Former. The template-based reasoning chain drives GPT-4 to generate a series of template question pairs through design prompts, gradually guiding the model to perform hierarchical reasoning to support the final prediction. Through experimental evaluation, the performance of the current MLLMs may not meet the requirements, and our model outperforms all existing models in predicting future events. It shows that the capabilities of MLLMs can be further explored.},
  archive      = {J_TMM},
  author       = {Chenghang Lai and Weifeng Ge and Xiangyang Xue},
  doi          = {10.1109/TMM.2025.3599038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal complementary learning and template-based reasoning chains for future event prediction in videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multi-feature relationship in retinex decomposition for low-light image enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent advancements in deep learning techniques, existing unsupervised low-light image enhancement methods fail to improve global brightness and restore colour due to the lack of high-quality training targets. Moreover, real-world low-light images inevitably contain noise, which significantly reduces image visibility and quality, further complicating the enhancement process. However, current unsupervised approaches tend to oversimplify or ignore the noise in low-light images. To address these issues, we first revise the traditional Retinex decomposition to better integrate with unsupervised deep learning frameworks. Then, we design a Local and Global Illumination-Guided Network for removing corruption from the reflectance component, which improves enhancement quality by not only investigating multi-feature similarity and attention mechanism based on the Retinex theory but also leveraging local details and long-range dependencies. Furthermore, by analysing the attributes of corruption within the reflectance component, we introduce a novel reflectance enhancement loss to effectively remove noise without using ground truth. The code is available at: https://github.com/RuoyuGuo/ErcRetinex.},
  archive      = {J_TMM},
  author       = {Ruoyu Guo and Maurice Pagnucco and Yang Song},
  doi          = {10.1109/TMM.2025.3599099},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring multi-feature relationship in retinex decomposition for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature quality assessment: A database and a lightweight objective method. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Artificial Intelligence, visual data gathered by edge devices could be primarily utilized for machine vision tasks. The prominent coding frameworks accomplish this by extracting and compressing features extracted from input data. As such, the quality of these features is vital, as they reflect the performance of the coding framework. However, much less work has been dedicated to quality assessment on features, impeding the optimization of the coding system. In this work, we pioneer to explore the feature quality assessment by creating a novel database tailored for features, with the quality ground-truth for each feature. Then, we propose a lightweight feature quality assessment method, called Lightweight Feature Quality Assessment (LFQA). We analyze the feature characteristics from the perspective of spatial and channel thoroughly, and the framework of LFQA is designed based on the analysis results. Experimental results demonstrate that LFQA accurately evaluates the quality of features, reaching a notable Spearman Rank-Order Correlation Coefficient of 85.38%, and exhibits competitive performance in improving the performance of video coding for machine system. Furthermore, LFQA has fewer model parameters and faster inference speed, ensuring a wide range of promising applications.},
  archive      = {J_TMM},
  author       = {Shipei Wang and Ping An and Chao Yang and Gongyang Li and Xinpeng Huang and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3599090},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Feature quality assessment: A database and a lightweight objective method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCInet: Fusing low-light visible-infrared image via max-merge complementary information. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing complementary information in the visible-infrared image offers a promising approach to enhance the performance of downstream computer vision tasks (e.g., object detection, segmentation etc) in complicated imaging conditions (e.g., low-illumination). However, due to the robust imaging capacity of the infrared sensor in complicated imaging conditions, most existing methods primarily rely on the salient object intensity information in the infrared modality for fusion, while the visible information (e.g., color, texture etc) is not adequately utilized, and thus limit their generalization capacity in downstream computer vision tasks. In this study, we present a novel image fusion framework, i.e., MCInet, which attempts to Maximize and merge the Complementary Information across visible-infrared modalities for more informative image fusion. To this end, we first introduce the modality-specific processing module into the fusion framework to improve the information representation of each modality image. For visible images, a pre-trained low-light enhance module is adopted to enhance its color and texture information. In addition, for infrared images, a nonlinear mapping module is constructed to suppress the excessive salient object intensity information of infrared modality. Then we establish a reusable MCI block that embeds a cross-image mutual information minimization scheme into an input-aware fusion module. This empowers us to dynamically maximize and merge the complementary information between two input images according to their feature representation. In addition, we introduce a cycle reconstruction loss to self-supervised regularize the fusion results for further enhancement. Experiments on image fusion, object detection, and segmentation tasks demonstrate that the proposed framework can produce more informative fusion results and exhibit better performance in downstream computer vision tasks.},
  archive      = {J_TMM},
  author       = {Jiangtao Nie and Boxiong Wu and Wei Wei and Lei Zhang and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3599042},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MCInet: Fusing low-light visible-infrared image via max-merge complementary information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-guided video frame interpolation with spatial-temporal global attention. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation technology improves visual experience with the development of deep learning. However, capturing large motions while synthesizing fine texture details remains a challenging task. Regarding large motion scenarios, some pioneering Transformer-based methods primarily rely on local attention, which does not fully leverage the global receptive field advantage. To address this issue, this paper proposes to further broaden the receptive field of the Transformer to capture more correlations in the video frame interpolation task. Specifically, we propose a global self-attention mechanism in the form of spatial-temporal separation. Regarding texture details, since roughly enlarging the receptive field results in the loss of details, we propose to use large motion information in both feature and pixel spaces as a dual-guided prior to enhance detail synthesis. The separable attention mechanism and the straightforward frame synthesis design significantly enhance the resource efficiency of our model. Extensive experiments show that our method achieves state-of-the-art performance, effectively capturing large motions and preserving texture details.},
  archive      = {J_TMM},
  author       = {Baojun Zhou and Xinpeng Huang and Gongyang Li and Chao Yang and Liquan Shen and Ping An},
  doi          = {10.1109/TMM.2025.3599095},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-guided video frame interpolation with spatial-temporal global attention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale up composed image retrieval learning via modification text generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed Image Retrieval (CIR) aims to search an image of interest using a combination of a reference image and modification text as the query. Despite recent advancements, this task remains challenging due to limited training data and laborious triplet annotation processes. To address this issue, this paper proposes to synthesize the training triplets to augment the training resource for the CIR problem. Specifically, we commence by training a modification text generator exploiting large-scale multimodal models and scale up the CIR learning throughout both the pretraining and fine-tuning stages. During pretraining, we leverage the trained generator to directly create Modification Text-oriented Synthetic Triplets (MTST) conditioned on pairs of images. For fine-tuning, we first synthesize reverse modification text to connect the target image back to the reference image. Subsequently, we devise a two-hop alignment strategy to incrementally close the semantic gap between the multimodal pair and the target image. We initially learn an implicit prototype utilizing both the original triplet and its reversed version in a cycle manner, followed by combining the implicit prototype feature with the modification text to facilitate accurate alignment with the target image. Extensive experiments validate the efficacy of the generated triplets and confirm that our proposed methodology attains competitive recall on both the CIRR and FashionIQ benchmarks. Codes and datasets will be made publicly accessible.},
  archive      = {J_TMM},
  author       = {Yinan Zhou and Yaxiong Wang and Haokun Lin and Chen Ma and Li Zhu and Zhedong Zheng},
  doi          = {10.1109/TMM.2025.3599088},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scale up composed image retrieval learning via modification text generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilevel direction preserving for few-shot open-set recognition. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3599083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot open-set recognition (FSOSR) poses a significant challenge as it requires identifying unknown classes while maintaining the classification performance of known classes, despite having limited access to labeled training samples. Current methods often employ non-directional metric-based losses to encapsulate feature attributes within the embedding space, inadvertently disregarding the potential influence of spatial distribution deviations of feature representations on open-set recognition performance. To address this, we present a novel directional metric-based method termed Bilevel Direction Preserving (BiDirP). This method incorporates two direction-preserving regularizers operating at distinct levels, specifically at the instance and prototype levels. The combined application of these two direction-preserving regularizers effectively enhances the spatial separation between prototypes of different classes and refines the classification decision boundaries, which results in an improved discriminative ability to differentiate unknown classes within a broader open space. Comprehensive experiments on public benchmarks show that BiDirP can significantly improve the detection ability of unknown classes while correctly classifying known classes.},
  archive      = {J_TMM},
  author       = {Zihui Zhang and Chenghao Xu and Jiexi Yan and Cheng Deng},
  doi          = {10.1109/TMM.2025.3599083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bilevel direction preserving for few-shot open-set recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuV-SLAM: Fast neural multiresolution voxel optimization for RGBD dense SLAM. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3599100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping pipeline based on neural multiresolution voxels, characterized by ultra-fast convergence and incremental expansion capabilities. This pipeline utilizes RGBD images as input to construct multiresolution neural voxels, achieving rapid convergence while maintaining robust incremental scene reconstruction and camera tracking. Central to our methodology is to propose a novel implicit representation, termed VDF that combines the implementation of neural signed distance field (SDF) voxels with an SDF activation strategy. This approach entails the direct optimization of color features and SDF values anchored within the voxels, substantially enhancing the rate of scene convergence. To ensure the acquisition of clear edge delineation, SDF activation is designed, which maintains exemplary scene representation fidelity even under constraints of voxel resolution. Furthermore, in pursuit of advancing rapid incremental expansion with low computational overhead, we developed hashMV, a novel hash-based multiresolution voxel management structure. This architecture is complemented by a strategically designed voxel generation technique that synergizes with a two-dimensional scene prior. Our empirical evaluations, conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's exceptional efficacy in terms of convergence speed, tracking accuracy, scene reconstruction, and rendering quality.},
  archive      = {J_TMM},
  author       = {Wenzhi Guo and Bing Wang and Lijun Chen},
  doi          = {10.1109/TMM.2025.3599100},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {NeuV-SLAM: Fast neural multiresolution voxel optimization for RGBD dense SLAM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-spatial attention for refined object placement in text-to-image synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3599077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solely based on given prompts, text-guided diffusion models have enjoyed a unique capability in generating diverse and creative images. Nevertheless, the conveyance of image information through text presents a series of challenges, particularly in controlling the positioning of objects in synthesized images. Despite attempts of recent efforts in exploring alternative conditions, such as bounding box/mask-image pairs, the requirement of a substantial amount of paired data and time-consuming fine-tuning emerge as new issues. Given the observations that not only prompt-related cross-attention maps reveal the spatial arrangement and centroid positions of the objects, but also out-of-prompt markers enjoy rich semantic information, we thus engineer a weighted optimization loss. Specifically, three spatial sub-losses, namely inner box reinforcement loss, outer box attenuation loss, and centroid loss, are devised and seamlessly integrated into the sampling step of current vanilla diffusion models. Without any annotations of layout data required, the final approach runs in a training-free fashion. Extensive experiments with new performance scores demonstrate that our proposal not only successfully addresses the issue of object positioning but also boosts the capabilities of most current models, such as Stable Diffusion and GLIGEN, in high-quality synthesis and coverage of various concepts. Moreover, the proposed mechanism plays a plug-and-play role.},
  archive      = {J_TMM},
  author       = {Jianwei Zheng and Ni Xu and Wei Li and Jiawei Jiang and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3599077},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-spatial attention for refined object placement in text-to-image synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-frequency collaborative learning for camouflaged object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a challenging task that struggles to accurately detect the objects concealed in the surrounding environment. This is largely attributed to the intrinsic similarity of the camouflaged objects with the surrounding environment. To address this challenge, we propose a Spatial-Frequency Collaborative Learning network for COD (SFCNet). Specifically, we propose a Domain Transformation Fusion (DTF) module to handle the similarity between the camouflaged objects and the background, because when processed in the frequency domain, the features of the camouflaged object and the background become easy to discriminate. Then, we design a Cross-domain Integration Unit (CIU) to integrate the high-level features progressively through a Spatial-Frequency Coordinated Fusion (SFCF) module and a Multi-scale Feature Enhancement (MFE) module. Finally, the low-level features are combined with the high-level features from different decoding stages to correct the camouflaged objects in detail. In addition, an Edge Amplification (EA) module is designed to enable the model to pay attention to the global contour of the camouflaged object. It can facilitate the generation of prediction maps with accurate object boundaries. Extensive experiments on four benchmark COD datasets show that SFCNet outperforms state-of-the-art (SOTA) COD models. Meanwhile, it also has the characteristics of low parameters (21.01 M) and low computational complexity (24.14 G). Codes and results are released on https://github.com/Zhaorui328/SFCNet.},
  archive      = {J_TMM},
  author       = {Rui Zhao and Mengyin Wang and Fasheng Wang and Fuming Sun and Haojie Li},
  doi          = {10.1109/TMM.2025.3599041},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-frequency collaborative learning for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative learning framework with coupling graph transformers for 3D tooth segmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3599046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of 3D dental models into individual teeth is an important step in orthodontic computer-aided design (CAD) systems. However, most existing methods rely on single-view dental models and ignore the intrinsic relationships between upper and lower dental models, hindering the handling of complex tooth structures. In this paper, a collaborative learning framework with coupling graph Transformers (CGT-CLF) is proposed for automatic tooth segmentation on 3D dental models. The framework collaboratively learns geometric features of both upper and lower dental models, capturing their interactivity and complementarity by facilitating interaction between graph-Transformer encoders to improve segmentation of complex and diverse teeth. Specifically, CGT-CLF consists of three key components as follows: First, a graph embedding-based boundary perception module (GEBPM) is developed to aggregate fine-grained geometric features within the neighborhood graph domain, enhancing the network's ability to perceive and distinguish intricate tooth boundaries. Then, coupling geometric Transformers are designed to capture the intrinsic relationships of pair-wise dental models by promoting the exchange of relevant information to gain a comprehensive understanding of the overall tooth structure, allowing for better identification of adjacent teeth with similar appearances. Finally, a collaborative cross-scale feature fusion (CCFF) strategy is utilized to obtain interactive and complementary information by modeling the inter-relationships between dual-stream features. Experimental results on a clinical dental model dataset demonstrate that the proposed CGT-CLF framework outperforms state-of-the-art methods, delivering superior segmentation performance.},
  archive      = {J_TMM},
  author       = {Zhijie Lin and Zhaoshui He and Chang Liu and Hao Liang and Wenqing Su and Ji Tan and Jing Guo},
  doi          = {10.1109/TMM.2025.3599046},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A collaborative learning framework with coupling graph transformers for 3D tooth segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-DINO: Cross the deep MLP and transformer for small object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3599074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small Object Detection (SOD) poses significant challenges due to limited information and the model's low class prediction score. While Transformer-based detectors have shown promising performance, their potential for SOD remains largely unexplored. In typical DETR-like frameworks, the CNN backbone network, specialized in aggregating local information, struggles to capture the necessary contextual information for SOD. The multiple attention layers in the Transformer Encoder face difficulties in effectively attending to small objects and can also lead to blurring of features. Furthermore, the model's lower class prediction score of small objects compared to large objects further increases the difficulty of SOD. To address these challenges, we introduce a novel approach called Cross-DINO. This approach incorporates the deep MLP network to aggregate initial feature representations with both short and long range information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to integrate these initial representations to the Transformer Encoder feature, enhancing the details of small objects. Additionally, we introduce a new kind of soft label named Category-Size (CS), integrating the Category and Size of objects. By treating CS as new ground truth, we propose a new loss function called Boost Loss to improve the class prediction score of the model. Extensive experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D datasets demonstrate that Cross-DINO efficiently improves the performance of DETR-like models on SOD. Specifically, our model achieves 36.4% AP$_{S}$ on COCO for SOD with only 45M parameters, outperforming the DINO by +4.4% AP$_{S}$ (36.4% vs. 32.0%) with fewer parameters and FLOPs, under 12 epochs training setting.},
  archive      = {J_TMM},
  author       = {Guiping Cao and Wenjian Huang and Xiangyuan Lan and Jianguo Zhang and Dongmei Jiang and Yaowei Wang},
  doi          = {10.1109/TMM.2025.3599074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-DINO: Cross the deep MLP and transformer for small object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complementary and contrastive learning for audio-visual segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3599048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps that correlate with the auditory signals of objects. This field has seen significant progress with numerous CNN and Transformer-based methods enhancing the segmentation accuracy and robustness. Traditional CNN approaches manage audio-visual interactions through basic operations like padding and multiplications but are restricted by CNNs' limited local receptive field. More recently, Transformer-based methods treat auditory cues as queries, utilizing attention mechanisms to enhance audio-visual cooperation within frames. Nevertheless, they typically struggle to extract multimodal coefficients and temporal dynamics adequately. To overcome these limitations, we present the Complementary and Contrastive Transformer (CCFormer), a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively. Our CCFormer initiates with the Early Integration Module (EIM) that employs a parallel bilateral architecture, merging multi-scale visual features with audio data to boost cross-modal complementarity. To extract the intra-frame spatial features and facilitate the perception of temporal coherence, we introduce the Multi-query Transformer Module (MTM), which dynamically endows audio queries with learning capabilities and models the frame and video-level relations simultaneously. Furthermore, we propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across both modalities in the unified feature space. Through the effective combination of those designs, our method sets new state-of-the-art benchmarks across the S4, MS3 and AVSS datasets.},
  archive      = {J_TMM},
  author       = {Sitong Gong and Yunzhi Zhuge and Lu Zhang and Pingping Zhang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3599048},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Complementary and contrastive learning for audio-visual segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SkeFormer: Skeletal cues-aware bone point relationship learning for efficient FBIC via transformers. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3603431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to identify endangered bird species in complex outdoor environments has attracted significant attention in the fields of computer vision and machine learning. Previous studies on fine-grained bird image classification (FBIC) face numerous challenges, such as environmental occlusions and arbitrary postures, which limit the accuracy and robustness of existing methods. To address these challenges and enable more reliable bird species identification in extreme outdoor conditions, we propose a novel skeletal cues-aware bone point relationship learning for efficient FBIC via Transformers (SkeFormer). To the best of our knowledge, this is the first time skeletal relationships have been introduced to the FBIC task. Our model introduces three key modules: the skeletal relationship mining (SRM) module, the multilevel feature generation (MFG) module, and the key feature selection (KFS) module. Specifically, in SRM, the model mines the skeletal relationships among different bird species. In MFG, multiscale information is aggregated by connecting features across multiple layers. The KFS module selects key immutable regions of birds based on the learned skeletal relationships. Extensive experiments on two benchmark datasets, CUB-200-2011 and NABirds, show that SkeFormer outperforms existing state-ofthe- art models. The code for SkeFormer will be publicly available.},
  archive      = {J_TMM},
  author       = {Hai Liu and Qiang Chen and Zhibing Liu and Tingting Liu and Li Zhao and Zhaoli Zhang and You-Fu Li},
  doi          = {10.1109/TMM.2025.3603431},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SkeFormer: Skeletal cues-aware bone point relationship learning for efficient FBIC via transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PIMG: Progressive image-to-music generation with contrastive diffusion models. <em>TMM</em>, 1-9. (<a href='https://doi.org/10.1109/TMM.2025.3586119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Image-to-Music Generation is to create pure music according to the given image. Unlike existing tasks such as text-to-image generation, there is no explicit connection between image content and musical melody. Some existing studies attempt to generate music by directly mapping image features (such as color, edges, etc.) into musical notes, which may result in the melodic incoherence. Inspired by neuroscience, it is desirable to employ emotion to bridge these two modalities. However, the continuity and complexity of emotions make it difficult to capture the cross-modal correlation. Drawing from human perception mechanisms of emotions, a Progressive Image-to-Music Generation (PIMG) framework is proposed. The framework designs a mean-teacher based association network to guide the music generation process progressively, starting from highly correlated image-music pairs. The generation network receives more challenging sample pairs gradually, eventually capturing complex cross-modal emotional correspondences. Additionally, a contrastive learning strategy is introduced into the diffusion models to better capture the consistency between pieces of music with the similar emotions. Extensive experimental results demonstrate that the proposed framework is able to generate high-quality and emotionally consistent music from images.},
  archive      = {J_TMM},
  author       = {Mulin Chen and Yajie Wang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3586119},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PIMG: Progressive image-to-music generation with contrastive diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-prototype discrimination: Boosting support-query matching for few-shot segmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3586125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims at training a model on base classes with sufficient annotations and then tasking the model with predicting a binary mask to identify novel class pixels with limited labeled images. Mainstream FSS methods adopt a support-query matching paradigm that activates target regions of the query image according to their similarity with a single support class prototype. However, this prototype vector is inclined to overfit the support images, leading to potential under-matching in latent query object regions and incorrect mismatches with base class features in the query image. To address these issues, this study reformulates conventional single foreground prototype matching to a multi-prototype matching paradigm. In this paradigm, query features exhibiting high confidence with non-target prototypes will be categorized as background. Specifically, the target query features are drawn closer to the novel class prototype through a Masked Cross-Image Encoding (MCE) module and a Semantic Multi-prototype Matching (SMM) module is incorporated to collaboratively filter unexpected base class regions on multi-scale features. Furthermore, we devise an adaptive class activation map, termed target-aware class activation map (TCAM) to preserve semantically coherent regions that might be inadvertently suppressed under pixel-wise matching guidance. Experimental results on PASCAL-5$^{i}$ and COCO-20$^{i}$ datasets demonstrate the advantage of the proposed novel modules, with the holistic approach outperforming compared state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wenbo Xu and Huaxi Huang and Yongshun Gong and Litao Yu and Qiang Wu and Jian Zhang},
  doi          = {10.1109/TMM.2025.3586125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-prototype discrimination: Boosting support-query matching for few-shot segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust coverless video steganography based on two-level DCT features against video attacks. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3586104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with traditional video steganography, coverless video steganography (CVS) can completely avoid being detected by steganalysis algorithms. Recently, the study of CVS has developed rapidly. However, it is still far from the theoretical maximum values in capacity, i.e., the theoretical limit is $2^\ell$ for a hash sequence length of $\ell$. Besides, most existing CVS methods have only considered limited types of video attacks in robustness. In this paper, a novel coverless video steganography based on two-level discrete cosine transform (DCT) features is proposed. First, pre-processing is accomplished on the public video datasets. Then, two-level DCT features are calculated and the Coverless Video Database (CVD) is constructed by the K-means++ clustering algorithm. After that, the mapping table is established to map the secret segments to the CVD. Finally, each secret segment corresponds to a video sequence in the CVD by the mapping table to complete the process of information embedding and extraction. The proposed method first evaluates the robustness against the frame swapping attack, which is a common video attack. Experimental results show that the proposed method can achieve the theoretical maximum value in effective capacity and better robustness compared to the state-of-the-art works.},
  archive      = {J_TMM},
  author       = {Laijin Meng and Xinghao Jiang and Qiang Xu and Tanfeng Sun},
  doi          = {10.1109/TMM.2025.3586104},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A robust coverless video steganography based on two-level DCT features against video attacks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeMaskFormer: Adapting mask transformer for semantic edge detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3586134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic Edge Segmentation (SED) is crucial for intelligent agents to understand and interact with their environments, as it enables them to locate and recognize semantic boundaries. The prevailing framework in the field of SED is multi-label learning, which identifies edges and their semantics by learning to assign multiple labels that indicate the categories of the objects forming the edges. However, this framework has demonstrated limited performance when dealing with complex scenarios. In this paper, we propose a mask classification framework specifically tailored for the SED task, termed EdgeMaskFormer. Within this framework, we develop a query-based edge semantic extractor to learn semantic embeddings for edge mask classification with assistance from regional semantic supervision. Additionally, we design a context-aware hierarchical edge extractor to serve as an edge mask head, which can capture multi-scale edges of different categories under the guidance from the semantic embeddings via dynamic convolution. Furthermore, we develop matching and supervision mechanisms specifically for edge mask classification in order to reduce edge noise and address the imbalance between edge and non-edge samples. Our extensive experiments on three public datasets demonstrate that the proposed approach achieves outstanding performance in semantic edge detection, particularly on those datasets with complex scenarios.},
  archive      = {J_TMM},
  author       = {Lijun Dong and Wei Ma and Hongbin Zha},
  doi          = {10.1109/TMM.2025.3586134},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EdgeMaskFormer: Adapting mask transformer for semantic edge detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCIGeo: Cross-view and cross-day-night image geo-localization using daytime image supervision. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3586124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view image geo-localization is a technique to determine the geographic location of the query image by matching it with geo-tagged aerial images. However, when the query image is captured at nighttime, the existing methods could not extract geographic-related information from low and uneven illumination areas effectively, thus geo-localizing the nighttime ground image with poor performance. In this work, we propose a cross-view and cross-day-night image geo-localization method (CCIGeo), which contains three branches, taking the query nighttime ground image, the supervision daytime ground image, and the reference satellite image as inputs, respectively. Inspired by knowledge distillation, the proposed method takes daytime ground image branch as the teacher model, which would supervise the nighttime ground image branch to overcome the interference of the uneven and low illumination, and pay more attention to the areas containing rich geographic-related information. And to better adapt to the cross-day-night environment, a dual-constraint loss function is designed inspired by the concept of knowledge distillation. Extensive experimental results show that CCIGeo significantly improves the performance on nighttime image geo-localization, exceeding the state-of-the-art (SOTA) methods by 1.83%, 3.84%, and 1.64% on three datasets.},
  archive      = {J_TMM},
  author       = {Nan Wu and Chunfang Yang and Baojun Qi and Ma Zhu and Jiangshan Li and Xiangyang Luo},
  doi          = {10.1109/TMM.2025.3586124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCIGeo: Cross-view and cross-day-night image geo-localization using daytime image supervision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MossVLN: Memory-observation synergistic system for continuous vision-language navigation. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3586105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating in continuous environments with vision-language cues presents critical challenges, particularly in the accuracy of waypoint prediction and the quality of navigation decision-making. Traditional methods, which predominantly rely on spatial data from depth images or straightforward RGB-depth integrations, frequently encounter difficulties in environments where waypoints share similar spatial characteristics, leading to erroneous navigational outcomes. Additionally, the capacity for effective navigation decisions is often hindered by the inadequacies of traditional topological maps and the issue of uneven data sampling. In response, this paper introduces a robust memory-observation synergistic vision-language navigation framework to substantially enhance the navigation capabilities of agents operating in continuous environments. We present an advanced observation-driven waypoint predictor that effectively utilizes spatial data and integrates aligned visual and textual cues to significantly improve the accuracy of waypoint predictions within complex real-world scenarios. Additionally, we develop a strategic memory-observation planning approach that leverages memory panoramic environmental data and detailed current observation information, enabling more informed and precise navigation decisions. Our framework sets new performance benchmarks on the VLN-CE dataset, achieving a 60.25% Success Rate (SR) and a 50.89% Path Length Score (SPL) on the R2R-CE dataset's unseen validation splits. Furthermore, when adapted to a discrete environment, our model also shows exceptional performance on the R2R dataset, achieving a 74% SR and a 64% SPL on the unseen validation split. The code is available at https://github.com/OpenMICG/MossVLN.},
  archive      = {J_TMM},
  author       = {Ting Yu and Yifei Wu and Qiongjie Cui and Qingming Huang and Jun Yu},
  doi          = {10.1109/TMM.2025.3586105},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MossVLN: Memory-observation synergistic system for continuous vision-language navigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive prompt-driven low-light image enhancement with frequency aware learning. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3586101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light Image Enhancement (LLIE) aims to rectify inadequate illumination conditions and achieve superior visual quality in images, which plays a pivotal role in the domain of low-level computer vision. Due to poor illumination in images, many high-frequency details are obscured, which leads to an uneven distribution of low- and high-frequency information. However, most existing LLIE methods do not pay special attention to the restoration of high-frequency detail information and some challenging-to-recover areas in images. To address this issue, we propose a novel progressive prompt-driven LLIE framework with frequency aware learning, through a two-stage coarse-to-fine learning mechanism. Specifically, the proposed method fully utilizes both the specially designed brightness-aware prompt and detail-aware prompt on the prior trained model, to achieve an excellent enhanced image that exhibits more natural brightness and richer detail information. Furthermore, the proposed frequency aware learning objective can adaptively adjust the contribution of individual pixels for image reconstruction based on the statistics of high- and low-frequency features, which enables the network to focus on learning intricate details and other challenging areas in low-light images. Extensive experimental results demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art methods on representative real-world and synthetic datasets. Our source code is available at https://github.com/MSL502/PPFAL.},
  archive      = {J_TMM},
  author       = {Xiaoyan Sun and De Cheng and Yan Li and Nannan Wang and Dingwen Zhang and Xinbo Gao and Jiande Sun},
  doi          = {10.1109/TMM.2025.3586101},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive prompt-driven low-light image enhancement with frequency aware learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathology-preserving transformer based on multi-color space for low-quality medical image enhancement. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3586133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images acquired under suboptimal conditions often suffer from quality degradation, such as low-light, blurring, and artifacts. Such degradations obscure the lesions and anatomical structures in medical images, making it difficult to distinguish key pathological regions. This significantly increases the risk of misdiagnosis by automated medical diagnostic systems or clinicians. To address this challenge, we propose a multi-Color space-based quality enhancement network (MSQNet) that effectively eliminates global low-quality factors while preserving pathology-related characteristics for improved clinical observation and analysis. We first revisit the properties of image quality enhancement in different color spaces, where the V-channel in the HSV space can better represent the contrast and brightness enhancement process, whereas the A/B-channel in the LAB space is more focused on the color change of low-quality images. The proposed framework harnesses the unique properties of different color spaces to optimize the image enhancement process. Specifically, we propose a pathology-preserving transformer, designed to selectively aggregate features across different color spaces and enable comprehensive multiscale feature fusion. Leveraging these capabilities, MSQNet effectively enhances low-quality RGB medical images while preserving key pathological features, thereby establishing a new paradigm in medical image enhancement. Extensive experiments on three public medical image datasets demonstrate that MSQNet outperforms traditional enhancement techniques and state-of-the-art methods, in terms of both quantitative metrics and qualitative visual assessment. MSQNet successfully improves image quality while preserving pathological features and anatomical structures, facilitating accurate diagnosis and analysis by medical professionals and automated systems.},
  archive      = {J_TMM},
  author       = {Qingshan Hou and Yaqi Wang and Peng Cao and Jianguo Ju and Huijuan Tu and Xiaoli Liu and Jinzhu Yang and Huazhu Fu and Yih Chung Tham and Osmar R. Zaiane},
  doi          = {10.1109/TMM.2025.3586133},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pathology-preserving transformer based on multi-color space for low-quality medical image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised semantic segmentation with multi-constraint consistency learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency regularization has prevailed in semi-supervised semantic segmentation and achieved promising performance. However, existing methods typically concentrate on enhancing the Image-augmentation based Prediction consistency and optimizing the segmentation network as a whole, resulting in insufficient utilization of potential supervisory information. In this paper, we propose a Multi-Constraint Consistency Learning (MCCL) approach to facilitate the staged enhancement of the encoder and decoder. Specifically, we first design a feature knowledge alignment (FKA) strategy to promote the feature consistency learning of the encoder from image-augmentation. Our FKA encourages the encoder to derive consistent features for strongly and weakly augmented views from the perspectives of point-to-point alignment and prototype-based intra-class compactness. Moreover, we propose a self-adaptive intervention (SAI) module to increase the discrepancy of aligned intermediate feature representations, promoting Feature-perturbation based Prediction consistency learning. Self-adaptive feature masking and noise injection are designed in an instance-specific manner to perturb the features for robust learning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes datasets demonstrate that our proposed MCCL achieves new state-of-the-art performance. The source code and models are made available at https://github.com/NUST-Machine-Intelligence-Laboratory/MCCL.},
  archive      = {J_TMM},
  author       = {Jianjian Yin and Tao Chen and Gensheng Pei and Huafeng Liu and Yazhou Yao and Liqiang Nie and Xiansheng Hua},
  doi          = {10.1109/TMM.2025.3586111},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semi-supervised semantic segmentation with multi-constraint consistency learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UBTransformer: Uncertainty-based transformer model for complex scenarios detection in autonomous driving. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3586103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional object detection algorithm in the intelligent vehicle perception system cannot maintain stable recognition performance in the unknown and changing road environment. We find that uncertainty quantification is of great significance in detecting unknown complex environments and helps to improve the robustness and safety of autonomous driving systems. Therefore, this paper proposes an Uncertaintybased Transformer (UBT) object detection algorithm. Firstly, the double Gaussian feature map network (DGF) is designed to quantify and utilize the uncertainty of the features derived from the backbone network. Secondly, we propose a RBFbased query filtering model(RBQF), which takes uncertainty sum as the index of query vector screening. At the same time, this paper proposes an uncertainty detection head (UDH); the final model output results are quantitative uncertainty, improved detection performance and enhanced algorithm reliability. To further prove the detection performance of the proposed method in real driving scenes, we use COCO, Cityscapes, FoggyCi-tyscapes, RainCityscapes and self-made traffic scene datasets for verification, which shows that our algorithm is well applicable to large datasets and complex road scenes. Our codebase and pre-trained models can be accessed at https://github.com/sotifma/UBT/tree/master},
  archive      = {J_TMM},
  author       = {Ke Wang and Qi Ma and Xingcan Li and Chongqiang Shen and Rui Leng and Jianbo Lu},
  doi          = {10.1109/TMM.2025.3586103},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {UBTransformer: Uncertainty-based transformer model for complex scenarios detection in autonomous driving},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesizing multi-person and rare pose images for human pose estimation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) models underperform in recognizing rare poses because they suffer from data imbalance problems (i.e., there are few image samples for rare poses) in their training datasets. From a data perspective, the most intuitive solution is to synthesize data for rare poses. Specifically, the rule-based methods apply manual manipulations (such as Cutout and GridMask) to the existing data, so the limited diversity of the data constrains the model. An alternative method is to learn the underlying data distribution via deep generative models (such as ControlNet and HumanSD) and then sample “new data” from the distribution. This works well for generating frequent poses in common scenes, but suffers when applied to rare poses or complex scenes (such as multiple persons with overlapping limbs). In this paper, we aim to address the above two issues, i.e., rare poses and complex scenes, for person image generation. We propose a two-stage method. In the first stage, we design a controllable pose generator named PoseFactory to synthesize rare poses. This generator is specifically trained on augmented pose data, and each pose is labelled with its level of difficulty and rarity. In the second stage, we introduce a multi-person image generator named MultipGenerator. It is conditioned on multiple human poses and textual descriptions of complex scenes. Both stages are controllable in terms of the diversity of poses and the complexity of scenes. For evaluation, we conduct extensive experiments on three widely used datasets: MS-COCO, HumanArt, and OCHuman. We compare our method against traditional pose data augmentation and person image generation methods, and it demonstrates its superior performance both quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Liuqing Zhao and Zichen Tian and Peng Zou and Richang Hong and Qianru Sun},
  doi          = {10.1109/TMM.2025.3586122},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Synthesizing multi-person and rare pose images for human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond 3D: Generic IoU for 3D object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection from point clouds is a fundamental task for 3D scene understanding and has a wide range of applications in the field of multimedia data processing and analysis, such as autonomous driving and virtual interaction. The IoU evaluates the overlap between the two bounding boxes to ensure consistency across network optimization and testing, becoming a recognized regression loss in the field of 3D object detection. However, there is a kind of error coupling between the IoU and the angle, i.e., the IoU does not decrease as the angle error increases and vice versa. This problem leads to sub-optimal solutions for the neural network model, which severely hampers the improvement of 3D object detection accuracy. In this paper, a novel 4DIoU method is introduced for detecting 3D objects from point clouds, which provides a comprehensive rethinking of IoU computation by integrating angular information as an additional dimension. 4DIoU not only solves the problem of error coupling between IoU and angular but also facilitates neural network optimization using angle information. Furthermore, to solve the different impacts of various object shapes on IoU variations, a special 4DIoU called TV4DIoU is proposed to fuse shape information based on three orthogonal projection views, which can adaptively learn the information of objects with different shapes. In addition, to enhance the generalization of the 4DIoU method, a high-flexibility anchor encoding method and a cyclic consistent computation formula for angular errors are designed to make 4DIoU a plug-and-play module for both anchor-based and anchor-free frameworks. Extensive evaluations conducted on the nuScenes, Waymo, and KITTI datasets have confirmed the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Hengsheng Lun and Ke Lu and Liping Hou and Shuhua Wang and Jian Xue},
  doi          = {10.1109/TMM.2025.3586127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond 3D: Generic IoU for 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DomainVerse: A benchmark towards real-world distribution shifts for training-free adaptive domain generalization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3586108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cross-domain tasks, including unsupervised domain adaptation (UDA), domain generalization (DG) and test-time adaptation (TTA), rely heavily on the training model by source domain data whether for specific or arbitrary target domains. With the recent advance of vision-language models (VLMs), recognized as natural source models that can be transferred to various downstream tasks without any parameter training, we propose a novel cross-domain task directly combining the strengths of both UDA and DG, named Training-Free Adaptive Domain Generalization (TF-ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which hinder the real-world application of current cross-domain models due to the lack of accurate and fair evaluation of fine-grained realistic domains. These insights motivate us to establish a novel realistic benchmark for TF-ADG. Benefiting from the introduced hierarchical definition of domain shifts, our proposed dataset DomainVerse addresses these issues by providing about 0.5 million images from 390 realistic, hierarchical, and balanced domains, allowing for decomposition across multiple domains within each image. With the help of the constructed DomainVerse and VLMs, we further propose two algorithms called Domain CLIP and Domain++ CLIP for training-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.},
  archive      = {J_TMM},
  author       = {Feng Hou and Jin Yuan and Ying Yang and Yao Zhang and Yang Liu and Yang Zhang and Cheng Zhong and Zhongchao Shi and Jianping Fan and Zhiqiang He and Yong Rui},
  doi          = {10.1109/TMM.2025.3586108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DomainVerse: A benchmark towards real-world distribution shifts for training-free adaptive domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MVP-shot: Multi-velocity progressive-alignment framework for few-shot action recognition. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent few-shot action recognition (FSAR) methods typically perform semantic matching on learned discriminative features to achieve promising performance. However, most FSAR methods focus on single-scale (e.g., frame-level, segment-level, etc.) feature alignment, which ignores that human actions with the same semantic may appear at different velocities. To this end, we develop a novel Multi-Velocity Progressive-alignment (MVP-Shot) framework to progressively learn and align semantic-related action features at multi-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA) module is designed to measure the similarity between features from support and query videos with different velocity scales and then merge all similarity scores in a residual fashion. To avoid the multiple velocity features deviating from the underlying motion semantic, our proposed Progressive Semantic-Tailored Interaction (PSTI) module injects velocity-tailored text information into the video feature via feature interaction on channel and temporal domains at different velocities. The above two modules compensate for each other to make more accurate query sample predictions under the few-shot settings. Experimental results show our method outperforms current state-of-the-art methods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101, Kinetics, SSv2-full, and SSv2-small).},
  archive      = {J_TMM},
  author       = {Hongyu Qu and Rui Yan and Xiangbo Shu and Hailiang Gao and Peng Huang and Guo-Sen Xie},
  doi          = {10.1109/TMM.2025.3586118},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MVP-shot: Multi-velocity progressive-alignment framework for few-shot action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCNet: Lightweight cycle network driven by physical and deep prior for compressed sensing. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) networks have recently achieved excellent performance on image compressed sensing. However, most existing methods rely on burdened and complex network structures, resulting in significant computational and storage requirements that defeat the purpose of compressed sensing. This severely hinders their applicability in real-world resource-limited devices. In this paper, a lightweight cycle network driven by physical and deep priors for image compressed sensing is proposed which integrates the learning of the sensing matrix and compressive image reconstruction. Specifically, the regularization terms and a likelihood term derived from the physical observation model are learned in an end-to-end cycle network, simultaneously estimating the reconstructed image and sensing matrix in the image and feature domains. Moreover, a dual-domain fusion reconstruction module is proposed. It creates simulated measurement residuals for enhancing reconstruction in the compressed domain, which leads to high reconstruction performance and reduces computational load by bonding together the compressed image domains in the cyclic network. Extensive experiments demonstrate that our model delivers superior performance and alleviates model complexity, which is of great importance in low-budget applications. The reproducible code is available at:https://github.com/shuowenyang/LCNet.},
  archive      = {J_TMM},
  author       = {Shuowen Yang and Fernando Pérez-Bueno and Hanlin Qin and Rafael Molina and Aggelos K. Katsaggelos},
  doi          = {10.1109/TMM.2025.3586145},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LCNet: Lightweight cycle network driven by physical and deep prior for compressed sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing light field salient object detection with variance-maximized key focal slice selection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field saliency object detection (LF SOD) methods have made significant progress recently. Most of them explore abundant multi-modal information from the all-focus image and the focal stacks at all focal planes to enrich scene details and depth perception. However, in light-field images, the spatial and depth information varies slightly across different slices, raising redundancy within focal stacks. Besides, the noise can appear repeatedly in multiple images of the focal stacks, which brings interference. To address these issues, in this work, we propose VMKNet, an effective approach that leverages innovative variance-maximized key slice selection and interacts with the all-focus image, to improve LF SOD. Specifically, we measure consistency differences between the all-focus image and each focal slice in the salient region as saliency scores. Then, we randomly assemble sets of them, where each score corresponds to a certain slice. The one exhibiting the highest variance is singled out to determine key focal slices as they reveal the diversity of salient objects. Then, the bidirectional guidance module (BGM) is presented to learn attentive features of all-focus and selected key slices in a mutual guidance manner, thus producing enhanced and holistic features. With hierarchical BGMs, our model can progressively aggregate common salient semantics and meaningful contextual details, generating more discriminative representations. Moreover, we introduce the edge enhancement module in conjunction with BGM to improve the sharpness of saliency maps. Extensive experiments on common light field datasets demonstrate that our method, termed VMKNet, outperforms recent state-of-the-art LF, RGB-D, and RGB methods. Our code is available at https://github.com/Han-jiaxin/VMKNet.},
  archive      = {J_TMM},
  author       = {Jiaxin Han and Feng Li and Anqi Li and Mengmeng Zhang and Huihui Bai and Jimin Xiao and Yao Zhao},
  doi          = {10.1109/TMM.2025.3586131},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing light field salient object detection with variance-maximized key focal slice selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S4R: Rethinking point cloud sampling via guiding upsampling-aware perception. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3586148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud sampling aims to derive a sparse point cloud from a relatively dense point cloud, which is essential for efficient data transmission and storage. While existing deep sampling methods prioritize preserving the perception of sampled point clouds for downstream networks, few studies have critically examined the rationale behind this goal. Specifically, we observe that sampling can lead to a perceptual degradation phenomenon in many influential downstream networks, impairing their ability to effectively process sampled point clouds. We theoretically reveal the nature of the phenomenon and attempt to construct a novel sampling target by uniting upsampling and perceptual reconstruction. Accordingly, we propose a Maximum A Posteriori (MAP) sampling framework named Sample for Reconstruct (S4R), which impels the sampling stage to infer upsampling-guided perception. In S4R, we design very simple but effective sampling and upsampling networks using residual-based graph convolutions and incorporate a pseudo-residual connection to introduce prior knowledge. This architecture takes advantage of reconstruction properties and allows the sampling network to be trained in an unsupervised manner. Extensive experiments on classical networks demonstrates the excellent performance of S4R compared with the previous sampling schemes and reveals its advantages on different point cloud downstream tasks, i.e., classification, reconstruction and segmentation. Code will be made available.},
  archive      = {J_TMM},
  author       = {Zhuangzi Li and Shan Liu and Wei Gao and Guanbin Li and Ge Li},
  doi          = {10.1109/TMM.2025.3586148},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {S4R: Rethinking point cloud sampling via guiding upsampling-aware perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCFMN: Hierarchical cross-modal fine-grained mining network for temporal sentence grounding. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Sentence Grounding (TSG) requires a thorough understanding of the complex cross-modal semantic relationships between videos and text. However, existing methods fail to accurately capture content at diverse granularity levels with distinct semantics, making it difficult to achieve fine alignment of visuals and text. To overcome this issue, we attempt to mine for rich semantic clues by utilizing the hierarchical correspondence structure and multi-granularity visual-to-text reconstruction, achieving fine-grained reasoning. Specifically, for the TSG task, we propose a novel Hierarchical Cross-modal Fine-grained Mining Network (HCFMN), which utilizes an attention mechanism based on temporal hierarchical relationships to extract temporal features corresponding to the text of different granularities. We leverage the reconstructability of visual-to-text, recovering multi-granularity textual content from coarse to fine by focusing on temporal features at different layers, hierarchically extracting temporal features and the dependencies related to the text, and implementing fine-grained cross-modal semantic alignment. Furthermore, HCFMN introduces a novel partitioned efficient attention mechanism, which significantly enhances the model's efficiency through a two-stage attention based on sequence and channel compression. Extensive experimental results on three public datasets (ActivityNet-Captions, TACoS, and Charades-STA) demonstrate that the proposed method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Ran Ran and Jiwei Wei and Yuyang Zhou and Xiang Guan and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3586156},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HCFMN: Hierarchical cross-modal fine-grained mining network for temporal sentence grounding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A blockchain and improved perception hash based copyright protection scheme for purely chromatic background images. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purely chromatic background images are widely used in computer wallpapers and advertisements, leading to issues such as copyright infringement and the loss of interest of holders. Image hashing is a technique used for comparing the similarity between images, and is often used for image verification, search, and copy detection due to its insensitivity to subtle changes in the original image. In a purely chromatic background image, the central detail of the image is the primary part and the key for copyright authentication. As the perception hash (pHash) algorithm only retains the low-frequency portion of the discrete cosine transform (DCT) matrix, it is unsuitable for purely chromatic background images. To deal with this issue, we propose an improved perception hash (ipHash) algorithm to enhance the universality of the algorithm by extracting purely chromatic background image features. Meanwhile, the development of image hashing is restricted due to the requirement of a trusted third party. To solve this issue, a secure blockchain-based image copyright protection scheme is designed. It realizes the copyright authentication and traceability, and overcomes the issue of a lack of trusted third parties. Experimental results show that the proposed method outperforms the state-of-theart image copyright protection schemes.},
  archive      = {J_TMM},
  author       = {Guangyong Gao and Tongchao Feng and Chongtao Guo and Zhihua Xia and Yun-Qing Shi},
  doi          = {10.1109/TMM.2025.3586150},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A blockchain and improved perception hash based copyright protection scheme for purely chromatic background images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint distribution weighted alignment for multi-source domain adaptation via kernel relative entropy estimation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3586109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of Multi-Source Domain Adaptation (MSDA) is to train a neural network on labeled data from multiple joint source distributions (source domains) and unlabeled data from a joint target distribution (target domain), and use the trained network to estimate the target data labels. The challenge in this MSDA problem is that the multiple joint source distributions are relevant but distinct from the joint target distribution. To address this challenge, we propose a Joint Distribution Weighted Alignment (JDWA) approach to align a weighted joint source distribution to the joint target distribution under the relative entropy. Specifically, the weighted joint source distribution is defined as the weighted sum of the multiple joint source distributions, and is parameterized by the relevance weights. Since the relative entropy is unknown in practice, we propose a Kernel Relative Entropy Estimation (KREE) method to estimate it from data. Our KREE method first reformulates relative entropy as the negative of the minimal value of a functional, then exploits a function from the Reproducing Kernel Hilbert Space (RKHS) as the functional's input, and finally solves the resultant convex problem with a global optimal solution. We also incorporate entropy regularization to enhance the network's performance. Together, we minimize cross entropy, relative entropy, and entropy to learn both the relevance weights and the neural network. Experimental results on benchmark image classification datasets demonstrate that our JDWA approach performs better than the comparison methods. Pytorch code of our approach will be released upon the paper's publication.},
  archive      = {J_TMM},
  author       = {Sentao Chen and Ping Xuan and Zhifeng Hao},
  doi          = {10.1109/TMM.2025.3586109},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint distribution weighted alignment for multi-source domain adaptation via kernel relative entropy estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral discrepancy and cross-modal semantic consistency learning for object detection in hyperspectral images. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed Spectral Discrepancy and Cross-Modal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.},
  archive      = {J_TMM},
  author       = {Xiao He and Chang Tang and Xinwang Liu and Wei Zhang and Zhimin Gao and Chuankun Li and Shaohua Qiu and Jiangfeng Xu},
  doi          = {10.1109/TMM.2025.3586155},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spectral discrepancy and cross-modal semantic consistency learning for object detection in hyperspectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rivisting source-free domain adaptation object detection in thresholds. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3590905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptive object detection (SFOD) aims to transfer models pre-trained on the source domain to the unlabeled target domain without requiring access to the source data. Most existing SFOD methods leverage pseudo-labels for self-supervised training in the target domain. We investigate the limitations of threshold techniques to obtain high-quality pseudo-labels. In response, we design the Sequential SourceFree domain adaptive Object Detection (S-SFOD) algorithm, which enhances the quality of pseudo-labels at both the image and instance levels. At the image level, we reconstruct the training dataset, prioritizing the training of images that yield more reliable pseudo-labels to help the model acquire valuable target domain knowledge in the initial training stages. At the instance level, we introduce an adaptive local-global threshold method to balance the quality and quantity of pseudo-labels by dynamically adjusting the thresholds based on the model's learning progress. By improving the quality of pseudo-labels through these complementary techniques at both the image and instance levels, we effectively transfer knowledge from the source domain to the target domain. Extensive experiments on multiple cross-domain object detection datasets demonstrate that our proposed method outperforms current state-of-the-art SFOD algorithms. The code and model will be released.},
  archive      = {J_TMM},
  author       = {Yuchen Dong and Chengeyang Li and Yongqiang Xie and Zhongbo Li},
  doi          = {10.1109/TMM.2025.3590905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rivisting source-free domain adaptation object detection in thresholds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and controllable privacy-preserving image retrieval in multi-user settings. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3586149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud environments, privacy-preserving contentbased image retrieval (PPCBIR) enables users to retrieve images while protecting image privacy. Existing PPCBIR systems often use a single image key, which causes low efficiency and makes it difficult to achieve fine-grained access control over images. This paper proposes a lightweight and controllable privacy-preserving image retrieval in multi-user settings (named LCPIRM) to improve time efficiency and access control performance. A one-time image encryption method based on reversible embedding is proposed to balance the contradiction between complexity and security without increasing the difficulty of key management. A robust hash generation method is designed by combining piecewise mean quantization and encryption image features, which can effectively improve retrieval efficiency because the robust hashes embedded in the encrypted images can be extracted and establish inverted indexing in the cloud. When dealing with authorized encrypted images, the cloud server uses proxy re-encryption to convert the image keys embedded within themselves from the owner's public key protection to the authorized user's public key protection, achieving fine-grained access control over images in a multi-user setting. Theoretical analysis and experimental results show that LCPIRM has better performance in terms of retrieval accuracy, consumption, and search efficiency while meeting security requirements. In the real datasets Caltech256 and Caltech101, the search efficiency has increased by 74% and 58% respectively compared to the existing schemes.},
  archive      = {J_TMM},
  author       = {Zhuo Feng and Hongjie He and Fan Chen and Jie Bai},
  doi          = {10.1109/TMM.2025.3586149},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Lightweight and controllable privacy-preserving image retrieval in multi-user settings},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointMax: Self-boosted local sampling for 3D point cloud analysis. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local sampling plays a key role in modeling 3D point clouds. Due to the disordered and unstructured nature of point cloud data, conventional 3D deep models such as PointNet++ and its variants usually employ random or fixed rules to sample local neighborhoods, leading to considerable redundancy in the feature aggregation process. In this paper, we propose a self-supervised method for learning to adaptively select effective neighbors. Firstly, we observe that only a part of sampled points contributes to the aggregated features after the max-pooling operation in existing point cloud models. Then, based on this observation, we propose a simple and task-oriented metric to evaluate the sampling efficiency by measuring the effective neighbors in the feature aggregation process. The metric is also used to supervise a lightweight neighborhood scoring module (NSM), which is designed to efficiently select effective neighboring points from a wider range of neighbors to reduce the computational cost and keep the performance superior. To further improve the performance, we introduce Neighborhood Attention in the feature aggregation process according to the importance score of neighborhood points predicted by NSM. Experimental results show that our method is simple and efficient, and can be applied to most tasks and models to reduce the computational cost and keep the performance superiority. Our code is available at https://github.com/sunshuofeng/PointMax_Code},
  archive      = {J_TMM},
  author       = {Shuofeng Sun and Yongming Rao and Jiwen Lu and Haibin Yan},
  doi          = {10.1109/TMM.2025.3590932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointMax: Self-boosted local sampling for 3D point cloud analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt-guided prototype-aware commonality and discrimination learning for zero-shot skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Skeleton-Based Action Recognition (ZSSAR) is an emerging research field focused on developing alignment models that connect skeleton movements with action definitions, thus enabling generalization to unobserved actions. Current methods often employ generative models to reconstruct cross-modal features or enhance mutual information across modalities for alignment. However, when applied to unseen action categories, these models often neglect the inherent consistency among basic actions, thereby diminishing their generalization capabilities. Furthermore, imprecise annotations fail to capture the rich semantic details of actions, resulting in misalignment. Inspired by human cognitive processes and chain of thought, we argue that integrating prior information about human actions with intrinsic commonality knowledge of basic actions is essential for ZSSAR. To actualize this, we propose a novel method termed Prompt-guided Prototype-aware Commonality and Discrimination Learning (PP-CDL). This method utilize the comprehensive world knowledge contained in LLMs, employing tailored prompts to partition seen action categories into distinct, non-overlapping prototype spaces that embody the commonality knowledge of basic actions. Subsequently, we introduce the Inter- and Intra-Prototype Discriminating (I2PD) module and the Intra-Prototype Commonality Mining (IPCM) module. The I2PD amplifies the distinctiveness of knowledge within prototypes, furnishing a personalized search space for the recognition of unseen actions. In contrast, the IPCM models the shared commonality concept within prototypes, bolstering the consistency between skeleton action representations and corresponding text knowledge representations. Experiments on different skeleton action benchmarks demonstrate the significant improvement of our method over existing alternatives.},
  archive      = {J_TMM},
  author       = {Xingyu Zhu and Xiangbo Shu and Peng Huang and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3590904},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prompt-guided prototype-aware commonality and discrimination learning for zero-shot skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel secure and robust recoverable cryptographic mosaic technique. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image mosaic is a prevalent technique to conceal critical content in images. However, conventional mosaic techniques cannot be recovered using a small-sized key, as they require retransmission of the original images for perfect recovery. In this work, we propose a novel, computationally efficient, and effective recoverable image-mosaic technique. A key advantage of our proposed image-mosaic scheme is its robust performance across a range of adjustable key lengths. Our technique effectively conceals original information even with a small-sized key of only a few bits. To evaluate its performance, we introduce a new image-similarity metric based on the magnitude of the discrete cosine transform (DCT). This metric exhibits several advantageous mathematical properties, including the ability to quantify the perceptibility of major content in mosaicked images, invariance under image reflections and 180-degree rotations, and insensitivity to small translations. Finally, numerical experiments demonstrate that our method outperforms existing recoverable image-mosaic techniques and performs consistent across varying key lengths. We also compare the run-times required by our proposed new scheme with those required by other existing recoverable image-mosaic methods and the state-of-the-art image-encryption methods to exhibit the computational efficiency of our proposed new scheme.},
  archive      = {J_TMM},
  author       = {Chi Yung and Scott C.-H. Huang and Hsiao-Chun Wu and Che-Hua Li},
  doi          = {10.1109/TMM.2025.3590922},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Novel secure and robust recoverable cryptographic mosaic technique},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image super-resolution with taylor expansion approximation and large field reception. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity techniques are booming in blind super-resolution (SR) due to accurate estimation of the degradation types involved in low-resolution images. However, high-dimensional matrix multiplication within self-similarity computation prohibitively consumes massive computational costs. We find that the high-dimensional attention map is derived from the matrix multiplication between query and key, followed by a softmax function. This softmax makes the matrix multiplication inseparable, posing a great challenge in simplifying computational complexity. To address this issue, we first propose a second-order Taylor expansion approximation (STEA) to separate the matrix multiplication of query and key, resulting in the complexity reduction from $\mathcal {O}(N^{2})$ to $\mathcal {O}(N)$. Then, we design a multi-scale large field reception (MLFR) to compensate for the performance degradation caused by STEA. Finally, we apply these two core designs to laboratory and real-world scenarios by constructing LabNet and RealNet, respectively. Extensive experimental results tested on five synthetic datasets demonstrate that our LabNet sets a new benchmark in qualitative and quantitative evaluations. Tested on the real-world dataset, our RealNet achieves superior visual quality over existing methods. Ablation studies further verify the contributions of STEA and MLFR towards both LabNet and RealNet frameworks. Codes are available at https://github.com/GZHU-DVL/STEA-MLFR.},
  archive      = {J_TMM},
  author       = {Jiancong Feng and Yuan-Gen Wang and Mingjie Li and Fengchuang Xing},
  doi          = {10.1109/TMM.2025.3590917},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image super-resolution with taylor expansion approximation and large field reception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manifold embedding for fast and accurate 3D reconstruction. <em>TMM</em>, 1-20. (<a href='https://doi.org/10.1109/TMM.2025.3590908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the fusion process in RGB-D reconstruction systems is to verify and update the 3D model while ensuring both completeness and accuracy. However, achieving precise dense correspondences in a point-to-point or pixel model during this process is challenging and computationally intensive. To address this challenge, we propose a Manifold Embedding framework that facilitates rapid point-to-surface fusion, removing the need for direct point-to-point or pixel correspondences. Our approach consists of three main steps: 1) Manifold Voxel: We transform discrete point sets into smooth surfaces using the Implicit Moving Least Squares (IMLS) method; 2) Two-Step Filtering: We enhance reconstruction accuracy through a two-step filtering technique that evaluates sampling points based on probabilistic measures; 3) Embedding for Smooth Surface: Lastly, we embed points into a smooth manifold surface represented via IMLS, ensuring high-quality reconstructed surfaces. Extensive experiments on both real and synthetic 3D scenes demonstrate the effectiveness of our Manifold Embedding framework. For instance, on the public Replica dataset, our method surpasses state-of-the-art fusion techniques regarding both completeness and accuracy. Our average accuracy is 2.11 cm and completeness is 2.80 cm, while NICE-SLAM achieves 2.85 cm and 3.00 cm, respectively (with lower values indicating better performance). Overall, our proposed method provides superior reconstruction quality and enhanced computational efficiency (See Fig. 1).},
  archive      = {J_TMM},
  author       = {Duo Chen and Zixin Tang and Ke Song and Xingyu Peng and Wuque Cai and Hongze Sun and Dezhong Yao and Daqing Guo},
  doi          = {10.1109/TMM.2025.3590908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Manifold embedding for fast and accurate 3D reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointCloud-text matching: Benchmark dataset and baseline. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3590931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching (PTM), which aims to identify the exact cross-modal instance that matches a given point-cloud query or text query. PTM has potential applications in various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there is a lack of suitable and targeted datasets for PTM in practice. To address this issue, we present a new PTM benchmark dataset, namely SceneDepict-3D2T. We observe that the data poses significant challenges due to its inherent characteristics, such as the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which render existing cross-modal matching methods ineffective for PTM. To overcome these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two key modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention mechanisms to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL enhances robustness against mismatching by dividing negative pairs into clean and noisy subsets and assigning them forward and reverse optimization directions, respectively. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa.},
  archive      = {J_TMM},
  author       = {Yanglin Feng and Yang Qin and Dezhong Peng and Hongyuan Zhu and Xi Peng and Peng Hu},
  doi          = {10.1109/TMM.2025.3590931},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointCloud-text matching: Benchmark dataset and baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient partially relevant video retrieval with active moment discovering. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untrimmed videos. Existing PRVR methods, which typically focus on modeling multi-scale clip representations, however, suffer from content independence and information redundancy, impairing retrieval performance. To overcome these limitations, we propose a simple yet effective approach with active moment discovering (AMDNet). We are committed to discovering video moments that are semantically consistent with their queries. By using learnable span anchors to capture distinct moments and applying masked multi-moment attention to emphasize salient moments while suppressing redundant backgrounds, we achieve more compact and informative video representations. To further enhance moment modeling, we introduce a moment diversity loss to encourage different moments of distinct regions and a moment relevance loss to promote semantically query-relevant moments, which cooperate with a partially relevant retrieval loss for end-to-end optimization. Extensive experiments on two large-scale video datasets (i.e., TVR and ActivityNet Captions) demonstrate the superiority and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller (#parameters) while 6.0 points higher (SumR) than the up-to-date method GMMFormer on TVR.},
  archive      = {J_TMM},
  author       = {Peipei Song and Long Zhang and Long Lan and Weidong Chen and Dan Guo and Xun Yang and Meng Wang},
  doi          = {10.1109/TMM.2025.3590937},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient partially relevant video retrieval with active moment discovering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Micro-image domain view synthesizer for free navigation with focused plenoptic cameras. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel, first-of-its-kind view synthesis method for plenoptic images, which enables the direct manipulation of images in the micro-images array format, thereby bypassing intermediate transformation steps. Current plenoptic imaging approaches typically rely on an initial conversion to dense multiview images, also known as subaperture images extraction. However, the use of subaperture images presents two main limitations that ultimately impact further processing. First, existing subaperture view extraction methods offer limited control over camera parameters, resolutions, and poses of the subaperture views, which are also constrained to a small area around the main lens, thus restricting free navigation. Second, subaperture images are susceptible to artifacts which can propagate to subsequent processes such as calibration, depth estimation and view synthesis. In this paper, we propose a camera model that enables depth image-based rendering with plenoptic cameras, in a way that allows for the direct synthesis of any target viewpoint. In our evaluation, we show that our method expands view synthesis extrapolation to a range that is two to three times greater than that of pipelines requiring a conversion to subaperture images, including generally accepted tools such as depth image-based rendering and learning-based rendering approaches.},
  archive      = {J_TMM},
  author       = {Sarah Fachada and Daniele Bonatto and Gauthier Lafruit and Mehrdad Teratani},
  doi          = {10.1109/TMM.2025.3590906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Micro-image domain view synthesizer for free navigation with focused plenoptic cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging fuzzy manifold intra-class correlation and inter-class separability for online multilabel streaming features analysis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional multilabel feature selection (MFS) typically relies on pre-computing global information within the feature space. However, in real-world applications, features are dynamically generated and continuously arrive over time, known as streaming features, rendering many existing approaches ineffective. Some MFS methods for streaming features have been developed, several challenges persist: (1) Previous research often uses certain strategies to model streaming feature evaluation, failing to process fuzzy information effectively; (2) The maximum correlation between features and class is emphasized, while inter-class separability is ignored, leading to inaccurate feature evaluation; (3) The continuous influx of streaming features brings the dynamics and unknowns to data distribution, has been largely overlooked in previous work; (4) Streaming feature selection requires immediate feedback on newly arriving features, posing challenges to the algorithm's real-time responsiveness. Motivated by these observations, this paper introduces a novel online MFS strategy for streaming features. First, the weighted manifold distance is designed, and the fuzzy manifold similarity learning strategy is formalized to analyze the instance relationships of unknown distribution. Second, the fuzzy manifold intra-class correlation and inter-class separability are devised to quantify feature discriminability. Finally, a novel multilabel streaming feature analysis framework is established, with feature discriminability as the guiding factor. Incoming features are categorized as weakly relevant, strongly relevant, or redundant, culminating in generating a reliable feature selection subset. Extensive experiments on fifteen public datasets demonstrate that our algorithm achieves competitive performance compared to nine state-of-the-art offline and online algorithms.},
  archive      = {J_TMM},
  author       = {Tengyu Yin and Hongmei Chen and Jihong Wan and Keyu Liu and Zhong Yuan and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1109/TMM.2025.3590919},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging fuzzy manifold intra-class correlation and inter-class separability for online multilabel streaming features analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFAN: An attention-driven forgery adversarial network for blind image inpainting. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image inpainting is a challenging task aimed at reconstructing corrupted regions without relying on mask information. Due to the lack of mask priors, previous methods usually integrate a mask prediction network in the initial phase, followed by an inpainting backbone. However, this multi-stage generation process may result in feature misalignment. While recent end-to-end generative methods bypass the mask prediction step, they typically struggle with weak perception of contaminated regions and introduce structural distortions. This study presents a novel mask region perception strategy for blind image inpainting by combining adversarial training with forgery detection. To implement this strategy, we propose an attention-driven forgery adversarial network (AFAN), which leverages adaptive contextual attention (ACA) blocks for effective feature modulation. Specifically, within the generator, ACA employs self-attention to enhance content reconstruction by utilizing the rich contextual information of adjacent tokens. In the discriminator, ACA utilizes cross-attention with noise priors to guide adversarial learning for forgery detection. Moreover, we design a high-frequency omni-dimensional dynamic convolution (HODC) based on edge feature enhancement to improve detail representation. Extensive evaluations across multiple datasets demonstrate that the proposed AFAN model outperforms existing generative methods in blind image inpainting, particularly in terms of quality and texture fidelity.},
  archive      = {J_TMM},
  author       = {Jiahao Wang and Gang Pan and Di Sun and Jinyuan Li and Jiawan Zhang},
  doi          = {10.1109/TMM.2025.3590914},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AFAN: An attention-driven forgery adversarial network for blind image inpainting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSLL-face: Distributed supervision-integrated framework for low-light face detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-light environments, human vision is severely limited by weak light sources, leading to significantly reduced visual capabilities. Similarly, in machine vision, low-light recognition tasks such as nighttime autonomous driving and surveillance tasks involving the detection of small faces in low-light conditions are more challenging than tasks in normal lighting. Current low-light face detection models lack adaptability to different low-light conditions, and the accuracy of face detection remains unsatisfactory. In this paper, we propose a novel face detection framework DSLL-Face, specifically designed to tackle the challenges of face detection in low-light environments. Our proposed DarkHead, featuring a specialized branch designed to predict the distribution of bounding boxes, thereby substantially enhances the supervision of bounding box localization. This innovative approach effectively resolves the issue of blurry bounding boxes and significantly increases the accuracy of predicted positions. We employ a novel loss function tailored for detecting small faces, enhancing the sensitivity and effectively addressing the blurriness issues in small face detection. Furthermore, we leverage the Channel Grouping and Partial Convolution block (CGP) to enhance multi-scale expression capabilities. We develop the EMNet-pro model with the aim of further enhancing images to improve their adaptability under various low-light conditions. Extensive experiments demonstrate that our model exhibits outstanding capability in low-light face detection on the DARK FACE dataset and achieves significantly better performance compared to existing state-of-the-art frameworks.},
  archive      = {J_TMM},
  author       = {Shuhong Chen and Kairen Chen and Guojun Wang and Sheng Wen and Zhili Zhou},
  doi          = {10.1109/TMM.2025.3590911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSLL-face: Distributed supervision-integrated framework for low-light face detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal depression detection in interview via exploring emotional distribution information. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, automatic depression detection (ADD) technology has been rapidly developed to boost an objective and assistive diagnosis for major depressive disorder (MDD) with the help of artificial intelligence technology and various physiological and psychological data. Despite emotion being an important reflection of mental status and frequently related to depression symptoms, few recent multi-modal ADD methods take emotional information into account. To address the above issue, we propose to explore emotional distribution information in interviews to assist multi-modal ADD model. On one hand, we use large language models (LLMs) to automatically recognize emotion of text data, and re-organize the data guided by the valence attribute of emotion, which facilitates our model being aware of difference in emotion distribution. On the other hand, we design the emotion encoding which enhances the proposed model to consider the emotional distribution information in its decision-making process. Extensive experiments are conducted by comparing with state-of-the-art ADD methods as well as the ablation study on different modules of the proposed method. More importantly, our experimental results can confirm the research findings in the psychology field, where more attention on negative emotion information is demanded in distinguishing different depressive status.},
  archive      = {J_TMM},
  author       = {Zhiyuan Zhou and Yanrong Guo and Shijie Hao and Richang Hong},
  doi          = {10.1109/TMM.2025.3590939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal depression detection in interview via exploring emotional distribution information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards student actions in classroom scenes: New dataset and baseline. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing student actions is an important and challenging task in educational research. Existing efforts have been hampered by the lack of accessible datasets to capture the nuanced action dynamics in classrooms. In this paper, we present a new multi-label Student Action Video (SAV) dataset, specifically designed for action detection in classroom settings. The SAV dataset consists of 4,324 carefully trimmed video clips from 758 different classrooms, annotated with 15 distinct student actions. Compared to existing action detection datasets, the SAV dataset stands out by providing a wide range of real classroom scenarios, high-quality video data, and unique challenges, including subtle movement differences, dense object engagement, significant scale differences, varied shooting angles, and visual occlusion. These complexities introduce new opportunities and challenges to advance action detection methods. To benchmark this, we propose a novel baseline method based on a visual transformer, designed to enhance attention to key local details within small and dense object regions. Our method demonstrates excellent performance with a mean Average Precision (mAP) of 67.9% and 27.4% on the SAV and AVA datasets, respectively. This paper not only provides the dataset but also calls for further research into AI-driven educational tools that may transform teaching methodologies and learning outcomes. The code and dataset are released at https://github.com/Ritatanz/SAV.},
  archive      = {J_TMM},
  author       = {Zhuolin Tan and Chenqiang Gao and Anyong Qin and Ruixin Chen and Tiecheng Song and Feng Yang and Deyu Meng},
  doi          = {10.1109/TMM.2025.3590899},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards student actions in classroom scenes: New dataset and baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting modal-specific representations for sentiment analysis with incomplete modalities. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims at exploiting complementary information from multiple modalities or data sources to enhance the understanding and interpretation of sentiment. While existing multi-modal fusion techniques offer significant improvements in sentiment analysis, real-world scenarios often involve missing modalities, introducing complexity due to uncertainty of which modalities may be absent. To tackle the challenge of incomplete modality-specific feature extraction caused by missing modalities, this paper proposes a Cosine Margin-Aware Network (CMANet) which centers on the Cosine Margin-Aware Distillation (CMAD) module. The core module measures distance between samples and the classification boundary, enabling CMANet to focus on samples near the boundary. So, it effectively captures the unique features of different modal combinations. To address the issue of modality imbalance during modality-specific feature extraction, this paper proposes a Weak Modality Regularization (WMR) strategy, which aligns the feature distributions between strong and weak modalities at the dataset-level, while also enhancing the prediction loss of samples at the sample-level. This dual mechanism improves the recognition robustness of weak modality combination. Extensive experiments demonstrate that the proposed method outperforms the previous best model, MMIN, with a 3.82% improvement in unweighted accuracy. These results underscore the robustness of the approach under conditions of uncertain and missing modalities.},
  archive      = {J_TMM},
  author       = {Xin Jiang and Lihuo He and Fei Gao and Kaifan Zhang and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3590909},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting modal-specific representations for sentiment analysis with incomplete modalities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-division based progressive learning for source-free domain adaptation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing privacy and portability concerns, source-free domain adaptation requires only a source pre-trained model and an unlabeled target domain, allowing for effective adaptation to the target data. Most existing self-training methods focus on selecting and exploiting samples with reliable predictions, often neglecting others. Inspired by the finding that deep models learn clean samples faster than noisy ones, we propose a domain-division based progressive learning method named DPL. Specifically, our approach consists of two alternating stages, each beginning with the division of the target domain into easy-to-adapt and hard-to-adapt subdomains based on adaptation difficulty, followed by neighborhood-based pseudo label assignment. In stage one, we enhance classification accuracy through uncertainty-aware self-training and alignment of corresponding classes between subdomains. Stage two then applies tailored learning strategies to each subdomain, starting with consistency learning on the easy-to-adapt samples and progressing to utilizing local structural information for the more challenging ones, thereby mining the intrinsic properties of the target data. Extensive experiments on several widely used benchmarks validate the effectiveness of our approach, demonstrating superior performance compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Pan Liu and Jing Li and Meng Zhao and Wanli Xue and Qinghua Hu and Shengyong Chen},
  doi          = {10.1109/TMM.2025.3590903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain-division based progressive learning for source-free domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced reasoning via multimodal LLMs and collaborative inference. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a prevalent task that can facilitate the perception of the real world by the visually impaired. However, many VQA models tend to rely on superficial correlations in datasets for predictions rather than genuine reasoning, limiting their real-world applicability. While existing methods address this issue by incorporating debiasing strategies during training, they typically assume prior knowledge of out-of-distribution (OOD) test sets and then tailor debiasing strategies and select optimal models on the basis of the OOD samples. This reliance on OOD test data, however, is unrealistic in practical applications. To address this, some works introduce test-time adaptation techniques to mitigate dataset shifts during model deployment. Despite their potential, these methods risk catastrophic forgetting as they update models at test time without access to the ground-truth answers or the source data. An emerging solution involves leveraging the extensive knowledge embedded in Large Language Models (LLMs) to support reasoning tasks, yet their language-only input restricts flexibility in multimodal tasks. To bridge this gap, we propose leveraging the zero-shot capability of Multimodal Large Language Models (MLLMs). To optimise computational efficiency, we introduce a novel VQA Collaborative Inference framework (VQA-CI) that integrates MLLMs (e.g. BLIP-2 Flan T5) with VQA specialists (e.g. UpDn). This framework initially processes samples through VQA specialists and subsequently determines the necessity for re-evaluation with MLLMs based on predefined bias and reliability indicators. Experiments on the GQA-OOD and VQA-CP v2 datasets show that our VQA-CI achieves significant performance gains, with accuracy improvements of around 6% over state-of-the-art methods, underscoring the effectiveness of our VQA-CI.},
  archive      = {J_TMM},
  author       = {Zhiquan Wen and Mingkui Tan and Yaowei Wang and Qingyao Wu and Qi Wu},
  doi          = {10.1109/TMM.2025.3590940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced reasoning via multimodal LLMs and collaborative inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive clustering and weighted regularization contrastive learning framework for unsupervised person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (ReID) has recently gained significant attention from researchers. ReID matches images of the same person from different camera views in various scenes without any labels. Existing clustering methods primarily rely on a fixed threshold (the maximum distance between sample points and clustering centroids) and overlook the importance of adjusting this threshold during continuous model optimization. This mismatch between clustering thresholds and inter- or intra-class spacing reduces clustering accuracy. To address this issue, this study proposes an Adaptive Clustering and Weighted Regularization Contrastive Learning (ACWRCL) framework for unsupervised person ReID. The ACWRCL framework comprises two main components: (1) the Clustering Threshold Adaptive Adjustment (CTAA) module, and (2) the Weighted Regularization Contrastive Learning (WRCL) module. The CTAA module dynamically adjusts the clustering threshold to align with model optimization, ensuring that the threshold remains within an appropriate range to prevent under- or over-robustness in the clustering model. The WRCL module uses the similarity ratio between the query sample and the clustering centroid relative to the overall similarity of all samples with the same labels as the query sample. This ratio is used as the weight in the loss function to penalize incorrect clustering and improve pseudo-label generation accuracy. Extensive experiments on public ReID datasets—Market-1501, MSMT17, Veri776, CUHK03, and PersonX—demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Mingfu Xiong and Kaikang Hu and Zhongyuan Wang and Ruimin Hu and Khan Muhammad and Javier Del Ser and Xiaokang Yang and anbd Bin Sheng},
  doi          = {10.1109/TMM.2025.3590938},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive clustering and weighted regularization contrastive learning framework for unsupervised person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical token-aware cross-modality reconstruction for visible-infrared person re-identification. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3590933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) aims to query the same pedestrian's visible (infrared) images in the gallery set from the infrared (visible) images. VI-ReID not only needs to deal with the challenging factors like pose variation and occlusion, but also requires handling the large modality discrepancy. Previous methods mainly focus on learning single-scale modality-shared features and do not effectively explore the multi-scale features of two modalities from both short-range and long-range perspectives. In order to solve these problems, this paper proposes a novel Hierarchical Token-Aware Cross-Modality Reconstruction (HTCR) network to significantly mitigate the modality discrepancy for effective VI-ReID. The HTCR network consists of two main components, i.e., Hierarchical Token-aware Fusion (HTF) and Cross-modality Feature Reconstruction (CFR). The HTF module first bidirectionally exchanges the short-range and long-range multi-scale modality-shared features with a few learnable tokens to achieve discriminative pedestrian features by making full use of the advantages of both Convolutional Neural Network (CNN) and Transformer. Moreover, the CFR module reconstructs global and local pedestrian features of one modality by using the token sequence of the other modality with multi-scale cues to further explore the relationship between the two distinct modalities and alleviate the modality discrepancy. In addition, the Modality-shared feature Reconstruction (MR) loss is leveraged to reduce the noises between the reconstructed and the target features. Experimental results indicate that the proposed HTCR can significantly improve the VI-ReID performance and outperform the state-of-the-art methods on the cross-modality SYSU-MM01, RegDB, and LLCM datasets.},
  archive      = {J_TMM},
  author       = {Si Chen and Liuxiang Qiu and Da-Han Wang and Wentao Zhu and Yang Hua and Yan Yan},
  doi          = {10.1109/TMM.2025.3590933},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical token-aware cross-modality reconstruction for visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-domain generalized object detection with frequency whitening and contrastive learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-Domain Generalization Object Detection (Single-DGOD) refers to training a model with only one source domain, enabling the model to generalize to any unseen domain. For instance, a detector trained on a sunny daytime dataset should also perform well in scenarios such as rainy nighttime. The main challenge is to improve the detector's ability to learn the domain-invariant representation (DIR) while removing domain-specific information. Recent progress in Single-DGOD has demonstrated the efficacy of removing domain-specific information by adjusting feature distributions. Nonetheless, simply adjusting the global feature distribution in Single-DGOD task is insufficient to learn the potential relationship from sunny to adverse weather, as these ignore the significant domain gaps between instances across different weathers. In this paper, we propose a novel object detection method for more robust single-domain generalization. In particular, it mainly consists of a frequency-aware selective whitening module (FSW) for removing redundant domain-specific information and a contrastive feature alignment module (CFA) for enhancing domain-invariant information among instances. Specially, FSW extracts the magnitude spectrum of the feature and uses a group whitening loss to selectively eliminate redundant domain-specific information in the magnitude. To further eliminate domain differences among instances, we apply the style transfer method for data augmentation and use the augmented data in the CFA module. CFA formulates both the original and the augmentd RoI features into a series of groups with different categories, and utilizes contrastive learning across them to facilitate the learning of DIR in various categories. Experiments show that our method achieves favorable performance on existing standard benchmarks.},
  archive      = {J_TMM},
  author       = {Xiaolong Guo and Chengxu Liu and Xueming Qian and Zhixiao Wang and Xubin Feng and Yao Xue},
  doi          = {10.1109/TMM.2025.3590915},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Single-domain generalized object detection with frequency whitening and contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid network for extended reality environments. <em>TMM</em>, 1-17. (<a href='https://doi.org/10.1109/TMM.2025.3590913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly evolving realm of Extended Reality (XR) demands high bandwidth and low-latency communication to support immersive experiences such as high-resolution 360-degree videos and real-time interactions in virtual reality gaming. In this study, “resources” are defined as digital assets essential for XR applications, divided into “static resources” (immutable media files such as textures and video segments) and “dynamic resources” (real-time user data and interactive elements crucial for user interactions). A primary challenge in XR environments is optimizing the delivery and caching of these resources within existing network infrastructures to enhance the Quality of Experience (QoE) for users. We introduce a novel hybrid network architecture that integrates resource caching, user-to-user communication, and central server oversight. This architecture not only ensures reliable delivery but also significantly reduces communication latency. Preliminary experiments, conducted under conditions where each node in the network has a 10% chance of failing at any given time, demonstrate that our approach enhances the delivery efficiency of static resources by 68%, affecting 38% of communications, with an increase in latency observed in 4% of cases by 22%. For dynamic resources, it reduces latency in 89% of the cases by an average of 30%, though 8% of cases experienced a 36% increase in latency. These results affirm the effectiveness of our architecture in enhancing user experience in XR environments under challenging network conditions.},
  archive      = {J_TMM},
  author       = {Hexu Xing and Torsten Braun},
  doi          = {10.1109/TMM.2025.3590913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hybrid network for extended reality environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dual video summarization: From dynamic keyframes to captions. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization and captioning condense content by selecting keyframes and generating language descriptions, integrating both visual and textual perspectives. Existing video-and-language learning models typically select multiple frames as proxies rather than analyzing all frames, which improves computational efficiency but may not adequately represent the original content without redundancy. In this paper, we propose an adaptive dual video summarization framework and demonstrate its effectiveness within the context of video captioning. Given the video frames, we extract visual representations using a video-domain fine-tuned ViT model to narrow the domain shift. The keyframes are summarized based on the frame-level scores. To minimize the number of keyframes while ensuring captioning quality, we introduce a cross-modal video summarizer that selects the most semantically consistent frames according to pseudo score labels. Furthermore, we incorporate an adaptive keyframe selector that determines the optimal number of keyframes based on the video's complexity and content, enhancing the framework's adaptability and generalization. The proposed adaptive keyframe selector enables the framework to handle diverse video content, making it more generalizable and applicable to real-world scenarios.We designed a ranking scheme to assess the video's static appearance and temporal dynamics from score-based and time-based perspectives. To conclude, we use a lightweight LSTM decoder to generate descriptions. Experimental results on the MSR-VTT, MSVD and VATEX benchmarks demonstrate that our adaptive dual video summarization framework can effectively convey the same semantic information as the original video while using a significantly reduced number of keyframes, leading to improved video captioning performance.},
  archive      = {J_TMM},
  author       = {Zhenzhen Hu and Ao Sun and Zhenshan Wang and Jia Li and Zijie Song and Richang Hong and Meng Wang},
  doi          = {10.1109/TMM.2025.3590936},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive dual video summarization: From dynamic keyframes to captions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROSA: A robust self-adaptive model for multimodal emotion recognition with uncertain missing modalities. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of online media has heightened the importance of multimodal emotion recognition (MER) in video analysis. However, practical applications often encounter challenges due to missing modalities caused by various interferences. It is difficult to predict the specific missing situations, such as the number and types of missing modalities. Current approaches to modality missing typically apply a uniform method to address various missing cases, which are insufficiently adaptive to dynamic conditions. For example, translation-based methods can efficiently complete missing text from audio, but generating audio or video features that retain the original emotional information from other modalities is challenging and may introduce additional noise. In this paper, we introduce ROSA, a novel robust self-adaptive model designed to address various missing cases with tailored approaches, leveraging available modalities effectively and reducing the introduction of additional noise. Specifically, the A-T Completion module based on the encoder-decoder architecture enables ROSA to generate missing raw text from audio rather than mere embedding representations, capturing more nuanced modal features. Additionally, we design the T-V Fusion module based on a vision-language large model for deep extraction and fusion of textual and visual features. Comprehensive experiments conducted on three widely used public datasets demonstrate the superiority and effectiveness of our model. ROSA outperforms other models in both fixed missing rate and fixed missing modality cases. The ablation studies further highlights the contribution of each designed module.},
  archive      = {J_TMM},
  author       = {Ziming Li and Yaxin Liu and Chuanpeng Yang and Yan Zhou and Songlin Hu},
  doi          = {10.1109/TMM.2025.3590929},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ROSA: A robust self-adaptive model for multimodal emotion recognition with uncertain missing modalities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual semantic contextualization network for multi-query image retrieval. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Query Image Retrieval (MQIR) aims to establish connections between vision and language by exploring fine-grained region-query alignments. It is still a challenging task owing to its intrinsical ambiguity, where a query matches with multiple semantically similar regions and introduces misleading noises. Although researchers have made great efforts to alleviate the ambiguity in many retrieval-related tasks, there are few attempts considering this bottleneck in MQIR, which greatly limits present performance. To this end, we propose a novel Visual Semantic Contextualization Network (VSCN) to mitigate ambiguity by capturing the contextual knowledge within each image-text pair. Specifically, we first develop a Context Semantic Perception (CSP) module to capture the dual-level context, where a visual context transformer explores the intra-context within regions, and a cross-modal context transformer mines the inter-context among concatenated visual-linguistic embeddings. Then, to yield superior contextual understanding, we strengthen the connotations in context via a Context Semantic Interaction (CSI) module. Particularly, knowledge distillation is first employed to transfer the CLIP-guided semantic into the regional intra-context to complement the potential background information. Then, the intra-context & inter-context interaction is conducted via the self-attention mechanism to link the dual-level context and obtain the interacted contextual knowledge. Our method is evaluated on the Visual Genome dataset and substantially outperforms the state-of-the-art methods (30.3% improvements on Recall@1 in the first round). Our source codes will be released at https://github.com/zhli-cs/VSCN.},
  archive      = {J_TMM},
  author       = {Zhong Ji and Zhihao Li and Yan Zhang and Yanwei Pang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3590927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual semantic contextualization network for multi-query image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR-HMR: 3D human mesh recovery from LiDAR. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3590928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human mesh recovery (HMR) holds significant utility in many applications. Studying HMR involving various types of sensors is necessary, as it enables the acquisition of human meshes in diverse scenes. Unlike HMR based on RGB images, HMR based on LiDAR has received considerably less attention in previous works. The major challenge in estimating human poses and meshes from sparse point clouds lies in the sparsity, noise, and incompletion of LiDAR point clouds. To address these challenges, we propose a LiDAR-based 3D human mesh recovery algorithm, called LiDAR-HMR. This algorithm involves estimating a sparse representation of a human (3D human pose) and gradually reconstructing the body mesh. To better leverage the 3D structural information of point clouds, we propose a point-cloud-to-SMPL pipeline that uses the original point cloud features to guide the reconstruction. The experimental results on four publicly available datasets demonstrate the effectiveness of LiDAR-HMR. The codes are available at https://github.com/soullessrobot/LiDAR-HMR.},
  archive      = {J_TMM},
  author       = {Bohao Fan and Wenzhao Zheng and Jianjiang Feng and Jie Zhou},
  doi          = {10.1109/TMM.2025.3590928},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LiDAR-HMR: 3D human mesh recovery from LiDAR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XMusic: Towards a generalized and controllable symbolic music generation framework. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current stateof-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is: https://xmusicproject.github.io},
  archive      = {J_TMM},
  author       = {Sida Tian and Can Zhang and Wei Yuan and Wei Tan and Wenjie Zhu},
  doi          = {10.1109/TMM.2025.3590912},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {XMusic: Towards a generalized and controllable symbolic music generation framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural-enhanced rate adaptation and computation distribution for emerging mmwave multi-user 3D video streaming systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3590902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate multitask edge-user communication-computation resource allocation for $360^\circ$ video streaming in an edge-computing enabled millimeter wave (mmWave) multi-user virtual reality system. To balance the communication-computation trade-offs that arise herein, we formulate a video quality maximization problem that integrates interdependent multitask/multi-user action spaces and rebuffering time/quality variation constraints. We formulate a deep reinforcement learning framework for multi-task rate adaptation and computation distribution (MTRC) to solve the problem of interest. Our solution does not rely on a priori knowledge about the environment and uses only prior video streaming statistics (e.g., throughput, decoding time, and transmission delay), and content information, to adjust the assigned video bitrates and computation distribution, as it observes the induced streaming performance online. Moreover, to capture the task interdependence in the environment, we leverage neural network cascades to extend our MTRC method to two novel variants denoted as R1C2 and C1R2. We train all three methods with real-world mmWave network traces and $360^\circ$ video datasets to evaluate their performance in terms of expected quality of experience (QoE), viewport peak signal-to-noise ratio (PSNR), rebuffering time, and quality variation. We outperform state-of-the-art rate adaptation algorithms, with C1R2 showing best results and achieving $5.21-6.06$ dB PSNR gains, $2.18-2.70$x rebuffering time reduction, and $4.14-4.50$ dB quality variation reduction.},
  archive      = {J_TMM},
  author       = {Babak Badnava and Jacob Chakareski and Morteza Hashemi},
  doi          = {10.1109/TMM.2025.3590902},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural-enhanced rate adaptation and computation distribution for emerging mmwave multi-user 3D video streaming systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-grained vision-and-language model for medical image and text alignment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing interest in learning from paired medical images and textual reports highlights the need for methods that can achieve multi-grained alignment between these two modalities. However, most existing approaches overlook finegrained semantic alignment, which can constrain the quality of the generated representations. To tackle this problem, we propose the Multi-Grained Vision-and-Language Alignment (MGVLA) model, which effectively leverages multi-grained correspondences between medical images and texts at different levels, including disease, instance, and token levels. For disease-level alignment, our approach adopts the concept of contrastive learning and uses medical terminologies detected from textual reports as soft labels to guide the alignment process. At the instance level, we propose a strategy for sampling hard negatives, where images and texts with the same disease type but differing in details such as disease locations and severity are considered as hard negatives. This strategy helps our approach to better distinguish between positive and negative image-text pairs, ultimately enhancing the quality of our learned representations. For token-level alignment, we employ a masking and recovery technique to achieve finegrained semantic alignment between patches and sub-words. This approach effectively aligns the different levels of granularity between the image and language modalities. To assess the efficacy of our MGVLA model, we conduct comprehensive experiments on the image-text retrieval and phrase grounding tasks.},
  archive      = {J_TMM},
  author       = {Huimin Yan and Xian Yang and Liang Bai and Jiamin Li and Jiye Liang},
  doi          = {10.1109/TMM.2025.3590930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-grained vision-and-language model for medical image and text alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-city: A source-free domain adaptation method for city-scale point cloud semantic segmentation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3590934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {City-scale point cloud semantic segmentation is an important yet challenging task. Despite progress, existing methods rely heavily on point-wise annotations. An alternative solution is to apply the Unsupervised Domain Adaptation (UDA) approach. Recently, the 2D foundation model has achieved significant progress with training with internet-scale images. Therefore, adapting 2D foundation models to 3D City-scale point clouds is an attempting idea. Due to the data protection and storage issue, 2D source domain data is typically unavailable. Thus, we focus on Source-Free Domain Adaptation (SFDA) and propose a Source-Free City-scale point cloud semantic segmentation method, namely SF-City. Our method leverages knowledge from 2D pre-trained models to generate point-wise pseudo labels for training a 3D semantic segmentation network. We convert point clouds into remote-sensing-like images using Bird's-Eye-View (BEV) projection. However, directly using source models for pseudo label generation is hindered by domain gaps such as viewpoint variations, concept divergences, and geometry loss. To tackle these problems, we propose a Multi-scale Content Feature Extractor (MCFE) to extract holistic and contextual feature representations. Then, an Uncertainty-guided Inter-Model Feature Integrator (UIFI) is introduced to integrate inherent knowledge across source models. Furthermore, the Geometric-guided Pseudo Label Generator (GPLG) is leveraged to introduce geometric information to regulate pseudo labels. Through extensive experiments on two public benchmarks, SF-City demonstrates superior performance, achieving an mIoU of 28.8% on the SensatUrban dataset, outperforming recent state-of-the-art methods CLIPFO3D by about 6.3%.},
  archive      = {J_TMM},
  author       = {Yan Liu and Hongyuan Zhu and Yinjie Lei and Hao Liu and Yun Pei and Yulan Guo},
  doi          = {10.1109/TMM.2025.3590934},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SF-city: A source-free domain adaptation method for city-scale point cloud semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-projection distilling knowledge for omnidirectional image quality assessment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, virtual reality technology is advancing rapidly and becoming increasingly matured. Omnidirectional images have integrated into the daily lives of many individuals. However, these images are susceptible to irreversible distortion during the encoding and transmission processes. Given the unique characteristics of deformation and distortion in omnidirectional images, the development of a quality assessment method is crucial. To ensure that our network not only delivers efficient and stable performance but also maintains a minimal parameter count, we have integrated the concept of knowledge distillation into our network. This involves utilizing a full-reference (FR) teacher network to guide the training of a no-reference (NR) student network by cross-projection distilling knowledge. To specifically implement this method, a Dual Projection Format Fusion (DPFF) module is specifically designed to complement and integrate the mutual fusion of the two projection formats of omnidirectional images. In the design of our knowledge distillation process and loss function, we have introduced a review mechanism to enhance the performance and efficiency of response-based knowledge, as well as utilized intermediate fusion features to improve the effectiveness of feature-based knowledge. These components are combined to formulate the final loss function. Experimental results validate the superiority of our proposed model over existing FR and NR methods when evaluated on four omnidirectional image data-bases. This highlights the effectiveness of our proposed model in elevating the quality assessment of omnidirectional images.},
  archive      = {J_TMM},
  author       = {Huixin Hu and Feng Shao and Hangwei Chen and Xiongli Chai and Qiuping Jiang},
  doi          = {10.1109/TMM.2025.3590920},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-projection distilling knowledge for omnidirectional image quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FoodLMM: A versatile food assistant using large multi-modal model. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3590924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Multi-modal Models (LMMs) have made impressive progress in many vision-language tasks. Nevertheless, the performance of general LMMs in specific domains is still far from satisfactory. This paper proposes FoodLMM, a versatile food assistant based on LMMs with various capabilities, including food recognition, ingredient recognition, recipe generation, nutrition estimation, food segmentation, and multi-round conversation. To facilitate FoodLMM in dealing with tasks beyond pure text output, we introduce a series of novel task-specific tokens and heads, enabling the model to predict food nutritional values and multiple segmentation masks. We adopt a two-stage training strategy. In the first stage, we utilize multiple public food benchmarks for multi-task learning by leveraging the instruct-following paradigm. In the second stage, we construct a multi-round conversation dataset and a reasoning segmentation dataset to fine-tune the model, enabling it to conduct professional dialogues and generate segmentation masks based on complex reasoning in the food domain. Our fine-tuned FoodLMM achieves state-of-the-art results across several food benchmarks. Our code, models, and datasets are available at https://github.com/YuehaoYin/FoodLMM.},
  archive      = {J_TMM},
  author       = {Yuehao Yin and Huiyan Qi and Bin Zhu and Jingjing Chen and Yu-Gang Jiang and Chong-Wah Ngo},
  doi          = {10.1109/TMM.2025.3590924},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FoodLMM: A versatile food assistant using large multi-modal model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plug-in open-set cross-modal hashing. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3581813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Cross-Modal Hashing (UCMH) models the intrinsic semantic correlations across different modalities to generate binary hash codes, facilitating efficient cross-modal retrieval. This technology offers notable advantages, such as independence from labeled data and superior generalization capabilities compared to supervised methods. However, most UCMH methods are designed for closed-set retrieval scenarios and have difficulty generalizing to open multi-modal data, which is common in real-world retrieval settings. This limitation hampers their performance in open retrieval tasks, particularly when these tasks involve novel categories. To address the above issue, we propose an Open-set Cross-Modal Hashing (OCMH) method, which enhances the generalization capability of trained UCMH models in an efficient plug-in manner for open cross-modal retrieval. Our method enables the model to learn from novel categories in open-set scenarios by increasing the pre-defined hash code length, while simultaneously preventing the catastrophic forgetting of trained knowledge from the closed-set domain using basic hash codes. Additionally, we introduce a historical-category detection module and an asymmetric optimization strategy to support the joint learning of basic and increased hash codes by replaying detected samples related to historical categories. By plugging our proposed method into several representative UCMH methods on three widely used datasets, experimental results show that the enhanced UCMH methods achieve superior retrieval performance in both open-set and closed-set scenarios. The source code is available at: https://github.com/WangBowen7/OCMH/.},
  archive      = {J_TMM},
  author       = {Bowen Wang and Lei Zhu and Fengling Li and Hui Cui and Jingjing Li},
  doi          = {10.1109/TMM.2025.3581813},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Plug-in open-set cross-modal hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTR: A unified deep tensor representation framework for multimedia data recovery. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3581777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the transform-based tensor representation has attracted increasing attention in multimedia data (e.g., images and videos) recovery problems, which consists of two indispensable components, i.e., the transform and the characterization. Previously, the development of transform-based tensor representation has focused mainly on the transform perspective. Although several attempts have considered shallow matrix factorization (e.g., singular value decomposition and nonnegative matrix factorization) for characterizing the frontal slices of the transformed tensor (termed the latent tensor), the faithful characterization perspective has been underexplored. To address this issue, we propose a unified Deep Tensor Representation (DTR) framework by synergistically combining the deep latent generative module and the deep transform module. Especially, the deep latent generative module can faithfully generate the latent tensor as compared with shallow matrix factorization. The new DTR framework not only allows us to better understand the classical shallow representations but also leads us to explore new representations. To examine the representation capability of the proposed DTR, we consider the representative multidimensional data recovery task and suggest an unsupervised DTR-based multidimensional data recovery model. Extensive experiments demonstrate that DTR achieves superior performance compared to the state-of-the-art methods from both quantitative and qualitative aspects, especially for fine detail recovery.},
  archive      = {J_TMM},
  author       = {Ting-Wei Zhou and Xi-Le Zhao and Jian-Li Wang and Yi-Si Luo and Min Wang and Xiao-Xuan Bai and Hong Yan},
  doi          = {10.1109/TMM.2025.3581777},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTR: A unified deep tensor representation framework for multimedia data recovery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical uncertainty-aware salient object detection for $360 ^{\circ }$ images via bi-projection collaborative learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3581812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$360^{\circ }$ salient object detection has recently received much attention for 3D scene perception owing to its omnidirectional field of view (FoV). The capability of recognizing salient objects of $360^{\circ }$ images remains technically challenging due to severe spherical distortion. In this paper, we develop a hierarchical uncertainty-aware $360^{\circ }$ image salient object detection methodology that explicitly explores the geometric and spatial complementary coherence of Tangent projection (TP) and Equirectangular projection (ERP) by a collaborative learning strategy. Concretely, to mitigate spherical distortion, we first intend to learn saliency-related features from less-distorted tangent images, in which a deformation-aware attention block is introduced to mitigate the geometric distortion caused by projecting a $360^{\circ }$ image onto a 2D plane. However, the discrepancies among tangent images pose a new challenge to $360^{\circ }$ image salient object detection. To tackle this issue and achieve accurate localization for salient objects of all sizes, we design a spatial-frequency saliency feature aggregation module to leverage fast Fourier convolution to capture global contextual information from ERP images, such that obtaining more representative saliency features. Moreover, a hierarchical uncertainty-aware bi-projection consistency learning module with strong local-global information embedding capabilities is constructed, which learns the geometric and spatial correlations between tangent images and ERP images via a collaborative learning strategy. Ultimately, salient object maps are produced for $360^{\circ }$ images on the basis of the merged saliency features driven by the uncertainty. Extensive experiments show that our developed method improves ${\mathrm{F}}_\beta ^{\sigma }$ by an average of 31.67% compared to twenty existing advanced methods on the publicly available 360-SOD dataset.},
  archive      = {J_TMM},
  author       = {Qiudan Zhang and Kaiyu Ji and Jie Zhang and Xu Wang and Zhaoqing Pan and Jianmin Jiang},
  doi          = {10.1109/TMM.2025.3581812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical uncertainty-aware salient object detection for $360 ^{\circ }$ images via bi-projection collaborative learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEHand: Deformable encoding for photo-realistic free-view and free-pose hand rendering. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3581749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input encoding has proven crucial in the success of methods based on neural radiance field. Compared to the literature on general static scene modeling, input encoding for dynamic hand modeling has been less explored. However, this aspect is critical to the modeling of deformation and rendering, as it maps a sampled point in space to the representation containing all the information associated with dynamic hand for inferring the geometry and appearance property of this point. The design of input encoding determines how well the neural network can learn for photo-realistic hand rendering. We offer an in-depth examination of this key component and introduce DEHand, a new representation utilizing Deformable Encoding for photo-realistic free-view and free-pose Hand rendering. DEHand leverages deformable encoding with a latent code map to achieve high-quality, pose-controlled rendering. Deformable encoding is achieved by adapting static input encoding techniques for the view synthesis of dynamic hands, using parametric hand mesh model as a proxy to construct encodings that map sampled points into a space capable of integrating over different poses and providing rich information for hand modeling. Our findings demonstrate that with our deformable encoding, a single Multilayer Perceptron (MLP) can achieve high-quality dynamic hand rendering, learning solely from images. Extensive experiments on InterHand2.6M validate the superior rendering quality of our method and the effectiveness of each component in our design.},
  archive      = {J_TMM},
  author       = {Yunzhi Teng and Xiaoke Huang and Kejie Li and Xiao-Ping Zhang and Yansong Tang},
  doi          = {10.1109/TMM.2025.3581749},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DEHand: Deformable encoding for photo-realistic free-view and free-pose hand rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WI3D: Weakly incremental 3D detection via vision foundation models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3581776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental 3D object detection demands a 3D detector to locate and recognize novel categories in a stream fashion while preserving its base detection ability. However, existing methods require delicate 3D annotations for learning novel categories, resulting in significant labeling costs. To this end, we explore a label-efficient approach called Weakly Incremental 3D Detection (WI3D), which teaches a 3D detector to learn incrementally with off-the-shelf vision foundation models. We propose a novel dual-teaching framework incorporating both intra-modal and inter-modal knowledge from pseudo labels and feature space. Specifically, our framework features a class-agnostic pseudo-label refinement module, designed for the generation of high-quality 3D pseudo labels. This module is built on a lightweight transformer that models the spatial relationships between pseudo labels and their interactions with rich contextual information in point clouds. Additionally, we introduce a cross-modal knowledge transfer module to enhance the representation learning of novel classes, along with a reweighting knowledge distillation strategy that dynamically assesses and distills knowledge from previously learned categories. Extensive experiments show that our approach can efficiently learn novel concepts while preserving knowledge of base classes in WI3D scenarios, and surpass baseline approaches on both SUN-RGBD and ScanNet.},
  archive      = {J_TMM},
  author       = {Mingsheng Li and Sijin Chen and Shengji Tang and Hongyuan Zhu and Yanyan Fang and Xin Chen and Zhuoyuan Li and Fukun Yin and Tao Chen},
  doi          = {10.1109/TMM.2025.3581776},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WI3D: Weakly incremental 3D detection via vision foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DGeoDet: General-purpose geometry-aware image-based 3D object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3581780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection approach that effectively handles single- and multi-view RGB images in indoor and outdoor environments, showcasing its general-purpose applicability. The key challenge for image-based 3D object detection tasks is the lack of 3D geometric cues, which leads to ambiguity in establishing correspondences between images and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D geometric representations in both explicit and implicit manners based on predicted depth information. Specifically, we utilize the predicted depth to learn voxel occupancy and optimize the voxelized 3D feature volume explicitly through the proposed voxel occupancy attention. To further enhance 3D awareness, the feature volume is integrated with an implicit 3D representation, the truncated signed distance function (TSDF). Without requiring supervision from 3D signals, we significantly improve the model's comprehension of 3D geometry by leveraging intermediate 3D representations and achieve end-to-end training. Our approach surpasses the performance of state-of-the-art image-based methods on both single- and multi-view benchmark datasets across diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19 $\text{AP}_{\text{3D}}$@0.7 improvement on the KITTI dataset. The project page is available at: https://cindy0725.github.io/3DGeoDet/},
  archive      = {J_TMM},
  author       = {Yi Zhang and Yi Wang and Yawen Cui and Lap-Pui Chau},
  doi          = {10.1109/TMM.2025.3581780},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3DGeoDet: General-purpose geometry-aware image-based 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3581811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of visual and textual data in Vision-Language Pre-training (VLP) models is crucial for enhancing vision-language understanding. However, the adversarial robustness of these models, especially in the alignment of image-text features, has not yet been sufficiently explored. In this paper, we introduce a novel gradient-based multimodal adversarial attack method, underpinned by contrastive learning, to improve the transferability of multimodal adversarial samples in VLP models. This method concurrently generates adversarial texts and images within imperceptive perturbation, employing both image-text and intra-modal contrastive loss. We evaluate the effectiveness of our approach on image-text retrieval and visual entailment tasks, using publicly available datasets in a black-box setting. Extensive experiments indicate a significant advancement over existing single-modal transfer-based adversarial attack methods and current multimodal adversarial attack approaches.},
  archive      = {J_TMM},
  author       = {Youze Wang and Wenbo Hu and Yinpeng Dong and Hanwang Zhang and Hang Su and Richang Hong},
  doi          = {10.1109/TMM.2025.3581811},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFCM-AEG: Source-free cross-modal adversarial example generation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3581781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel task of source-free cross-modal adversarial example generation, which generates adversarial examples based on textual descriptions of attackers. This task has two challenges as follows. First, how to generate adversarial examples when the clean examples are missing or inaccessible. Second, how to achieve fine-grained custom adversarial example generation according to the semantic descriptions of the attackers. Existing adversarial example generation methods can not effectively deal with these two challenges. To address these challenges, we propose a Source-Free Cross-Modal Adversarial Example Generation framework, abbreviated as SFCM-AEG. Within the SFCM-AEG model, we firstly leverage a pre-trained GPT as a simulator to construct textual descriptions of attackers by labels. Following this, we employ a diffusion model to synthesize an image that aligns with the generated textual description. Finally, the generated images are converted into adversarial examples using an adversarial example generation method. Experimental results demonstrate that our proposed SFCM-AEG method can generate adversarial examples with customized semantic descriptions, without relying on clean examples, while achieving strong attack performance in a white-box setting.},
  archive      = {J_TMM},
  author       = {Yan Gan and Xinyao Xiao and Tao Xiang and Chengqian Wu and Deqiang Ouyang},
  doi          = {10.1109/TMM.2025.3581781},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SFCM-AEG: Source-free cross-modal adversarial example generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multi-stage tracking via multi-scale and multi-level representation learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3581755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to learn multi-scale and multi-level representations is crucial for robust tracking. However, most current one-stream structure based trackers with visual transformers (dubbed ViTs) cannot effectively capture multi-scale representations due to the structure of their adopted ViTs is non-hierarchical. Meanwhile, they often only use the output features from the final layer for predicting results (i.e., ignoring the utilization of low-level features from the shallow layers) which may result in a certain degree of lacking multi-level representation learning ability. To address these issues, we propose a robust multi-stage tracker that effectively combines the advantages of both hierarchical and one-stream structured ViT as a tracking backbone to improve the multi-scale and multi-level representation learning abilities. Specifically, first of all, we design a hierarchical tracker with a three-stage backbone. In the first two stages of our tracker, we utilize a dual-branch structure to obtain multi-scale features of the template and search region separately. Especially, We design the local scale awareness modules based on simple MLP layers to capture multi-scale features. These modules remove complex operations such as convolutions or shifted window attentions, thus avoiding the performance degradation caused by traditional hierarchical ViTs. In the third stage (i.e. the main stage), we construct a global encoder based on the one-stream ViT to achieve efficient feature extraction and feature interaction for our tracker. Then, we design a multi-level feature integration module in the main stage to explicitly utilize the representation information learned from the shallow layers and fuse them with the features of the final layer to obtain multi-level representation information. Lastly, benefit from the these designs, our tracker can effectively capture more multi-scale and multi-level representations for robust tracking. Comprehensive experiments on GOT-10k, LaSOT, LaSOT$_{ext}$, TNL2K, UAV123, TrackingNet and VOT2020 benchmarks validate the effectiveness and robustness of our method.},
  archive      = {J_TMM},
  author       = {Ning Li and Bineng Zhong and Qihua Liang and Zhiyi Mo and Shuxiang Song},
  doi          = {10.1109/TMM.2025.3581755},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust multi-stage tracking via multi-scale and multi-level representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-enhanced video moment retrieval with large language models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3581797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for Video Moment Retrieval (VMR) struggle to align complex situations involving specific environmental details, character descriptions, and action narratives. To tackle this issue, we propose a Large Language Model-guided Moment Retrieval (LMR) approach that employs the extensive knowledge of Large Language Models (LLMs) to improve video context representation as well as cross-modal alignment, facilitating accurate localization of target moments. Specifically, LMR introduces a context enhancement technique with LLMs to generate crucial target-related context semantics. These semantics are integrated with visual features for producing discriminative video representations. Finally, a language-conditioned transformer is designed to decode free-form language queries, on the fly, using aligned video representations for moment retrieval. Extensive experiments demonstrate that LMR achieves state-of-the-art results, outperforming the nearest competitor by up to 3.28% and 4.06% on the challenging QVHighlights and Charades-STA benchmarks, respectively. More importantly, the performance gains are significantly higher for localization of complex queries.},
  archive      = {J_TMM},
  author       = {Weijia Liu and Bo Miao and Jiuxin Cao and Xuelin Zhu and Jiawei Ge and Bo Liu and Mehwish Nasim and Ajmal Mian},
  doi          = {10.1109/TMM.2025.3581797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Context-enhanced video moment retrieval with large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TalkCLIP: Talking head generation with text-guided expressive speaking styles. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3581808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-driven talking head generation has drawn growing attention. To produce talking head videos with desired facial expressions, previous methods rely on extra reference videos to provide expression information, which may be difficult to find and hence limits their usage. In this work, we propose TalkCLIP, a framework that can generate talking heads where the expressions are specified by natural language, hence allowing for specifying expressions more conveniently. To model the mapping from text to expressions, we first construct a text-video paired talking head dataset where each video has diverse text descriptions that depict both coarse-grained emotions and fine-grained facial movements. Leveraging the proposed dataset, we introduce a CLIP-based style encoder that projects natural language-based descriptions to the representations of expressions. TalkCLIP can even infer expressions for descriptions unseen during training. TalkCLIP can also use text to modulate expression intensity and edit expressions. Extensive experiments demonstrate that TalkCLIP achieves the advanced capability of generating photo-realistic talking heads with vivid facial expressions guided by text descriptions.},
  archive      = {J_TMM},
  author       = {Yifeng Ma and Suzhen Wang and Yu Ding and Bowen Ma and Tangjie Lv and Changjie Fan and Zhipeng Hu and Zhidong Deng and Xin Yu},
  doi          = {10.1109/TMM.2025.3581808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TalkCLIP: Talking head generation with text-guided expressive speaking styles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible optimal transport with contrastive graphical modeling for multimodal hate detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3581795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal hate detection plays a crucial role in maintaining harmonious online environments by identifying harmful content, such as hateful memes. Although previous research has made significant progress in detecting explicit hate speech, there remains a critical gap in analyzing implicit hate, which is particularly challenging due to the absence of explicit harmful text claims or demographic visual cues. Despite the promising results based on cross-modal attention, previous methods may suffer from the distributional modality gap caused by the non-literal associations between multimodal elements, which lacks apparent alignment in implicit hateful contents. In this work, we propose a novel framework: Flexible Optimal Transport (FLOT) to capture the non-literal cross-modal alignment for multimodal hate in the context of memes. FLOT formulates the problem of cross-modal alignment as finding optimal transportation plans, which leverages a kernel method to capture complementary information from multiple modalities. The kernel embeddings reproduce a kernel Hilbert space (RKHS) to serve as a non-linear transformation of alignment, which effectively reduces the distributional modality gap with more interpretability. Moreover, we established topological structures with contrastive modeling for the aligned representations, which are optimized to achieve comprehensive alignment between different modalities, and facilitate local reasoning based on multimodal elements. Experimental results have demonstrated that our FLOT achieved state-of-the-art performance on three publicly available benchmark datasets. Furthermore, extensive qualitative analysis confirms the superior ability of FLOT in capturing implicit cross-modal alignment.},
  archive      = {J_TMM},
  author       = {Linhao Zhang and Li Jin and Xiaoyu Li and Xian Sun and Xin Wang and Zequn Zhang and Jian Liu and Zhicong Lu and Guangluan Xu},
  doi          = {10.1109/TMM.2025.3581795},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Flexible optimal transport with contrastive graphical modeling for multimodal hate detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized skewed histogram shifting based reversible data hiding by differential evolution. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3581804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skewed histogram shifting (SHS) is an efficient scheme in reversible data hiding (RDH) research. By employing a pair of symmetric predictors which averages part of sorted pixels around the to-be-predicted pixel, two skewed histograms are generated. With the embedding and shifting directions toward the short tail of the two histograms, SHS reduces many invalid modifications. However, the design of the symmetric predictors pair is strictly constrained, which seriously degrades the performance on both embedding capacity and distortion of this SHS scheme. In this work, we propose a generalized SHS model to remove the weight and symmetry constraints. With the help of differential evolution algorithm, the optimized parameters are obtained in a short period of time, avoiding wasting time using exhaustive search. What is more, adaptive pairwise mapping and embedding bin selection are also realized by adding parameters into the evolutionary process, which greatly improve the embedding performance without increasing too much computational complexity. Experiments demonstrate the superiority of our method by comparing it with state-of-the-art RDH schemes.},
  archive      = {J_TMM},
  author       = {Guojun Fan and Lei Lu and Zijing Li and Ping Li and Quan Zhou and Zhibin Pan},
  doi          = {10.1109/TMM.2025.3581804},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generalized skewed histogram shifting based reversible data hiding by differential evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointMT: Efficient point cloud analysis with hybrid MLP-transformer architecture. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3581747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, point cloud analysis methods based on the Transformer architecture have made significant progress, particularly in the context of multimedia applications such as 3D modeling, virtual reality, and autonomous systems. However, the Transformer architecture's high computational demands limit its scalability and deployment on resource-constrained platforms, hindering its practical applications in on-device multimedia processing. To address this challenge, we propose an efficient point cloud analysis architecture, Point MLP-Transformer (PointMT). This study tackles the quadratic complexity of the self-attention mechanism by introducing a linear complexity local attention mechanism for effective feature aggregation. Additionally, to counter the Transformer's focus on token differences while neglecting channel differences, we introduce a parameter-free channel temperature adaptation mechanism that adaptively adjusts the attention weight distribution in each channel, enhancing the precision of feature aggregation. To improve the Transformer's slow convergence speed due to the limited scale of point cloud datasets, we propose an MLP-Transformer hybrid module, which significantly enhances the model's convergence speed. Furthermore, to boost the feature representation capability of point tokens, we refine the classification head, enabling point tokens to directly participate in prediction. Experimental results on multiple evaluation benchmarks demonstrate that PointMT achieves performance comparable to state-of-the-art methods while maintaining an optimal balance between performance and accuracy. This research provides an innovative solution for efficient point cloud analysis, offering significant potential for multimedia applications and other domains.},
  archive      = {J_TMM},
  author       = {Qiang Zheng and Chao Zhang and Jian Sun},
  doi          = {10.1109/TMM.2025.3581747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointMT: Efficient point cloud analysis with hybrid MLP-transformer architecture},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EraW-net: Enhance-refine-align W-net for scene-associated driver attention estimation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3565934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associating driver attention with driving scene across two fields of view is a challenging cross-domain perception problem, which requires comprehensive consideration of cross-view mapping, dynamic driving scene analysis and driver status tracking. Previous methods typically analyze a single view or map attention to the scene through a two-step projection, failing to exploit their implicit connections and establish accurate associations. Moreover, simple fusion modules are inadequate for modeling the complex relationships between the two views, making information integration complicated. To address these issues, we propose EraW-Net, a novel end-to-end framework for scene-associated driver attention estimation by aggregating information from dual views. This method enhances the most discriminative dynamic cues, refines feature representations, and facilitates semantically aligned cross-domain integration through a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive Filter Module (DAF-Module) is proposed to address the challenges of frequently changing driving environments by extracting vital regions. It suppresses the indiscriminately recorded dynamics and highlights crucial ones by innovative joint frequency-spatial analysis, enhancing the model's ability to parse complex dynamics. Additionally, to track driver states during non-fixed facial poses, we propose a Global Context Sharing Module (GCS-Module) to construct refined feature representations by capturing hierarchical features that adapt to various scales of head and eye movements. Finally, W-Net achieves systematic cross-view information integration through its unique two-stage decoding strategy, addressing semantic misalignment in heterogeneous data integration. Experiments demonstrate that the proposed method robustly and accurately estimates scene-associated driver attention on large public datasets.},
  archive      = {J_TMM},
  author       = {Jun Zhou and Chunsheng Liu and Faliang Chang and Wenqian Wang and Penghui Hao and Yiming Huang and Zhiqiang Yang},
  doi          = {10.1109/TMM.2025.3565934},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EraW-net: Enhance-refine-align W-net for scene-associated driver attention estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenSlot: Mixed open-set recognition with object-centric learning. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3565972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing open-set recognition (OSR) studies typically assume that each image contains only one class label, with the unknown test set (negative) having a disjoint label space from the known test set (positive), a scenario referred to as full-label shift. This paper introduces the mixed OSR problem, where test images contain multiple class semantics, with both known and unknown classes co-occurring in the negatives, leading to a more complex super-label shift that better reflects real-world scenarios. To tackle this challenge, we propose the OpenSlot framework, based on object-centric learning, which uses slot features to represent diverse class semantics and generate class predictions. The proposed anti-noise slot (ANS) technique helps mitigate the impact of noise (invalid or background) slots during classification training, addressing the semantic misalignment between class predictions and ground truth. We evaluate OpenSlot on both mixed and conventional OSR benchmarks. Without elaborate designs, our method not only excels existing approaches in detecting super-label shifts across OSR tasks, but also achieves state-of-the-art performance on conventional benchmarks. Meanwhile, OpenSlot can localize class objects without using bounding boxes during training, demonstrating competitive performance in open-set object detection and potential for generalization.},
  archive      = {J_TMM},
  author       = {Xu Yin and Fei Pan and Guoyuan An and Yuchi Huo and Zixuan Xie and Sung-Eui Yoon},
  doi          = {10.1109/TMM.2025.3565972},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OpenSlot: Mixed open-set recognition with object-centric learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor-based late fusion incomplete multiview clustering. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3565971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Late fusion-based algorithms have attracted extensive attention because of their low time and space complexity for handling in-complete multiview data. However, these methods have certain limitations. First, the basic clustering indicator matrices generated by incomplete views are susceptible to low-quality imputation. Second, traditional methods often fail to adequately consider the high-order correlations between these basic clustering indicator matrices, leading to suboptimal performance. Third, conventional methods focus primarily on improving speed, with less emphasis on enhancing clustering performance. To address these issues, we propose two novel models. The first is called tensor-based late fusion incomplete multiview clustering (TLF-IMVC-1). Specifical-ly, TLF-IMVC-1 first seeks a consensus clustering matrix from the basic clustering indicator matrices and subsequently imputes the incomplete portions of these matrices via the learned consen-sus matrix. This approach seamlessly integrates the clustering process with the imputation of missing elements into a unified framework. Furthermore, we construct a third-order tensor from these basic clustering matrices, constrained by the tensor nuclear norm, to capture their high-order correlations. Although this model is effective, it lacks proper guidance in the learning process of the basic clustering indicator matrices, making them susceptible to low-quality imputation. Therefore, we introduce the second novel model, i.e., TLF-IMVC-2, to address this issue. Specifically, TLF-IMVC-2 uses the learned consensus representation matrix as a new component to construct the third-order tensor. This strategy leverages the robust clustering structure inherent in the consensus matrix to guide the learning process of the basic clustering matrices. The experimental results demonstrate that both models outperform state-of-the-art methods in clustering.},
  archive      = {J_TMM},
  author       = {Xiaoxing Guo and Ming Yang and Gui-Fu Lu},
  doi          = {10.1109/TMM.2025.3565971},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tensor-based late fusion incomplete multiview clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking like human: Dynamic scene learning reasoning tracker in satellite videos. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3565976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In satellite video object tracking, the individual frame analysis method is usually used for target localization, ignoring informative cues of the dynamic scene. Temporal information could contribute to identifying the target from distractors. In this work, a novel dynamic scene learning reasoning tracker is proposed for satellite videos, which reasons over temporal dynamic information to derive the target location. It is inspired by the tracking pattern through human perception and reasoning. First, static-dynamic united analysis is designed to construct dynamic scenes by concatenating the static searching results along the temporal dimension. Second, the information of each response object is aggregated by wavelet transforms. Meanwhile, these scenes are projected into low-frequency and high-frequency subspaces, which could imitate different levels of perceptions of humans for scenes. Third, an object-aware reasoning transformer is proposed to utilize the temporal dynamics of input response objects. In each subspace, it models the mutual interactions between dynamic objects and further learns the intrinsic property of each object for target reasoning. Finally, to obtain the current reasoning result, inverse wavelet transforms are utilized to integrate the results of low-frequency and high-frequency subspaces. The effectiveness of the proposed method is validated on three public satellite video datasets, including SV248S, SkySat, and VISO. Qualitative and quantitative experimental results show that the proposed tracker outperforms 22 popular approaches in seven challenging tracking satellite scenarios.},
  archive      = {J_TMM},
  author       = {Xiaoyan Yang and Licheng Jiao and Yangyang Li and Xu Liu and Lingling Li and Puhua Chen and Fang Liu and Wenping Ma and Shuyuan Yang},
  doi          = {10.1109/TMM.2025.3565976},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tracking like human: Dynamic scene learning reasoning tracker in satellite videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment-enhanced graph-based sarcasm explanation in dialogue. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3565959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which play important roles in reflecting sarcasm that essentially involves subtle sentiment contrasts. Nevertheless, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sentiment inference module, where a heuristic utterance sentiment refinement strategy is devised. We then develop a module named Joint Cross Attention-based Sentiment Inference (JCA-SI) by extending the multimodal sentiment analysis model JCA to derive the joint sentiment label for each video-audio clip. Thereafter, we devise a context-sentiment graph to comprehensively model the semantic relations among the utterances, utterance sentiments, and video-audio sentiments, to facilitate sarcasm explanation generation. Extensive experiments on the publicly released dataset WITS verify the superiority of our model over cutting-edge methods.},
  archive      = {J_TMM},
  author       = {Kun Ouyang and Liqiang Jing and Xuemeng Song and Meng Liu and Yupeng Hu and Liqiang Nie},
  doi          = {10.1109/TMM.2025.3565959},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sentiment-enhanced graph-based sarcasm explanation in dialogue},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRIF: Data-free knowledge distillation via stable regulation and input filtering. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3565963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free knowledge distillation (DFKD) enables knowledge transfer from a pre-trained teacher to a student network without accessing the real dataset. However, generator-based DFKD methods struggle to ensure that the synthetic images accurately reflect the real dataset distribution. The update of the generator network relies heavily on teacher category guidance, but varying teacher prediction accuracy across categories leads to inconsistent synthetic image quality. Such variations introduce a distribution shift between synthetic and real datasets, negatively impacting student network performance during knowledge distillation. To address this challenge, we propose the SRIF, comprising two components: Student-Driven Flexible Filtering (SDFF) and Re-weighting for Independent Regularization (RIR). SDFF filters out synthetic images affected by the category distribution shift during data generation, producing a more reliable dataset. RIR, applied during distillation, encourages the student to learn stable causal relationships through sample reweighting. Both components flexibly integrate into existing DFKD frameworks, improving performance while reducing training costs.},
  archive      = {J_TMM},
  author       = {Zherui Zhang and Rongtao Xu and Changwei Wang and Shibiao Xu and Jie Zhou and Wenhao Xu and Longxiang Gao and Wenbo Xu and Li Guo},
  doi          = {10.1109/TMM.2025.3565963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SRIF: Data-free knowledge distillation via stable regulation and input filtering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive frame patching for FoV-based point cloud video streaming. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3565928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many XR applications require the delivery of volumetric video to users. Point Cloud has become a popular volumetric video format. A dense point cloud consumes much higher bandwidth than a 2D/360 $^{\circ }$ video frame. User Field of View (FoV) is more dynamic with 6-DoF movement than 3-DoF movement. To save bandwidth, FoV-adaptive streaming predicts a user's FoV and only downloads point cloud data falling in the predicted FoV. However, it is vulnerable to FoV prediction errors, which can be significant when a long buffer is utilized for smoothed streaming. In this work, we propose a multi-round progressive refinement framework for point cloud video streaming. Instead of sequentially downloading point cloud frames, our solution simultaneously downloads/patches multiple frames falling into a sliding time-window, leveraging the inherent scalability of octree-based point-cloud coding. The optimal rate allocation among all tiles of active frames are solved numerically using the heterogeneous tile rate-quality functions calibrated by the predicted user FoV. Multi-frame downloading/patching simultaneously takes advantage of the streaming smoothness resulting from long buffer and the FoV prediction accuracy at short buffer length. We evaluate our streaming solution using simulations driven by real point cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users. Our solution is robust against the bandwidth/FoV prediction errors, and can deliver high and smooth view quality in the face of bandwidth variations and dynamic user and point cloud movements.},
  archive      = {J_TMM},
  author       = {Tongyu Zong and Yixiang Mao and Chen Li and Yong Liu and Yao Wang},
  doi          = {10.1109/TMM.2025.3565928},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive frame patching for FoV-based point cloud video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot text-driven dynamic neural radiance fields stylization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3565983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven style transfer for Neural Radiance Fields (NeRFs) is an emerging research topic that leverages text descriptions instead of reference style images to apply style transfer. However, existing methods for stylizing NeRFs predominantly struggle to extend to 4D dynamic scenes, due to NeRFs' inherent limitation to static environments. Moreover, these current methods require training for each specific text input, which limits them to a single style description and significantly hampers generalizability and applications. In this paper, we introduce a novel approach to zero-shot text-driven 4D style transfer that adopts text inputs into the CLIP's style space with a canonical feature volume. Specifically, using geometric priors from pre-trained dynamic Neural Radiance Fields, we train a canonical feature volume by rendering feature maps under the supervision of a pre-trained VGG encoder. Then we utilize CLIP's multi-modal embedding to connect the text descriptions with style images and learn a canonical style transformation matrix in CLIP's feature space. Experiments show that our method achieves zero-shot text-driven style transfer for dynamic neural radiance fields and maintains good multi-view and cross-time consistency.},
  archive      = {J_TMM},
  author       = {Wanlin Liang and Hongbin Xu and Wanshui Gan and Wenxiong Kang},
  doi          = {10.1109/TMM.2025.3565983},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot text-driven dynamic neural radiance fields stylization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instruction-driven 3D facial expression generation and transition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3565929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/},
  archive      = {J_TMM},
  author       = {Anh H. Vo and Tae-Seok Kim and Hulin Jin and Soo-Mi Choi and Yong-Guk Kim},
  doi          = {10.1109/TMM.2025.3565929},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instruction-driven 3D facial expression generation and transition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of HEVC double compression based on deep representations of in-loop filtering and CU depth maps. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3565961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of HEVC (High Efficiency Video Coding) double compression detection, relocated I-frame (RI frame) detection and original GOP size estimation are two significant problems for video forensics. However, little research explores the interconnection between the two problems, and effective methods to resolve them are still lacking. In this paper, a novel feature model called In-loop Filtering and CU Depth Map (IFCDM) is proposed to accurately detect RI frames, and the intrinsic correlation between RI frames and GOP structure is explored, which can be used for original GOP size estimation. Theoretical and statistical analysis of HEVC recompression process is first carried out. Then, sub-features of HEVC in-loop filtering modes and CU partition depth are extracted, and transformed into grey-scale maps to construct IFCDM. A neural network, consisting of tiny Vision Transformer and LSTM, is trained to learn spatial and temporal representations of input features, and further derive the RI frame detection results. Finally, an adaptive periodic analysis algorithm is designed, to integrate the RI frame detection results and estimate the original GOP size of recompressed videos. Experiments show that our method can outperform the existing state-of-the-art methods in both frame level and video level.},
  archive      = {J_TMM},
  author       = {Xing Yan and Tanfeng Sun and Qiang Xu and Ke Xu and Xinghao Jiang},
  doi          = {10.1109/TMM.2025.3565961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detection of HEVC double compression based on deep representations of in-loop filtering and CU depth maps},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion direction awareness: A biomimetic dynamic capture mechanism for video prediction. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3565988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is an important yet challenging task that generates future frames based on previous observations. Despite recent progress, existing methods still suffer from motion blur, due to weak motion perception capabilities leading to uncertainty in motion direction. To address this, we propose a Motion Direction Awareness (MDA) mechanism inspired by the direction-selective mechanism in animal visual systems. Specifically, MDA can decompose complex motions into horizontal and vertical components, allowing dimension reduction and independent processing, thereby effectively enhancing motion perception and reducing uncertainty in predicted motion directions. Based on MDA, we design a multi-scale feature fusion network named MDANet for video prediction, which incorporates different scales of spatially encoded features in conjunction with MDA mechanism to extract the temporal evolution information of global and local spatial features. Extensive experiments on representative datasets demonstrate that MDANet can alleviate motion blurring, improving prediction accuracy and temporal consistency over state-of-the-art models. Furthermore, we validate the generalizability and effectiveness of our MDA mechanism by integrating it into other advanced models. The code is available at supplementary.},
  archive      = {J_TMM},
  author       = {Lianqiang Gan and Junyu Lai and Junhong Zhu and Huashuo Liu and Lianli Gao},
  doi          = {10.1109/TMM.2025.3565988},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Motion direction awareness: A biomimetic dynamic capture mechanism for video prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical distortion learning for fast lossy compression of point clouds. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3565958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of 3D point cloud applications requires efficient compression techniques for high-quality and low-latency services. Recently, learning-based point cloud compression models have made significant progress. However, geometric distortion resulting from downsampling limits the feature depth within large-scale point clouds, thereby constraining the receptive field and suppressing the redundant removal. Moreover, the issues of computational efficiency and reconstruction quality still persist in the compression of large-scale point clouds. To address these challenges, we propose a hierarchical distortion learning framework for end-to-end lossy compression of point clouds. First, we design a feature residual compression module to efficiently transmit shallow semantics between the encoder and the decoder, which enables a lightweight design of our framework. Second, we introduce a geometry residual compression module to progressively complement the reconstruction distortion, avoiding the accumulation of geometric distortion. By integrating these two modules and employing sufficient downsampling processes, we develop a high-performance framework with a significantly enlarged receptive field and low computational cost. Extensive experiments demonstrate that our method achieves state-ofthe- art performance in geometry lossy compression, while delivering competitive performance in joint geometry and color lossy compression with fast running speed. Code is available at https://github.com/pengpeng-yu/FastPCC.},
  archive      = {J_TMM},
  author       = {Pengpeng Yu and Ye Zhang and Fan Liang and Haoran Li and Yulan Guo},
  doi          = {10.1109/TMM.2025.3565958},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical distortion learning for fast lossy compression of point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAT-net: Structure-aware transformer-based attention fusion network for low-quality retinal fundus images enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3565935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ophthalmology diagnosis, high-fidelity fundus images are essential for disease diagnosis and intervention. However, many real-world clinical conditions may degrade the quality of the acquired images and thus affect clinical diagnostic accuracy. Traditional convolutional neural network-based retinal fundus image enhancement methods cannot always capture long-range dependencies, which reduces the overall visual quality of images, especially for real retinal fundus images. Furthermore, existing enhancement methods often fail to fully utilize low-resolution structural detail information, which potentially leads to inaccurate pivotal fundus vessel topology or capillary details. In this paper, we propose a novel Structure-Aware Transformer-based attention fusion Network (SAT-Net) for low-quality retinal fundus image enhancement. First, we introduce a Transformer-based attention fusion module which incorporates window-based self-attention and channel self-attention to capture global spatial dependencies and emphasize important feature channels simultaneously. This fusion significantly improves the overall perceptual quality of the image by enhancing both the local details and the uniformity of the non-vessel background regions. Second, we introduce a cross-quality knowledge distillation technique, which bridges the quality gap between high-quality and low-quality fundus images. By designing a high-performing teacher network to guide a lightweight student network, the student network enables to capture detailed features from low-quality fundus images, further preserving critical diagnostic information and fine topology structures. Moreover, we design a structure-aware multi-scale loss function by using a trainable subnetwork to obtain the edge structure from different scales to better constrain pivotal fundus vessel structure and capillary details. Comprehensive quantitative and qualitative experiments on both synthetic and real fundus image datasets robustly validate that our proposed SAT-Net outperforms other state-of-the-art methods for fundus image enhancement. In addition, extensive comparative experiments on both the vessel segmentation and Optic Disc/Cup detection tasks further validate the effectiveness and superiority of our proposed method.},
  archive      = {J_TMM},
  author       = {Yang Wen and Bin Luo and Wuzhen Shi and Jianhua Ji and Wenming Cao and Xiaokang Yang and Bin Sheng},
  doi          = {10.1109/TMM.2025.3565935},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SAT-net: Structure-aware transformer-based attention fusion network for low-quality retinal fundus images enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bring adaptive binding prototypes to generalized referring expression segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3565964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Expression Segmentation (RES), which aims to identify and segment objects based on natural language expressions is garnering increased research attention. While substantial progress has been made in RES, the emergence of Generalized Referring Expression Segmentation (GRES) introduces new challenges by allowing the expressions to describe multiple objects or lack specific object references. Existing RES methods usually rely on sophisticated encoder-decoder and feature fusion modules, and have difficulty generating class prototypes that match each instance individually when confronted with the complex referent and binary labels of GRES. In this paper, reevaluating the differences between RES and GRES, we propose a novel Model with Adaptive Binding Prototypes (MABP) that adaptively binds queries to object features in the corresponding region. It enables different query vectors to match instances of different categories, or different parts of the same instance, significantly expanding the decoder's flexibility, dispersing global pressure across all the queries, and easing the demands on the encoder. The experimental results demonstrate that MABP significantly outperforms the state-of-the-art methods in all three splits on the gRefCOCO dataset. Moreover, MABP outperforms the state-of-the-art methods on the RefCOCO+ and G-Ref datasets, and achieves very competitive results on RefCOCO. The code is available at https://github.com/buptLwz/MABP.},
  archive      = {J_TMM},
  author       = {Weize Li and Zhicheng Zhao and Haochen Bai and Fei Su},
  doi          = {10.1109/TMM.2025.3565964},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bring adaptive binding prototypes to generalized referring expression segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-task mutual reinforcing embedded joint video paragraph retrieval and grounding. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3565981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Paragraph Grounding (VPG) aims to precisely locate the most appropriate moments within a video that are relevant to a given textual paragraph query. However, existing methods typically rely on large-scale annotated temporal labels and assume that the correspondence between videos and paragraphs is known. This is impractical in real-world applications, as constructing temporal labels requires significant labor costs, and the correspondence is often unknown. To address this issue, we propose a Dual-task Mutual Reinforcing Embedded Joint Video Paragraph Retrieval and Grounding method (DMR-JRG). In this method, retrieval and grounding tasks are mutually reinforced rather than being treated as separate issues. DMR-JRG mainly consists of two branches: a retrieval branch and a grounding branch. The retrieval branch uses inter-video contrastive learning to roughly align the global features of paragraphs and videos, reducing modality differences and constructing a coarse-grained feature space to break free from the need for correspondence between paragraphs and videos. Additionally, this coarse-grained feature space further facilitates the grounding branch in extracting fine-grained contextual representations. In the grounding branch, we achieve precise cross-modal matching and grounding by exploring the consistency between local, global, and temporal dimensions of video segments and textual paragraphs. By synergizing these dimensions, we construct a fine-grained feature space for video and textual features, greatly reducing the need for large-scale annotated temporal labels. Meanwhile, we design a grounding reinforcement retrieval module (GRRM) that brings the coarse-grained feature space of the retrieval branch closer to the fine-grained feature space of the grounding branch, thereby reinforcing retrieval branch through grounding branch, and finally achieving mutual reinforcement between tasks. Extensive experiments on three challenging datasets demonstrate the effectiveness of our proposed method. The code is available at https://github.com/X7J92/DMR-JRG.},
  archive      = {J_TMM},
  author       = {Mengzhao Wang and Huafeng Li and Yafei Zhang and Jinxing Li and Minghong Xie and Dapeng Tao},
  doi          = {10.1109/TMM.2025.3565981},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-task mutual reinforcing embedded joint video paragraph retrieval and grounding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGDefect: A mask-guided high-quality defect image generation method for improving defect inspection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3565978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based defect inspection methods have achieved promising performance, which usually relies on a large number of well-labeled training samples. However, it requires much effort to obtain enough annotated samples especially pixel-level annotations in practical production. Generative adversarial networks (GANs) can be utilized to generate defect samples. However, training GANs typically requires a large amount of defect data and most of them cannot generate defect samples with pixel-level annotations. In this paper, we present a Mask-Guided Defect image generation method, called MGDefect, which can generate high-quality defect samples with pixel-level annotations and effectively improves the performance of downstream tasks. Specifically, MGDefect consists of a Mask-Guided Defect Generation GAN (MGDG-GAN) and a Defect Mask GAN (DM-GAN). MGDG-GAN generates images containing defects with specific locations, shapes, and sizes via mask guidance and the dual discrimination for defects at the region level and image level. DM-GAN aims to generate diverse and rational masks for MGDG-GAN. It also adopts region-level and image-level dual discrimination for masks to generate compatible masks with the target objects. MGDG-GAN mainly focuses on generating local defect regions and DM-GAN specializes in generating masks, which are both trained on limited defect samples and abundant normal samples. Experiments conducted on the MVTec AD, DAGM 2007, and KolektorSDD2 benchmark datasets demonstrate that our method achieves promising results compared with other state-of-the-art approaches. Meanwhile, the generated defect samples significantly improve the performance of defect inspection tasks including classification and segmentation. Specifically, our method achieves KID$\times 10^{3}$/IS scores of 48.35/2.27 on MVTec AD, 15.37/2.44 on DAGM 2007, and 19.70/2.01 on KolektorSDD2. Furthermore, our method improves mIoU by 10.59%, 2.20%, and 2.17% on these datasets, respectively, using U-Net as the segmentation model.},
  archive      = {J_TMM},
  author       = {Xiaoheng Jiang and Yingjie Li and Feng Yan and Yang Lu and Changsheng Xu and Mingliang Xu},
  doi          = {10.1109/TMM.2025.3565978},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MGDefect: A mask-guided high-quality defect image generation method for improving defect inspection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards robust semi-supervised distribution alignment against label distribution shift with noisy annotations. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3565967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based AI models typically require a large amount of high-quality annotated data to achieve optimal performance. However, the label distribution shift caused by noisy annotations can lead to perturbations in the classification boundary, reducing the robustness and generalization capabilities of deep learning models. To mitigate this issue, we transform the problem of learning from noisy labels into a semi-supervised learning problem, and propose a novel Semi-Supervised Distribution Alignment (SSDA) framework that strategically integrates noise-robust distribution alignment within a unified semi-supervised learning paradigm for combating noisy labels. By leveraging the similarity distribution between historical predictions, the proposed SSDA approach benefits from a flexible multi-historical regression modeling strategy, which aims to identify high-confidence samples/pairs and recalibrate the label shift through pseudo-labels. Furthermore, our approach employs a comprehensive multi-granularity distribution adaptation strategy, incorporating both instance-wise and class-aware distribution alignment to quantitatively minimize semantic discrepancies across different mixed feature domains. In this way, our SSDA approach ultimately achieves more resilient and generalizable performance against label noise, even in the presence of substantial noise. Extensive experiments conducted on multiple simulated and real-world noisy benchmark datasets consistently demonstrate the superiority and effectiveness of our SSDA method compared to existing state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Bingzhi Chen and Zhanhao Ye and Yishu Liu and Xiaozhao Fang and Guangming Lu and Shengli Xie and Xuelong Li},
  doi          = {10.1109/TMM.2025.3565967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards robust semi-supervised distribution alignment against label distribution shift with noisy annotations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Black-box adversarial defense based on image decomposition and reconstruction. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3565987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have challenged the security of deep neural networks (DNNs) recently. The most prominent adversarial attack methods include backdoor attacks, adversarial examples, etc. These attack methods inject triggers or perturbations into images, leading to extremely dangerous security vulnerability in deep learning domain. The various forms of adversarial attacks can contaminate DNNs with their distinct characteristics. The complexity of adversarial attack poses a great challenge to designing a general defense strategy. In this paper, we propose a novel defense method against most of adversarial attacks through Image Decomposition and Reconstruction (IDR). Our method can be applied to poisoned images without the need for internal information about the model or any prior knowledge of the clean/poisoned images. We apply a linear transformation on the poisoned image to destroy the perturbations or triggers and deploy a pre-trained diffusion model to reconstruct the original information. In particular, we propose a novel reverse process that utilizes the consistency of range-null space decomposition to guide the generation of purified images. The decomposition of the range-null space can guarantee the retrieval of image information, which enhances the robustness of our method and contributes to the reliable purification of poisoned images. We assess the effectiveness of our proposed IDR against various prevalent backdoor attacks, adversarial examples and Image-Scaling attack methods. The experimental results highlight the outstanding defensive capabilities of our proposed IDR, demonstrating an exceptionally high defense success rate.},
  archive      = {J_TMM},
  author       = {Jimiao Yu and Honglong Chen and Junjian Li and Linghan Chen and Yudong Gao and Weifeng Liu and Lei Zhang},
  doi          = {10.1109/TMM.2025.3565987},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Black-box adversarial defense based on image decomposition and reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High fidelity face swapping via facial texture and structure consistency mining. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3565975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face swapping task has always been attractive for its wide range of applications. However, existing face swapping methods suffer from two main challenges: a) degraded generation fidelity due to insufficient facial texture information; b) inconsistent synthesized face structure due to the lack of effective forms of face structure supervision. To address the above issues, we propose a novel Texture and Structure Consistency Mining (TSCM) framework to achieve high-fidelity face swapping with rich textures and consistent facial structure. For one thing, a Dual-Scale Oriented Identity Transfer module is devised to globally transfer source identity to the well-disentangled target identity features at dual-level feature spaces, which achieves more efficient identity transfer and promotes target attribute texture preservation. Then, to compensate for the local facial textures, a Semantic-Guided Texture Enhancing module is developed by exploiting disentangled identity and attribute semantics to ensure local texture consistency. For another thing, different from previous methods that directly apply abstract 3D coefficients, a Structure-Aware Head Modeling module is designed to provide intuitive face structure supervision, which is adaptively integrated with local facial texture information in a self-learning manner. Moreover, a structure-consistency discriminator is introduced to effectively restrict the synthesized face structure consistency. Comprehensive experiments demonstrate that our TSCM yields a substantial advantage over the state-of-the-art methods in synthesizing texture- and structure-consistent swapped faces. Codes are available at https://github.com/tyrink/TSCM.},
  archive      = {J_TMM},
  author       = {Fengyuan Liu and Lingyun Yu and Quanwei Yang and Meng Shao and Hongtao Xie},
  doi          = {10.1109/TMM.2025.3565975},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High fidelity face swapping via facial texture and structure consistency mining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-shift attention in polarization domain for fine-grained classification of satellite ISAR images. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3565957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fine-grained classification focuses on visible light domains, such as animals and cars. However, these methods often perform poorly when applied to radar images and images of satellites because of challenges such as distinguishing between noise and objects and the significant scale differences among object components. To address these unique scenarios, we propose the scale-shift attention in polarization domain (SAPD) method for fine-grained classification in satellite ISAR images. Specifically, radar emits different types of waves, each with distinct imaging effects. We utilize multipolarization inputs and introduce a polarization domain query module to integrate complementary features from various radar wave types captured from the same viewpoint. This multipolarization learning helps distinguish noise and leverages complementary features from different inputs. Moreover, to handle the substantial scale differences between centimeter-level payloads and the overall meter-level structure of satellites, we propose a scale-shift attention mechanism based on shift kernels. This mechanism extends attention in the direction specified by the shift kernel by incorporating adjacent pixels, allowing for the diffusion of attention. This is beneficial for capturing features of satellite components with varying scales and shapes. Extensive experiments on a novel satellite ISAR image dataset validate the effectiveness and superiority of the SAPD.},
  archive      = {J_TMM},
  author       = {Zewei Xin and Qinya Li and Bowen Sheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMM.2025.3565957},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scale-shift attention in polarization domain for fine-grained classification of satellite ISAR images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transition-aware point cloud completion by a progressive refinement generative adversarial network. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3565974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional reconstruction can help robots and vehicles understand their surroundings for subsequent navigation and manipulation tasks. However, in the case of target occlusion, it is difficult for visual sensors to acquire complete information about objects. In this work, we propose a progressive refinement generative adversarial network (PR-GAN) to recover object shapes guided by transition-awareness. This method directly predicts the missing point cloud from the partial point cloud. Our PR-GAN contains a progressive generation module (PGM) and a discriminator. A self-attention-based encoder is proposed in PGM to capture contextual information between local and global features. To guide encoders in generating accurate point clouds, we further propose a progressive fusion module (PFM) that extracts transition information between point clouds of different scales. Moreover, a part-whole correlation module (PWCM) is designed to extract the transition-awareness between the partial and the whole point clouds to further preserve the details. With the above modules, we enhance the spatial logic perception capability of the network so that PR-GAN can fully extract point cloud features and predict the high-fidelity point cloud. Experimental results show that PR-GAN performs better compared to other methods, evaluated on three public datasets. The code is available at https://github.com/luxurylf/PR-GAN.},
  archive      = {J_TMM},
  author       = {Feng Luan and Jiarui Hu and Zhipeng Wang and Jiguang Yue and Yanmin Zhou and Bin He},
  doi          = {10.1109/TMM.2025.3565974},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transition-aware point cloud completion by a progressive refinement generative adversarial network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MarkPlugger: Generalizable watermark framework for latent diffusion models without retraining. <em>TMM</em>, 1-9. (<a href='https://doi.org/10.1109/TMM.2025.3565960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the family of latent diffusion models (LDMs) has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches typically involve training specific components or entire generative models to embed a watermark in generated images for traceability and responsibility. However, in the fast-evolving era of AI-generated content (AIGC), the rapid iteration and modification of LDMs makes retraining with watermark models costly. To address the problem, we propose MarkPlugger, a generalizable plug-and-play watermark framework without LDM retrain. In particular, to reduce the disturbance of the watermark on the semantic of the generated image, we try to identify a watermark representation that is approaching orthogonal to the semantic in latent space, and the theoretical study shows that we can achieve approximate orthogonality in a high-dimensional space. Moreover, the offset through an additive fusion strategy for the watermark and the semantic is also bounded. Without modifying any components of the LDMs, we embed diverse watermarks in latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark recovery rate. We also have validated that our method is generalized to multiple official versions and modified variants of LDMs, even without retraining the watermark model. Furthermore, it performs robustly under various attacks of different intensities.},
  archive      = {J_TMM},
  author       = {Guokai Zhang and Lanjun Wang and Yuting Su and An-An Liu},
  doi          = {10.1109/TMM.2025.3565960},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MarkPlugger: Generalizable watermark framework for latent diffusion models without retraining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nutrition estimation for dietary management: A transformer approach with depth sensing. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3565966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nutrition estimation is crucial for effective dietary management and overall health and well-being. Existing methods often struggle with sub-optimal accuracy and can be time-consuming. In this paper, we propose NuNet, a transformer-based network designed for nutrition estimation that utilizes both RGB and depth information from food images. We have designed and implemented a multi-scale encoder and decoder, along with two types of feature fusion modules, specialized for estimating five nutritional factors. These modules effectively balance the efficiency and effectiveness of feature extraction with flexible usage of our customized attention mechanisms and fusion strategies. Our experimental study shows that NuNet significantly outperforms its variants and existing solutions for nutrition estimation. It achieves an error rate of 15.65%, the lowest known to us, largely due to our multi-scale architecture and fusion modules. This research holds practical values for dietary management with huge potential for transnational research and deployment and could inspire other applications involving multiple data types with varying degrees of importance.},
  archive      = {J_TMM},
  author       = {Zhengyi Kwan and Wei Zhang and Zhengkui Wang and Aik Beng Ng and Simon See},
  doi          = {10.1109/TMM.2025.3565966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Nutrition estimation for dietary management: A transformer approach with depth sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FrDiff: Framelet-based conditional diffusion model for multispectral and panchromatic image fusion. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3565985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of fusing low-resolution multispectral (LRMS) and high-resolution panchromatic (PAN) imagery, commonly referred to as pansharpening, is intended to generate high-resolution multispectral (HRMS) imagery. Typically, most pre-existing pansharpening frameworks mainly emphasize the straightforward learning of the mapping relationship among PAN and LRMS images to HRMS images. However, a key limitation of these frameworks is their potential overemphasis on spatial information, particularly the enhancement of low-frequency components. As a result, such an oversight potentially hinders the model's ability to simultaneously restore both spectral and spatial details. To address this issue, we propose a novel pansharpening model based on the denoising diffusion probabilistic model (DDPM), dubbed FrDiff. Specifically, we build a framelet-based conditional diffusion model that leverages the generative power of diffusion models to produce more refine results. Different from conventional methods directly inferring HRMS images, our strategy is designed to project their framelet coefficients, utilizing the available PAN and LRMS images as resources. This approach enables the separation of high-frequency and low-frequency components through framelet transformation, which are subsequently recombined to create a novel set of conditional embeddings that feed into the diffusion process. At the same time, the powerful predictive power of the diffusion model is exploited to simultaneously recover the high-frequency and low-frequency components of the HRMS. Moreover, we introduce a framelet-oriented cross-attention module dedicated to honing spectral fidelity. This module is crucial for improving the spectral precision of the HRMS images, ensuring a balanced emphasis on both spatial and spectral enhancements. Quantitative and qualitative experiments on multiple benchmark datasets demonstrate that the proposed method achieves more robustness and high-quality results than other state-of-the-art pansharpening methods.},
  archive      = {J_TMM},
  author       = {Junkang Zhang and Faming Fang and Tingting Wang and Guixu Zhang and Haichuan Song},
  doi          = {10.1109/TMM.2025.3565985},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FrDiff: Framelet-based conditional diffusion model for multispectral and panchromatic image fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PID controller-driven network for image fusion. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3565970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With its well-designed network architecture, the deep learning-based infrared and visible image fusion (IVIF) method shows its efficiency and effectiveness by realizing a fine feature extraction and fusion mechanism. However, disparities in cross-modal features often result in an imbalance between texture details and contextual information, causing detailed features to be overshadowed by prevailing contextual information. To tackle this issue, this study introduces PIDFusion, a fusion model driven by a PID controller, designed to dynamically optimize cross-modal feature fusion deviations. The core of PIDFusion is the dynamic adaptation capability of the PID controller, which facilitates real-time corrections for deviations encountered during the fusion process, thereby maintaining a harmonious balance between texture details and contextual information. Additionally, we introduced the Cyclic Self-Supervised Feature Refinement (CSSFR), which under the constraint of self-supervised loss, minimizes redundant information within the feature flow and ensures the preservation of salient feature through the cyclic input of decoupled features. Concurrently, we developed the Iterative Attention Module (IAM), utilizing the unique gating mechanism of LSTM to capture feature changes across successive iterations, thereby driving the model to cultivate more discriminative feature representations. Extensive experiments revealed that PIDFusion outperforms SOTA methods in terms of both efficiency and cost-effectiveness, through static statistics and high-level vision tasks. Our code is available at https://github.com/wang-x-1997/PIDFusion.},
  archive      = {J_TMM},
  author       = {Xue Wang and Zheng Guan and Wenhua Qian and Jinde Cao and Runzhuo Ma},
  doi          = {10.1109/TMM.2025.3565970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PID controller-driven network for image fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring kernel transformations for implicit neural representations. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3565979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit neural representations (INRs), which leverage neural networks to represent signals by mapping coordinates to their corresponding attributes, have garnered significant attention. They are extensively utilized for image representation, with pixel coordinates as input and pixel values as output. In contrast to prior works focusing on investigating the effect of the model's inside components (activation function, for instance), this work pioneers the exploration of the effect of kernel transformation of input/output while keeping the model itself unchanged. A byproduct of our findings is a simple yet effective method that combines scale and shift to significantly boost INR with negligible computation overhead. Moreover, we present two perspectives, depth and normalization, to interpret the performance benefits caused by scale and shift transformation. Overall, our work provides a new avenue for future works to understand and improve INR through the lens of kernel transformation.},
  archive      = {J_TMM},
  author       = {Sheng Zheng and Chaoning Zhang and Dongshen Han and Fachrina Dewi Puspitasari and Xinhong Hao and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3565979},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring kernel transformations for implicit neural representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph consistency learning with relational distillation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3543068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of semi-supervised learning on graphs, which has recently aroused widespread interest in relational data mining. The focal point of exploration in this area has been the utilization of graph neural networks (GNNs), which stand out for excellent performance. Previous methods, however, typically rely on the limited labeled data while ignoring the abundant structural information in unlabeled nodes inherently on graphs, easily resulting in overfitting, especially in scenarios where only a few label nodes are available. Even worse, GNNs, despite their success, are constrained by their ability to solely capture local neighborhood information through message-passing mechanisms, thereby falling short in modeling higher-order dependencies among nodes. To circumvent the above drawbacks, we propose a simple yet effective framework called Hypergraph COnsistency LeArning (HOLA). Specifically, we employ a collaborative distillation framework consisting of a teacher network and a student network. To achieve effective interaction, we propose momentum distillation, a self-training method that enables the student network to learn from pseudo-targets generated by a momentum teacher network. Further, a novel hypergraph structure learning network is developed to model complex high-order relations among nodes with relational consistency learning, thereby transferring the knowledge to the student network. Extensive experiments conducted on a variety of benchmark datasets demonstrate the superior performance of the HOLA over various state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Siyu Yi and Zhengyang Mao and Yifan Wang and Yiyang Gu and Zhiping Xiao and Chong Chen and Xian-Sheng Hua and Ming Zhang and Wei Ju},
  doi          = {10.1109/TMM.2025.3543068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hypergraph consistency learning with relational distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual collaborative learning for weakly supervised video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3535377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised anomaly detection is to identify the time window when an anomaly event happened based on the video-level label indicating whether the video contains anomaly event or not. Recent efforts have focused on leveraging multi-modal data, specifically combining visual and audio information, to enhance detection accuracy. While some studies have explored intra-video separation techniques, the primary emphasis remains on distinguishing potentially anomalous events scoring highest from those scoring lowest as normal events. Nevertheless, challenges persist in delineating boundaries between normal and abnormal events, particularly when visual differences are subtle. Our proposed framework, called Audio-Visual Collaborative Learning (AVCL), addresses the challenge of ambiguity in weakly supervised anomaly detection. Our core idea centers around utilizing both audio track variations and the perceptual robustness of visual information to detect and differentiate challenging cases, which composed of two essential modules: the Audio-Visual collaborative Hard cases Separation (AVHS) module and the Multi-modal Mutual Learning (MML) module. The AVHS module aims to address the challenge of discerning visually ambiguous clips in anomaly videos, differentiating between normal and abnormal events. To further improve detection accuracy, we introduce the Multi-modal Mutual Learning (MML) module, and this module enables a process of mutual learning to facilitate the exchange of knowledge and expertise between the single-modal branch and the multi-modal branch. We demonstrate that the proposed approach achieves state-of-the-art detection performance on benchmarks of XD-Violence and CCTV-Fights$_{sub}$ datasets.},
  archive      = {J_TMM},
  author       = {Jingke Meng and Huilin Tian and Ge Lin and Jian-Fang Hu and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3535377},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Audio-visual collaborative learning for weakly supervised video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gifts from gallery: Advancing image retrieval via unsupervised asymmetric feature fusion. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3535354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric retrieval systems, characterized by the deployment of models with varying capacities on platforms with differing computational and storage resources, pose a challenge in balancing retrieval efficiency and accuracy. The recent introduction of the Asymmetric Feature Fusion (AFF) paradigm has shown promise by enhancing existing asymmetric retrieval systems through feature fusion on the gallery side. However, its reliance on extensive human-annotated data hinders practical applicability. To this end, we introduce an innovative unsupervised training method tailored for AFF. Leveraging multiple gallery models as feature extractors, our approach exploits similarities among images encoded by these models as pseudolabels. For each gallery model, we calculate its adaptive weight through an in-depth exploration of contextual relationships among ranking list images in its embedding spaces. Subsequently, these weights are utilized to effectively fuse the image similarities encoded by different gallery models into a more robust one. The fused image similarities provide powerful pseudo-supervision for training AFF. Our unsupervised training approach enhances the generality and utility of AFF in real-world scenarios, particularly when labeled data is limited or expensive to obtain. Exhaustive experiments on various landmark retrieval datasets demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Wengang Zhou and Hui Wu and Min Wang and Houqiang Li},
  doi          = {10.1109/TMM.2025.3535354},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Gifts from gallery: Advancing image retrieval via unsupervised asymmetric feature fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards better distortion feature learning for object detection in top-view fisheye cameras. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2024.3521808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning in recent years, the performance of object detection under conventional cameras has been significantly improved. Nevertheless, due to the distortion caused by the fisheye cameras, detecting objects in this scenario remains a significant challenge. The dominant approaches focus on modifying the shape of the bounding box to better align the boundaries of the distorted object. However, these methods neglect the learning of spatial distortion information, which prevents them from satisfactory results. In this paper, we propose a novel fisheye camera detection network to learn distortion features better, dubbed SDANet. SDANet is composed of a series of SDABlocks, which are designed to learn spatial distortion features. Each SDABlock consists of multiple convolution kernels of different sizes, and it can generate the most suitable kernel based on the current input's distortion characteristics. Moreover, to address the limitations of the scarcity and uneven spatial distribution of fisheye image datasets on performance improvement, we propose a dedicated data augmentation strategy called Prominent Fisheye Distortion Augmentation (PFDAug). PFDAug can further introduce distortions to fisheye images, effectively alleviating these problems. Experimental results on the CEPDOF, MW-R, HABBOF, LOAF, and FishEye8k fisheye image datasets demonstrate that our method achieves state-ofthe-art performance.},
  archive      = {J_TMM},
  author       = {Pengbo Guo and Chengxu Liu and Xingsong Hou and Xueming Qian},
  doi          = {10.1109/TMM.2024.3521808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards better distortion feature learning for object detection in top-view fisheye cameras},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning visual conditioning tokens to correct domain shift for fully test-time adaptation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2024.3443633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully test-time adaptation aims to adapt the network model based on sequential analysis of input samples during the inference stage to address the cross-domain performance degradation problem of deep neural networks. This work is based on the following interesting finding: in transformer-based image classification, the class token at the first transformer encoder layer can be learned to capture the domain-specific characteristics of target samples during test-time adaptation. This learned token, when combined with input image patch embeddings, is able to gradually remove the domain-specific information from the feature representations of input samples during the transformer encoding process, thereby significantly improving the test-time adaptation performance of the source model across different domains. We refer to this class token as visual conditioning token (VCT). To successfully learn the VCT, we propose a bi-level learning approach to capture the longterm variations of domain-specific characteristics while accommodating local variations of instance-specific characteristics. Experimental results on the benchmark datasets demonstrate that our proposed bi-level visual conditioning token learning method is able to achieve significantly improved test-time adaptation performance by up to 1.9%},
  archive      = {J_TMM},
  author       = {Yushun Tang and Shuoshuo Chen and Zhehan Kan and Yi Zhang and Qinghai Guo and Zhihai He},
  doi          = {10.1109/TMM.2024.3443633},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning visual conditioning tokens to correct domain shift for fully test-time adaptation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DEER: Distribution divergence-based graph contrast for partial label learning on graphs. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2024.3408038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have emerged as powerful tools for graph classification tasks. However, contemporary graph classification methods are predominantly studied in fully supervised scenarios, while there could be label ambiguity and noise in real-world applications. In this work, we explore the weakly supervised problem of partial label learning on graphs, where each graph sample is assigned a collection of candidate labels. A novel method called D istribution Div e rgence-bas e d Graph Cont r ast (DEER) is proposed to address this issue. At the heart of our DEER is to measure the divergence among the underlying semantic distributions in the hidden space and this metric enables the identification of accurate positive graph pairs for effective graph contrastive learning. Specifically, we generate graph representations of augmented graph views that retain semantics and can be regarded as samples from the underlying semantic distributions. We employ a non-parametric metric to measure distribution divergence, which is then combined with pseudo-labeling to generate unbiased and target-oriented graph pairs. Furthermore, we introduce a label-correction method to eliminate noisy candidate labels, updating target labels using posterior distributions in a soft manner. Comprehensive experiments on various benchmarks demonstrate the superiority of our DEER in different settings compared to a range of state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Yiyang Gu and Zihao Chen and Yifang Qin and Zhengyang Mao and Zhiping Xiao and Wei Ju and Chong Chen and Xian-Sheng Hua and Yifan Wang and Xiao Luo and Ming Zhang},
  doi          = {10.1109/TMM.2024.3408038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DEER: Distribution divergence-based graph contrast for partial label learning on graphs},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cross-task transferability of adversarial examples via spatial and channel attention. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2024.3349925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples are well known to pose a security risk, when attacking deep learning models. While, most of existing adversarial attacks are designed to attack a single deep learning-based task, such as image classification. In practical scenarios, it is more necessary to study adversarial examples transferring across different vision tasks. However, it is challenging to create cross-task adversarial examples that can destroy multiple vision tasks at once due to unavailable various task-specific models and loss functions for attackers. To deal with this problem, we propose a Dual Attention-Guided Method (DAGM) for crafting cross-task adversarial examples by designing a spatial attention module and a channel attention module to capture overlapping discriminative regions and features that contribute to various tasks. Then we craft cross-task adversarial examples via reducing the dispersion ( i.e. , standard deviation) of feature maps re-weighted by both attention modules, which can destroy the overlapping discriminative regions and features for various tasks. Furthermore, to present theoretical explanation, we systematically analyze our method, and rigorously prove that both attention modules can provide better effectiveness of our adversarial examples, compared with existing cross-task adversarial attacks. Extensive experiments on two datasets demonstrate that our method can significantly degrade the performance of various tasks, even online CV APIs, and consistently outperform state-of-the-art methods by a large margin.},
  archive      = {J_TMM},
  author       = {Weiwei Feng and Nanqing Xu and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TMM.2024.3349925},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing cross-task transferability of adversarial examples via spatial and channel attention},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReE3D: Boosting novel view synthesis for monocular images using residual encoders. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2023.3347642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, novel view synthesis from a monocular image has become a research hot-spot that attracts significant attention. Some recent work identifies latent vectors for high-quality view generation via iterative optimisation, which is a time-consuming process. In contrast, some others utilise an encoder learning a mapping function to approximately estimate optimal latent codes, which significantly reduces its processing time but sacrifices reconstruction quality. Consequently, how to balance synthesis quality and its generation efficiency still remains challenging. In this paper, we propose a residual-based encoder to incorporate with a 3D Generative Adversarial Networks (GAN), named ReE3D, for novel view synthesis. It applies an iterative prediction of latent codes to ensure much higher quality of novel view synthesis with an insignificant increase of processing time when compared to existing encoder-based 3D GAN inversion methods. Additionally, we enforce a novel geometric loss constraint on the encoder to predict view-invariant latent codes, thus effectively mitigating the trade-off between geometric and texture quality in 3D GAN inversion. Extensive experimental results demonstrate that our extended encoder-based method has achieved best trade-off performance in terms of novel view synthesis quality and its execution time. Our method has gained comparable synthesis quality with exponentially decreased processing time when compared to iterative optimisation methods, while improved synthesis performance of encoder-based methods significantly.},
  archive      = {J_TMM},
  author       = {KeHua Guo and Tianyu Chen and Sheng Ren and Bin Hu and Zheng Wu and Shaojun Guo and Hui Fang},
  doi          = {10.1109/TMM.2023.3347642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ReE3D: Boosting novel view synthesis for monocular images using residual encoders},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PSAM: Parameter-free spatiotemporal attention mechanism for video question answering. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2023.3333192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal attention learning has always been a challenging research task in video question answering (VideoQA). It needs to consider not only the modelling of local neighbourhood dependencies between the adjacent frames in a video but also the modelling of long-term dependencies between nonadjacent frames. Although the existing methods are usually good at modelling temporal dependencies in one aspect, they cannot simultaneously and effectively model the temporal dependencies between adjacent and nonadjacent frames. To address this issue, we first derive a novel statistic-driven difference-aware generation function, which can efficiently calculate the difference between a sequence feature value and the whole mean value to identify the significance of the feature. Subsequently, we design a novel parameter-free spatiotemporal attention mechanism (PSAM), which captures the most relevant cues scattered in the context of a spatiotemporal video by generating functions and utilizes a gating mechanism to adaptively integrate and filter relevant and irrelevant information. Finally, we use the PSAM and hierarchical modelling to construct a lightweight multiscale context fusion- and reasoning-based VideoQA model. Extensive experimental research results obtained on five benchmark datasets for the VideoQA task show that our VideoQA model has high Q&A performance and lightweight characteristics. Simultaneously, comprehensive ablation experimental results show that the PSAM can not only improve the performance of the model but also significantly reduce the number of model parameters. In addition, extensive experimental findings obtained on the benchmark dataset of joint tasks (video moment retrieval and video highlight detection) further demonstrate that the PSAM is a general and effective spatiotemporal attention mechanism.},
  archive      = {J_TMM},
  author       = {Fuwei Zhang and Ruomei Wang and Fan Zhou and Yuanmao Luo and Jinyu Li},
  doi          = {10.1109/TMM.2023.3333192},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PSAM: Parameter-free spatiotemporal attention mechanism for video question answering},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic reasoning for movie QA: A character-centric approach. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2023.3322321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movie story understanding necessitates modeling of, and reasoning about characters and their relationships with the surroundings and others as the story goes. In Movie QA, this poses the challenges of effectively capturing the visual moments relevant to questions in long videos, and efficiently navigating the web of dynamic, contextual character-centric relationships over time. This paper presents a novel character-centric method that efficiently supports reasoning about relational dynamics for Movie QA. Central to the method is a T ime- E volving C onditional C H aracter-centric graph network ( ${\rm{TECH}}$ ) which models the characters, objects, and their question-conditioned relationships in space-time. ${\rm{TECH}}$ first maps the raw video data into a question-focused temporal neural graph over visual entities within and across shots and then distills the graph into a character-centric network which gives rise to the answer. At the core of this graph reasoning machine, TECH uses a two-stage feature refinement process for feature movie characters and their relationships, using their interactions with the surroundings as contextual information. ${\rm{TECH}}$ draws its efficiency over long videos from a “skim and scan” technique to rapidly localize the most query-relevant moments in the movie. Tested on the three large-scale datasets, TECH clearly shows advantages over recent state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Long Hoang Dang and Thao Minh Le and Vuong Le and Tu Minh Phuong and Truyen Tran},
  doi          = {10.1109/TMM.2023.3322321},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic reasoning for movie QA: A character-centric approach},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CHFusion: A cross-modality high-resolution representation framework for infrared and visible image fusion. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2023.3294814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our study, we proposed a novel infrared and visible image fusion framework based on cross-modality transfer and high-resolution representation, termed as CHFusion. On the one hand, the high-resolution representation backbone is devised to receive multi-scale information and maintain high-resolution representation. More specifically, the proposed method involves the pyramid cross-modality feature transfer module to achieve information interaction with different modalities and resolutions. In particular, we introduce the gradient block to obtain texture information of the source image and then supplement it with high-resolution features. We utilize the adaptive channel attention block to compress high-resolution features and then guide image reconstruction. Moreover, a cross-modality high-resolution aggregation block is used to integrate multi-scale information. On the other hand, we propose a difference-aware algorithm, which can generate a pair of weights and then use the weights to construct the difference-aware loss, and then difference-aware loss, a texture loss, and an intensity loss to guide our network to preserve abundant texture information and optimally salient target. Extensive experiments demonstrate the superiority of our method over state-of-the-art alternatives in terms of object maintenance and texture preservation.},
  archive      = {J_TMM},
  author       = {Biaojian Jin and Rencan Nie and Jinde Cao and Ying Zhang and Dongyang Li},
  doi          = {10.1109/TMM.2023.3294814},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CHFusion: A cross-modality high-resolution representation framework for infrared and visible image fusion},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning fine-grained information with capsule-wise attention for salient object detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2023.3234436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of convolutional neural networks being used for salient object detection (SOD), the performance has been significantly improved. However, how to integrate crucial features for modeling salient objects needs further exploration. In this work, we propose an effective feature selection scheme to solve this task. Firstly, we provide a Simplified Atrous Spatial Pyramid Pooling (SASPP) module to lightweight the multi-scale features. Dealing with the SASSP features, we design a pixel-level local feature selection scheme named Multi-Scale Capsule-wise Attention (MSCA). It aggregates features from multi-scales by dynamic routing and helps the network to generate fine-grained prediction maps. In addition, we exploit holistic features by the Spatial-wise Attention and Channel-wise Attention (SA/CA) mechanisms, which adaptively extracts spatial or channel information. We also propose a Multi-crossed Layer Connections (MLC) structure in the upsampling stage, to fuse features from not only different levels but also different scales. The salient object prediction is performed in a coarse-to-fine manner. By conducting comprehensive experiments on five benchmark datasets, our method achieves the best performance when compared to existing state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Sanyuan Zhao and Zongzheng Wen and Qi Qi and Kin-Man Lam and Jianbing Shen},
  doi          = {10.1109/TMM.2023.3234436},
  journal      = {IEEE Transactions on Multimedia},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning fine-grained information with capsule-wise attention for salient object detection},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging component learning with degradation modelling for blind image super-resolution. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2022.3216115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network (CNN)-based image super-resolution (SR) has exhibited impressive success on known degraded low-resolution (LR) images. However, this type of approach is hard to hold its performance in practical scenarios when the degradation process ( i.e. blur and downsampling) is unknown. Despite existing blind SR methods proposed to solve this problem using blur kernel estimation, the perceptual quality and reconstruction accuracy are still unsatisfactory. In this paper, we analyze the degradation of a high-resolution (HR) image from image intrinsic components according to a degradation-based formulation model. We propose a components decomposition and co-optimization network (CDCN) for blind SR. Firstly, CDCN decomposes the input LR image into structure and detail components in feature space. Then, the mutual collaboration block (MCB) is presented to exploit the relationship between both two components. In this way, the detail component can provide informative features to enrich the structural context and the structure component can carry structural context for better detail revealing via a mutual complementary manner. After that, we present a degradation-driven learning strategy to jointly supervise the HR image detail and structure restoration process. Finally, a multi-scale fusion module followed by an upsampling layer is designed to fuse the structure and detail features and perform SR reconstruction. Empowered by such degradation-based components decomposition, collaboration, and mutual optimization, we can bridge the correlation between component learning and degradation modelling for blind SR, thereby producing SR results with more accurate textures. Extensive experiments on both synthetic SR datasets and real-world images show that the proposed method achieves the state-of-the-art performance compared to existing methods.},
  archive      = {J_TMM},
  author       = {Yixuan Wu and Feng Li and Huihui Bai and Weisi Lin and Runmin Cong and Yao Zhao},
  doi          = {10.1109/TMM.2022.3216115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bridging component learning with degradation modelling for blind image super-resolution},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intention-interaction graph based hierarchical reasoning networks for human trajectory prediction. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2022.3182151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding crowd motion dynamics and forecasting the future pedestrian trajectories are critical to various applications, e.g. autonomous driving and surveillance system. This task is challenging because when pedestrians plan the future paths in real crowd scenes, they will distinguish the priorities of following their predetermined destinations and responding to the motion behaviors of neighboring pedestrians. However, most of the existing methods ignore the problem of intention-interaction trade-off. In this paper, we tackle this problem by a hierarchical network, which achieves dynamically reasoning predetermined destinations and future trajectories. A novel graph structure called Intention-Interaction Graph (IIG) is designed to jointly model the self intentions and social interactions. To aggregate information in IIG, Interaction Gated Graph Attention Networks (IGGAN) consisting of a gate mechanism and an attention mechanism is proposed, thus achieving reasoning the influence degree of neighboring pedestrians and destinations. Experimental results on multiple widely used pedestrian trajectory prediction datasets, including two datasets in ETH and three datasets in UCY, demonstrate the effectiveness of the proposed model.},
  archive      = {J_TMM},
  author       = {Cunyan Li and Hua Yang and Jun Sun},
  doi          = {10.1109/TMM.2022.3182151},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intention-interaction graph based hierarchical reasoning networks for human trajectory prediction},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
