<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc">TC - 54</h2>
<ul>
<li><details>
<summary>
(2025). An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wearable electronic technology has facilitated the integration of smart wearable devices into artificial intelligence (AI)-driven medical assisted diagnosis. Embedded multi-core processors (MPs) have gradually emerged as pivotal hardware components for smart wearable medical diagnostic devices due to their high performance and flexibility. However, embedded MPs face the challenge of balancing performance, power consumption, and load-balancing. In response, we introduce a Pareto-based iterated local search (PILS) algorithm for task scheduling, which systematically optimizes multiple objectives, alongside a task list model to reduce the dimension of the decision space and enhance scheduling performance. In addition, we present a two-stage discretization scheme to ensure that the proposed algorithm offers meaningful guidance throughout the scheduling process. Simulation and on-board testing results show that the proposed algorithm effectively optimizes energy consumption, task execution time, and load balancing in embedded MPs task scheduling, indicating the potential of the proposed algorithm in enhancing the performance of smart wearable medical diagnostic devices powered by embedded MPs.},
  archive      = {J_TC},
  author       = {Qinglin Zhao and Lixin Zhang and Qi Pan and Kunbo Cui and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TC.2025.3603699},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetKG: Synthesizing interpretable network router configurations with knowledge graph. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced router configuration synthesizers aim to prevent network outages by automatically synthesizing configurations that implement routing protocols. However, the lack of interpretability makes operators uncertain about how low-level configurations are synthesized and whether the automatically generated configurations correctly align with routing intents. This limitation restricts the practical deployment of synthesizers. In this paper, we present NetKG, an interpretable configuration synthesis tool.(i) NetKG leverages a knowledge graph as the intermediate representation for configurations, reformulating the configuration synthesis problem as a configuration knowledge completion task; (ii) NetKG regards network intents as query tasks that need to be satisfied in the current configuration space, achieving this through knowledge reasoning and completion; (iii) NetKG explains the synthesis process and the consistency between configuration and intent through the configuration knowledge involved in reasoning and completion. We show that NetKG can scale to realistic networks and automatically synthesize intent-compliant configurations for static routes, OSPF, and BGP. It can explain the consistency between configuration and intent at different granularities through a visual interface. Experimental results indicate that NetKG synthesizes configurations in 2 minutes for a network with up to 197 routers, which is 7.37x faster than the SMT-based synthesizer.},
  archive      = {J_TC},
  author       = {Zhenbei Guo and Fuliang Li and Peng Zhang and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603712},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetKG: Synthesizing interpretable network router configurations with knowledge graph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multi-attribute data. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reverse k-nearest neighbor (RkNN) query schemes typically assume that users are available online in real-time for interactive key reception, overlooking scenarios where users might be offline. Moreover, existing privacy-preserving RkNN query schemes primarily focus on user features or spatial data, neglecting the significance of user reputation values. To address these limitations, we propose a privacy-preserving resilient RkNN query scheme over encrypted outsourced multi-attribute data (PRRQ). Specifically, to mitigate the challenges posed by resilient online presence (i.e., non-real-time online) of users for interactive key reception, we incorporate a non-interactive key exchange (NIKE) protocol and the Diffie-Hellman two-party key exchange algorithm to propose a multi-party NIKE algorithm (2K-NIKE), facilitating non-interactive key reception for multiple users. Considering the privacy leakage issues, PRRQ encodes original multi-attribute data (i.e., spatial, feature, and reputation values) alongside query requests based on formalized criteria. Additionally, we integrate the proposed 2K-NIKE and the improved symmetric homomorphic encryption (iSHE) algorithms to encrypt them. Furthermore, catering to the requirements of ciphertext-based RkNN queries, we propose a private RkNN query eligibility-checking (PREC) algorithm and a private reputation-verifying (PRRV) algorithm, which validate the compliance of encrypted outsourced multi-attribute data with query requests. Security analysis demonstrates that PRRQ achieves simulation-based security under an honest-but-curious model. Experimental results show that PRRQ offers superior computational efficiency compared to comparative schemes.},
  archive      = {J_TC},
  author       = {Jing Wang and Haiyong Bao and Na Ruan and Qinglei Kong and Cheng Huang and Hong-Ning Dai},
  doi          = {10.1109/TC.2025.3603688},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multi-attribute data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of Large Language Models (LLMs) requires dependable operation in the presence of errors in the hardware (caused by for example radiation) as this has become a pressing concern. At the same time, the scale and complexity of LLMs limit the overhead that can be added to detect errors. Therefore, there is a need for low-cost error detection schemes. Concurrent Error Detection (CED) uses the properties of a system to detect errors, so it is an appealing approach. In this paper, we present a new methodology and scheme for error detection in LLMs: Concurrent Linguistic Error Detection (CLED). Its main principle is that the output of LLMs should be valid and generate coherent text; therefore, when the text is not valid or differs significantly from the normal text, it is likely that there is an error. Hence, errors can potentially be detected by checking the linguistic features of the text generated by LLMs. This has the following main advantages: 1) low overhead as the checks are simple and 2) general applicability, so regardless of the LLM implementation details because the text correctness is not related to the LLM algorithms or implementations. The proposed CLED has been evaluated on two LLMs: T5 and OPUS-MT. The results show that with a 1% overhead, CLED can detect more than 87% of the errors, making it suitable to improve LLM dependability at low cost.},
  archive      = {J_TC},
  author       = {Jinhua Zhu and Javier Conde and Zhen Gao and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2025.3603682},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FSA-hash: Flow-size-aware sketch hashing for software switches. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data centers and enterprise networks, software switches have become critical components for achieving flexible and efficient network management. Due to resource constraints in software switches, sketches have emerged as a promising approach for network traffic measurement. However, their accuracy is often impacted by hash collisions. Existing hash functions treat all collisions equally, failing to account for the differing impacts of collisions involving elephant flows versus mouse flows. We propose FSA-Hash, a novel flow-size-aware hashing scheme that separates elephant flows from each other and from mouse flows, minimizing the most detrimental collisions. FSA-Hash is designed based on two insights: separating elephant flows from mouse flows avoids overestimating mouse flows, while separating elephant flows from each other enables accurate heavy-hitter detection. We implement FSA-Hash using machine learning models trained on network traffic data (LFSA-Hash), and also design a lightweight online variant (OLFSA-Hash) that learns the hash model solely from sketch queries on the software switch, obviating traffic collection overheads. Evaluations across four sketches and two tasks demonstrate FSA-Hash’s superior accuracy over standard hash functions. Moreover, OLFSA-Hash closely matches LFSA-Hash’s performance, making it an attractive option for adaptively refining the hash model without monitoring traffic.},
  archive      = {J_TC},
  author       = {Fuliang Li and Kejun Guo and Yiming Lv and Jiaxing Shen and Yuting Liu and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603716},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FSA-hash: Flow-size-aware sketch hashing for software switches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared L1-memory clusters of streamlined instruction processors (processing elements - PEs) are commonly used as building blocks in modern, massively parallel computing architectures (e.g. GP-GPUs). Scaling out these architectures by increasing the number of clusters incurs computational and power overhead, caused by the requirement to split and merge large data structures in chunks and move chunks across memory hierarchies via the high-latency global interconnect. Scaling up the cluster reduces buffering, copy, and synchronization overheads. However, the complexity of a fully connected cores-to-L1-memory crossbar grows quadratically with PE-count, posing a major physical implementation challenge. We present TeraPool, a physically implementable, >1000 floating-point-capable RISC-V PEs scaled-up cluster design, sharing a Multi-MegaByte >4000-banked L1 memory via a low latency hierarchical interconnect (1-7/9/11 cycles, depending on target frequency). Implemented in 12nm FinFET technology, TeraPool achieves near-gigahertz frequencies (910MHz) typical, 0.80V/25 °C. The energy-efficient hierarchical PE-to-L1-memory interconnect consumes only 9-13.5 pJ for memory bank accesses, just 0.74-1.1× the cost of a FP32 FMA. A high bandwidth main memory link is designed to manage data transfers in/out of the shared L1, sustaining transfers at the full bandwidth of an HBM2E main memory. At 910MHz, the cluster delivers up to 1.89 single precision TFLOP/s peak performance and up to 200GFLOP/s/W energy efficiency (at a high IPC/PE of 0.8 on average) in benchmark kernels, demonstrating the feasibility of scaling a shared-L1 cluster to a thousand PEs, four times the PE count of the largest clusters reported in literature.},
  archive      = {J_TC},
  author       = {Yichao Zhang and Marco Bertuletti and Chi Zhang and Samuel Riedel and Diyou Shen and Bowen Wang and Alessandro Vanelli Coralli and Luca Benini},
  doi          = {10.1109/TC.2025.3603692},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load balancing scheduling for batch-ordered job-store: Online vs. offline. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3603725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization is crucial in real-world applications, especially for balancing loads across machines handling specific job types. This paper introduces a novel batch-ordered job-store scheduling model, where jobs in a batch are scheduled sequentially, with their operations allocated in a round-robin fashion across two scenarios. We establish that this problem is NP-hard and analyze it in both online and offline settings. In the online case, we first examine the exclusive scenario, where operations within the same job must be scheduled on different machines, and show that a load greedy (LG) algorithm achieves a tight competitive ratio of $2 - \frac{1}{m}$, with m representing the number of machines. Next, we consider the circular scenario, which requires maintaining the circular order of operations across ordered machines. In this context, we analyze potential anomalies in load distribution during local optimality achieved by the ordered load greedy (OLG) algorithm and provide bounds on the occurrence of these anomalies and the maximum load in each local scheduling round. In the offline case, we abstract each OLG scheduling process as a generalized circular sequence alignment (CSA) problem and develop a dynamic programming-based matching (DPM) algorithm to solve it. To further enhance load balancing, we develop a dynamic programming-based optimization (DPO) algorithm to schedule multiple jobs simultaneously in both scenarios. Experimental results confirm the efficiency of DPM for the CSA problem, and we validate the load balancing effectiveness of both online and offline algorithms using real traffic datasets. These theoretical findings and algorithmic implementations lay a solid groundwork for future practical advancements.},
  archive      = {J_TC},
  author       = {Mengbing Zhou and Yang Wang and Bocong Zhao and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3603725},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Load balancing scheduling for batch-ordered job-store: Online vs. offline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BaDFL: Mitigating model poisoning in decentralized federated learning. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3603683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized federated learning (DFL) has gained significant attention due to its ability to facilitate collaborative model training without relying on a central server. However, it is highly vulnerable to backdoor attacks, where malicious participants can manipulate model updates to embed hidden functionalities. In this paper, we propose BaDFL, a novel Backdoor Attack defense mechanism for Decentralized Federated Learning. BaDFL enhances robustness by applying strategic model clipping at the local update level. To the best of our knowledge, BaDFL is the first decentralized federated learning algorithm with theoretical guarantees against model poisoning attacks. Specifically, BaDFL achieves an asymptotically optimal convergence rate of $O(\frac{1}{\sqrt{nT}})$, where n is the number of nodes and T is the global maximum iteration number. Furthermore, we provide a comprehensive analysis under two different attack scenarios, showing that BaDFL maintains robustness within a specific defense radius. Extensive experimental results show that, on average, BaDFL can effectively defend against model poisoning within 6 mitigation rounds, with less than a 1% drop in accuracy.},
  archive      = {J_TC},
  author       = {Yuan Yuan and Anhao Zhou and Xiao Zhang and Yifei Zou and Yangguang Shi and Dongxiao Yu},
  doi          = {10.1109/TC.2025.3603683},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BaDFL: Mitigating model poisoning in decentralized federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3604487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing systems struggle to efficiently manage multiple concurrent deep neural network (DNN) workloads while meeting strict latency requirements, minimizing power consumption, and maintaining environmental sustainability. This paper introduces Ecomap, a sustainability-driven framework that dynamically adjusts the maximum power threshold of edge devices based on real-time carbon intensity. Ecomap incorporates the innovative use of mixed-quality models, allowing it to dynamically replace computationally heavy DNNs with lighter alternatives when latency constraints are violated, ensuring service responsiveness with minimal accuracy loss. Additionally, it employs a transformer-based estimator to guide efficient workload mappings. Experimental results using NVIDIA Jetson AGX Xavier demonstrate that Ecomap reduces carbon emissions by an average of 30% and achieves a 25% lower carbon delay product (CDP) compared to state-of-the-art methods, while maintaining comparable or better latency and power efficiency.},
  archive      = {J_TC},
  author       = {Varatheepan Paramanayakam and Andreas Karatzas and Dimitrios Stamoulis and Iraklis Anagnostopoulos},
  doi          = {10.1109/TC.2025.3604487},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning over geo-distributed clouds enables joint training of data located in different regions, alleviating the burden of transferring large volumes of training datasets, which greatly saves bandwidth. However, the limited capacity of WAN links slows down the inter-cloud communications, which significantly decelerates the synchronization of distributed machine learning over geo-distributed clouds. Besides, the multi-tenancy in clouds results in multiple training tasks running simultaneously, whose synchronizations consistently compete for the limited WAN bandwidth with each other, which further aggravates the training performance of each task. While existing works optimize synchronizations through techniques like gradient compression, multi-resource interleaving and so on, none of them targets at the synchronization congestion especially due to multi-tenant learning, which results in inferior training performance. To solve these problems, we propose a simple but effective scheme, SCC, for fast and efficient multi-tenant learning via synchronization congestion control. SCC monitors the cross-cloud network conditions and evaluates the synchronization congestion level based on the round-trip transmission time for each synchronization. Then SCC alleviates synchronization congestion via controlling the synchronization frequency according to the synchronization congestion level in a probabilistic way. Extensive experiments are conducted within our testbeds consisted of 16 NVIDIA V100 GPUs to evaluate the performance of SCC, and comparison results show that SCC can reduce the average training completion time and makespan by up to 28.6% and 43.2% over SAP-SGD [1]. Targeted experiments are conducted to demonstrate the effectiveness and robustness of SCC.},
  archive      = {J_TC},
  author       = {Chengxi Gao and Fuliang Li and Kejiang Ye and Yang Wang and Pengfei Wang and Xingwei Wang and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3604486},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-aware optimization of task offloading for UAV-assisted edge computing. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3604463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAV) are widely used for edge computing in poor infrastructure scenarios due to their deployment flexibility and mobility. In UAV-assisted edge computing systems, multiple UAVs can cooperate with the cloud to provide superior computing capability for diverse innovative services. However, many service-related computational tasks may fail due to the unreliability of UAVs and wireless transmission channels. Diverse solutions were proposed, but most of them employ timedriven strategies which introduce unwanted decision waiting delays. To address this problem, this paper focuses on a taskdriven reliability-aware cooperative offloading problem in UAV-assisted edge-enhanced networks. The issue is formulated as an optimization problem which jointly optimizes UAV trajectories, offloading decisions, and transmission power, aiming to maximize the long-term average task success rate. Considering the discrete-continuous hybrid action space of the problem, a dependenceaware latent-space representation algorithm is proposed to represent discrete-continuous hybrid actions. Furthermore, we design a novel deep reinforcement learning scheme by combining the representation algorithm and a twin delayed deep deterministic policy gradient algorithm. We compared our proposed algorithm with four alternative solutions via simulations and a realistic Kubernetes testbed-based setup. The test results show how our scheme outperforms the other methods, ensuring significant improvements in terms of task success rate.},
  archive      = {J_TC},
  author       = {Hao Hao and Changqiao Xu and Wei Zhang and Xingyan Chen and Shujie Yang and Gabriel-Miro Muntean},
  doi          = {10.1109/TC.2025.3604463},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliability-aware optimization of task offloading for UAV-assisted edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-storage verifiable data streaming with efficient revocation approach. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3604531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) is proposed to authenticate a sequence of ordered data, such that the misbehavior on the data returned by cloud server can be effectively detected. VDS also allows to efficiently replace the outsourced data by another value. However, the old authentication information can make the expired data pass the verification. To prevent this attack, VDS schemes must provide a revocation approach to revoke the old authentication information. The current approach employs the tree-like authentication structure or cryptographic accumulator, which will influence the efficiency of the VDS scheme. In this work, we find an approach to construct the low-storage VDS scheme supporting efficient revocation. Towards this end, we fully exploit the property of chameleon hash function with ephemeral trapdoor to propose a signature, which is the crucial step to construct the VDS scheme. In our VDS scheme, the size of the authentication information can be reduced to be less than the scale of the data streaming (i.e., low storage). Furthermore, the client is able to revoke the old authentication information in an efficient manner, where she only needs to release a message (i.e., efficient revocation). The performance evaluation shows that the proposed VDS scheme is efficient and practical.},
  archive      = {J_TC},
  author       = {Haining Yang and Dengguo Feng and Jing Qin},
  doi          = {10.1109/TC.2025.3604531},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-storage verifiable data streaming with efficient revocation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoFormer: Collaborating with heterogeneous edge devices for scalable transformer inference. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1× inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3%. CoFormer can also reduce energy consumption by approximately 40% while maintaining satisfactory inference performance.},
  archive      = {J_TC},
  author       = {Guanyu Xu and Zhiwei Hao and Li Shen and Yong Luo and Fuhui Sun and Xiaoyan Wang and Han Hu and Yonggang Wen},
  doi          = {10.1109/TC.2025.3604473},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CoFormer: Collaborating with heterogeneous edge devices for scalable transformer inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain enhances trust and collaboration among entities through its inherent features of transparency, immutability, and traceability, leading to its extensive integration into Multi-access Edge Computing (MEC). However, existing transaction propagation mechanisms require MEC devices to consume significant computing resources for complex transaction verification, increasing their vulnerability to malicious attacks. Adversaries can exploit this by flooding the blockchain network with spam transactions, aiming to deplete device energy and disrupt system performance. To cope with these issues, this paper proposes a reputation-based energy-efficient transaction propagation mechanism that alleviates spam transaction attacks while reducing computing resources and energy consumption. Firstly, we design a subjective logic-based reputation scheme that assesses node trust by integrating local and recommended opinions and incorporates opinion acceptance to counteract false evidence. Then, we optimize the transaction verification method by adjusting transaction discard and verification probabilities based on the proposed reputation scheme to curb the propagation of spam transactions and reduce verification consumption. Finally, we enhance the transaction transmission strategy by prioritizing nodes with higher reputations, enhancing both resilience to spam transactions and transmission reliability. A series of simulations demonstrate the effectiveness of the proposed mechanism.},
  archive      = {J_TC},
  author       = {Xijia Lu and Qiang He and Xingwei Wang and Jaime Lloret and Peichen Li and Ying Qian and Min Huang},
  doi          = {10.1109/TC.2025.3604480},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient conjunctive geometric range query over encrypted spatial data with learned index. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of geo-positioning technologies and mobile Internet, spatial data query services have attracted extensive attention. To protect the confidentiality of sensitive information outsourced to cloud servers, much efforts have been devoted to designing geometric range query schemes over encrypted spatial data without affecting availability. However, existing works focus on the privacy-preserving schemes with traditional tree indexes, causing more computing and storage issues. In this paper, we propose an efficient conjunctive geometric range query scheme over encrypted spatial data with a learned index. In particular, we design a new privacy-preserving learned index for spatial data to reduce the search space and storage overhead. The main idea is to add noise disturbance to the objective function instead of directly adding it to output results, reducing the leakage of private information and ensuring the correctness of output results. Moreover, we propose a spatial segmentation algorithm to avoid accessing a large number of unnecessary Z codes in the query process. The formal security analysis shows that our scheme ensures index data security and query privacy. Simulation results show that the query efficiency is improved while the storage overhead is significantly reduced compared with the state-of-the-art schemes.},
  archive      = {J_TC},
  author       = {Mingyue Li and Chunfu Jia and Ruizhong Du and Guanxiong Ha},
  doi          = {10.1109/TC.2025.3604470},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient conjunctive geometric range query over encrypted spatial data with learned index},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sketching for heavy item-oriented data stream mining with memory constraints. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and fast data stream mining is critical to many tasks, including real-time series analysis for mobile sensor data, big data management and machine learning. Various heavy-oriented item detection tasks, such as identifying heavy hitters, heavy changers, persistent items, and significant items, have garnered considerable attention from both industry and academia. Unfortunately, as data stream speeds continue to increase and the available memory, particularly in L1 cache, remains limited for real-time processing, existing schemes face challenges in simultaneously achieving high detection accuracy, memory efficiency, and fast update throughput, as we reveal. To tackle this conundrum, we propose a versatile and elegant sketch framework named Tight-Sketch, which supports a spectrum of heavy-based detection tasks. Recognizing that, in practice, most items are cold (non-heavy/persistent/significant), we implement distinct eviction strategies for different item types. This approach allows us to swiftly discard potentially cold items while offering enhanced protection to hot ones (heavy/persistent/significant). Additionally, we introduce an eviction method based on stochastic decay, ensuring that Tight-Sketch incurs only small one-sided errors without overestimation. To further enhance detection accuracy under extremely constrained memory allocations, we introduce Tight-Opt, a variant incorporating two optimization strategies. We conduct extensive experiments across various detection tasks to demonstrate that Tight-Sketch significantly outperforms existing methods in terms of both accuracy and update speed. Furthermore, by utilizing Single Instruction Multiple Data (SIMD) instructions, we enhance Tight-Sketch’s update throughput by up to 36%. We also implement Tight-Sketch on FPGA to validate its practicality and low resource overhead in hardware deployments.},
  archive      = {J_TC},
  author       = {Weihe Li and Paul Patras},
  doi          = {10.1109/TC.2025.3604467},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient sketching for heavy item-oriented data stream mining with memory constraints},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3604468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multiprocessor systems scale up, h-extra connectivity and h-extra diagnosability serve as two pivotal metrics for assessing the reliability of the underlying interconnection networks. To ensure that each component of the survival graph holds no fewer than h + 1 vertices, the h-extra connectivity and h-extra diagnosability have been proposed to characterize the fault tolerability and self-diagnosing capability of networks, respectively. Many efforts have been made to establish the quantifiable relationship between these metrics but it is less than optimal. This work addresses the flaws of the existing results and proposes a novel proof to determine the metric relationship between h-extra connectivity and h-extra diagnosability under the PMC and MM* models. Our approach overcomes the defect of previous results by abandoning the network’s regularity and independence number. Furthermore, we apply the suggested metric to establish the h-extra diagnosability of a new network class, named generalized exchanged X-cube-like network GEXC(s, t), which takes dual-cube-like network, generalized exchanged hypercube, generalized exchanged crossed cube, and locally generalized exchanged twisted cube as special cases. Finally, we propose the h-extra diagnosis strategy (h-EDS) and design two self-diagnosis algorithms AhED-PMC and AhED-MM*, and then conduct experiments on GEXC(s, t) and the real-world network DD-g648 to show the high accuracy and superior performance of the proposed algorithms.},
  archive      = {J_TC},
  author       = {Yifan Li and Shuming Zhou and Sun-Yuan Hsieh and Qifan Zhang},
  doi          = {10.1109/TC.2025.3604468},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-path bound for parallel tasks with conditional branches. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel execution and conditional execution are increasingly prevalent in modern embedded systems. In real-time scheduling, a fundamental problem is how to upper-bound the response times of a task. Recent work applied the multi-path technique to reduce the response time bound for tasks with parallel execution, but left tasks with conditional execution as an open problem. This paper focuses on upper-bounding response times for tasks with both parallel execution and conditional execution using the multi-path technique. By designing a delicate abstraction regarding the multiple paths of various conditional branches, we derive a new response time bound. We further apply this response time bound into the scheduling of multiple parallel tasks with conditional branches. Experiments demonstrate that the proposed bound significantly advances the state-of-the-art, reducing the response time bound by 9.4% and improving the schedulability by 31.2% on average.},
  archive      = {J_TC},
  author       = {Qingqiang He and Nan Guan and Zhe Jiang and Mingsong Lv},
  doi          = {10.1109/TC.2025.3604469},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-path bound for parallel tasks with conditional branches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-power multiplier designs by leveraging correlations of 2×2 encoded partial products. <em>TC</em>, 1-8. (<a href='https://doi.org/10.1109/TC.2025.3604478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipliers, particularly those with small bit widths, are essential for modern neural network (NN) applications. In addition, multiple-precision multipliers are in high demand for efficient NN accelerators; therefore, recursive multipliers used in low-precision fusion schemes are gaining increasing attention. In this work, we design exact recursive multipliers based on customized approximate full adders (AFAs) for low-power purposes. Initially, the partial products (PPs) encoded by 2×2 multiplications are analyzed, which reveals the correlations among adjacent PPs. Based on these correlations, we propose 4×4 recursive multiplier architectures where certain full adders (FAs) can be simplified without affecting the correctness of the multiplication. Manually and synthesis tool-based FA simplifications are performed separately. The obtained 4×4 multipliers are then used to construct 8×8 multipliers based on a low-power recursive architecture. Finally, the proposed signed and unsigned 4×4 and 8×8 multipliers are evaluated using a 28nm CMOS technology. Compared with DesignWare (DW) multipliers, the proposed signed and unsigned 4×4 multipliers achieve power reductions of 16.5% and 11.6%, respectively, without compromising area or delay; alternatively, the delay can be reduced by 20.9% and 39.4%, respectively, without compromising power or area. For signed and unsigned 8×8 multipliers, the maximum power reductions are 9.7% and 13.7%, respectively, albeit with a trade-off in area.},
  archive      = {J_TC},
  author       = {Ao Liu and Siting Liu and Hui Wang and Qin Wang and Fabrizio Lombardi and Zhigang Mao and Honglan Jiang},
  doi          = {10.1109/TC.2025.3604478},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-power multiplier designs by leveraging correlations of 2×2 encoded partial products},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StageWise: Accelerating persistent key-value stores by thread model redesigning. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of fast NVMe SSDs, key-value stores are becoming more CPU-efficient in order to reap their bandwidth. However, current CPU-optimized key-value stores adopt suboptimal intra- and inter-thread models, hence incurring memory-level stalling and load imbalance that hinder cores from realizing their full potential. We present STAGEWISE, an CPU-efficient key-value store on fast NVMe SSDs with high throughput. To achieve this, we introduce a new thread model for StageWise to process KV requests. Specifically, STAGEWISE converts the processing of each KV request into multiple asynchronous stages, and thus enables pipelining across all stages. STAGEWISE further introduces a client-driven share-index architecture to ease inter-thread load imbalance and maximize the pipelining opportunity. Guided by Little’s Law, STAGEWISE improves concurrency, and therefore efficiently uses CPU to reach higher throughput. Extensive experimental results show that STAGEWISE outperforms CPUoptimized key-value stores (e.g., KVell) by up to 3.5× with writeintensive workloads, and storage-optimized ones (e.g., RocksDB) by over an order of magnitude. STAGEWISE also shows higher read performance and excellent scalability under various workloads.},
  archive      = {J_TC},
  author       = {Zeqi Li and Youmin Chen and Qing Wang and Youyou Lu and Jiwu Shu},
  doi          = {10.1109/TC.2025.3605763},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {StageWise: Accelerating persistent key-value stores by thread model redesigning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competition-style sorting networks (CSN): A framework for hardware-based sorting operations. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting operations are considered to be a significant part of any computer system and are widely used in many applications. In applications where sorting has to be efficiently accomplished (i.e., in O(1) time) on small-sized entries, hardware accelerators, such as ASICs, FPGAs, or GPUs, are used to speed up the sorting operations. In the literature, the bitonic sort algorithm (or variants thereof) is still considered to be the most commonly used approach in many hardware sort implementations for decades. However, the time complexity of the bitonic sort is O((log(n))2) for sorting n elements, which does not satisfy the constant-time constraint we demand for our setting. In this paper, we propose competition-style sorting networks (CSNs), a framework for designing hardware-based competition-style class of sorting networks that captures all forms of two-stage sorting networks where the first stage (competition) consists of pairwise comparisons and the second stage (evaluation) ranks the entries and sorts them. To illustrate the utility of this framework, we develop and test one instance of this design, called the Competition Sort Algorithm (CSA), which has a time complexity of O(1), and specifically, one clock cycle. We implemented and tested CSA on both an Intel Cyclone V FPGA and the NVIDIA Quadro T1000 GPU then measured its gain, which combines the trade-offs between the relative speedup and the relative area increase, against the bitonic sort. Our results show that the CSA achieves a significant gain of up to 11.01× on the FPGA and a relative speedup of up to 3.32× on the GPU. We also compare the area, power, and latency of CSA with the bitonic sort algorithm on the FPGA.},
  archive      = {J_TC},
  author       = {Abbas A. Fairouz and Jassim M. Aljuraidan and Ameer Mohammed},
  doi          = {10.1109/TC.2025.3605766},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Competition-style sorting networks (CSN): A framework for hardware-based sorting operations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Multi-access Edge Computing (MEC) has empowered Internet of Things (IoT) devices and edge servers to deploy sophisticated Deep Neural Network (DNN) applications, enabling real-time inference. Many concurrent inference requests and intricate DNN models demand efficient multi-DNN inference in MEC networks. However, the resource-limited IoT device/edge server and expanding model size force models to be dynamically deployed, resulting in significant undesired energy consumption. In addition, parallel multi-DNN inference on the same device complicates the inference process due to the resource competition among models, increasing the inference latency. In this paper, we propose a Resource-aware and Dynamic DNN Deployment (R3D) scheme with the collaboration of end-edge-cloud. To mitigate resource competition and waste during multi-DNN parallel inference, we develop a Resource Adaptive Management (RAM) algorithm based on the Roofline model, which dynamically allocates resources by accounting for the impact of device-specific performance bottlenecks on inference latency. Additionally, we design a Deep Reinforcement Learning (DRL)-based online optimization algorithm that dynamically adjusts DNN deployment strategies to achieve fast and energy-efficient inference across heterogeneous devices. Experiment results demonstrate that R3D is applicable in MEC environments and performs well in terms of inference latency, resource utilization, and energy consumption.},
  archive      = {J_TC},
  author       = {Tong Zheng and Yuanguo Bi and Guangjie Han and Xingwei Wang and Yuheng Liu and Yufei Liu and Xiangyi Chen},
  doi          = {10.1109/TC.2025.3605749},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal embodied carbon models with dual carbon attribution for embodied carbon accounting of computer systems. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3605743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied carbon is the carbon emissions in the manufacturing process of products, which dominates the overall carbon footprint in many industries. Existing studies derive the embodied carbon through life cycle analysis (LCA) reports. Current LCA reports only provide the carbon emission of a product class, e.g. 28nm CPU, whereas a product instance can be made in various regions and time periods. Carbon emissions depend on the electricity generation process, which has spatial-temporal dynamics. Therefore, the embodied carbon of a product instance can differ from its product class. Additionally, different carbon attribution methods (e.g., location-based and market-based) can affect the carbon emissions of electricity, thus further affecting the embodied carbon of products. In this paper, we present new Spatial-Temporal Embodied Carbon (STEC) accounting models with dual attribution methods. We observe significant differences between STEC and current models, e.g., for 7nm CPU the difference is 13.69%. We further examine the impact of STEC models on existing embodied carbon accounting schemes on computer applications, such as Large Language Model (LLM) training and LLM inference. We observe that using STEC results in much greater differences in the embodied carbon of certain applications as compared to others (e.g., 32.26% vs. 6.35%).},
  archive      = {J_TC},
  author       = {Xiaoyang Zhang and Yijie Yang and Dan Wang},
  doi          = {10.1109/TC.2025.3605743},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spatial-temporal embodied carbon models with dual carbon attribution for embodied carbon accounting of computer systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OOLU: An operation-based optimized sparse LU decomposition accelerator for circuit simulation. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As scientific and engineering challenges grow in complexity and scale, the demand for effective solutions for sparse matrix computations becomes increasingly critical. LU decomposition, known for its ability to reduce computational load and enhance numerical stability, serves as a promising approach. This study focuses on accelerating sparse LU decomposition for circuit simulations, addressing the prolonged simulation times caused by large circuit matrices. We present a novel Operation-based Optimized LU (OOLU) decomposition architecture that significantly improves circuit analysis efficiency. OOLU employs a VLIW-like processing element array and incorporates a scheduler that decomposes computations into a fine-grained operational task flow graph, maximizing inter-operation parallelism. Specialized scheduling and data mapping strategies are applied to align with the adaptable pipelined framework and the characteristics of circuit matrices. The OOLU architecture is prototyped on an FPGA and validated through extensive tests on the University of Florida sparse matrix collection, benchmarked against multiple platforms. The accelerator achieves speedups ranging from 3.48× to 32.25× (average 12.51×) over the KLU software package. It also delivers average speedups of 2.64× over a prior FPGA accelerator and 25.18× and 32.27× over the GPU accelerators STRUMPACK and SFLU, respectively, highlighting the substantial efficiency gains our approach delivers.},
  archive      = {J_TC},
  author       = {Ke Hu and Fan Yang},
  doi          = {10.1109/TC.2025.3605751},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OOLU: An operation-based optimized sparse LU decomposition accelerator for circuit simulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JCSRC: Joint client selection and resource configuration for energy-efficient multi-task federated learning. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables privacy-preserving distributed machine learning by training models on edge client devices using their local data without revealing their raw data. In edge environments, various applications require different neural network models, making it crucial to perform joint training of multiple models on edge devices, known as multi-task FL. While existing multi-task FL approaches enhance resource utilization on edge devices through adaptive resource configuration or client selection, optimizing either of these aspects alone may lead to suboptimality. Therefore, in this paper, we explore a joint client selection and resource configuration method called JCSRC for multi-task FL, aiming to maximize energy efficiency in environments with limited computation and communication resources and heterogeneous client devices. Firstly, we formalize this problem as a mixed-integer nonlinear programming problem considering all these characteristics and prove its NP-hardness. To address this problem, we first design a multi-agent reinforcement learning (MARL)-based client selection method that selects appropriate clients for each task to train their models. The MARL method makes client selection decisions based on the clients’ data quality, energy efficiency, communication, and computation capacity to ensure fast convergence and energy efficiency. Then, we design a particle swarm optimization (PSO)-based resource configuration scheme that configures appropriate computation and bandwidth resources for each task on each client. The PSO scheme makes resource configuration decisions based on theoretically derived optimal CPU frequency and bandwidth to achieve high energy efficiency. Finally, we carry out extensive simulations and testbed-based experiments to validate our proposed JCSRC. The results demonstrate that, in comparison to state-of-the-art solutions, JCSRC can save energy consumption by up to 59% to achieve the target accuracy.},
  archive      = {J_TC},
  author       = {Junpeng Ke and Junlong Zhou and Dan Meng and Yue Zeng and Yizhou Shi and Xiangmou Qu and Song Guo},
  doi          = {10.1109/TC.2025.3605765},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {JCSRC: Joint client selection and resource configuration for energy-efficient multi-task federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the performance demands and stringent timing requirements of safety-critical systems like avionics and autonomous vehicles, research has focused on providing timing guarantees for the scheduling of Directed Acyclic Graph (DAG) tasks in multicore systems. The structural complexity and timing anomalies make this problem challenging. Existing methods bound the Worst-Case Response Time (WCRT) of tasks through static analysis, but these bounds are complicated, difficult to validate, and often remain pessimistic for many scheduling scenarios. Runtime intervention can be effective in eliminating timing anomalies and providing timing guarantees; however, it is ineffective for anomaly-free scheduling scenarios, leads to non-work-conserving schedules, and incurs additional overhead. This paper proposes a hybrid approach to identify timing anomalies in DAG scheduling scenarios within a system, providing tighter WCRT solutions. The static analysis first offers a sufficient anomaly test to directly identify some anomaly-free DAG scheduling scenarios. Leveraging a wide range of scheduling data collected from the running system or its simulator, we then apply a machine learning approach to train a binary classification model, achieving an accuracy of 99.5%. Identifying the anomaly status enables the application of more precise WCRT bounds for different scheduling scenarios, leading to improved system performance. Specifically, we shorten the WCRT bounds for anomaly-free DAG scheduling by an average of up to 21.58%, with a maximum reduction of up to 55.47% compared to the state-of-the-art method.},
  archive      = {J_TC},
  author       = {Nan Chen and Xiaotian Dai and Alan Burns and Iain Bate},
  doi          = {10.1109/TC.2025.3603674},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic modeling of intrusion tolerant systems based on redundancy and diversity. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3606189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with unforeseen attacks to software systems in critical application domains, redundancy-based ITSs schemes are among popular countermeasures to deploy. Designing the adequate ITS for the stated security requirements calls for stochastic analysis supports, able to assess the impact of variety of attack patterns on different ITS configurations. As contribution to this purpose, a stochastic model for ITS is proposed, whose novel aspects are the ability to account for both camouflaging components and for correlation aspects between the security failures affecting the diverse implementations of the software cyber protections adopted in the ITS. Extensive analyses are conducted to show the applicability of the model; the obtained results allow to understand the limits and strengths of selected ITS configurations when subject to attacks occurring in unfavorable conditions for the defender.},
  archive      = {J_TC},
  author       = {Silvano Chiaradonna and Felicita Di Giandomenico and Giulio Masetti},
  doi          = {10.1109/TC.2025.3606189},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stochastic modeling of intrusion tolerant systems based on redundancy and diversity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIVIDE: Efficient RowHammer defense via in-DRAM cache-based hot data isolation. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RowHammer poses a serious reliability challenge to modern DRAM systems. As technology scales down, DRAM resistance to RowHammer has decreased by 30× over the past decade, causing an increasing number of benign applications to suffer from this issue. However, existing defense mechanisms have three limitations: 1) they rely on inefficient mitigation techniques, such as time-consuming victim row refresh; 2) they do not reduce the number of effective RowHammer attacks, leading to frequent mitigations; and 3) they fail to recognize that frequently accessed data is not only a root cause of RowHammer but also presents an opportunity for performance optimization. In this paper, we observe that frequently accessed hot data plays a distinct role in security and efficiency: it can induce RowHammer by interfering with adjacent cold data, while also being performance-critical due to its frequent accesses. To this end, we propose Data Isolation via In-DRAM Cache (DIVIDE), a novel defense mechanism that leverages in-DRAM cache to isolate and exploit hot data. DIVIDE offers three key benefits: 1) It reduces the number of effective RowHammer attacks, as hot data in the cache cannot interfere with each other. 2) It provides a simple yet effective mitigation measure by isolating hot data from cold data. 3) It caches frequently accessed hot data, improving average access latency. DIVIDE employs a two-level protection structure: the first level mitigates RowHammer in cache arrays with high efficiency, while the second level addresses the remaining threats in normal arrays to ensure complete protection. Owing to the high in-DRAM cache hit rate, DIVIDE efficiently mitigates RowHammer while preserving both the performance and energy efficiency of the in-DRAM cache. At a RowHammer threshold of 128, DIVIDE with probabilistic mitigation achieves an average performance improvement of 19.6% and energy savings of 20.4% over DDR4 DRAM for fourcore workloads. Compared to an unprotected in-DRAM cache DRAM, DIVIDE incurs only a 2.1% performance overhead while requiring just a modest 1KB per-channel CAM in the memory controller, with no modification to the DRAM chip.},
  archive      = {J_TC},
  author       = {Haitao Du and Yuxuan Yang and Song Chen and Yi Kang},
  doi          = {10.1109/TC.2025.3603729},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DIVIDE: Efficient RowHammer defense via in-DRAM cache-based hot data isolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical signature-free multivalued validated byzantine agreement and asynchronous common subset in constant time. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3607476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract—Asynchronous common subset (ACS) is a powerful paradigm enabling applications such as Byzantine fault-tolerance (BFT) and multi-party computation (MPC). The most efficient ACS framework in the information-theoretic setting is due to Ben-Or, Kelmer, and Rabin (BKR, 1994). The BKR ACS protocol has been both theoretically and practically impactful. BKR ACS has an O(log $n$) running time (where $n$ is the number of replicas) due to the usage of $n$ parallel asynchronous binary agreement (ABA) instances, impacting both performance and scalability. Indeed, for a network of 16∼64 replicas, the parallel ABA phase occupies about 95%∼97% of the total runtime. A long-standing open problem is whether we can build an ACS framework with O(1) time while not increasing the message or communication complexity of the BKR protocol. We resolve the open problem, presenting the first constant-time ACS protocol with O($n$3) messages in the information-theoretic and signature-free settings. Our key ingredient is the first information-theoretic and constant-time multivalued validated Byzantine agreement (MVBA) protocol. Our results can improveߞasymptotically and concretelyߞvarious applications using ACS and MVBA. As an example, we implement FIN, a BFT protocol instantiated using our framework. Via a 121-server deployment on Amazon EC2, we show FIN reduces the overhead of the ABA phase to as low as 1.23% of the total runtime.},
  archive      = {J_TC},
  author       = {Xin Wang and Xiao Sui and Sisi Duan and Haibin Zhang},
  doi          = {10.1109/TC.2025.3607476},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Practical signature-free multivalued validated byzantine agreement and asynchronous common subset in constant time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid kernels: Seamlessly conquering the embedded computing continuum. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve seamless portability across the embedded computing continuum, we introduce a new kernel architecture: fluid kernels. Fluid kernels can be thought of as the intersection between embedded unikernels and general purpose monolithic kernels, allowing to seamlessly develop applications both in kernel space and user space in a unified way. This scalable kernel architecture can manage the trade-off between performance, code size, isolation and security. We compare our fluid kernel implementation, Miosix, to Linux and FreeRTOS on the same hardware with standard benchmarks. Compared to Linux, we achieve an average speedup of 3.5× and a maximum of up to 15.4×. We also achieve an average code size reduction of 84% and a maximum of up to 90%. By moving application code from user space to kernel space, an additional code size reduction up to 56% and a speedup up to 1.3× can be achieved. Compared to FreeRTOS, the use of Miosix only costs a moderate amount of code size (at most 47KB) for significant advantages in application performance with speedups averaging at 1.5× and up to 5×.},
  archive      = {J_TC},
  author       = {Federico Terraneo and Daniele Cattaneo},
  doi          = {10.1109/TC.2025.3605745},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fluid kernels: Seamlessly conquering the embedded computing continuum},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight graph partitioning enhanced by implicit knowledge. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3612730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning as a classic NP-complete problem, is the most fundamental procedure that needs to be performed before parallel computations. Partitioners can be divided into vertex- and edge-based approaches. Recently, both approaches are employing a streaming heuristic to find approximate solutions. It is lightweight in space and time complexities, but suffers from suboptimal partitioning quality, especially for directed graphs where the explicit knowledge provided for heuristic is limited. This paper thereby proposes new heuristics for not only vertex-based but also edge-based partitioning. They improve quality by additionally utilizing implicit knowledge, which is embedded in the local streaming view and the global graph view. Memory reduction techniques are presented to extract this knowledge with negligible space costs. That preserves the lightweight advantages of streaming partitioning. Besides, we study parallel acceleration and restreaming, to further boost the partitioning efficiency and quality. Extensive experiments validate that our proposals outperform the state-of-the-art competitors.},
  archive      = {J_TC},
  author       = {Zhigang Wang and Gongtai Sun and Ning Wang and Lixin Gao and Chuanfei Xu and Yu Gu and Ge Yu and Zhihong Tian},
  doi          = {10.1109/TC.2025.3612730},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight graph partitioning enhanced by implicit knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ElasticEC: Achieving fast and elastic redundancy transitioning in erasure-coded clusters. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3614839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding has been extensively deployed in today’s commodity HPC systems against unexpected failures. To adapt to the varying access characteristics and reliability demands, storage clusters have to perform redundancy transitioning via tuning the coding parameters, which unfortunately gives rise to substantial transitioning traffic. We present ElasticEC, a fast and elastic redundancy transitioning approach for erasure-coded clusters. ElasticEC first minimizes the transitioning traffic via proposing a relocation-aware stripe reorganization mechanism and a collecting-and-encoding algorithm. It further heuristically balances the transitioning traffic across nodes. We implement ElasticEC in Hadoop HDFS and conduct extensive experiments on a real-world cloud storage cluster, showing that ElasticEC can reduce 71.1-92.6% of the transitioning traffic and shorten 65.9-90.7% of the transitioning time.},
  archive      = {J_TC},
  author       = {Yuhui Cai and Guowen Gong and Zhirong Shen and Jiahui Yang and Jiwu Shu},
  doi          = {10.1109/TC.2025.3614839},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ElasticEC: Achieving fast and elastic redundancy transitioning in erasure-coded clusters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient delay-bounded dependent task offloading with service caching at edges. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3598749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are now embracing an era of edge computing and artificial intelligence, and the combination of the two has spawned a new field of research called edge intelligence. Massive amounts of data is generated at the edge of network, which relies on artificial intelligence to realize its potential. Meanwhile, artificial intelligence is able to flourish when processing diverse edge data. However, the computation and storage resources of edge servers are not unlimited. For some large-scale intelligent applications, it is difficult to meet their service quality requirements by directly offloading the entire application to a nearby server for processing. Due to the heterogeneity of server resources in edge environments, how to balance the workload among edge servers to provide better services also becomes complicated. The goal of this paper is to minimize the total cost of offloading large-scale applications consisting of many dependent tasks in an edge system. We formulate the Dependent task Offloading with Service Caching (DOSC) problem, which is proved to be NP-hard. A dynamic planning-based algorithm is introduced to solve fixed-DOSC, in which some services are pre-configured on the edge server, and other services can not be downloaded from the remote cloud. We also present a theoretical analysis on the performance guarantee of the dynamic planning-based algorithm. Then, we propose a near-optimal algorithm using the Gibbs sampling to solve the general DOSC problem. Testbed experiments and trace-driven simulations are conducted to verify the performance of our algorithm. Our algorithm, shown to be the most effective in terms of cost, considers both service caching and task dependencies when task offloading in comparison to other baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Liang and Sheng Zhang and Jie Wu},
  doi          = {10.1109/TC.2025.3598749},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cost-efficient delay-bounded dependent task offloading with service caching at edges},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel indirect methodology based on execution traces for grading functional test programs. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3600005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing functional test programs for hardware testing is time-consuming and experience-wise. A functional test program’s quality is usually assessed only through expensive fault simulation campaigns during early development. This paper presents indirect quality measurements of fault detection capabilities of functional test programs to reduce the total cost of fault simulation in the early development stages. We present a methodology that analyzes the instruction trace generated by running functional test programs on-chip and building its control and dataflow graph. We use the graph to identify potential flaws that affect the program’s fault detection capabilities. We present different graph-based techniques to measure the programs’ quality indirectly. By exploiting standard debugging formats, we individuate instructions in the source code that affect the graph-based measurements. We perform experiments on an automotive device manufactured by STMicroelectronics, running functional test programs of different natures. Our results show that our metric allows test engineers to develop better functional test programs without basing their development solely on fault simulation campaigns.},
  archive      = {J_TC},
  author       = {Francesco Angione and Paolo Bernardi and Andrea Calabrese and Lorenzo Cardone and Stefano Quer and Claudia Bertani and Vincenzo Tancorre},
  doi          = {10.1109/TC.2025.3600005},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel indirect methodology based on execution traces for grading functional test programs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3602297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers use geo-distributed datacenters to provide resources and services to clients located in different regions. However, uneven population density leads to unbalanced development of geo-distributed datacenters and cloud service providers face a shortage of land resources to further develop datacenters in densely populated regions. Thus, it is a real challenge for cloud service providers to meet the increasing demand from clients in affluent regions with saturated resources and to better utilize underutilized data centers in other regions. To address this challenge, we study an online resource allocation problem in geo-distributed clouds, whose goal is to assign each user request upon arrival to an appropriate geographic cloud region to minimize the resulting peak utilization of resource pools with different cost coefficients. To this end, we formulate the problem as a dynamic bin packing problem with heterogeneous dependent bins where user requests correspond to items to be packed and heterogeneous cloud resources are bins. To solve this online problem with high uncertainty, we propose a simulation based memetic algorithm to generate robust offline proactive policies based on historical data, which enable fast decision making for online packing. Our experiments based on realistic data show that the proposed approach leads to a reduction in total costs of up to 15% compared to the current practice, while being much faster for decision making compared to a popular online method.},
  archive      = {J_TC},
  author       = {Yinuo Li and Jin-Kao Hao and Liwei Song},
  doi          = {10.1109/TC.2025.3602297},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantized Neural Networks (QNNs) have received increasing attention, since they can enrich intelligent applications deployed on embedded devices with limited resources, such as mobile devices and AIoT systems. Unfortunately, the numerical and computational discrepancies between training systems (i.e., servers) and deployment systems (e.g., embedded ends) may lead to large accuracy drop for QNNs in real deployments. We propose a Computation-Quantized Training Framework (CQTF), which simulates deployment-time fixed-point computation during training to enable one-shot, lossless deployment. The training procedure of CQTF is built upon a well-formulated quantization-specific numerical representation that quantifies both numerical and computational discrepancies between training and deployment. Leveraging this representation, forward propagation executes all computations in quantization mode to simulate deployment-time inference, while backward propagation identifies and mitigates gradient vanishing through an efficient floating-point gradient update scheme. Benchmark-based experiments demonstrate the efficiency of our approach, which can achieve no accuracy loss from training to deployment. Compared with existing five frameworks, the deployed accuracy of CQTF can be improved by up to 18.41%.},
  archive      = {J_TC},
  author       = {Xingzhi Zhou and Wei Jiang and Jinyu Zhan and Lingxin Jin and Lin Zuo},
  doi          = {10.1109/TC.2025.3603732},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information sharing in multi-tenant metaverse via intent-driven multicasting. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-tenant metaverse enables multiple users in a common virtual world to interact with each other online. Information sharing will occur when interactions between a user and the environment are multicast to other users by an interactive metaverse (IM) service. However, ineffective information-sharing strategies intensify competitions among users for limited resources in networks, and fail to interpret optimization intent prompts conveyed in high-level natural languages, ultimately diminishing user immersion. In this paper, we explore reliable information sharing in a multi-tenant metaverse with time-varying resource capacities and costs, where IM services are unreliable and alter the volumes of data processed by them, while the service provider dynamically adjusts global intent to minimize multicast delays and costs. To this end, we first formulate the information sharing problem as a Markov decision process and show its NP-hardness. Then, we propose a learning-based system GTP, which combines the proximal policy optimization reinforcement learning with feature extraction networks, including graph attention network and gated recurrent unit, and a Transformer encoder for multi-feature comparison to process a sequence of incoming multicast requests without the knowledge of future arrival information. The GTP operates through three modules: a deployer that allocates primary and backup IM services across the network to minimize a weighted goal of server computation costs and communication distances between users and services, an intent extractor that dynamically infers provider intent conveyed in natural language, and a router that constructs on-demand multicast routing trees adhering to users, the provider, and network constraints. We finally conduct theoretical and empirical analysis on the proposed algorithms for the system. Experimental results show that the proposed algorithms are promising, and superior to their comparison baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato},
  doi          = {10.1109/TC.2025.3603720},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Information sharing in multi-tenant metaverse via intent-driven multicasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing in-network computing deployment via collaboration across planes. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3603730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new paradigm of In-network computing (INC) permits service computation to be executed within network paths, rather than solely on dedicated servers. Although the programmable data plane has showcased notable performance advantages for INC application deployments, its effectiveness is constrained by resource limitations, potentially impeding the expressiveness and scalability of these deployments. Conversely, delegating computational tasks to the control plane, supported by general-purpose servers with abundant resources, offers increased flexibility. Nonetheless, this strategy compromises efficiency to a considerable extent, particularly when the system operates under heavy load. To simultaneously exploit the efficiency of data plane and the flexibility of control plane, we propose Carlo, a cross-plane collaborative optimization framework to support the network-wide deployment of multiple INC applications across both the control and data plane. Carlo first analyzes resource requirements of various INC applications across different planes. It then establishes mathematical models for resource allocation in cross-plane and automatically generates solutions using proposed algorithms. We have implemented the prototype of Carlo on Intel Tofino ASIC switches and DPDK. Experimental results demonstrate that Carlo can effectively trade off between computation time and deployment performance while avoiding performance degradation.},
  archive      = {J_TC},
  author       = {Xiaoquan Zhang and Lin Cui and WaiMing Lau and Fung Po Tso and Yuhui Deng and Weijia Jia},
  doi          = {10.1109/TC.2025.3603730},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing in-network computing deployment via collaboration across planes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable encrypted deduplication based on location-hiding secret sharing of data keys. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3603710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted deduplication is attractive because it can provide high storage efficiency while protecting data privacy. Most existing schemes achieve encrypted deduplication against brute-force attacks (BFAs) based on server-aided encryption. Unfortunately, the centralized key server in server-aided encryption can potentially become a single point of failure. To this end, distributed server-aided encryption is presented, which splits a system-level master key into multiple shares and distributes them across several key servers. However, it is hard to improve security and scalability with this method simultaneously. This paper presents a secure and scalable encrypted deduplication scheme ScalaDep. ScalaDep achieves a new design paradigm centered on location-hiding secret sharing of data keys. As the number of deployed key servers increases, the attack cost of adversaries increases while the number of requests handled by each key server decreases, enhancing both scalability and security. Furthermore, we propose a two-phase duplicate detection method for our paradigm, which utilizes short hashes and key identifiers to achieve secure duplicate detection against BFAs. Additionally, based on the allreduce algorithm, ScalaDep enables all key servers to collaboratively record the number of client requests and resist online BFAs by enforcing rate limiting. Security analysis and performance evaluation demonstrate the security and efficiency of ScalaDep.},
  archive      = {J_TC},
  author       = {Guanxiong Ha and Yuchen Chen and Chunfu Jia and Keyan Chen and Rongxi Wang and Qiaowen Jia},
  doi          = {10.1109/TC.2025.3603710},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable encrypted deduplication based on location-hiding secret sharing of data keys},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of differential fault attacks on QARMAv2. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3603728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QARMAv2, a family of lightweight block ciphers introduced in ToSC 2023, is an evolution of the original QARMA design, specifically constructed to accommodate more extended tweak values while simultaneously enhancing security measures. In this paper, for the first time, we present differential fault analysis (DFA) of all the QARMAv2 variants by introducing an approach to utilize the fault propagation patterns at the nibble level, with the goal of identifying relevant faulty ciphertexts and vulnerable fault positions. Introducing six random nibble faults strategically into the (r – 1)-th and (r – 2)-th backward rounds of the r-round QARMAv2-64 significantly reduces the secret key space from 2128 to 232. Additionally, when targeting QARMAv2-128-128, it demands the introduction of six random nibble faults to effectively reduce the secret key space from 2128 to a remarkably reduced 224. To conclude, we also explore the potential extension of our methods to conduct DFA on other versions of QARMAv2. To the best of our knowledge, this marks the first instance of a differential fault attack targeting the QARMAv2 tweakable block cipher family, signifying an important direction in cryptographic analysis.},
  archive      = {J_TC},
  author       = {Soumya Sahoo and Debasmita Chakraborty and Santanu Sarkar},
  doi          = {10.1109/TC.2025.3603728},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Unleashing the power of differential fault attacks on QARMAv2},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3603717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ciphertext-policy attribute-based encryption (CP-ABE) has garnered significant attention for enabling fine-grained access control over encrypted data in cloud environments. However, in traditional CP-ABE schemes, access policies are transmitted in plaintext, which can lead to sensitive information leakage. To mitigate this risk, hiding access policies has become essential. Under the condition of full hidden access policies, realizing efficient and accurate decryption and dynamic policy updating has become an urgent challenge. To tackle these challenges, we present an efficient attribute-based encryption with reliable policy updating under full policy hiding (EABE-PUFPH) scheme, which effectively integrates full policy hiding with policy updating capabilities. Furthermore, we conduct a rigorous security analysis and performance evaluation of the EABE-PUFPH scheme. Evaluation results show that the EABE-PUFPH scheme achieves full hidden access policies without affecting decryption efficiency, and its efficiency surpasses other similar schemes that achieve full policy hiding.},
  archive      = {J_TC},
  author       = {Chenghao Gu and Jiguo Li and Yichen Zhang and Yang Lu and Jian Shen},
  doi          = {10.1109/TC.2025.3603717},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive federated learning through dynamic model splitting and multi-objective clustering. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables multiple parties to collaboratively train models without centralizing data, making it ideal for privacy-sensitive applications. However, the heterogeneity and resource limitation of devices pose a critical challenge to the collaborative training process, incurring a significant communication cost to achieve convergence. Existing research has attempted to use clustering to address these issues. However, these approaches relied on a single clustering objective, limiting their effectiveness in a multifaceted heterogeneous environment. In this paper, we propose FedMSC, which employs an evolutionary-based multi-objective optimization approach to organize clients into distinct clusters via their similarities on independent factors such as response speed and local model updates. FedMSC iteratively generates Pareto-optimal cluster solutions, ensuring that no single solution outperforms another, while concurrently optimizing multiple objectives. Moreover, to account for computational diversity across clusters, FedMSC adopts a multi-exit training strategy in which the model is divided into blocks of layers, each equipped with auxiliary classifiers for early inference. Meanwhile, we devise a unique algorithm which dynamically assigns model blocks to devices through combinatorial optimization of devices’ resource capabilities and the computational requirements of the blocks. Experimental results demonstrate that FedMSC significantly reduce communication costs while maintaining a comparable accuracy to the baselines.},
  archive      = {J_TC},
  author       = {Ousman Manjang and Yanlong Zhai and Jun Shen and Adil Sarwar and Liehuang Zhu},
  doi          = {10.1109/TC.2025.3603681},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive federated learning through dynamic model splitting and multi-objective clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain sharding is a promising solution for scalability but struggles to reach the expected performance due to the high ratio of cross-shard transactions. Account migration has emerged as a critical approach to optimizing shard performance. However, existing migration solutions suffer from inefficient handling of queued withdrawal transactions from a migrating account and inadequate priority mechanism for migration transaction, resulting in prolonged transaction makespan and reduced system throughput. This paper proposes Caravan, a novel blockchain sharding system for optimizing account migration. First, Caravan proposes a transaction aggregation-based migration scheme to efficiently handle withdrawal congestion post-migration. It incorporates a multi-level Merkle tree and cross-shard synchronization protocol to ensure cross-shard security. Second, Caravan presents an economic incentive-driven priority mechanism that motivates miners to perform transaction aggregation and prioritize migration transactions by increasing the associated revenue. Furthermore, its gas recycling strategy enables users to finance migration costs without awareness or extra expenses. Finally, we develop the Caravan prototype, deploy it on Alibaba Cloud, and experiment with real Ethereum transactions. The results show that compared to the state-of-the-art account migration schemes, Caravan significantly mitigates the transaction surge caused by migration, achieving up to a 3.2× throughput improvement and a 65% reduction in transaction confirmation latency. And users share considerable migration costs without extra expenses, significantly reduce system costs. The code for Caravan is available on GitHub.11Caravan are available at https://github.com/Caravan-project/Caravan.},
  archive      = {J_TC},
  author       = {Yu Tao and Shouchen Zhou and Lu Zhou and Zhe Liu},
  doi          = {10.1109/TC.2025.3603672},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Servers in modern data centers face increasing challenges from energy inefficiency and thermal-related outages, both of which significantly contribute to their overall carbon footprint. These challenges often arise from a lack of coordination between computational resource provisioning and thermal management capabilities. This paper introduces the concept of thermal elasticity, a system’s intrinsic ability to absorb thermal stress without requiring additional cooling, as a guiding metric for sustainable thermal management. Building on this, we propose a collaborative in-band and out-of-band resource provisioning framework that adjusts CPU allocation based on real-time thermal feedback. By leveraging a machine learning model and runtime monitoring, the framework dynamically provisions CPU clusters to virtual machines co-located on the same host. Evaluations on real servers with multiple workloads show that our method reduces peak power consumption from 5.2% to 9.6%, and lowers peak temperatures between 4°C and 6.5°C (up to 40°C in extreme cases). Carbon emissions are also reduced from 7% to 37% during SPEC benchmark runs. These results highlight the framework’s potential to alleviate stress on power and cooling infrastructure, thereby enhancing energy efficiency, reducing carbon footprint, and improving service continuity during thermal challenges.},
  archive      = {J_TC},
  author       = {Da Zhang and Haojun Xia and Xiaotong Wang and Yanchang Feng and Haohao Liu and Bibo Tu},
  doi          = {10.1109/TC.2025.3603698},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bandwidth on a budget: Real-time configuration for edge video analysis. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era marked by technological innovation, visual applications have become ubiquitous in everyday life. Harnessing the power of computer vision, these applications process and interpret video data from edge cameras, facilitating tasks such as object detection and vehicle counting. Yet, implementing complex deep learning models on cameras with limited computational capacity poses significant challenges. Furthermore, the bandwidth constraints and fluctuating nature of wide-area networks present substantial difficulties for video analysis systems dependent on cloud computing. This paper first characterizes the relationship between different parameter combinations (such as frame rate and resolution) and video analysis accuracy through offline analysis. It proposes a video stream analysis configuration selection scheme, SPStream, for slowly changing scenes, and a configuration file switching strategy, SPStream+, for rapidly changing scenes. These strategies use idle resources at the camera edge end to select the optimal configuration in real-time, adjust video encoding quality, and dynamically switch configuration files based on the changing states of object motion. Finally, a real-time video stream analysis system for vehicle counting and pedestrian detection suitable for both scenarios is designed, which saves bandwidth to the greatest extent while meeting the accuracy requirements of users and achieving high accuracy of video analysis.},
  archive      = {J_TC},
  author       = {Sheng Chen and Jie Deng and Xiaoyi Tao and Xin Xie and Renrui Tan and Tu Hong and Xiulong Liu},
  doi          = {10.1109/TC.2025.3603711},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bandwidth on a budget: Real-time configuration for edge video analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved error bounds for floating-point quotients. <em>TC</em>, 1-8. (<a href='https://doi.org/10.1109/TC.2025.3585341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $x_{0}, y_{1}, \dots, y_{k}$ be nonzero floating-point numbers in base $\beta \geq 2$ and precision $p \geq 1$. Let $z := x_{0}/ y_{1}/ \dots/ y_{k}$, whereby the divisions are evaluated from left to right, and let $\widehat{z}$ be the corresponding floating-point evaluation according to the IEEE 754 standard in rounding to nearest. We prove that, in absence of underflow and overflow, $|\widehat{z}-z| \leq k\text{u}|z|$ provided that $k \leq \sqrt{\omega/\beta} \text{u}^{-1/3}$. Here $u := \frac{1}{2} \beta^{1-p}$ denotes the relative rounding error unit and $\omega := 2$ if $\beta$ is even and $\omega := 1$ if $\beta$ is odd. Thus, the relative rounding error of k consecutive floating-point divisions is bounded by ku. This improves on the classical Wilkinson-type bound $\gamma_{k} := k \text{u}/(1 - k \text{u})$.},
  archive      = {J_TC},
  author       = {Florian Bünger},
  doi          = {10.1109/TC.2025.3585341},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improved error bounds for floating-point quotients},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deconstructing the smart redbelly blockchain. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2024.3475573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical blockchain nodes replicate the execution of requests, which include native transfers and smart contract executions. With the shift of the Web towards Web3, modern blockchains suffer from congestion which prevents them to handle user requests. Recent studies showed that modern blockchains perform poorly or lose requests to realistic Decentralized Application (DApp) workloads, impairing the shift towards Web3. In this paper, we present Smart Redbelly Blockchain (SRBB) which handles realistic DApp workloads. Smart Redbelly Blockchain improves blockchain performance with (1) Transaction Validation and Propagation Reduction, (2) caching optimizations and (3) fast block execution. SRBB outperforms Algorand, Avalanche, Diem, Ethereum, Quorum and Solana when deployed over 5 continents and under the realistic workloads of NASDAQ, Uber and FIFA using the Diablo benchmark suite. Next, we decouple SRBB that improves the peak throughput of SRBB for the NASDAQ workload by 33% and reduces its latency by 20%.},
  archive      = {J_TC},
  author       = {Deepal Tennakoon and Vincent Gramoli},
  doi          = {10.1109/TC.2024.3475573},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deconstructing the smart redbelly blockchain},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local migration model of images based on deep learning against adversarial attacks. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2021.3075715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have achieved remarkable results in various tasks. However, DNNs are easily deceived by small input disturbances, which are called adversarial attacks. The adversarial attack is to deliberately add some subtle interference that humans cannot detect to the input sample, causing the model to give a wrong output with high confidence. Deep-Learning-as-a-Service (DLaaS) has become a current hot trend, and it also introduces challenging security issues. Therefore, in this paper, we propose a local migration model of confrontational attack images based on deep learning. The confrontational examples of the physical world are disguised as natural styles through the migration model to deceive human observers. Specifically, the model converts the small counter-interference into a specific pattern, and then camouflages the foreground or background or local target area of the image to achieve a high degree of invisibility. Due to the flexibility of the interference setting of this method, it can be used to help DNNs assess their robustness, and it can be used to achieve privacy protection and data security detection.},
  archive      = {J_TC},
  author       = {Hua Huang and Xinxin Liu},
  doi          = {10.1109/TC.2021.3075715},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Local migration model of images based on deep learning against adversarial attacks},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Notice of retraction: Deadlock-free adaptive routing in networks -on-chip using overlapping virtual networks. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2021.3067320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retracted.},
  archive      = {J_TC},
  author       = {Dong Xiang and Xiang Ji and Yuan Cai and Binzhang Fu},
  doi          = {10.1109/TC.2021.3067320},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Notice of retraction: Deadlock-free adaptive routing in networks -on-chip using overlapping virtual networks},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2016). SSD in-storage computing for search engines. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2016.2608818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SSD-based in-storage computing (called ”Smart SSDs”) allows application-specific codes to execute inside SSDs to exploit the high internal bandwidth and energy-efficient processors. As a result, Smart SSDs have been successfully deployed in many industry settings, e.g., Samsung, IBM, Teradata, and Oracle. Moreover, researchers have also demonstrated their potential opportunities in database systems, data mining, and big data processing. However, it remains unknown whether search engine systems can benefit from Smart SSDs. This work takes a first step to answer this question. The major research issue is what search engine query processing operations can be cost-effectively offloaded to SSDs. For this, we carefully identified the five most commonly used search engine operations that could potentially benefit from Smart SSDs: intersection, ranked intersection, ranked union, difference, and ranked difference. With close collaboration with Samsung, we offloaded the above five operations of Apache Lucene (a widely used open-source search engine) to Samsungs Smart SSD. Finally, we conducted extensive experiments to evaluate the system performance and tradeoffs by using both synthetic datasets and real datasets. The experimental results show that Smart SSDs significantly reduce the query latency by a factor of 2-3 and energy consumption by 6-10 for most of the aforementioned operations.},
  archive      = {J_TC},
  author       = {Jianguo Wang and Dongchul Park and Yannis Papakonstantinou and Steven Swanson},
  doi          = {10.1109/TC.2016.2608818},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SSD in-storage computing for search engines},
  year         = {2016},
}
</textarea>
</details></li>
<li><details>
<summary>
(2016). An innovation approach for optimal resource allocation in emergency management. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2016.2601331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In metropolitan regions, emergency events with different severity levels usually require multiple resources that have appropriate functionalities, money expenditure, moving velocities, etc. These resources could distribute over an extensive area with different ownerships. Solving the resource allocation problem for such an event involves complicated collaboration of multiple emergency departments under strict time constraints. Traditional resource allocation approaches usually have difficulties to efficiently find out the best resource assignment within the time limits by considering the large number of possibilities, which result in a considerable increase in fatalities. In this paper, a multiagent-based decentralised resource allocation approach using the domain transportation theory is proposed to handle a multi-task emergency event. The proposed approach is designed to effectively select appropriate resources without the global information and to concurrently generate the resource deployment plans for multiple tasks by considering the severity level of an emergency event. In the experiments, the proposed approach is tested along with other related approaches, and the experimental results indicate that the proposed approach can efficiently generate the optimal solution in terms of resource allocation time and money expenditure.},
  archive      = {J_TC},
  author       = {Jihang Zhang and Minjie Zhang and Fenghui Ren and Jiakun Liu},
  doi          = {10.1109/TC.2016.2601331},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An innovation approach for optimal resource allocation in emergency management},
  year         = {2016},
}
</textarea>
</details></li>
<li><details>
<summary>
(2016). In-storage computing for hadoop MapReduce framework: Challenges and possibilities. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2016.2595566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid State Drives (SSDs) were initially developed as faster storage devices intended to replace conventional magnetic Hard Disk Drives (HDDs). However, high computational capabilities enable SSDs to be computing nodes, not just faster storage devices. Such capability is generally called ”In-Storage Computing (ISC)”. Today’s Hadoop MapReduce framework has become a de facto standard for big data processing. This paper explores In-Storage Computing challenges and opportunities for the Hadoop MapReduce framework. For this, we integrate a Hadoop MapReduce system with ISC SSD devices that implement the Hadoop Mapper inside real SSD firmware. This offloads Map tasks from the host MapReduce system to the ISC SSDs. We additionally optimize the host Hadoop system to make the best use of our proposed ISC Hadoop system. Experimental results demonstrate our ISC Hadoop MapReduce system achieves a remarkable performance gain (2.3 faster) as well as significant energy savings (11.5 lower) compared to a typical Hadoop MapReduce system. Further, the experiment suggests such ISC augmented systems can provide a very promising computing model in terms of a system scalability.},
  archive      = {J_TC},
  author       = {Dongchul Park and Jianguo Wang and Yang-Suk Kee},
  doi          = {10.1109/TC.2016.2595566},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {In-storage computing for hadoop MapReduce framework: Challenges and possibilities},
  year         = {2016},
}
</textarea>
</details></li>
<li><details>
<summary>
(2013). Novel techniques for smart adaptive multiprocessor SoCs. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2013.57'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing concerns of power efficiency, silicon reliability and performance scalability motivate research in the area of adaptive embedded systems, i.e. systems endowed with decisional capacity, capable of online decision making so as to meet certain performance criteria. The scope of possible adaptation strategies is subject to the targeted architecture specifics, and may range from simple scenario-driven frequency/voltage scaling to rather complex heuristic-driven algorithm selection. This paper advocates the design of distributed memory homogeneous multiprocessor systems as a suitable template for best exploiting adaptation features, thereby tackling the aforementioned challenges. The proposed solution lies in the combined use of a typical application processor for global orchestration along with such an adaptive multiprocessor core for the handling of data-intensive computation. This paper describes an exploratory homogeneous multiprocessor template designed from the ground up for scalability and adaptation. The proposed contributions aim at increasing architecture efficiency through smart distributed control of architectural parameters such as frequency, and enhanced techniques for load balancing such as task migration and dynamic multithreading.},
  archive      = {J_TC},
  author       = {Luciano Ost and Rafael Garibotti and Gilles Sassatelli and Gabriel Marchesan Almeida and Rémi Busseuil and Anastasiia Butko and Michel Robert and Jürgen Becker},
  doi          = {10.1109/TC.2013.57},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Novel techniques for smart adaptive multiprocessor SoCs},
  year         = {2013},
}
</textarea>
</details></li>
<li><details>
<summary>
(2011). A new chaos-based cryptosystem for secure transmitted images. <em>TC</em>, 1. (<a href='https://doi.org/10.1109/TC.2011.16'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel and robust chaos-based cryptosystem for secure transmitted images and four other versions. In the proposed block encryption/decryption algorithm, a 2D chaotic map is used to shuffle the image pixel positions. Then, substitution (confusion) and permutation (diffusion) operations on every block, with multiple rounds, are combined using two perturbed chaotic PWLCM maps. The perturbing orbit technique improves the statistical properties of encrypted images. The obtained error propagation in various standard cipher block modes demonstrates that the proposed cryptosystem is suitable to transmit cipher data over a corrupted digital channel. Finally, to quantify the security level of the proposed cryptosystem, many tests are performed and experimental results show that the suggested cryptosystem has a high security level.},
  archive      = {J_TC},
  author       = {Abir Awad},
  doi          = {10.1109/TC.2011.16},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new chaos-based cryptosystem for secure transmitted images},
  year         = {2011},
}
</textarea>
</details></li>
</ul>

</body>
</html>
