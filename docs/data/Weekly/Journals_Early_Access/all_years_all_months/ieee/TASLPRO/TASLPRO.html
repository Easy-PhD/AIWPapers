<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TASLPRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taslpro">TASLPRO - 24</h2>
<ul>
<li><details>
<summary>
(2025). Robust detection of partially spoofed audio using semantic-aware inconsistency learning. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Partially spoofed technology subtly manipulates interested parts in an audio to alter the original meaning, with its fine-grained forgery posing great challenges to existing fully spoofed detection countermeasures. Existing partially spoofed audio detection methods have shown excellent effectiveness in distinguishing clean and long-duration spoofed segments. However, their robustness remains limited when malicious attackers manipulate a finer-grained segment (e.g., only a single phoneme) and employ post-processing operations to reduce detectable discontinuities. To face these challenges, we propose the Semantic-Aware Inconsistency Learning (SAIL) method for robust frame-level detection. It incorporates a robust augmentation module (RAM), a Multi-Scale Semantic Inconsistency Learning (MSIL) module, and a Semantic Separation Module (SSM) to learn robust discriminative features by capturing multi-segment discontinuities and semantic inconsistencies introduced by partially spoofed manipulations. Specifically, the RAM is applied to suppress the model's erroneous attention to additional interference caused by post-processing operations on the subtle spoofed artifacts. Then, the MSIL module is proposed to extract semantic inconsistency features after manipulations, using attention mechanisms at different scales to highlight forgery differences at various granularities. Finally, the SSM is devised to refine these features for robust frame-level detection, utilizing contrastive learning to ensure a clear distinction of inconsistent semantic features in the feature space. Extensive experiments are conducted on three public datasets, including ASVS2019PS, HAD, and LAV-DF, showing that our proposed method achieves the best performance under various noisy scenarios.},
  archive  = {J},
  author   = {Jialu Cao and Hui Tian and Peng Tian and Haizhou Li and Jianzong Wang},
  doi      = {10.1109/TASLPRO.2025.3617241},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {Robust detection of partially spoofed audio using semantic-aware inconsistency learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diffusion-based generative models for speech enhancement often face challenges in balancing performance and inference efficiency. To address this, we propose a model of Variance-Preserving Velocity-guided Interpolant Diffusion (VPVID), a novel framework that achieves competitive enhancement performance while maintaining high computational efficiency. Our approach incorporates a scalable interpolant framework that reconstructs the reverse diffusion process using velocity terms and state variables. Unlike traditional score-matching objectives, we employ a velocity-based loss function that directly estimates the instantaneous rate of change, providing more stable training and efficient data distribution learning. We further combine stochastic diffusion sampling with probability flow ordinary differential equations, augmented by an adaptive corrector mechanism, creating a flexible sampling strategy that balances quality and efficiency. Extensive experiments on VoiceBank-DEMAND and WSJ0-CHiME3 datasets demonstrate that VPVID significantly outperforms existing baselines across multiple metrics, particularly excelling in noise separation with SI-SIR improvement up to 4.7 dB. Furthermore, VPVID achieves up to 7× faster inference than existing diffusion-based methods while maintaining excellent speech enhancement and dereverberation performance.},
  archive  = {J},
  author   = {Gang Yang and Yangjie Wei and Ben Niu and Yuqiao Wang},
  doi      = {10.1109/TASLPRO.2025.3617254},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASKSER: A robust emotion recognition model based on voice data and noisy transcripts. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, emotion recognition has become an increasingly vital tool for enhancing customer service applications. Especially in telephonic interactions, detecting emotions accurately is crucial for improving human-computer interaction experiences. Despite significant advances in deep learning, current emotion recognition systems that integrate voice and text face challenges such as noise interference in transcripts and inadequate multimodal fusion, which hinder precise emotion detection. In this paper, we introduce MASKSER, a methodology that combines vocal signals and transcribed text in a robust manner. Our approach involves pretraining noisy transcripts with ChatGPT-4 using few-shot learning based on techniques such as masking and sentiment word replacement. This enhances emotion discernment significantly by leveraging the strengths of both modalities. To address the challenges posed by noisy data, we propose a mask-based noise generation model and use it to pretrain the transcript-based model, which helps mitigate inaccuracies. Additionally, we introduce a novel loss function that evaluates the Kullback-Leibler divergence between text and voice encoder distributions, ensuring balanced contributions from both modalities. Experiments are conducted in both English and Korean to validate the language independence and robustness of the proposed approach in different linguistic contexts. The results demonstrate substantial improvements in emotion recognition capabilities, achieving high performance metrics while reducing reliance on costly speech recognition resources.},
  archive  = {J},
  author   = {Yeo-Chan Yoon and Sookyun Kim},
  doi      = {10.1109/TASLPRO.2025.3617234},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {MASKSER: A robust emotion recognition model based on voice data and noisy transcripts},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel approach centered on the decoding stage of Automatic Speech Recognition (ASR) that enhances multilingual performance, especially for low-resource languages. It utilizes a cross-lingual embedding clustering method to construct a hierarchical Softmax (H-Softmax) decoder, which enables similar tokens across different languages to share similar decoder representations. It addresses the limitations of the previous Huffman-based H-Softmax method, which relied on shallow features in token similarity assessments. Through experiments on a downsampled dataset of 15 languages, we demonstrate the effectiveness of our approach in improving low-resource multilingual ASR accuracy.},
  archive  = {J},
  author   = {Zhengdong Yang and Qianying Liu and Sheng Li and Fei Cheng and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3617233},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring width-adaptive transformers for automatic speech recognition. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer architectures with multiple heads with wide attention dimensions (widths) are over-parameterized. This leads to parameter redundancy and high correlations across attention heads with only a minority of heads actively contributing to the task. In this study, we quantitatively analyze the parameter redundancy by comparing the linear centered kernel alignment (CKA) similarity of learned representations extracted across attention layers and heads. Observing that widening the network can exacerbate these correlations, leading to representations with high CKA similarity, we question the design choice with uniform attention widths across all attention heads or layers and investigate how this choice impacts correlations across heads in the same layer. We design a width-adaptive training method to dynamically tune the model to keep the main contributing widths in each attention head and layer while no knowledge distillation or re-training process is needed. Experimental results on both English and Dutch corpora show our adaptive training method effectively reduces cross-head correlations and improves accuracy in automatic speech recognition. We also demonstrate the effectiveness of width-adaptive training by finetuning the OWSM speech foundation model.},
  archive  = {J},
  author   = {Pu Wang and Hugo Van Hamme},
  doi      = {10.1109/TASLPRO.2025.3617232},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {Exploring width-adaptive transformers for automatic speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot out-of-distribution (OOD) detection is a critical yet under explored scenario in dialogue systems. Existing data augmentation techniques either incorporate external data or generate hard negative samples within the feature space, which often leads to issues such as introducing knowledge bias, failing to align with the discrete nature of text, and inadequately addressing the problem of under-representation caused by in-distribution (IND) overfitting. Motivated by the recent findings that enhancing intra-class discrimination can mitigate IND overfitting, and the class of a sentence is predominantly determined by salient words, we propose EnSal, a method designed to strengthen the features of salient words in order to enhance the correlation between intent features and their corresponding classes. To achieve this, we jointly train k-nearest neighbors contrastive learning (KCL) alongside cross-entropy (CE) to improve the intra-class discrimination of intent features. Salient words are identified using both the k-nearest neighbors condition and the prediction probability condition. These words are retained as templates for synthesizing text samples, thereby avoiding the introduction of knowledge bias while preserving consistency with the discrete characteristics of text. Furthermore, we treat the synthetic text as noise samples associated with their corresponding training samples and perform denoising autoencoder (DAE) training on the augmented dataset. This process enables the identification of common and significant class features, effectively alleviating the under-representation issue. Extensive experimental results demonstrate that our method surpasses the current state-of-the-art in few-shot OOD intent detection. The code and models will be made available at https://github.com/wangpei2009job/EnSal.},
  archive  = {J},
  author   = {Pei Wang and Jiangtao Ren},
  doi      = {10.1109/TASLPRO.2025.3617229},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superdirective beamformers are highly appealing for their superior directivity and effectiveness in suppressing diffuse noise. However, their sensitivity to sensor noise and array imperfections poses significant challenges in practice. Achieving higher robustness often necessitates a trade-off in directivity, thereby reducing their ability to suppress directional and diffuse noises. A key concern, therefore, is how to improve noise suppression while maintaining robustness. To address this, we propose in this paper a novel temporal-spatial joint high-gain beamforming method based on a Kronecker product decomposition, making use of the inter-frame correlation to improve performance. The signal model in the proposed work uses recent pairs of time frames and employs the Kronecker product of the steering vector with a frequency- and angle-dependent inter-frame correlation vector. The high-gain beamformers are formulated as Kronecker product filters, where the temporal filter is optimized to maximize the white noise gain (WNG) and the spatial filter is optimized to enhance the directivity factor (DF). With accurate estimation of the correlation vector, Kronecker product high-gain beamformers can simultaneously improve both WNG and DF. The proposed method offers flexibility and can be extended to design other types of beamformers, with a maximum WNG (MWNG) beamformer presented as an example within the same framework. This paper also explores three approaches to estimating the correlation vector: time-invariant, time-varying, and data-driven estimations. Simulation results show notable improvements in noise suppression performance across various scenarios, highlighting the practical effectiveness of the proposed method.},
  archive  = {J},
  author   = {Xiaoran Yang and Hanchen Pei and Jacob Benesty and Gongping Huang and Jingdong Chen},
  doi      = {10.1109/TASLPRO.2025.3617242},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain dialogue state tracking with large language model rationale and disentangled domain-slot attention. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3604650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dialogue state tracking (DST) is a core component of task-oriented dialogue systems. A significant challenge in this task is multi-domain DST, which involves considering dialogue states across multiple domains. Recent advancements have addressed this challenge by exploring various approaches to model the correlations among different domains with domain-slot-specific representations derived from dialogue context and aggregating domain-slot queries using sorts of attention mechanisms. However, existing models still exhibit deficiencies in handling these correlations, either by overlooking or overestimating them. In this paper, we propose a multi-domain DST framework with large language model (LLM) rationale and Disentangled Domain-Slot Attention to address this challenge. Specifically, we introduce a multi-domain aware instruction prompt to guide the LLM to generate the corresponding rationale to dialogue history, realizing these correlations along with the robustness against sorts of variations in spoken conversation. Additionally, we present a novel mechanism, termed Disentangled Domain-Slot Attention. This mechanism enables a dynamic, flexible, and context-dependent manner to extract domain-slot-specific information by disentangling domain-slot queries within the attention mechanism. Through extensive experiments on the MultiWOZ 2.0 and MultiWOZ 2.4 datasets, the results present that the proposed approaches improve the performance of multi-domain DST. In addition, we conducted empirical analyses to comprehensively understand the effectiveness of our proposed approaches.},
  archive  = {J},
  author   = {Longfei Yang and Jiyi Li and Sheng Li and Takahiro Shinozaki},
  doi      = {10.1109/TASLPRO.2025.3604650},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-14},
  title    = {Multi-domain dialogue state tracking with large language model rationale and disentangled domain-slot attention},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring self-supervised audio models for generalized anomalous sound detection. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine anomalous sound detection (ASD) is a valuable technique across various applications. However, its generalization performance is often limited due to challenges in data collection and the complexity of acoustic environments. Inspired by the success of large pre-trained models in numerous fields, this paper introduces a robust ASD model that leverages self-supervised pre-trained models trained on large-scale speech and audio datasets. Although there are inconsistencies between the pre-training datasets and the ASD task, our findings indicate that pre-training still provides substantial benefits for ASD. To mitigate overfitting and retain learned knowledge when fine-tuning with limited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an alternative to full fine-tuning. Additionally, we propose a Machine-aware Group Adapter module, which enables the model to capture differences between various machines within a unified framework, thereby enhancing the generalization performance of ASD systems. To address the challenge of missing attribute labels, we design a novel objective function that dynamically clusters unattributed data using vector quantization and optimizes through a dual-level contrastive learning loss. The proposed methods are evaluated on all benchmark datasets, including the DCASE 2020-2024 five ASD challenges, and the experimental results show significant improvements of our new approach and demonstrate the effectiveness of our proposed strategies.},
  archive  = {J},
  author   = {Bing Han and Anbai Jiang and Xinhu Zheng and Wei-Qiang Zhang and Jia Liu and Pingyi Fan and Yanmin Qian},
  doi      = {10.1109/TASLPRO.2025.3606200},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-15},
  title    = {Exploring self-supervised audio models for generalized anomalous sound detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cross-utterance speech contexts for conformer-transducer speech recognition systems. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper investigates four types of cross-utterance speech contexts modeling approaches for streaming and non-streaming Conformer-Transformer (C-T) ASR systems: i) input audio feature concatenation; ii) cross-utterance Encoder embeddings concatenation; iii) cross-utterance Encoder embeddings pooling projection; or iv) a novel chunk-based approach applied to C-T models for the first time. An efficient batch training scheme is proposed for contextual C-Ts that uses spliced speech utterances within each minibatch to minimize the synchronization overhead while preserving the sequential order of cross-utterance speech contexts. Experiments are conducted on four benchmark speech datasets across three languages: the English GigaSpeech and Mandarin Wenetspeech corpora used in contextual C-T models pre-training; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets used in domain fine-tuning. The best performing contextual C-T systems consistently outperform their respective baselines using no cross-utterance speech contexts in pre-training and fine-tuning stages with statistically significant average word error rate (WER) or character error rate (CER) reductions up to 0.9%, 1.1%, 0.51%, and 0.98% absolute (6.0%, 5.4%, 2.0%, and 3.4% relative) on the four tasks respectively. Their performance competitiveness against Wav2vec2.0-Conformer, XLSR-128, and Whisper models highlights the potential benefit of incorporating cross-utterance speech contexts into current speech foundation models.},
  archive  = {J},
  author   = {Mingyu Cui and Mengzhe Geng and Jiajun Deng and Chengxi Deng and Jiawen Kang and Shujie Hu and Guinan Li and Tianzi Wang and Zhaoqing Li and Xie Chen and Xunying Liu},
  doi      = {10.1109/TASLPRO.2025.3606235},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-14},
  title    = {Exploring cross-utterance speech contexts for conformer-transducer speech recognition systems},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-perspective inductive answering of subjective questions on products. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3608961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article focuses on the topic of answering subjective questions about products by multi-perspective induction. Unlike traditional factoid QA, whose answers are unique and can be extracted directly from the text, the answers to subjective QA are not a simple text span, but involve various viewpoints and facts. A good answer should incorporate both objective facts and subjective opinions from various sources. Facts often involve multiple fine-grained product aspects, some of which are implicit and not explicitly mentioned. The opinions are complex, hidden, and scattered. Questions may be vague, sometimes requiring commonsense inference. This challenging task has wide-ranging applications, but limited research has studied it. To address this problem, we propose a new model to answer subjective questions in an inductive way. Given a question, we first retrieve the missing but necessary commonsense knowledge to supplement its implicit aspects and hidden relations, to better understand the ask points and users' needs. We then parse and build an aspect tree from the factual data, such as product descriptions, to incorporate all aspects and their parent-child relations. Based on the tree, we infer the question-related aspects from the retrieved content. For each aspect, we aggregate its scattered opinions and objective facts to yield a summary. To deal with diverse types adaptively, we construct an aspect-controlled generation model. Each aspect summary would add a prefix that is learned through a trainable gating mechanism and fused into a multi-aspect decoder to derive a comprehensive answer. The whole model with a retriever and generator is jointly trained by a reinforced framework. Extensive experiments are conducted on our created large-scale SupQA dataset, and the results show the effectiveness of our approach.},
  archive  = {J},
  author   = {Jianxing Yu and Yufeng Zhang and Hanjiang Lai and Wenqing Chen and Yanghui Rao and Jian Yin},
  doi      = {10.1109/TASLPRO.2025.3608961},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-15},
  title    = {Multi-perspective inductive answering of subjective questions on products},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring visual information enhancement for multimodal customized opinion generation. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3611282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel task called customized opinion generation, which aims to generate a customized review that a specific user would give to a product that has not been yet reviewed by the user. This task can assist users in writing a high-quality review of an unreviewed product. Furthermore, the customized review of a product is useful to reduce the time it takes for users to learn about the product, which ideally should be tailored to the reader. To this end, we study the customized opinion generation task under the approach of pretrained language models. In particular, we first introduce a visual information enhancement module. In this module, we generate pseudo input words and image captions based on the images, and we introduce various prompt strategies to fully exploit the semantics of the model input text. Besides, we design an interaction model to explicitly capture the influence of input text and product images during customized opinion generation. Experimental results indicate that the proposed model can generate customized reviews, which in many cases are high quality. The results also show that the proposed model gives better results compared to several strong baselines. To our knowledge, we are the first to generate a customized review for an unreviewed product.},
  archive  = {J},
  author   = {Minjie Qiang and Zhongqing Wang and Guodong Zhou},
  doi      = {10.1109/TASLPRO.2025.3611282},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-12},
  title    = {Exploring visual information enhancement for multimodal customized opinion generation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emilia: A large-scale, extensive, multilingual, and diverse dataset for speech generation. <em>TASLPRO</em>, 1-10. (<a href='https://doi.org/10.1109/TASLPRO.2025.3612835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.},
  archive  = {J},
  author   = {Haorui He and Zengqiang Shang and Chaoren Wang and Xuyuan Li and Yicheng Gu and Hua Hua and Liwei Liu and Chen Yang and Jiaqi Li and Peiyang Shi and Yuancheng Wang and Kai Chen and Pengyuan Zhang and Zhizheng Wu},
  doi      = {10.1109/TASLPRO.2025.3612835},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-10},
  title    = {Emilia: A large-scale, extensive, multilingual, and diverse dataset for speech generation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LatentVoiceGrad: Nonparallel voice conversion with latent Diffusion/Flow-matching models. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3613926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC) technique enabling mel-spectrogram conversion from source to target speakers using a score-based diffusion model. The concept involves training a score network to predict the gradient of the log density of mel-spectrograms from various speakers. VC is executed by iteratively adjusting an input mel-spectrogram until resembling the target speaker's. However, challenges persist: audio quality needs improvement, and conversion is slower compared to modern VC methods designed to operate at very high speeds. To address these, we introduce latent diffusion models into VoiceGrad, proposing an improved version with reverse diffusion in the autoencoder bottleneck. Additionally, we propose using a flow matching model as an alternative to the diffusion model to further speed up the conversion process without compromising the conversion quality. Experimental results show enhanced speech quality and accelerated conversion compared to the original.},
  archive  = {J},
  author   = {Hirokazu Kameoka and Takuhiro Kaneko and Kou Tanaka and Yuto Kondo},
  doi      = {10.1109/TASLPRO.2025.3613926},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-15},
  title    = {LatentVoiceGrad: Nonparallel voice conversion with latent Diffusion/Flow-matching models},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separate and transcribe: Deep guitar string separation and its application for tablature transcription enhancement. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3614466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Guitar string separation constitutes a source separation task in which the individual stems to be retrieved are the signals produced by each distinct guitar string. Certain hardware solutions for string-wise signal manipulation are gaining traction among the guitarist community for applications such as MIDI control and string-level sound effects. However, software techniques for string separation are lagging behind, even though they could form a competitive alternative. This work applies established deep learning architectures commonly used for standard music source separation, to address the task of guitar string separation. It opts for a waveform-to-waveform approach using a multi-channel version of Wave-U-Net. It further suggests that this model can serve as a bridge from separation to transcription by facilitating guitar tablature inference. Tablature transcription enhancement proved feasible using a method that relies on feeding the separated signals to standard tablature inference models, by simply modifying their initial convolution layer to handle multi-source input. Moreover, this work seeks to address the challenges encountered in the more specific string separation scenario where no target signals are readily available for training, meaning that these signals are not provided directly through sophisticated hardware equipment such as polyphonic pickups. Instead, data manipulation and augmentation techniques that produce sample-level synthesized targets are proposed. Thus, two newly created datasets are introduced, namely GS-Aux and ADGP, based on standard guitar datasets: GuitarSet and DadaGP.},
  archive  = {J},
  author   = {Grigoris Bastas and Vassilis Katsouros and Petros Maragos},
  doi      = {10.1109/TASLPRO.2025.3614466},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-12},
  title    = {Separate and transcribe: Deep guitar string separation and its application for tablature transcription enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M4SER: Multimodal, multirepresentation, multitask, and multistrategy learning for speech emotion recognition. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3614428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human–machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance. To address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities. Building on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features. We refer to our proposed method as M4SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.},
  archive  = {J},
  author   = {Jiajun He and Xiaohan Shi and Cheng-Hung Hu and Jinyi Mi and Xingfeng Li and Tomoki Toda},
  doi      = {10.1109/TASLPRO.2025.3614428},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-16},
  title    = {M4SER: Multimodal, multirepresentation, multitask, and multistrategy learning for speech emotion recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A learning time-graph frequency representation for monaural speech enhancement. <em>TASLPRO</em>, 1-11. (<a href='https://doi.org/10.1109/TASLPRO.2025.3615450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Graph Fourier Transform (GFT) has recently demonstrated promising results in speech enhancement. However, existing GFT-based speech enhancement approaches often employ fixed graph topologies to build the graph Fourier basis, whose the representation lacks the adaptively and flexibility. In addition, they suffer from the numerical errors and instability introduced by matrix inversion in GFT based on both Singular Value Decomposition (GFT-SVD) and Eigen Vector Decomposition (GFT-EVD). Motivated by these limitations, this paper propose a simple yet effective learnable GFT-SVD framework for speech enhancement. Specifically, we leverage graph shift operators to construct a learnable graph topology and define a learnable graph Fourier basis by the singular value matrices using 1-D convolution (Conv-1D) neural layer. This eliminates the need for matrix inversion, thereby avoiding the associated numerical errors and stability problem. In contrast to complex-valued representation, our proposed learnable Fourier basis provides a real-valued time-graph representation, enabling better magnitude–phase alignment in speech enhancement. Comprehensive evaluations on the VCTK+DEMAND and DNS-2020 benchmarks demonstrate the consistent performance superiority of our learnable GFT-SVD over fixed STFT, GFT-EVD, and GFT-SVD within existing neural speech enhancement frameworks. Source code is available at:https://github.com/Wangfighting0015/GFT_project.},
  archive  = {J},
  author   = {Tingting Wang and Tianrui Wang and Meng Ge and Qiquan Zhang and Xi Shao},
  doi      = {10.1109/TASLPRO.2025.3615450},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-11},
  title    = {A learning time-graph frequency representation for monaural speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IWPO: Sample importance weight-based human preference optimization for large language models. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3615458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Direct preference optimization (DPO) methods for Large Language Models (LLMs) have emerged as an efficient alternative to Reinforcement Learning from Human Feedback (RLHF), owing to the lightweight training pipeline and lower computational cost. However, these methods still face significant challenges in enhancing the capacity for generalized text generation of language models. First, offline approaches remove the reward model (RM) and learn directly from preferences; while this simplifies training, it also eliminates an effective check on whether samples follow the optimal policy distribution, thereby deviating from standard Maximum Likelihood Estimation (MLE). Second, the preference levels across tasks or attributes often overlap in mixed datasets, which can induce catastrophic reward hacking. To address these issues, we introduce a new offline learning framework that explicitly accounts for how strongly each sample conforms to the optimal policy. We propose an Importance Resampling-based Preferred Sample Sampling (IRPSS) algorithm to recover standard MLE for estimating the optimal policy, and introduce a Multi-Cluster-based Reward Model (MCRM) that leverages feature clustering to mitigate reward distribution overlap during training. Combining these components, we present a Sample Importance Weight-based Human Preference Optimization (IWPO) method, which emphasizes the importance of target samples and can be plugged into mainstream preference optimization methods. Built on standard DPO, we evaluate IWPO across multiple base models and datasets. On Mistral-7B model, IWPO improves AlpacaEval win rate against GPT-4 Turbo and text-davinci-003 by 6.3% and MT-Bench multi-turn score by 5.1%. We release our code on github in https://github.com/matenglearn/IWPO.},
  archive  = {J},
  author   = {Teng Ma and Xiong Luo and Yuqi Yuan},
  doi      = {10.1109/TASLPRO.2025.3615458},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-13},
  title    = {IWPO: Sample importance weight-based human preference optimization for large language models},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving parameter-efficient fine-tuning for large language model services. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3612842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Parameter-Efficient Fine-Tuning (PEFT) provides a practical way for users to customize Large Language Models (LLMs) with their private data in LLM service scenarios. However, the inherently sensitive nature of private data demands robust privacy preservation measures during the customization of LLM services to ensure data security, maintain user trust, and comply with stringent regulatory standards. Based on PEFT, we propose Privacy-Preserving Parameter-Efficient Fine-Tuning (RAPT), a framework that offers privacy protection for LLM services. RAPT adopts a local privacy approach, enabling users to privatize their data locally using a text-to-text local differential privacy mechanism. Since PEFT performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.},
  archive  = {J},
  author   = {Yansong Li and Zhixing Tan and Paula Branco and Yang Liu},
  doi      = {10.1109/TASLPRO.2025.3612842},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-14},
  title    = {Privacy-preserving parameter-efficient fine-tuning for large language model services},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source separation of sperm whales' echolocation clicks. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3602321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Echolocation clicks, which are used for both localization and locomotion, are often emitted by sperm whales (Physeter macrocephalus). The separation between the sources of overlapping echolocation clicks is a useful tool for biodiversity and behavioral analysis. The problem is challenging due to the similarities between these vocalizations. Current approaches for separating the sources of echolocation clicks are based on short-term signal features, but neglect similarities in the channel impulse response and between sequences of clicks. As a result, they cannot distinguish between sources over long periods of time and thus neglect the temporal relationships of echolocation clicks arranged as click sequences or trains. In this paper, we present a novel three-stage source separation algorithm that utilizes both temporal and spectral click attributes as well as spatial information derived from the arrival time delays between direct and reflected click paths. The algorithm separates clicks by first identifying click sequences in short time windows using MAP (Maximum a Posteriori) estimates, i.e. determining the most probable sequences from a series of measured clicks. These sequences are then grouped into click trains using a flow network model, which are defined as sets of consecutive clicks from a single whale with inter-click intervals (ICI) between 0.4 and 3 seconds. Finally, the resulting click trains are assigned to individual whales using a linear assignment approach. Our method also mitigate errors in the process of click identification. The algorithm is of low complexity and can be processed in real time. Performance evaluation using real measurements of sperm whale vocalizations, emulation of recorded data and noise, and a controlled sea experiment in which recorded echolocation clicks were played back shows better source separation performance compared to benchmarks. High accuracy in click verification and source attribution is demonstrated for different numbers of whales, even under challenging conditions such as overlapping clicks and low signal-to-noise ratio with multiple noise transients.},
  archive  = {J},
  author   = {Guy Gubnitky and Yaly Mevorach and Dan Tchernov and Roee Diamant},
  doi      = {10.1109/TASLPRO.2025.3602321},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {8},
  pages    = {1-15},
  title    = {Source separation of sperm whales' echolocation clicks},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alignment information via optimal transport and pre-training for neural machine translation. <em>TASLPRO</em>, 1-10. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The demand for translation between different languages has increased rapidly as globalization has progressed. Although neural machine translation (NMT) has achieved excellent results through pre-training, existing models lack alignment information, which makes them unsuitable for specific domains. In this paper, a NMT model with alignment information via optimal transport (OT) and pre-training is proposed. First, the representation gap between different languages is narrowed by using alignment information via OT and pre-training (OTAP) to generate domain-specific data for information alignment, thereby learning richer semantic information. Second, a lightweight model, called the DR-Reformer, is introduced, which uses the Reformer as the backbone network and incorporates Dropout and Reduction layers to reduce model parameters and improve computational efficiency without sacrificing accuracy. Experiments conducted on the Chinese and English datasets of AI Challenger 2018 and WMT-17 show that our proposed algorithm outperforms existing algorithms.},
  archive  = {J},
  author   = {Xueping Su and Xingkai Zhao and Yunhong Li and Lina Yao and Matthias Ratsch},
  doi      = {10.1109/TASLPRO.2025.3570944},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {5},
  pages    = {1-10},
  title    = {Alignment information via optimal transport and pre-training for neural machine translation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain adaptation for dependency parsing via in-depth feature transfer. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3572796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dependency parsing endeavours to extract both syntactic and semantic knowledge of the input sentence via a dependency tree. Recently, supervised dependency parsers have achieved significant improvements thanks to the strong representation of pre-trained language models. However, the parsing accuracy drops dramatically when the model is trained on multiple out-of-domain training datasets. The key to addressing this problem is to learn the commonalities and differences between different domains. Although the widely used sharedprivate model attempts to capture domain-invariant and domainspecific feature representations by separated encoders, the two representations may interfere with each other, and the in-depth relationship between different domain features may be ignored. In this work, we explore to emphasize the useful domain-specific features and filter out the harmful ones in explicit and implicit aspects via different feature transfer strategies. Simultaneously, we leverage adversarial learning to capture more effective domain-invariant features that belong to both source and target domains. Experimental results show that our proposed model can improve the parsing accuracy of several strong baseline models significantly. The thorough analysis helps us to gain more insights into the knowledge transfer process between different modules.},
  archive  = {J},
  author   = {Ying Li and Shichang Zhu and Jianjian Liu and Zhengtao Yu and Yuxin Huang},
  doi      = {10.1109/TASLPRO.2025.3572796},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {5},
  pages    = {1-14},
  title    = {Multi-source domain adaptation for dependency parsing via in-depth feature transfer},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OmniDialog: An omnipotent pre-training model for task-oriented dialogue system. <em>TASLPRO</em>, 1-11. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pre-trained conversation models (PCMs) have demonstrated remarkable results in task-oriented dialogue (TOD) systems. Many PCMs focus predominantly on dialogue management tasks like dialogue state tracking, dialogue generation tasks like response generation, or both. However, the existing PCMs seldom consider dialogue comprehension tasks, such as dialogue question answering and summarization tasks. These tasks allow PCMs to glean dialogue context from various angles. This observation naturally raises the question: Can the performance of downstream dialogue tasks be enhanced if a PCM is pre-trained on dialogue management, generation, and comprehension tasks? To investigate this, we proposed an Omnipotent Dialogue pre-training model (OmniDialog). It unifies these three dialogue tasks into a monolithic framework by multi-task learning, fostering inter-task communication. The pre-training corpus of OmniDialog spans 7 dialogue-focused tasks, drawing from 15 datasets and encompassing over 3.2 million dialogue utterances. To our knowledge, OmniDialog is a pioneering PCM pre-trained across dialogue management, generation, and comprehension domains. We evaluated its performance across four tasks: dialogue summarization, end-to-end dialogue modeling, dialogue state tracking, and intent classification. The results underscore its efficacy in domain transfer learning, low-resource, and full-dataset scenarios. Furthermore, to glean a nuanced understanding of OmniDialog's strengths and potential pitfalls, we designed a fine-grained analysis framework for dialogue-centric tasks. Experimental results show that the OmniDialog is good at hard samples, such as long dialogues and lengthy responses. We make our code and datasets publicly available. https://github.com/Itaaaachi/OmniDialog},
  archive  = {J},
  author   = {Mingtao Yang and See-Kiong Ng and Jinlan Fu},
  doi      = {10.1109/TASLPRO.2025.3542301},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {2},
  pages    = {1-11},
  title    = {OmniDialog: An omnipotent pre-training model for task-oriented dialogue system},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MvSR-NAT: Multi-view subset regularization for non-autoregressive machine translation. <em>TASLPRO</em>, 1-10. (<a href='https://doi.org/10.1109/TASLP.2022.3221043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by predicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model. Specifically, MvSR consists of two parts: (1) shared mask consistency: we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) model consistency, we maintain an exponential moving average of the model weights, and enforce the predictions to be consistent between the average model and the online model. Without changing the CMLM-based architecture, our approach achieves remarkable performance on three public benchmarks with 0.7-1.15 BLEU gains over previous NAT models. And, we reduce the gap to the stronger Transformer baseline.},
  archive  = {J},
  author   = {Pan Xie and Zexian Li and Zheng Zhao and Jiaqi Liu and Xiaohui Hu},
  doi      = {10.1109/TASLP.2022.3221043},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {11},
  pages    = {1-10},
  title    = {MvSR-NAT: Multi-view subset regularization for non-autoregressive machine translation},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
