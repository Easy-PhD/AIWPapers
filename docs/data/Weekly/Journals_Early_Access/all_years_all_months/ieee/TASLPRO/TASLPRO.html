<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TASLPRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taslpro">TASLPRO - 24</h2>
<ul>
<li><details>
<summary>
(2025). Robust detection of partially spoofed audio using semantic-aware inconsistency learning. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Partially spoofed technology subtly manipulates interested parts in an audio to alter the original meaning, with its fine-grained forgery posing great challenges to existing fully spoofed detection countermeasures. Existing partially spoofed audio detection methods have shown excellent effectiveness in distinguishing clean and long-duration spoofed segments. However, their robustness remains limited when malicious attackers manipulate a finer-grained segment (e.g., only a single phoneme) and employ post-processing operations to reduce detectable discontinuities. To face these challenges, we propose the Semantic-Aware Inconsistency Learning (SAIL) method for robust frame-level detection. It incorporates a robust augmentation module (RAM), a Multi-Scale Semantic Inconsistency Learning (MSIL) module, and a Semantic Separation Module (SSM) to learn robust discriminative features by capturing multi-segment discontinuities and semantic inconsistencies introduced by partially spoofed manipulations. Specifically, the RAM is applied to suppress the model's erroneous attention to additional interference caused by post-processing operations on the subtle spoofed artifacts. Then, the MSIL module is proposed to extract semantic inconsistency features after manipulations, using attention mechanisms at different scales to highlight forgery differences at various granularities. Finally, the SSM is devised to refine these features for robust frame-level detection, utilizing contrastive learning to ensure a clear distinction of inconsistent semantic features in the feature space. Extensive experiments are conducted on three public datasets, including ASVS2019PS, HAD, and LAV-DF, showing that our proposed method achieves the best performance under various noisy scenarios.},
  archive  = {J},
  author   = {Jialu Cao and Hui Tian and Peng Tian and Haizhou Li and Jianzong Wang},
  doi      = {10.1109/TASLPRO.2025.3617241},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {Robust detection of partially spoofed audio using semantic-aware inconsistency learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diffusion-based generative models for speech enhancement often face challenges in balancing performance and inference efficiency. To address this, we propose a model of Variance-Preserving Velocity-guided Interpolant Diffusion (VPVID), a novel framework that achieves competitive enhancement performance while maintaining high computational efficiency. Our approach incorporates a scalable interpolant framework that reconstructs the reverse diffusion process using velocity terms and state variables. Unlike traditional score-matching objectives, we employ a velocity-based loss function that directly estimates the instantaneous rate of change, providing more stable training and efficient data distribution learning. We further combine stochastic diffusion sampling with probability flow ordinary differential equations, augmented by an adaptive corrector mechanism, creating a flexible sampling strategy that balances quality and efficiency. Extensive experiments on VoiceBank-DEMAND and WSJ0-CHiME3 datasets demonstrate that VPVID significantly outperforms existing baselines across multiple metrics, particularly excelling in noise separation with SI-SIR improvement up to 4.7 dB. Furthermore, VPVID achieves up to 7× faster inference than existing diffusion-based methods while maintaining excellent speech enhancement and dereverberation performance.},
  archive  = {J},
  author   = {Gang Yang and Yangjie Wei and Ben Niu and Yuqiao Wang},
  doi      = {10.1109/TASLPRO.2025.3617254},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASKSER: A robust emotion recognition model based on voice data and noisy transcripts. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, emotion recognition has become an increasingly vital tool for enhancing customer service applications. Especially in telephonic interactions, detecting emotions accurately is crucial for improving human-computer interaction experiences. Despite significant advances in deep learning, current emotion recognition systems that integrate voice and text face challenges such as noise interference in transcripts and inadequate multimodal fusion, which hinder precise emotion detection. In this paper, we introduce MASKSER, a methodology that combines vocal signals and transcribed text in a robust manner. Our approach involves pretraining noisy transcripts with ChatGPT-4 using few-shot learning based on techniques such as masking and sentiment word replacement. This enhances emotion discernment significantly by leveraging the strengths of both modalities. To address the challenges posed by noisy data, we propose a mask-based noise generation model and use it to pretrain the transcript-based model, which helps mitigate inaccuracies. Additionally, we introduce a novel loss function that evaluates the Kullback-Leibler divergence between text and voice encoder distributions, ensuring balanced contributions from both modalities. Experiments are conducted in both English and Korean to validate the language independence and robustness of the proposed approach in different linguistic contexts. The results demonstrate substantial improvements in emotion recognition capabilities, achieving high performance metrics while reducing reliance on costly speech recognition resources.},
  archive  = {J},
  author   = {Yeo-Chan Yoon and Sookyun Kim},
  doi      = {10.1109/TASLPRO.2025.3617234},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {MASKSER: A robust emotion recognition model based on voice data and noisy transcripts},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel approach centered on the decoding stage of Automatic Speech Recognition (ASR) that enhances multilingual performance, especially for low-resource languages. It utilizes a cross-lingual embedding clustering method to construct a hierarchical Softmax (H-Softmax) decoder, which enables similar tokens across different languages to share similar decoder representations. It addresses the limitations of the previous Huffman-based H-Softmax method, which relied on shallow features in token similarity assessments. Through experiments on a downsampled dataset of 15 languages, we demonstrate the effectiveness of our approach in improving low-resource multilingual ASR accuracy.},
  archive  = {J},
  author   = {Zhengdong Yang and Qianying Liu and Sheng Li and Fei Cheng and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3617233},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring width-adaptive transformers for automatic speech recognition. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer architectures with multiple heads with wide attention dimensions (widths) are over-parameterized. This leads to parameter redundancy and high correlations across attention heads with only a minority of heads actively contributing to the task. In this study, we quantitatively analyze the parameter redundancy by comparing the linear centered kernel alignment (CKA) similarity of learned representations extracted across attention layers and heads. Observing that widening the network can exacerbate these correlations, leading to representations with high CKA similarity, we question the design choice with uniform attention widths across all attention heads or layers and investigate how this choice impacts correlations across heads in the same layer. We design a width-adaptive training method to dynamically tune the model to keep the main contributing widths in each attention head and layer while no knowledge distillation or re-training process is needed. Experimental results on both English and Dutch corpora show our adaptive training method effectively reduces cross-head correlations and improves accuracy in automatic speech recognition. We also demonstrate the effectiveness of width-adaptive training by finetuning the OWSM speech foundation model.},
  archive  = {J},
  author   = {Pu Wang and Hugo Van Hamme},
  doi      = {10.1109/TASLPRO.2025.3617232},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {Exploring width-adaptive transformers for automatic speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot out-of-distribution (OOD) detection is a critical yet under explored scenario in dialogue systems. Existing data augmentation techniques either incorporate external data or generate hard negative samples within the feature space, which often leads to issues such as introducing knowledge bias, failing to align with the discrete nature of text, and inadequately addressing the problem of under-representation caused by in-distribution (IND) overfitting. Motivated by the recent findings that enhancing intra-class discrimination can mitigate IND overfitting, and the class of a sentence is predominantly determined by salient words, we propose EnSal, a method designed to strengthen the features of salient words in order to enhance the correlation between intent features and their corresponding classes. To achieve this, we jointly train k-nearest neighbors contrastive learning (KCL) alongside cross-entropy (CE) to improve the intra-class discrimination of intent features. Salient words are identified using both the k-nearest neighbors condition and the prediction probability condition. These words are retained as templates for synthesizing text samples, thereby avoiding the introduction of knowledge bias while preserving consistency with the discrete characteristics of text. Furthermore, we treat the synthetic text as noise samples associated with their corresponding training samples and perform denoising autoencoder (DAE) training on the augmented dataset. This process enables the identification of common and significant class features, effectively alleviating the under-representation issue. Extensive experimental results demonstrate that our method surpasses the current state-of-the-art in few-shot OOD intent detection. The code and models will be made available at https://github.com/wangpei2009job/EnSal.},
  archive  = {J},
  author   = {Pei Wang and Jiangtao Ren},
  doi      = {10.1109/TASLPRO.2025.3617229},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superdirective beamformers are highly appealing for their superior directivity and effectiveness in suppressing diffuse noise. However, their sensitivity to sensor noise and array imperfections poses significant challenges in practice. Achieving higher robustness often necessitates a trade-off in directivity, thereby reducing their ability to suppress directional and diffuse noises. A key concern, therefore, is how to improve noise suppression while maintaining robustness. To address this, we propose in this paper a novel temporal-spatial joint high-gain beamforming method based on a Kronecker product decomposition, making use of the inter-frame correlation to improve performance. The signal model in the proposed work uses recent pairs of time frames and employs the Kronecker product of the steering vector with a frequency- and angle-dependent inter-frame correlation vector. The high-gain beamformers are formulated as Kronecker product filters, where the temporal filter is optimized to maximize the white noise gain (WNG) and the spatial filter is optimized to enhance the directivity factor (DF). With accurate estimation of the correlation vector, Kronecker product high-gain beamformers can simultaneously improve both WNG and DF. The proposed method offers flexibility and can be extended to design other types of beamformers, with a maximum WNG (MWNG) beamformer presented as an example within the same framework. This paper also explores three approaches to estimating the correlation vector: time-invariant, time-varying, and data-driven estimations. Simulation results show notable improvements in noise suppression performance across various scenarios, highlighting the practical effectiveness of the proposed method.},
  archive  = {J},
  author   = {Xiaoran Yang and Hanchen Pei and Jacob Benesty and Gongping Huang and Jingdong Chen},
  doi      = {10.1109/TASLPRO.2025.3617242},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retrieval-augmented prompt learning for pre-trained foundation models. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3608936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the “pre-train, prompt, and predict” paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RETROPROMPT, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RETROPROMPT leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RETROPROMPT, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RETROPROMPT effectively reduces the reliance on rote memorization, leading to enhanced generalization.},
  archive  = {J},
  author   = {Xiang Chen and Yixin Ou and Quan Feng and Lei Li and Piji Li and Haibo Ye and Sheng-Jun Huang and Shuofei Qiao and Shumin Deng and Huajun Chen and Ningyu Zhang},
  doi      = {10.1109/TASLPRO.2025.3608936},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-12},
  title    = {Retrieval-augmented prompt learning for pre-trained foundation models},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-path state-space modeling with cross-domain interaction for multichannel speech enhancement. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3618543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel dual-path state-space model within an encoder-decoder architecture for multichannel speech enhancement that leverages joint temporal and spectral modeling to significantly improve speech quality in noisy and reverberant environments. At the core of our framework is the S5 state-space model, which efficiently captures complex temporal dependencies across multiple speech channels by modeling both short and long-term dynamics. To effectively integrate spatial and spectral features, our encoder employs S3Conv layers that extract salient characteristics from the raw input, while a dedicated cross-domain interaction mechanism facilitates a dynamic exchange of information between two parallel data streams used for coarse magnitude estimation and complex spectral refinement, respectively. This dual-path design enables the network to jointly enhance amplitude and phase information, resulting in improved perceptual quality and intelligibility. Extensive experiments on public datasets demonstrate that our model outperforms state-of-the-art methods across multiple evaluation metrics. Ablation studies further validate the effectiveness of each component in the overall architecture, confirming that the integration of state-space-based temporal sequence modeling and cross-domain feature fusion is critical for robust, high-quality speech enhancement. Our results also indicate that the proposed framework is well-suited for real-world applications where computational efficiency and superior performance are essential.},
  archive  = {J},
  author   = {Xingyu Shen and Runze Wang and Wei-Ping Zhu and Benoit Champagne},
  doi      = {10.1109/TASLPRO.2025.3618543},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-15},
  title    = {Dual-path state-space modeling with cross-domain interaction for multichannel speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuFFIN: Multifaceted pronunciation feedback model with interactive hierarchical neural modeling. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.},
  archive  = {J},
  author   = {Bi-Cheng Yan and Ming-Kang Tsai and Berlin Chen},
  doi      = {10.1109/TASLPRO.2025.3619765},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {MuFFIN: Multifaceted pronunciation feedback model with interactive hierarchical neural modeling},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian sound field reconstruction using partial boundary information. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of reconstructing a spatial sound field from microphone signals and a coarse, partial, and/or uncertain point cloud representation of the boundaries of the room is considered. This problem has downstream applications within sound field control for which precise reconstruction is essential. Typical for these applications is that only microphone measurements are considered, resulting in poor reconstruction in a large spatial region and at high frequencies when few microphones are available. In contrast, in an idealistic setting, where boundary geometry and acoustic properties are known, the sound field can be simulated as a forward problem. However, since the acquisition of such information can be costly and time-consuming, we consider the intermediate setting where partial information of the boundary geometry is available. We formulate the problem in a Bayesian setting, where the boundary information is used to form a prior distribution on the sound field. The paper extends our preliminary work in [1] by allowing for multiple impedance boundary conditions and by introducing a weighting of the boundary points. A scheme for finding an optimal weighting is introduced to reduce the influence of points far from the region of interest or points not consistent with the microphone measurements. Finally, extensive numerical simulation experiments are performed to understand the properties of the boundary-informed regularizer. To further validate the performance and robustness on real data in relation to commonly used regularizers, we release the Field LAser-calibrated Impulse Response (FLAIR) dataset. This dataset consists of 135 microphone measurements along with a laser calibrated, millimeter accurate point cloud of the room geometry and microphone positions that is aimed at stimulating further research in this domain.},
  archive  = {J},
  author   = {David Sundström and Filip Elvander and Andreas Jakobsson},
  doi      = {10.1109/TASLPRO.2025.3619822},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-12},
  title    = {Bayesian sound field reconstruction using partial boundary information},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Less means more: Single stream audio-visual sound source localization via shared-parameter network. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The architecture of two-stream network has been widely adopted in the task of audio-visual learning, especially for sound source localization. With a common way to separately process the different modalities, most current approaches establish the audio-visual correlation by maximizing the cosine similarity of representations from two streams. Unfortunately, the challenge of abundant inference parameters still limits this scheme to be further developed mainly because the parameter of modality-specific networks cannot be reused. Inspired by the mechanism of model averaging, in this study, an Iterative Multi-Modal Parameters Fusion (IMP-Fusion) strategy is proposed to fuse the network parameters during the training phase. By integrating the audio and visual knowledge into a unified architecture, a single-stream network is proposed to handle both modalities in the same time-round. Substantial experiments conducted on challenging benchmarks have validated a superior performance, even with only half of the inference parameter in comparison to the other state-of-the-art works. As a plug-and-play mechanism, the proposed IMP-Fusion strategy is also promising to benefit the design of future audio-visual networks.},
  archive  = {J},
  author   = {Tianyu Liu and Peng Zhang and Junwen Xiong and Chuanyue Li and Yue Huo and Wei Huang and Yufei Zha and Lei Xie and Yanning Zhang},
  doi      = {10.1109/TASLPRO.2025.3619850},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Less means more: Single stream audio-visual sound source localization via shared-parameter network},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining deterministic enhanced conditions with dual-streaming encoding for diffusion-based speech enhancement. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Score-based diffusion models for speech enhancement (SE) need to incorporate correct prior knowledge as reliable conditions to generate accurate predictions. However, providing reliable conditions using noisy features is challenging. One solution is to use features enhanced by deterministic methods as conditions. However, the information distortion and loss caused by deterministic methods might affect the diffusion process. In this paper, we first investigate the effects of using different deterministic SE models as conditions for diffusion. We validate two conditions depending on whether the noisy feature was used as part of the condition: one using only the deterministic feature (deterministic-only), and the other using both deterministic and noisy features (deterministic-noisy). Preliminary investigation found that using deterministic enhanced conditions improves DNSMOS and UTMOS on real data, while the choice between using deterministic-only or deterministic-noisy conditions depends on the deterministic models. Based on these findings, we propose the deterministic-diffusion unified model for SE to more effectively utilize both conditions. Moreover, we found that fine-grained deterministic models have greater potential in objective evaluation metrics, while UNet-based deterministic models provide more stable diffusion performance. Therefore, we also propose a deterministic model that combines coarse- and fine-grained processing for the diffusion. Experimental results on CHiME4 show that the proposed models effectively leverage deterministic models to achieve better SE evaluation scores on the DNSMOS and UTMOS for real evaluation sets. In addition, the proposed deterministic model proves to be more stable than other deterministic models when it is used for diffusion.},
  archive  = {J},
  author   = {Hao Shi and Xugang Lu and Kazuki Shimada and Tatsuya Kawahara},
  doi      = {10.1109/TASLPRO.2025.3619824},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Combining deterministic enhanced conditions with dual-streaming encoding for diffusion-based speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain dialogue state tracking with large language model rationale and disentangled domain-slot attention. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3604650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dialogue state tracking (DST) is a core component of task-oriented dialogue systems. A significant challenge in this task is multi-domain DST, which involves considering dialogue states across multiple domains. Recent advancements have addressed this challenge by exploring various approaches to model the correlations among different domains with domain-slot-specific representations derived from dialogue context and aggregating domain-slot queries using sorts of attention mechanisms. However, existing models still exhibit deficiencies in handling these correlations, either by overlooking or overestimating them. In this paper, we propose a multi-domain DST framework with large language model (LLM) rationale and Disentangled Domain-Slot Attention to address this challenge. Specifically, we introduce a multi-domain aware instruction prompt to guide the LLM to generate the corresponding rationale to dialogue history, realizing these correlations along with the robustness against sorts of variations in spoken conversation. Additionally, we present a novel mechanism, termed Disentangled Domain-Slot Attention. This mechanism enables a dynamic, flexible, and context-dependent manner to extract domain-slot-specific information by disentangling domain-slot queries within the attention mechanism. Through extensive experiments on the MultiWOZ 2.0 and MultiWOZ 2.4 datasets, the results present that the proposed approaches improve the performance of multi-domain DST. In addition, we conducted empirical analyses to comprehensively understand the effectiveness of our proposed approaches.},
  archive  = {J},
  author   = {Longfei Yang and Jiyi Li and Sheng Li and Takahiro Shinozaki},
  doi      = {10.1109/TASLPRO.2025.3604650},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-14},
  title    = {Multi-domain dialogue state tracking with large language model rationale and disentangled domain-slot attention},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring self-supervised audio models for generalized anomalous sound detection. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine anomalous sound detection (ASD) is a valuable technique across various applications. However, its generalization performance is often limited due to challenges in data collection and the complexity of acoustic environments. Inspired by the success of large pre-trained models in numerous fields, this paper introduces a robust ASD model that leverages self-supervised pre-trained models trained on large-scale speech and audio datasets. Although there are inconsistencies between the pre-training datasets and the ASD task, our findings indicate that pre-training still provides substantial benefits for ASD. To mitigate overfitting and retain learned knowledge when fine-tuning with limited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an alternative to full fine-tuning. Additionally, we propose a Machine-aware Group Adapter module, which enables the model to capture differences between various machines within a unified framework, thereby enhancing the generalization performance of ASD systems. To address the challenge of missing attribute labels, we design a novel objective function that dynamically clusters unattributed data using vector quantization and optimizes through a dual-level contrastive learning loss. The proposed methods are evaluated on all benchmark datasets, including the DCASE 2020-2024 five ASD challenges, and the experimental results show significant improvements of our new approach and demonstrate the effectiveness of our proposed strategies.},
  archive  = {J},
  author   = {Bing Han and Anbai Jiang and Xinhu Zheng and Wei-Qiang Zhang and Jia Liu and Pingyi Fan and Yanmin Qian},
  doi      = {10.1109/TASLPRO.2025.3606200},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-15},
  title    = {Exploring self-supervised audio models for generalized anomalous sound detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cross-utterance speech contexts for conformer-transducer speech recognition systems. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3606235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper investigates four types of cross-utterance speech contexts modeling approaches for streaming and non-streaming Conformer-Transformer (C-T) ASR systems: i) input audio feature concatenation; ii) cross-utterance Encoder embeddings concatenation; iii) cross-utterance Encoder embeddings pooling projection; or iv) a novel chunk-based approach applied to C-T models for the first time. An efficient batch training scheme is proposed for contextual C-Ts that uses spliced speech utterances within each minibatch to minimize the synchronization overhead while preserving the sequential order of cross-utterance speech contexts. Experiments are conducted on four benchmark speech datasets across three languages: the English GigaSpeech and Mandarin Wenetspeech corpora used in contextual C-T models pre-training; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets used in domain fine-tuning. The best performing contextual C-T systems consistently outperform their respective baselines using no cross-utterance speech contexts in pre-training and fine-tuning stages with statistically significant average word error rate (WER) or character error rate (CER) reductions up to 0.9%, 1.1%, 0.51%, and 0.98% absolute (6.0%, 5.4%, 2.0%, and 3.4% relative) on the four tasks respectively. Their performance competitiveness against Wav2vec2.0-Conformer, XLSR-128, and Whisper models highlights the potential benefit of incorporating cross-utterance speech contexts into current speech foundation models.},
  archive  = {J},
  author   = {Mingyu Cui and Mengzhe Geng and Jiajun Deng and Chengxi Deng and Jiawen Kang and Shujie Hu and Guinan Li and Tianzi Wang and Zhaoqing Li and Xie Chen and Xunying Liu},
  doi      = {10.1109/TASLPRO.2025.3606235},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-14},
  title    = {Exploring cross-utterance speech contexts for conformer-transducer speech recognition systems},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-perspective inductive answering of subjective questions on products. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3608961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article focuses on the topic of answering subjective questions about products by multi-perspective induction. Unlike traditional factoid QA, whose answers are unique and can be extracted directly from the text, the answers to subjective QA are not a simple text span, but involve various viewpoints and facts. A good answer should incorporate both objective facts and subjective opinions from various sources. Facts often involve multiple fine-grained product aspects, some of which are implicit and not explicitly mentioned. The opinions are complex, hidden, and scattered. Questions may be vague, sometimes requiring commonsense inference. This challenging task has wide-ranging applications, but limited research has studied it. To address this problem, we propose a new model to answer subjective questions in an inductive way. Given a question, we first retrieve the missing but necessary commonsense knowledge to supplement its implicit aspects and hidden relations, to better understand the ask points and users' needs. We then parse and build an aspect tree from the factual data, such as product descriptions, to incorporate all aspects and their parent-child relations. Based on the tree, we infer the question-related aspects from the retrieved content. For each aspect, we aggregate its scattered opinions and objective facts to yield a summary. To deal with diverse types adaptively, we construct an aspect-controlled generation model. Each aspect summary would add a prefix that is learned through a trainable gating mechanism and fused into a multi-aspect decoder to derive a comprehensive answer. The whole model with a retriever and generator is jointly trained by a reinforced framework. Extensive experiments are conducted on our created large-scale SupQA dataset, and the results show the effectiveness of our approach.},
  archive  = {J},
  author   = {Jianxing Yu and Yufeng Zhang and Hanjiang Lai and Wenqing Chen and Yanghui Rao and Jian Yin},
  doi      = {10.1109/TASLPRO.2025.3608961},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-15},
  title    = {Multi-perspective inductive answering of subjective questions on products},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separate and transcribe: Deep guitar string separation and its application for tablature transcription enhancement. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3614466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Guitar string separation constitutes a source separation task in which the individual stems to be retrieved are the signals produced by each distinct guitar string. Certain hardware solutions for string-wise signal manipulation are gaining traction among the guitarist community for applications such as MIDI control and string-level sound effects. However, software techniques for string separation are lagging behind, even though they could form a competitive alternative. This work applies established deep learning architectures commonly used for standard music source separation, to address the task of guitar string separation. It opts for a waveform-to-waveform approach using a multi-channel version of Wave-U-Net. It further suggests that this model can serve as a bridge from separation to transcription by facilitating guitar tablature inference. Tablature transcription enhancement proved feasible using a method that relies on feeding the separated signals to standard tablature inference models, by simply modifying their initial convolution layer to handle multi-source input. Moreover, this work seeks to address the challenges encountered in the more specific string separation scenario where no target signals are readily available for training, meaning that these signals are not provided directly through sophisticated hardware equipment such as polyphonic pickups. Instead, data manipulation and augmentation techniques that produce sample-level synthesized targets are proposed. Thus, two newly created datasets are introduced, namely GS-Aux and ADGP, based on standard guitar datasets: GuitarSet and DadaGP.},
  archive  = {J},
  author   = {Grigoris Bastas and Vassilis Katsouros and Petros Maragos},
  doi      = {10.1109/TASLPRO.2025.3614466},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-12},
  title    = {Separate and transcribe: Deep guitar string separation and its application for tablature transcription enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A learning time-graph frequency representation for monaural speech enhancement. <em>TASLPRO</em>, 1-11. (<a href='https://doi.org/10.1109/TASLPRO.2025.3615450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Graph Fourier Transform (GFT) has recently demonstrated promising results in speech enhancement. However, existing GFT-based speech enhancement approaches often employ fixed graph topologies to build the graph Fourier basis, whose the representation lacks the adaptively and flexibility. In addition, they suffer from the numerical errors and instability introduced by matrix inversion in GFT based on both Singular Value Decomposition (GFT-SVD) and Eigen Vector Decomposition (GFT-EVD). Motivated by these limitations, this paper propose a simple yet effective learnable GFT-SVD framework for speech enhancement. Specifically, we leverage graph shift operators to construct a learnable graph topology and define a learnable graph Fourier basis by the singular value matrices using 1-D convolution (Conv-1D) neural layer. This eliminates the need for matrix inversion, thereby avoiding the associated numerical errors and stability problem. In contrast to complex-valued representation, our proposed learnable Fourier basis provides a real-valued time-graph representation, enabling better magnitude–phase alignment in speech enhancement. Comprehensive evaluations on the VCTK+DEMAND and DNS-2020 benchmarks demonstrate the consistent performance superiority of our learnable GFT-SVD over fixed STFT, GFT-EVD, and GFT-SVD within existing neural speech enhancement frameworks. Source code is available at:https://github.com/Wangfighting0015/GFT_project.},
  archive  = {J},
  author   = {Tingting Wang and Tianrui Wang and Meng Ge and Qiquan Zhang and Xi Shao},
  doi      = {10.1109/TASLPRO.2025.3615450},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {9},
  pages    = {1-11},
  title    = {A learning time-graph frequency representation for monaural speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source separation of sperm whales' echolocation clicks. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3602321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Echolocation clicks, which are used for both localization and locomotion, are often emitted by sperm whales (Physeter macrocephalus). The separation between the sources of overlapping echolocation clicks is a useful tool for biodiversity and behavioral analysis. The problem is challenging due to the similarities between these vocalizations. Current approaches for separating the sources of echolocation clicks are based on short-term signal features, but neglect similarities in the channel impulse response and between sequences of clicks. As a result, they cannot distinguish between sources over long periods of time and thus neglect the temporal relationships of echolocation clicks arranged as click sequences or trains. In this paper, we present a novel three-stage source separation algorithm that utilizes both temporal and spectral click attributes as well as spatial information derived from the arrival time delays between direct and reflected click paths. The algorithm separates clicks by first identifying click sequences in short time windows using MAP (Maximum a Posteriori) estimates, i.e. determining the most probable sequences from a series of measured clicks. These sequences are then grouped into click trains using a flow network model, which are defined as sets of consecutive clicks from a single whale with inter-click intervals (ICI) between 0.4 and 3 seconds. Finally, the resulting click trains are assigned to individual whales using a linear assignment approach. Our method also mitigate errors in the process of click identification. The algorithm is of low complexity and can be processed in real time. Performance evaluation using real measurements of sperm whale vocalizations, emulation of recorded data and noise, and a controlled sea experiment in which recorded echolocation clicks were played back shows better source separation performance compared to benchmarks. High accuracy in click verification and source attribution is demonstrated for different numbers of whales, even under challenging conditions such as overlapping clicks and low signal-to-noise ratio with multiple noise transients.},
  archive  = {J},
  author   = {Guy Gubnitky and Yaly Mevorach and Dan Tchernov and Roee Diamant},
  doi      = {10.1109/TASLPRO.2025.3602321},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {8},
  pages    = {1-15},
  title    = {Source separation of sperm whales' echolocation clicks},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alignment information via optimal transport and pre-training for neural machine translation. <em>TASLPRO</em>, 1-10. (<a href='https://doi.org/10.1109/TASLPRO.2025.3570944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The demand for translation between different languages has increased rapidly as globalization has progressed. Although neural machine translation (NMT) has achieved excellent results through pre-training, existing models lack alignment information, which makes them unsuitable for specific domains. In this paper, a NMT model with alignment information via optimal transport (OT) and pre-training is proposed. First, the representation gap between different languages is narrowed by using alignment information via OT and pre-training (OTAP) to generate domain-specific data for information alignment, thereby learning richer semantic information. Second, a lightweight model, called the DR-Reformer, is introduced, which uses the Reformer as the backbone network and incorporates Dropout and Reduction layers to reduce model parameters and improve computational efficiency without sacrificing accuracy. Experiments conducted on the Chinese and English datasets of AI Challenger 2018 and WMT-17 show that our proposed algorithm outperforms existing algorithms.},
  archive  = {J},
  author   = {Xueping Su and Xingkai Zhao and Yunhong Li and Lina Yao and Matthias Ratsch},
  doi      = {10.1109/TASLPRO.2025.3570944},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {5},
  pages    = {1-10},
  title    = {Alignment information via optimal transport and pre-training for neural machine translation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain adaptation for dependency parsing via in-depth feature transfer. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3572796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dependency parsing endeavours to extract both syntactic and semantic knowledge of the input sentence via a dependency tree. Recently, supervised dependency parsers have achieved significant improvements thanks to the strong representation of pre-trained language models. However, the parsing accuracy drops dramatically when the model is trained on multiple out-of-domain training datasets. The key to addressing this problem is to learn the commonalities and differences between different domains. Although the widely used sharedprivate model attempts to capture domain-invariant and domainspecific feature representations by separated encoders, the two representations may interfere with each other, and the in-depth relationship between different domain features may be ignored. In this work, we explore to emphasize the useful domain-specific features and filter out the harmful ones in explicit and implicit aspects via different feature transfer strategies. Simultaneously, we leverage adversarial learning to capture more effective domain-invariant features that belong to both source and target domains. Experimental results show that our proposed model can improve the parsing accuracy of several strong baseline models significantly. The thorough analysis helps us to gain more insights into the knowledge transfer process between different modules.},
  archive  = {J},
  author   = {Ying Li and Shichang Zhu and Jianjian Liu and Zhengtao Yu and Yuxin Huang},
  doi      = {10.1109/TASLPRO.2025.3572796},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {5},
  pages    = {1-14},
  title    = {Multi-source domain adaptation for dependency parsing via in-depth feature transfer},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OmniDialog: An omnipotent pre-training model for task-oriented dialogue system. <em>TASLPRO</em>, 1-11. (<a href='https://doi.org/10.1109/TASLPRO.2025.3542301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pre-trained conversation models (PCMs) have demonstrated remarkable results in task-oriented dialogue (TOD) systems. Many PCMs focus predominantly on dialogue management tasks like dialogue state tracking, dialogue generation tasks like response generation, or both. However, the existing PCMs seldom consider dialogue comprehension tasks, such as dialogue question answering and summarization tasks. These tasks allow PCMs to glean dialogue context from various angles. This observation naturally raises the question: Can the performance of downstream dialogue tasks be enhanced if a PCM is pre-trained on dialogue management, generation, and comprehension tasks? To investigate this, we proposed an Omnipotent Dialogue pre-training model (OmniDialog). It unifies these three dialogue tasks into a monolithic framework by multi-task learning, fostering inter-task communication. The pre-training corpus of OmniDialog spans 7 dialogue-focused tasks, drawing from 15 datasets and encompassing over 3.2 million dialogue utterances. To our knowledge, OmniDialog is a pioneering PCM pre-trained across dialogue management, generation, and comprehension domains. We evaluated its performance across four tasks: dialogue summarization, end-to-end dialogue modeling, dialogue state tracking, and intent classification. The results underscore its efficacy in domain transfer learning, low-resource, and full-dataset scenarios. Furthermore, to glean a nuanced understanding of OmniDialog's strengths and potential pitfalls, we designed a fine-grained analysis framework for dialogue-centric tasks. Experimental results show that the OmniDialog is good at hard samples, such as long dialogues and lengthy responses. We make our code and datasets publicly available. https://github.com/Itaaaachi/OmniDialog},
  archive  = {J},
  author   = {Mingtao Yang and See-Kiong Ng and Jinlan Fu},
  doi      = {10.1109/TASLPRO.2025.3542301},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {2},
  pages    = {1-11},
  title    = {OmniDialog: An omnipotent pre-training model for task-oriented dialogue system},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MvSR-NAT: Multi-view subset regularization for non-autoregressive machine translation. <em>TASLPRO</em>, 1-10. (<a href='https://doi.org/10.1109/TASLP.2022.3221043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by predicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model. Specifically, MvSR consists of two parts: (1) shared mask consistency: we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) model consistency, we maintain an exponential moving average of the model weights, and enforce the predictions to be consistent between the average model and the online model. Without changing the CMLM-based architecture, our approach achieves remarkable performance on three public benchmarks with 0.7-1.15 BLEU gains over previous NAT models. And, we reduce the gap to the stronger Transformer baseline.},
  archive  = {J},
  author   = {Pan Xie and Zexian Li and Zheng Zhao and Jiaqi Liu and Xiaohui Hu},
  doi      = {10.1109/TASLP.2022.3221043},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {11},
  pages    = {1-10},
  title    = {MvSR-NAT: Multi-view subset regularization for non-autoregressive machine translation},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
