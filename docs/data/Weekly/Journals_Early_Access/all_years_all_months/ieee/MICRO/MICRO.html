<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro">MICRO - 23</h2>
<ul>
<li><details>
<summary>
(2025). Comparative analysis of loosely and tightly coupled accelerator architectures for machine learning. <em>MICRO</em>, 1-12. (<a href='https://doi.org/10.1109/MM.2025.3603837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid innovation of AI/ML workloads, accelerators have become an important part of the computing resources in datacenters. While characteristics such as performance, power, and efficiency are critical, developer velocity, which dictates how quickly a new workload can be deployed on an accelerator, is equally important. The accelerator architecture and properties it exposes to the higher-level programming model play a key role in determining the programming model and economic viability of the accelerators. We compare two classes of AI/ML accelerator architectures: an efficient loosely coupled accelerator already implemented in silicon and a tightly coupled accelerator with a traditional and user-friendly programming model. Our comparison shows that the performance of the design depends on the amount of hardware resources and not on how they are integrated, concluding that it is possible to architect accelerators in a much simpler and more compiler-friendly manner, while still maintaining the benefits of offload computing.},
  archive      = {J_MICRO},
  author       = {Amin Firoozshahian and Joel Coburn and Ajit Punj and Aravind Sukumaran Rajam and Colby Boyer and Rakesh Nattoji and Mahima Bathla and Bob Dreyer and Sujith Srinivasan and Harshitha Pilla and Michael Rotzin and Surendra Rajupalem and K. Rajesh Jagannath and Krishna Noru and Harikrishna Reddy and Chris Yang and Charlie Hong-Men Su and Charlie Cheng},
  doi          = {10.1109/MM.2025.3603837},
  journal      = {IEEE Micro},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Micro},
  title        = {Comparative analysis of loosely and tightly coupled accelerator architectures for machine learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UB-mesh: A hierarchically localized nD-FullMesh datacenter network architecture. <em>MICRO</em>, 1-9. (<a href='https://doi.org/10.1109/MM.2025.3592688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scaling of Large-scale Language Models (LLMs) demands unprecedented computational power and bandwidth. We present UB-Mesh, an innovative AI datacenter network architecture that enhances scalability, performance, and cost-efficiency through a hierarchical nD-FullMesh topology. Unlike traditional symmetrical designs, UB-Meshoptimizes LLM training by prioritizing localized data movement and minimizing switch usage. The architecture features UB-Mesh-Pod, a physical implementation of 4D-FullMesh using custom hardware including NPUs, CPUs, Low/High-Radix Switches (LRS/HRS), and NICs, interconnected via our Unified Bus (UB) technology for dynamic resource allocation. For network optimization, we introduce All-Path-Routing (APR) to efficiently manage data traffic. Combined with topology-aware performance tuning and robust reliability mechanisms like 64 + 1 backup, UB-Meshachieves 2.04× better cost-efficiency and 7.2% higher availability than Clos networks. These innovations address the critical challenges of building practical, high-performance AI infrastructure at scale.},
  archive      = {J_MICRO},
  author       = {Heng Liao and Bingyang Liu and Xianping Chen and Zhigang Guo and Chuanning Cheng and Jianbing Wang and Xiangyu Chen and Peng Dong and Rui Meng and Wenjie Liu and Zhe Zhou and Ziyang Zhang and Yuhang Gai and Cunle Qian and Yi Xiong and Zhongwu Cheng and Jing Xia and Yuli Ma and Xi Chen and Wenhua Du and Shizhong Xiao and Chungang Li and Yong Qin and Liudong Xiong and Zhou Yu and Lv Chen and Lei Chen and Buyun Wang and Pei Wu and Junen Gao and Xiaochu Li and Jian He and Shizhuan Yan and Bill McColl},
  doi          = {10.1109/MM.2025.3592688},
  journal      = {IEEE Micro},
  month        = {9},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {UB-mesh: A hierarchically localized nD-FullMesh datacenter network architecture},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corsair: An in-memory computing chiplet architecture for inference-time compute acceleration. <em>MICRO</em>, 1-11. (<a href='https://doi.org/10.1109/MM.2025.3593444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in Generative AI (GenAI) have reinvigorated research into novel computing architectures such as Transformer. Transformer, characterized by low arithmetic intensity during most of the inference time, has become the cornerstone of GenAI underlying Large Language (LLM) and Reasoning Models (RM). Numerous solutions to the intense memory bandwidth problem have been proposed. Corsair is an architecture that targets this need using chiplet design, digital in-memory computing-based matrix engine, efficient die-to-die interconnects, block floating point numerics, and large high-bandwidth on-chip memories. We describe the Corsair chiplet, scaling approaches to compose larger systems, and outline the software stack. We formulate the inference-time requirements of LLM and RM computation, memory bandwidth, memory capacity, and interconnect efficiency for scaling. We also show how Corsair design perfectly fits these workloads. We present benchmark results from Corsair silicon that correlate strongly with the design and preview an estimate of workload-level improvements expected with Corsair.},
  archive      = {J_MICRO},
  author       = {Satyam Srivastava and Akhil Arunkumar and Nithesh Kurella and Amrit Panda and Gaurav Jain and Purushotham Kamath and Mark Wutzke and Arun Tiruvur and Mike Gupta and Ilya Soloveychik and Vamsi Darsi and Malav Dalal and Vinayak Patankar and Sasidhar Dudyala and Senthil Duraisamy and Santhosh Ramchandran and Raghav Venkatasubramanian and Yuwei Qin and Xin Wang and Jayaprakash Balachandran and Ali Gok and Piotr Wojciechowski and Saliya Ekanayake and Chris Ng and Ranju Sarma and Shubhankit Rathore and Tristan Trouwen and Siwei Zhuang and Chris Nicol and Sudeep Bhoja},
  doi          = {10.1109/MM.2025.3593444},
  journal      = {IEEE Micro},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Micro},
  title        = {Corsair: An in-memory computing chiplet architecture for inference-time compute acceleration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiFrost: A composable, resilient interconnect network architecture for scalable AI systems. <em>MICRO</em>, 1-11. (<a href='https://doi.org/10.1109/MM.2025.3594150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models demand extreme memory bandwidth and compute density, surpassing traditional architectures. Chiplet-based designs and 3D memory stacking offer solutions, but introduce challenges in integration, yield, and software compatibility. We present BiFrost, a composable network architecture that addresses these challenges through three innovations. BiFrost implements a FatTree network on chiplets, enabling intra- and inter-chiplet communication with 820 TB/s aggregate raw bandwidth and sub-100 ns latency in a 24 chiplet setup. A unified address space, enabled by a novel packet address compute unit, ensures compatibility with POSIX shared-memory models. Resilience is achieved through adaptive routing and dynamic memory remapping, ensuring graceful degradation upon faults. Built on a 3nm process, BiFrost features 0.48 mm2 per 16 × 16 router and 17.2 mm2 per chiplet. Flexible routing supports topologies like Dragonfly and HyperX. BiFrost maintains 90% peak bandwidth under uniform random traffic and offers OS-level configurability, enabling seamless scaling from single chiplet to multi-rack systems.},
  archive      = {J_MICRO},
  author       = {Gurpreet Singh Kalsi and Hong Wang and Jason M Howard and Joshua B Fryman and Fabrizio Petrini and Daniel S Klowden and Sanjaya Tayal and Anil Rao and Steve Pawlowski},
  doi          = {10.1109/MM.2025.3594150},
  journal      = {IEEE Micro},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Micro},
  title        = {BiFrost: A composable, resilient interconnect network architecture for scalable AI systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing and efficiently accelerating multimodal generation model inference. <em>MICRO</em>, 1-11. (<a href='https://doi.org/10.1109/MM.2025.3596539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (AI) technology is revolutionizing the computing industry, posing new system design and optimization opportunities. In particular, AI’s ability to understand and respond in multiple modalities comes with significant system resource demands. To sustainably scale generative AI capabilities to billions of users in the world, inference must be fast and efficient. This paper pinpoints key system design and optimization opportunities by characterizing a family of emerging multi-modal generation models on real systems. Auto-regressive token generation is a critical latency performance bottleneck, typically dominated by GPU idle time. In addition to memory-intensive attention across the generative AI models, linear operations constitute significant inference latency due to the feed forward networks in Transformer-based models. We demonstrate that state-of-the-art optimization levers, spanning from applications to system software and hardware, set a 3.88× better baseline.},
  archive      = {J_MICRO},
  author       = {Yejin Lee and Alicia Golden and Anna Sun and Basil Hosmer and Bilge Acun and Can Balioglu and Changhan Wang and Charles David Hernandez and Christian Puhrsch and Daniel Haziza and Driss Guessous and Francisco Massa and Jacob Kahn and Jeffrey Wan and Jeremy Reizenstein and Jiaqi Zhai and Joe Isaacson and Joel Schlosser and Juan Pino and Kaushik Ram Sadagopan and Leonid Shamis and Linjian Ma and Min-Jae Hwang and Mingda Chen and Mostafa Elhoushi and Pedro Rodriguez and Ram Pasunuru and Samuel Hsia and Scott Yih and Sravya Popuri and Xing Liu and Carole-Jean Wu},
  doi          = {10.1109/MM.2025.3596539},
  journal      = {IEEE Micro},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Micro},
  title        = {Characterizing and efficiently accelerating multimodal generation model inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CXL-GPU: Pushing GPU memory boundaries with the integration of CXL technologies. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3582433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a GPU storage expansion solution utilizing CXL, featuring a novel GPU system design with multiple CXL root ports for integrating diverse storage media (DRAMs and/or SSDs). We developed and siliconized a custom CXL controller integrated at the hardware RTL level, achieving two-digit nanosecond round-trip latency, the first in the field. This study also includes speculative read and deterministic store mechanisms to efficiently manage read and write operations to hide the endpoint’s backend media latency variation. Performance evaluations reveal our approach significantly outperforms existing methods, marking a substantial advancement in GPU storage technology.},
  archive      = {J_MICRO},
  author       = {Donghyun Gouk and Seungkwan Kang and Seungjun Lee and Jiseon Kim and Kyungkuk Nam and Eojin Ryu and Sangwon Lee and Dongpyung Kim and Junhyeok Jang and Hanyeoreum Bae and Myoungsoo Jung},
  doi          = {10.1109/MM.2025.3582433},
  journal      = {IEEE Micro},
  month        = {7},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {CXL-GPU: Pushing GPU memory boundaries with the integration of CXL technologies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From block to byte: Transforming PCIe SSDs with CXL memory protocol and instruction annotation. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3581448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We focus on cacheability as a critical feature for adapting block storage to CXL’s memory-centric architecture and introduce Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9× better performance than PCIe-based memory expanders and further reduces the latency by 5.4× with annotation enhancements. This work demonstrates the feasibility of integrating block storage into CXL’s ecosystem and provides a foundation for future memory-storage convergence.},
  archive      = {J_MICRO},
  author       = {Miryeong Kwon and Donghyun Gouk and Junhyeok Jang and Jinwoo Baek and Hyunwoo You and Sangyoon Ji and Hongjoo Jung and Junseok Moon and Seungkwan Kang and Seungjun Lee and Myoungsoo Jung},
  doi          = {10.1109/MM.2025.3581448},
  journal      = {IEEE Micro},
  month        = {7},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {From block to byte: Transforming PCIe SSDs with CXL memory protocol and instruction annotation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance characterizations and usage guidelines of samsung CMM-H. <em>MICRO</em>, 1-9. (<a href='https://doi.org/10.1109/MM.2025.3591577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the first publicly available study for comprehensive characterizations of an FPGA-based Samsung’s CXL Memory Module Hybrid (CMM-H) prototype. It leverages emerging CXL technology to enable high-speed and cacheline-granular communication between CPUs and NAND flash, offering three-fold benefits: byte-addressability, scalable capacity, and persistence at a low cost. Samsung’s CMM-H delivers these benefits through a hardware-only solution, i.e., it does not incur any OS/IO overheads like conventional block devices. In particular, CMM-H integrates a DRAM cache with NAND flash in a single device to deliver near-DRAM latency. Through this study, we address users’ concerns about whether a wide variety of applications can successfully run on a memory device backed by NAND flash medium. Additionally, based on these characterizations, we provide key insights into how to best take advantage of the CMM-H device.},
  archive      = {J_MICRO},
  author       = {Jianping Zeng and Shuyi Pei and Da Zhang and Yuchen Zhou and Amir Beygi and Xuebin Yao and Ramdas Kachare and Tong Zhang and Zongwang Li and Marie Nguyen and Rekha Pitchumani and Yang Soek Ki and Changhee Jung},
  doi          = {10.1109/MM.2025.3591577},
  journal      = {IEEE Micro},
  month        = {7},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {Performance characterizations and usage guidelines of samsung CMM-H},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An introduction to life-cycle emissions of AI hardware. <em>MICRO</em>, 1-10. (<a href='https://doi.org/10.1109/MM.2025.3592568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specialized hardware accelerators aid the rapid advancement of artificial intelligence (AI), and their efficiency impacts AI’s environmental sustainability. This study presents the first publication of a comprehensive AI accelerator life-cycle assessment (LCA) of greenhouse gas emissions, including the first publication of manufacturing emissions of an AI accelerator. Our analysis of five Tensor Processing Units (TPUs) encompasses all stages of the hardware lifespan—from material extraction and manufacturing, to energy consumption during training and serving of AI models. Using first-party data, it offers the most comprehensive evaluation of AI hardware’s environmental impact. We introduce a new metric, compute carbon intensity (CCI), that will help evaluate AI hardware sustainability and estimate the carbon footprint of training and inference. We show that CCI improves 3x from TPU v4i to TPU v6e. Moreover, while this paper’s focus is on hardware, software advancements leverage and amplify these gains.},
  archive      = {J_MICRO},
  author       = {Ian Schneider and Hui Xu and Stephan Benecke and David Patterson and Keguo Huang and Parthasarathy Ranganathan and Cooper Elsworth},
  doi          = {10.1109/MM.2025.3592568},
  journal      = {IEEE Micro},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Micro},
  title        = {An introduction to life-cycle emissions of AI hardware},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speculative decoding on the SN40L reconfigurable dataflow unit. <em>MICRO</em>, 1-10. (<a href='https://doi.org/10.1109/MM.2025.3592570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speculative decoding has emerged as a promising optimization strategy to accelerate generative AI inference. This technique enhances the autoregressive decoding phase by using a smaller draft model to generate a few tokens, which are then validated by a larger model. However, synchronization overheads on both GPUs and hosts limit the performance gains. This article explores the implementation and optimization potential of batched speculative decoding within SambaNova’s SN40L reconfigurable dataflow unit (RDU). We achieve more than 75% of the theoretical maximum performance for speculative decoding and delivering a 6× speedup compared to the baseline model. Furthermore, we demonstrate that a single SN40L rack, containing 16 sockets and offering HBM bandwidth comparable to the DGX H100, outperforms the latter by up to 1.7×. The techniques and models discussed are deployed in SambaNova’s production AI inference cloud, cloud.sambanova.ai, showcasing their tangible impact on large-scale AI applications.},
  archive      = {J_MICRO},
  author       = {Pushkar Nandkar and Darshan Gandhi and Nasim Farahini and Håkan Zeffer and John Long and Samuel Rydh and Matheen Musaddiq and Tuowen Zhao and Joshua Brot and Reid Goodbar and Yun Du and Mingran Wang and Raghu Prabhakar},
  doi          = {10.1109/MM.2025.3592570},
  journal      = {IEEE Micro},
  month        = {7},
  pages        = {1-10},
  shortjournal = {IEEE Micro},
  title        = {Speculative decoding on the SN40L reconfigurable dataflow unit},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlexMem: Tiered cache sharing service for virtual machine images based on memory pool. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3574139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtualization-based cloud computing, numerous redundant tiered memory pages exist across independent virtual machines (VMs) deployed on different physical hosts within the data center. For instance, each VM typically loads the data from its virtual disk into its memory (local tier), without considering the existing data in other remote VMs’ memory (remote tier). In this paper, we propose FlexMem, a preliminary architecture designed to eliminate memory redundancy for qcow2-based VMs. First, FlexMem introduces a virtual persistent memory (PMem) device that facilitates cross-host memory consolidation by leveraging Remote Direct Memory Access (RDMA) for fast remote read operations. Second, FlexMem employs flexible backing specifications that are memory-mapped (mmapped) to sparse and layered qcow2 images. By delivering images to VMs through FlexMem and enabling guest Direct Access (DAX), user programs within the guest can directly access identical images already cached in the remote host’s memory. Our evaluation demonstrates that FlexMem significantly improves memory efficiency and the availability of VMs with reduced hardware costs, ultimately enhancing the competitiveness of cloud vendors.},
  archive      = {J_MICRO},
  author       = {Zhihao Zhang and Weinan Liu and Zhenlong Song and Xinbiao Gan and Yue Yu and Yiming Zhang},
  doi          = {10.1109/MM.2025.3574139},
  journal      = {IEEE Micro},
  month        = {6},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {FlexMem: Tiered cache sharing service for virtual machine images based on memory pool},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compute-enabled CXL memory expansion for efficient retrieval augmented generation. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3575280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional near-memory processing architectures often strike a trade-off between memory capacity and memory bandwidth, leading to high initial data movement or high capital costs due to memory stranding. In this work, we introduce compute-enabled memory expansion enabled by CXL as a solution for the widespread adoption of near-memory processing at scale. We present the Intelligent Knowledge Store (IKS), which is fundamentally a memory expander with lightweight near-memory accelerators that leverage high internal memory bandwidth to accelerate dense retrieval, a key component of retrieval-augmented generation (RAG). IKS disaggregates its internal memory capacity and supports both spatial and temporal multi-tenancy. It significantly accelerates high-quality dense retrieval while enabling multi-tenancy with modest memory access interference.},
  archive      = {J_MICRO},
  author       = {Derrick Quinn and Neel Patel and Mohammad Alian},
  doi          = {10.1109/MM.2025.3575280},
  journal      = {IEEE Micro},
  month        = {6},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Compute-enabled CXL memory expansion for efficient retrieval augmented generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing interconnect bandwidth and efficiency in NVMe-based key-value SSDs with fine-grained value transfer. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3572475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Key-Value SSD (KV-SSD) redefines storage interfaces by integrating a key-value store directly within the device, offering native support for non-page-aligned key-value pairs. This architectural innovation enables KV-SSDs to offload storage management from the host system, positioning them as ideal candidates for resource disaggregation. However, KV-SSDs face significant challenges, notably I/O amplification caused by conflicts with traditional storage protocols like NVMe, which are designed around memory page units. Specifically, this results in a substantial increase in data traffic over interconnect between the host and SSD. This paper introduces BandSlim, a novel solution addressing these challenges in data transfer by leveraging (i) NVMe command piggybacking for universally compatible, fine-grained transfers without requiring interconnect-level support and (ii) the remote memory access capabilities of emerging Compute eXpress Link (CXL) interconnects for higher-performance fine-grained transfers. Through extensive evaluations, BandSlim achieves up to a 97.9% reduction in PCIe traffic compared to conventional NVMe KV-SSDs.},
  archive      = {J_MICRO},
  author       = {Junhyeok Park and Chang-Gyu Lee and Soon Hwang and Seung-Jun Cha and Woosuk Chung and Youngjae Kim},
  doi          = {10.1109/MM.2025.3572475},
  journal      = {IEEE Micro},
  month        = {5},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Maximizing interconnect bandwidth and efficiency in NVMe-based key-value SSDs with fine-grained value transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving SQL join algorithms for distributed systems: A case study of CXL-based multi-host shared memory. <em>MICRO</em>, 1-9. (<a href='https://doi.org/10.1109/MM.2025.3574357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Compute Express Link (CXL) has introduced the possibility of multi-host shared memory architectures. Despite this advancement, there has been limited exploration of shared memory at the application layer. Traditional distributed systems typically partition data across multiple servers, enabling independent processing. However, cross-partition operations, such as joins, require data repartitioning, leading to significant communication overhead. To address this challenge, we propose Merge Hash Join (MHJ), a novel SQL join algorithm that leverages shared memory to eliminate the need for repartitioning. By storing the joining table in shared memory and making it directly accessible to all servers, MHJ significantly reduces communication overhead. To validate our approach, we implemented MHJ and the necessary shared memory functionalities on a CXL-based shared memory prototype. Extensive evaluations using the industry-standard TPC-DS benchmark demonstrate that MHJ achieves up to 1.5× performance improvement compared to conventional join algorithms.},
  archive      = {J_MICRO},
  author       = {JaeYung Jun and HyunWoong Ahn and Joohee Lee and Jungmin Choi and Byungil Koh and Donguk Moon},
  doi          = {10.1109/MM.2025.3574357},
  journal      = {IEEE Micro},
  month        = {5},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {Improving SQL join algorithms for distributed systems: A case study of CXL-based multi-host shared memory},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FS2: A fast, scalable and flexible switching system for emerging interconnects. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3574732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cache-coherent interconnects such as CXL and CCIX have been introduced thanks to their cache coherence for a shared address space. However, we observe that it is difficult to scale such interconnects with 10-100s devices to fulfill the ever-increasing memory demands of bigdata applications. In this paper, we propose a switch-assisted scalable system architecture on top of those interconnects. Specifically, we introduce shared caches in interconnect-switches to efficiently exploit data reuse opportunities and to overcome the topological limitations by flexibly changing the topologies based on workload patterns. Our evaluation shows that our switch-assisted architecture provides higher scalability and up to 4.4× higher performance than native designs.},
  archive      = {J_MICRO},
  author       = {Heetaek Jeong and Kanghyun Choi and Hamin Jang and Donup-Kwon and Eunjin-Baek and Pyeongsu-Park and Jangwoo Kim},
  doi          = {10.1109/MM.2025.3574732},
  journal      = {IEEE Micro},
  month        = {5},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {FS2: A fast, scalable and flexible switching system for emerging interconnects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Containerized in-storage processing and computing-enabled SSD disaggregation. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3574261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ISP minimizes data transfer for analytics but faces challenges in adaptation and disaggregation. We propose DockerSSD, an ISP model leveraging OS-level virtualization and lightweight firmware to enable containerized data processing directly on SSDs. Key features include Ethernet over NVMe for network-based ISP management and Virtual Firmware for secure, efficient container execution. DockerSSD supports disaggregated storage pools, reducing host overhead and enhancing large-scale services like LLM inference. It achieves up to 2.0× better performance for I/O-intensive workloads, and 7.9× improvement in distributed LLM inference.},
  archive      = {J_MICRO},
  author       = {Miryeong Kwon and Donghyun Gouk and Eunjee Na and Jiseon Kim and Junhee Kim and Hyein Woo and Eojin Ryu and Hyunkyu Choi and Jinwoo Baek and Hanyeoreum Bae and Mahmut Kandemir and Myoungsoo Jung},
  doi          = {10.1109/MM.2025.3574261},
  journal      = {IEEE Micro},
  month        = {5},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Containerized in-storage processing and computing-enabled SSD disaggregation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CXL topology-aware and expander-driven prefetching: Unlocking SSD performance. <em>MICRO</em>, 1-9. (<a href='https://doi.org/10.1109/MM.2025.3574293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating compute express link (CXL) with SSDs allows scalable access to large memory but has slower speeds than DRAMs. We present ExPAND, an expander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching from host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for prefetching and ensures data consistency with CXL.mem’s back-invalidation. We examine prefetch timeliness for accurate latency estimation. ExPAND, being aware of CXL multi-tiered switching, provides end-to-end latency for each CXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD reliance and enables direct host cache access for most data. ExPAND enhances graph application performance and SPEC CPU’s performance by 9.0× and 14.7×, respectively, surpassing CXL-SSD pools with diverse prefetching strategies.},
  archive      = {J_MICRO},
  author       = {Dongsuk Oh and Miryeong Kwon and Jiseon Kim and Eunjee Na and Junseok Moon and Hyunkyu Choi and Seonghyeon Jang and Hanjin Choi and Hongjoo Jung and Sangwon Lee and Myoungsoo Jung},
  doi          = {10.1109/MM.2025.3574293},
  journal      = {IEEE Micro},
  month        = {5},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {CXL topology-aware and expander-driven prefetching: Unlocking SSD performance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient disaggregated cloud storage for cold videos with neural enhancement. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3562625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of video-sharing platforms has driven immense storage demands, with disaggregated cloud storage emerging as a scalable and reliable solution. However, the proportional cost of cloud storage relative to capacity and duration limits the cost-efficiency for managing large-scale video data. This is particularly critical for cold videos, which constitute the majority of video data but are accessed infrequently. To address this challenge, this paper proposes Neural Cloud Storage (NCS), leveraging content-aware super-resolution (SR) powered by deep neural networks. By reducing the resolution of cold videos, NCS decreases file sizes while preserving perceptual quality. optimizing the cost trade-offs in multi-tiered disaggregated storage. This approach extends the cost-efficiency benefits to a greater range of cold videos and achieves up to a 21.2% reduction in total cost of ownership (TCO), providing a scalable, cost-effective solution for video storage.},
  archive      = {J_MICRO},
  author       = {Jinyeong Lim and Junhyeok Jang and Myoungsoo Jung and Dongsu Han},
  doi          = {10.1109/MM.2025.3562625},
  journal      = {IEEE Micro},
  month        = {4},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Efficient disaggregated cloud storage for cold videos with neural enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ginkgo: A learned index enhanced tiered memory system. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3564395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL), as an emerging high-speed interconnect protocol, offers a promising approach to memory expansion. Organizing fast DDR DRAM and large CXL memory as a tiered memory system is preferred to obtain the large memory capacity while maintaining high memory performance. Additionally, such a tiered memory system also adopts huge pages to alleviate address translation overhead. However, due to memory bloating issues in huge pages, existing hardware-based tiered memory management systems suffer from severe cache conflicts and DDR DRAM underutilization, resulting in significant performance degradation. We introduce Ginkgo, a learned-index enhanced tiered memory system, which builds a distribution-aware cache index mechanism while leveraging the learned index to minimize metadata overhead. Evaluation reveals that Ginkgo reduces average memory access latency with an average of 3.1% and 13.5% compared to state-of-the-art Alloy Cache and Unison Cache, respectively.},
  archive      = {J_MICRO},
  author       = {Xiran Yang and Yifei Yu and Chuandong Li and Jianqiang Zeng and Ke Zhou and Diyu Zhou and Xiaolin Wang and Zhenlin Wang and Yingwei Luo},
  doi          = {10.1109/MM.2025.3564395},
  journal      = {IEEE Micro},
  month        = {4},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Ginkgo: A learned index enhanced tiered memory system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving remote file access in distributed object stores by decoupling metadata and data paths using NVMe-oF. <em>MICRO</em>, 1-8. (<a href='https://doi.org/10.1109/MM.2025.3564477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storage network protocols such as NVMe-oF operate below the file system layer. Therefore, even when NVMe-oF allows storage volumes to be shared across the network, compute nodes cannot access remote files managed by another node’s file system without a cluster file system. In conventional distributed systems, accessing files owned by a remote node requires communication with the remote node via RPC. The remote node then retrieves the data from a disaggregated storage node and transfers it to the requesting node. To reduce redundant network traffic, this study proposes RDIO, which separates RPC-based remote data access into two distinct planes: a metadata plane for file mapping and a data plane for direct access to remote storage. The data plane ensures data flows only through the storage network. We integrate RDIO into MinIO and show that RDIO significantly improves performance by reducing remote data movement between nodes.},
  archive      = {J_MICRO},
  author       = {Daegyu Han and Sungho Moon and Kyeungpyo Kim and Sung-Soon Park and Beomseok Nam},
  doi          = {10.1109/MM.2025.3564477},
  journal      = {IEEE Micro},
  month        = {4},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {Improving remote file access in distributed object stores by decoupling metadata and data paths using NVMe-oF},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MUDDLE: Multi-modal dynamic detector loopback evaluator to expose trojans in zero-trust PCB systems. <em>MICRO</em>, 1-9. (<a href='https://doi.org/10.1109/MM.2024.3520996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the MUDDLE framework for hardware Trojan detection in Printed Circuit Boards (PCBs) in a golden-free and zero-trust setting (unavailable known-good PCB, measurements from processors on the PCB are not trustworthy). Trojans could be inserted by adversaries during/after manufacturing to impact safety, stability, and performance of cyber-physical systems (CPS). MUDDLE monitors side channels under defender-controlled software-driven excitations and I/O connections to create loopbacks. Side channel measurements are acquired by an external test rig that is configurable (using an FPGA). Using FPGA-based reconfigurable logic, the defender instantiates active dynamic loopbacks crafted to illuminate Trojans. Side channel time series are evaluated against models constructed from golden-free design-based characterizations to probabilistically detect Trojans. We demonstrate efficacy of MUDDLE on a reconfigurable PCB Trojan testbed (built atop the OpenPLC programmable logic controller (PLC) platform) combined with an external configurable test rig.},
  archive      = {J_MICRO},
  author       = {Prashanth Krishnamurthy and Hammond Pearce and Virinchi Roy Surabhi and Joshua Trujillo and Ramesh Karri and Farshad Khorrami},
  doi          = {10.1109/MM.2024.3520996},
  journal      = {IEEE Micro},
  month        = {12},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {MUDDLE: Multi-modal dynamic detector loopback evaluator to expose trojans in zero-trust PCB systems},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReDL: A hybrid memory system with scalable data management strategies for DNN applications. <em>MICRO</em>, 1-7. (<a href='https://doi.org/10.1109/MM.2024.3458992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory resource is a critical bottleneck for large-scale Deep Neural Network (DNN) applications. Hybrid Memory System (HMS) provides a promising solution to increase memory capacity in an affordable way. However, to release the power of HMS, data migration plays an important role. Deploying DNN on HMS imposes enormous challenges on data migration strategy and inspires us to pursue smart solutions. We propose a runtime system for HMS that automatically optimizes scalable data migrations and exploits domain knowledge on DNNs to manage data between fast and slow memory in HMS. We also introduce a reference distance and location based data management strategy (ReDL), treating short-lived and long-lived data objects with Idle and Dynamic migration methods, respectively. The experimental results demonstrate that with configuring the size of fast memory to be 20% of each workload’s peak memory consumption, our work achieves similar performance to the fast memory-only system.},
  archive      = {J_MICRO},
  author       = {Ningning Pan and Lei Ma and Xinting Ge and Hang Xiao and Xiaodan Sui and Yanyun Jiang and Yuanjie Zheng and Wei Rang},
  doi          = {10.1109/MM.2024.3458992},
  journal      = {IEEE Micro},
  month        = {9},
  pages        = {1-7},
  shortjournal = {IEEE Micro},
  title        = {ReDL: A hybrid memory system with scalable data management strategies for DNN applications},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CIMNet: Joint search for neural network and computing-in-memory architecture. <em>MICRO</em>, 1-12. (<a href='https://doi.org/10.1109/MM.2024.3409068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing-in-memory (CIM) architecture has been proven to effectively transcend the memory wall bottleneck, expanding the potential of low-power and high-throughput applications such as machine learning. Neural architecture search (NAS) designs ML models to meet a variety of accuracy, latency, and energy constraints. However, integrating CIM into NAS presents a major challenge due to additional simulation overhead from the non-ideal characteristics of CIM hardware. This work introduces a quantization and device aware accuracy predictor that jointly scores quantization policy, CIM architecture, and neural network architecture, eliminating the need for time-consuming simulations in the search process. We also propose reducing the search space based on architectural observations, resulting in a well-pruned search space customized for CIM. These allow for efficient exploration of superior combinations in mere CPU minutes. Our methodology yields CIMNet, which consistently improves the trade-off between accuracy and hardware efficiency on benchmarks, providing valuable architectural insights.},
  archive      = {J_MICRO},
  author       = {Xuan-Jun Chen and Chia-Lin Yang},
  doi          = {10.1109/MM.2024.3409068},
  journal      = {IEEE Micro},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Micro},
  title        = {CIMNet: Joint search for neural network and computing-in-memory architecture},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
