<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TROB</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="trob">TROB - 29</h2>
<ul>
<li><details>
<summary>
(2025). Integration of robot and scene kinematics for sequential mobile manipulation planning. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3605261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Sequential Mobile Manipulation Planning (SMMP) framework that can solve long-horizon multi-step mobile manipulation tasks with coordinated whole-body motion, even when interacting with articulated objects. By abstracting environmental structures as kinematic models and integrating them with the robot's kinematics, we construct an Augmented Configuration Apace (A-Space) that unifies the previously separate task constraints for navigation and manipulation, while accounting for the joint reachability of the robot base, arm, and manipulated objects. This integration facilitates efficient planning within a tri-level framework: a task planner generates symbolic action sequences to model the evolution of A-Space, an optimization-based motion planner computes continuous trajectories within A-Space to achieve desired configurations for both the robot and scene elements, and an intermediate plan refinement stage selects action goals that ensure long-horizon feasibility. Our simulation studies first confirm that planning in A-Space achieves an 84.6% higher task success rate compared to baseline methods. Validation on real robotic systems demonstrates fluid mobile manipulation involving (i) seven types of rigid and articulated objects across 17 distinct contexts, and (ii) long-horizon tasks of up to 14 sequential steps. Our results highlight the significance of modeling scene kinematics into planning entities, rather than encoding task-specific constraints, offering a scalable and generalizable approach to complex robotic manipulation.},
  archive      = {J_TROB},
  author       = {Ziyuan Jiao and Yida Niu and Zeyu Zhang and Yangyang Wu and Yao Su and Yixin Zhu and Hangxin Liu and Song-Chun Zhu},
  doi          = {10.1109/TRO.2025.3605261},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Integration of robot and scene kinematics for sequential mobile manipulation planning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tip-growing robots: Design, theory, application. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3608701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Growing robots apically extend through material eversion or deposition at their tip. This endows them with unique capabilities such as follow the leader navigation, long-reach, inherent compliance, and large force delivery bandwidth. Tip-growing robots can therefore conform to sensitive, intricate, and difficult-to-access environments. This review paper categorizes, compares, and critically evaluates state-of-the-art growing robots with emphasis on their designs, fabrication processes, actuation and steering mechanisms, mechanics models, controllers, and applications. Finally, the paper discusses the main challenges that the research area still faces and proposes future directions.},
  archive      = {J_TROB},
  author       = {Shamsa Al Harthy and S.M.Hadi Sadati and CÃ©dric Girerd and Sukjun Kim and Alessio Mondini and Zicong Wu and Brandon Saldarriaga and Carlo A. Seneci and Barbara Mazzolai and Tania K. Morimoto and Christos Bergeles},
  doi          = {10.1109/TRO.2025.3608701},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Tip-growing robots: Design, theory, application},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curb-tracker: An integrated curb following system for autonomous vehicles. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3608695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curb following is a critical technology for autonomous road sweeping vehicles. However, existing solutions face two primary challenges: unreliable curb detection and inefficient motion generation. Unreliable curb detection stems from the wide variability in curb dimensions and types, as well as interference from roadside features such as vegetation and infrastructure. Inefficient motion generation occurs when existing methods prioritize tracking accuracy while neglecting task completion efficiency, leading to prolonged operation times. To address these challenges, we propose Curb-Tracker, an integrated curb-following system designed for autonomous vehicles operating in diverse road environments. Firstly, we develop a robust and adaptive curb detection algorithm that leverages a 2.5D elevation map of the local environment and dynamically adjusts key parameters online to ensure reliable detection across varying scenarios. Secondly, to achieve accurate and efficient curb-aligned motion generation, we leverage Model Predictive Contouring Control (MPCC) as a tailored framework specifically designed for the curb-following task to generate an optimal control sequence for the vehicle to maintain a specified lateral offset from the curb while maximizing travel progress along it. The proposed system has been implemented on a Hunter 2.0, a front-wheel Ackerman-steering mobile robot, and has been validated through extensive experiments in both Gazebo simulation and real-world environments. Experimental results demonstrate the effectiveness, adaptability, and robustness of the proposed system across a wide range of road scenarios. Video of real-world experiments is available at https://youtu.be/H1xaV6QdJ10.},
  archive      = {J_TROB},
  author       = {Jiahao Liang and Yuanzhe Wang and Guohao Peng and Zhenyu Wu and Danwei Wang},
  doi          = {10.1109/TRO.2025.3608695},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Curb-tracker: An integrated curb following system for autonomous vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tactile robotics: An outlook. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3608686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotics research has long sought to give robots the ability to perceive the physical world through touch in an analogous manner to many biological systems. Developing such tactile capabilities is important for numerous emerging applications that require robots to co-exist and interact closely with humans. Consequently, there has been growing interest in tactile sensing, leading to the development of various technologies, including piezoresistive and piezoelectric sensors, capacitive sensors, magnetic sensors, and optical tactile sensors. These diverse approaches utilise different transduction methods and materials to equip robots with distributed sensing capabilities, enabling more effective physical interactions. These advances have been supported in recent years by simulation tools that generate largescale tactile datasets to support sensor designs and algorithms to interpret and improve the utility of tactile data. The integration of tactile sensing with other modalities, such as vision, as well as with action strategies for active tactile perception highlights the growing scope of this field. To further the transformative progress in tactile robotics, a holistic approach is essential. In this outlook article, we examine several challenges associated with the current state of the art in tactile robotics and explore potential solutions to inspire innovations across multiple domains, including manufacturing, healthcare, recycling and agriculture.},
  archive      = {J_TROB},
  author       = {Shan Luo and Nathan F. Lepora and Wenzhen Yuan and Kaspar Althoefer and Gordon Cheng and Ravinder Dahiya},
  doi          = {10.1109/TRO.2025.3608686},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Tactile robotics: An outlook},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-dynamic crowd vetting: Collaborative detection of malicious robots in dynamic communication networks. <em>TROB</em>, 1-17. (<a href='https://doi.org/10.1109/TRO.2025.3608702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the problem where robots traverse through an environment modeled by a graph of discrete sites, and an unknown subset of the multi-robot team is malicious. Previous works require that each robot gathers information about the trustworthiness of all other robots, called trust observations, which can be time consuming in large networks. This paper decreases the time required to estimate trustworthiness by building upon an algorithm that leverages the concept of âCrowd Vettingâ and the opinion of trusted neighbors. This allows each robot to estimate trust in dynamic scenarios, where the team size, robot neighborhoods, and robot legitimacy can change. In particular, we employ an assumption that there exists quasi-dynamic time periods, where if a robot's legitimacy remains fixed for a sufficient length of time, its trustworthiness can be characterized. In this setting, we develop a closed-form expression for the critical number of time-steps required for our algorithm to successfully identify the true legitimacy of each robot within a specified failure probability. We show that the number of time-steps required for robots to correctly estimate the trust of all other robots increases logarithmically with the number of robots when robots do not leverage neighboring opinions, called the Direct Protocol. Conversely, for most general graph topologies, the number of time-steps required remains constant as the number of robots increases when our proposed algorithm, called quasi-Dynamic Crowd Vetting (DCV), is used, for a fixed ratio of legitimate to malicious robots. Finally, our theoretical results are successfully validated through simulated persistent surveillance tasks where robots maintain a desired distribution of robots over sites in the environment.},
  archive      = {J_TROB},
  author       = {Matthew Cavorsi and Frederik Mallmann-Trenn and David SaldaÃ±a and Stephanie Gil},
  doi          = {10.1109/TRO.2025.3608702},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Quasi-dynamic crowd vetting: Collaborative detection of malicious robots in dynamic communication networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plan optimal collision-free trajectories with non-convex cost functions using graphs of convex sets. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed approach to motion planning in Graphs of Convex Sets (GCS) provides an efficient framework for computing shortest-distance collision-free paths using convex optimization. This new motion planner is notably more computationally efficient than popular sampling-based motion planners, but it does not support non-convex cost functions. This paper develops a novel motion planning algorithm, Graph of Convex Sets with General Costs (GCSGC), to solve this problem. A given non-convex cost function is accurately approximated by a multiple-layer ReLU neural network and the configuration space is decomposed into a set of linear-cost regions using the hidden layers of the neural network. These linear-cost regions are intersected with a set of collision-free regions, and the resulting collision-free linear-cost regions are intersected to form the vertices and edges of the motion planner's underlying graph structure. The edge costs have a closed-form solution within each collision-free linear-cost region, but it is non-convex, so the McCormick relaxation is applied to convexify the edge costs. Finally, a graph pre-processing technique is developed to compute a representative graph structure that acts as a heuristic for the edge costs of the underlying GCS and then simplify the underlying graph structure by removing cycles and high-cost paths, which can significantly improve the efficiency of the planner and quality of the produced trajectories. The proposed motion planner is first validated in a 2D configuration space with comparisons between different sized neural networks with and without pre-processing, comparisons between optimal trajectories from GCSGC with shortest-distance trajectories, and comparisons between GCSGC and GCS-SLP. The GCSGC planner is further validated in a complex 7D configuration space by comparing to state-of-the-art multi-query (PRM*, GCS-SLP) and single-query (TrajOpt, BIT*, AIT*, RRT*) planners. The results show that the proposed motion planner is very competitive in terms of computational efficiency, trajectory cost, and memory footprint. Two physical experiments further validate the effectiveness of the proposed motion planner in real-world motion planning applications.},
  archive      = {J_TROB},
  author       = {Charles L. Clark and Biyun Xie},
  doi          = {10.1109/TRO.2025.3610175},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Plan optimal collision-free trajectories with non-convex cost functions using graphs of convex sets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unilateral active knee exoskeleton to assist individuals with hemiparesis â A pilot study. <em>TROB</em>, 1-14. (<a href='https://doi.org/10.1109/TRO.2025.3610187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most individuals who experience a stroke exhibit several sensorimotor impairments that limit their independence in everyday activities. Hemiparetic gait is frequently characterized by reduced knee flexion in swing due to knee stiffness or muscle weakness and knee hyperextension or knee buckling in the stance phase. Recently, unilateral-powered orthoses have been designed to overcome the limitations of the passive knee-ankle-foot orthoses. This study presents a unilateral Active Knee Orthosis Exoskeleton, AKO-$\beta$, endowed with a series elastic actuator and designed to assist the knee in flexion and extension movements. The paper describes the system mechatronic design and its characterization on the bench, the control system, and pilot experiments with three post-stroke participants. The device has a weight of 1.78 kg on the user's leg, with a lateral encumbrance of 76 mm. The pilot experiments aimed to verify the effects of the exoskeleton assistance in hemiparetic gait patterns. When walking with the device, participants on average increased the knee flexion on the paretic side by 18.70 deg (+44.9%) during swing and decreased knee hyperextension in stance by 4.50 deg, compared to walking without it. Overall, when walking with the exoskeleton, subjects showed improved Gait Variable Score of the paretic knee profile by 37.5% compared to walking without it. The temporal and spatial gait symmetry indexes did not show clear changes, although an improvement in symmetry was observed in two of the three participants. These preliminary results suggest the potential benefits of the unilateral Active Knee Orthosis exoskeleton to enhance and restore mobility in individuals with hemiparetic gait.},
  archive      = {J_TROB},
  author       = {Andrea Pergolini and Clara Beatriz Sanz-MorÃ¨re and Chiara Livolsi and Matteo Fantozzi and Filippo Dell'Agnello and Tommaso Ciapetti and Alessandro Maselli and Andrea Baldoni and Emilio Trigili and Simona Crea and Nicola Vitiello},
  doi          = {10.1109/TRO.2025.3610187},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Robot.},
  title        = {A unilateral active knee exoskeleton to assist individuals with hemiparesis â A pilot study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anytime probabilistically constrained provably convergent online belief space planning. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking into account future risk is essential for an autonomously operating robot to find online not only the best but also a safe action to execute. In this paper, we build upon the recently introduced formulation of probabilistic belief-dependent constraints. In our methodology safety can be materialized with any general belief-dependent operator we call payoff. We present an anytime approach employing the Monte Carlo Tree Search (MCTS) method in continuous domains in terms of states, actions and observations and general-belief dependent reward and payoff operators. Unlike previous approaches, our method ensures safety anytime with respect to the currently expanded search tree without relying on the convergence of the search. We prove convergence in probability with an exponential rate of a version of our algorithms and study proposed techniques via extensive simulations. Even with a tiny number of tree queries, the best action found by our approach is much safer than the baseline. Moreover, our approach constantly yields better than the baseline action in terms of objective function. This is because we revise the values and statistics maintained in the search tree and remove from them the contribution of the pruned actions. We rigorously show that our cleaning routine is necessary. Without it, at the limit of convergence of MCTS, an infinite amount of sampled dangerous actions can be detrimental to the objective function.},
  archive      = {J_TROB},
  author       = {Andrey Zhitnikov and Vadim Indelman},
  doi          = {10.1109/TRO.2025.3610176},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Anytime probabilistically constrained provably convergent online belief space planning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-efficiency vector field by time-optimal spatial iterative learning. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel model-free spatial iterative learning (IL) framework to enhance the efficiency of vector field (VF) navigation for mobile robots. By integrating the idea of iterative learning control (ILC) with VF, this framework utilizes historical data to enhance navigation efficiency significantly, reducing traversal time and expanding the applicability of IL to rapid navigation. Importantly, it has low time complexity with $O(n)$ per iteration, where $n$ denotes the waypoints number, preventing the significant computational overhead caused by the increasing waypoints in existing methods, which often exceeds $O(n^{2})$, making it well-suited for real-time planning. Moreover, the approach is inherently model-free, leaning on historical data, thus enabling agile navigation with limited reliance on intricate model details. The paper presents a comprehensive theoretical analysis of the stability, time optimality, time complexity, parameter insensitivity, robustness, and usage. Extensive simulations and experiments highlight its efficiency, promising a transformative impact on mobile robot navigation through the proposed IL.},
  archive      = {J_TROB},
  author       = {Shuli Lv and Yan Gao and Quan Quan},
  doi          = {10.1109/TRO.2025.3610174},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {High-efficiency vector field by time-optimal spatial iterative learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive body awareness in soft robots: A bayesian variational autoencoder fusing multimodal sensory data. <em>TROB</em>, 1-16. (<a href='https://doi.org/10.1109/TRO.2025.3610170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the causal flow by fusing multimodal perception is fundamental for constructing the bodily awareness of soft robots. However, forming such a predictive model while fusing the multimodal sensory data of soft robots remains challenging and less explored. In this study, we leverage the free energy principle within a Bayesian probabilistic deep learning framework to merge visual, pressure, and flex sensing signals. Our proposed multimodal association mechanism enhances the fusion process, establishing a robust computational methodology. We train the model using a newly collected dataset that captures the grasping dynamics of a soft gripper equipped with multimodal perception capabilities. By incorporating the current state and image differences, the forward model can predict the soft gripper's physical interaction and movement in the image flow, which amounts to imagining future motion events. Moreover, we showcase effective predictions across modalities as well as for grasping outcomes. Notably, our enhanced variational autoencoder approach can pave the way for unprecedented possibilities of bodily awareness in soft robotics.},
  archive      = {J_TROB},
  author       = {Shuyu Wang and Dongling Liu and Changzeng Fu and Xiaoming Yuan and Peng Shan and Victor C.M. Leung},
  doi          = {10.1109/TRO.2025.3610170},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Predictive body awareness in soft robots: A bayesian variational autoencoder fusing multimodal sensory data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards accurate, efficient and robust RGB-D simultaneous localization and mapping in challenging environments. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Simultaneous Localization and Mapping (SLAM) is crucial to many applications such as self-driving vehicles and robot tasks. However, it is still challenging for existing visual SLAM approaches to achieve good performance in low-texture or illumination-changing scenes. In recent years, some researchers have turned to edge-based SLAM approaches to deal with the challenging scenes, which are more robust than feature-based and direct SLAM methods. Nevertheless, existing edge-based methods are computationally expensive and inferior than other visual SLAM systems in terms of accuracy. In this study, we propose EdgeSLAM, a novel RGB-D edge-based SLAM approach to deal with challenging scenarios that is efficient, accurate, and robust. EdgeSLAM is built on two innovative modules: efficient edge selection and adaptive robust motion estimation. The edge selection module can efficiently select a small set of edge pixels, which significantly improves the computational efficiency without sacrificing the accuracy. The motion estimation module improves the system's accuracy and robustness by adaptively handling outliers in motion estimation. Extensive experiments were conducted on TUM RGBD, ICL-NUIM and ETH3D datasets, and experimental results show that EdgeSLAM significantly outperforms five state-of-the-art (SOTA) methods in terms of efficiency, accuracy, and robustness, which achieves 29.17% accuracy improvements with a high processing speed of up to 120 FPS and a high positioning success rate of 97.06%.},
  archive      = {J_TROB},
  author       = {Hui Zhao and Fuqiang Gu and Jianga Shang and Xianlei Long and Jiarui Dou and Chao Chen and Huayan Pu and Jun Luo},
  doi          = {10.1109/TRO.2025.3610173},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Towards accurate, efficient and robust RGB-D simultaneous localization and mapping in challenging environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-varying foot placement control for humanoid walking on swaying rigid surface. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3612326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locomotion on dynamic rigid surface (i.e., rigid surface accelerating in an inertial frame) presents complex challenges for controller design, which are essential to address for deploying humanoid robots in dynamic real-world environments such as moving trains, ships, and airplanes. This paper introduces a real-time, provably stabilizing control approach for humanoid walking on periodically swaying rigid surface. The first key contribution is an analytical extension of the classical angular momentum-based linear inverted pendulum model from static to swaying grounds whose motion period may be different than the robot's gait period. This extension results in a time-varying, nonhomogeneous robot model, which is fundamentally different from the existing pendulum models. We synthesize a discrete footstep control law for the model and derive a new set of sufficient stability conditions that verify the controller's stabilizing effect. Finally, experiments conducted on a Digit humanoid robot, both in simulations and on hardware, demonstrate the framework's effectiveness in addressing bipedal locomotion on swaying ground, even under uncertain surface motions and unknown external pushes.},
  archive      = {J_TROB},
  author       = {Yuan Gao and Victor Paredes and Yukai Gong and Zijian He and Ayonga Hereid and Yan Gu},
  doi          = {10.1109/TRO.2025.3612326},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Time-varying foot placement control for humanoid walking on swaying rigid surface},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic-rule-compliant trajectory repair via satisfiability modulo theories and reachability analysis. <em>TROB</em>, 1-18. (<a href='https://doi.org/10.1109/TRO.2025.3613550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complying with traffic rules is challenging for automated vehicles, as numerous rules need to be considered simultaneously. If a planned trajectory violates traffic rules, it is common to replan a new trajectory from scratch. We instead propose a trajectory repair technique to save computation time. By coupling satisfiability modulo theories with set-based reachability analysis, we determine if and in what manner the initial trajectory can be repaired. Experiments in high-fidelity simulators and in the real world demonstrate the benefits of our proposed approach in various scenarios. Even in complex environments with intricate rules, we efficiently and reliably repair rule-violating trajectories, enabling automated vehicles to swiftly resume legally safe operation in real time.},
  archive      = {J_TROB},
  author       = {Yuanfei Lin and Zekun Xing and Xuyuan Han and Matthias Althoff},
  doi          = {10.1109/TRO.2025.3613550},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Traffic-rule-compliant trajectory repair via satisfiability modulo theories and reachability analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and transparency in mixed reality bilateral human teleoperation. <em>TROB</em>, 1-16. (<a href='https://doi.org/10.1109/TRO.2025.3613464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work introduced the concept of human teleoperation (HT), where the remote robot typically considered in conventional bilateral teleoperation is replaced by a novice person wearing a mixed reality head-mounted display and tracking the motion of a virtual tool controlled by an expert. HT has advantages in cost, complexity, and patient acceptance for telemedicine in low-resource communities or remote locations. However, the stability, transparency, and performance of bilateral HT are unexplored. In this paper, we therefore develop a mathematical model of the HT system using test data. We then analyze various control architectures with this model and implement them with the HT system, testing volunteer operators and a virtual fixture-based simulated patient to find the achievable performance, investigate stability, and determine the most promising teleoperation scheme in the presence of time delays. We show that instability in HT, while not destructive or dangerous, makes the system impossible to use. However, stable and transparent teleoperation are possible with small time delays ($&lt; 200$ ms) through 3-channel teleoperation, or with large time delays through model-mediated teleoperation with local pose and force feedback for the novice.},
  archive      = {J_TROB},
  author       = {David Black and Septimiu Salcudean},
  doi          = {10.1109/TRO.2025.3613464},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Stability and transparency in mixed reality bilateral human teleoperation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grasp like humans: Learning generalizable multi-fingered grasping from human proprioceptive sensorimotor integration. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile and kinesthetic perceptions are crucial for human dexterous manipulation, enabling reliable grasping of objects via proprioceptive sensorimotor integration. For robotic hands, even though acquiring such tactile and kinesthetic feedback is feasible, establishing a direct mapping from this sensory feedback to motor actions remains challenging. In this paper, we propose a novel glove-mediated tactile-kinematic perception-prediction framework for grasp skill transfer from human intuitive and natural operation to robotic execution based on imitation learning, and its effectiveness is validated through generalized grasping tasks, including those involving deformable objects. Firstly, we integrate a data glove to capture tactile and kinesthetic data at the joint level. The glove is adaptable for both human and robotic hands, allowing data collection from natural human hand demonstrations across different scenarios. It ensures consistency in the raw data format, enabling evaluation of grasping for both human and robotic hands. Secondly, we establish a unified representation of multi-modal inputs based on graph structures with polar coordinates. We explicitly integrate the morphological differences into the designed representation, enhancing the compatibility across different demonstrators and robotic hands. Furthermore, we introduce the Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage multidimensional subgraph convolutions and attention-based LSTM layers to extract spatio-temporal features from graph inputs to predict node-based states for each hand joint. These predictions are then mapped to final commands through a force-position hybrid mapping. Comparative experiments and ablation studies demonstrate that our approach surpasses other methods in grasp success rate, finger coordination, contact force management, and both grasp and computational efficiency, achieving results most akin to human grasping. The robustness of our approach is also validated through multiple randomized experimental setups, and its generalization capability is tested across diverse objects and robotic hands. The dataset and additional supporting videos are accessible via https://grasplikehuman.github.io/.},
  archive      = {J_TROB},
  author       = {Ce Guo and Xieyuanli Chen and Zhiwen Zeng and Zirui Guo and Yihong Li and Haoran Xiao and Dewen Hu and Huimin Lu},
  doi          = {10.1109/TRO.2025.3613541},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Grasp like humans: Learning generalizable multi-fingered grasping from human proprioceptive sensorimotor integration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoVINS: Geographic-visual-inertial navigation system for large-scale drift-free aerial state estimation. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces GeoVINS, a vision-based navigation framework designed for large-scale global state estimation. By utilizing geographic information from satellite orthoimagery, GeoVINS tackles scale ambiguity and accumulative drift problems inherent in standard visual-inertial simultaneous localization and mapping (VI-SLAM) systems, and provides accurate, robust, and real-time global localization. In particular, to address the challenge of memory explosion issue for large-scale localization, we propose a novel aerial âclassify-then-retrieveâ aerial VPR approach, where geographic locations can be efficiently identified and memory usage can be reduced by 3 to 4 orders of magnitude compared to classical retrieval-based approaches. In addition, a hierarchical geographic data association scheme, enhanced by state-of-the-art deep learning-based feature matching, guarantees high efficiency and robustness against variations in appearance and viewpoint. Relying on the obtained three-dimensional (3D) geographic information, GeoVINS achieves efficient global state initialization and precise motion tracking. To address GPU limitations in embedded devices, a collaborative CPU-GPU utilization approach is proposed, seamlessly integrating asynchronous global information to eliminate accumulative errors. Relying solely on satellite imagery and without requiring any other prior information, GeoVINS achieves rapid place recognition in unseen environments of city-scale (e.g., 2500${\mathrm{km}}^{2}$) on an embedded computing device with an inference time of 43 ms, and performs state estimation at a frequency of 25 Hz. The system enables autonomous UAV navigation as an alternative to GNSS.},
  archive      = {J_TROB},
  author       = {Chunyu Li and Mengfan He and Chao Chen and Jiacheng Liu and Xu Lyu and Guoquan Huang and Ziyang Meng},
  doi          = {10.1109/TRO.2025.3613467},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {GeoVINS: Geographic-visual-inertial navigation system for large-scale drift-free aerial state estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VINGS-mono: Visual-inertial gaussian splatting monocular SLAM in large scenes. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer, Score Manager, and Pose Refinement, which collectively improve mapping speed and localization accuracy. This enables the SLAM system to handle large-scale urban environments with up to 50 million Gaussian ellipsoids. To ensure global consistency in large-scale scenes, we design a Loop Closure module, which innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian Splatting for loop closure detection and correction of the Gaussian map. Additionally, we propose a Dynamic Eraser to address the inevitable presence of dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor and outdoor environments demonstrate that our approach achieves localization performance on par with Visual-Inertial Odometry while surpassing recent GS/NeRF SLAM methods. It also significantly outperforms all existing methods in terms of mapping and rendering quality. Furthermore, we developed a mobile app and verified that our framework can generate high-quality Gaussian maps in real time using only a smartphone camera and a low-frequency IMU sensor. To the best of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method capable of operating in outdoor environments and supporting kilometer-scale large scenes.},
  archive      = {J_TROB},
  author       = {Ke Wu and Zicheng Zhang and Muer Tie and Ziqing Ai and Zhongxue Gan and Wenchao Ding},
  doi          = {10.1109/TRO.2025.3613536},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {VINGS-mono: Visual-inertial gaussian splatting monocular SLAM in large scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object-centric kinodynamic planning for nonprehensile robot rearrangement manipulation. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonprehensile actions such as pushing are crucial for addressing multi-object rearrangement problems. Many traditional methods generate robot-centric actions, which differ from intuitive human strategies and are typically inefficient. To this end, we adopt an object-centric planning paradigm and propose a unified framework for addressing a range of large-scale, physics-intensive nonprehensile rearrangement problems challenged by modeling inaccuracies and real-world uncertainties. By assuming each object can actively move without being driven by robot interactions, our planner first computes desired object motions, which are then realized through robot actions generated online via a closed-loop pushing strategy. Through extensive experiments and in comparison with state-of-the-art baselines in both simulation and on a physical robot, we show that our object-centric planning framework can generate more intuitive and task-effective robot actions with significantly improved efficiency. In addition, we propose a benchmarking protocol to standardize and facilitate future research in nonprehensile rearrangement.},
  archive      = {J_TROB},
  author       = {Kejia Ren and Gaotian Wang and Andrew S. Morgan and Lydia E. Kavraki and Kaiyu Hang},
  doi          = {10.1109/TRO.2025.3613532},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Object-centric kinodynamic planning for nonprehensile robot rearrangement manipulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online multi-robot coordination and cooperation with task precedence relationships. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new formulation for the multi-robot task allocation problem that incorporates (a)Â complex precedence relationships between tasks, (b)Â efficient intra-task coordination, and (c)Â cooperation through the formation of robot coalitions. A task graph specifies the tasks and their relationships, and a set of reward functions models the effects of coalition size and preceding task performance. Maximizing task rewards is NP-hard; hence, we propose network flow-based algorithms to approximate solutions efficiently. A novel online algorithm performs iterative re-allocation, providing robustness to task failures and model inaccuracies to achieve higher performance than offline approaches. We comprehensively evaluate the algorithms in a testbed with random missions and reward functions and compare them to a mixed- integer solver and a greedy heuristic. Additionally, we validate the overall approach in an advanced simulator, modeling reward functions based on realistic physical phenomena and executing the tasks with realistic robot dynamics. Results establish efficacy in modeling complex missions and efficiency in generating high-fidelity task plans while leveraging task relationships.},
  archive      = {J_TROB},
  author       = {Walker Gosrich and Saurav Agarwal and Kashish Garg and Siddharth Mayya and Matthew Malencia and Mark Yim and Vijay Kumar},
  doi          = {10.1109/TRO.2025.3613558},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Online multi-robot coordination and cooperation with task precedence relationships},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrivacyâPreserving robotic perception for object detection in curious cloud robotics. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3613551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud robotics allows lowâpower robots to perform computationally intensive inference tasks by offloading them to the cloud, raising privacy concerns when transmitting sensitive images. Although endâtoâend encryption secures data in transit, it does not prevent misuse by inquisitive thirdâparty services since data must be decrypted for processing. This paper tackles these privacy issues in cloudâbased object detection tasks for service robots. We propose a coâtrained encoderâdecoder architecture that retains only taskâspecific features while obfuscating sensitive information, utilizing a novel weak loss mechanism with proposal selection for privacy preservation. A theoretical analysis of the problem is provided, along with an evaluation of the tradeâoff between detection accuracy and privacy preservation through extensive experiments on public datasets and a real robot.},
  archive      = {J_TROB},
  author       = {Michele Antonazzi and Matteo Alberti and Alex Bassot and Matteo Luperto and Nicola Basilico},
  doi          = {10.1109/TRO.2025.3613551},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {PrivacyâPreserving robotic perception for object detection in curious cloud robotics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUTO-IceNav: A local navigation strategy for autonomous surface ships in broken ice fields. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ice conditions often require ships to reduce speed and deviate from their main course to avoid damage to the ship. In addition, broken ice fields are becoming the dominant ice conditions encountered in the Arctic, where the effects of collisions with ice are highly dependent on where contact occurs and on the particular features of the ice floes. In this paper, we present AUTO-IceNav, a framework for the autonomous navigation of ships operating in ice floe fields. Trajectories are computed in a receding-horizon manner, where we frequently replan given updated ice field data. During a planning step, we assume a nominal speed that is safe with respect to the current ice conditions, and compute a reference path. We formulate a novel cost function that minimizes the kinetic energy loss of the ship from ship-ice collisions and incorporate this cost as part of our lattice-based path planner. The solution computed by the lattice planning stage is then used as an initial guess in our proposed optimization-based improvement step, producing a locally optimal path. Extensive experiments were conducted both in simulation and in a physical testbed to validate our approach.},
  archive      = {J_TROB},
  author       = {Rodrigue de Schaetzen and Alexander Botros and Ninghan Zhong and Kevin Murrant and Robert Gash and Stephen L. Smith},
  doi          = {10.1109/TRO.2025.3613472},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {AUTO-IceNav: A local navigation strategy for autonomous surface ships in broken ice fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient decentralised parallel task allocation for multiple robots. <em>TROB</em>, 1-18. (<a href='https://doi.org/10.1109/TRO.2025.3613566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with large-scale decentralised task allocation problems for multiple heterogeneous robots. One of the grand challenges with decentralised task allocation problems is the NP-hardness for computation and communication. This paper proposes a decentralised Decreasing Threshold Task Allocation (DTTA) algorithm that enables parallel allocation by leveraging a decreasing threshold to handle the NP-hardness. DTTA can release both computation and communication burdens for multiple robots in a decentralised network. Additionally, DTTA provides a theoretical guarantee of the quality of the solution for maximising submodular utility functions. Theoretical analysis indicates that DTTA can provide an optimality guarantee of $(1-\epsilon)/2$ with computation complexity of $O(\min (r^{2}, \frac{r}{\epsilon }\ln \frac{r}{\epsilon }))$ for each robot, where $\epsilon$ is the parameter controlling the decreasing speed of the threshold, $r$ is the number of tasks. To examine the performance of the proposed algorithm, we conduct numerical simulations based on a multi-target surveillance scenario. Simulation results demonstrate that DTTA delivers comparable solution quality significantly faster than state-of-the-art task allocation algorithms. Its advantages are particularly pronounced in large-scale missions with thousands of tasks and robots.},
  archive      = {J_TROB},
  author       = {Teng Li and Hyo-Sang Shin and Antonios Tsourdos},
  doi          = {10.1109/TRO.2025.3613566},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Efficient decentralised parallel task allocation for multiple robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based motion planning leveraging multivariate deep evidential regression. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based motion planning methods have shown significant promise in enhancing the efficiency of traditional algorithms. However, they often face performance degradation in novel environments with drastic scene changes due to the limited generalization ability of deep neural networks (DNNs). This paper introduces a confidence-driven motion planning network (CDMPNet), comprising a feature extraction autoencoder and a confidence-driven sampling network (CDSNet). The autoencoder compresses point clouds into latent vectors. The CDSNet is a closed-form continuous-time neural network, which predicts hyperparameters of an evidential distribution over the subsequent state's mean and covariance for robot configuration sampling. We also present a CDMPNet-based neural planner and a CDMPNet-guided RRTConnect algorithm. Simulations and ablation studies are conducted on 2-D, 3-D, and 7-D planning tasks to validate the generalization ability of our method. Furthermore, we transfer the approach to a 7-DOF Sawyer robotic arm to demonstrate the potential for real-world deployment.},
  archive      = {J_TROB},
  author       = {Rixin Wang and Shuopeng Wang and Jintao Ye and Ying Zhang and Lina Hao},
  doi          = {10.1109/TRO.2025.3613568},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Learning-based motion planning leveraging multivariate deep evidential regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and scalable multi-robot localization using stereo UWB arrays. <em>TROB</em>, 1-18. (<a href='https://doi.org/10.1109/TRO.2025.3587854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In environments where robots operate with limited global navigation satellite system accessibility, ultra-wideband (UWB) localization technology is a popular auxiliary solution to assist visual.inertial odometry systems. However, current UWB approaches lack 3D pairwise localization capability and suffer from rapidly declining localization update rates as the network scales, limiting their effectiveness for swarm robotic applications. This paper presents a novel UWB sensor that enables 3D pairwise localization and a localization scheme that can deliver robust, scalable, and accurate position awareness for multi-robot systems. Our approach begins with calibrating intrinsic UWB errors from hardware deviations and propagation effects, yielding high-accuracy distance and direction measurements. Using these measurements, we perform distributed relative localization through inter- and intra-node cooperation by integrating UWB and inertial measurement unit data. To enable swarm-scale operation, our platform implements the signal-multiplexing network ranging (SM-NR) protocol to maximize update rates and network capacity. Experimental results show that our approach achieves centimeter-level localization accuracy at high update rates (100 Hz for UWB only), validating its robustness, scalability, and accuracy for robotic applications.},
  archive      = {J_TROB},
  author       = {Hanying Zhao and Lingwei Xu and Yi Li and Feiyang Wen and Haoran Gao and Changwu Liu and Jincheng Yu and Yu Wang and Yuan Shen},
  doi          = {10.1109/TRO.2025.3587854},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Robust and scalable multi-robot localization using stereo UWB arrays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D-printable crease-free origami vacuum bending actuators for soft robots. <em>TROB</em>, 1-15. (<a href='https://doi.org/10.1109/TRO.2025.3588726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vacuum-based bending actuation offers benefits such as safety and compactness in soft robotics, it is often overlooked due to its limited actuation pressure, which restricts both bending angle and force output. This study presents a crease-free, origami-inspired vacuum bending actuator that advances both state-of-the-art vacuum bending actuators and traditional origami deformation principles by introducing orderly self-folding through optimized stiffness distribution. Achieved through finite element method (FEM), this design provides several advantages: (i) Self-folding allows for high bending angles (up to 138$^{\circ }$) in a compact form. (ii) The crease-free design facilitates 3D printing from a single soft material using a consumer-level fused filament fabrication (FFF) printer, specifically thermoplastic polyurethane (TPU) with a Shore hardness of 60A, potentially higher flexibility and durability. (iii) The compact configuration enables modular design, supporting reconfiguration as demonstrated in adaptable locomotion soft robots. (iv) The large bending angles allow the actuator to wrap around objects, offering extensive contact compared to other designs. This capability, combined with its vacuum-driven mechanism, enables synergy with self-closing suction cups in an octopus-like vacuum gripper, providing large versatility and grasping force for handling a wide range of objects, from small, irregular shapes to larger, flat items.},
  archive      = {J_TROB},
  author       = {Zhanwei Wang and Huaijin Chen and Syeda Shadab Zehra Zaidi and Ellen Roels and Hendrik Cools and Bram Vanderborght and Seppe Terryn},
  doi          = {10.1109/TRO.2025.3588726},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Robot.},
  title        = {3D-printable crease-free origami vacuum bending actuators for soft robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-level similarity approach for single-view object grasping: Matching, planning, and fine-tuning. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3588720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grasping unknown objects from a single view has remained a challenging topic in robotics due to the uncertainty of partial observation. Recent advances in large-scale models have led to benchmark solutions such as GraspNet-1Billion. However, such learning-based approaches still face a critical limitation in performance robustness for their sensitivity to sensing noise and environmental changes. To address this bottleneck in achieving highly generalized grasping, we abandon the traditional learning framework and introduce a new perspective: similarity matching, where similar known objects are utilized to guide the grasping of unknown target objects. We newly propose a method that robustly achieves unknown-object grasping from a single viewpoint through three key steps: 1) Leverage the visual features of the observed object to perform similarity matching with an existing database containing various object models, identifying potential candidates with high similarity; 2) Use the candidate models with pre-existing grasping knowledge to plan imitative grasps for the unknown target object; 3) Optimize the grasp quality through a local fine-tuning process. To address the uncertainty caused by partial and noisy observation, we propose a multi-level similarity matching framework that integrates semantic, geometric, and dimensional features for comprehensive evaluation. Especially, we introduce a novel point cloud geometric descriptor, the C-FPFH descriptor, which facilitates accurate similarity assessment between partial point clouds of observed objects and complete point clouds of database models. In addition, we incorporate the use of large language models, introduce the semi-oriented bounding box, and develop a novel point cloud registration approach based on plane detection to enhance matching accuracy under single-view conditions. Real-world experiments demonstrate that our proposed method significantly outperforms existing benchmarks in grasping a wide variety of unknown objects in both isolated and cluttered scenarios, showcasing exceptional robustness across varying object types and operating environments.},
  archive      = {J_TROB},
  author       = {Hao Chen and Takuya Kiyokawa and Zhengtao Hu and Weiwei Wan and Kensuke Harada},
  doi          = {10.1109/TRO.2025.3588720},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {A multi-level similarity approach for single-view object grasping: Matching, planning, and fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A propagation perspective on recursive forward dynamics for systems with kinematic loops. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3593081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the concept of constraint embedding as a means for dealing with kinematic loop constraints during dynamics computations for rigid-body systems. Specifically, we consider the local loop constraints emerging from common actuation sub-mechanisms in modern robotics systems (e.g., geared motors, differential drives, and four-bar mechanisms). As a complementary perspective to prior work on constraint embedding, we present an analysis that generalizes the traditional concepts of joint models and motion/force subspaces between individual rigid bodies to generalized joint models and motion/force subspaces between groups of rigid bodies subject to loop constraints. We then use these generalized concepts to derive the constraint-embedded recursive forward dynamics algorithm using multi-handle articulated bodies. We demonstrate the broad applicability of the generalized joint concepts by showing how they also lead to the constraint-embedding-based recursive algorithm for inverse dynamics. Lastly, we benchmark our open-source implementation in C++ for the forward dynamics algorithm against state-of-the-art, sparsity-exploiting algorithms. Our alternative derivation is intended to make the constraint embedding methodology more accessible to the broader robotics community, while the benchmarking study clarifies the relative strengths and limitations of constraint embedding versus sparsity-exploiting methods. Indeed, our benchmarking validates that constraint embedding outperforms the non-recursive alternative in cases involving local kinematic loops.},
  archive      = {J_TROB},
  author       = {Matthew Chignoli and Nicholas Adrian and Sangbae Kim and Patrick M. Wensing},
  doi          = {10.1109/TRO.2025.3593081},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {A propagation perspective on recursive forward dynamics for systems with kinematic loops},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProxQP: An efficient and versatile quadratic programming solver for real-time robotics applications and beyond. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3577107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex Quadratic programming (QP) has become a core component in the modern engineering toolkit, particularly in robotics, where QP problems are legions, ranging from real-time whole-body controllers to planning and estimation algorithms. Many of those QPs need to be solved at high frequency. Meeting timing requirements requires taking advantage of as many structural properties as possible for the problem at hand. For instance, it is generally crucial to resort to warm-starting to exploit the resemblance of consecutive control iterations. While a large range of off-the-shelf QP solvers is available, only a few are suited to exploit problem structure and warm-starting capacities adequately. In this work, we propose the ProxQP algorithm, a new and efficient QP solver that exploits QP structures by leveraging primal-dual augmented Lagrangian techniques. For convex QPs, ProxQP features a global convergence guarantee to the closest feasible QP, an essential property for safe closed-loop control. We illustrate its practical performance on various standard robotic and control experiments, including a real-world closed-loop model predictive control application. While originally tailored for robotics applications, we show that ProxQP also performs at the level of state of the art on generic QP problems, making ProxQP suitable for use as an off-the-shelf solver for regular applications beyond robotics.},
  archive      = {J_TROB},
  author       = {Antoine Bambade and Fabian Schramm and Sarah El-Kazdadi and StÃ©phane Caron and Adrien Taylor and Justin Carpentier},
  doi          = {10.1109/TRO.2025.3577107},
  journal      = {IEEE Transactions on Robotics},
  month        = {6},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {ProxQP: An efficient and versatile quadratic programming solver for real-time robotics applications and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRANE: A redundant, multi-degree-of-freedom computed tomography robot for heightened needle dexterity within a medical imaging bore. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2024.3364986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed Tomography (CT) image guidance enables accurate and safe minimally invasive treatment of diseases, including cancer and chronic pain, with needle-like tools via a percutaneous approach. The physician incrementally inserts and adjusts the needle with intermediate images due to the accuracy limitation of free-hand adjustment and patient physiological motion. Scanning frequency is limited to minimize ionizing radiation exposure for the patient and physician. Robots can provide high positional accuracy and compensate for physiological motion with fewer scans. To accomplish this, the robots must operate within the confined imaging bore while retaining sufficient dexterity to insert and manipulate the needle. This paper presents CRANE: CT Robotic Arm and Needle Emplacer, a CT-compatible robot with a design focused on system dexterity that enables physicians to manipulate and insert needles within the scanner bore as naturally as they would be able to by hand. We define abstract and measurable clinically motivated metrics for in-bore dexterity applicable to general-purpose intra-bore image-guided needle placement robots, develop an automatic robot planning and control method for intra-bore needle manipulation and device setup, and demonstrate the redundant linkage design provides dexterity across various human morphology and meets the clinical requirements for target accuracy during an in-situ evaluation.},
  archive      = {J_TROB},
  author       = {Dimitrious Schreiber and Zhaowei Yu and Taylor Henderson and Derek Chen and Alexander Norbash and Michael C. Yip},
  doi          = {10.1109/TRO.2024.3364986},
  journal      = {IEEE Transactions on Robotics},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {CRANE: A redundant, multi-degree-of-freedom computed tomography robot for heightened needle dexterity within a medical imaging bore},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
