<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TROB</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="trob">TROB - 19</h2>
<ul>
<li><details>
<summary>
(2025). Integration of robot and scene kinematics for sequential mobile manipulation planning. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3605261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Sequential Mobile Manipulation Planning (SMMP) framework that can solve long-horizon multi-step mobile manipulation tasks with coordinated whole-body motion, even when interacting with articulated objects. By abstracting environmental structures as kinematic models and integrating them with the robot's kinematics, we construct an Augmented Configuration Apace (A-Space) that unifies the previously separate task constraints for navigation and manipulation, while accounting for the joint reachability of the robot base, arm, and manipulated objects. This integration facilitates efficient planning within a tri-level framework: a task planner generates symbolic action sequences to model the evolution of A-Space, an optimization-based motion planner computes continuous trajectories within A-Space to achieve desired configurations for both the robot and scene elements, and an intermediate plan refinement stage selects action goals that ensure long-horizon feasibility. Our simulation studies first confirm that planning in A-Space achieves an 84.6% higher task success rate compared to baseline methods. Validation on real robotic systems demonstrates fluid mobile manipulation involving (i) seven types of rigid and articulated objects across 17 distinct contexts, and (ii) long-horizon tasks of up to 14 sequential steps. Our results highlight the significance of modeling scene kinematics into planning entities, rather than encoding task-specific constraints, offering a scalable and generalizable approach to complex robotic manipulation.},
  archive      = {J_TROB},
  author       = {Ziyuan Jiao and Yida Niu and Zeyu Zhang and Yangyang Wu and Yao Su and Yixin Zhu and Hangxin Liu and Song-Chun Zhu},
  doi          = {10.1109/TRO.2025.3605261},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Integration of robot and scene kinematics for sequential mobile manipulation planning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anytime probabilistically constrained provably convergent online belief space planning. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking into account future risk is essential for an autonomously operating robot to find online not only the best but also a safe action to execute. In this paper, we build upon the recently introduced formulation of probabilistic belief-dependent constraints. In our methodology safety can be materialized with any general belief-dependent operator we call payoff. We present an anytime approach employing the Monte Carlo Tree Search (MCTS) method in continuous domains in terms of states, actions and observations and general-belief dependent reward and payoff operators. Unlike previous approaches, our method ensures safety anytime with respect to the currently expanded search tree without relying on the convergence of the search. We prove convergence in probability with an exponential rate of a version of our algorithms and study proposed techniques via extensive simulations. Even with a tiny number of tree queries, the best action found by our approach is much safer than the baseline. Moreover, our approach constantly yields better than the baseline action in terms of objective function. This is because we revise the values and statistics maintained in the search tree and remove from them the contribution of the pruned actions. We rigorously show that our cleaning routine is necessary. Without it, at the limit of convergence of MCTS, an infinite amount of sampled dangerous actions can be detrimental to the objective function.},
  archive      = {J_TROB},
  author       = {Andrey Zhitnikov and Vadim Indelman},
  doi          = {10.1109/TRO.2025.3610176},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Anytime probabilistically constrained provably convergent online belief space planning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards accurate, efficient and robust RGB-D simultaneous localization and mapping in challenging environments. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3610173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Simultaneous Localization and Mapping (SLAM) is crucial to many applications such as self-driving vehicles and robot tasks. However, it is still challenging for existing visual SLAM approaches to achieve good performance in low-texture or illumination-changing scenes. In recent years, some researchers have turned to edge-based SLAM approaches to deal with the challenging scenes, which are more robust than feature-based and direct SLAM methods. Nevertheless, existing edge-based methods are computationally expensive and inferior than other visual SLAM systems in terms of accuracy. In this study, we propose EdgeSLAM, a novel RGB-D edge-based SLAM approach to deal with challenging scenarios that is efficient, accurate, and robust. EdgeSLAM is built on two innovative modules: efficient edge selection and adaptive robust motion estimation. The edge selection module can efficiently select a small set of edge pixels, which significantly improves the computational efficiency without sacrificing the accuracy. The motion estimation module improves the system's accuracy and robustness by adaptively handling outliers in motion estimation. Extensive experiments were conducted on TUM RGBD, ICL-NUIM and ETH3D datasets, and experimental results show that EdgeSLAM significantly outperforms five state-of-the-art (SOTA) methods in terms of efficiency, accuracy, and robustness, which achieves 29.17% accuracy improvements with a high processing speed of up to 120 FPS and a high positioning success rate of 97.06%.},
  archive      = {J_TROB},
  author       = {Hui Zhao and Fuqiang Gu and Jianga Shang and Xianlei Long and Jiarui Dou and Chao Chen and Huayan Pu and Jun Luo},
  doi          = {10.1109/TRO.2025.3610173},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Towards accurate, efficient and robust RGB-D simultaneous localization and mapping in challenging environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-varying foot placement control for humanoid walking on swaying rigid surface. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3612326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locomotion on dynamic rigid surface (i.e., rigid surface accelerating in an inertial frame) presents complex challenges for controller design, which are essential to address for deploying humanoid robots in dynamic real-world environments such as moving trains, ships, and airplanes. This paper introduces a real-time, provably stabilizing control approach for humanoid walking on periodically swaying rigid surface. The first key contribution is an analytical extension of the classical angular momentum-based linear inverted pendulum model from static to swaying grounds whose motion period may be different than the robot's gait period. This extension results in a time-varying, nonhomogeneous robot model, which is fundamentally different from the existing pendulum models. We synthesize a discrete footstep control law for the model and derive a new set of sufficient stability conditions that verify the controller's stabilizing effect. Finally, experiments conducted on a Digit humanoid robot, both in simulations and on hardware, demonstrate the framework's effectiveness in addressing bipedal locomotion on swaying ground, even under uncertain surface motions and unknown external pushes.},
  archive      = {J_TROB},
  author       = {Yuan Gao and Victor Paredes and Yukai Gong and Zijian He and Ayonga Hereid and Yan Gu},
  doi          = {10.1109/TRO.2025.3612326},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Time-varying foot placement control for humanoid walking on swaying rigid surface},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic-rule-compliant trajectory repair via satisfiability modulo theories and reachability analysis. <em>TROB</em>, 1-18. (<a href='https://doi.org/10.1109/TRO.2025.3613550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complying with traffic rules is challenging for automated vehicles, as numerous rules need to be considered simultaneously. If a planned trajectory violates traffic rules, it is common to replan a new trajectory from scratch. We instead propose a trajectory repair technique to save computation time. By coupling satisfiability modulo theories with set-based reachability analysis, we determine if and in what manner the initial trajectory can be repaired. Experiments in high-fidelity simulators and in the real world demonstrate the benefits of our proposed approach in various scenarios. Even in complex environments with intricate rules, we efficiently and reliably repair rule-violating trajectories, enabling automated vehicles to swiftly resume legally safe operation in real time.},
  archive      = {J_TROB},
  author       = {Yuanfei Lin and Zekun Xing and Xuyuan Han and Matthias Althoff},
  doi          = {10.1109/TRO.2025.3613550},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Traffic-rule-compliant trajectory repair via satisfiability modulo theories and reachability analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and transparency in mixed reality bilateral human teleoperation. <em>TROB</em>, 1-16. (<a href='https://doi.org/10.1109/TRO.2025.3613464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work introduced the concept of human teleoperation (HT), where the remote robot typically considered in conventional bilateral teleoperation is replaced by a novice person wearing a mixed reality head-mounted display and tracking the motion of a virtual tool controlled by an expert. HT has advantages in cost, complexity, and patient acceptance for telemedicine in low-resource communities or remote locations. However, the stability, transparency, and performance of bilateral HT are unexplored. In this paper, we therefore develop a mathematical model of the HT system using test data. We then analyze various control architectures with this model and implement them with the HT system, testing volunteer operators and a virtual fixture-based simulated patient to find the achievable performance, investigate stability, and determine the most promising teleoperation scheme in the presence of time delays. We show that instability in HT, while not destructive or dangerous, makes the system impossible to use. However, stable and transparent teleoperation are possible with small time delays ($&lt; 200$ ms) through 3-channel teleoperation, or with large time delays through model-mediated teleoperation with local pose and force feedback for the novice.},
  archive      = {J_TROB},
  author       = {David Black and Septimiu Salcudean},
  doi          = {10.1109/TRO.2025.3613464},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Stability and transparency in mixed reality bilateral human teleoperation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grasp like humans: Learning generalizable multi-fingered grasping from human proprioceptive sensorimotor integration. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile and kinesthetic perceptions are crucial for human dexterous manipulation, enabling reliable grasping of objects via proprioceptive sensorimotor integration. For robotic hands, even though acquiring such tactile and kinesthetic feedback is feasible, establishing a direct mapping from this sensory feedback to motor actions remains challenging. In this paper, we propose a novel glove-mediated tactile-kinematic perception-prediction framework for grasp skill transfer from human intuitive and natural operation to robotic execution based on imitation learning, and its effectiveness is validated through generalized grasping tasks, including those involving deformable objects. Firstly, we integrate a data glove to capture tactile and kinesthetic data at the joint level. The glove is adaptable for both human and robotic hands, allowing data collection from natural human hand demonstrations across different scenarios. It ensures consistency in the raw data format, enabling evaluation of grasping for both human and robotic hands. Secondly, we establish a unified representation of multi-modal inputs based on graph structures with polar coordinates. We explicitly integrate the morphological differences into the designed representation, enhancing the compatibility across different demonstrators and robotic hands. Furthermore, we introduce the Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage multidimensional subgraph convolutions and attention-based LSTM layers to extract spatio-temporal features from graph inputs to predict node-based states for each hand joint. These predictions are then mapped to final commands through a force-position hybrid mapping. Comparative experiments and ablation studies demonstrate that our approach surpasses other methods in grasp success rate, finger coordination, contact force management, and both grasp and computational efficiency, achieving results most akin to human grasping. The robustness of our approach is also validated through multiple randomized experimental setups, and its generalization capability is tested across diverse objects and robotic hands. The dataset and additional supporting videos are accessible via https://grasplikehuman.github.io/.},
  archive      = {J_TROB},
  author       = {Ce Guo and Xieyuanli Chen and Zhiwen Zeng and Zirui Guo and Yihong Li and Haoran Xiao and Dewen Hu and Huimin Lu},
  doi          = {10.1109/TRO.2025.3613541},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Grasp like humans: Learning generalizable multi-fingered grasping from human proprioceptive sensorimotor integration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoVINS: Geographic-visual-inertial navigation system for large-scale drift-free aerial state estimation. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces GeoVINS, a vision-based navigation framework designed for large-scale global state estimation. By utilizing geographic information from satellite orthoimagery, GeoVINS tackles scale ambiguity and accumulative drift problems inherent in standard visual-inertial simultaneous localization and mapping (VI-SLAM) systems, and provides accurate, robust, and real-time global localization. In particular, to address the challenge of memory explosion issue for large-scale localization, we propose a novel aerial ‘classify-then-retrieve’ aerial VPR approach, where geographic locations can be efficiently identified and memory usage can be reduced by 3 to 4 orders of magnitude compared to classical retrieval-based approaches. In addition, a hierarchical geographic data association scheme, enhanced by state-of-the-art deep learning-based feature matching, guarantees high efficiency and robustness against variations in appearance and viewpoint. Relying on the obtained three-dimensional (3D) geographic information, GeoVINS achieves efficient global state initialization and precise motion tracking. To address GPU limitations in embedded devices, a collaborative CPU-GPU utilization approach is proposed, seamlessly integrating asynchronous global information to eliminate accumulative errors. Relying solely on satellite imagery and without requiring any other prior information, GeoVINS achieves rapid place recognition in unseen environments of city-scale (e.g., 2500${\mathrm{km}}^{2}$) on an embedded computing device with an inference time of 43 ms, and performs state estimation at a frequency of 25 Hz. The system enables autonomous UAV navigation as an alternative to GNSS.},
  archive      = {J_TROB},
  author       = {Chunyu Li and Mengfan He and Chao Chen and Jiacheng Liu and Xu Lyu and Guoquan Huang and Ziyang Meng},
  doi          = {10.1109/TRO.2025.3613467},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {GeoVINS: Geographic-visual-inertial navigation system for large-scale drift-free aerial state estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VINGS-mono: Visual-inertial gaussian splatting monocular SLAM in large scenes. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer, Score Manager, and Pose Refinement, which collectively improve mapping speed and localization accuracy. This enables the SLAM system to handle large-scale urban environments with up to 50 million Gaussian ellipsoids. To ensure global consistency in large-scale scenes, we design a Loop Closure module, which innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian Splatting for loop closure detection and correction of the Gaussian map. Additionally, we propose a Dynamic Eraser to address the inevitable presence of dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor and outdoor environments demonstrate that our approach achieves localization performance on par with Visual-Inertial Odometry while surpassing recent GS/NeRF SLAM methods. It also significantly outperforms all existing methods in terms of mapping and rendering quality. Furthermore, we developed a mobile app and verified that our framework can generate high-quality Gaussian maps in real time using only a smartphone camera and a low-frequency IMU sensor. To the best of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method capable of operating in outdoor environments and supporting kilometer-scale large scenes.},
  archive      = {J_TROB},
  author       = {Ke Wu and Zicheng Zhang and Muer Tie and Ziqing Ai and Zhongxue Gan and Wenchao Ding},
  doi          = {10.1109/TRO.2025.3613536},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {VINGS-mono: Visual-inertial gaussian splatting monocular SLAM in large scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object-centric kinodynamic planning for nonprehensile robot rearrangement manipulation. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonprehensile actions such as pushing are crucial for addressing multi-object rearrangement problems. Many traditional methods generate robot-centric actions, which differ from intuitive human strategies and are typically inefficient. To this end, we adopt an object-centric planning paradigm and propose a unified framework for addressing a range of large-scale, physics-intensive nonprehensile rearrangement problems challenged by modeling inaccuracies and real-world uncertainties. By assuming each object can actively move without being driven by robot interactions, our planner first computes desired object motions, which are then realized through robot actions generated online via a closed-loop pushing strategy. Through extensive experiments and in comparison with state-of-the-art baselines in both simulation and on a physical robot, we show that our object-centric planning framework can generate more intuitive and task-effective robot actions with significantly improved efficiency. In addition, we propose a benchmarking protocol to standardize and facilitate future research in nonprehensile rearrangement.},
  archive      = {J_TROB},
  author       = {Kejia Ren and Gaotian Wang and Andrew S. Morgan and Lydia E. Kavraki and Kaiyu Hang},
  doi          = {10.1109/TRO.2025.3613532},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Object-centric kinodynamic planning for nonprehensile robot rearrangement manipulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online multi-robot coordination and cooperation with task precedence relationships. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new formulation for the multi-robot task allocation problem that incorporates (a) complex precedence relationships between tasks, (b) efficient intra-task coordination, and (c) cooperation through the formation of robot coalitions. A task graph specifies the tasks and their relationships, and a set of reward functions models the effects of coalition size and preceding task performance. Maximizing task rewards is NP-hard; hence, we propose network flow-based algorithms to approximate solutions efficiently. A novel online algorithm performs iterative re-allocation, providing robustness to task failures and model inaccuracies to achieve higher performance than offline approaches. We comprehensively evaluate the algorithms in a testbed with random missions and reward functions and compare them to a mixed- integer solver and a greedy heuristic. Additionally, we validate the overall approach in an advanced simulator, modeling reward functions based on realistic physical phenomena and executing the tasks with realistic robot dynamics. Results establish efficacy in modeling complex missions and efficiency in generating high-fidelity task plans while leveraging task relationships.},
  archive      = {J_TROB},
  author       = {Walker Gosrich and Saurav Agarwal and Kashish Garg and Siddharth Mayya and Matthew Malencia and Mark Yim and Vijay Kumar},
  doi          = {10.1109/TRO.2025.3613558},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Online multi-robot coordination and cooperation with task precedence relationships},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy–Preserving robotic perception for object detection in curious cloud robotics. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3613551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud robotics allows low–power robots to perform computationally intensive inference tasks by offloading them to the cloud, raising privacy concerns when transmitting sensitive images. Although end–to–end encryption secures data in transit, it does not prevent misuse by inquisitive third–party services since data must be decrypted for processing. This paper tackles these privacy issues in cloud–based object detection tasks for service robots. We propose a co–trained encoder–decoder architecture that retains only task–specific features while obfuscating sensitive information, utilizing a novel weak loss mechanism with proposal selection for privacy preservation. A theoretical analysis of the problem is provided, along with an evaluation of the trade–off between detection accuracy and privacy preservation through extensive experiments on public datasets and a real robot.},
  archive      = {J_TROB},
  author       = {Michele Antonazzi and Matteo Alberti and Alex Bassot and Matteo Luperto and Nicola Basilico},
  doi          = {10.1109/TRO.2025.3613551},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Privacy–Preserving robotic perception for object detection in curious cloud robotics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUTO-IceNav: A local navigation strategy for autonomous surface ships in broken ice fields. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ice conditions often require ships to reduce speed and deviate from their main course to avoid damage to the ship. In addition, broken ice fields are becoming the dominant ice conditions encountered in the Arctic, where the effects of collisions with ice are highly dependent on where contact occurs and on the particular features of the ice floes. In this paper, we present AUTO-IceNav, a framework for the autonomous navigation of ships operating in ice floe fields. Trajectories are computed in a receding-horizon manner, where we frequently replan given updated ice field data. During a planning step, we assume a nominal speed that is safe with respect to the current ice conditions, and compute a reference path. We formulate a novel cost function that minimizes the kinetic energy loss of the ship from ship-ice collisions and incorporate this cost as part of our lattice-based path planner. The solution computed by the lattice planning stage is then used as an initial guess in our proposed optimization-based improvement step, producing a locally optimal path. Extensive experiments were conducted both in simulation and in a physical testbed to validate our approach.},
  archive      = {J_TROB},
  author       = {Rodrigue de Schaetzen and Alexander Botros and Ninghan Zhong and Kevin Murrant and Robert Gash and Stephen L. Smith},
  doi          = {10.1109/TRO.2025.3613472},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {AUTO-IceNav: A local navigation strategy for autonomous surface ships in broken ice fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient decentralised parallel task allocation for multiple robots. <em>TROB</em>, 1-18. (<a href='https://doi.org/10.1109/TRO.2025.3613566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with large-scale decentralised task allocation problems for multiple heterogeneous robots. One of the grand challenges with decentralised task allocation problems is the NP-hardness for computation and communication. This paper proposes a decentralised Decreasing Threshold Task Allocation (DTTA) algorithm that enables parallel allocation by leveraging a decreasing threshold to handle the NP-hardness. DTTA can release both computation and communication burdens for multiple robots in a decentralised network. Additionally, DTTA provides a theoretical guarantee of the quality of the solution for maximising submodular utility functions. Theoretical analysis indicates that DTTA can provide an optimality guarantee of $(1-\epsilon)/2$ with computation complexity of $O(\min (r^{2}, \frac{r}{\epsilon }\ln \frac{r}{\epsilon }))$ for each robot, where $\epsilon$ is the parameter controlling the decreasing speed of the threshold, $r$ is the number of tasks. To examine the performance of the proposed algorithm, we conduct numerical simulations based on a multi-target surveillance scenario. Simulation results demonstrate that DTTA delivers comparable solution quality significantly faster than state-of-the-art task allocation algorithms. Its advantages are particularly pronounced in large-scale missions with thousands of tasks and robots.},
  archive      = {J_TROB},
  author       = {Teng Li and Hyo-Sang Shin and Antonios Tsourdos},
  doi          = {10.1109/TRO.2025.3613566},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Efficient decentralised parallel task allocation for multiple robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based motion planning leveraging multivariate deep evidential regression. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2025.3613568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based motion planning methods have shown significant promise in enhancing the efficiency of traditional algorithms. However, they often face performance degradation in novel environments with drastic scene changes due to the limited generalization ability of deep neural networks (DNNs). This paper introduces a confidence-driven motion planning network (CDMPNet), comprising a feature extraction autoencoder and a confidence-driven sampling network (CDSNet). The autoencoder compresses point clouds into latent vectors. The CDSNet is a closed-form continuous-time neural network, which predicts hyperparameters of an evidential distribution over the subsequent state's mean and covariance for robot configuration sampling. We also present a CDMPNet-based neural planner and a CDMPNet-guided RRTConnect algorithm. Simulations and ablation studies are conducted on 2-D, 3-D, and 7-D planning tasks to validate the generalization ability of our method. Furthermore, we transfer the approach to a 7-DOF Sawyer robotic arm to demonstrate the potential for real-world deployment.},
  archive      = {J_TROB},
  author       = {Rixin Wang and Shuopeng Wang and Jintao Ye and Ying Zhang and Lina Hao},
  doi          = {10.1109/TRO.2025.3613568},
  journal      = {IEEE Transactions on Robotics},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {Learning-based motion planning leveraging multivariate deep evidential regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D-printable crease-free origami vacuum bending actuators for soft robots. <em>TROB</em>, 1-15. (<a href='https://doi.org/10.1109/TRO.2025.3588726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vacuum-based bending actuation offers benefits such as safety and compactness in soft robotics, it is often overlooked due to its limited actuation pressure, which restricts both bending angle and force output. This study presents a crease-free, origami-inspired vacuum bending actuator that advances both state-of-the-art vacuum bending actuators and traditional origami deformation principles by introducing orderly self-folding through optimized stiffness distribution. Achieved through finite element method (FEM), this design provides several advantages: (i) Self-folding allows for high bending angles (up to 138$^{\circ }$) in a compact form. (ii) The crease-free design facilitates 3D printing from a single soft material using a consumer-level fused filament fabrication (FFF) printer, specifically thermoplastic polyurethane (TPU) with a Shore hardness of 60A, potentially higher flexibility and durability. (iii) The compact configuration enables modular design, supporting reconfiguration as demonstrated in adaptable locomotion soft robots. (iv) The large bending angles allow the actuator to wrap around objects, offering extensive contact compared to other designs. This capability, combined with its vacuum-driven mechanism, enables synergy with self-closing suction cups in an octopus-like vacuum gripper, providing large versatility and grasping force for handling a wide range of objects, from small, irregular shapes to larger, flat items.},
  archive      = {J_TROB},
  author       = {Zhanwei Wang and Huaijin Chen and Syeda Shadab Zehra Zaidi and Ellen Roels and Hendrik Cools and Bram Vanderborght and Seppe Terryn},
  doi          = {10.1109/TRO.2025.3588726},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Robot.},
  title        = {3D-printable crease-free origami vacuum bending actuators for soft robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-level similarity approach for single-view object grasping: Matching, planning, and fine-tuning. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3588720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grasping unknown objects from a single view has remained a challenging topic in robotics due to the uncertainty of partial observation. Recent advances in large-scale models have led to benchmark solutions such as GraspNet-1Billion. However, such learning-based approaches still face a critical limitation in performance robustness for their sensitivity to sensing noise and environmental changes. To address this bottleneck in achieving highly generalized grasping, we abandon the traditional learning framework and introduce a new perspective: similarity matching, where similar known objects are utilized to guide the grasping of unknown target objects. We newly propose a method that robustly achieves unknown-object grasping from a single viewpoint through three key steps: 1) Leverage the visual features of the observed object to perform similarity matching with an existing database containing various object models, identifying potential candidates with high similarity; 2) Use the candidate models with pre-existing grasping knowledge to plan imitative grasps for the unknown target object; 3) Optimize the grasp quality through a local fine-tuning process. To address the uncertainty caused by partial and noisy observation, we propose a multi-level similarity matching framework that integrates semantic, geometric, and dimensional features for comprehensive evaluation. Especially, we introduce a novel point cloud geometric descriptor, the C-FPFH descriptor, which facilitates accurate similarity assessment between partial point clouds of observed objects and complete point clouds of database models. In addition, we incorporate the use of large language models, introduce the semi-oriented bounding box, and develop a novel point cloud registration approach based on plane detection to enhance matching accuracy under single-view conditions. Real-world experiments demonstrate that our proposed method significantly outperforms existing benchmarks in grasping a wide variety of unknown objects in both isolated and cluttered scenarios, showcasing exceptional robustness across varying object types and operating environments.},
  archive      = {J_TROB},
  author       = {Hao Chen and Takuya Kiyokawa and Zhengtao Hu and Weiwei Wan and Kensuke Harada},
  doi          = {10.1109/TRO.2025.3588720},
  journal      = {IEEE Transactions on Robotics},
  month        = {7},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {A multi-level similarity approach for single-view object grasping: Matching, planning, and fine-tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProxQP: An efficient and versatile quadratic programming solver for real-time robotics applications and beyond. <em>TROB</em>, 1-19. (<a href='https://doi.org/10.1109/TRO.2025.3577107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex Quadratic programming (QP) has become a core component in the modern engineering toolkit, particularly in robotics, where QP problems are legions, ranging from real-time whole-body controllers to planning and estimation algorithms. Many of those QPs need to be solved at high frequency. Meeting timing requirements requires taking advantage of as many structural properties as possible for the problem at hand. For instance, it is generally crucial to resort to warm-starting to exploit the resemblance of consecutive control iterations. While a large range of off-the-shelf QP solvers is available, only a few are suited to exploit problem structure and warm-starting capacities adequately. In this work, we propose the ProxQP algorithm, a new and efficient QP solver that exploits QP structures by leveraging primal-dual augmented Lagrangian techniques. For convex QPs, ProxQP features a global convergence guarantee to the closest feasible QP, an essential property for safe closed-loop control. We illustrate its practical performance on various standard robotic and control experiments, including a real-world closed-loop model predictive control application. While originally tailored for robotics applications, we show that ProxQP also performs at the level of state of the art on generic QP problems, making ProxQP suitable for use as an off-the-shelf solver for regular applications beyond robotics.},
  archive      = {J_TROB},
  author       = {Antoine Bambade and Fabian Schramm and Sarah El-Kazdadi and Stéphane Caron and Adrien Taylor and Justin Carpentier},
  doi          = {10.1109/TRO.2025.3577107},
  journal      = {IEEE Transactions on Robotics},
  month        = {6},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Robot.},
  title        = {ProxQP: An efficient and versatile quadratic programming solver for real-time robotics applications and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRANE: A redundant, multi-degree-of-freedom computed tomography robot for heightened needle dexterity within a medical imaging bore. <em>TROB</em>, 1-20. (<a href='https://doi.org/10.1109/TRO.2024.3364986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed Tomography (CT) image guidance enables accurate and safe minimally invasive treatment of diseases, including cancer and chronic pain, with needle-like tools via a percutaneous approach. The physician incrementally inserts and adjusts the needle with intermediate images due to the accuracy limitation of free-hand adjustment and patient physiological motion. Scanning frequency is limited to minimize ionizing radiation exposure for the patient and physician. Robots can provide high positional accuracy and compensate for physiological motion with fewer scans. To accomplish this, the robots must operate within the confined imaging bore while retaining sufficient dexterity to insert and manipulate the needle. This paper presents CRANE: CT Robotic Arm and Needle Emplacer, a CT-compatible robot with a design focused on system dexterity that enables physicians to manipulate and insert needles within the scanner bore as naturally as they would be able to by hand. We define abstract and measurable clinically motivated metrics for in-bore dexterity applicable to general-purpose intra-bore image-guided needle placement robots, develop an automatic robot planning and control method for intra-bore needle manipulation and device setup, and demonstrate the redundant linkage design provides dexterity across various human morphology and meets the clinical requirements for target accuracy during an in-situ evaluation.},
  archive      = {J_TROB},
  author       = {Dimitrious Schreiber and Zhaowei Yu and Taylor Henderson and Derek Chen and Alexander Norbash and Michael C. Yip},
  doi          = {10.1109/TRO.2024.3364986},
  journal      = {IEEE Transactions on Robotics},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Robot.},
  title        = {CRANE: A redundant, multi-degree-of-freedom computed tomography robot for heightened needle dexterity within a medical imaging bore},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
