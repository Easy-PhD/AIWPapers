<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami">TPAMI - 292</h2>
<ul>
<li><details>
<summary>
(2025). Structure-induced gradient regulation for generalizable vision-language models. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3604454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning, a recently emerging paradigm, adapts vision-language pre-trained models to new tasks efficiently by learning “soft prompts” for frozen models. However, in few-shot scenarios, its effectiveness is limited by sensitivity to the initialization and the time-consuming search for optimal initialization, hindering rapid adaptation. Additionally, prompt tuning risks reducing the models' generalizability due to overfitting on scarce training samples. To overcome these challenges, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization for better adaptation and a lightweight gradient regulating function for strong cross-domain generalizability in a meta-learning paradigm using only the weakly labeled image-text pre-training data. This is achieved through a Cross-Modal Hierarchical Clustering algorithm that organizes extensive image-text data into a structured hierarchy, facilitating robust meta-learning across diverse domains. Rather than designing a specific prompt tuning method, our GRAM can be easily incorporated into various prompt tuning methods in a model-agnostic way and bring about consistent improvement for them. Further, we consider a more practical but challenging setting: test-time prompt tuning with only unlabeled test samples and propose an improved structure-induced gradient regulating function to leverage the structured semantics of the meta-learning data for zero-shot generalization. This novel approach exploits the hierarchically clustered meta-learning data to model relationships between test-time data and meta-learning prototypes, facilitating the transfer of invariant knowledge without explicit annotations. Meanwhile, we introduce a structure complexity-informed strategy for adaptively constructing meta-training tasks and generating prototypes, which fully considers the diverse semantics within hierarchical clusters of different complexities. Comprehensive experiments demonstrate the state-of-the-art few- and zero-shot generalizability of our method.},
  archive      = {J_TPAMI},
  author       = {Juncheng Li and Minghe Gao and Siliang Tang and Longhui Wei and Jun Xiao and Fei Wu and Richang Hong and Meng Wang and Qi Tian},
  doi          = {10.1109/TPAMI.2025.3604454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structure-induced gradient regulation for generalizable vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end depth-based pipeline for selfie image rectification. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3604574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portraits or selfie images taken from a close distance typically suffer from perspective distortion. In this paper, we propose an end-to-end deep learning-based rectification pipeline to mitigate the effects of perspective distortion. We learn to predict the facial depth by training a deep CNN. The estimated depth is utilized to adjust the camera-to-subject distance by moving the camera farther, increasing the camera focal length, and reprojecting the 3D image features to the new perspective. The reprojected features are then fed to an inpainting module to fill in the missing pixels. We leverage a differentiable renderer to enable end-to-end training of our depth estimation and feature extraction nets to improve the rectified outputs. To boost the results of the inpainting module, we incorporate an auxiliary module to predict the horizontal movement of the camera which decreases the area that requires hallucination of challenging face parts such as ears. Unlike previous works, we process the full-frame input image at once without cropping the subject's face and processing it separately from the rest of the body, eliminating the need for complex post-processing steps to attach the face back to the subject's body. To train our network, we utilize the popular game engine Unreal Engine to generate a large synthetic face dataset containing various subjects, head poses, expressions, eyewear, clothes, and lighting. Quantitative and qualitative results show that our rectification pipeline outperforms previous methods, and produces comparable results with a time-consuming 3D GAN-based method while being more than 260 times faster.},
  archive      = {J_TPAMI},
  author       = {Ahmed Alhawwary and Janne Mustaniemi and Phong Nguyen-Ha and Janne Heikkilä},
  doi          = {10.1109/TPAMI.2025.3604574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An end-to-end depth-based pipeline for selfie image rectification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LatentAugment: Data augmentation via guided manipulation of GAN's latent space. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3598866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic to the dataset and the downstream task. A wide set of experiments shows that LatentAugment improves the generalisation of a deep model translating from MRI-to-CT beating both standard DA as well GAN-based sampling. We further demonstrate its effectiveness when translating from low-energy mammograms to dual-energy subtracted images in contrast-enhanced spectral mammography. Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic samples show superior mode coverage and diversity. Code is available at: https://github.com/ltronchin/LatentAugment.},
  archive      = {J_TPAMI},
  author       = {Lorenzo Tronchin and Minh H. Vu and Paolo Soda and Tommy Löfstedt},
  doi          = {10.1109/TPAMI.2025.3598866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LatentAugment: Data augmentation via guided manipulation of GAN's latent space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MovieChat+: Question-aware sparse memory for long video question answering. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3604614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific vision tasks. Yet, existing methods either employ complex spatial-temporal modules or rely heavily on additional perception models to extract temporal features for video understanding, performing well only on short videos. For long videos, the computational complexity and memory costs associated with long-term temporal connections are significantly increased, posing additional challenges. Leveraging the hierarchical memory structure of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination, we propose MovieChat within a training-free memory consolidation mechanism to overcome these challenges, which transfers dense frames from short-term memory into sparse tokens in long-term memory by temporally merging adjacent frames. We lift pre-trained large multi-modal models for understanding long videos without additional trainable modules, employing a zero-shot approach. Additionally, in our new version, MovieChat+, we design an enhanced training-free vision-question matching-based memory consolidation mechanism to better anchor predictions to relevant visual content. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video, 2K temporal grounding labels, and 14K manual annotations. Resources are available at: https://github.com/rese1f/MovieChat.},
  archive      = {J_TPAMI},
  author       = {Enxin Song and Wenhao Chai and Tian Ye and Jenq-Neng Hwang and Xi Li and Gaoang Wang},
  doi          = {10.1109/TPAMI.2025.3604614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MovieChat+: Question-aware sparse memory for long video question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRANCE: Joint token-optimization and structural channel-pruning for adaptive ViT inference. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3605239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The troublesome model size and quadratic computational complexity associated with token quantity pose significant deployment challenges for Vision Transformers (ViTs) in practical applications. Despite recent advancements in model pruning and token reduction techniques speed up the inference speed of ViTs, these approaches either adopt a fixed sparsity ratio or overlook the meaningful interplay between architectural optimization and token selection. Consequently, this static and single-dimension compression often leads to pronounced accuracy degradation under aggressive compression rates, as they fail to fully explore redundancies across these two orthogonal dimensions. Therefore, we introduce PRANCE, a framework which can jointly optimize activated channels and tokens on a per-sample basis, aiming to accelerate ViTs' inference process from a unified data and architectural perspective. However, the joint framework poses challenges to both architectural and decision-making aspects. Firstly, while ViTs inherently support variable-token inference, they do not facilitate dynamic computations for variable channels. To overcome this limitation, we propose a meta-network using weight-sharing techniques to support arbitrary channels of the Multi-Head Self-Attention (MHSA) and Multi-Layer Perceptron (MLP) layers, serving as a foundational model for architectural decision-making. Secondly, simultaneously optimizing the model structure and input data constitutes a combinatorial optimization problem with an extremely large decision space, reaching up to around $10^{14}$, making supervised learning infeasible. To this end, we design a lightweight selector employing Proximal Policy Optimization algorithm (PPO) for efficient decision-making. Furthermore, we introduce a novel “Result-to-Go” training mechanism that models ViTs' inference process as a Markov decision process, significantly reducing action space and mitigating delayed-reward issues during training. Additionally, our framework simultaneously supports different kinds of token optimization methods such as pruning, merging, and sequential pruning-merging strategies. Extensive experiments demonstrate the effectiveness of PRANCE in reducing FLOPs by approximately 50%, retaining only about 10% of tokens while achieving lossless Top-1 accuracy.},
  archive      = {J_TPAMI},
  author       = {Ye Li and Chen Tang and Yuan Meng and Jiajun Fan and Zenghao Chai and Xinzhu Ma and Zhi Wang and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2025.3605239},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PRANCE: Joint token-optimization and structural channel-pruning for adaptive ViT inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Make identity indistinguishable: Utility-preserving face dataset publication with provable privacy guarantees. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3605195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of personal devices, there are abundant valuable face image datasets in the industry, which provides opportunities for the development of visual models. However, privacy concerns related to identity sensitive information hinder face datasets sharing. Despite existing works dedicated to removing identity sensitive information from images, they either lack provable privacy guarantees or compromise crucial face dataset utilities, e.g., identity correlation and image naturalness. To overcome these weaknesses, we propose a novel face dataset publication scheme that protects face images by obfuscating face features. The obfuscated features still retain a certain level of correlation, allowing the protected dataset to be used for training. In the process of obfuscating the features, we design a novel metric differential privacy mechanism, which can enhance the correlation between features while ensuring privacy. Furthermore, we construct a latent diffusion model with identity and attribute as inputs to improve the naturalness of generated images. Extensive experimental results and theoretical analysis demonstrate our scheme significantly outperforms existing works in providing privacy protection while maintaining high dataset utility for downstream tasks.},
  archive      = {J_TPAMI},
  author       = {Yushu Zhang and Junhao Ji and Tao Wang and Ruoyu Zhao and Wenying Wen and Yong Xiang},
  doi          = {10.1109/TPAMI.2025.3605195},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Make identity indistinguishable: Utility-preserving face dataset publication with provable privacy guarantees},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3606874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the decentralized composite optimization problem. We propose a novel decentralized variance-reduction proximal-gradient algorithmic framework, called PMGT-VR, which combines several techniques, including multi-consensus, gradient tracking, and variance reduction. The proposed framework imitates centralized algorithms and algorithms under this framework achieve convergence rates similar to that of their centralized counterparts. We also describe and analyze two representative algorithms, PMGT-SAGA and PMGT-LSVRG, and compare them to existing state-of-the-art proximal algorithms. To the best of our knowledge, PMGT-VR is the first linearly convergent decentralized stochastic algorithm that can solve decentralized composite optimization problems. Numerical experiments are provided to demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_TPAMI},
  author       = {Haishan Ye and Wei Xiong and Tong Zhang},
  doi          = {10.1109/TPAMI.2025.3606874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCGS: Multiview consistency enhancement for sparse-view 3D gaussian radiance fields. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3607103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiance fields represented by 3D Gaussians excel at synthesizing novel views, offering both high training efficiency and fast rendering. However, with sparse input views, the lack of multi-view consistency constraints results in poorly initialized Gaussians and unreliable heuristics for optimization, leading to suboptimal performance. Existing methods often incorporate depth priors from dense estimation networks but overlook the inherent multi-view consistency in input images. Additionally, they rely on dense initialization, which limits the efficiency of scene representation. To overcome these challenges, we propose a view synthesis framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic scene reconstruction from sparse views. The key innovations of MCGS in enhancing multi-view consistency are as follows: i) We leverage matching priors from a sparse matcher to initialize Gaussians primarily on textured regions, while low-texture areas are populated with randomly distributed Gaussians. This yields a compact yet sufficient set of initial Gaussians. ii) We propose a multi-view consistency-guided progressive pruning strategy to dynamically eliminate inconsistent Gaussians. This approach confines their optimization to a consistency-constrained space, which ensures robust and coherent scene reconstruction. These strategies enhance robustness to sparse views, accelerate rendering, and reduce memory consumption, making MCGS a practical framework for 3D Gaussian Splatting.},
  archive      = {J_TPAMI},
  author       = {Yuru Xiao and Deming Zhai and Wenbo Zhao and Kui Jiang and Junjun Jiang and Xianming Liu},
  doi          = {10.1109/TPAMI.2025.3607103},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MCGS: Multiview consistency enhancement for sparse-view 3D gaussian radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Centroiding point-objects with event cameras. <em>TPAMI</em>, 1-10. (<a href='https://doi.org/10.1109/TPAMI.2025.3604385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based sensors (EBS), with their low latency and high dynamic range, are a promising means for tracking unresolved point-objects. Conventional EBS centroiding methods assume the generated events follow a Gaussian distribution and require long event streams ($\gt 1$s) for accurate localization. However, these assumptions are inadequate for centroiding unresolved objects, since the EBS circuitry causes non-Gaussian event distributions, and because using long event streams negates the low-latency advantage of EBS. In this work, we derive a closed-form spatiotemporal event distribution that accounts for these non-Gaussian effects and relaxes the long-time window requirement. Using Fisher analysis, we show that the spatial distribution of events in short time windows ($\leq 20$ ms) contains sufficient information for accurately estimating both position and velocity. To validate our analysis, we create the first EBS dataset of unresolved point-objects with subpixel ground truth using a high-speed monitor. We demonstrate that a small LSTM network can estimate an object's position within 1pixel and velocity within $\pm 17\%$ using only 5ms of event data, outperforming traditional approaches. These improvements enable accurate and quick centroiding of fast and dim objects, and we publish all code and data to support future research.},
  archive      = {J_TPAMI},
  author       = {Connor Hashemi and Dennis Melamed and Albert W. Reed and Nitesh Menon and Keigo Hirakawa and Scott McCloskey},
  doi          = {10.1109/TPAMI.2025.3604385},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Centroiding point-objects with event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving generalized visual grounding with instance-aware joint learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3607387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized visual grounding tasks, including Generalized Referring Expression Comprehension (GREC) and Segmentation (GRES), extend the classical visual grounding paradigm by accommodating multi-target and non-target scenarios. Specifically, GREC focuses on accurately identifying all referential objects at the coarse bounding box level, while GRES aims for achieve fine-grained pixel-level perception. However, existing approaches typically treat these tasks independently, overlooking the benefits of jointly training GREC and GRES to ensure consistent multi-granularity predictions and streamline the overall process. Moreover, current methods often treat GRES as a semantic segmentation task, neglecting the crucial role of instance-aware capabilities and the necessity of ensuring consistent predictions between instance-level boxes and masks. To address these limitations, we propose InstanceVG, a multi-task generalized visual grounding framework equipped with instance-aware capabilities, which leverages instance queries to unify the joint and consistency predictions of instance-level boxes and masks. To the best of our knowledge, InstanceVG is the first framework to simultaneously tackle both GREC and GRES while incorporating instance-aware capabilities into generalized visual grounding. To instantiate the framework, we assign each instance query a prior reference point, which also serves as an additional basis for target matching. This design facilitates consistent predictions of points, boxes, and masks for the same instance. Extensive experiments obtained on ten datasets across four tasks demonstrate that InstanceVG achieves state-of-the-art performance, significantly surpassing the existing methods in various evaluation metrics. The code and model will be made publicly available.},
  archive      = {J_TPAMI},
  author       = {Ming Dai and Wenxuan Cheng and Jiang-Jiang Liu and Lingfeng Yang and Zhenhua Feng and Wankou Yang and Jingdong Wang},
  doi          = {10.1109/TPAMI.2025.3607387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving generalized visual grounding with instance-aware joint learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning of stochastic kriging for individualized prediction. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3607773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Kriging (SK) is a generalized variant of Gaussian process regression, and it is developed for dealing with non-i.i.d. noise in functional responses. Although SK has achieved substantial success in various engineering applications, its intrinsic modeling strategy by focusing on the sample mean limits its flexibility and capability of predicting individual functional samples. Moreover, the performance of SK can be impaired under scarce data scenarios, which are commonly encountered in engineering applications, especially for start-up or just deployed systems. In this paper, we propose a novel transfer learning framework to address the challenges of individualization and data scarcity in traditional SK. The proposed framework features a within-process model to facilitate individualized prediction and a between-process model to leverage information from related processes for resolving the issue of data scarcity. The within- and between-process models are integrated through a tailored convolution process, which quantifies interactions within and between processes using a specially designed covariance matrix and corresponding kernel parameters. Statistical properties are investigated on the parameter estimation of the proposed framework, which provide theoretical guarantees for the performance of transfer learning. The proposed method is compared with benchmark methods through various numerical and real case studies, and the results demonstrate the superiority of the proposed method in dealing with individualized prediction of functional responses, especially when limited data are available in the process of interest.},
  archive      = {J_TPAMI},
  author       = {Jinwei Yao and Jianguo Wu and Yongxiang Li and Chao Wang},
  doi          = {10.1109/TPAMI.2025.3607773},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transfer learning of stochastic kriging for individualized prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentence-level relation semantics learning via contrastive sentences. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3607794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence-level semantics plays a key role in language understanding. There exist subtle relations and dependencies among sentence-level samples, which is to be exploited. For example, in relational triple extraction, existing models overemphasize extraction modules, ignoring the sentence-level semantics and relation information, which causes (1) the semantics fed to extraction modules is relation-unaware; (2) each sample is trained individually without considering inter-sample dependency. To address these issues, we first propose the model-agnostic multi-relation detection task, which incorporates relation information into text encoding to generate the relation-aware semantics. Then we propose the model-agnostic multi-relation supervised contrastive learning, which leverages the relation-derived inter-sample dependencies as a supervised signal to learn discriminative semantics via drawing together or pushing away the sentence-level semantics regarding whether they share the same/similar relations. Besides, we design the reverse label frequency weighting and hierarchical label embedding mechanisms to alleviate label imbalance and integrate relation hierarchy. Our method can be applied to any RTE model and we conduct extensive experiments on five backbones by augmenting them with our method. Experimental results on four public benchmarks show that our method can bring significant and consistent improvements to various backbones and model analysis further verify the effectiveness of our method.},
  archive      = {J_TPAMI},
  author       = {Bowen Xing and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2025.3607794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sentence-level relation semantics learning via contrastive sentences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3607711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework, Combo, for harmonious co-speech holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes both speech signals and character guidance (e.g., identity and emotion), which hinders further adaptation to varying guidance; on the output end, holistic human motions mainly consist of facial expressions and body movements, which are inherently correlated but non-trivial to coordinate in current data-driven generation process. In response to the above challenge, we propose tailored designs to both ends. For the former, we propose to pre-train on data regarding a fixed identity with neutral emotion, and defer the incorporation of customizable conditions (identity and emotion) to fine-tuning stage, which is boosted by our novel X-Adapter for parameter-efficient fine-tuning. For the latter, we propose a simple yet effective transformer design, DU-Trans, which first divides into two branches to learn individual features of face expression and body movements, and then unites those to learn a joint bi-directional distribution and directly predicts combined coefficients. Evaluated on BEAT2 and SHOW datasets, Combo is highly effective in generating high-quality motions but also efficient in transferring identity and emotion. Project website: https://xc-csc101.github.io/combo/.},
  archive      = {J_TPAMI},
  author       = {Chao Xu and Mingze Sun and Zhi-Qi Cheng and Fei Wang and Yang Liu and Baigui Sun and Ruqi Huang and Alexander Hauptmann},
  doi          = {10.1109/TPAMI.2025.3607711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmenting the motion components of a video: A long-term unsupervised model. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3608065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings have the ability to continuously analyze a video and immediately extract the motion components. We want to adopt this paradigm to provide a coherent and stable motion segmentation over the video sequence. In this perspective, we propose a novel long-term spatio-temporal model operating in a totally unsupervised way. It takes as input the volume of consecutive optical flow (OF) fields, and delivers a volume of segments of coherent motion over the video. More specifically, we have designed a transformer-based network, where we leverage a mathematically well-founded framework, the Evidence Lower Bound (ELBO), to derive the loss function. The loss function combines a flow reconstruction term involving spatio-temporal parametric motion models combining, in a novel way, polynomial (quadratic) motion models for the spatial dimensions and B-splines for the time dimension of the video sequence, and a regularization term enforcing temporal consistency on the segments. We report experiments on four VOS benchmarks, demonstrating competitive quantitative results while performing motion segmentation on a sequence in one go. We also highlight through visual results the key contributions on temporal consistency brought by our method.},
  archive      = {J_TPAMI},
  author       = {Etienne Meunier and Patrick Bouthemy},
  doi          = {10.1109/TPAMI.2025.3608065},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Segmenting the motion components of a video: A long-term unsupervised model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward effective knowledge distillation: Navigating beyond small-data pitfall. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3607982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectacular success of training large models on extensive datasets highlights the potential of scaling up for exceptional performance. To deploy these models on edge devices, knowledge distillation (KD) is commonly used to create a compact model from a larger, pretrained teacher model. However, as models and datasets rapidly scale up in practical applications, it is crucial to consider the applicability of existing KD approaches originally designed for limited-capacity architectures and small-scale datasets. In this paper, we revisit current KD methods and identify the presence of a small-data pitfall, where most modifications to vanilla KD prove ineffective on large-scale datasets. To guide the design of consistently effective KD methods across different data scales, we conduct a meticulous evaluation of the knowledge transfer process. Our findings reveal that incorporating more useful information is crucial for achieving consistently effective KD methods, while modifications in loss functions show relatively less significance. In light of this, we present a paradigmatic example that combines vanilla KD with deep supervision, incorporating additional information into the student during distillation. This approach surpasses almost all recent KD methods. We believe our study will offer valuable insights to guide the community in navigating beyond the small-data pitfall and toward consistently effective KD.},
  archive      = {J_TPAMI},
  author       = {Zhiwei Hao and Jianyuan Guo and Kai Han and Han Hu and Chang Xu and Yunhe Wang},
  doi          = {10.1109/TPAMI.2025.3607982},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward effective knowledge distillation: Navigating beyond small-data pitfall},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards real zero-shot camouflaged object segmentation without camouflaged annotations. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Segmentation (COS) faces significant challenges due to the scarcity of annotated data, where meticulous pixel-level annotation is both labor-intensive and costly, primarily due to the intricate object-background boundaries. Addressing the core question, ”Can COS be effectively achieved in a zero-shot manner without manual annotations for any camouflaged object?”, we propose an affirmative solution. We analyze the learned attention patterns for camouflaged objects and introduce a robust zero-shot COS framework. Our findings reveal that while transformer models for salient object segmentation (SOS) prioritize global features in their attention mechanisms, camouflaged object segmentation exhibits both global and local attention biases. Based on these findings, we design a framework that adapts with the inherent local pattern bias of COS while incorporating global attention patterns and a broad semantic feature space derived from SOS. This enables efficient zero-shot transfer for COS. Specifically, We incorporate an Masked Image Modeling (MIM) based image encoder optimized for Parameter-Efficient Fine-Tuning (PEFT), a Multimodal Large Language Model (M-LLM), and a Multi-scale Fine-grained Alignment (MFA) mechanism. The MIM encoder captures essential local features, while the PEFT module learns global and semantic representations from SOS datasets. To further enhance semantic granularity, we leverage the M-LLM to generate caption embeddings conditioned on visual cues, which are meticulously aligned with multi-scale visual features via MFA. This alignment enables precise interpretation of complex semantic contexts. Moreover, we introduce a learnable codebook to represent the M-LLM during inference, significantly reducing computational demands while maintaining performance. Our framework demonstrates its versatility and efficacy through rigorous experimentation, achieving state-of-the-art performance in zero-shot COS with $F_{\beta }^{w}$ scores of 72.9% on CAMO and 71.7% on COD10K. By removing the M-LLM during inference, we achieve an inference speed comparable to that of traditional end-to-end models, reaching 18.1 FPS. Additionally, our method excels in polyp segmentation, and underwater scene segmentation, outperforming challenging baselines in both zero-shot and supervised settings, thereby highlighting its potential for broad applicability in diverse segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Cheng Lei and Jie Fan and Xinran Li and Tian-zhu Xiang and Ao Li and Ce Zhu and Le Zhang},
  doi          = {10.1109/TPAMI.2025.3600461},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards real zero-shot camouflaged object segmentation without camouflaged annotations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H2OT: Hierarchical hourglass tokenizer for efficient video pose transformers. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3608284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.},
  archive      = {J_TPAMI},
  author       = {Wenhao Li and Mengyuan Liu and Hong Liu and Pichao Wang and Shijian Lu and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3608284},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {H2OT: Hierarchical hourglass tokenizer for efficient video pose transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinterpreting hypergraph kernels: Insights through homomorphism analysis. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3608902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing expressive hypergraph kernels that can effectively capture high-order structural information is a fundamental challenge in hypergraph learning. In this paper, we propose a novel comparison framework based on hypergraph homomorphisms to evaluate and compare the expressive ability of existing hypergraph kernels. We revisit classical kernels such as Hypergraph Weisfeiler-Lehman (HG WL) and Hypergraph Rooted kernels, providing theoretical conditions under which they fail to distinguish non-isomorphic hypergraphs. Motivated by these insights, we introduce the Hypergraph Subtree-Cycle Kernel, which augments subtree-based features with cycle-based structural patterns to enhance expressiveness. We propose two variants: HG SCKernelv1 and HG SCKernelv2. Extensive experiments on five graph and ten hypergraph classification benchmarks demonstrate the superior performance of our methods, confirming the effectiveness of integrating homomorphism-guided design into hypergraph kernels.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhang and Shaoyi Du and Yifan Feng and Shihui Ying and Yue Gao},
  doi          = {10.1109/TPAMI.2025.3608902},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reinterpreting hypergraph kernels: Insights through homomorphism analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M3D: A multimodal, multilingual and multitask dataset for grounded document-level information extraction. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal information extraction (IE) tasks have attracted increasing attention because many studies have shown that multimodal information benefits text information extraction. However, existing multimodal IE datasets mainly focus on sentence-level image-facilitated IE in English text, and pay little attention to video-based multimodal IE and fine-grained visual grounding. Therefore, in order to promote the development of multimodal IE, we constructed a multimodal multilingual multitask dataset, named M3D, which has the following features: (1) It contains paired document-level text and video to enrich multimodal information; (2) It supports two widely-used languages, namely English and Chinese; (3) It includes more multimodal IE tasks such as entity recognition, entity chain extraction, relation extraction and visual grounding. In addition, our dataset introduces an unexplored theme, i.e., biography, enriching the domains of multimodal IE resources. To establish a benchmark for our dataset, we propose an innovative hierarchical multimodal IE model. This model effectively leverages and integrates multimodal information through a Denoised Feature Fusion Module (DFFM). Furthermore, in non-ideal scenarios, modal information is often incomplete. Thus, we designed a Missing Modality Construction Module (MMCM) to alleviate the issues caused by missing modalities. Our model achieved an average performance of 53.80% and 53.77% on four tasks in English and Chinese datasets, respectively, which set a reasonable standard for subsequent research. In addition, we conducted more analytical experiments to verify the effectiveness of our proposed module. We believe that our work can promote the development of the field of multimodal IE.},
  archive      = {J_TPAMI},
  author       = {Jiang Liu and Bobo Li and Xinran Yang and Na Yang and Hao Fei and Mingyao Zhang and Fei Li and Donghong Ji},
  doi          = {10.1109/TPAMI.2025.3609288},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {M3D: A multimodal, multilingual and multitask dataset for grounded document-level information extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified perspective for loss-oriented imbalanced learning via localization. <em>TPAMI</em>, 1-19. (<a href='https://doi.org/10.1109/TPAMI.2025.3609440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent imbalance in real-world datasets, naïve Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Zitai Wang and Qianqian Xu and Zhiyong Yang and Zhikang Xu and Linchao Zhang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2025.3609440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified perspective for loss-oriented imbalanced learning via localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian unsupervised disentanglement of anatomy and geometry for deep groupwise image registration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a general Bayesian learning framework for multi-modal groupwise image registration. The method builds on probabilistic modelling of the image generative process, where the underlying common anatomy and geometric variations of the observed images are explicitly disentangled as latent variables. Therefore, groupwise image registration is achieved via hierarchical Bayesian inference. We propose a novel hierarchical variational auto-encoding architecture to realise the inference procedure of the latent variables, where the registration parameters can be explicitly estimated in a mathematically interpretable fashion. Remarkably, this new paradigm learns groupwise image registration in an unsupervised closed-loop self-reconstruction process, sparing the burden of designing complex image-based similarity measures. The computationally efficient disentangled network architecture is also inherently scalable and flexible, allowing for groupwise registration on large-scale image groups with variable sizes. Furthermore, the inferred structural representations from multi-modal images via disentanglement learning are capable of capturing the latent anatomy of the observations with visual semantics. Extensive experiments were conducted to validate the proposed framework, including four different datasets from cardiac, brain, and abdominal medical images. The results have demonstrated the superiority of our method over conventional similarity-based approaches in terms of accuracy, efficiency, scalability, and interpretability.},
  archive      = {J_TPAMI},
  author       = {Xinzhe Luo and Xin Wang and Linda Shapiro and Chun Yuan and Jianfeng Feng and Xiahai Zhuang},
  doi          = {10.1109/TPAMI.2025.3609521},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bayesian unsupervised disentanglement of anatomy and geometry for deep groupwise image registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MV2DFusion: Leveraging modality-specific object semantics for multi-modal 3D detection. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3609348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of autonomous vehicles has significantly increased the demand for robust 3D object detection systems. While cameras and LiDAR sensors each offer unique advantages-cameras provide rich texture information and LiDAR offers precise 3D spatial data-relying on a single modality often leads to performance limitations. This paper introduces MV2DFusion, a multi-modal detection framework that integrates the strengths of both worlds through an advanced query-based fusion mechanism. By introducing an image query generator to align with image-specific attributes and a point cloud query generator, MV2DFusion effectively combines modality-specific object semantics without biasing toward one single modality. Then the sparse fusion process can be accomplished based on the valuable object semantics, ensuring efficient and accurate object detection across various scenarios. Our framework's flexibility allows it to integrate with any image and point cloud-based detectors, showcasing its adaptability and potential for future advancements. Extensive evaluations on the nuScenes and Argoverse2 datasets demonstrate that MV2DFusion achieves state-of-the-art performance, particularly excelling in long-range detection scenarios.},
  archive      = {J_TPAMI},
  author       = {Zitian Wang and Zehao Huang and Yulu Gao and Naiyan Wang and Si Liu},
  doi          = {10.1109/TPAMI.2025.3609348},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MV2DFusion: Leveraging modality-specific object semantics for multi-modal 3D detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single voter spreading for efficient correspondence grouping and 3D registration. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3609474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining highly consistent correspondences between point clouds is crucial for computer vision tasks such as 3D registration and recognition. Due to nuisances such as limited overlap and noise, initial correspondences often contain a large number of outliers, imposing a great challenge to downstream tasks. In this paper, we present a novel single voter spreading (SVOS) method for efficient 3D correspondence grouping and 3D registration. Our core insight is to leverage low-order graph constraints only in a single voter spreading voting scheme to achieve comparable constrain-ability as complex constraints without searching them. First, a simple first-order graph is constructed for the initial correspondence set. Second, a two-stage voting method is proposed, including single voter voting and spread voters voting. Each voting stage involves both local and global voting via edge constraints only. This promises good selectivity while making the voting process time- and storage-efficient. Finally, top-scored correspondences are opted for robust transformation estimation. Experiments on U3M, 3DMatch/3DLoMatch, ETH, and KITTI-LC datasets verify that SVOS achieves new state-of-the-art correspondence grouping and registration performance, while being light-weight and robust to graph construction parameters. The code will be available at https://github.com/ZhaoZeng-pro/SVOS.},
  archive      = {J_TPAMI},
  author       = {Siwen Quan and Zhao Zeng and Xiyu Zhang and Jiaqi Yang},
  doi          = {10.1109/TPAMI.2025.3609474},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single voter spreading for efficient correspondence grouping and 3D registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-DiFuser: A masked diffusion model for unified unsupervised image fusion. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of ground truth (GT) in most fusion tasks poses significant challenges for model optimization, evaluation, and generalization. Existing fusion methods achieving complementary context aggregation predominantly rely on hand-crafted fusion rules and sophisticated loss functions, which introduce subjectivity and often fail to adapt to complex real-world scenarios. To address this challenge, we propose Mask-DiFuser, a novel fusion paradigm that ingeniously transforms the unsupervised image fusion task into a dual masked image reconstruction task by incorporating masked image modeling with a diffusion model, overcoming various issues arising from the absence of GT. In particular, we devise a dual masking scheme to simulate complementary information and employ a diffusion model to restore source images from two masked inputs, thereby aggregating complementary contexts. A content encoder with an attention parallel feature mixer is deployed to extract and integrate complementary features, offering local content guidance. Moreover, a semantic encoder is developed to supply global context which is integrated into the diffusion model via a cross-attention mechanism. During inference, Mask-DiFuser begins with a Gaussian distribution and iteratively denoises it conditioned on multi-source images to directly generate fused images. The masked diffusion model, learning priors from high-quality natural images, ensures that fusion results align more closely with human visual perception. Extensive experiments on several fusion tasks, including infrared-visible, medical, multi-exposure, and multi-focus image fusion, demonstrate that Mask-DiFuser significantly outshines SOTA fusion alternatives.},
  archive      = {J_TPAMI},
  author       = {Linfeng Tang and Chunyu Li and Jiayi Ma},
  doi          = {10.1109/TPAMI.2025.3609323},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mask-DiFuser: A masked diffusion model for unified unsupervised image fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REST: Holistic learning for end-to-end semantic segmentation of whole-scene remote sensing imagery. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of remote sensing imagery (RSI) is a fundamental task that aims at assigning a category label to each pixel. To pursue precise segmentation with one or more fine-grained categories, semantic segmentation often requires holistic segmentation of whole-scene RSI (WRI), which is normally characterized by a large size. However, conventional deep learning methods struggle to handle holistic segmentation of WRI due to the memory limitations of the graphics processing unit (GPU), thus requiring to adopt suboptimal strategies such as cropping or fusion, which result in performance degradation. Here, we introduce the Robust End-to-end semantic Segmentation architecture for whole-scene remoTe sensing imagery (REST). REST is the first intrinsically end-to-end framework for truly holistic segmentation of WRI, supporting a wide range of encoders and decoders in a plug-and-play fashion. It enables seamless integration with mainstream semantic segmentation methods, and even more advanced foundation models. Specifically, we propose a novel spatial parallel interaction mechanism (SPIM) within REST to overcome GPU memory constraints and achieve global context awareness. Unlike traditional parallel methods, SPIM enables REST to process a WRI effectively and efficiently by combining parallel computation with a divide-and-conquer strategy. Both theoretical analysis and experiments demonstrate that REST attains near-linear throughput scalability as additional GPUs are employed. Extensive experiments demonstrate that REST consistently outperforms existing cropping-based and fusion-based methods across a variety of scenarios, ranging from single-class to multi-class segmentation, from multispectral to hyperspectral imagery, and from satellite to drone platforms. The robustness and versatility of REST are expected to offer a promising solution for the holistic segmentation of WRI, with the potential for further extension to large-size medical imagery segmentation. The source code will be released at https://weichenrs.github.io/REST.},
  archive      = {J_TPAMI},
  author       = {Wei Chen and Lorenzo Bruzzone and Bo Dang and Yuan Gao and Youming Deng and Jin-Gang Yu and Liangqi Yuan and Yansheng Li},
  doi          = {10.1109/TPAMI.2025.3609767},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {REST: Holistic learning for end-to-end semantic segmentation of whole-scene remote sensing imagery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamReward-X: Boosting high-quality 3D generation with human preference alignment. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3609680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 3D content generation have shown remarkable success by leveraging pretrained large-scale diffusion models. However, existing 3D generation results are far from perfect as one primary challenge lies in aligning 3D content with human preference, especially in text-driven 3D generation. In this paper, we propose a novel 3D generation framework, coined DreamReward, to learn and improve text-driven 3D generation models from human preference feedback. First, we collect 25K+ expert comparisons based on a systematic annotation pipeline including filtering, rating, and ranking. Then, we build Reward3D, the first general-purpose text-to-3D human preference reward model to encode human preferences effectively. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL) algorithm to guide the noisy pretrained distribution toward the actual user-prompt distributions in optimization. With the rapid development and growing popularity of 4D and image-driven 3D generation, we further extend our DreamReward into 4D generation (DreamReward-4D) and image-to-3D generation (DreamReward-img) in a low-cost but effective manner. Despite the impressive results created by DreamReward, the diversity in text-driven 3D generation is limited due to inherent maximum likelihood-seeking issues. To address this, we explore the gap between Denoising Diffusion Implicit Models (DDIM) and SDS-based DreamFL in the generation process and propose DreamReward++, where we introduce a reward-aware noise sampling strategy to unleash text-driven diversity during the generation process while ensuring human preference alignment. Grounded by theoretical proof and extensive experiment comparisons, our method successfully generates high-fidelity and diverse 3D results with significant boosts in prompt alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve 3D generation.},
  archive      = {J_TPAMI},
  author       = {Fangfu Liu and Junliang Ye and Yikai Wang and Hanyang Wang and Zhengyi Wang and Jun Zhu and Yueqi Duan},
  doi          = {10.1109/TPAMI.2025.3609680},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamReward-X: Boosting high-quality 3D generation with human preference alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object detection data synthesis via box-to-image generation based on diffusion models. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3609962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multi-class objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domain-specific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. We also design an evaluation protocol based on COCO-2014 to validate the synthetic data of ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods. In addition, we employ a series of large-scale object detection datasets to train a general model named Stable Box Diffusion, which covers thousands of object categories in most common scenes.},
  archive      = {J_TPAMI},
  author       = {Jingyuan Zhu and Huimin Ma and Jiansheng Chen and Jian Yuan},
  doi          = {10.1109/TPAMI.2025.3609962},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Object detection data synthesis via box-to-image generation based on diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parse trees guided LLM prompt compression. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3609956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.},
  archive      = {J_TPAMI},
  author       = {Wenhao Mao and Chengbin Hou and Tianyu Zhang and Xinyu Lin and Ke Tang and Hairong Lv},
  doi          = {10.1109/TPAMI.2025.3609956},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parse trees guided LLM prompt compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards size-invariant salient object detection: A generic evaluation and optimization approach. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach.},
  archive      = {J_TPAMI},
  author       = {Shilong Bao and Qianqian Xu and Feiran Li and Boyu Han and Zhiyong Yang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2025.3609882},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards size-invariant salient object detection: A generic evaluation and optimization approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive batch size time evolving stochastic gradient descent for federated learning. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3610169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variance reduction has been shown to improve the performance of Stochastic Gradient Descent (SGD) in centralized machine learning. However, when it is extended to federated learning systems, many issues may arise, including (i) mega-batch size settings; (ii) additional noise introduced by the gradient difference between the current iteration and the snapshot point; and (iii) gradient (statistical) heterogeneity. In this paper, we propose a lightweight algorithm termed federated adaptive batch size time evolving variance reduction (FedATEVR) to tackle these issues, consisting of an adaptive batch size setting scheme and a time-evolving variance reduction gradient estimator. In particular, we use the historical gradient information to set an appropriate mega-batch size for each client, which can steadily accelerate the local SGD process and reduce the computation cost. The historical information involves both global and local gradient, which mitigates unstable varying in mega-batch size introduced by gradient heterogeneity among the clients. For each client, the gradient difference between the current iteration and the snapshot point is used to tune the time-evolving weight of the variance reduction term in the gradient estimator. This can avoid meaningless variance reduction caused by the out-of-date snapshot point gradient. We theoretically prove that our algorithm can achieve a linear speedup of of $\mathcal {O}(\frac{1}{\sqrt{SKT}})$ for non-convex objective functions under partial client participation. Extensive experiments demonstrate that our proposed method can achieve higher test accuracy than the baselines and decrease communication rounds greatly.},
  archive      = {J_TPAMI},
  author       = {Xuming An and Li Shen and Yong Luo and Han Hu and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3610169},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive batch size time evolving stochastic gradient descent for federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACLI: A CNN pruning framework leveraging adjacent convolutional layer interdependence and $\gamma$-weakly submodularity. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, convolutional neural network (CNN) pruning techniques often rely on manually crafted importance criteria and pruning structures. Due to their heuristic nature, these methods may lack generality, and their performance is not guaranteed. In this paper, we propose a theoretical framework to address this challenge by leveraging the concept of $\gamma$-weak submodularity, based on a new efficient importance function. By deriving an upper bound on the absolute error in the layer subsequent to the pruned layer, we formulate the importance function as a $\gamma$-weakly submodular function. This formulation enables the development of an easy-to-implement, low-complexity, and data-free oblivious algorithm for selecting filters to be removed from a convolutional layer. Extensive experiments show that our method outperforms state-of-the-art benchmark networks across various datasets, with a computational cost comparable to the simplest pruning techniques, such as $l_{2}$-norm pruning. Notably, the proposed method achieves an accuracy of 76.52%, compared to 75.15% for the overall best baseline, with a 25.5% reduction in network parameters. According to our proposed resource-efficiency metric for pruning methods, the ACLI approach demonstrates orders-of-magnitude higher efficiency than the other baselines, while maintaining competitive accuracy.},
  archive      = {J_TPAMI},
  author       = {S. Tofigh and M. Askarizadeh and M. Omair Ahmad and M.N.S. Swamy and KK Nguyen},
  doi          = {10.1109/TPAMI.2025.3610113},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ACLI: A CNN pruning framework leveraging adjacent convolutional layer interdependence and $\gamma$-weakly submodularity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting transferable adversarial images: Systemization, evaluation, and new insights. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable adversarial images raise critical security concerns for computer vision systems in real-world, blackbox attack scenarios. Although many transfer attacks have been proposed, existing research lacks a systematic and comprehensive evaluation. In this paper, we systemize transfer attacks into five categories around the general machine learning pipeline and provide the first comprehensive evaluation, with 23 representative attacks against 11 representative defenses, including the recent, transfer-oriented defense and the real-world Google Cloud Vision. In particular, we identify two main problems of existing evaluations: (1) for attack transferability, lack of intra-category analyses with fair hyperparameter settings, and (2) for attack stealthiness, lack of diverse measures. Our evaluation results validate that these problems have indeed caused misleading conclusions and missing points, and addressing them leads to new, consensuschallenging insights, such as (1) an early attack, DI, even outperforms all similar follow-up ones, (2) the state-of-the-art (whitebox) defense, DiffPure, is even vulnerable to (black-box) transfer attacks, and (3) even under the same Lp constraint, different attacks yield dramatically different stealthiness results regarding diverse imperceptibility metrics, finer-grained measures, and a user study. We hope that our analyses will serve as guidance on properly evaluating transferable adversarial images and advance the design of attacks and defenses.},
  archive      = {J_TPAMI},
  author       = {Zhengyu Zhao and Hanwei Zhang and Renjue Li and Ronan Sicre and Laurent Amsaleg and Michael Backes and Qi Li and Qian Wang and Chao Shen},
  doi          = {10.1109/TPAMI.2025.3610085},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting transferable adversarial images: Systemization, evaluation, and new insights},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative causality-driven network for graph multi-task learning. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) is a standard learning paradigm in machine learning. The central idea of MTL is to capture the shared knowledge among multiple tasks for mitigating the problem of data sparsity where the annotated samples for each task are quite limited. Recent studies indicate that graph multi-task learning (GMTL) yields the promising improvement over previous MTL methods. GMTL represents tasks on a task relation graph, and further leverages graph neural networks (GNNs) to learn complex task relationships. Although GMTL achieves the better performance, the construction of task relation graph heavily depends on simple heuristic tricks, which results in the existence of spurious task correlations and the absence of true edges between tasks with strong connections. This problem largely limits the effectiveness of GMTL. To this end, we propose the Generative Causality-driven Network (GCNet), a novel framework that progressively learns the causal structure between tasks to discover which tasks are beneficial to be jointly trained for improving generalization ability and model robustness. To be specific, in the feature space, GCNet first introduces a feature-level generator to generate the structure prior for reducing learning difficulty. Afterwards, GCNet develops a output-level generator which is parameterized as a new causal energy-based model (EBM) to refine the learned structure prior in the output space driven by causality. Benefiting from our proposed causal framework, we theoretically derive an intervention contrastive estimation for training this causal EBM efficiently. Experiments are conducted on multiple synthetic and real-world datasets. Extensive empirical results and model analyses demonstrate the superior performance of GCNet over several competitive MTL baselines.},
  archive      = {J_TPAMI},
  author       = {Xixun Lin and Qing Yu and Yanan Cao and Lixin Zou and Chuan Zhou and Jia Wu and Chenliang Li and Peng Zhang and Shirui Pan},
  doi          = {10.1109/TPAMI.2025.3610096},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generative causality-driven network for graph multi-task learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Step-wise distribution-aligned style prompt tuning for source-free cross-domain few-shot learning. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cross-domain few-shot learning (CDFSL) methods, which develop training strategies in the source domain to enhance model transferability, face challenges when applied to large-scale pre-trained models (LMs), as their source domains and training strategies are not accessible. Besides, fine-tuning LMs specifically for CDFSL requires substantial computational resources, which limits their practicality. Therefore, this paper investigates the source-free CDFSL (SF-CDFSL) problem to solve the few-shot learning (FSL) task in target domain using only a pre-trained model and a few target samples, without requiring source data or training strategies. However, the inaccessibility of source data prevents explicitly reducing the domain gaps between the source and target. To tackle this challenge, this paper proposes a novel approach, Step-wise Distribution-aligned Style Prompt Tuning (StepSPT), to implicitly narrow the domain gaps from the perspective of prediction distribution optimization. StepSPT initially proposes a style prompt that adjusts the target samples to mirror the expected distribution. Furthermore, StepSPT tunes the style prompt and classifier by exploring a dual-phase optimization process (external and internal processes). In the external process, a step-wise distribution alignment strategy is introduced to tune the proposed style prompt by factorizing the prediction distribution optimization problem into the multi-step distribution alignment problem. In the internal process, the classifier is updated via standard cross-entropy loss. Evaluation on 5 datasets illustrates the superiority of StepSPT over existing prompt tuning-based methods and state-of-the-art methods (SOTAs). Furthermore, ablation studies and performance analyzes highlight the efficacy of StepSPT. The code will be made public at https://github.com/xuhuali-mxj/StepSPT.},
  archive      = {J_TPAMI},
  author       = {Huali Xu and Li Liu and Tianpeng Liu and Shuaifeng Zhi and Shuzhou Sun and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2025.3610039},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Step-wise distribution-aligned style prompt tuning for source-free cross-domain few-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I&S-ViT: An inclusive & stable method for post-training ViTs quantization. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3610466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Albeit the scalable performance of vision transformers (ViTs), the dense computational costs undermine their position in industrial applications. Post-training quantization (PTQ), tuning ViTs with a tiny dataset and running in a low-bit format, well addresses the cost issue but unluckily bears more performance drops in lower-bit cases. In this paper, we introduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an inclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of ViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for post-Softmax activations; (2) Rugged and magnified loss landscape in coarse-grained quantization granularity for post-LayerNorm activations. Then, I&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2 quantizer (SULQ) that incorporates a shift mechanism followed by uniform quantization to achieve both an inclusive domain representation and accurate distribution approximation; (2) A three-stage smooth optimization strategy (SOS) that amalgamates the strengths of channel-wise and layer-wise quantization to enable stable learning. Comprehensive evaluations across diverse vision tasks validate I&S-ViT's superiority over existing PTQ of ViTs methods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the performance of W3A3 ViT-B by an impressive 50.68%. Our code is available at https://github.com/zysxmu/IaS-ViT.},
  archive      = {J_TPAMI},
  author       = {Yunshan Zhong and Jiawei Hu and Mingbao Lin and Mengzhao Chen and Rongrong Ji},
  doi          = {10.1109/TPAMI.2025.3610466},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {I&S-ViT: An inclusive & stable method for post-training ViTs quantization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFA image denoising using physics-based noise model and noise-decoupled network. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3610243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral filter array (MSFA) camera is increasingly used due to its compact size and fast capturing speed. However, because of its narrow-band property, it often suffers from the light-deficient problem, and images captured are easily overwhelmed by noise. As a type of commonly used denoising method, neural networks have shown their power to achieve satisfactory denoising results. However, their performance highly depends on high-quality noisy-clean image pairs. For the task of MSFA image denoising, there is currently neither a paired real dataset nor an accurate noise model capable of generating realistic noisy images. To this end, we present a physics-based noise model that is capable to match the real noise distribution and synthesize realistic noisy images. In our noise model, those different types of noise can be divided into SimpleDist component and ComplexDist component. The former contains all the types of noise that can be described using a simple probability distribution like Gaussian or Poisson distribution, and the latter contains the complicated color bias noise that cannot be modeled using a simple probability distribution. Besides, we design a noise-decoupled network consisting of a SimpleDist noise removal network (SNRNet) and a ComplexDist noise removal network (CNRNet) to sequentially remove each component. Moreover, according to the non-uniformity of color bias noise in our noise model, we introduce a learnable position embedding in CNRNet to indicate the position information. To verify the effectiveness of our physics-based noise model and noise-decoupled network, we collect a real MSFA denoising dataset with paired long-exposure clean images and short-exposure noisy images. Experiments are conducted to prove that the network trained using synthetic data generated by our noise model performs as well as trained using paired real data, and our noise-decoupled network outperforms other state-of-the-art denoising methods. The project page is avaliable at https://github.com/ying-fu/msfa denoising.},
  archive      = {J_TPAMI},
  author       = {Yuqi Jiang and Ying Fu and Qiankun Liu and Jun Zhang},
  doi          = {10.1109/TPAMI.2025.3610243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MSFA image denoising using physics-based noise model and noise-decoupled network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D hand pose estimation via articulated anchor-to-joint 3D local regressors. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3609907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to address monocular 3D hand pose estimation from a single RGB or depth image via articulated anchor-to-joint 3D local regressors, in form of A2J-Transformer+. The key idea is to make the local regressors (i.e., anchor points) in 3D space be aware of hand's local fine details and global articulated context jointly, to facilitate predicting their 3D offsets toward hand joints with linear weighted aggregation for joint localization. Our intuition is that, local fine details help to estimate accurate offset but may suffer from the issues including serious occlusion, confusing similar patterns, and overfitting risk. On the other hand, hand's global articulated context can essentially provide additional descriptive clues and constraints to alleviate these issues. To set anchor points adaptively in 3D space, A2J-Transformer+ runs in a 2-stage manner. At the first stage, since the input modality property anchor points distribute more densely on X-Y plane, it leads to lower prediction accuracy along Z direction compared with those in the X and Y directions. To alleviate this, at the second stage anchor points are set near the joints yielded by the first stage evenly along X, Y, and Z directions. This treatment brings two main advantages: (1) balancing the prediction accuracy along X, Y, and Z directions, and (2) ensuring the anchor-joint offsets are of small values relatively easy to estimate. Wide-range experiments on three RGB hand datasets (InterHand2.6M, HO-3D V2 and RHP) and three depth hand datasets (NYU, ICVL and HANDS 2017) verify A2J-Transformer+'s superiority and generalization ability for different modalities (i.e., RGB and depth) and hand cases (i.e., single hand, interacting hands, and hand-object interaction), even outperforming model-based manners. The test on ITOP dataset reveals that, A2J-Transformer+ can also be applied to 3D human pose estimation task. The source code and supporting material will be released upon acceptance.},
  archive      = {J_TPAMI},
  author       = {Changlong Jiang and Yang Xiao and Jinghong Zheng and Haohong Kuang and Cunlin Wu and Mingyang Zhang and Zhiguo Cao and Min Du and Joey Tianyi Zhou and Junsong Yuan},
  doi          = {10.1109/TPAMI.2025.3609907},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D hand pose estimation via articulated anchor-to-joint 3D local regressors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient nearest neighbor search using dynamic programming. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a collection of points in $\mathbb {R}^{3}$, KD-Tree and R-Tree are well-known nearest neighbor search (NNS) algorithms that rely on spatial partitioning and indexing techniques. However, when the query point is far from the data points or the data points inherently represent a 2-manifold surface, their query performance may degrade. To address this, we propose a novel dynamic programming technique that precomputes a Directed Acyclic Graph (DAG) to encode the proximity structure between data points. More specifically, the DAG captures how the proximity structure evolves during the incremental construction of the Voronoi diagram of the data points. Experimental results demonstrate that our method achieves a speed increase of 1-10x. Furthermore, our algorithm demonstrates significant practical value in diverse applications. We validated its effectiveness through extensive testing in four key applications: Point-to-Mesh Distance Queries, Iterative Closest Point (ICP) Registration, Density Peak Clustering, and Point-to-Segments Distance Queries. A particularly notable feature of our approach is its unique ability to efficiently identify the nearest neighbor among the first $k$ points in the point cloud, a capability that enables substantial acceleration in low-dimensional applications like Density Peak Clustering. As a natural extension of our incremental construction process, our method can also be readily adapted for farthest-point sampling tasks. These experimental results across multiple domains underscore the broad applicability and practical importance of our approach.},
  archive      = {J_TPAMI},
  author       = {Pengfei Wang and Jiantao Song and Shiqing Xin and Shuangmin Chen and Changhe Tu and Wenping Wang and Jiaye Wang},
  doi          = {10.1109/TPAMI.2025.3610211},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient nearest neighbor search using dynamic programming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-distributionally robust data-free meta-learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3609625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-Free Meta-Learning (DFML) aims to enable efficient learning of unseen few-shot tasks, by meta-learning from multiple pre-trained models without accessing their original training data. While existing DFML methods typically generate synthetic data from these models to perform meta-learning, a comprehensive analysis of DFML's robustness-particularly its failure modes and vulnerability to potential attacks-remains notably absent. Such an analysis is crucial as algorithms often operate in complex and uncertain real-world environments. This paper fills this significant gap by systematically investigating the robustness of DFML, identifying two critical but previously overlooked vulnerabilities: Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). TDS refers to the sequential shifts in the evolving task distribution, leading to the catastrophic forgetting of previously learned meta-knowledge. TDC exposes a security flaw of DFML, revealing its susceptibility to attacks when the pre-trained model pool includes untrustworthy models that deceptively claim to be beneficial but are actually harmful. To mitigate these vulnerabilities, we propose a trustworthy DFML framework comprising three components: synthetic task reconstruction, meta-learning with task memory interpolation, and automatic model selection. Specifically, utilizing model inversion techniques, we reconstruct synthetic tasks from multiple pre-trained models to perform meta-learning. To prevent forgetting, we introduce a strategy to replay interpolated historical tasks to efficiently recall previous meta-knowledge. Furthermore, our framework seamlessly incorporates an automatic model selection mechanism to automatically filter out untrustworthy models during the meta-learning process. Extensive experiments across various datasets with two types of untrustworthy models confirm the superiority of our method in significantly enhancing the robustness of DFML. Code is available at https://github.com/Egg-Hu/Trustworthy-DFML.},
  archive      = {J_TPAMI},
  author       = {Zixuan Hu and Yongxian Wei and Li Shen and Zhenyi Wang and Baoyuan Wu and Chun Yuan and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3609625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Task-distributionally robust data-free meta-learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNNTracker: Online high-speed multi-object tracking with spike camera. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is crucial for applications such as autonomous driving and robotics, yet traditional image-based methods struggle in high-speed scenarios due to motion blur and temporal gaps caused by low frame rates. Spike cameras, with their ability to continuously record spatiotemporal signals, overcome these limitations. However, existing spike-based methods often rely on intermediate image reconstruction or discrete clustering, which limits their real-time performance and temporal continuity. To address this, we propose SNNTracker, the first fully spiking neural network (SNN)-based MOT algorithm tailored for spike cameras. SNNTracker integrates a dynamic neural field (DNF)-based attention mechanism for target detection and a winner-take-all (WTA)-based tracking module with online spike-timing-dependent plasticity (STDP) for adaptive learning of object trajectories. By directly processing spike streams without reconstruction, SNNTracker reduces latency, computational overhead, and dependency on image quality, making it ideal for ultra-high-speed environments. It maintains robust, continuous tracking even under occlusions, severe lighting variations, or temporary object disappearance, by leveraging SNN-estimated motion predictions and long-term online clustering. We construct three types of spike-camera MOT datasets covering dense and sparse annotations across diverse real-world scenarios, including camera ego-motion, deformable and ultra-fast motion (up to 2600 RPM), occlusion, indoor/outdoor lighting changes, and low-visibility tracking. Extensive experiments demonstrate that SNNTracker consistently outperforms state-of-the-art MOT methods—both ANN- and SNN-based—achieving MOTA scores above 96% and up to 100% in many sequences. Our results highlight the advantages of spike-driven SNNs for low-latency, high-speed, and label-free multi-object tracking, advancing neuromorphic vision for real-time perception.},
  archive      = {J_TPAMI},
  author       = {Yajing Zheng and Chengen Li and Jiyuan Zhang and Zhaofei Yu and Tiejun Huang},
  doi          = {10.1109/TPAMI.2025.3610696},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SNNTracker: Online high-speed multi-object tracking with spike camera},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PlaneRecTR++: Unified query learning for joint 3D planar reconstruction and pose estimation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3610500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D plane reconstruction from images can usually be divided into several sub-tasks of plane detection, segmentation, parameters regression and possibly depth prediction for per-frame, along with plane correspondence and relative camera pose estimation between frames. Previous works tend to divide and conquer these sub-tasks with distinct network modules, overall formulated by a two-stage paradigm. With an initial camera pose and per-frame plane predictions provided from the first stage, exclusively designed modules, potentially relying on extra plane correspondence labelling, are applied to merge multi-view plane entities and produce 6DoF camera pose. As none of existing works manage to integrate above closely related sub-tasks into a unified framework but treat them separately and sequentially, we suspect it potentially as a main source of performance limitation for existing approaches. Motivated by this finding and the success of query-based learning in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR++, a Transformer-based architecture, which for the first time unifies all sub-tasks related to multi-view reconstruction and pose estimation with a compact single-stage model, refraining from initial pose estimation and plane correspondence supervision. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across sub-tasks, obtaining a new state-of-the-art performance on public ScanNetv1, ScanNetv2, NYUv2-Plane, and MatterPort3D datasets.},
  archive      = {J_TPAMI},
  author       = {Jingjia Shi and Shuaifeng Zhi and Kai Xu},
  doi          = {10.1109/TPAMI.2025.3610500},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PlaneRecTR++: Unified query learning for joint 3D planar reconstruction and pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StyleShot: A snapshot on any style. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Style Transfer aims to replicate the style of a reference image based on the content from a text description or another image. With the significant advancements in image generation through diffusion models, recent studies have attempted to either fine-tuning embeddings to learn the single style or utilizing the pre-trained CLIP image encoder to extract style representations. However, style-tuning requires substantial computational resources and the pre-trained CLIP image encoder is trained for semantic understanding rather than for style representation. To address these challenges, we introduce a style-aware encoder and a well-organized style dataset called StyleGallery to learn a good style representation that is crucial and sufficient for generalized style transfer without test-time tuning. With dedicated design for style learning, this style-aware encoder is trained to extract expressive style representation from multi-level patches with decoupling training strategy, and StyleGallery enables the generalization ability. Moreover, we employ a content extraction and content-fusion encoder to enhance image-driven style transfer. We highlight that, our approach, named StyleShot, is simple yet effective in mimicking various desired styles, i.e., 3D, flat, abstract or even fine-grained styles, without test-time tuning. Rigorous experiments validate that, StyleShot achieves superior performance across a wide range of styles compared to existing state-of-the-art text- and image-driven methods.},
  archive      = {J_TPAMI},
  author       = {Junyao Gao and Yanan Sun and Yanchen Liu and Yinhao Tang and Yanhong Zeng and Ding Qi and Kai Chen and Cairong Zhao},
  doi          = {10.1109/TPAMI.2025.3610614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StyleShot: A snapshot on any style},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LVOS: A benchmark for large-scale long-term video object segmentation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3611020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation (VOS) aims to distinguish and track target objects in a video. Despite the excellent performance achieved by off-the-shelf VOS models, part of the existing VOS benchmarks mainly focuses on short-term videos, where objects remain visible most of the time. However, these benchmarks may not fully capture challenges encountered in practical applications, and the absence of long-term datasets restricts further investigation of VOS in realistic scenarios. Thus, we propose a novel benchmark named LVOS, comprising 720 videos with 296,401 frames and 407,945 high-quality annotations. Videos in LVOS last 1.14 minutes on average. Each video includes various attributes, especially challenges encountered in the wild, such as long-term reappearing and cross-temporal similar objects. Compared to previous benchmarks, our LVOS better reflects VOS models' performance in real scenarios. Based on LVOS, we evaluate 15 existing VOS models under 3 different settings and conduct a comprehensive analysis. On LVOS, these models suffer a large performance drop, highlighting the challenge of achieving precise tracking and segmentation in real-world scenarios. Attribute-based analysis indicates that one of the significant factors contributing to accuracy decline is the increased video length, interacting with complex challenges such as long-term reappearance, cross-temporal confusion, and occlusion, which emphasize LVOS's crucial role. We hope our LVOS can advance development of VOS in real scenes.},
  archive      = {J_TPAMI},
  author       = {Lingyi Hong and Zhongying Liu and Wenchao Chen and Chenzhi Tan and Yuang Feng and Xinyu Zhou and Pinxue Guo and Jinglun Li and Zhaoyu Chen and Shuyong Gao and Wei Zhang and Wenqiang Zhang},
  doi          = {10.1109/TPAMI.2025.3611020},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LVOS: A benchmark for large-scale long-term video object segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SS-NeRF: Physically based sparse spectral rendering with neural radiance field. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3611376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose SS-NeRF, the end-to end Neural Radiance Field (NeRF)-based architectures for highquality physically based rendering with sparse inputs. We modify the classical spectral rendering into two main steps, 1) the generation of a series of spectrum maps spanning different wavelengths, 2) the combination of these spectrum maps for the RGB output. The proposed architecture follows these two steps through the proposed multi-layer perceptron (MLP)-based architecture (SpectralMLP) and spectrum attention UNet (SAUNet). Given the ray origin and the ray direction, the SpectralMLP constructs the spectral radiance field to obtain spectrum maps of novel views, which are then sent to the SAUNet to produce RGB images of white-light illumination. Applying NeRF to build up the spectral rendering is a more physically-based way from the perspective of ray-tracing. Further, the spectral radiance fields decompose difficult scenes and improve the performance of NeRF-based methods. Previous baseline, such as SpectralNeRF, outperforms recent methods in synthesizing novel views but requires relatively dense viewpoints for accurate scene reconstruction. To tackle this, we propose SS-NeRF to enhance the detail of scene representation with sparse inputs. In SS-NeRF, we first design the depth-aware continuity to optimize the reconstruction based on single-view depth predictions. Then, the geometric-projected consistency is introduced to optimize the multi-view geometry alignment. Additionally, we introduce a superpixel-aligned consistency to ensure that the average color within each superpixel region remains consistent. Comprehensive experimental results demonstrate that the proposed method is superior to recent state-ofthe-art methods when synthesizing new views on both synthetic and real-world datasets.},
  archive      = {J_TPAMI},
  author       = {Ru Li and Jia Liu and Guanghui Liu and Shengping Zhang and Bing Zeng and Shuaicheng Liu},
  doi          = {10.1109/TPAMI.2025.3611376},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SS-NeRF: Physically based sparse spectral rendering with neural radiance field},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defenses in adversarial machine learning: A systematic survey from the lifecycle perspective. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3611340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial phenomena have been widely observed in machine learning (ML) systems, especially those using deep neural networks. These phenomena describe situations where ML systems may produce predictions that are inconsistent and incomprehensible to humans in certain specific cases. Such behavior poses a serious security threat to the practical application of ML systems. To exploit this vulnerability, several advanced attack paradigms have been developed, mainly including backdoor attacks, weight attacks, and adversarial examples. For each individual attack paradigm, various defense mechanisms have been proposed to enhance the robustness of models against the corresponding attacks. However, due to the independence and diversity of these defense paradigms, it is challenging to assess the overall robustness of an ML system against different attack paradigms. This survey aims to provide a systematic review of all existing defense paradigms from a unified lifecycle perspective. Specifically, we decompose a complete ML system into five stages: pre-training, training, post-training, deployment, and inference. We then present a clear taxonomy to categorize representative defense methods at each stage. The unified perspective and taxonomy not only help us analyze defense mechanisms but also enable us to understand the connections and differences among different defense paradigms. It inspires future research to develop more advanced and comprehensive defense strategies.},
  archive      = {J_TPAMI},
  author       = {Baoyuan Wu and Mingli Zhu and Meixi Zheng and Zihao Zhu and Shaokui Wei and Mingda Zhang and Hongrui Chen and Danni Yuan and Li Liu and Qingshan Liu},
  doi          = {10.1109/TPAMI.2025.3611340},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Defenses in adversarial machine learning: A systematic survey from the lifecycle perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting deformable convolution on graphs: Large-range modeling and robustness. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3611386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolution Networks (GCNs) have achieved remarkable success in representation of structured graph data. As we know that traditional GCNs are generally defined on the fixed first-order neighborhood receptive field which makes them be incapable to capture the long-range dependencies between distant nodes and also vulnerable to graph attacks and noises. To address these limitations, we revisit deformable convolution on graphs and propose a novel deformable graph convolution, termed Neighborhood-Deformable Graph Convolution (NDGC). The core of NDGC is to explicitly achieve the deformable convolution on graphs by introducing virtual neighbors which encode large-range information via the offsetting and interpolation function. That is, the introduced virtual neighbors can provide a larger receptive field with deformable receptive shape for graph convolution definition. Also, NDGC conducts message aggregation on the deformable virtual neighbors which thus performs more robustly w.r.t. graph attacks and noises. In particular, NDGC provides a general neighborhood deformable scheme, seamlessly integrating with many graph convolution definitions to derive their deformable variants. Experimental results validate the effectiveness and advantages of the proposed NDGC networks on several graph learning tasks.},
  archive      = {J_TPAMI},
  author       = {Ziyan Zhang and Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3611386},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting deformable convolution on graphs: Large-range modeling and robustness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep lookup network. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3605660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires more energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).},
  archive      = {J_TPAMI},
  author       = {Yulan Guo and Longguang Wang and Wendong Mao and Xiaoyu Dong and Yingqian Wang and Li Liu and Wei An},
  doi          = {10.1109/TPAMI.2025.3605660},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep lookup network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SOOD++: Leveraging unlabeled data to boost oriented object detection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3611519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised object detection (SSOD), leveraging unlabeled data to boost object detectors, has become a hot topic recently. However, existing SSOD approaches mainly focus on horizontal objects, leaving oriented objects common in aerial images unexplored. At the same time, the annotation cost of oriented objects is significantly higher than that of their horizontal counterparts (an approximate 36.5% increase in costs). Therefore, in this paper, we propose a simple yet effective Semi-supervised Oriented Object Detection method termed SOOD++. Specifically, we observe that objects from aerial images usually have arbitrary orientations, small scales, and dense distribution, which inspires the following core designs: a Simple Instance-aware Dense Sampling (SIDS) strategy is used to generate comprehensive dense pseudo-labels; the Geometry-aware Adaptive Weighting (GAW) loss dynamically modulates the importance of each pair between pseudo-label and corresponding prediction by leveraging the intricate geometric information of aerial objects; we treat aerial images as global layouts and explicitly build the many-to-many relationship between the sets of pseudo-labels and predictions via the proposed Noise-driven Global Consistency (NGC). Extensive experiments conducted on various oriented object datasets under various labeled settings demonstrate the effectiveness of our method. For example, on the DOTA-V2.0/DOTA-V1.5 benchmark, the proposed method outperforms previous state-of-the-art (SOTA) by a large margin (+2.90/2.14, +2.16/2.18, and +2.66/2.32) mAP under 10%, 20%, and 30% labeled data settings, respectively, with single-scale training and testing. More importantly, it still improves upon a strong supervised baseline with 70.66 mAP, trained using the full DOTA-V1.5 train-val set, by +1.82 mAP, resulting in a 72.48 mAP, pushing the new state-of-the-art. Moreover, our method demonstrates stable generalization ability across different oriented detectors, even for multi-view oriented 3D object detectors. The code will be made available.},
  archive      = {J_TPAMI},
  author       = {Dingkang Liang and Wei Hua and Chunsheng Shi and Zhikang Zou and Xiaoqing Ye and Xiang Bai},
  doi          = {10.1109/TPAMI.2025.3611519},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SOOD++: Leveraging unlabeled data to boost oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway-aware multimodal transformer (PAMT): Integrating pathological image and gene expression for interpretable cancer survival analysis. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3611531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multimodal data of pathological image and gene expression for cancer survival analysis can achieve better results than using a single modality. However, existing multimodal learning methods ignore fine-grained interactions between both modalities, especially the interactions between biological pathways and pathological image patches. In this article, we propose a novel Pathway-Aware Multimodal Transformer (PAMT) framework for interpretable cancer survival analysis. Specifically, the PAMT learns fine-grained modality interaction through three stages: (1) In the intra-modal pathway-pathway / patch-patch interaction stage, we use the Transformer model to perform intra-modal information interaction; (2) In the inter-modal pathway-patch alignment stage, we introduce a novel label-free contrastive loss to aligns semantic information between different modalities so that the features of the two modalities are mapped to the same semantic space; and (3) In the inter-modal pathway-patch fusion stage, to model the medical prior knowledge of “genotype determines phenotype”, we propose a pathway-to-patch cross fusion module to perform inter-modal information interaction under the guidance of pathway prior. In addition, the inter-modal cross fusion module of PAMT endows good interpretability, helping a pathologist to screen which pathway plays a key role, to locate where on whole slide image (WSI) are affected by the pathway, and to mine prognosis-relevant pathology image patterns. Experimental results based on three datasets of bladder urothelial carcinoma, lung squamous cell carcinoma, and lung adenocarcinoma demonstrate that the proposed framework significantly outperforms the state-of-the-art methods. Finally, based on the PAMT model, we develop a website that directly visualizes the impact of 186 pathways on all areas of WSI, available at http://222.128.10.254:18822/#/.},
  archive      = {J_TPAMI},
  author       = {Rui Yan and Xueyuan Zhang and Zihang Jiang and Baizhi Wang and Xiuwu Bian and Fei Ren and S. Kevin Zhou},
  doi          = {10.1109/TPAMI.2025.3611531},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pathway-aware multimodal transformer (PAMT): Integrating pathological image and gene expression for interpretable cancer survival analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-guidance: Boosting flow and diffusion generation on their own. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3611831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper guidance strategies are essential to achieve high-quality generation results without retraining diffusion and flow-based text-to-image models. Existing guidance either requires specific training or strong inductive biases of diffusion model networks, which potentially limits their ability and application scope. Motivated by the observation that artifact outliers can be detected by a significant decline in the density from a noisier to a cleaner noise level, we propose Self-Guidance (SG), which can significantly improve the quality of the generated image by suppressing the generation of low-quality samples. The biggest difference from existing guidance is that SG only relies on the sampling score function of the original diffusion or flow model at different noise levels, with no need for any tricky and expensive guidance-specific training. This makes SG highly flexible to be used in a plug-and-play manner by any diffusion or flow models. We also introduce an efficient variant of SG, named SG-prev, which reuses the output from the immediately previous diffusion step to avoid additional forward passes of the diffusion network. We conduct extensive experiments on text-to-image and text-to-video generation with different architectures, including UNet and transformer models. With open-sourced diffusion models such as Stable Diffusion 3.5 and FLUX, SG exceeds existing algorithms on multiple metrics, including both FID and Human Preference Score. SG-prev also achieves strong results over both the baseline and the SG, with 50 percent more efficiency. Moreover, we find that SG and SG-prev both have a surprisingly positive effect on the generation of physiologically correct human body structures such as hands, faces, and arms, showing their ability to eliminate human body artifacts with minimal efforts. We have released our code at https://github.com/maple-research-lab/Self-Guidance.},
  archive      = {J_TPAMI},
  author       = {Tiancheng Li and Weijian Luo and Zhiyang Chen and Liyuan Ma and Guo-Jun Qi},
  doi          = {10.1109/TPAMI.2025.3611831},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-guidance: Boosting flow and diffusion generation on their own},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-level prototypical federated learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3612302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing concerns about privacy and data regulations, federated learning (FL) has been emerging as a solution to train machine learning models collaboratively with non-exchangeable data from multiple clients. As a result of data locality, data is usually not identically or independently (non-IID) distributed across clients, and the non-IID property has long been the key challenge in FL. Furthermore, in real-world cross-silo scenarios, it is ubiquitous that clients are organizations owning private data from multiple domains internally, which exacerbates the non-IID issue. For example, in healthcare applications, each client (hospital) gathers data from patients with heterogeneous demographics. While previous works have made efforts to address the non-IID challenge across clients by assuming various relations among client-level data distributions and enabling personalized models at the client level, they ignore the internal data heterogeneity within each client or require explicit data domain indicators, which are hardly accessible in real-world data. Here, we propose (SL-PFL) to bridge the gap. SL-PFL incorporates prototypical learning under the FL framework and provides a fine-grained personalized model for each data sample instead of learning one uniform model for all samples of each client. Meanwhile, it can be trained using data without ground-truth domain indicators. Experimental results demonstrate that our proposed method with sample-level personalized models outperforms existing FL methods with a global model or client-level personalized models on various real-world regression and classification tasks from weather, computer vision, and healthcare applications.},
  archive      = {J_TPAMI},
  author       = {Chuizheng Meng and Jianke Yang and Hao Niu and Guillaume Habault and Roberto Legaspi and Shinya Wada and Chihiro Ono and Yan Liu},
  doi          = {10.1109/TPAMI.2025.3612302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sample-level prototypical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geo-NI: Geometry-aware neural interpolation for light field rendering. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3594705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel Geometry-aware Neural Interpolation (Geo-NI) framework for light field rendering. Previous learning-based approaches either perform direct interpolation via neural networks, which we dubbed Neural Interpolation (NI), or explore scene geometry for novel view synthesis, also known as Depth Image-Based Rendering (DIBR). Both kinds of approaches have their own strengths and weaknesses in addressing non-Lambert effect and large disparity problems. In this paper, we incorporate the ideas behind these two kinds of approaches by launching the NI within a specific DIBR pipeline. Specifically, a DIBR network in the proposed Geo-NI serves to construct a novel reconstruction cost volume for neural interpolated light fields sheared by different depth hypotheses. The reconstruction cost can be interpreted as an indicator reflecting the reconstruction quality under a certain depth hypothesis, and is further applied to guide the rendering of the final high angular resolution light field. To implement the Geo-NI framework more practically, we further propose an efficient modeling strategy to encode high-dimensional cost volumes using a lower-dimension network. By combining the superiorities of NI and DIBR, the proposed Geo-NI is able to render views with large disparities with the help of scene geometry while also reconstructing the non-Lambertian effect when depth is prone to be ambiguous. Extensive experiments on various datasets demonstrate the superior performance of the proposed geometry-aware light field rendering framework.},
  archive      = {J_TPAMI},
  author       = {Gaochang Wu and Yuemei Zhou and Lu Fang and Yebin Liu and Tianyou Chai},
  doi          = {10.1109/TPAMI.2025.3594705},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geo-NI: Geometry-aware neural interpolation for light field rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning an adaptive sparse transformer for efficient image restoration. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3594910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based approaches have shown promising performance in image restoration tasks due to their ability to model long-range dependencies, which are essential for recovering clear images. Although various efficient attention mechanisms have been proposed to address the intensive computational loads of transformers, they often suffer from redundant information and noisy interactions from irrelevant regions, as they consider all available tokens. In this work, we propose an Adaptive Sparse Transformer (AST-v2) to mitigate these issues by reducing noisy interactions in irrelevant areas and removing feature redundancy along channel dimension. AST-v2 incorporates two core components: an Adaptive Sparse Self-Attention (ASSA) block and a Feature Refinement Feed-forward Network (FRFN). ASSA adopts a dual-branch design, where the sparse branch guides the modulation of standard dense attention weights. This paradigm reduces the negative impact of irrelevant token interactions while preserving the important ones. Meanwhile, FRFN utilizes an enhance-and-ease scheme to eliminate feature redundancy across channels, enhancing the restoration of clear images. Experimental results on commonly used benchmarks show the competitive performance of our method for 6 restoration tasks, including rain streak removal, haze removal, shadow removal, snow removal, blur removal, and low-light enhancement. The code is available in the supplementary materials.},
  archive      = {J_TPAMI},
  author       = {Shihao Zhou and Jinshan Pan and Jufeng Yang},
  doi          = {10.1109/TPAMI.2025.3594910},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning an adaptive sparse transformer for efficient image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-efficient fine-tuning in spectral domain for point cloud learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3594749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, leveraging pre-training techniques to enhance point cloud models has become a prominent research topic. However, existing approaches typically require full fine-tuning of pre-trained models to achieve satisfactory performance on downstream tasks, which is storage-intensive and computationally demanding. To address this issue, we propose a novel Parameter-Efficient Fine-Tuning (PEFT) method for point cloud, called PointGST (Point cloud Graph Spectral Tuning). PointGST freezes the pre-trained model and introduces a lightweight, trainable Point Cloud Spectral Adapter (PCSA) for fine-tuning parameters in the spectral domain. The core idea is built on two observations: 1) The inner tokens from frozen models might present confusion in the spatial domain; 2) Task-specific intrinsic information is important for transferring the general knowledge to the downstream task. Specifically, PointGST transfers the point tokens from the spatial domain to the spectral domain, effectively de-correlating confusion among tokens by using orthogonal components for separation. Moreover, the generated spectral basis involves intrinsic information about the downstream point clouds, enabling more targeted tuning. As a result, PointGST facilitates the efficient transfer of general knowledge to downstream tasks while significantly reducing training costs. Extensive experiments on challenging point cloud datasets across various tasks demonstrate that PointGST not only outperforms its fully fine-tuning counterpart but also significantly reduces trainable parameters, making it a promising solution for efficient point cloud learning. Moreover, it achieves superior accuracies of 99.48%, 97.76%, and 96.18% on the ScanObjNN OBJ_BG, OBJ_ONLY, and PB_T50_RS datasets, respectively, establishing a new state-of-the-art, while using only 0.67% of the trainable parameters.},
  archive      = {J_TPAMI},
  author       = {Dingkang Liang and Tianrui Feng and Xin Zhou and Yumeng Zhang and Zhikang Zou and Xiang Bai},
  doi          = {10.1109/TPAMI.2025.3594749},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parameter-efficient fine-tuning in spectral domain for point cloud learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards few-shot learning in the open world: A review and beyond. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3594686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human intelligence is characterized by our ability to absorb and apply knowledge from the world around us, especially in rapidly acquiring new concepts from minimal examples, underpinned by prior knowledge. Few-shot learning (FSL) aims to mimic this capacity by enabling significant generalizations and transferability. However, traditional FSL frameworks often rely on assumptions of clean, complete, and static data, conditions that are seldom met in real-world environments. Such assumptions falter in the inherently uncertain, incomplete, and dynamic contexts of the open world. This paper presents a comprehensive review of recent advancements designed to adapt FSL to open-world environments. We categorize existing methods into three distinct types of FSL in the open world: those involving varying instances, varying classes, and varying distributions. Each category is discussed in terms of its specific challenges and methods, as well as its strengths and weaknesses. We standardize experimental settings and metric benchmarks across scenarios and provide a comparative analysis of the performance of various methods. In conclusion, we outline potential future research directions for this evolving field. It is our hope that this review will catalyze further development of effective solutions to these complex challenges, thereby advancing the field of artificial intelligence.},
  archive      = {J_TPAMI},
  author       = {Hui Xue and Yuexuan An and Yongchun Qin and Wenqian Li and Yixin Wu and Yongjuan Che and Pengfei Fang and Min-Ling Zhang},
  doi          = {10.1109/TPAMI.2025.3594686},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards few-shot learning in the open world: A review and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating prior errors in causal structure learning: A resilient approach via bayesian networks. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal structure learning (CSL), a prominent technique for encoding cause-and-effect relationships among variables, through Bayesian Networks (BNs). Although recovering causal structure solely from data is a challenge, the integration of prior knowledge, revealing partial structural truth, can markedly enhance learning quality. However, current methods based on prior knowledge exhibit limited resilience to errors in the prior, with hard constraint methods disregarding priors entirely, and soft constraints accepting priors based on a predetermined confidence level, which may require expert intervention. To address this issue, we propose a strategy resilient to edge-level prior errors for CSL, thereby minimizing human intervention. We classify prior errors into different types and provide their theoretical impact on the Structural Hamming Distance (SHD) under the presumption of sufficient data. Intriguingly, we discover and prove that the strong hazard of prior errors is associated with a unique acyclic closed structure, defined as “ quasi-circle”. Leveraging this insight, a post-hoc strategy is employed to identify the prior errors by its impact on the increment of “ quasi-circles”. Through empirical evaluation on both real and synthetic datasets, we demonstrate our strategy's robustness against prior errors. Specifically, we highlight its substantial ability to resist order-reversed errors while maintaining the majority of correct prior.},
  archive      = {J_TPAMI},
  author       = {Lyuzhou Chen and Taiyu Ban and Xiangyu Wang and Derui Lyu and Huanhuan Chen},
  doi          = {10.1109/TPAMI.2025.3594755},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mitigating prior errors in causal structure learning: A resilient approach via bayesian networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing 3D object detection with depth-aware spatial knowledge distillation. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3594805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D object detection from images can be hindered by inherent depth ambiguity. While knowledge distillation (KD) from privileged sensors such as LiDAR offers a promising direction, it often suffers from a critical cross-sensor domain gap. To address this, we introduce DK3D, a novel depth-aware knowledge distillation framework for 3D detection. Our core strategy involves providing the teacher with privileged ground-truth depth during training. This directly avoids the feature representation mismatch and subsequent inefficient knowledge transfer required when distilling from a LiDAR teacher (sparse, geometric) to a camera-based student (dense, semantic). DK3D introduces specialized modules tailored for two primary student paradigms. For depth-assisted models, we employ a channel-wise projection layer (CPL) and an adversarial scoring block (ASB) to align intermediate features at both the pixel and distribution levels. For depth-independent models, a novel vision-depth association module allows the student to implicitly reason about geometry by fusing depth cues with visual features. Both approaches are further enhanced by target-aware spatial response distillation, which captures complex inter-object spatial relationships. Extensive experiments on the KITTI and nuScenes benchmarks demonstrate that DK3D significantly improves performance for both monocular and multi-view 3D detection, outperforming state-of-the-art methods. As a versatile, plug-and-play framework, DK3D boosts existing models without requiring additional training data or increasing the computational cost at inference.},
  archive      = {J_TPAMI},
  author       = {Zizhang Wu and Fan Song and Yuanzhu Gan and Yunzhe Wu and Tianhao Xu and Xiaoquan Wang and Rui Tang and Jian Pu},
  doi          = {10.1109/TPAMI.2025.3594805},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Advancing 3D object detection with depth-aware spatial knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward resolution mismatching: Modality-aware feature-aligned network for pan-sharpening. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3594898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panchromatic (PAN) and multi-spectral (MS) remote satellite image fusion, known as pan-sharpening, aims to produce high-resolution MS images by combining the complementary information from the high-resolution, texture-rich PAN and the low-resolution but high spectral-resolution MS counterparts. Despite notable advancements in this field, the current state-of-the-art pan-sharpening techniques do not explicitly address the spatial resolution mismatching problem between the two modalities of PAN and MS images. This mismatching issue can lead to misalignment in feature representation and the creation of blurry artifacts in the model output, ultimately hindering the generation of high-frequency textures and impeding the performance improvement of such methods. To address the aforementioned spatial resolution mismatching problem in pan-sharpening, we propose a novel modality-aware feature-aligned pan-sharpening framework in this paper. The framework comprises three primary stages: modality-aware feature extraction, modality-aware feature aligning, and context integrated image reconstruction. First, we introduce the half-instance normalization strategy as the backbone to filter out the inconsistent features and promote the learning of consistent features between the PAN and MS modalities. Second, a learnable modality-aware feature interpolation is devised to effectively address the misalignment issue. Specifically, the extracted features from the backbone are integrated to predict the transformation offsets of each pixel, which allows for the adaptive selection of custom contextual information and enables the modality-aware features to be more aligned. Finally, within the context of the interactive offset correction, multi-stage information is aggregated to generate the feasible pan-sharpened model output. Extensive experimental results over multiple satellite datasets demonstrate that the proposed algorithm outperforms other state-of-the-art methods both qualitatively and quantitatively, exhibiting great generalization ability to real-world scenes.},
  archive      = {J_TPAMI},
  author       = {Man Zhou and Xuanhua He and Danfeng Hong},
  doi          = {10.1109/TPAMI.2025.3594898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward resolution mismatching: Modality-aware feature-aligned network for pan-sharpening},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decouple before align: Visual disentanglement enhances prompt tuning. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3594894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at https://github.com/Ferenas/DAPT.},
  archive      = {J_TPAMI},
  author       = {Fei Zhang and Tianfei Zhou and Jiangchao Yao and Ya Zhang and Ivor W. Tsang and Yanfeng Wang},
  doi          = {10.1109/TPAMI.2025.3594894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Decouple before align: Visual disentanglement enhances prompt tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based visual microphone based on specular reflections off angularly deformed surfaces. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3595008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and demonstrate the event-based visual microphone (EBVM), a passive electro-optical technique for remotely capturing audio signals using an event-based camera without any use of a conventional microphone. The event-based camera records local angular deformations of a surface induced by the sound propagation by observing the changes in the specular reflections at each pixel. By interpreting the timings of the specular incidences deduced from the event stream as signal level-crossings, we reconstruct the audio signal by imposing short-time Fourier sparsity conditions. The recovered audio signal is qualitatively comparable to or better than the prior art (intensitybased visual microphone), while simultaneously expanding the field of view by approximately 25 times and reducing data volume by three orders of magnitude. The proposed EBVM was tested on speech signal reconstruction as well as novel event-based acousto-optical passive ranging.},
  archive      = {J_TPAMI},
  author       = {Matthew Howard and Ryan Jones and Keigo Hirakawa},
  doi          = {10.1109/TPAMI.2025.3595008},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Event-based visual microphone based on specular reflections off angularly deformed surfaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep equilibrium object detection and segmentation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3595380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query-based object detectors and segmenters have made great progress in their respective tasks by employing an iterative refinement decoder. These query-based methods directly represent object instances with a set of learnable queries. These query vectors are progressively refined to stable, meaningful representations through a sequence of decoder layers and then used to directly predict object locations (mask or box) and categories with different heads. In this paper, we present a novel perspective on query-based object decoder design with infinite refinement (DEQ-Decoder) through a deep equilibrium model. Our DEQ-Decoder models the query vector refinement as the fixed point solving of an implicit layer. To be more specific to query refinement, we use a two-step unrolled equilibrium equation to explicitly capture the query vector refinement. Accordingly, we are able to incorporate refinement awareness into the DEQ-Decoder training with the inexact gradient back-propagation (RAG). In addition, to stabilize the training of our DEQ-Decoder and improve its generalization ability, we devise a deep supervision scheme on the optimization path of DEQ-Decoder with refinement-aware perturbation (RAP). To demonstrate the effectiveness of DEQ-Decoder, we apply it to object detection and instance segmentation. For object detection, we propose DEQDet based on our deep equilibrium decoder. DEQDet converges faster, consumes less memory, and achieves better results than the baseline counterpart (AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries achieves the 49.6 mAP and 33.9 AP$_{s}$ on the MS COCO benchmark under $2\times$ training scheme (24 epochs). For instance segmentation, Our DEQSeg achieves much better box mAP metrics and slightly better mask metrics for different mask decoding branches. Code and trained models are publicly available at https://github.com/MCG-NJU/DEQDet.},
  archive      = {J_TPAMI},
  author       = {Shuai Wang and Yao Teng and Limin Wang},
  doi          = {10.1109/TPAMI.2025.3595380},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep equilibrium object detection and segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple lifelong learning machines. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3595364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In lifelong learning, data are used to improve performance not only on the present task, but also on past and future (unencountered) tasks. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be to use data to improve performance on both future tasks (forward transfer) and past tasks (backward transfer). In this paper, we show that a simple approach—representation ensembling—demonstrates both forward and backward transfer in a variety of simulated and benchmark data scenarios, including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, Food1k, and CORe50), and speech (spoken digit), in contrast to various reference algorithms, which typically failed to transfer either forward or backward, or both. Moreover, our proposed approach can flexibly operate with or without a computational budget.},
  archive      = {J_TPAMI},
  author       = {Joshua T. Vogelstein and Jayanta Dey and Hayden S. Helm and Will LeVine and Ronak D. Mehta and Tyler M. Tomita and Haoyin Xu and Ali Geisa and Qingyang Wang and Gido M. van de Ven and Chenyu Gao and Weiwei Yang and Bryan Tower and Jonathan Larson and Christopher M. White and Carey E. Priebe},
  doi          = {10.1109/TPAMI.2025.3595364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Simple lifelong learning machines},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VolGen: Volumetric latent diffusion models for 3D object generation. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to extend 2D latent diffusion models, well known from the Stable-Diffusion series, to volumetric latent diffusion models for 3D object generation. Specifically, we first train a Volumetric Variational Auto-Encoder (VVAE) to compress 3D occupancy grids into a latent space, which compresses the $512^{3}$ occupancy grid into a $32^{3}$ latent code. We then train a diffusion model on this latent space, utilizing 3D convolutions and cross-attention layers for image conditioning. This Volumetric Latent Diffusion Model (VLDM) generates accurate and smooth mesh surfaces from single-view image inputs, and generalizes well to unseen domains during inference in around 10 seconds. Our key insight is that a simple volume-based latent diffusion model can also perform well for 3D generation tasks, without relying on sparse representations like point clouds or 3D specific techniques like triplane Neural Radiance Fields (NeRF). Extensive experiments demonstrate the effectiveness of our latent diffusion models in the 3D domain, indicating a promising direction for 3D generation tasks.},
  archive      = {J_TPAMI},
  author       = {Jiaxiang Tang and Xiang Wen and Hao-Xiang Guo and Hao Jiang and Zhihang Li and Jing Xu and Gang Zeng},
  doi          = {10.1109/TPAMI.2025.3594029},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VolGen: Volumetric latent diffusion models for 3D object generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CycleACR: Cycle modeling of actor-context relations for video action detection. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3595393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relation modeling between actors and scene context advances video action detection where the correlation of multiple actors makes their action recognition challenging. Existing studies model each actor and scene relation to improve action recognition. However, the scene variations and background interference limit the effectiveness of this relation modeling. In this paper, we propose to select actor-related scene context, rather than directly leverage raw video scenario, to improve relation modeling. We develop a Cycle Actor-Context Relation network (CycleACR) where there is a symmetric graph that models the actor and context relations in a bidirectional form. Specifically, our CycleACR is constituted of two modules: 1) Actor-to-Context Reorganization (A2C-R), which adaptively collects actor features for context feature reorganizations, and 2) Context-to-Actor Enhancement (C2A-E), which dynamically utilizes reorganized context features for actor feature enhancement. Stacking multiple CycleACR modules is able to effectively capture the high-order relation and efficiently exchange useful information between actors and context. To fully exploit temporal-dependent and holistic context information, we further design a parallel local and global temporal context modeling branch. The outputs of two branches are integrated as the final context-enhanced actor feature representations. Finally, we propose a context-aware memory bank for long-term relation modeling. The proposed bank can effectively store actor-related scene context from other clips without additional memory overhead. Compared to existing designs that focus on C2A-E, our CycleACR introduces the core design of A2C-R for more effective relation modeling. This cycle modeling advances our CycleACR to achieve state-of-the-art performance on two popular action detection datasets: AVA (40.6 mAP) and UCF101-24 (84.7 mAP). We also provide ablation studies and visualizations as well to show how our cycle actor-context relation modeling improves video action detection. Code and trained models are available at https://github.com/MCG-NJU/CycleACR.},
  archive      = {J_TPAMI},
  author       = {Lei Chen and Zhan Tong and Yibing Song and Gangshan Wu and Limin Wang},
  doi          = {10.1109/TPAMI.2025.3595393},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CycleACR: Cycle modeling of actor-context relations for video action detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the instance-dependent transition matrix estimation by exploiting self-supervised learning. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3595613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition matrix reveals the transition relationship between clean labels and noisy labels. It plays an important role in building statistically consistent classifiers for learning with noisy labels. However, in real-world applications, the transition matrix is usually unknown and has to be estimated. It is a challenging task to accurately estimate the transition matrix which usually depends on the instance. With both instances and noisy labels at hand, the major difficulty of estimating the transition matrix comes from the absence of clean label information. Recent work suggests that self-supervised learning methods can effectively infer clean label information. These methods could even achieve comparable performance with supervised learning on many benchmark datasets but without requiring any labels. Motivated by this, our paper presents a practical approach that harnesses self-supervised learning to extract clean label information, which reduces the estimation error of the instance-dependent transition matrix. By exploiting the estimated transition matrix, the performance of classifiers is improved. Empirical results on different datasets illustrate that our proposed methodology outperforms existing state-of-the-art methods in terms of both classification accuracy and transition matrix estimation.},
  archive      = {J_TPAMI},
  author       = {Yexiong Lin and Yu Yao and Zhaoqing Wang and Xu Shen and Jun Yu and Bo Han and Tongliang Liu},
  doi          = {10.1109/TPAMI.2025.3595613},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving the instance-dependent transition matrix estimation by exploiting self-supervised learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled dynamic intrusion detection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3595671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in attacks (e.g., 18% F1 for the MITM and 93% F1 for DDoS by a GCN-based state-of-the-art method), and perform poorly in few-shot intrusion detections (e.g., dramatically drops from 91% to 36% in 3D-IDS, and drops from 89% to 20% in E-GraphSAGE). We reveal that the underlying cause is entangled distributions of flow features. This motivates us to propose DIDS-MFL, a disentangled intrusion detection approach for various scenarios. DIDS-MFL involves two key components: a double Disentanglement-based Intrusion Detection System (DIDS) and a plug-and-play Multi-scale Few-shot Learning-based (MFL) intrusion detection module. Specifically, the proposed DIDS first disentangles traffic features by a non-parameterized optimization, automatically differentiating tens and hundreds of complex features. Such differentiated features will be further disentangled to highlight the attack-specific features. Our DIDS additionally uses a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. Furthermore, the proposed MFL involves an alternating optimization framework to address the entangled representations in few-shot traffic threats with rigorous derivation. MFL first captures multi-scale information in latent space to distinguish attack-specific information and then optimizes the disentanglement term to highlight the attack-specific information. Finally, MFL fuses and alternately solves them in an end-to-end way. To the best of our knowledge, DIDS-MFL takes the first step toward disentangled dynamic intrusion detection under various attack scenarios. Equipped with DIDS-MFL, administrators can effectively identify various attacks in encrypted traffic, including known, unknown, and few-shot threats that are not easily detected. Comprehensive experiments show the superiority of our proposed DIDS-MFL. For few-shot NIDS, our DIDS-MFL achieves a 71.91% - 125.19% improvement in average F1-score over 14 baselines and shows versatility in multiple baselines and multiple tasks. Our code is available at https://github.com/qcydm/DIDS-MFL},
  archive      = {J_TPAMI},
  author       = {Chenyang Qiu and Guoshun Nan and Hongrui Xia and Zheng Weng and Xueting Wang and Meng Shen and Xiaofeng Tao and Jun Liu},
  doi          = {10.1109/TPAMI.2025.3595671},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Disentangled dynamic intrusion detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InfoBound: A provable information-bounds inspired framework for both OoD generalization and OoD detection. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3595676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, distribution shifts give rise to the importance of two problems: out-of-distribution (OoD) generalization, which focuses on models' generalization ability against covariate shifts (i.e., the changes of environments), and OoD detection, which aims to be aware of semantic shifts (i.e., test-time unseen classes). Real-world testing environments often involve a combination of both covariate and semantic shifts. While numerous methods have been proposed to address these critical issues, only a few works tackled them simultaneously. Moreover, prior works often improve one problem but sacrifice the other. To overcome these limitations, we delve into boosting OoD detection and OoD generalization from the perspective of information theory, which can be easily applied to existing models and different tasks. Building upon the theoretical bounds for mutual information and conditional entropy, we provide a unified approach, composed of Mutual Information Minimization (MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and comprehensive evaluations on multi-label image classification and object detection have demonstrated the superiority of our method. It successfully mitigates trade-offs between the two challenges compared to competitive baselines.},
  archive      = {J_TPAMI},
  author       = {Lin Zhu and Yifeng Yang and Zichao Nie and Yuan Gao and Jiarui Li and Qinying Gu and Xinbing Wang and Chenghu Zhou and Nanyang Ye},
  doi          = {10.1109/TPAMI.2025.3595676},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {InfoBound: A provable information-bounds inspired framework for both OoD generalization and OoD detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FiGVCL: Fine-grained benchmark and method for video copy localization. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3595625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based video copy localization (VCL) aims to detect and locate copied segments in pairs of videos. VCL requires fine-grained video analysis to robustly identify copied segments that have been edited. Despite recent progress, the prohibitive cost of annotating copied segments and the lack of a fine-grained benchmark hinder the development of effective VCL systems. In this work, we annotate a new real-world dataset, FiGVCL, with challenging scenarios designed to evaluate VCL methods. FiGVCL is carefully annotated to preserve the temporal correspondences observed in copied segments. Moreover, we propose a novel fine-grained VCL benchmark metric based on temporal correspondences to improve discriminability. Finally, we design a simple but effective baseline model that uses fine-grained local embeddings for accurate copied segment localization. We also present an unsupervised training strategy that outperforms previous supervised VCL methods. The FiGVCL benchmark dataset, metric, and baseline models will be available at https://github.com/vslab-vcl/FiGVCL.},
  archive      = {J_TPAMI},
  author       = {Wenyang Luo and Yufan Liu and Bing Li and Weiming Hu and Stephen Maybank},
  doi          = {10.1109/TPAMI.2025.3595625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FiGVCL: Fine-grained benchmark and method for video copy localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protecting feature privacy in person re-identification. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3590979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) is to identify the same person across non-overlapping camera views. After a decade of development, the methods based on deep networks have achieved high performance on benchmarks and become mainstream. In applications, the features of gallery images extracted by deep learning-based methods are stored to speed up the query process and protect the sensitive information contained in the images. Unfortunately, it is demonstrated that turning the images into features cannot properly protect privacy, as these features could be reversed to the corresponding images, revealing the sensitive information they contain. Therefore, for preventing privacy leakage, recent methods learn their features against some feature reversal methods, and most conventional reversal methods focus on minimizing the difference between a reconstruction and its original image. However, there could be many reasonable reconstruction results from a single feature, and the conventional reversal methods will inevitably generate reconstruction results that lie in a different distribution from one of the original images, which cannot properly assess the private information for learning to protect and thus hamper the privacy-protected feature learning. To mitigate this problem, we enforce the reconstructions to follow the same distribution as the original images by the generative adversarial network (GAN). We operate this GAN-based feature reversal module accompanied by the conventional ReID feature extraction module and form a novel GAN-based feature privacy-protected person ReID model, which is expected to protect feature privacy so as against reversal attack and maintain ReID utility. We demonstrate that optimizing ReID model to accommodate privacy protection faces a double adversarial objective and is thus challenging. As a remedy, we design a novel two-step training and lazy update strategy that alternatively optimizes the feature extraction module and stabilizes the update process of the GAN-based feature reversal module. To evaluate the efficiency of the model in balancing its ReID utility and feature privacy protection, we introduce a novel metric called utility-reversibility ratio (URR). Compared with existing privacy-protected feature extraction models, the proposed method achieves a better balance between privacy protection and person ReID performance. Extensive experiments validate that our model can effectively protect feature privacy at a tiny accuracy cost, and validate the effectiveness of our model with the emerging diffusion model.},
  archive      = {J_TPAMI},
  author       = {Xiao Li and Yi-Xing Peng and Wei-Shi Zheng},
  doi          = {10.1109/TPAMI.2025.3590979},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Protecting feature privacy in person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NUPES: Non-uniform post-training quantization via power exponent search. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3593987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e. the optimization of the quantization operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and data-driven configurations. Our empirical benchmarks highlight the ability of NUPES to circumvent the limitations of previous post-training quantization techniques on transformers and large language models in particular.},
  archive      = {J_TPAMI},
  author       = {Edouard Yvinec and Arnaud Dapogny and Kevin Bailly},
  doi          = {10.1109/TPAMI.2025.3593987},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NUPES: Non-uniform post-training quantization via power exponent search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stereo-talker: Audio-driven 3D human synthesis with prior-guided mixture-of-experts. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3596160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Stereo-Talker, a novel one-shot audio-driven human video synthesis system that generates 3D talking videos with precise lip synchronization, expressive body gestures, temporally consistent photo-realistic quality, and continuous viewpoint control. The process follows a two-stage approach. In the first stage, the system maps audio input to high-fidelity motion sequences, encompassing upper-body gestures and facial expressions. To enrich motion diversity and authenticity, large language model (LLM) priors are integrated with text-aligned semantic audio features, leveraging LLMs' cross-modal generalization power to enhance motion quality. In the second stage, we improve diffusion-based video generation models by incorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided MoE focuses on view-specific attributes, while a mask-guided MoE enhances region-based rendering stability. Additionally, a mask prediction module is devised to derive human masks from motion data, enhancing the stability and accuracy of masks and enabling mask guiding during inference. We also introduce a comprehensive human video dataset with 2,203 identities, covering diverse body gestures and detailed annotations, facilitating broad generalization. The code, data, and pre-trained models will be released for research purposes on our https://xiang-deng00.github.io/stereo-talker.github.io/.},
  archive      = {J_TPAMI},
  author       = {Xiang Deng and Youxin Pang and Xiaochen Zhao and Chao Xu and Lizhen Wang and Hongjiang Xiao and Shi Yan and Hongwen Zhang and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3596160},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Stereo-talker: Audio-driven 3D human synthesis with prior-guided mixture-of-experts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DREAM: A dual variational framework for unsupervised graph domain adaptation. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3596054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification has been a prominent problem in graph machine learning fields. This problem has been investigated by leveraging message passing neural networks (MPNNs) to learn powerful graph representations. However, MPNNs extract topological semantics implicitly under label supervision, which could suffer from domain shift and label scarcity in unsupervised domain adaptation settings. In this paper, we propose an effective solution named Dual Variational Semantics Graph Mining (DREAM) for unsupervised graph domain adaptation by combining graph structural semantics from complementary perspectives. Besides a message passing branch to learn implicit semantics, our DREAM trains a path aggregation branch, which can provide explicit high-order structural semantics as a supplement. To train these two branches conjointly, we employ an expectation-maximization (EM) style variational framework for the maximization of likelihood. In the E-step, we fix the message passing branch and construct a graph-of-graph to indicate the geometric correlation between source and target domains, which would be adopted for the optimization of the other branch. In the M-step, we train the message passing branch and update the graph neural networks on the graph-of-graph with the other branch fixed. The alternative optimization improves the collaboration of knowledge from two branches. Extensive experiments on several benchmark datasets validate the superiority of the proposed DREAM compared with various baselines.},
  archive      = {J_TPAMI},
  author       = {Nan Yin and Li Shen and Mengzhu Wang and Xinwang Liu and Chong Chen and Xian-Sheng Hua},
  doi          = {10.1109/TPAMI.2025.3596054},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DREAM: A dual variational framework for unsupervised graph domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InstructLayout: Instruction-driven 2D and 3D layout synthesis with semantic graph prior. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3595880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehending natural language instructions is a charming property for both 2D and 3D layout synthesis systems. Existing methods implicitly model object joint distributions and express object relations, hindering generation's controllability. We introduce InstructLayout, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 2D and 3D layout synthesis. The proposed semantic graph prior learns layout appearances and object distributions simultaneously, demonstrating versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 2D and 3D scene synthesis, we respectively curate two high-quality datasets of layout-instruction pairs from public Internet resources with large language and multimodal models. Extensive experimental results reveal that the proposed method outperforms existing state-of-the-art approaches by a large margin in both 2D and 3D layout synthesis tasks. Thorough ablation studies confirm the efficacy of crucial design components.},
  archive      = {J_TPAMI},
  author       = {Chenguo Lin and Yuchen Lin and Panwang Pan and Xuanyang Zhang and Yadong Mu},
  doi          = {10.1109/TPAMI.2025.3595880},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {InstructLayout: Instruction-driven 2D and 3D layout synthesis with semantic graph prior},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-world adversarial defense against patch attacks based on diffusion model. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial patches present significant challenges to the robustness of deep learning models, making the development of effective defenses become critical for real-world applications. This paper introduces DIFFender, a novel DIFfusion-based DeFender framework that leverages the power of a text-guided diffusion model to counter adversarial patch attacks. At the core of our approach is the discovery of the Adversarial Anomaly Perception (AAP) phenomenon, which enables the diffusion model to accurately detect and locate adversarial patches by analyzing distributional anomalies. DIFFender seamlessly integrates the tasks of patch localization and restoration within a unified diffusion model framework, enhancing defense efficacy through their close interaction. Additionally, DIFFender employs an efficient few-shot prompt-tuning algorithm, facilitating the adaptation of the pre-trained diffusion model to defense tasks without the need for extensive retraining. Our comprehensive evaluation, covering image classification and face recognition tasks, as well as real-world scenarios, demonstrates DIFFender's robust performance against adversarial attacks. The framework's versatility and generalizability across various settings, classifiers, and attack methodologies mark a significant advancement in adversarial patch defense strategies. Except for the popular visible domain, we have identified another advantage of DIFFender: its capability to easily expand into the infrared domain. Consequently, we demonstrate the good flexibility of DIFFender, which can defend against both infrared and visible adversarial patch attacks alternatively using a universal defense framework.},
  archive      = {J_TPAMI},
  author       = {Xingxing Wei and Caixin Kang and Yinpeng Dong and Zhengyi Wang and Shouwei Ruan and Yubo Chen and and Hang Su},
  doi          = {10.1109/TPAMI.2025.3596462},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Real-world adversarial defense against patch attacks based on diffusion model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global and local semantic completion learning for vision-language pre-training. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3596394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal alignment plays a crucial role in vision-language pre-training (VLP) models, enabling them to capture meaningful associations across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-local alignment, i.e., associations between image patches and text tokens. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations to local features of the other modality. Therefore, in this paper, we propose a novel Global and Local Semantic Completion Learning (GLSCL) task to facilitate global-local alignment and local-local alignment simultaneously. Specifically, the GLSCL task complements the missing semantics of masked data and recovers global and local features by cross-modal interactions. Our GLSCL consists of masked global semantic completion (MGSC) and masked local token completion (MLTC). MGSC promotes learning more representative global features, which have a great impact on the performance of downstream tasks, while MLTC reconstructs modal-fusion local tokens, further enhancing accurate comprehension of multimodal data. To evaluate the proposed approaches on cross-modal alignment, we develop a validation benchmark called ALIGN-BENCH. Moreover, we present a flexible vision encoder, enabling our model to simultaneously perform image-text and video-text multimodal tasks. Experimental results show that our proposed method obtains state-of-the-art performance on various vision-language benchmarks, such as visual question answering, image-text retrieval, and video-text retrieval.},
  archive      = {J_TPAMI},
  author       = {Rong-Cheng Tu and Yatai Ji and Jie Jiang and Weijie Kong and Chengfei Cai and Wenzhe Zhao and Hongfa Wang and Yujiu Yang and Wei Liu},
  doi          = {10.1109/TPAMI.2025.3596394},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global and local semantic completion learning for vision-language pre-training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tri-perspective view decomposition for geometry aware depth completion and super-resolution. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion and super-resolution are crucial tasks for comprehensive RGB-D scene understanding, as they involve reconstructing the precise 3D geometry of a scene from sparse or low-resolution depth measurements. However, most existing methods either rely solely on 2D depth representations or directly incorporate raw 3D point clouds for compensation, which are still insufficient to capture the fine-grained 3D geometry of the scene. In this paper, we introduce Tri-Perspective View Decomposition (TPVD) frameworks that can explicitly model 3D geometry. To this end, (1) TPVD ingeniously decomposes the original 3D point cloud into three 2D views, one of which corresponds to the sparse or low-resolution depth input. (2) For sufficient geometric interaction, TPV Fusion is designed to update the 2D TPV features through recurrent 2D-3D-2D aggregation. (3) By adaptively searching for TPV affinitive neighbors, two additional refinement heads are developed for these two tasks to further improve the geometric consistency. Meanwhile, we build novel datasets named TOFDC for depth completion and TOFDSR for depth super-resolution. Both datasets are acquired using time-of-flight (TOF) sensors and color cameras on smartphones. Extensive experiments on TOFDC, KITTI, NYUv2, SUN RGBD, VKITTI, TOFDSR, RGB-D-D, Lu, and Middlebury datasets indicate that our TPVD outperforms previous depth completion and super-resolution methods, reaching the state of the art.},
  archive      = {J_TPAMI},
  author       = {Zhiqiang Yan and Kun Wang and Xiang Li and Guangwei Gao and Jun Li and Jian Yang},
  doi          = {10.1109/TPAMI.2025.3596391},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tri-perspective view decomposition for geometry aware depth completion and super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPHM: Gaussian parametric head model for monocular head avatar reconstruction. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating high-fidelity 3D human head avatars is crucial for applications in VR/AR, digital human, and film production. Recent advances have leveraged morphable face models to generate animated head avatars from easily accessible data, representing varying identities and expressions within a low-dimensional parametric space. However, existing methods often struggle with modeling complex appearance details, e.g., hairstyles, and suffer from low rendering quality and efficiency. In this paper we introduce a novel approach, 3D Gaussian Parametric Head Model, which employs 3D Gaussians to accurately represent the complexities of the human head, allowing precise control over both identity and expression. The Gaussian model can handle intricate details, enabling realistic representations of varying appearances and complex expressions. Furthermore, we presents a well-designed training framework to ensure smooth convergence, providing a robust guarantee for learning the rich content. Our method achieves high-quality, photo-realistic rendering with real-time efficiency, making it a valuable contribution to the field of parametric head models. Finally, we apply the 3D Gaussian Parametric Head Model to monocular video or few-shot head avatar reconstruction tasks, which enables instant reconstruction of high-quality 3D head avatars even when input data is extremely limited, surpassing previous methods in terms of reconstruction quality and training speed. Project page: https://yuelangx.github.io/gphmv2/.},
  archive      = {J_TPAMI},
  author       = {Yuelang Xu and Zhaoqi Su and Qingyao Wu and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3596331},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GPHM: Gaussian parametric head model for monocular head avatar reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced generative structure prior for chinese text image super-resolution. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3596329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faithful text image super-resolution (SR) is challenging because each character has a unique structure and usually exhibits diverse font styles and layouts. While existing methods primarily focus on English text, less attention has been paid to more complex scripts like Chinese. In this paper, we introduce a high-quality text image SR framework designed to restore the precise strokes of low-resolution (LR) Chinese characters. Unlike methods that rely on character recognition priors to regularize the SR task, we propose a novel structure prior that offers structure-level guidance to enhance visual quality. Our framework incorporates this structure prior within a StyleGAN model, leveraging its generative capabilities for restoration. To maintain the integrity of character structures while accommodating various font styles and layouts, we implement a codebook-based mechanism that restricts the generative space of StyleGAN. Each code in the codebook represents the structure of a specific character, while the vector $w$ in StyleGAN controls the character's style, including typeface, orientation, and location. Through the collaborative interaction between the codebook and style, we generate a high-resolution structure prior that aligns with LR characters both spatially and structurally. Experiments demonstrate that this structure prior provides robust, character-specific guidance, enabling the accurate restoration of clear strokes in degraded characters, even for real-world LR Chinese text with irregular layouts. Our code and pre-trained models will be available at https://github.com/csxmli2016/MARCONetPlusPlus.},
  archive      = {J_TPAMI},
  author       = {Xiaoming Li and Wangmeng Zuo and Chen Change Loy},
  doi          = {10.1109/TPAMI.2025.3596329},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced generative structure prior for chinese text image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniAV: Unified audio-visual perception for multi-task video event localization. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3593932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video event localization tasks include temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods tend to over-specialize on individual tasks, neglecting the equal importance of these different events for a complete understanding of video content. In this work, we aim to develop a unified framework to solve TAL, SED and AVEL tasks together to facilitate holistic video understanding. However, it is challenging since different tasks emphasize distinct event characteristics and there are substantial disparities in existing task-specific datasets (size/domain/duration). It leads to unsatisfactory results when applying a naive multi-task strategy. To tackle the problem, we introduce UniAV, a Unified Audio-Visual perception network to effectively learn and share mutually beneficial knowledge across tasks and modalities. Concretely, we propose a unified audio-visual encoder to derive generic representations from multiple temporal scales for videos from all tasks. Meanwhile, task-specific experts are designed to capture the unique knowledge specific to each task. Besides, instead of using separate prediction heads, we develop a novel unified language-aware classifier by utilizing semantic-aligned task prompts, enabling our model to flexibly localize various instances across tasks with an impressive open-set ability to localize novel categories. Extensive experiments demonstrate that UniAV, with its unified architecture, significantly outperforms both single-task models and the naive multi-task baseline across all three tasks. It achieves superior or on-par performances compared to the state-of-the-art task-specific methods on ActivityNet 1.3, DESED and UnAV-100 benchmarks.},
  archive      = {J_TPAMI},
  author       = {Tiantian Geng and Teng Wang and Jinming Duan and Yanfu Zhang and Weili Guan and Feng Zheng and Ling Shao},
  doi          = {10.1109/TPAMI.2025.3593932},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {UniAV: Unified audio-visual perception for multi-task video event localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On structuring hyperspherical manifold for probing novel biomedical entities. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3596597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The insufficient high- throughput modeling capability for high-dimensional, multiscale, and nonlinear real-world observations and measurements stands as one of the major impediments for modern science advancements. In this regard, machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven disposition. With the ever-increasing stream of research data collection, it would be appealing to automate the exploration of patterns and insights from observational data for discovering novel classes of phenotypes and entities. However, in the discipline of biomedical investigation, the cumulative data is intrinsically subjected to non-i.i.d. distribution and severe biases amongst different clusters, inducing disorganization and ambiguity in the learned representation space. To contend with the inherent challenges, in this paper, we present a geometry- constrained probabilistic modeling treatment on hyperspherical manifolds. It firstly parameterizes the approximated posterior of instance- wise embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent shift, and thereafter incorporates a suite of critical inductive biases to organically shape the layout of tailored embedding space. Together, these advancements offer a systematic solution to regularize the uncontrollable risk for unseen class learning and prospecting. Furthermore, we propose a spectral graph-theoretic method to efficiently estimate the number of potential novel classes and endow the prediction with adorable taxonomy adaptability. Through extensive experiments under various settings, we demonstrate the effectiveness and general applicability of the proposed methods in recognizing and structurally phenotyping novel visual concepts.},
  archive      = {J_TPAMI},
  author       = {Jianan Fan and Dongnan Liu and Hang Chang and Heng Huang and Mei Chen and Weidong Cai},
  doi          = {10.1109/TPAMI.2025.3596597},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On structuring hyperspherical manifold for probing novel biomedical entities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeMatch++: Two-view correspondence learning via deep motion field decomposition and respective local-context aggregation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3596598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-view correspondence learning has increasingly focused on the coherence and smoothness of motion fields between image pairs. Conventional methods either regularize the complexity of the field function at substantial computational expense, or apply local filters that prove ineffective for large scene disparities. In this paper, we present DeMatch++, a novel network drawing inspiration from Fourier decomposition principles that decomposes the motion field to retain its primary “low-frequency” and smooth components. This approach achieves implicit regularization with lower computational overhead while exhibiting inherent piecewise smoothness. Specifically, our method decomposes the noise-contaminated motion field into multiple linearly independent basis vectors, generating smooth sub-fields that preserve the main energy of the original field. These sub-fields facilitate the recovery of a cleaner motion field for precise vector derivation. Within this framework, we aggregate local context within each sub-field while enhancing global information across all sub-fields. We also employ a masked decomposition strategy that mitigates the influence of false matches, and construct a compact representation to suppress redundant sub-fields. The complete pipeline is formulated as a discrete learnable architecture, circumventing the need for dense field computation. Extensive experiments demonstrate that DeMatch++ outperforms state-of-the-art methods while maintaining computational efficiency and piecewise smoothness. The code and trained models are publicly available at https://github.com/SuhZhang/DeMatchPlus.},
  archive      = {J_TPAMI},
  author       = {Shihua Zhang and Zizhuo Li and Jiayi Ma},
  doi          = {10.1109/TPAMI.2025.3596598},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DeMatch++: Two-view correspondence learning via deep motion field decomposition and respective local-context aggregation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized probabilistic graphical modeling for multi-view bipartite graph clustering. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3596764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view bipartite graph clustering (MVBGC) is an active pipeline in unsupervised learning to tackle the limited scalability issue of traditional graph clustering. Despite improved performance, numerous variants still fall under conventional modeling that plugs additional modules, which however induces increasingly intricate models and fails to reveal the inherent variable relationship. We make the first attempt to introduce probabilistic graphical models for modeling the multi-view bipartite graph clustering task, reformulating it as a maximum likelihood estimation (MLE) problem. Such a setting uncovers the underlying probabilistic correlations among the commonality, view-specific variables, and noisy components. By pruning redundancy and disturbance collectively referred to as noise, we prove that minimizing the total noise is an approximation of the lower bound of MLE for multi-view data observations. We further generalize the MLE setting with clustering-suited constraints, deriving a Generalized Probabilistic Graphical Modeling framework (GProM), achieving an interpretable, concise, and flexible MVBGC framework. Extensive experiments verify the effectiveness of our framework. Furthermore, statistical significance analysis reveals the effectiveness of different distribution assumptions, providing valuable insights for model design.},
  archive      = {J_TPAMI},
  author       = {Liang Li and Yuangang Pan and Yinghua Yao and Junpu Zhang and Moyun Liu and Xueling Zhu and Xinwang Liu and Kenli Li and Ivor W. Tsang and Keqin Li},
  doi          = {10.1109/TPAMI.2025.3596764},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized probabilistic graphical modeling for multi-view bipartite graph clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing high quality raw video using temporal affinity and diffusion prior. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3596623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rich information and original data distribution, RAW data are widely used in many computer vision applications. However, the use of RAW video remains limited because of the high storage costs associated with data collection. Previous works have attempted to reconstruct RAW frames from sRGB data using small sampled metadata from the original RAW frames. Yet, these algorithms struggle with RAW video reconstruction due to the high computational cost of sampling metadata on cameras. To address these issues, we propose a new RAW video reconstruction pipeline that de-renders high-quality RAW videos from sRGB data using only one initial RAW frame as a reference. Specifically, we introduce three new models to achieve this goal. First, we present the Temporal-Affinity Guided De-rendering Network. This network leverages the temporal affinity between adjacent frames to construct a reference RAW image from previous RAW pixels. The corresponding RAW pixels in the previous frame provide valuable information about the original RAW data distribution, aiding in the precise reconstruction of the current frame. Second, to recover the missing RAW pixels caused by camera and foreground movement, we fully exploit the rich prior information from a pre-trained diffusion model and propose the RAW In-painting Model. This model can accurately fill in hollow regions in a RAW image based on the corresponding sRGB image and the surrounding RAW context. Lastly, we present a lightweight content-aware video clipper that automatically adjusts the clip length used for RAW video reconstruction, thereby balancing storage requirements with reconstruction quality. To better evaluate the performance of the proposed framework across different devices, we introduce the first RAW video reconstruction benchmark that comprises RAW videos from six types of camera devices with challenging scenarios. Experimental results demonstrate that our algorithm can accurately reconstruct RAW videos across all the scenarios. To facilitate further research, the code, pre-trained weight, dataset, and demo web will be publicly available at: https://um-lab.github.io/VideoRAW/.},
  archive      = {J_TPAMI},
  author       = {Wencheng Han and Jianbing Shen and David J. Crandall and Cheng-Zhong Xu},
  doi          = {10.1109/TPAMI.2025.3596623},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstructing high quality raw video using temporal affinity and diffusion prior},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMOv2: Pushing 5 m vision model frontier. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3596776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5 M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i222222222222222222222222rmb) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4 G/5 G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5 M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1 M/2M/5 M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5 M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5 M by +2.6$\uparrow$ . When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level.},
  archive      = {J_TPAMI},
  author       = {Jiangning Zhang and Teng Hu and Haoyang He and Zhucun Xue and Yabiao Wang and Chengjie Wang and Yong Liu and Xiangtai Li and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3596776},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EMOv2: Pushing 5 m vision model frontier},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep temporal graph clustering: A comprehensive benchmark and datasets. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3596609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.},
  archive      = {J_TPAMI},
  author       = {Meng Liu and Ke Liang and Siwei Wang and Xingchen Hu and Sihang Zhou and Xinwang Liu},
  doi          = {10.1109/TPAMI.2025.3596609},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep temporal graph clustering: A comprehensive benchmark and datasets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AW-GBGAE: An adaptive weighted graph autoencoder based on granular-balls for general data clustering. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current scenario, a vast amount of unlabeled high-dimensional data exhibits intrinsic relationships, making it suitable for information extraction through graph-based clustering methods. However, these datasets often lack edge structure information and contain numerous irrelevant features. To address these challenges, we propose a comprehensive solution that involves: (1) applying a feature weighting approach to manage features, (2) constructing edges based on weighted granular-balls, and (3) integrating graph convolutional networks (GCNs) with edge generation to develop an autoencoder network. Our method significantly enhances the extraction of relevant information from high-dimensional, unlabeled data, improving the overall performance and reliability of the clustering process. Extensive experimental results demonstrate that our model, AW-GBGAE, excels in clustering tasks and exhibits strong competitiveness compared to baseline models. The code is publicly available at https://github.com/xjnine/AWGBGAE.},
  archive      = {J_TPAMI},
  author       = {Jiang Xie and Yuxin Cheng and Shuyin Xia and Chunfeng Hua and Guoyin Wang and Xinbo Gao},
  doi          = {10.1109/TPAMI.2025.3596615},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AW-GBGAE: An adaptive weighted graph autoencoder based on granular-balls for general data clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware medical diagnostic phrase identification and grounding. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical phrase grounding is crucial for identifying relevant regions in medical images based on phrase queries, facilitating accurate image analysis and diagnosis. However, current methods rely on manual extraction of key phrases from medical reports, reducing efficiency and increasing the workload for clinicians. Additionally, the lack of model confidence estimation limits clinical trust and usability. In this paper, we introduce a novel task—Medical Report Grounding (MRG)—which aims to directly identify diagnostic phrases and their corresponding grounding boxes from medical reports in an end-to-end manner. To address this challenge, we propose uMedGround, a robust and reliable framework that leverages a multimodal large language model to predict diagnostic phrases by embedding a unique token, $\lt $$\mathtt {BOX}$$\gt $, into the vocabulary to enhance detection capabilities. A vision encoder-decoder processes the embedded token and input image to generate grounding boxes. Critically, uMedGround incorporates an uncertainty-aware prediction model, significantly improving the robustness and reliability of grounding predictions. Experimental results demonstrate that uMedGround outperforms state-of-the-art medical phrase grounding methods and fine-tuned large visual-language models, validating its effectiveness and reliability. This study represents a pioneering exploration of the MRG task, marking the first-ever endeavor in this domain. Additionally, we demonstrate the applicability of uMedGround in medical visual question answering and class-based localization tasks, where it highlights visual evidence aligned with key diagnostic phrases, supporting clinicians in interpreting various types of textual inputs, including free-text reports, visual question answering queries, and class labels.},
  archive      = {J_TPAMI},
  author       = {Ke Zou and Yang Bai and Bo Liu and Yidi Chen and Zhihao Chen and Yang Zhou and Xuedong Yuan and Meng Wang and Xiaojing Shen and Xiaochun Cao and Yih Chung Tham and Huazhu Fu},
  doi          = {10.1109/TPAMI.2025.3596878},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uncertainty-aware medical diagnostic phrase identification and grounding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video demoireing using focused-defocused dual-camera system. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3596700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moire patterns, unwanted color artifacts in images and videos, arise from the interference between spatially high-frequency scene contents and the spatial discrete sampling of digital cameras. Existing demoireing methods primarily rely on single-camera image/video processing, which faces two critical challenges: 1) distinguishing moire patterns from visually similar real textures, and 2) preserving tonal consistency and temporal coherence while removing moire artifacts. To address these issues, we propose a dual-camera framework that captures synchronized videos of the same scene: one in focus (retaining high-quality textures but may exhibit moire patterns) and one defocused (with significantly reduced moire patterns but blurred textures). We use the defocused video to help distinguish moire patterns from real texture, so as to guide the demoireing of the focused video. We propose a frame-wise demoireing pipeline, which begins with an optical flow based alignment step to address any discrepancies in displacement and occlusion between the focused and defocused frames. Then, we leverage the aligned defocused frame to guide the demoireing of the focused frame using a multi-scale CNN and a multi-dimensional training loss. To maintain tonal and temporal consistency, our final step involves a joint bilateral filter to leverage the demoireing result from the CNN as the guide to filter the input focused frame to obtain the final output. Experimental results demonstrate that our proposed framework largely outperforms state-of-the-art image and video demoireing methods.},
  archive      = {J_TPAMI},
  author       = {Xuan Dong and Xiangyuan Sun and Xia Wang and Jian Song and Ya Li and Weixin Li},
  doi          = {10.1109/TPAMI.2025.3596700},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video demoireing using focused-defocused dual-camera system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-spectral analysis of bivariate graph signals. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3596918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements in technology and monitoring tools, we often encounter multivariate graph signals, which can be seen as the realizations of multivariate graph processes, and revealing the relationship between their constituent quantities is one of the important problems. To address this issue, we propose a cross-spectral analysis tool for bivariate graph signals. The main goal of this study is to extend the scope of spectral analysis of graph signals to bivariate graph signals. In this study, we define joint weak stationarity graph processes and introduce graph cross-spectral density and coherence for bivariate graph processes. We propose several estimators for the cross-spectral density and investigate the theoretical properties of the proposed estimators. Furthermore, we demonstrate the effectiveness of the proposed estimators through numerical experiments, including simulation studies and a real data application. Finally, as an interesting extension, we discuss robust spectral analysis of graph signals in the presence of outliers.},
  archive      = {J_TPAMI},
  author       = {Kyusoon Kim and Hee-Seok Oh},
  doi          = {10.1109/TPAMI.2025.3596918},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-spectral analysis of bivariate graph signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and accurate multi-view stereo with confidence-aware diffusion model. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3597148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D.},
  archive      = {J_TPAMI},
  author       = {Fangjinhua Wang and Qingshan Xu and Yew-Soon Ong and Marc Pollefeys},
  doi          = {10.1109/TPAMI.2025.3597148},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lightweight and accurate multi-view stereo with confidence-aware diffusion model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Left barrier loss for unbiased survival analysis prediction. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3597163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis (SA) prediction involves the prediction of the time until an event of interest occurs (TTE), based on input attributes. The main challenge of SA is instances where the event is not observed (censored), typically through an alternative (censoring) event. Most SA prediction methods suffer from drawbacks limiting the usage of advanced machine learning methods: Ignoring the input of the censored samples, no separation between model and loss, and typical small datasets and high input dimensions. We propose a loss function, denoted suRvival Analysis lefT barrIer lOss (RATIO), that explicitly incorporates the censored samples input in the prediction. RATIO accounts for the difference between censored and uncensored samples, by only considering censoring events occurring after the predicted, and through a linear term on the uncensored data event time. RATIO can be used with any prediction model. We further propose FIESTA a data augmentation method, combining the TTE of uncensored samples with the input of censored samples. We show that RATIO drastically improves the precision and reduces the bias of SA prediction in both models and real-life SA problems, and FIESTA allows for the inclusion of high-dimension data in SA methods even with a small number of uncensored samples.},
  archive      = {J_TPAMI},
  author       = {Oshrit Shtossel and Omry Koren and Yoram Louzoun},
  doi          = {10.1109/TPAMI.2025.3597163},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Left barrier loss for unbiased survival analysis prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeCoV2: Semantic connectivity-driven pseudo-labeling for robust cross-domain semantic segmentation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3596943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-labeling is a dominant strategy for cross-domain semantic segmentation (CDSS), yet its effectiveness is limited by fragmented and noisy pixel-level predictions under severe domain shifts. To address this, we propose a semantic connectivity-driven pseudo-labeling framework, SeCo, which constructs and refines pseudo-labels at the connectivity level by aggregating high-confidence pixels into coherent semantic regions. The framework includes two key components: Pixel Semantic Aggregation (PSA), which leverages a dual prompting strategy to preserve category-specific granularity, and Semantic Connectivity Correction with Loss Distribution (SCC-LD), which filters noisy regions based on early-loss statistics. Building upon this foundation, we further present SeCoV2, which introduces SCC-Unc, a novel uncertainty-aware correction module that constructs a connectivity graph and enforces relational consistency for robust refinement in ambiguous regions. SeCoV2 also broadens the applicability of SeCo by extending evaluation to more challenging scenarios, including open-set and multimodal adaptation, semi-supervised domain generalization, and by validating compatibility with different interactive foundation segmentation models such as SAM [1], SEEM [2], and Fast-SAM [3]. Extensive experiments across six CDSS tasks demonstrate that SeCoV2 achieves consistent improvements over previous methods, with an average performance gain of up to +4.6%, establishing new state-of-the-art results. These findings highlight the effectiveness and generalization ability for robust adaptation in diverse real-world environments.},
  archive      = {J_TPAMI},
  author       = {Dong Zhao and Qi Zang and Nan Pu and Shuang Wang and Nicu Sebe and Zhun Zhong},
  doi          = {10.1109/TPAMI.2025.3596943},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SeCoV2: Semantic connectivity-driven pseudo-labeling for robust cross-domain semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-aware knowledge aligning and prototyping for non-exemplar lifelong person re-identification. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3597023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong person re-identification (LReID) suffers from the catastrophic forgetting problem when learning from non-stationary data streams. Existing exemplar-based and knowledge distillation-based LReID methods encounter data privacy and limited acquisition capacity, respectively. In this paper, we introduce the prototype, which is under-investigated in LReID, to better balance knowledge retention and acquisition. Previous prototype-based works primarily focused on the classification task, where prototypes were modeled as discrete points or statistical distributions. However, they either discarded the distribution information or omitted instance-level diversity, which are crucial fine-grained clues for LReID. Furthermore, the domain shifts between data sources result in a feature gap between the new and old data, which restricts the utilization of the fine-grained information in prototypes. To address these challenges, we propose Distribution-aware Knowledge Aligning and Prototyping (DKP++), a novel framework for modeling and leveraging prototypes in LReID. First, an Instance-level Distribution Modeling network is introduced to capture the local diversity of each instance. Next, a Distribution-oriented Prototype Generation algorithm transforms the instance-level diversity into identity-level distributions which are stored as prototypes. Then, a Prototype-based Knowledge Transfer module distills the knowledge within the prototypes to the new model. To mitigate the impact of domain shifts during knowledge transfer, we introduce a privacy-friendly Distribution Aligning module that transforms new input data to fit the historical distribution, which is incorporated with feature-level alignment constraints to enhance the coherence between new and old knowledge, effectively improving historical prototype utilization. Extensive experiments demonstrate that our method achieves a superior balance between plasticity and stability, outperforming state-of-the-art LReID methods by a large margin.},
  archive      = {J_TPAMI},
  author       = {Jiahuan Zhou and Kunlun Xu and Fan Zhuo and Xu Zou and Yuxin Peng},
  doi          = {10.1109/TPAMI.2025.3597023},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Distribution-aware knowledge aligning and prototyping for non-exemplar lifelong person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PC-SRGAN: Physically consistent super-resolution generative adversarial network for general transient simulations. <em>TPAMI</em>, 1-8. (<a href='https://doi.org/10.1109/TPAMI.2025.3596647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super-Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional SR methods, even with limited training data (e.g., only 13% of training data is required to achieve performance similar to SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning by improving accuracy and efficiency, enhancing process understanding, and broadening applications to scientific research. We publicly release the complete source code of PC-SRGAN and all experiments at https://github.com/hasan-rakibul/PC-SRGAN.},
  archive      = {J_TPAMI},
  author       = {Md Rakibul Hasan and Pouria Behnoudfar and Dan MacKinlay and Thomas Poulet},
  doi          = {10.1109/TPAMI.2025.3596647},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PC-SRGAN: Physically consistent super-resolution generative adversarial network for general transient simulations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MBA-SLAM: Motion blur aware dense visual SLAM with radiance fields representation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3596976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras' local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach.},
  archive      = {J_TPAMI},
  author       = {Peng Wang and Lingzhe Zhao and Yin Zhang and Shiyu Zhao and Peidong Liu},
  doi          = {10.1109/TPAMI.2025.3596976},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MBA-SLAM: Motion blur aware dense visual SLAM with radiance fields representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HandBooster+: Boosting 3D hand-mesh reconstruction from data synthesis to progressive multi-hypothesis aggregation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3596986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustly reconstructing 3D hand mesh from a single image is very challenging, due to (i) the lack of diversity in existing real-world datasets and (ii) the ambiguity in occluded hand regions. While data synthesis helps relieve issue (i), the syn-to-real gap still hinders its usage. For issue (ii), most previous works produce deterministic results while other probabilistic methods rely on ground truths to choose the best hypothesis. In this work, we explore the diffusion model to alleviate these problems by collectively considering two perspectives: (i) conditional synthesis and sampling approach for realistic data generation and (ii) probabilistic modeling with progressive multi-hypothesis aggregation. First, we present HandBooster, a new approach to uplift the data diversity by training a conditional generative space on hand-object interactions and sampling the space to synthesize effective data with reliable 3D annotations and diverse hand appearances, poses, views, and backgrounds. Second, we design HandBooster+, a probabilistic diffusion-based model to further boost the 3D hand-mesh reconstruction performance by progressively aggregating the multiple hypotheses. Extensive experimental results show that our method significantly improves several baselines and achieves SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster+_PyTorch.},
  archive      = {J_TPAMI},
  author       = {Hao Xu and Haipeng Li and Yinqiao Wang and Shuaicheng Liu and Chi-Wing Fu},
  doi          = {10.1109/TPAMI.2025.3596986},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HandBooster+: Boosting 3D hand-mesh reconstruction from data synthesis to progressive multi-hypothesis aggregation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time evidence fusion network: Multi-source view in long-term time series forecasting. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3596905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical scenarios, time series forecasting necessitates not only accuracy but also efficiency. Consequently, the exploration of model architectures remains a perennially trending topic in research. To address these challenges, we propose a novel backbone architecture named Time Evidence Fusion Network (TEFN) from the perspective of information fusion. Specifically, we introduce the Basic Probability Assignment (BPA) Module based on evidence theory to capture the uncertainty of multivariate time series data from both channel and time dimensions. Additionally, we develop a novel multi-source information fusion method to effectively integrate the two distinct dimensions from BPA output, leading to improved forecasting accuracy. Lastly, we conduct extensive experiments to demonstrate that TEFN achieves performance comparable to state-of-the-art methods while maintaining significantly lower complexity and reduced training time. Also, our experiments show that TEFN exhibits high robustness, with minimal error fluctuations during hyperparameter selection. Furthermore, due to the fact that BPA is derived from fuzzy theory, TEFN offers a high degree of interpretability. Therefore, the proposed TEFN balances accuracy, efficiency, stability, and interpretability, making it a desirable solution for time series forecasting.},
  archive      = {J_TPAMI},
  author       = {Tianxiang Zhan and Yuanpeng He and Yong Deng and Zhen Li and Wenjie Du and Qingsong Wen},
  doi          = {10.1109/TPAMI.2025.3596905},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Time evidence fusion network: Multi-source view in long-term time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dubbing movies via hierarchical phoneme modeling and acoustic diffusion denoising. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3597267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a piece of text, a video clip, and reference audio, the movie dubbing (also known as Visual Voice Cloning, V2C) task aims to generate speeches that clone reference voice and align well with the video in both emotion and lip movement, which is more challenging than conventional text-to-speech synthesis tasks. To align the generated speech with the inherent lip motion of the given silent video, most existing works utilize each video frame to query textual phonemes. However, such an attention operation usually leads to mumble speech because different phonemes are fused for video frames corresponding to one phoneme (video frames are finer-grained than phonemes). To address this issue, we propose a diffusion-based movie dubbing architecture, which improves pronunciation by Hierarchical Phoneme Modeling (HPM) and generates better mel-spectrogram through Acoustic Diffusion Denoising (ADD). We term our model as HD-Dubber. Specifically, our HPM bridges the visual information and corresponding speech prosody from three aspects: (1) aligning lip movement with the speech duration based on each phoneme unit by contrastive learning; (2) conveying facial expression to phoneme-level energy and pitch; and (3) injecting global emotions captured from video scenes into prosody. On the other hand, ADD exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via a parameterized Markov chain conditioned on textual phonemes and reference audio. ADD has two novel denoisers, the Style-adaptive Residual Denoiser (SRD) and the Phoneme-enhanced U-net Denoiser (PUD), to enhance speaker similarity and improve pronunciation quality. Extensive experimental results on the three benchmark datasets demonstrate the state-of-the-art performance of the proposed method. The source code and trained models will be made available to the public.},
  archive      = {J_TPAMI},
  author       = {Liang Li and Gaoxiang Cong and Yuankai Qi and Zheng-Jun Zha and Qi Wu and Quan Z. Sheng and Qingming Huang and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2025.3597267},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dubbing movies via hierarchical phoneme modeling and acoustic diffusion denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bringing equity to classification: Domain generalization for domain-linked classes. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3593407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) focuses on transferring domain-invariant knowledge from multiple source (training) domains to an a priori unseen target domain(s). This task implicitly requires that classes of interest are expressed in multiple sources (domain-shared) to break spurious domain-class correlations. However, real-world data scarcity challenges may often result in classes present in only a specific domain (domain-linked), which we show leads to extremely poor generalization. In this work, we introduce the domain-linked DG task to the community and develop a methodology to learn useful domain-invariant representations from domain-shared classes for domain-linked ones. Specifically, we propose FOND, a Fairness-inspired and cONtrastive learning objective for Domain-linked DG. Rigorous and reproducible experimental results communicate that FOND accomplishes state-of-the-art improvements for domain-linked classes, given a sufficient number of domain-shared classes and with minimal performance trade-offs. Complementary to these contributions, we theoretically analyze this task and provide practical insights for domain-linked class generalizability.},
  archive      = {J_TPAMI},
  author       = {Kimathi Kaai and Saad Hossain and Sirisha Rambhatla},
  doi          = {10.1109/TPAMI.2025.3593407},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bringing equity to classification: Domain generalization for domain-linked classes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards natural machine unlearning. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3597350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning (MU) aims to eliminate information that has been learned from specific training data, namely forgetting data, from a pretrained model. Currently, the mainstream of relabeling-based MU methods involves modifying the forgetting data with incorrect labels and subsequently fine-tuning the model. While learning such incorrect information can indeed remove knowledge, the process is quite unnatural as the unlearning process undesirably reinforces the incorrect information and leads to over-forgetting. Towards more natural machine unlearning, we inject correct information from the remaining data to the forgetting samples when changing their labels. Through pairing these adjusted samples with their labels, the model tends to use the injected correct information and naturally suppress the information meant to be forgotten. Albeit straightforward, such a first step towards natural machine unlearning can significantly outperform current state-of-the-art approaches. In particular, our method substantially reduces the over-forgetting problem and leads to strong robustness across different unlearning tasks, making it a promising candidate for practical machine unlearning.},
  archive      = {J_TPAMI},
  author       = {Zhengbao He and Tao Li and Xinwen Cheng and Zhehao Huang and Xiaolin Huang},
  doi          = {10.1109/TPAMI.2025.3597350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards natural machine unlearning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DCOMPAT++: An improved large-scale 3D vision dataset for compositional recognition. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3597476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present 3DCOMPAT++, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the partinstance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCOMPAT ++ covers 42 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at the CVPR conference, showcasing the winning method's utilization of a modified PointNet++ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision. The dataset and code have been made publicly available at https://3dcompat-dataset.org/v2/. 3D vision, dataset, 3D modeling, multimodal learning, compositional learning. },
  archive      = {J_TPAMI},
  author       = {Habib Slim and Xiang Li and Yuchen Li and Mahmoud Ahmed and Mohamed Ayman and Ujjwal Upadhyay and Ahmed Abdelreheem and Arpit Prajapati and Suhail Pothigara and Peter Wonka and Mohamed Elhoseiny},
  doi          = {10.1109/TPAMI.2025.3597476},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3DCOMPAT++: An improved large-scale 3D vision dataset for compositional recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic inference by model reduction. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3595670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can agents infer the intentions of others by simply observing their behavior? And how can they generate fast and accurate actions such as grasping a moving object on the fly? Recent advances in Bayesian model reduction have led to innovative, biologically plausible approaches to actively infer the state of affairs of the world and perform planning with continuous signals. However, reducing the surrounding environment into a small set of simpler hypotheses remains a challenge in highly dynamic contexts. In this study, we propose an approach, based on active inference, that employs dynamic priors sampled from reduced versions of a generative model. Each dynamic prior corresponds to an alternative evolution of the world, which the agent can evaluate by accumulating continuous data. We test our approach on two everyday tasks: inferring a trajectory and grasping a moving object. Our findings reveal how agents can smoothly infer and enact dynamic intentions, and emphasize the key role of intentional gain or precision in motor learning.},
  archive      = {J_TPAMI},
  author       = {Matteo Priorelli and Ivilin Peev Stoianov},
  doi          = {10.1109/TPAMI.2025.3595670},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic inference by model reduction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning generalized medical image representation by decoupled feature queries. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3597364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images are usually collected from multiple clinical centers with various types of scanners. When confronted with such significant cross-domain distribution discrepancy, a deep network tends to capture similar patterns by multiple channels, while different cross-domain patterns are also allowed to rest in the same channel. Such channel redundancy limits the expressive capability of a representation, resulting in less preferable generalization ability. To address this fundamental yet challenging issue, we propose a novel decoupled feature as query (DFQ) framework for domain generalized medical image representation learning. Its general idea is to leverage the channel-wise decoupled deep features as queries. Particularly, a deep instance whitening transform with restricted isometry is proposed, which enforces each channel orthogonal to the rest channels after decoupling. Besides, the long-range dependency between decoupled deep and shallow features is implicitly constrained to minimize channel redundancy throughout training. Extensive experiments show its state-of-the-art performance on three medical domain generalization tasks with four modalities.},
  archive      = {J_TPAMI},
  author       = {Qi Bi and Jingjun Yi and Hao Zheng and Wei Ji and Yawen Huang and Yuexiang Li and Yefeng Zheng},
  doi          = {10.1109/TPAMI.2025.3597364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning generalized medical image representation by decoupled feature queries},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiBBDM: Bidirectional image translation with brownian bridge diffusion models. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3597667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the challenging realm of image-to-image translation, most traditional methods require separate models for different translation directions, leading to inefficient use of computational resources. This paper introduces the Bidirectional Brownian Bridge Diffusion Model (BiBBDM), a novel approach that leverages Brownian Bridge processes for bidirectional image-to-image translation. Unlike conventional Diffusion Models (DMs) that treat image-to-image translation as a unidirectional conditional generation process, BiBBDM models the translation as a stochastic Brownian Bridge process, enabling simultaneous learning of bidirectional translation between two domains. This innovation allows our method to achieve bidirectional image translation using different sampling directions of a single model, eliminating the need for multiple models for both translation directions. To the best of our knowledge, BiBBDM is the first image translation framework to achieve simultaneous dualdomain sampling with the same model and parameters, based on Brownian Bridge diffusion processes. Extensive experimental results on various benchmarks demonstrate that BiBBDM achieves competitive performance, as evidenced by both visual inspection and quantitative metrics.},
  archive      = {J_TPAMI},
  author       = {Kaitao Xue and Bo Li and Ziyi Liu and Zhifen He and Bin Liu and Congxuan Zhang and Yu-Kun Lai},
  doi          = {10.1109/TPAMI.2025.3597667},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BiBBDM: Bidirectional image translation with brownian bridge diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable programmatic weak supervision with confidence intervals for label probabilities. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3597508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate labeling of datasets is often both costly and time-consuming. Given an unlabeled dataset, programmatic weak supervision obtains probabilistic predictions for the labels by leveraging multiple weak labeling functions (LFs) that provide rough guesses for labels. Weak LFs commonly provide guesses with assorted types and unknown interdependences that can result in unreliable predictions. Furthermore, existing techniques for programmatic weak supervision cannot provide assessments for the reliability of the probabilistic predictions for labels. This paper presents a methodology for programmatic weak supervision that can provide confidence intervals for label probabilities and obtain more reliable predictions. In particular, the methods proposed use uncertainty sets of distributions that encapsulate the information provided by LFs with unrestricted behavior and typology. Experiments on multiple benchmark datasets show the improvement of the presented methods over the state-of-the-art and the practicality of the confidence intervals presented.},
  archive      = {J_TPAMI},
  author       = {Ver´onica Alvarez and Santiago Mazuelas and Steven An and Sanjoy Dasgupta},
  doi          = {10.1109/TPAMI.2025.3597508},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reliable programmatic weak supervision with confidence intervals for label probabilities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLPose: Bridging the domain gap in pose estimation with language-vision tuning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios. However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality. With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient. Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies. Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose. VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains. Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies.},
  archive      = {J_TPAMI},
  author       = {Jingyao Li and Pengguang Chen and Xuan Ju and Shu Liu and Hong Xu and Jiaya Jia},
  doi          = {10.1109/TPAMI.2025.3594097},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VLPose: Bridging the domain gap in pose estimation with language-vision tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPT4Point++: Advancing unified point-language understanding and generation. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3597938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have made significant progress in 2D image-text tasks, but the 3D domain remains challenging. To bridge this gap, we introduce GPT4Point and its enhanced version, GPT4Point++, both of which are pioneering point-language multimodal models designed for 3D object understanding and generation. They excel in tasks such as 3D object recognition, 3D point cloud captioning and question answering. Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, and it can get high-quality results through a low-quality point-text feature that maintains geometric shapes and colors. GPT4Point's training consists of two stages: first, aligning point-text features, followed by integrating the LLM. Our advanced version GPT4Point++ simplifies this with a single, unified end-to-end training approach for improved performance. To support the substantial demand for 3D object-text pairs, we have developed Capverse, a point-language dataset annotation engine. Capverse constructs a large-scale database with diverse levels of text granularity by leveraging the Objaverse dataset. We established a comprehensive benchmark to assess 3D point-language understanding. Extensive evaluations show that GPT4Point and GPT4Point++ excel in both understanding and generation tasks. Additionally, GPT4Point effectively evaluates 3D object generation methods and demonstrates strong understanding of both individual objects and indoor scenes, highlighting its robustness. 3D Multimodal Large Model, 3D Object Recognition, 3D Object Generation. },
  archive      = {J_TPAMI},
  author       = {Zhangyang Qi and Ye Fang and Zeyi Sun and Xiaoyang Wu and Tong Wu and Jiaqi Wang and Dahua Lin and Hengshuang Zhao},
  doi          = {10.1109/TPAMI.2025.3597938},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GPT4Point++: Advancing unified point-language understanding and generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards the flatter landscape and better generalization in federated learning under client-level differential privacy. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3597922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-$top_{k}$ by adopting the local update sparsification technique. From the theoretical perspective, we present the convergence analysis to investigate how our algorithms mitigate the performance degradation induced by DP. Meanwhile, we give rigorous privacy guarantees with Rényi DP, the sensitivity analysis of local updates, and generalization analysis. At last, we empirically confirm that our algorithms achieve state-of-the-art (SOTA) performance compared with existing SOTA baselines in DPFL.},
  archive      = {J_TPAMI},
  author       = {Yifan Shi and Kang Wei and Li Shen and Yingqi Liu and Xueqian Wang and Bo Yuan and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3597922},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards the flatter landscape and better generalization in federated learning under client-level differential privacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenCIR: Conditional image repainting with open condition mixture. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3597936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce OpenCIR, a fullyfunctional Conditional Image Repainting (CIR) model designed for local image editing. Given an image and a combination of conditions related to geometry, texture, and color, CIR models are required to repaint instances and seamlessly composite them with the original images. Previous CIR models suffer from limited object categories, restricted condition modalities, and demanded geometry precision. In contrast, leveraging the generative priors from pre-trained models, OpenCIR could repaint open object categories. Equipped with redesigned condition injection modules and the condition extension strategy, OpenCIR is able to understand open condition modalities. Adopting the contour refinement strategy, OpenCIR allows users to specify instances with open geometry precision. In addition, we contribute the OPEN-CIR dataset, which includes detailed annotations, tailored for the comprehensive training and evaluation of the OpenCIR model. Extensive experiments demonstrate that OpenCIR outperforms relevant state-of-the-art methods, achieving superior visual quality, and more favorable results by human evaluators.},
  archive      = {J_TPAMI},
  author       = {Shuchen Weng and Xiaocheng Gong and Haojie Zheng and Xinlong Wang and Si Li and Boxin Shi},
  doi          = {10.1109/TPAMI.2025.3597936},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {OpenCIR: Conditional image repainting with open condition mixture},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoE-adapters++: Towards more efficient continual learning of vision-language models via dynamic mixture-of-experts adapters. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3597942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first propose MoE-Adapters, a parameter-efficient training framework to alleviate long-term forgetting issues in incremental learning with Vision-Language Models (VLM). Our MoE-Adapters leverages incrementally added routers to activate and integrate exclusive expert adapters from a pre-defined static expert set, enabling the pre-trained CLIP to efficiently adapt to new tasks. To preserve the zero-shot capability of VLM, a Distribution Discriminative Auto-Selector (DDAS) is introduced that automatically routes in-distribution and out-of-distribution inputs to the MoE-Adapters and the original CLIP, respectively. However, relying on a static expert set and a separate distribution selector can lead to parameter redundancy and increased training complexity. In response, we further extend an MoE-Adapters++ framework by introducing dynamic MoE-adapters, which allows experts to be adaptively involved during the continual learning process. Additionally, a Latent Embedding Auto-Selector (LEAS) is proposed that incorporates distribution selection within CLIP to create a more unified architecture. Extensive experiments across diverse settings demonstrate that the proposed method consistently surpasses previous state-of-the-art approaches while concurrently improving training efficiency.},
  archive      = {J_TPAMI},
  author       = {Jiazuo Yu and Zichen Huang and Yunzhi Zhuge and Lu Zhang and Ping Hu and Dong Wang and Huchuan Lu and You He},
  doi          = {10.1109/TPAMI.2025.3597942},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MoE-adapters++: Towards more efficient continual learning of vision-language models via dynamic mixture-of-experts adapters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PIT: A plug-and-play image translator for making off-the-shelf models adapt to corruptions. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual recognition models pretrained on clean images usually do not perform well in the presence of image corruptions, such as blurring or noise, which limits their applicability in real-world scenarios. To solve this problem, existing approaches usually design complex data augmentations to train a robust model from scratch or adapt a pretrained model to corrupted scenarios. These approaches ignore the existence of the large number of deployed models in our community, causing extensive computation and storage costs for making deployed models adapted. Based on this consideration, this paper focuses on solving a practical problem of making many clean-image-pretrained models adapt to unlabeled corrupted images through one training procedure. To this end, we aim to learn a Plug-and-play Image Translator (PIT) that can be directly combined with recognition models after training. Existing approaches, such as vanilla image translation and restoration, are not proper for solving this problem, as they are mostly based on supervised training and are not recognition-oriented. To address this issue, we propose a recognition-oriented unsupervised image translation framework to make PIT produce images with indistinguishable recognition predictions from the clean ones. We verify the effectiveness of PIT on several recognition tasks and show that PIT boosts the performance of clean-image-pretrained models significantly in the presence of image corruptions.},
  archive      = {J_TPAMI},
  author       = {Yinqi Li and Hong Chang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2025.3598147},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PIT: A plug-and-play image translator for making off-the-shelf models adapt to corruptions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EA-HAS-bench and language-enhanced shrinkage search for energy-aware NAS. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper takes a crucial step in the development of energy-aware (EA) NAS methods by offering a benchmark that enhances the reproducibility and accessibility of EA-NAS research. Specifically, we introduce EA-HAS-Bench, the first large-scale energy-aware benchmark designed to enable the study of AutoML methods in achieving improved trade-offs between performance and search energy consumption. EA-HAS-Bench offers a vast architecture/hyperparameter joint search space, encompassing diverse configurations relevant to energy consumption, and proposes a novel surrogate model based on Bézier curves for predicting learning curves with versatile shapes and lengths. On the other hand, recent studies have started integrating large language models (LLMs) into AutoML frameworks to enhance model search efficiency and configuration prediction, yet challenges remain in adapting these methods for energy-efficient searches across vast configuration spaces, as they often neglect energy consumption metrics. As a result, we introduce the Language-Enhanced Shrinkage Search (LESS), a plug-and-play method that utilizes the analytical capabilities of LLMs to enhance the energy efficiency of existing hyperparameter optimization techniques. Moreover, we adapt existing AutoML algorithms to construct baselines. Our experiments demonstrate that these modified energy-aware AutoML methods and LESS achieve an improved balance between energy consumption and model performance. The dataset and codebase of EA-HAS-Bench are available at https://github.com/microsoft/EA-HAS- Bench.},
  archive      = {J_TPAMI},
  author       = {Cairong Zhao and Shuguang Dou and Jiale Zhao and Xinyang Jiang and Junyao Gao and Yuge Zhang and Bo Li and Dongsheng Li},
  doi          = {10.1109/TPAMI.2025.3598206},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EA-HAS-bench and language-enhanced shrinkage search for energy-aware NAS},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demystifying chains, trees, and graphs of thoughts. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3598182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.},
  archive      = {J_TPAMI},
  author       = {Maciej Besta and Florim Memedi and Zhenyu Zhang and Robert Gerstenberger and Guangyuan Piao and Nils Blach and Piotr Nyczyk and Marcin Copik and Grzegorz Kwaśniewski and Jurgen Müller and Lukas Gianinazzi and Ales Kubicek and Hubert Niewiadomski and Aidan O'Mahony and Onur Mutlu and Torsten Hoefler},
  doi          = {10.1109/TPAMI.2025.3598182},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Demystifying chains, trees, and graphs of thoughts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient benchmarking via bias-bounded subset selection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating AI systems, particularly large models, is an essential yet computationally expensive task. The use of extensive benchmarks often leads to substantial computational/human costs that may even exceed those of pretraining. The efficiency of AI model evaluation focuses on estimating the model's score on the full benchmark based on its responses to a smaller subset. Various empirical selection methods have been proposed to identify valuable subsets within these benchmarks. In this paper, we formally define and approximate the subset selection problem inherent in efficient evaluation. We prove that this problem actually optimizes a submodular function and that a unified subset can be identified using a simple greedy algorithm. Importantly, this approach is the first to provide theoretical guarantees of bias control and generalizability in score estimation. Using language models as a case study, experimental results across 11 different benchmarks validate its superiority in estimating model scores and maintaining ranking consistency. It can achieve accurate score estimation using no more than 30% of the full benchmark, thus facilitating efficient and sparse benchmark design.},
  archive      = {J_TPAMI},
  author       = {Yan Zhuang and Junhao Yu and Qi Liu and Yuxuan Sun and Jiatong Li and Zhenya Huang and Enhong Chen},
  doi          = {10.1109/TPAMI.2025.3598031},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient benchmarking via bias-bounded subset selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on all-in-one image restoration: Taxonomy, evaluation and future trends. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3598132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration (IR) seeks to recover high-quality images from degraded observations caused by a wide range of factors, including noise, blur, compression, and adverse weather. While traditional IR methods have made notable progress by targeting individual degradation types, their specialization often comes at the cost of generalization, leaving them ill-equipped to handle the multifaceted distortions encountered in real-world applications. In response to this challenge, the all-in-one image restoration (AiOIR) paradigm has recently emerged, offering a unified framework that adeptly addresses multiple degradation types. These innovative models enhance the convenience and versatility by adaptively learning degradation-specific features while simultaneously leveraging shared knowledge across diverse corruptions. In this survey, we provide the first in-depth and systematic overview of AiOIR, delivering a structured taxonomy that categorizes existing methods by architectural designs, learning paradigms, and their core innovations. We systematically categorize current approaches and assess the challenges these models encounter, outlining research directions to propel this rapidly evolving field. To facilitate the evaluation of existing methods, we also consolidate widely-used datasets, evaluation protocols, and implementation practices, and compare and summarize the most advanced open-source models. As the first comprehensive review dedicated to AiOIR, this paper aims to map the conceptual landscape, synthesize prevailing techniques, and ignite further exploration toward more intelligent, unified, and adaptable visual restoration systems. A curated code repository is available at https://github.com/Harbinzzy/All-in-One-Image-Restoration-Survey.},
  archive      = {J_TPAMI},
  author       = {Junjun Jiang and Zengyuan Zuo and Gang Wu and Kui Jiang and Xianming Liu},
  doi          = {10.1109/TPAMI.2025.3598132},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey on all-in-one image restoration: Taxonomy, evaluation and future trends},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DXA-net: Dual-task cross-lingual alignment network for zero-shot cross-lingual spoken language understanding. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3597726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art zero-shot cross-lingual spoken language understanding (SLU) model utilizes cross-lingual unsupervised contrastive learning to achieve multilingual semantics alignment. While existing methods have achieved promising results, they still have two issues limiting cross-lingual knowledge transfer: (1) dual-task correlative knowledge is not explicitly modeled and transferred to target languages; (2) the semantics differences among samples are ignored, and the contrastive semantics knowledge is not transferred to target languages. In this paper, we propose a dual-task cross-lingual alignment network (DXA-Net), which makes the first attempt to tackle zero-shot cross-lingual SLU based on the prompt-tuning paradigm. To solve the first issue, we propose the co-guiding prompt, which allows the model to conditionally generate one task's label based on another one's. To solve the second issue, we propose the intent/slot contrastive prompt to teach the model to discriminate whether a pair of samples have the same or similar labels. Additionally, we propose multilingual semantics contrastive prompt to enhance multilingual semantics alignment. Experiments on the benchmark show that our model achieves new state-of-the-art performance on nine languages.},
  archive      = {J_TPAMI},
  author       = {Bowen Xing and Libo Qin and Zhihong Zhu and Zhou Yu and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2025.3597726},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DXA-net: Dual-task cross-lingual alignment network for zero-shot cross-lingual spoken language understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLC++: Source-free universal domain adaptation through global-local clustering and contrastive affinity learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3593669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify ”known” data belonging to common categories and segregate them from target-private ”unknown” data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of ”unknown” data, impeding the identification of distinct “unknown” categories. To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy. We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.8% and 18.9% in H-score on VisDA, respectively. GLC++ enhances the novel category clustering accuracy of GLC by 4.1% in open-set scenarios on Office-Home. Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies.},
  archive      = {J_TPAMI},
  author       = {Sanqing Qu and Tianpei Zou and Florian Röhrbein and Cewu Lu and Guang Chen and Dacheng Tao and Changjun Jiang},
  doi          = {10.1109/TPAMI.2025.3593669},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GLC++: Source-free universal domain adaptation through global-local clustering and contrastive affinity learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised skeleton representation learning via actionlet contrast and reconstruct. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3598138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has shown remarkable success in the domain of skeleton-based action recognition. However, the design of data transformations, which is crucial for effective contrastive learning, remains a challenging aspect in the context of skeleton-based action recognition. The difficulty lies in creating data transformations that capture rich motion patterns while ensuring that the transformed data retains the same semantic information. To tackle this challenge, we introduce an innovative framework called ActCLR+ (Actionlet-Dependent Contrastive Learning), which explicitly distinguishes between static and dynamic regions in a skeleton sequence. We begin by introducing the concept of actionlet, connecting self-supervised learning quantitatively with downstream tasks. Actionlets represent regions in the skeleton where features closely align with action prototypes, highlighting dynamic sequences as distinct from static ones. We propose an anchor-based method for unsupervised actionlet discovery, establishing a motion-adaptive data transformation approach based on this discovery. This motion-adaptive data transformation strategy tailors data transformations for actionlet and non-actionlet regions, respectively, introducing more diverse motion patterns while preserving the original motion semantics. Additionally, we incorporate a semantic-aware masked motion modeling technique to enhance the learning of actionlet representations. Our comprehensive experiments on well-established benchmark datasets such as NTU RGB+D and PKUMMD validate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Lilang Lin and Jiahang Zhang and Jiaying Liu},
  doi          = {10.1109/TPAMI.2025.3598138},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised skeleton representation learning via actionlet contrast and reconstruct},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view hand reconstruction with a point-embedded transformer. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3598089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel and generalizable multi-view Hand Mesh Reconstruction (HMR) model, named POEM, designed for practical use in real-world hand motion capture scenarios. The advances of the POEM model consist of two main aspects. First, concerning the modeling of the problem, we propose embedding a static basis point within the multi-view stereo space. A point represents a natural form of 3D information and serves as an ideal medium for fusing features across different views, given its varied projections across these views. Consequently, our method harnesses a simple yet effective idea: a complex 3D hand mesh can be represented by a set of 3D basis points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encompass the hand in it. The second advance lies in the training strategy. We utilize a combination of five large-scale multi-view datasets and employ randomization in the number, order, and poses of the cameras. By processing such a vast amount of data and a diverse array of camera configurations, our model demonstrates notable generalizability in the real-world applications. As a result, POEM presents a highly practical, plug-and-play solution that enables user-friendly, cost-effective multi-view motion capture for both left and right hands. The model and source codes are available at https://github.com/JubSteven/POEM-v2.},
  archive      = {J_TPAMI},
  author       = {Lixin Yang and Licheng Zhong and Pengxiang Zhu and Xinyu Zhan and Junxiao Kong and Jian Xu and Cewu Lu},
  doi          = {10.1109/TPAMI.2025.3598089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-view hand reconstruction with a point-embedded transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth dynamics via one-bit frequency probing in embedded direct time-of-flight sensing. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3598593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-of-flight (ToF) sensors with single-photon avalanche diodes (SPADs) estimate depth by accumulating a histogram of photon return times, which discards the timing information required to measure depth dynamics, such as vibrations or transient motions. We introduce a method that transforms a direct ToF sensor into a depth frequency analyzer capable of measuring high-frequency motion and transient events using only lightweight, on-sensor computations. By replacing conventional discrete Fourier transforms (DFTs) with one-bit probing sinusoids generated via oversampled sigma-delta modulation, we enable in-pixel frequency analysis without multipliers or floating-point operations. We extend the lightweight analysis of depth dynamics to Haar wavelets for time-localized detection of brief, non-repetitive depth changes. We validate our approach through simulation and hardware experiments, showing that it achieves noise performance approaching that of full-resolution DFTs, detects sub-millimeter motions above 6 kHz, and localizes millisecond-scale transients. Using a laboratory ToF setup, we demonstrate applications in oscillatory motion analysis and depth edge detection. This work has the potential to enable a new class of compact, motion-aware ToF sensors for embedded deployment in industrial predictive maintenance, structural health monitoring, robotic perception, and dynamic scene understanding.},
  archive      = {J_TPAMI},
  author       = {Seth Lindgren and Benjamin R. Johnson and Lucas J. Koerner},
  doi          = {10.1109/TPAMI.2025.3598593},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Depth dynamics via one-bit frequency probing in embedded direct time-of-flight sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video diffusion posterior sampling for seeing beyond dynamic scattering layers. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3598457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging through scattering is challenging, as even a thin layer can randomly perturb light propagation and obscure hidden objects. Accurate closed-form modeling of forward scattering remains difficult, particularly for dynamically varying or thick layers. Here, we introduce a plug-and-play inverse solver based on video diffusion models with a physically grounded forward model tailored to dynamic scattering layers. Our method extends Diffusion Posterior Sampling (DPS) to the spatio-temporal domain, thereby capturing statistical correlations between video frames and scattered signals more effectively. Leveraging these temporal correlations, our approach recovers high-resolution spatial details that spatial-only methods typically fail to reconstruct. We also propose an inference-time optimization with a lightweight mapping network, enabling joint estimation of low-dimensional forward-model parameters without additional training. This joint optimization significantly enhances adaptability to unknown, time-varying degradations, making our method suitable for blind inverse scattering problems. We validate across diverse conditions, including different scene types, layer thicknesses, and scene-layer distances. And real-world experiments using multiple datasets confirm the robustness and effectiveness of our approach, even under real noise and forward-model approximation mismatches. Finally, we validate our method as a general video-restoration framework across dehazing, deblurring, inpainting, and blind restoration under complex optical aberrations. Our implementation is available at: https://github.com/star-kwon/VDPS.},
  archive      = {J_TPAMI},
  author       = {Taesung Kwon and Gookho Song and Yoosun Kim and Jeongsol Kim and Jong Chul Ye and Mooseok Jang},
  doi          = {10.1109/TPAMI.2025.3598457},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video diffusion posterior sampling for seeing beyond dynamic scattering layers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPARE: Symmetrized point-to-plane distance for robust non-rigid 3D registration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing optimization-based methods for non-rigid registration typically minimize an alignment error metric based on the point-to-point or point-to-plane distance between corresponding point pairs on the source surface and target surface. However, these metrics can result in slow convergence or a loss of detail. In this paper, we propose SPARE, a novel formulation that utilizes a symmetrized point-to-plane distance for robust non-rigid registration. The symmetrized point-to-plane distance relies on both the positions and normals of the corresponding points, resulting in a more accurate approximation of the underlying geometry and can achieve higher accuracy than existing methods. To solve this optimization problem efficiently, we introduce an as-rigid-as-possible regulation term to estimate the deformed normals and propose an alternating minimization solver using a majorization-minimization strategy. Moreover, for effective initialization of the solver, we incorporate a deformation graph-based coarse alignment that improves registration quality and efficiency. Extensive experiments show that the proposed method greatly improves the accuracy of non-rigid registration problems and maintains relatively high solution efficiency. The code is publicly available at https://github.com/yaoyx689/spare.},
  archive      = {J_TPAMI},
  author       = {Yuxin Yao and Bailin Deng and Junhui Hou and Juyong Zhang},
  doi          = {10.1109/TPAMI.2025.3598630},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SPARE: Symmetrized point-to-plane distance for robust non-rigid 3D registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying semantic component for robust molecular property prediction. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although graph neural networks have achieved great success in the task of molecular property prediction in recent years, their generalization ability under out-of-distribution (OOD) settings is still under-explored. Most of the existing methods rely on learning discriminative representations for prediction, often assuming that the underlying semantic components are correctly identified. However, this assumption does not always hold, leading to potential misidentifications that affect model robustness. Different from these discriminative-based methods, we propose a generative model to ensure the Semantic-Components Identifiability, named SCI. We demonstrate that the latent variables in this generative model can be explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI) components, which contributes to better OOD generalization by involving minimal change properties of causal mechanisms. Specifically, we first formulate the data generation process from the atom level to the molecular level, where the latent space is split into SI substructures, SR substructures, and SR atom variables. Sequentially, to reduce misidentification, we restrict the minimal changes of the SR atom variables and add a semantic latent substructure regularization to mitigate the variance of the SR substructure under augmented domain changes. Under mild assumptions, we prove the block-wise identifiability of the SR substructure and the comment-wise identifiability of SR atom variables. Experimental studies achieve state-of-the-art performance and show general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the visualization results of the proposed SCI method provide insightful case studies and explanations for the prediction results.},
  archive      = {J_TPAMI},
  author       = {Zijian Li and Zunhong Xu and Ruichu Cai and Zhenhui Yang and Yuguang Yan and Zhifeng Hao and Guangyi Chen and Kun Zhang},
  doi          = {10.1109/TPAMI.2025.3598461},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Identifying semantic component for robust molecular property prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit correspondence matching for generalizable neural radiance fields. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3598711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new generalizable NeRF method that is able to directly generalize to new unseen scenarios and perform novel view synthesis with as few as two source views. The key to our approach lies in the explicitly modeled correspondence matching information, so as to provide the geometry prior to the prediction of NeRF color and density for volume rendering. The explicit correspondence matching is quantified with the cosine similarity between image features sampled at the 2D projections of a 3D point on different views, which is able to provide reliable cues about the surface geometry. Unlike previous methods where image features are extracted independently for each view, we consider modeling the cross-view interactions via Transformer cross-attention, which greatly improves the feature matching quality. Our method achieves state-of-the-art results on different evaluation settings, with the experiments showing a strong correlation between our learned cosine feature similarity and volume density, demonstrating the effectiveness and superiority of our proposed method. Code and pretrained weights are at https://github.com/donydchen/matchnerf.},
  archive      = {J_TPAMI},
  author       = {Yuedong Chen and Haofei Xu and Qianyi Wu and Chuanxia Zheng and Tat-Jen Cham and Jianfei Cai},
  doi          = {10.1109/TPAMI.2025.3598711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Explicit correspondence matching for generalizable neural radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HHAvatar: Gaussian head avatar with dynamic hairs. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3597940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head. Project page: https://liaozhanfeng.github.io/HHAvatar},
  archive      = {J_TPAMI},
  author       = {Zhanfeng Liao and Yuelang Xu and Zhe Li and Qijing Li and Boyao Zhou and Ruifeng Bai and Di Xu and Hongwen Zhang and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3597940},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HHAvatar: Gaussian head avatar with dynamic hairs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised 3D object detection by commonsense clue. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D object detectors, whether fully-, semi-, or weakly-supervised, rely heavily on extensive human annotations. In contrast, this paper introduces an unsupervised 3D object detector that automatically discerns object patterns without such annotations. To achieve this, we propose a Commonsense Prototype-based Detector (CPD) for unsupervised 3D object detection. CPD first constructs Commonsense Prototypes (CProto) to represent the geometric center and size of objects. It then generates high-quality pseudo-labels and guides detector convergence using size and geometry priors from CProto. Building on CPD, we further introduce CPD++, an enhanced version that improves performance by leveraging motion cues. CPD++ learns localization from stationary objects and recognition from moving objects, facilitating the mutual transfer of localization and recognition knowledge between these two object types. Both CPD and CPD++ outperform existing state-of-the-art unsupervised 3D detectors. Furthermore, when trained on Waymo Open Dataset (WOD) and tested on KITTI, CPD++ achieves 89.25% 3D Average Precision (AP) on the moderate car class at a 0.5 IoU threshold, reaching 95.3% of the performance attained by fully supervised counterparts. These results underscore the significant advancements brought by our method.},
  archive      = {J_TPAMI},
  author       = {Hai Wu and Shijia Zhao and Xun Huang and Qiming Xia and Chenglu Wen and Li Jiang and Xin Li and Cheng Wang},
  doi          = {10.1109/TPAMI.2025.3598341},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised 3D object detection by commonsense clue},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dark noise diffusion: Noise synthesis for low-light image denoising. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3598330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light photography produces images with low signal-to-noise ratios due to limited photons. In such conditions, common approximations like the Gaussian noise model fall short, and many denoising techniques fail to remove noise effectively. Although deep-learning methods perform well, they require large datasets of paired images that are impractical to acquire. As a remedy, synthesizing realistic low-light noise has gained significant attention. In this paper, we investigate the ability of diffusion models to capture the complex distribution of low-light noise. We show that a naive application of conventional diffusion models is inadequate for this task and propose three key adaptations that enable high-precision noise generation: a two-branch architecture to better model signal-dependent and signal-independent noise, the incorporation of positional information to capture fixed-pattern noise, and a tailored diffusion noise schedule. Consequently, our model enables the generation of large datasets for training low-light denoising networks, leading to state-of-the-art performance. Through comprehensive analysis, including statistical evaluation and noise decomposition, we provide deeper insights into the characteristics of the generated data.},
  archive      = {J_TPAMI},
  author       = {Liying Lu and Raphael Achddou and Sabine Susstrunk},
  doi          = {10.1109/TPAMI.2025.3598330},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dark noise diffusion: Noise synthesis for low-light image denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamCraft3D++: Efficient hierarchical 3D generation with multi-plane reconstruction model. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3598772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce DreamCraft3D++, an extension of DreamCraft3D that enables efficient high-quality generation of complex 3D assets. DreamCraft3D++ inherits the multi-stage generation process of DreamCraft3D, but replaces the time-consuming geometry sculpting optimization with a feed-forward multi-plane based reconstruction model, speeding up the process by 1000x. For texture refinement, we propose a training-free IP-Adapter module that is conditioned on the enhanced multi-view images to enhance texture and geometry consistency, providing a 4x faster alternative to DreamCraft3D's DreamBooth fine-tuning. Experiments on diverse datasets demonstrate DreamCraft3D++'s ability to generate creative 3D assets with intricate geometry and realistic 360° textures, outperforming state-of-the-art image-to-3D methods in quality and speed. The full implementation will be open-sourced to enable new possibilities in 3D content creation.},
  archive      = {J_TPAMI},
  author       = {Jingxiang Sun and Cheng Peng and Ruizhi Shao and Yuan-Chen Guo and Xiaochen Zhao and Yangguang Li and YanPei Cao and Bo Zhang and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3598772},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamCraft3D++: Efficient hierarchical 3D generation with multi-plane reconstruction model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting important photons for energy-efficient single-photon videography. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3598767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-photon avalanche diodes (SPAD) detect individual photons with fine temporal resolutions, enabling capabilities like imaging in near-total darkness, extreme dynamic range, and rapid motion. Due to these capabilities, and coupled with the recent emergence of high-resolution (> 1MP) arrays, SPADs have the potential to become workhorses for computer vision systems of the future that need to operate in a wide range of challenging conditions. However, SPADs' sensitivity comes at a high energy cost due to the underlying avalanche process, which consumes substantial energy per detected photon, limiting the scalability and practicality of high-resolution SPAD arrays. To address this, we propose approaches to predict and sample only the most salient photons for a given vision task. To this end, we design computationally lightweight photon-sampling strategies that allocate energy resources for detecting photons only in areas with significant motion and spatial variation, while continually adapting to changing signals. We demonstrate the effectiveness of the proposed methods in recovering comparable video to a fully-sampled SPAD capture using only a small fraction of the photons (up to 10× fewer), across diverse real-world scenes with motion, high dynamic range, and varying light conditions},
  archive      = {J_TPAMI},
  author       = {Shantanu Gupta and Varun Sundar and Lucas J. Koerner and Claudio Bruschini and Edoardo Charbon and Mohit Gupta},
  doi          = {10.1109/TPAMI.2025.3598767},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Predicting important photons for energy-efficient single-photon videography},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VisionUnite: A vision-language foundation model for ophthalmology enhanced with clinical knowledge. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3598734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for improved diagnostic methods in ophthalmology is acute, especially in the underdeveloped regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multidisease diagnosis, clinical explanation, and patient interaction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational aid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and underrepresented ophthalmic conditions. VisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and understanding of disease mechanisms. The source code is at https://github.com/HUANGLIZI/VisionUnite.},
  archive      = {J_TPAMI},
  author       = {Zihan Li and Diping Song and Zefeng Yang and Deming Wang and Fei Li and Xiulan Zhang and Paul E. Kinahan and Yu Qiao},
  doi          = {10.1109/TPAMI.2025.3598734},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VisionUnite: A vision-language foundation model for ophthalmology enhanced with clinical knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TherNet: Thermal segmentation network harnessing physical properties. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3598949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise segmentation of thermal infrared images is crucial in domains like surveillance, medical diagnostics, intelligent transportation, accurate guidance and remote sensing. However, current thermal segmentation methods often oversimplify by treating thermal images as grayscale, neglecting vital physical factors such as thermal imaging effects and material information, thereby constraining segmentation precision. To address these limitations, we propose TherNet, a novel thermal infrared segmentation framework integrating thermal imaging effects and material physical information. The study elucidates the impacts of object radiation, inter-object thermal exchange, atmospheric scattering, and camera thermal inertia on thermal infrared imaging, developing four modules to model or rectify these physical processes. To validate the proposed framework, two large-scale infrared datasets were created: TI-Cityscapes for multi-class semantic segmentation in traffic scenes (4,200 frames, 18 classes), and TBRSD for single-object blind road segmentation (5,180 frames from a pedestrian perspective). The proposed methods achieved SoTA performance across three infrared semantic segmentation datasets and the blind road segmentation dataset, underscoring the pivotal role of leveraging physical properties. TherNet provides innovative perspectives and robust benchmarks for future developments in the domain.},
  archive      = {J_TPAMI},
  author       = {Junzhang Chen and Shihao Shu and Cai Meng and Xiangzhi Bai},
  doi          = {10.1109/TPAMI.2025.3598949},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TherNet: Thermal segmentation network harnessing physical properties},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-aware coding function design for compressive single-photon 3D cameras. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3599073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-photon cameras are becoming increasingly popular in time-of-flight 3D imaging because they can time-tag individual photons with extreme resolution. However, their performance is susceptible to hardware limitations, such as system bandwidth, maximum laser power, sensor data rates, and in-sensor memory and compute resources. Compressive histograms were recently introduced as a solution to data rates through an online in-sensor compression of photon timestamp data. Although compressive histograms work within limited in-sensor memory and computation, they underperform when subjected to real-world illumination hardware constraints.To address this, we present a constrained optimization approach for designing practical coding functions for compressive single-photon 3D imaging. Using gradient descent, we jointly optimize an illumination and coding matrix that adheres to hardware constraints. We show through extensive simulations that our coding functions consistently outperform traditional coding designs under both bandwidth and peak power constraints. This advantage is particularly pronounced in systems constrained by peak power. Finally, we show that our approach adapts to arbitrary parameterized impulse responses by evaluating it on a real-world system with a non-ideal impulse response function.},
  archive      = {J_TPAMI},
  author       = {David Parra and Felipe Gutierrez-Barragan and Trevor Seets and Andreas Velten},
  doi          = {10.1109/TPAMI.2025.3599073},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hardware-aware coding function design for compressive single-photon 3D cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured light with a million light planes per second. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3599143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a structured light system that enables full-frame 3D scanning at speeds of 1000 fps, four times faster than the previous fastest systems. Our key innovation is the use of a custom acousto-optic light scanning device capable of projecting two million light planes per second. Coupling this device with an event camera allows our system to overcome the key bottleneck preventing previous structured light systems based on event cameras from achieving higher scanning speeds—the limited rate of illumination steering. Unlike these previous systems, ours uses the event camera's full-frame bandwidth, shifting the speed bottleneck from the illumination side to the imaging side. To mitigate this new bottleneck and further increase scanning speed, we introduce adaptive scanning strategies that leverage the event camera's asynchronous operation by selectively illuminating regions of interest, thereby achieving effective scanning speeds an order of magnitude beyond the camera's theoretical limit.},
  archive      = {J_TPAMI},
  author       = {Dhawal Sirikonda and Praneeth Chakravarthula and Ioannis Gkioulekas and Adithya Pediredla},
  doi          = {10.1109/TPAMI.2025.3599143},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structured light with a million light planes per second},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised graph embedding clustering. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3599185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning and $K$-means are two powerful techniques for data analysis in the field of artificial intelligence. When used for label learning, a promising strategy is to combine them directly and optimize both models simultaneously. However, a significant drawback of this approach is that it represents a naive and crude integration, requiring the optimization of all variables in both models without achieving a truly essential combination. Additionally, it introduces an extra hyperparameter and cannot ensure cluster balance. These challenges motivate us to explore whether a meaningful integration can be developed for dimensionality reduction clustering. In this paper, we propose a novel self-supervised manifold clustering framework that reformulates the two models into a unified framework, eliminating the need for additional hyperparameters while achieving dimensionality reduction clustering. Specifically, by analyzing the relationship between $K$-means and manifold learning, we construct a meaningful low-dimensional manifold clustering model that directly produces the label matrix of the data. The label information is then used to guide the learning of the manifold structure, ensuring consistency between the manifold structure and the labels. Notably, we identify a valuable role of ${\ell _{2,p}}$-norm regularization in clustering: maximizing the ${\ell _{2,p}}$-norm naturally maintains class balance during clustering, and we provide a theoretical proof of this property. Extensive experimental results demonstrate the efficiency of our proposed model.},
  archive      = {J_TPAMI},
  author       = {Fangfang Li and Quanxue Gao and Xiaoke Ma and Ming Yang and Cheng Deng},
  doi          = {10.1109/TPAMI.2025.3599185},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised graph embedding clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling up multimodal pre-training for sign language understanding. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3599313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language pre-training (SLP) has significantly improved the performance of diverse sign language understanding (SLU) tasks. However, many existing methods employ pre-training techniques that are tailored to a specific task with small data scale, resulting in limited model generalization. Some others focus solely on exploring visual cues, neglecting semantically textual cues embedded in sign translation texts. These limitations inherently diminish the representative capacity of pre-trained models. To this end, we present a multimodal SLP framework to leverage rich visual contextual information and vision-language semantic consistency with massively available data to enhance the representative capability of sign language video. Specifically, we first curate a large-scale text-labeled sign pose dataset ($\sim$ 1.5M), namely SL-1.5M, from various sources to alleviate the scarcity of pre-training data. Subsequently, we propose a pre-training framework, which integrates sign-text contrastive learning with masked pose modeling as the pretext task. In this way, our framework is empowered to effectively capture contextual cues within sign pose sequences and learn visual representation by aligning semantical text-rich features in a latent space. Moreover, in order to grasp the comprehensive meaning of sign language videos, we concurrently model manual and non-manual information to ensure the holistic integrity of visual content. To validate the generalization and superiority of our proposed pre-trained framework, we conduct extensive experiments without intricate design on diverse SLU tasks, achieving new state-of-the-art performance on multiple benchmarks.},
  archive      = {J_TPAMI},
  author       = {Wengang Zhou and Weichao Zhao and Hezhen Hu and Zecheng Li and Houqiang Li},
  doi          = {10.1109/TPAMI.2025.3599313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scaling up multimodal pre-training for sign language understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LRQuant: A unified and learnable framework to post-training quantization for transformer-based large foundation models. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3599479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-training quantization (PTQ) for transformer-based large foundation models (LFMs) significantly accelerates model inference and relieves memory constraints, without incurring model training. However, existing methods face three main issues: 1) The scaling factors, which are commonly used in scale reparameterization based weight-activation quantization for mitigating the quantization errors, are mostly hand-crafted defined which may lead to suboptimal results; 2) The formulation of current quantization error defined by L2-norm ignores the directional shifts after quantization; 3) Most methods are devised tailored for single scenario, i.e., only evaluated on LLMs or only designed for weight-only quantization, which lacks of a comprehensive evaluation on diverse benchmarks and a broad application scope. To address these challenges, this paper introduces a unified Learnable and Robust post-training Quantization framework for transformer based LFMs and various quantization scenarios, called LRQuant. Firstly, we consider an efficient block-wise learnable paradigm to find optimal scaling factors which are initialized by logarithmic activation equivalent and get suitable clipping range of quantization steps. In addition, we empirically find that only relying on MSE loss could hardly lead to optimal quantization results, so we reformulate the quantization error and then propose a novel loss function based on the negative logarithm of cosine similarity (NLC loss) between outputs of full-precision and quantized block. To fully investigate the potentiality of our learnable paradigm, we propose a more superior version LRQuant+. Specifically, we first propose a dynamically weighted scheme to balance MSE and NLC loss, and then devise learnable rotation vectors to further directly reduce directional gaps. In addition, we improve the block-wise optimization framework into a novel two-branch nature which jointly considers the error propagation and homologous reconstruction error. Extensive experiments demonstrate the superiority of our LRQuant and LRQuant+, as well as their unified effectiveness across various LFMs for both weight-activation and weight-only quantization, especially under challenging quantization scenarios, i.e., W4A4 and W2A16 on LLMs, ViTS, and MLLMs. Codes are available at https://github.com/zjq0455/LRQuant.},
  archive      = {J_TPAMI},
  author       = {Jiaqi Zhao and Chao Zeng and Ming Wang and Linxuan Han and Yuzhang Shang and Miao Zhang and Liqiang Nie},
  doi          = {10.1109/TPAMI.2025.3599479},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LRQuant: A unified and learnable framework to post-training quantization for transformer-based large foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular videos. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3599415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate reconstruction of a relightable, dynamic clothed human avatar from a monocular video is crucial for the entertainment industry. This paper presents SGIA (Surfel-based Gaussian Inverse Avatar), which introduces efficient training and rendering for relightable dynamic human reconstruction. SGIA advances previous Gaussian Avatar methods by comprehensively modeling Physically-Based Rendering (PBR) properties for clothed human avatars, allowing for the manipulation of avatars into novel poses under diverse lighting conditions. Specifically, our approach integrates pre-integration and image-based lighting for fast light calculations that surpass the performance of existing implicit-based techniques. To address challenges related to material lighting disentanglement and accurate geometry reconstruction, we propose an innovative occlusion approximation strategy and a progressive training approach. Extensive experiments demonstrate that SGIA not only achieves highly accurate physical properties but also significantly enhances the realistic relighting of dynamic human avatars, providing a substantial speed advantage. We exhibit more results in our project page: https://GS-IA.github.io.},
  archive      = {J_TPAMI},
  author       = {Yiqun Zhao and Chenming Wu and Binbin Huang and Yihao Zhi and Chen Zhao and Jingdong Wang and Shenghua Gao},
  doi          = {10.1109/TPAMI.2025.3599415},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remote sensing image generation via object text decoupling. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3599520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images usually reveal various objects with complex structures and different locations within vast ground area backgrounds. That leads to a major challenge for conventional generative models in handling remote sensing objects with correct shapes and clear textures. Integrating additional object-level controls can be a potential solution to improve generation quality, yet previous approaches inject the object-related conditions by specifying their locations, causing a limitation in object layout in generated results. To enable high object fidelity, high layout diversity and object customizable generation for remote sensing images, we propose a remote sensing image generation via object text decoupling, namely OTD-GAN. OTD-GAN takes advantage of the inherent text-toimage generation procedure and adaptively integrates the decoupled textual representations of visual objects into the global captions, thus achieving object-level controls without layout restrictions. Specifically, we design an object text decoupling module to predict a semantically consistent textual representation for each object. By decoupling the textual representation into a class invariant part and an object specific part, the converted representation is able to catch general semantics for similar objects as well as differentiated details for individual objects. After that, we use an object text semantic enhancement module to fuse the obtained object text representations with the global captions to enrich the object-related semantics within the textual modality. As a result, the generator will benefit from the object conditions and reinforce the generation quality while remaining flexible to create diverse layouts. Extensive experiments on remote sensing image-caption datasets including NWPU-Captions and RSICD demonstrate that our method achieves leading performance compared to existing state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Wenda Zhao and Zhepu Zhang and Fan Zhao and Haipeng Wang and You He and Huchuan Lu},
  doi          = {10.1109/TPAMI.2025.3599520},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Remote sensing image generation via object text decoupling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced computational complexity in continuous-depth models: Neural ordinary differential equations with trainable numerical schemes. <em>TPAMI</em>, 1-8. (<a href='https://doi.org/10.1109/TPAMI.2025.3599629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Ordinary Differential Equations (NODEs) serve as continuous-time analogs of residual networks. They provide a system-theoretic perspective on neural network architecture design and offer natural solutions for time series modeling, forecasting, and applications where invertible neural networks are essential. However, these models suffer from slow performance due to heavy numerical solver overhead. For instance, a popular solution for training and inference of NODEs consists in using adaptive step size solvers such as the popular Dormand–Prince 5(4) (DOPRI). These solvers dynamically adjust the Number of Function Evaluations (NFE) as the equation fits the training data and becomes more complex. However, this comes at the cost of an increased number of function evaluations, which reduces computational efficiency. In this work, we propose a novel approach: making the parameters of the numerical integration scheme trainable. By doing so, the numerical scheme dynamically adapts to the dynamics of the NODE, resulting in a model that operates with a fixed NFE. We compare the proposed trainable solvers with state-of-the-art approaches, including DOPRI, for different benchmarks, including classification, density estimation, and dynamical system modeling. Overall, we report a state-of-the-art performance for all benchmarks in terms of accuracy metrics, while enhancing the computational efficiency through trainable fixed-step-size solvers. This work opens up new possibilities for practical and efficient modeling applications with NODEs.},
  archive      = {J_TPAMI},
  author       = {Said Ouala and Laurent Debreu and Bertrand Chapron and Fabrice Collard and Lucile Gaultier and Ronan Fablet},
  doi          = {10.1109/TPAMI.2025.3599629},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced computational complexity in continuous-depth models: Neural ordinary differential equations with trainable numerical schemes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learned off-aperture encoding for wide field-of-view RGBD imaging. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3598340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end (E2E) designed imaging systems integrate coded optical designs with decoding algorithms to enhance imaging fidelity for diverse visual tasks. However, existing E2E designs encounter significant challenges in maintaining high image fidelity at wide fields of view, due to high computational complexity, as well as difficulties in modeling off-axis wave propagation while accounting for off-axis aberrations. In particular, the common approach of placing the encoding element into the aperture or pupil plane results in only a global control of the wavefront. To overcome these limitations, this work explores an additional design choice by positioning a DOE off-aperture, enabling a spatial unmixing of the degrees of freedom and providing local control over the wavefront over the image plane. Our approach further leverages hybrid refractive-diffractive optical systems by linking differentiable ray and wave optics modeling, thereby optimizing depth imaging quality and demonstrating system versatility. Experimental results reveal that the off-aperture DOE enhances the imaging quality by over 5 dB in PSNR at a FoV of approximately 45° when paired with a simple thin lens, outperforming traditional on-aperture systems. Furthermore, we successfully recover color and depth information at nearly 28° FoV using off-aperture DOE configurations with compound optics. Physical prototypes for both applications validate the effectiveness and versatility of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Haoyu Wei and Xin Liu and Yuhui Liu and Qiang Fu and Wolfgang Heidrich and Edmund Y. Lam and Yifan Peng},
  doi          = {10.1109/TPAMI.2025.3598340},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learned off-aperture encoding for wide field-of-view RGBD imaging},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-step latent diffusion for underwater image restoration. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3599775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models—which encode strong priors on the geometry and depth of scenes—with an explicit scene decomposition—which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium/degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over $200\times$ faster than existing diffusion-based methods while offering $\sim 3 dB$ improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website https://tianfwang.github.io/slurpp/.},
  archive      = {J_TPAMI},
  author       = {Jiayi Wu and Tianfu Wang and Md Abu Bakr Siddique and Md Jahidul Islam and Cornelia Fermuller and Yiannis Aloimonos and Christopher A. Metzler},
  doi          = {10.1109/TPAMI.2025.3599775},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single-step latent diffusion for underwater image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable rotation-equivariant multiary-valued network for attribute obfuscation. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3599592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of preventing information leakage in neural networks, i.e., assuming that attackers have obtained intermediate-layer features of a neural network, and preventing attackers from inverting these features to the input with private information. We propose a generic method to slightly revise each arbitrary traditional neural network into a multiary-valued rotation-equivariant neural network (RENN) for preventing information leakage. Specifically, we convert realvalued features in the network into multi-ary features, and each element in the feature vector is a multi-ary number. We hide the input information into a certain phase of the multi-ary feature, and rotate the multi-ary feature for attribute obfuscation in the encryption process. The rotation axis and angle can be considered as the private key. In this way, even when attackers have obtained network parameters and intermediate-layer features, they still cannot extract input information without knowing the rotation information. More crucially, the encryption operation does not damage the spatial correlations between features, so that the encrypted features can be easily processed by convolution operations in the neural network without difficulties. In order to implement successful encryption and decryption, the RENN is designed to satisfy the rotation equivariance property. To this end, we propose a set of rules to revise classic operations in the neural network to ensure the rotation equivariance property. Besides, we prove that the $d$-ary RENN is downward compatible with the $d^{\prime }$-ary RENN when $d^{\prime } \lt d$. In experiments, the RENN significantly boosts the capacity of preventing information leakage, yet with only mild degradation of classification accuracy, compared to traditional neural networks. Besides, the computational cost is much less than the homomorphic encryption.},
  archive      = {J_TPAMI},
  author       = {Quanshi Zhang and Hao Zhang and Yiting Chen and Qihan Ren and Jie Ren and Xu Cheng and Liyao Xiang},
  doi          = {10.1109/TPAMI.2025.3599592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpretable rotation-equivariant multiary-valued network for attribute obfuscation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning at initialization – A sketching perspective. <em>TPAMI</em>, 1-10. (<a href='https://doi.org/10.1109/TPAMI.2025.3598343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lottery ticket hypothesis (LTH) has increased attention to pruning neural networks at initialization. We study this problem in the linear setting. We show that finding a sparse mask at initialization is equivalent to the sketching problem introduced for efficient matrix multiplication. This gives us tools to analyze the LTH problem and gain insights into it. Specifically, using the mask found at initialization, we bound the approximation error of the pruned linear model at the end of training. We theoretically justify previous empirical evidence that the search for sparse networks may be data independent. By using the sketching perspective, we suggest a generic improvement to existing algorithms for pruning at initialization, which we show to be beneficial in the data-independent case.},
  archive      = {J_TPAMI},
  author       = {Noga Bar and Raja Giryes},
  doi          = {10.1109/TPAMI.2025.3598343},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pruning at initialization – A sketching perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph memory learning: Imitating lifelong remembering and forgetting of brain networks. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3599898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data in real-world scenarios undergo rapid and frequent changes, making it challenging for existing graph models to effectively handle the continuous influx of new data and accommodate data withdrawal requests. The approach to frequently retraining graph models is resource intensive and impractical. To address this pressing challenge, this paper introduces a new concept of graph memory learning. Its core idea is to enable a graph model to selectively remember new knowledge but forget old knowledge. Building on this approach, the paper presents a novel graph memory learning framework - Brain-inspired Graph Memory Learning (BGML), inspired by brain network dynamics and function-structure coupling strategies. BGML incorporates a multi-granular hierarchical progressive learning mechanism rooted in feature graph grain learning to mitigate potential conflict between memorization and forgetting in graph memory learning. This mechanism allows for a comprehensive and multi-level perception of local details within evolving graphs. In addition, to tackle the issue of unreliable structures in newly added incremental information, the paper introduces an information self-assessment ownership mechanism. This mechanism not only facilitates the propagation of incremental information within the model but also effectively preserves the integrity of past experiences. We design five types of graph memory learning tasks: regular, memory, unlearning, data-incremental, and class-incremental to evaluate BGML. Its excellent performance is confirmed through extensive experiments on multiple node classification datasets.},
  archive      = {J_TPAMI},
  author       = {Jiaxing Miao and Liang Hu and Qi Zhang and Longbing Cao},
  doi          = {10.1109/TPAMI.2025.3599898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph memory learning: Imitating lifelong remembering and forgetting of brain networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tomographic sparse view selection using the view covariance loss. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3600072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard computed tomography (CT) reconstruction algorithms such as filtered back projection (FBP) and Feldkamp-Davis-Kress (FDK) require many views for producing high-quality reconstructions, which can slow image acquisition and increase cost in non-destructive evaluation (NDE) applications. Over the past 20 years, a variety of methods have been developed for computing high-quality CT reconstructions from sparse views. However, the problem of how to select the best views for CT reconstruction remains open. In this paper, we present a novel view covariance loss (VCL) function that measures the joint information of a set of views by approximating the normalized mean squared error (NMSE) of the reconstruction. We present fast algorithms for computing the VCL along with an algorithm for selecting a subset of views that approximately minimizes its value. Our experiments on simulated and measured data indicate that for a fixed number of views our proposed view covariance loss selection (VCLS) algorithm results in reconstructions with lower NRMSE, fewer artifacts, and greater accuracy than current alternative approaches.},
  archive      = {J_TPAMI},
  author       = {Jingsong Lin and Amirkoushyar Ziabari and Singanallur V. Venkatakrishnan and Obaidullah Rahman and Gregery T. Buzzard and Charles A. Bouman},
  doi          = {10.1109/TPAMI.2025.3600072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tomographic sparse view selection using the view covariance loss},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model rectification with simultaneous incremental feature and partial label set. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3600033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional classification problems assume that features and labels are fixed. However, this assumption is easily violated in open environments. For example, the exponential growth of web pages leads to an expanding feature space with the accumulation of keywords. At the same time, rapid refresh makes it difficult to obtain accurate labels for web pages, often resulting in rough annotations containing potentially correct labels, i.e., partial label set. In such cases, the coupling between the incremental feature space and the partial label set introduces more complex real-world challenges, which deserve attention but have not been fully explored. In this paper, we address this issue by introducing a novel incremental learning approach with Simultaneous Incremental Feature and Partial Label (SIFPL). SIFPL models the data evolution in dynamic and open environments in a two-stage way, consisting of a previous stage and an adapting stage, to deal with the associated challenges. Specifically, to ensure the reusability of the model during adaptation, we impose classifier consistency constraints to enhance the stability of the current model. This constraint leverages historical information from the previous stage to improve the generalization ability of the current model, providing a reliable foundation for further refining the model with new features. Regarding label disambiguation, we filter out incorrect candidate labels based on the principle of minimizing classifier loss, ensuring that the new features and labels effectively support the model's adaptation to the incremental feature space, thereby further refining its performance. Furthermore, we also provide a solid theoretical analysis of the model's generalization bounds, which can validate the efficiency of model inheritance. Experiments on benchmark and real-world datasets validate that the proposed method achieves better accuracy performance than the baseline methods in most cases.},
  archive      = {J_TPAMI},
  author       = {Xijia Tang and Chao Xu and Chenping Hou},
  doi          = {10.1109/TPAMI.2025.3600033},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Model rectification with simultaneous incremental feature and partial label set},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-based comprehensive prompt learning for vision-language models. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3599830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper targets to learn multiple comprehensive text prompts that can describe the visual concepts from coarse to fine, thereby endowing pre-trained VLMs with better transfer ability to various downstream tasks. We focus on exploring this idea on transformer-based VLMs since this kind of architecture achieves more compelling performances than CNN-based ones. Unfortunately, unlike CNNs, the transformer-based visual encoder of pre-trained VLMs cannot naturally provide discriminative and representative local visual information. To solve this problem, we propose Frequency-based Comprehensive Prompt Learning (FCPrompt) to excavate representative local visual information from the redundant output features of the visual encoder. FCPrompt transforms these features into frequency domain via Discrete Cosine Transform (DCT). Taking the advantages of energy concentration and information orthogonality of DCT, we can obtain compact, informative and disentangled local visual information by leveraging specific frequency components of the transformed frequency features. To better fit with transformer architectures, FCPrompt further adopts and optimizes different text prompts to respectively align with the global and frequency-based local visual information via a dual-branch framework. Finally, the learned text prompts can thus describe the entire visual concepts from coarse to fine comprehensively. Extensive experiments indicate that FCPrompt achieves the state-of-the-art performances on various benchmarks. Code is available at https://github.com/llcllc1997/FCPrompt.},
  archive      = {J_TPAMI},
  author       = {Liangchen Liu and Nannan Wang and Chen Chen and Decheng Liu and Xi Yang and Xinbo Gao and Tongliang Liu},
  doi          = {10.1109/TPAMI.2025.3599830},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Frequency-based comprehensive prompt learning for vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rotation equivariant arbitrary-scale image super-resolution. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3600126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arbitrary-scale image super-resolution (ASISR), a recent popular topic in computer vision, aims to achieve arbitrary-scale high-resolution recoveries from a low-resolution input image. This task is realized by representing the image as a continuous implicit function through two fundamental modules, a deep-network-based encoder and an implicit neural representation (INR) module. Despite achieving notable progress, a crucial challenge of such a highly ill-posed setting is that many common geometric patterns, such as repetitive textures, edges, or shapes, are seriously warped and deformed in the low-resolution images, naturally leading to unexpected artifacts appearing in their high-resolution recoveries. Embedding rotation equivariance into the ASISR network is thus necessary, as it has been widely demonstrated that this enhancement enables the recovery to faithfully maintain the original orientations and structural integrity of geometric patterns underlying the input image. Motivated by this, we make efforts to construct a rotation equivariant ASISR method in this study. Specifically, we elaborately redesign the basic architectures of INR and encoder modules, incorporating intrinsic rotation equivariance capabilities beyond those of conventional ASISR networks. Through such amelioration, the ASISR network can, for the first time, be implemented with end-to-end rotational equivariance maintained from input to output. We also provide a solid theoretical analysis to evaluate its intrinsic equivariance error, demonstrating its inherent nature of embedding such an equivariance structure. The superiority of the proposed method is substantiated by experiments conducted on both simulated and real datasets. We also validate that the proposed framework can be readily integrated into current ASISR methods in a plug & play manner to further enhance their performance. Our code is available at https://github.com/XieQi2015/Equivariant-ASISR.},
  archive      = {J_TPAMI},
  author       = {Qi Xie and Jiahong Fu and Zongben Xu and Deyu Meng},
  doi          = {10.1109/TPAMI.2025.3600126},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rotation equivariant arbitrary-scale image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SKDF: A simple knowledge distillation framework for distilling open-vocabulary knowledge to open-world object detector. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open World Object Detection (OWOD) is a novel computer vision task with a considerable challenge, bridging the gap between classic object detection (OD) and real-world object detection. In addition to detecting and classifying seen/known objects, OWOD algorithms are expected to localize all potential unseen/unknown objects and incrementally learn them. The large pre-trained vision-language grounding models (VLM, e.g., GLIP) have rich knowledge about the open world, but are limited by text prompts and cannot localize indescribable objects. However, there are many detection scenarios in which pre-defined language descriptions are unavailable during inference. In this paper, we attempt to specialize the VLM model for OWOD tasks by distilling its open-world knowledge into a language-agnostic detector. Surprisingly, we observe that the simple knowledge distillation approach leads to unexpected performance for unknown object detection, even with a small amount of data. Unfortunately, knowledge distillation for unknown objects severely affects the learning of detectors with conventional structures, leading to catastrophic damage to the model's ability to learn about known objects. To alleviate these problems, we propose the down-weight training strategy for knowledge distillation from vision-language model to single visual modality one. Meanwhile, we propose the cascade decoupled decoders that decouple the learning of localization and recognition to reduce the impact of category interactions of known and unknown objects on the localization learning process. Ablation experiments demonstrate that both of them are effective in mitigating the impact of open-world knowledge distillation on the learning of known objects. Additionally, to alleviate the current lack of comprehensive benchmarks for evaluating the ability of the open-world detector to detect unknown objects in the open world, we refine the benchmark for evaluating the performance of unknown object detection by augmenting annotations for unknown objects which we name“IntensiveSet$\spadesuit$”. Comprehensive experiments performed on OWOD, MS-COCO, and our proposed benchmarks demonstrate the effectiveness of our methods.},
  archive      = {J_TPAMI},
  author       = {Shuailei Ma and Yuefeng Wang and Ying Wei and Enming Zhang and Jiaqi Fan and Xinyu Sun and Peihao Chen},
  doi          = {10.1109/TPAMI.2025.3600435},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SKDF: A simple knowledge distillation framework for distilling open-vocabulary knowledge to open-world object detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing satellites in 3D from amateur telescope images. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3599949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring space objects is crucial for space situational awareness, yet reconstructing 3D satellite models from ground-based telescope images is super challenging due to atmospheric turbulence, long observation distances, limited viewpoints, and low signal-to-noise ratios. In this paper, we propose a novel computational imaging framework that overcomes these obstacles by integrating a hybrid image pre-processing pipeline with a joint pose estimation and 3D reconstruction module based on controlled Gaussian Splatting (GS) and Branch-and-Bound (BnB) search. We validate our approach on both synthetic satellite datasets and on-sky observations of China's Tiangong Space Station and the International Space Station, achieving robust 3D reconstructions of low-Earth orbit satellites from ground-based data. Quantitative evaluations using SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method outperforms state-of-the-art NeRF-based approaches, and ablation studies confirm the critical role of each component. Our framework enables high-fidelity 3D satellite monitoring from Earth, offering a cost-effective alternative for space situational awareness. Project page: https://ai4scientificimaging.org/3DSatellites.},
  archive      = {J_TPAMI},
  author       = {Zhiming Chang and Boyang Liu and Yifei Xia and Youming Guo and Boxin Shi and He Sun},
  doi          = {10.1109/TPAMI.2025.3599949},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstructing satellites in 3D from amateur telescope images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MicroDreamer: Efficient 3D generation in $\sim$20 seconds by score-based iterative reconstruction. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3600494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample and the limitation of optimization confined to latent space. This paper introduces score-based iterative reconstruction (SIR), an efficient and general algorithm mimicking a differentiable 3D reconstruction process to reduce the NFEs and enable optimization in pixel space. Given a single set of images sampled from a multi-view score-based diffusion model, SIR repeatedly optimizes 3D parameters, unlike the single-step optimization in SDS. With other improvements in training, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks. In particular, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field while retaining a comparable performance and takes about 20 seconds to create meshes from 3D Gaussian splatting on a single A100 GPU, halving the time of the fastest optimization-based baseline DreamGaussian with significantly superior performance compared to the measurement standard deviation. Our code is available at https://github.com/ML-GSAI/MicroDreamer.},
  archive      = {J_TPAMI},
  author       = {Luxi Chen and Zhengyi Wang and Zihan Zhou and Tingting Gao and Hang Su and Jun Zhu and Chongxuan Li},
  doi          = {10.1109/TPAMI.2025.3600494},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MicroDreamer: Efficient 3D generation in $\sim$20 seconds by score-based iterative reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuMesh++: Towards versatile and efficient volumetric editing with disentangled neural mesh-based implicit field. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3600473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently neural implicit rendering techniques have evolved rapidly and demonstrated significant advantages in novel view synthesis and 3D scene reconstruction. However, existing neural rendering methods for editing purposes offer limited functionalities, e.g., rigid transformation and category-specific editing. In this paper, we present a novel mesh-based representation by encoding the neural radiance field with disentangled geometry, texture, and semantic codes on mesh vertices, which empowers a set of efficient and comprehensive editing functionalities, including mesh-guided geometry editing, designated texture editing with texture swapping, filling and painting operations, and semantic-guided editing. To this end, we develop several techniques including a novel local space parameterization to enhance rendering quality and training stability, a learnable modification color on vertex to improve the fidelity of texture editing, a spatial-aware optimization strategy to realize precise texture editing, and a semantic-aided region selection to ease the laborious annotation of implicit field editing. Extensive experiments and editing examples on both real and synthetic datasets demonstrate the superiority of our method on representation quality and editing ability.},
  archive      = {J_TPAMI},
  author       = {Chong Bao and Yuan Li and Bangbang Yang and Yujun Shen and Hujun Bao and Zhaopeng Cui and Yinda Zhang and Guofeng Zhang},
  doi          = {10.1109/TPAMI.2025.3600473},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NeuMesh++: Towards versatile and efficient volumetric editing with disentangled neural mesh-based implicit field},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamStory: Open-domain story visualization by LLM-guided multi-subject consistent diffusion. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3600149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each scene's subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA module ensures detailed appearance consistency with reference images, while MMCA captures key attributes of subjects from their reference text to ensure semantic consistency. Both modules employ masking mechanisms to restrict each scene's subjects to referencing the multimodal information of the corresponding subject, effectively preventing blending between multiple subjects. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at https://dream-xyz.github.io/dreamstory.},
  archive      = {J_TPAMI},
  author       = {Huiguo He and Huan Yang and Zixi Tuo and Yuan Zhou and Qiuyue Wang and Yuhang Zhang and Zeyu Liu and Wenhao Huang and Hongyang Chao and Jian Yin},
  doi          = {10.1109/TPAMI.2025.3600149},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamStory: Open-domain story visualization by LLM-guided multi-subject consistent diffusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neovascularization segmentation via a multilateral interaction-enhanced graph convolutional network. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3600335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choroidal neovascularization (CNV), a primary characteristic of wet age-related macular degeneration (wet AMD), represents a leading cause of blindness worldwide. In clinical practice, optical coherence tomography angiography (OCTA) is commonly used for studying CNV-related pathological changes, due to its micron-level resolution and non-invasive nature. Thus, accurate segmentation of CNV regions and vessels in OCTA images is crucial for clinical assessment of wet AMD. However, challenges existed due to irregular CNV shapes and imaging limitations like projection artifacts, noises and boundary blurring. Moreover, the lack of publicly available datasets constraints the CNV analysis. To address these challenges, this paper constructs the first publicly accessible CNV dataset (CNVSeg), and proposes a novel multilateral graph convolutional interaction-enhanced CNV segmentation network (MTG-Net). This network integrates both region and vessel morphological information, exploring semantic and geometric duality constraints within the graph domain. Specifically, MTG-Net consists of a multi-task framework and two graph-based cross-task modules: Multilateral Interaction Graph Reasoning (MIGR) and Multilateral Reinforcement Graph Reasoning (MRGR). The multi-task framework encodes rich geometric features of lesion shapes and surfaces, decoupling the image into three task-specific feature maps. MIGR and MRGR iteratively reason about higher-order relationships across tasks through a graph mechanism, enabling complementary optimization for task-specific objectives. Additionally, an uncertainty-weighted loss is proposed to mitigate the impact of artifacts and noise on segmentation accuracy. Experimental results demonstrate that MTG-Net outperforms existing methods, achieving a Dice socre of 87.21% for region segmentation and 88.12% for vessel segmentation.},
  archive      = {J_TPAMI},
  author       = {Tao Chen and Dan Zhang and Da Chen and Huazhu Fu and Kai Jin and Shanshan Wang and Laurent D. Cohen and Yitian Zhao and Quanyong Yi and Jiong Zhang},
  doi          = {10.1109/TPAMI.2025.3600335},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neovascularization segmentation via a multilateral interaction-enhanced graph convolutional network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-constrained clustering ensemble. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3600256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing clustering ensemble methods typically fuse all base clusterings in one shot under unsupervised settings, making it difficult to distinguish the quality of individual base clusterings and to exploit latent prior knowledge; consequently, their adaptability to data distributions and overall performance are limited. To address these issues, this paper proposes the Self-Constrained Clustering Ensemble (SCCE) algorithm. SCCE treats the pseudo-labels automatically generated from current clustering results as self-supervised signals and performs metric learning to obtain a linear transformation that enlarges inter-class distances while compressing intra-class distances. The base clusterings are then reclustered in the new metric space to enhance separability and consistency. Afterward, ensemble updating is iteratively applied, forming a self-driven closed loop that continuously improves model performance. Theoretical analysis shows that the model converges efficiently via alternating optimization, with computational complexity on the same order as mainstream methods. Experiments on public datasets demonstrate that the proposed algorithm significantly outperforms representative clustering ensemble approaches, validating its effectiveness and robustness in scenarios lacking external supervision.},
  archive      = {J_TPAMI},
  author       = {Wei Wei and Jianguo Wu and Xinyao Guo and Jing Yan and Jiye Liang},
  doi          = {10.1109/TPAMI.2025.3600256},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-constrained clustering ensemble},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UrbanGen: Urban generation with compositional and controllable neural fields. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3600440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rapid progress in generative radiance fields, most existing methods focus on object-centric applications and are not able to generate complex urban scenes. In this paper, we propose UrbanGen, a solution for the challenging task of generating urban radiance fields with photorealistic rendering, accurate geometry, high controllability, and diverse city styles. Our key idea is to leverage a coarse 3D panoptic prior, represented by a semantic voxel grid for stuff and bounding boxes for countable objects, to condition a compositional generative radiance field. This panoptic prior simplifies the task of learning complex urban geometry, enables disentanglement of stuff and objects, and provides versatile control over both. Moreover, by combining semantic and geometry losses with adversarial training, our method faithfully adheres to the input conditions, allowing for joint rendering of semantic and depth maps alongside RGB images. In addition, we collect a unified dataset with images and their panoptic priors in the same format from 3 diverse real-world datasets: KITTI-360, nuScenes, and Waymo, and train a city style-aware model on this data. Our systematic study shows that UrbanGen outperforms state-of-the-art generative radiance field baselines in terms of image fidelity and geometry accuracy for urban scene generation. Furthermore, UrbenGen brings a new set of controllability features, including large camera movements, stuff editing, and city style control.},
  archive      = {J_TPAMI},
  author       = {Yuanbo Yang and Yujun Shen and Yue Wang and Andreas Geiger and Yiyi Liao},
  doi          = {10.1109/TPAMI.2025.3600440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {UrbanGen: Urban generation with compositional and controllable neural fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MinD-3D++: Advancing fMRI-based 3D reconstruction with high-quality textured mesh generation and a comprehensive dataset. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3599860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4,768 3D objects. The dataset consists of two components: fMRI-Shape, previously introduced and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape , and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the core set in fMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Moreover, we propose MinD-3D++, a novel framework for decoding textured 3D visual information from fMRI signals. The framework evaluates the feasibility of not only reconstructing 3D objects from the human mind but also generating, for the first time, 3D textured meshes with detailed textures from fMRI data. We establish new benchmarks by designing metrics at the semantic, structural, and textured levels to evaluate model performance. Furthermore, we assess the model's effectiveness in out-of-distribution settings and analyze the attribution of the proposed 3D pari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our experiments demonstrate that MinD-3D++ not only reconstructs 3D objects with high semantic and spatial accuracy but also provides deeper insights into how the human brain processes 3D visual information. Project page: https://jianxgao.github.io/MinD-3D.},
  archive      = {J_TPAMI},
  author       = {Jianxiong Gao and Yanwei Fu and Yuqian Fu and Yun Wang and Xuelin Qian and Jianfeng Feng},
  doi          = {10.1109/TPAMI.2025.3599860},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MinD-3D++: Advancing fMRI-based 3D reconstruction with high-quality textured mesh generation and a comprehensive dataset},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MeViS: A multi-modal dataset for referring motion expression video segmentation. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are released at https://henghuiding.github.io/MeViS.},
  archive      = {J_TPAMI},
  author       = {Henghui Ding and Chang Liu and Shuting He and Kaining Ying and Xudong Jiang and Chen Change Loy and Yu-Gang Jiang},
  doi          = {10.1109/TPAMI.2025.3600507},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MeViS: A multi-modal dataset for referring motion expression video segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DyCrowd: Towards dynamic crowd reconstruction from a large-scene video. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3600465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction of dynamic crowds in large scenes has become increasingly important for applications such as city surveillance and crowd analysis. However, current works attempt to reconstruct 3D crowds from a static image, causing a lack of temporal consistency and inability to alleviate the typical impact caused by occlusions. In this paper, we propose DyCrowd, the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from a large-scene video. We design a coarse-to-fine group-guided motion optimization strategy for occlusion-robust crowd reconstruction in large scenes. To address temporal instability and severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based human motion prior along with a segment-level group-guided optimization. The core of our strategy leverages collective crowd behavior to address long-term dynamic occlusions. By jointly optimizing the motion sequences of individuals with similar motion segments and combining this with the proposed Asynchronous Motion Consistency (AMC) loss, we enable high-quality unoccluded motion segments to guide the motion recovery of occluded ones, ensuring robust and plausible motion recovery even in the presence of temporal desynchronization and rhythmic inconsistencies. Additionally, in order to fill the gap of no existing well-annotated large-scene video dataset, we contribute a virtual benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction from large-scene videos. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in the large-scene dynamic crowd reconstruction task. The code and dataset will be available for research purposes.},
  archive      = {J_TPAMI},
  author       = {Hao Wen and Hongbo Kang and Jian Ma and Jing Huang and Yuanwang Yang and Haozhe Lin and Yu-Kun Lai and Kun Li},
  doi          = {10.1109/TPAMI.2025.3600465},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DyCrowd: Towards dynamic crowd reconstruction from a large-scene video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling up your kernels: Large kernel design in ConvNets towards universal representations. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet, promoting further research and development in the community},
  archive      = {J_TPAMI},
  author       = {Yiyuan Zhang and Xiaohan Ding and Xiangyu Yue},
  doi          = {10.1109/TPAMI.2025.3600702},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scaling up your kernels: Large kernel design in ConvNets towards universal representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundation model for skeleton-based human action understanding. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action understanding serves as a foundational pillar in the field of intelligent motion perception. Skeletons serve as a modality- and device-agnostic representation for human modeling, and skeleton-based action understanding has potential applications in humanoid robot control and interaction. However, existing works often lack the scalability and generalization required to handle diverse action understanding tasks. There is no skeleton foundation model that can be adapted to a wide range of action understanding tasks. This paper presents a Unified Skeleton-based Dense Representation Learning (USDRL) framework, which serves as a foundational model for skeleton-based human action understanding. USDRL consists of a Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The DSTE module adopts two parallel streams to learn temporal dynamic and spatial structure features. The MG-FD module collaboratively performs feature decorrelation across temporal, spatial, and instance domains to reduce dimensional redundancy and enhance information extraction. The MPCT module employs both multi-view and multi-modal self-supervised consistency training. The former enhances the learning of high-level semantics and mitigates the impact of low-level discrepancies, while the latter effectively facilitates the learning of informative multimodal features. We perform extensive experiments on 25 benchmarks across across 9 skeleton-based action understanding tasks, covering coarse prediction, dense prediction, and transferred prediction. Our approach significantly outperforms the current state-of-the-art methods. We hope that this work would broaden the scope of research in skeleton-based action understanding and encourage more attention to dense prediction tasks. This code is available at: https://github.com/wengwanjiang/FoundSkelModel.},
  archive      = {J_TPAMI},
  author       = {Hongsong Wang and Wanjiang Weng and Junbo Wang and Fang Zhao and Guo-Sen Xie and Xin Geng and Liang Wang},
  doi          = {10.1109/TPAMI.2025.3600658},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Foundation model for skeleton-based human action understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised gaze representation learning by switching features. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3600680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is prevalent to leverage unlabeled data to train deep learning models when it is difficult to collect large-scale annotated datasets. However, for 3D gaze estimation, most existing unsupervised learning methods face challenges in distinguishing subtle gaze-relevant information from dominant gaze-irrelevant information. To address this issue, we propose an unsupervised learning framework to disentangle the gaze-relevant and the gaze-irrelevant information, by seeking the shared information of a pair of input images with the same gaze and with the same eye respectively. Specifically, given two images, the framework finds their shared information by first encoding the images into two latent features via two encoders and then switching part of the features before feeding them to the decoders for image reconstruction. We theoretically prove that the proposed framework is able to encode different information into different parts of the latent feature if we properly select the training image pairs and their shared information. Based on the framework, we derive Cross-Encoder and Cross-Encoder++ to learn gaze representation from the eye images and face images, respectively. Experiments on pubic gaze datasets demonstrate that the Cross-Encoder and Cross-Encoder++ outperform the competitive methods. The ablation study quantitatively and qualitatively shows that the gaze feature is successfully extracted.},
  archive      = {J_TPAMI},
  author       = {Yunjia Sun and Jiabei Zeng and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2025.3600680},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised gaze representation learning by switching features},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBONet: Supervised spectral descriptors for shape analysis. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3600873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Laplace-Beltrami operator has established itself in the field of non-rigid shape analysis due to its many useful properties such as being invariant under isometric transformation, having a countable eigensystem forming an orthornormal basis, and fully characterizing geodesic distances of the manifold. However, this invariancy only applies under isometric deformations, which leads to a performance breakdown in many real-world applications. In recent years emphasis has been placed upon extracting optimal features using deep learning methods, however spectral signatures play a crucial role and still add value. In this paper we take a step back, revisiting the LBO and proposing a supervised way to learn several operators on a manifold. Depending on the task, by applying these functions, we can train the LBO eigenbasis to be more task-specific. The optimization of the LBO leads to enormous improvements to established descriptors such as the heat kernel signature in various tasks such as retrieval, classification, segmentation, and correspondence, proving the adaptation of the LBO eigenbasis to both global and highly local learning settings.},
  archive      = {J_TPAMI},
  author       = {Oguzhan Yigit and Richard C. Wilson},
  doi          = {10.1109/TPAMI.2025.3600873},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LBONet: Supervised spectral descriptors for shape analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rehearsal-free and efficient continual learning for cross-domain face anti-spoofing. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3601053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Anti-Spoofing (FAS) is constantly challenged by new attack types and mediums, and thus it is crucial for a FAS model to not only mitigate Catastrophic Forgetting (CF) of previously learned spoofing knowledge on the training data during continual learning but also enhance the model's generalization ability to potential spoofing attacks. In this paper, we first highlight that current strategies for catastrophic forgetting are not well-suited to the imperceptible nature of spoofing information in FAS and lack the focus on improving generalization capability. Then, the instance-wise dynamic central difference convolutional adapter module with the weighted ensemble strategy for Vision Transformer (ViT) is proposed for efficiently fine-tuning with low-shot data by extracting generalized spoofing texture information. Furthermore, we find that catastrophic forgetting in FAS can be reflected through the inconsistent attention matrices of ViT between different continual sessions, as the attention matrices embody relationships of spoofing clues between different patch tokens. Hence, we introduce attention consistency regularization by learning and reusing attention matrices to alleviate catastrophic forgetting. Finally, we devise new protocols and conduct extensive experiments to validate the superior performance of alleviating catastrophic forgetting and generalization on unseen domains. The code and protocol files are released on https://github.com/RizhaoCai/DCL-FAS-ICCV2023.},
  archive      = {J_TPAMI},
  author       = {Rizhao Cai and Yawen Cui and Zitong Yu and Xun Lin and Changsheng Chen and Alex Kot},
  doi          = {10.1109/TPAMI.2025.3601053},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rehearsal-free and efficient continual learning for cross-domain face anti-spoofing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FACETRACER: Unveiling source identities from swapped face images and videos for fraud prevention. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3601141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face-swapping techniques have advanced rapidly with the evolution of deep learning, leading to widespread use and growing concerns about potential misuse, especially in cases of fraud. While many efforts have focused on detecting swapped face images or videos, these methods are insufficient for tracing the malicious users behind fraudulent activities. Intrusive watermark-based approaches also fail to trace unmarked identities, limiting their practical utility. To address these challenges, we introduce FACETRACER, the first non-intrusive framework specifically designed to trace the identity of the source person from swapped face images or videos. Specifically, FACETRACER leverages a disentanglement module that effectively suppresses identity information related to the target person while isolating the identity features of the source person. This allows us to extract robust identity information that can directly link the swapped face back to the original individual, aiding in uncovering the actors behind fraudulent activities. Extensive experiments demonstrate FACETRACER's effectiveness across various face-swapping techniques, successfully identifying the source person in swapped content and enabling the tracing of malicious actors involved in fraudulent activities. Additionally, FACETRACER shows strong transferability to unseen face-swapping methods including commercial applications and robustness against transmission distortions and adaptive attacks.},
  archive      = {J_TPAMI},
  author       = {Zhongyi Zhang and Jie Zhang and Wenbo Zhou and Xinghui Zhou and Qing Guo and Weiming Zhang and Tianwei Zhang and Nenghai Yu},
  doi          = {10.1109/TPAMI.2025.3601141},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FACETRACER: Unveiling source identities from swapped face images and videos for fraud prevention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent and optimal solution to camera motion estimation. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3601430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given 2D point correspondences between an image pair, inferring the camera motion is a fundamental issue in the computer vision community. The existing works generally set out from the epipolar constraint and estimate the essential matrix, which is not optimal in the maximum likelihood (ML) sense. In this paper, we dive into the original measurement model with respect to the rotation matrix and normalized translation vector and formulate the ML problem. We then propose an optimal two-step algorithm to solve it: In the first step, we estimate the variance of measurement noises and devise a consistent estimator based on bias elimination; In the second step, we execute a one-step Gauss-Newton iteration on manifold to refine the consistent estimator. We prove that the proposed estimator achieves the same asymptotic statistical properties as the ML estimator: The first is consistency, i.e., the estimator converges to the ground truth as the point number increases; The second is asymptotic efficiency, i.e., the mean squared error of the estimator converges to the theoretical lower bound — Cramer-Rao bound. In addition, we show that our algorithm has linear time complexity. These appealing characteristics endow our estimator with a great advantage in the case of dense point correspondences. Experiments on both synthetic data and real images demonstrate that when the point number reaches the order of hundreds, our estimator outperforms the state-of-the-art ones in terms of estimation accuracy and CPU time.},
  archive      = {J_TPAMI},
  author       = {Guangyang Zeng and Qingcheng Zeng and Xinghan Li and Biqiang Mu and Jiming Chen and Ling Shi and Junfeng Wu},
  doi          = {10.1109/TPAMI.2025.3601430},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Consistent and optimal solution to camera motion estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning compact discriminant representation via low-rank bilinear pooling. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3601355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explain the mechanism of bilinear pooling as a module of hard sample generation, and find that bilinear pooling significantly expands variances of the first-order vectors when it produces discriminative bilinear features. In conjunction with the extremely high dimensionality of the obtained bilinear features, those variances lead to overfitting in subsequent learning models. To solve this issue, we construct a bi-level optimization problem, where the high-level problem is the supervised classification loss, and the low-level problem is the principal component analysis (PCA). Then, we find that PCA on bilinear features is equivalent to spectral clustering, which allows us to mathematically prove that the first $\log _{2}(C)$ principal components can support the discriminant information of $C$ classes. By removing the rest principal components, the dimensionality and variances are simultaneously reduced. To the best of our knowledge, this is the first work providing a lower bound for dimension reduction for bilinear pooling. However, the PCA projection matrix $\mathbf{L}$ is prone to overfitting due to having many parameters. To address this issue, we propose a rank-$k$ general bilinear projection (RK-GBP) that decomposes $\mathbf{L}$ into two small matrices $\mathbf{U}$ and $\mathbf{V}$, whose learnable parameters are smaller. Different from traditional bilinear projections used in factorized bilinear pooling (FBiP), our RK-GBP can preserve the orthogonality of columns in $\mathbf{L}$ by constraining the orthogonality of columns in $\mathbf{U}$ and $\mathbf{V}$. For computational efficiency, we relax the PCA in the low-level task into a dictionary learning problem, obtaining the rank-$k$ orthogonal factorization bilinear pooling (RK-OFBP). The RK-OFBP can be considered as a general form of current factorization bilinear pooling methods (e.g. Hadamard product-based ones). Finally, we evaluate our approach on fine-grained images and large-scale datasets, demonstrating that our proposed method not only produces extremely low-dimensional features but also outperforms other methods in classification tasks. For example, our RK-OFBP can employ 32-dimensional vectors to achieve comparable results to B-CNN [1] (dimension: 512*512) for the 200-class classification task.},
  archive      = {J_TPAMI},
  author       = {Kun Song and Hao Li and Gong Cheng and Junwei Han and Feiping Nie and Bin Gu and Fakhri Karray},
  doi          = {10.1109/TPAMI.2025.3601355},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning compact discriminant representation via low-rank bilinear pooling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified modality separation: A vision-language framework for unsupervised domain adaptation. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3597436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) enables models trained on a labeled source domain to handle new unlabeled domains. Recently, pre-trained vision-language models (VLMs) have demonstrated promising zero-shot performance by leveraging semantic information to facilitate target tasks. By aligning vision and text embeddings, VLMs have shown notable success in bridging domain gaps. However, inherent differences naturally exist between modalities, which is known as modality gap. Our findings reveal that direct UDA with the presence of modality gap only transfers modality-invariant knowledge, leading to suboptimal target performance. To address this limitation, we propose a unified modality separation framework that accommodates both modality-specific and modality-invariant components. During training, different modality components are disentangled from VLM features then handled separately in a unified manner. At test time, modality-adaptive ensemble weights are automatically determined to maximize the synergy of different components. To evaluate instance-level modality characteristics, we design a modality discrepancy metric to categorize samples into modality-invariant, modality-specific, and uncertain ones. The modality-invariant samples are exploited to facilitate cross-modal alignment, while uncertain ones are annotated to enhance model capabilities. Building upon prompt tuning techniques, our methods achieve up to 9% performance gain with 9 times of computational efficiencies. Extensive experiments and analysis across various backbones, baselines, datasets and adaptation settings demonstrate the efficacy of our design.},
  archive      = {J_TPAMI},
  author       = {Xinyao Li and Jingjing Li and Zhekai Du and Lei Zhu and Heng Tao Shen},
  doi          = {10.1109/TPAMI.2025.3597436},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unified modality separation: A vision-language framework for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient federated multi-view clustering. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3601533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated multi-view clustering is an emerging machine learning paradigm that groups the data with each view distributed on an isolated client while preserving their privacies. Although recent researches have proposed a few feasible solutions, they are severely limited by two drawbacks. In specific, the clients are required to share their data representations at each iteration of model training, leading to heavy communication overhead. On the other hand, existing researches handle large-scale data by employing the matrix factorization and neural network encoding techniques, failing to utilize their similarity information sufficiently. To address these issues, we propose a communication-efficient federated multi-view clustering framework by approximating the data representation with pseudo-label and centroid matrix, where the latter two are shared in model training. Meanwhile, the framework is instanced by incorporating linear kernel function to consider the data pairwise similarities. Note that, corresponding linear kernels are not required to compute explicitly, making the resultant method able to be optimized in linear complexity to the number of samples. Nevertheless, the proposed method is evaluated on benchmark datasets. It not only achieves inspiring results (26.84% accuracy improvement on average, 2.9$_\times$-2153$_\times$ computation speedup and 98.4% communication overhead reduction at most) compared with existing federated multi-view clustering methods, but also outperforms centralized multi-view clustering approaches on performance and computation efficiency.},
  archive      = {J_TPAMI},
  author       = {Jiyuan Liu and Xinwang Liu and Siqi Wang and Xinhang Wan and Dongsheng Li and Kai Lu and Kunlun He},
  doi          = {10.1109/TPAMI.2025.3601533},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Communication-efficient federated multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VQ-FedDiff: Federated learning algorithm of diffusion models with client-specific vector-quantized conditioning. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3602282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern generative models, particularly denoising diffusion probabilistic models (DDPMs), provide high-quality synthetic images, enabling users to generate diverse images and videos that are realistic. However, in a number of situations, edge devices or individual institutions may possess locally collected data that is highly sensitive and should ensure data privacy, such as in the field of healthcare and finance. Under such federated learning (FL) settings, various methods on training generative models have been studied, but most of them assume generative adversarial networks (GANs), and the algorithms are specific to GANs and not other forms of generative models such as DDPM. This paper proposes a new algorithm for training DDPMs under federated learning settings, VQ-FedDiff, which provides a personalized algorithm for training diffusion models that can generate higher-quality images FID while still keeping risk of breaching sensitive information as low as locally-trained secure models. We demonstrate that VQ-FedDiff shows state-of-the-art performance on existing federated learning of diffusion models in both IID and non-IID settings, and in benchmark photorealistic and medical image datasets. Our results show that diffusion models can efficiently learn with decentralized, sensitive data, generating high-quality images while preserving data privacy.},
  archive      = {J_TPAMI},
  author       = {Tehrim Yoon and Minyoung Hwang and Eunho Yang},
  doi          = {10.1109/TPAMI.2025.3602282},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VQ-FedDiff: Federated learning algorithm of diffusion models with client-specific vector-quantized conditioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RingMo-aerial: An aerial remote sensing foundation model with affine transformation contrastive learning. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3602237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial Remote Sensing (ARS) vision tasks present significant challenges due to the unique viewing angle characteristics. Existing research has primarily focused on algorithms for specific tasks, which have limited applicability in a broad range of ARS vision applications. This paper proposes RingMo-Aerial, aiming to fill the gap in foundation model research in the field of ARS vision. A Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism is introduced to strengthen the model's capacity for small-object representation. Complementarily, an affine transformation-based contrastive learning method improves its adaptability to the tilted viewing angles inherent in ARS tasks. Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is proposed to improve the model's adaptability and performance in various ARS vision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This indicates the practicality and efficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.},
  archive      = {J_TPAMI},
  author       = {Wenhui Diao and Haichen Yu and Kaiyue Kang and Tong Ling and Di Liu and Yingchao Feng and Hanbo Bi and Libo Ren and Xuexue Li and Yongqiang Mao and Xian Sun},
  doi          = {10.1109/TPAMI.2025.3602237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RingMo-aerial: An aerial remote sensing foundation model with affine transformation contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing three-dimensional models of interacting humans. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3601974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling. However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspects–the essence of the event–and are of little use for detailed behavioral understanding. This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged to ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing 2,525 contact events, 728,664 ground truth 3d poses, as well as FlickrCI3D, a dataset of 11,216 images, with 14,081 processed pairs of people, and 81,233 facet-level surface correspondences. Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled setup and (5) annotate all 3d interaction motions in CHI3D with textual descriptions. Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes at https://ci3d.imar.ro , together with an evaluation server and a public benchmark.},
  archive      = {J_TPAMI},
  author       = {Mihai Fieraru and Mihai Zanfir and Elisabeta Oneata and Alin-Ionut Popa and Vlad Olaru and Cristian Sminchisescu},
  doi          = {10.1109/TPAMI.2025.3601974},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstructing three-dimensional models of interacting humans},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit regularization. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3602216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments. Conventional works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity. Recent progresses have been made in object shape generation with generative models such as diffusion models, which increases the shape fidelity. However, these approaches separately treat 3D shape generation and layout generation. The synthesized scenes are usually hampered by layout collision, which suggests that the scene-level fidelity is still under-explored. In this paper, we aim at generating realistic and reasonable 3D indoor scenes from scene graph. To enrich the priors of the given scene graph inputs, large language model is utilized to aggregate the global-wise features with local node-wise and edge-wise features. With a unified graph encoder, graph features are extracted to guide joint layout-shape generation. Additional regularization is introduced to explicitly constrain the produced 3D layouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity. The source code will be released after publication. 3D indoor scene synthesis, generative model, scene graph, large language model, spatial arrangement, latent diffusion. },
  archive      = {J_TPAMI},
  author       = {Yao Wei and Martin Renqiang Min and George Vosselman and Li Erran Li and Michael Ying Yang},
  doi          = {10.1109/TPAMI.2025.3602216},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit regularization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OccScene: Semantic occupancy-based cross-task mutual learning for 3D scene generation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3602511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent diffusion models have demonstrated remarkable performance in both 3D scene generation and perception tasks. Nevertheless, existing methods typically separate these two processes, acting as a data augmenter to generate synthetic data for downstream perception tasks. In this work, we propose OccScene, a novel mutual learning paradigm that integrates fine-grained 3D perception and high-quality generation in a unified framework, achieving a cross-task win-win effect. OccScene generates new and consistent 3D realistic scenes only depending on text prompts, guided with semantic occupancy in a joint-training diffusion framework. To align the occupancy with the diffusion latent, a Mamba-based Dual Alignment module is introduced to incorporate fine-grained semantics and geometry as perception priors. Within OccScene, the perception module can be effectively improved with customized and diverse generated scenes, while the perception priors in return enhance the generation performance for mutual benefits. Extensive experiments show that OccScene achieves realistic 3D scene generation in broad indoor and outdoor scenarios, while concurrently boosting the perception models to achieve substantial performance improvements in the 3D perception task of semantic occupancy prediction.},
  archive      = {J_TPAMI},
  author       = {Bohan Li and Xin Jin and Jianan Wang and Yukai Shi and Yasheng Sun and Xiaofeng Wang and Zhuang Ma and Baao Xie and Chao Ma and Xiaokang Yang and Wenjun Zeng},
  doi          = {10.1109/TPAMI.2025.3602511},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {OccScene: Semantic occupancy-based cross-task mutual learning for 3D scene generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active learning for multiple target models. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3602682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel setting of active learning (AL) where multiple target models are simultaneously learned. This setting arises in real-world applications where machine learning systems require training multiple models on the same labeled dataset to accommodate diverse devices with varying computational resources. However, traditional AL methods are often limited by their model dependence and non-transferability. In this paper, we address the question of whether an effective AL method can be designed for multiple target models. We analyze the query complexity of active and passive learning in this setting and demonstrate the potential for AL to achieve improved query complexity. Based on this insight, we further propose an agnostic AL sampling strategy which selects examples located in the joint disagreement regions of different target models. Experimental evaluations on classification and regression benchmarks validate the effectiveness of our approach over traditional AL methods.},
  archive      = {J_TPAMI},
  author       = {Sheng-Jun Huang and Yi Li and Ying-Peng Tang},
  doi          = {10.1109/TPAMI.2025.3602682},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Active learning for multiple target models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SparseTSF: Lightweight and robust time series forecasting via sparse modeling. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3602445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces SparseTSF, a novel and extremely lightweight method for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by downsampling the original sequences to focus on cross-period trend prediction. This technique not only significantly reduces model complexity and the number of parameters but also serves as an implicit regularization mechanism that enhances the model's robustness, achieving an optimal balance between performance and efficiency. Based on this technique, SparseTSF uses fewer than 1,000 parameters to achieve competitive performance compared to state-of-the-art methods, with evident advantages under longer look-back windows (e.g., 720) that allow the model to better exploit inherent periodicity and trend information. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is publicly available at this repository: https://github.com/lss-1138/SparseTSF.},
  archive      = {J_TPAMI},
  author       = {Shengsheng Lin and Weiwei Lin and Wentai Wu and Haojun Chen and C. L. Philip Chen},
  doi          = {10.1109/TPAMI.2025.3602445},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SparseTSF: Lightweight and robust time series forecasting via sparse modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning logits generatively for principled black-box knowledge distillation in the wild. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3602663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Black-Box Knowledge Distillation (B2KD) is a conservative task in cloud-to-edge model compression, emphasizing the protection of data privacy and model copyrights on both the cloud and edge. With invisible data and models hosted on the server, B2KD aims to utilize only the API queries of the teacher model's inference results in the cloud to effectively distill a lightweight student model deployed on edge devices. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity in data distribution. To address these issues, we theoretically provide a new optimization direction from logits to cell boundary, different from direct logits alignment, and formalize a workflow comprising deprivatization, distillation, and adaptation at test time. Guided by this, we propose a method, Mapping-Emulation KD (MEKD), to enhance the robust prediction and anti-interference capabilities of the student model on edge devices for any unknown data distribution in real-world scenarios. Our method does not differentiate between treating soft or hard responses and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image points, and 3) adaptation: correcting the student's online prediction bias through a graph propagation-based only-forward test-time adaptation algorithm. Our method demonstrates inspiring performance for edge model distillation and adaptation across different teacher-student pairs. We validate the effectiveness of our method on multiple image recognition benchmarks and various Deep Neural Network models, achieving state-of-the-art performance and showcasing its practical value in remote sensing image recognition applications.},
  archive      = {J_TPAMI},
  author       = {Xiang Xiang and Jing Ma and Dongrui Wu and Zhigang Zeng and Xilin Chen},
  doi          = {10.1109/TPAMI.2025.3602663},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Aligning logits generatively for principled black-box knowledge distillation in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dual-stream conditional concepts in compositional zero-shot learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3597668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositional concepts composed of seen single concepts. One of the problems of CZSL is to model attributes interacting with objects and objects interacting with attributes. In this work, we focus on this problem and propose DualStream Conditional Network (DSCNet) that learns dual-stream conditional concepts as a solution, where the conditional visual and semantic embeddings of attributes and objects are learned. First, we argue that the condition of the attribute or object is supposed to contain the recognized object and input image, or the recognized attribute and input image. Next, for each concept which can either be an attribute or object, in the semantic stream, we propose to encode the recognized object or attribute semantic features and the input image visual features as the encoded condition, which is then injected into all concept semantic embeddings by a semantic cross encoder to acquire conditional semantic embeddings. In the visual stream, the conditional attribute or object visual embeddings are acquired by injecting the semantic features of the recognized object or attribute into the mapped attribute or object visual features. Experimental results on CZSL benchmarks demonstrate the superiority of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Qingsheng Wang and Lingqiao Liu and Chenchen Jing and Peng Wang and Yanning Zhang and Chunhua Shen},
  doi          = {10.1109/TPAMI.2025.3597668},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning dual-stream conditional concepts in compositional zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based domain adaptation for image-aware layout generation in advertising poster design. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3602846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout plays a crucial role in graphic design and poster generation. Recently, the application of deep learning models for layout generation has gained significant attention. This paper focuses on using a GAN-based model conditioned on images to generate advertising poster graphic layouts, requiring a dataset of paired product images and layouts. To address this task, we introduce the Content-aware Graphic Layout Dataset (CGL-Dataset), consisting of 60,548 paired inpainted posters with annotations and 121,000 clean product images. The inpainting artifacts introduce a domain gap between the inpainted posters and clean images. To bridge this gap, we design two GAN-based models. The first model, CGL-GAN, uses Gaussian blur on the inpainted regions to generate layouts. The second model combines unsupervised domain adaptation by introducing a GAN with a pixel-level discriminator (PD), abbreviated as PDA-GAN, to generate image-aware layouts based on the visual texture of input images. The PD is connected to shallow-level feature maps and computes the GAN loss for each input-image pixel. Additionally, we propose three novel content-aware metrics to assess the model's ability to capture the intricate relationships between graphic elements and image content. Quantitative and qualitative evaluations demonstrate that PDA-GAN achieves state-of-the-art performance and generates high-quality image-aware layouts.},
  archive      = {J_TPAMI},
  author       = {Chenchen Xu and Min Zhou and Tiezheng Ge and Weiwei Xu},
  doi          = {10.1109/TPAMI.2025.3602846},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GAN-based domain adaptation for image-aware layout generation in advertising poster design},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-oriented instruction tuning of large language models for generic graph mining. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3603062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs with abundant attributes are essential in modeling interconnected entities and enhancing predictions across various real-world applications. Traditional Graph Neural Networks (GNNs) often require re-training for different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced new paradigms in natural language processing, their potential for generic graph mining—training a single model to simultaneously handle diverse tasks and datasets—remains under-explored. To this end, our novel framework MuseGraph, seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets. This framework first features a compact graph description to encapsulate key graph information within language token limitations. Then, we propose a diverse instruction generation mechanism with Chain-of-Thought (CoT)-based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4. Finally, we design a graph-aware instruction tuning strategy to facilitate mutual enhancement across multiple tasks and datasets while preventing catastrophic forgetting of LLMs' generative abilities. Our experimental results demonstrate significant improvements in five graph tasks and ten datasets, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while improving the generation abilities of LLMs.},
  archive      = {J_TPAMI},
  author       = {Yanchao Tan and Hang Lv and Pengxiang Zhan and Shiping Wang and Carl Yang},
  doi          = {10.1109/TPAMI.2025.3603062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph-oriented instruction tuning of large language models for generic graph mining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compositional generative model of unbounded 4D cities. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3603078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.},
  archive      = {J_TPAMI},
  author       = {Haozhe Xie and Zhaoxi Chen and Fangzhou Hong and Ziwei Liu},
  doi          = {10.1109/TPAMI.2025.3603078},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Compositional generative model of unbounded 4D cities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-filtering: Implicit filtering for learning neural distance functions from 3D point clouds. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3602830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit functions including signed distance functions (SDFs) and unsigned distance functions (UDFs) have shown powerful ability in fitting the shape geometry. However, inferring continuous distance fields from discrete unoriented point clouds still remains a challenge. The neural network typically fits the shape with a rough surface and omits fine-grained geometric details such as shape edges and corners. In this paper, we propose a novel non-linear implicit filter to smooth the implicit field while preserving high-frequency geometry details. Our novelty lies in that we can filter the surface (zero level set) by the neighbor input points with gradients of the signed distance field. By moving the input raw point clouds along the gradient, our proposed implicit filtering can be extended to non-zero level sets to keep the promise consistency between different level sets, which consequently results in a better regularization of the zero level set. Since the unsigned distance function is non-differentiable at the zero level set and lacks a stable gradient field, we further propose a gradient immutable training schema to migrate the filter to the unsigned distance function learned from point clouds. By leveraging the UDF training schema, we also improve sparse-view reconstruction results. We conduct comprehensive experiments in surface reconstruction from objects, complex scene point clouds, and multi-view images, and we further extend to the point normal estimation and point cloud upsampling tasks. The numerical and visual comparisons demonstrate our improvements over the stateof- the-art methods under the widely used benchmarks.},
  archive      = {J_TPAMI},
  author       = {Shengtao Li and Yudong Liu and Ge Gao and Ming Gu and Yu-Shen Liu},
  doi          = {10.1109/TPAMI.2025.3602830},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {I-filtering: Implicit filtering for learning neural distance functions from 3D point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards human-level 3D relative pose estimation: Generalizable, training-free, with single reference. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3600413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can easily deduce the relative pose of a previously unseen object, without labeling or training, given only a single query-reference image pair. This is arguably achieved by incorporating i) 3D/2.5D shape perception from a single image, ii) render-and-compare simulation, and iii) rich semantic cue awareness to furnish (coarse) reference-query correspondence. Motivated by this, we propose a novel 3D generalizable relative pose estimation method by elaborating 3D/2.5D shape perception with a 2.5D shape from an RGB-D reference, fulfilling the render-and-compare paradigm with an off-the-shelf differentiable renderer, and leveraging the semantic cues from a pretrained model like DINOv2. Specifically, our differentiable renderer takes the 2.5D rotatable mesh textured by the RGB and the semantic maps (obtained by DINOv2 from the RGB input), then renders new RGB and semantic maps (with back-surface culling) under a novel rotated view. The refinement loss comes from comparing the rendered RGB and semantic maps with the query ones, back-propagating the gradients through the differentiable renderer to refine the 3D relative pose. As a result, our method can be readily applied to unseen objects, given only a single RGB-D reference, without labeling or training. Extensive experiments on LineMOD, LM-O, and YCB-V show that our training-free method significantly outperforms the state-of-the-art supervised methods, especially under the rigorous Acc@5/10/15$^\circ$ metrics and the challenging cross-dataset settings.},
  archive      = {J_TPAMI},
  author       = {Yuan Gao and Yajing Luo and Junhong Wang and Kui Jia and Gui-Song Xia},
  doi          = {10.1109/TPAMI.2025.3600413},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards human-level 3D relative pose estimation: Generalizable, training-free, with single reference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient high-order spatial interactions for visual perception. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3603181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework. We present the Recursive Gated Convolution (g nConv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation. g nConv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the proposed operation, we construct a new family of generic vision backbones for various visual modalities and tasks, including HorNet and HorFPN for image recognition, Hor3D for point cloud analysis, and HorCLIP for vision-language modeling. For image recognition, we propose HorNet as a stronger visual encoder, where we conduct extensive experiments on ImageNet classification, COCO object detection, and ADE20K semantic segmentation. HorNet outperforms Swin Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and larger model sizes. Apart from image encoders, we also show g nConv can be applied to task-specific decoders and consistently improve dense prediction performance with less computation. For point cloud analysis, we design Hor3D, demonstrating the efficacy of high-order interactions for unstructured point cloud data through experiments on challenging 3D semantic segmentation tasks in S3DIS and ScanNet V2. In vision-language modeling, our proposed HorCLIP surpasses mainstream Vision Transformer and ConvNeXt architectures with shorter training schedules on ImageNet zero-shot classification and shows remarkably higher performance on vision-language dense representation tasks on COCO Panoptic datasets. Our results demonstrate that g nConv with high-order spatial interactions can be a new basic operation for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code is available at https://github.com/raoyongming/HorNet.},
  archive      = {J_TPAMI},
  author       = {Zuyan Liu and Yongming Rao and Wenliang Zhao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2025.3603181},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient high-order spatial interactions for visual perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning heterogeneous mixture of scene experts for large-scale neural radiance fields. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3603305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent Neural Radiance Field (NeRF) methods on large-scale scenes have demonstrated promising results and underlined the importance of scene decomposition for scalable NeRFs. Although these methods achieved reasonable scalability, there are several critical problems remaining unexplored in the existing large-scale NeRF modeling methods, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. Our framework is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous Neural Radiance Fields efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decompose scenes into partitions and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. Our network architecture incorporates a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges. This enables effective learning of the heterogeneous representation of different decomposed scene parts within large-scale complex scenes. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets. Additionally, we also introduce a new dataset with very large-scale scenes ($ \gt 6.5km^{2}$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency gains, with an 8x acceleration in training and a 16x acceleration in rendering compared to the best-performing competitor Switch-NeRF. The codes and trained models will be released in https://github.com/MiZhenxing/Switch-NeRF.},
  archive      = {J_TPAMI},
  author       = {Zhenxing Mi and Ping Yin and Xue Xiao and Dan Xu},
  doi          = {10.1109/TPAMI.2025.3603305},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning heterogeneous mixture of scene experts for large-scale neural radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rendering humans behind occlusions. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3603154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering the visual appearance of moving humans from occluded monocular videos is a challenging task. Most existing research renders 3D humans under ideal conditions, requiring a clear and unobstructed scene. Those previous methods cannot be used to render humans in real-world scenes where obstacles may block the camera's view and lead to partial occlusions. In this work, we present Wild2Avatar, a neural rendering approach catered for occluded in-the-wild monocular videos. We propose occlusion-aware scene parameterization for decoupling the scene into three parts - occlusion, human, and background. Additionally, extensive objective functions are designed to help enforce the decoupling of the human from both the occlusion and the background and to ensure the completeness of the human model. Wild2Avatar is verified with experiments on 14 challenging in-the-wild videos.},
  archive      = {J_TPAMI},
  author       = {Tiange Xiang and Adam Sun and Scott Delp and Kazuki Kozuka and Li Fei-Fei and Ehsan Adeli},
  doi          = {10.1109/TPAMI.2025.3603154},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rendering humans behind occlusions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical spherical CNNs with lifting-based adaptive wavelets for pooling and unpooling. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3603601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling and unpooling are indispensable in constructing hierarchical spherical convolutional neural networks (HS-CNNs). Most existing models employ simple downsampling-based pooling, which ignores the sampling theorem and cannot adapt to different spherical signals (with different spectra) and tasks (dependent on different frequency components), thus suffering a significant information loss. Besides, signals reconstructed by the widely-adopted padding-based unpooling may also change unwantedly the spectra of original signals. To address these, we propose a novel framework of HS-CNNs with lifting structures to learn adaptive spherical wavelets for pooling and unpooling, named LiftHS-CNNs. Specifically, we learn spherical wavelets with a lifting structure to adaptively partition the input signal into low- and high-frequency sub-bands, with the down-scaled representations for pooling generated to preserve more information in the low-frequency sub-band. The lifting structure consists of learnable update and predict operators parameterized with graph attention to jointly consider the signal's characteristics and underlying geometries. We then propose an unpooling operation invertible to the lifting-based pooling for restoring the up-scaled representations, which can well preserve spectral characteristics of the original signal. Particular properties (i.e., spatial locality, vanishing moments, and stability) of the learned wavelets and the information preserving ability of the proposed pooling and unpooling are further studied. Experiments on benchmark spherical datasets for a wide range of tasks verify the superiority of our LiftHS-CNNs.},
  archive      = {J_TPAMI},
  author       = {Mingxing Xu and Chenglin Li and Wenrui Dai and Siheng Chen and Junni Zou and Pascal Frossard and Hongkai Xiong},
  doi          = {10.1109/TPAMI.2025.3603601},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical spherical CNNs with lifting-based adaptive wavelets for pooling and unpooling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph-based high-order correlation analysis for large-scale long-tailed data classification. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3603631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-order correlations, which capture complex interactions among multiple entities, extend beyond traditional graph representations and support a wider range of applications. However, existing neural network models for high-order correlations encounter scalability issues on large datasets due to the substantial computational complexity involved in processing large-scale structures. In addition, long-tailed distributions, which are common in real-world data, result in underrepresented categories and hinder the model's ability to learn effective high-order interaction patterns for rare instances. To address these issues, we introduce a novel framework known as HyperGraph-based High-order Correlation analysis (HGHC) for large-scale long-tailed data classification. Firstly, to tackle the long-tailed distribution problem, HGHC generates synthetic vertices and computes their attributed high-order correlations using an oversampling module inspired by SMOTE, termed HSMOTE, to enhance the representation of tail categories. Secondly, for efficient computational scaling, we treat the data as having two modalities: the structural modality capturing high-order relationships and the feature modality representing individual attributes. We perform computations on both CPU and GPU separately and then fuse the results to achieve a lightweight vertex transformation and aggregation scheme for high-order correlation data. Additionally, we contribute the first benchmark for large-scale long-tailed datasets involving high-order correlations, known as Amazon-LT, which includes multiple datasets with varying imbalance ratios. Our experimental results demonstrate that HGHC achieves state-of-the-art performance in handling high-order correlation analysis issues for large-scale, long-tailed data.},
  archive      = {J_TPAMI},
  author       = {Xiangmin Han and Yubo Zhang and Shihui Ying and Yue Gao},
  doi          = {10.1109/TPAMI.2025.3603631},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hypergraph-based high-order correlation analysis for large-scale long-tailed data classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Creating multimodal interactive digital twin characters from videos: A dataset and baseline. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3603653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel framework for creating multimodal interactive digital twin characters, from dialogue videos of TV shows. Specifically, these digital twin characters are capable of responding to user inputs with harmonious textual, vocal, and visual content. They not only replicate the external characteristics, such as appearance and tone, but also capture internal attributes, including personality and habitual behaviors. To support this ambitious task, we collect the Multimodal Character-Centric Conversation Dataset, named MCCCD, which includes character-specific and high-quality multimodal dialogue data with detailed annotations, featuring 6.8 k utterances and 4.6 hours of audio/video per character. Notably, the MCCCD dataset is approximately ten times larger than existing datasets in terms of per-character data volume, facilitating the detailed modeling of complex character-centric traits. Further, we propose a baseline framework to create digital twin characters, consists of dialogue generation through large language models, voice generation via speech synthesis models, and visual representation with 3D talking head models. Experimental results demonstrate that our approach significantly outperforms existing methods in generating consistent and character-specific responses, setting a new benchmark for digital character creation. Our collected dataset and proposed baseline have paved the way for the creation of highly interactive and natural digital avatars, opening the door to extensive and practical applications of digital humans. The full dataset and data collection code are publicly available.},
  archive      = {J_TPAMI},
  author       = {Meidai Xuanyuan and Yuwang Wang and Honglei Guo and Hanshi Qu and Kun Zhang and Zhongming Li and Danping Yan and Tao Yu and Jianhua Tao and Qionghai Dai},
  doi          = {10.1109/TPAMI.2025.3603653},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Creating multimodal interactive digital twin characters from videos: A dataset and baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly and self-supervised class-agnostic motion prediction for autonomous driving. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3604036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.},
  archive      = {J_TPAMI},
  author       = {Ruibo Li and Hanyu Shi and Zhe Wang and Guosheng Lin},
  doi          = {10.1109/TPAMI.2025.3604036},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Weakly and self-supervised class-agnostic motion prediction for autonomous driving},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial multiview incomplete multilabel learning via uncertainty-driven reliable dynamic fusion. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3603677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, an increasing number of researchers are focusing on partial multiview incomplete multilabel learning. However, many methods generally integrate features from multiple views via an average weighting strategy, which overlooks the potential mismatch between the contribution of each view and their assigned fusion weights and thus generates unreliable fused features. To address this issue, we propose a novel uncertainty-driven reliable dynamic fusion framework for partial multiview incomplete multilabel learning. Unlike existing methods, the proposed uncertainty-driven reliable sample-level dynamic fusion module operates on the principle that samples exhibiting greater uncertainty possess fewer reliable features. This module evaluates the uncertainty of each sample and, in turn, estimates the reliability of features with the uncertainty of sample judgement, thereby obtaining reliable weights to guide the information fusion of multiple views. Furthermore, many existing approaches for handling incomplete multilabel scenarios typically concentrate on the information from annotated labels, neglecting the potential information of unknown tags. To bridge this gap, we incorporate an innovative pseudolabelling strategy that effectively identifies trustworthy pseudolabels that correspond to those unannotated uncertain labels, thereby adding additional supervisory information to assist model training. Moreover, we also devise a feature masking strategy to further augment the encoder's representation learning capabilities. The experimental results across five datasets demonstrate that our method outperforms current state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Jie Wen and Jiang Long and Xiaohuan Lu and Chengliang Liu and Xiaozhao Fang and Yong Xu},
  doi          = {10.1109/TPAMI.2025.3603677},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partial multiview incomplete multilabel learning via uncertainty-driven reliable dynamic fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EBSnoR: Event-based snow removal by optimal dwell time thresholding. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3603854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an Event-Based Snow Removal algorithm called EBSnoR. We developed a technique to measure the dwell time of snowflakes on a pixel using event-based camera data, which is used to carry out a statistically optimal dwell time thresholding to partition event stream into snowflake and background events. The effectiveness of the proposed EBSnoR was verified qualitatively on a new dataset called UDayton25EBSnow comprised of front-facing event-based camera in a car driving through snow with manually annotated bounding boxes around surrounding vehicles, as well as a quantitatively using new snowflake event simulator called EBSnoGen. Qualitatively, EBSnoR correctly identifies events corresponding to snowflakes; and quantitatively, EBSnoR showed accuracy of 96.19%. Additional experiments showed that snow removal improved event-based object detection performance.},
  archive      = {J_TPAMI},
  author       = {Abigail Wolf and Osama Alsattam and Shannon Brooks-Lehnert and Keigo Hirakawa},
  doi          = {10.1109/TPAMI.2025.3603854},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EBSnoR: Event-based snow removal by optimal dwell time thresholding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StylizedGS: Controllable stylization for 3D gaussian splatting. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3604010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As XR technology continues to advance rapidly, 3D generation and editing are increasingly crucial. Among these, stylization plays a key role in enhancing the appearance of 3D models. By utilizing stylization, users can achieve consistent artistic effects in 3D editing using a single reference style image, making it a user-friendly editing method. However, recent NeRF-based 3D stylization methods encounter efficiency issues that impact the user experience, and their implicit nature limits their ability to accurately transfer geometric pattern styles. Additionally, the ability for artists to apply flexible control over stylized scenes is considered highly desirable to foster an environment conducive to creative exploration. To address the above issues, we introduce StylizedGS, an efficient 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. We propose a filter-based refinement to eliminate floaters that affect the stylization effects in the scene reconstruction process. The nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale, and regions during the stylization to possess customization capabilities. Our method achieves high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference speed.},
  archive      = {J_TPAMI},
  author       = {Dingxi Zhang and Yu-Jie Yuan and Zhuoxun Chen and Fang-Lue Zhang and Zhenliang He and Shiguang Shan and Lin Gao},
  doi          = {10.1109/TPAMI.2025.3604010},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StylizedGS: Controllable stylization for 3D gaussian splatting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional beta-tuned diffusion model. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3604039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have gained significant attention in the field of generative modeling due to their capability to produce high-quality samples. However, recent studies show that applying a uniform treatment to all distributions during the training of diffusion models is sub-optimal. In this paper, we present a comprehensive theoretical analysis of the forward process in diffusion models. Our findings indicate that distribution variations are not uniform throughout the diffusion process, with the sharpest changes occurring during the initial stages. Moreover, we observe that the initial distribution converges to a Gaussian distribution at an exponential rate, indicating that different initial distributions rapidly become quite similar during the forward diffusion process. Consequently, employing a uniform timestep sampling strategy does not effectively capture these dynamics, potentially leading to sub-optimal training outcomes for diffusion models. To remedy this, we introduce the Bidirectional Beta-Tuned Diffusion Model (BB-TDM). The BB-TDM leverages the Beta distribution to design the timestep sampling distribution and enhance the separation between different initial distributions during the diffusion process. By selecting appropriate parameters, the BB-TDM ensures that the timestep sampling distribution is aligned with the properties of the forward diffusion process and moderates the convergence speed of different initial distributions. Extensive experiments across various benchmark datasets on different diffusion models confirm the efficacy of the proposed BB-TDM.},
  archive      = {J_TPAMI},
  author       = {Tianyi Zheng and Jiayang Zou and Peng-Tao Jiang and Hao Zhang and Jinwei Chen and Jia Wang and Bo Li},
  doi          = {10.1109/TPAMI.2025.3604039},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bidirectional beta-tuned diffusion model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NaviNeRF++: Towards interpretable 3D reconstruction via unsupervised disentangled representation learning. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3601145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction is a pivotal technology that recreates three-dimensional structures from two-dimensional representations, facilitating AI's understanding and interaction with the real world. However, existing methods pose challenges from two perspectives, i.e., a lack of understanding of the semantics behind 3D representations and the excessive reliance on extra priors to achieve 3D control. To address these challenges, we propose an interpretable 3D reconstruction framework, dubbed NaviNeRF++, to discover and identify the underlying 3D semantics by integrating multimodal large language models (MLLMs) and Neural Radiance Fields (NeRF). The model achieves fine-grained 3D disentanglement from the perspective of disentangled representation learning (DRL), while preserving high-quality and view-consistent 3D reconstruction. Specifically, the framework consists of three key components: i) a lightweight 2D perception module designed to derive an orthogonal and well-disentangled latent space with knowledge distilled from a pre-trained DRL model; ii) a NeRF-based 3D navigation module dedicated to finding semantic factors in the learned latent space, while concurrently enabling high-quality 3D reconstruction; and iii) an attribute identification module that identifies textual concepts of semantic factors by leveraging the commonsense knowledge of MLLMs. To our knowledge, this work is the first to achieve interpretable 3D reconstruction and fine-grained 3D disentanglement in an unsupervised manner. Empirical results further demonstrate its superior performance compared to off-the-shelf solutions.},
  archive      = {J_TPAMI},
  author       = {Baao Xie and Zequn Zhang and Huanting Guo and Qiuyu Chen and Hu Zhu and Bohan Li and Wenjun Zeng and Xin Jin},
  doi          = {10.1109/TPAMI.2025.3601145},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NaviNeRF++: Towards interpretable 3D reconstruction via unsupervised disentangled representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spherical vision transformers for audio-visual saliency prediction in 360$^{\circ }$ videos. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3604091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional videos (ODVs) are redefining viewer experiences in virtual reality (VR) by offering an unprecedented full field-of-view (FOV). This study extends the domain of saliency prediction to 360$^{\circ }$ environments, addressing the complexities of spherical distortion and the integration of spatial audio. Contextually, ODVs have transformed user experience by adding a spatial audio dimension that aligns sound direction with the viewer's perspective in spherical scenes. Motivated by the lack of comprehensive datasets for 360$^{\circ }$ audio-visual saliency prediction, our study curates YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying audio-visual conditions. Our goal is to explore how to utilize audio-visual cues to effectively predict visual saliency in 360$^{\circ }$ videos. Towards this aim, we propose two novel saliency prediction models: SalViT360, a vision-transformer-based framework for ODVs equipped with spherical geometry-aware spatio-temporal attention layers, and SalViT360-AV, which further incorporates transformer adapters conditioned on audio input. Our results on a number of benchmark datasets, including our YT360-EyeTracking, demonstrate that SalViT360 and SalViT360-AV significantly outperform existing methods in predicting viewer attention in 360$^{\circ }$ scenes. Interpreting these results, we suggest that integrating spatial audio cues in the model architecture is crucial for accurate saliency prediction in omnidirectional videos. Code and dataset will be available at: https://cyberiada.github.io/SalViT360/.},
  archive      = {J_TPAMI},
  author       = {Mert Cokelek and Halit Ozsoy and Nevrez Imamoglu and Cagri Ozcinar and Inci Ayhan and Erkut Erdem and Aykut Erdem},
  doi          = {10.1109/TPAMI.2025.3604091},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spherical vision transformers for audio-visual saliency prediction in 360$^{\circ }$ videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamWaltz-G: Expressive 3D gaussian avatars from skeleton-guided 2D diffusion. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3586284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition. Codes and trained models are released at https://github.com/Yukun-Huang/DreamWaltz-G},
  archive      = {J_TPAMI},
  author       = {Yukun Huang and Jianan Wang and Ailing Zeng and Zheng-Jun Zha and Lei Zhang and Xihui Liu},
  doi          = {10.1109/TPAMI.2025.3586284},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamWaltz-G: Expressive 3D gaussian avatars from skeleton-guided 2D diffusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifying graph contrastive learning via graph message augmentation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3586651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs. As we know that GDA is an important issue for graph contrastive learning. Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes. However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data. To address this issue, in this paper, we first introduce the graph message representation of graph data. Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs. The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks. Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs. Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning. Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches.},
  archive      = {J_TPAMI},
  author       = {Ziyan Zhang and Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3586651},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unifying graph contrastive learning via graph message augmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPOT: Scalable 3D pre-training via occupancy prediction for learning transferable 3D representations. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3586961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating 3D LiDAR point clouds for perception tasks is fundamental for many applications e.g. autonomous driving, yet it still remains notoriously labor-intensive. Pretraining-finetuning approach can alleviate the labeling burden by fine-tuning a pre-trained backbone across various downstream datasets as well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training via Occupancy prediction for learning Transferable 3D representations under such a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on various public datasets with different downstream tasks, showcasing its general representation power, cross-domain robustness and data scalability which are three key factors for real-world application. Specifically, we both theoretically and empirically show, for the first time, that general representations learning can be achieved through the task of occupancy prediction. Then, to address the domain gap caused by different LiDAR sensors and annotation methods, we develop a beam re-sampling technique for point cloud augmentation combined with class-balancing strategy. Furthermore, scalable pre-training is observed, that is, the downstream performance across all the experiments gets better with more pre-training data. Additionally, such pre-training strategy also remains compatible with unlabeled data. The hope is that our findings will facilitate the understanding of LiDAR points and pave the way for future advancements in LiDAR pre-training.},
  archive      = {J_TPAMI},
  author       = {Xiangchao Yan and Runjian Chen and Bo Zhang and Hancheng Ye and Renqiu Xia and Jiakang Yuan and Hongbin Zhou and Xinyu Cai and Botian Shi and Wenqi Shao and Ping Luo and Yu Qiao and Tao Chen and Junchi Yan},
  doi          = {10.1109/TPAMI.2025.3586961},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SPOT: Scalable 3D pre-training via occupancy prediction for learning transferable 3D representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ManiDext: Hand-object manipulation synthesis via continuous correspondence embeddings and residual-guided diffusion. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3588302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic and dexterous manipulation of objects presents a complex challenge, requiring the synchronization of hand motions with the trajectories of objects to achieve seamless and physically plausible interactions. In this work, we introduce ManiDext, a unified hierarchical diffusion-based framework for generating hand manipulation and grasp poses based on 3D object trajectories. Our key insight is that accurately modeling the contact correspondences between objects and hands during interactions is crucial. Therefore, we propose a continuous correspondence embedding representation that specifies detailed hand correspondences at the vertex level between the object and the hand. This embedding is optimized directly on the hand mesh in a self-supervised manner, with the distance between embeddings reflecting the geodesic distance. Our framework first generates contact maps and correspondence embeddings on the object's surface. Based on these fine-grained correspondences, we introduce a novel approach that integrates the iterative refinement process into the diffusion process during the second stage of hand pose generation. At each step of the denoising process, we incorporate the current hand pose residual as a refinement target into the network, guiding the network to correct inaccurate hand poses. Introducing residuals into each denoising step inherently aligns with traditional optimization process, effectively merging generation and refinement into a single unified framework. Extensive experiments demonstrate that our approach can generate physically plausible and highly realistic motions for various tasks, including single and bimanual hand grasping as well as manipulating both rigid and articulated objects.},
  archive      = {J_TPAMI},
  author       = {Jiajun Zhang and Yuxiang Zhang and Liang An and Mengcheng Li and Hongwen Zhang and Zonghai Hu and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3588302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ManiDext: Hand-object manipulation synthesis via continuous correspondence embeddings and residual-guided diffusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ins-HOI: Instance aware human-object interactions recovery. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3588268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately modeling detailed interactions between human/hand and object is an appealing yet challenging task. Current multi-view capture systems are only capable of reconstructing multiple subjects into a single, unified mesh, which fails to model the states of each instance individually during interactions. To address this, previous methods use template-based representations to track human/hand and object. However, the quality of the reconstructions is limited by the descriptive capabilities of the templates so these methods inherently struggle with geometric details, pressing deformations and invisible contact surfaces. In this work, we propose an end-to-end Instance-aware Human-Object Interactions recovery (Ins-HOI) framework by introducing an instance-level occupancy field representation. However, the real-captured data is presented as a holistic mesh, unable to provide instance-level supervision. To address this, we further propose a complementary training strategy that leverages synthetic data to introduce instance-level shape priors, enabling the disentanglement of occupancy fields for different instances. Specifically, synthetic data, created by randomly combining individual scans of humans/hands and objects, guides the network to learn a coarse prior of instances. Meanwhile, real-captured data helps in learning the overall geometry and restricting interpenetration in contact areas. As demonstrated in experiments, our method Ins-HOI supports instance-level reconstruction and provides reasonable and realistic invisible contact surfaces even in cases of extremely close interaction. To facilitate research on this task, we collect a large-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans with real-world human-chair and hand-object interactions. The code and data will be public for research purposes.},
  archive      = {J_TPAMI},
  author       = {Jiajun Zhang and Yuxiang Zhang and Hongwen Zhang and Xiao Zhou and Boyao Zhou and Ruizhi Shao and Zonghai Hu and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3588268},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ins-HOI: Instance aware human-object interactions recovery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClusMatch: Improving deep clustering by unified positive and negative pseudo-label learning. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3588239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep clustering methods have achieved remarkable results compared to traditional clustering approaches. However, its performance remains constrained by the absence of annotations. A thought-provoking observation is that there is still a significant gap between deep clustering and semi-supervised classification methods. Even with only a few labeled samples, the accuracy of semi-supervised learning is much higher than that of clustering. Given that we can annotate a small number of samples in a certain unsupervised way, the clustering task can be naturally transformed into a semi-supervised setting, thereby achieving comparable performance. Based on this intuition, we propose ClusMatch, a unified positive and negative pseudo-label learning based semi-supervised learning framework, which is pluggable and can be applied to existing deep clustering methods. Specifically, we first leverage the pre-trained deep clustering network to compute predictions for all samples, and then design specialized selection strategies to pick out a few high-quality samples as labeled samples for supervised learning. For the unselected samples, the novel unified positive and negative pseudo-label learning is introduced to provide additional supervised signals for semi-supervised fine-tuning. We also propose an adaptive positive-negative threshold learning strategy to further enhance the confidence of generated pseudo-labels. Extensive experiments on six widely-used datasets and one large-scale dataset demonstrate the superiority of our proposed ClusMatch. For example, ClusMatch achieves a significant accuracy improvement of 5.4% over the state-of-the-art method ProPos on an average of these six datasets. Source code can be found at https://github.com/XY-ATOE/ClusMatch.},
  archive      = {J_TPAMI},
  author       = {Jianlong Wu and Zihan Li and Wei Sun and Jianhua Yin and Liqiang Nie and Zhouchen Lin},
  doi          = {10.1109/TPAMI.2025.3588239},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ClusMatch: Improving deep clustering by unified positive and negative pseudo-label learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal graph mamba for music-guided dance video synthesis. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3588237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the music-guided dance video synthesis task, i.e., to translate the input music to a dance video. STG-Mamba consists of two translation mappings: music-to-skeleton translation and skeleton-to-video translation. In the music-to-skeleton translation, we introduce a novel spatial-temporal graph Mamba (STGM) block to effectively construct skeleton sequences from the input music, capturing dependencies between joints in both the spatial and temporal dimensions. For the skeleton-to-video translation, we propose a novel self-supervised regularization network to translate the generated skeletons, along with a conditional image, into a dance video. Lastly, we collect a new skeleton-to-video translation dataset from the Internet, containing 54,944 video clips. Extensive experiments demonstrate that STG-Mamba achieves significantly better results than existing methods.},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Ling Shao and Zhenyu Zhang and Luc Van Gool and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3588237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spatial-temporal graph mamba for music-guided dance video synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient visual transformer by learnable token merging. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3588186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of mask module in our LTM blocks which generate the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers. The code of the LTM-Transformer is available at https://github.com/Statistical-Deep-Learning/LTM.},
  archive      = {J_TPAMI},
  author       = {Yancheng Wang and Yingzhen Yang},
  doi          = {10.1109/TPAMI.2025.3588186},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient visual transformer by learnable token merging},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One neuron saved is one neuron earned: On parametric efficiency of quadratic networks. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3588894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by neuronal diversity in the biological neural system, a plethora of studies proposed to design novel types of artificial neurons and introduce neuronal diversity into artificial neural networks. Recently proposed quadratic neuron, which replaces the inner-product operation in conventional neurons with a quadratic one, have achieved great success in many essential tasks. Despite the promising results of quadratic neurons, there is still an unresolved issue: Is the superior performance of quadratic networks simply due to the increased parameters or due to the intrinsic expressive capability? Without clarifying this issue, the performance of quadratic networks is always suspicious. Additionally, resolving this issue is reduced to finding killer applications of quadratic networks. In this paper, with theoretical and empirical studies, we show that quadratic networks enjoy parametric efficiency, thereby confirming that the superior performance of quadratic networks is due to the intrinsic expressive capability. This intrinsic expressive ability comes from that quadratic neurons can easily represent nonlinear interaction, while it is hard for conventional neurons. Theoretically, we derive the approximation efficiency of quadratic networks over conventional ones in terms of real space and manifolds. Moreover, from the perspective of the Barron space, we demonstrate that there exists a functional space whose functions can be approximated by quadratic networks in a dimension-free error, but the approximation error of conventional networks is dependent on dimensions. Empirically, experimental results on synthetic data, classic benchmarks, and real-world applications show that quadratic models broadly enjoy parametric efficiency, and the gain of efficiency depends on the task. We have shared our code in https://github.com/asdvfghg/quadratic_efficiency.},
  archive      = {J_TPAMI},
  author       = {Feng-Lei Fan and Hang-Cheng Dong and Zhongming Wu and Lecheng Ruan and Tieyong Zeng and Yiming Cui and Jing-Xiao Liao},
  doi          = {10.1109/TPAMI.2025.3588894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {One neuron saved is one neuron earned: On parametric efficiency of quadratic networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TV-3DG: Mastering text-to-3D customized generation with visual prompt. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3587105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, advancements in generative models have significantly expanded the capabilities of text-to-3D generation. Many approaches rely on Score Distillation Sampling (SDS) technology. However, SDS struggles to accommodate multi-condition inputs, such as text and visual prompts, in customized generation tasks. To explore the core reasons, we decompose SDS into a difference term and a classifier-free guidance term. Our analysis identifies the core issue as arising from the difference term and the random noise addition during the optimization process, both contributing to deviations from the target mode during distillation. To address this, we propose a novel algorithm, Classifier Score Matching (CSM), which removes the difference term in SDS and uses a deterministic noise addition process to reduce noise during optimization, effectively overcoming the low-quality limitations of SDS in our customized generation framework. Based on CSM, we integrate visual prompt information with an attention fusion mechanism and sampling guidance techniques, forming the Visual Prompt CSM (VPCSM) algorithm. Furthermore, we introduce a Semantic-Geometry Calibration (SGC) module to enhance quality through improved textual information integration. We present our approach as TV-3DG, with extensive experiments demonstrating its capability to achieve stable, high-quality, customized 3D generation. Project page: https://yjhboy.github.io/TV-3DG},
  archive      = {J_TPAMI},
  author       = {Jiahui Yang and Donglin Di and Baorui Ma and Jianxun Cui and Xun Yang and Yongjia Ma and Wenzhang Sun and Wei Chen and Zhou Xue and Meng Wang and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3587105},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TV-3DG: Mastering text-to-3D customized generation with visual prompt},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-boosting self-collaboration parallel prompt GAN for unsupervised image restoration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3589606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have demonstrated state-of-the-art performance in image restoration, especially when trained on large-scale paired datasets. However, acquiring paired data in real-world scenarios poses a significant challenge. Unsupervised restoration approaches based on generative adversarial networks (GANs) offer a promising solution without requiring paired datasets. Yet, these GAN-based approaches struggle to surpass the performance of conventional unsupervised GAN-based frameworks without significantly modifying model structures or increasing the computational complexity. To address these issues, we propose a self-collaboration (SC) strategy for existing restoration models. This strategy utilizes information from the previous stage as feedback to guide subsequent stages, achieving significant performance improvement without increasing the framework's inference complexity. The SC strategy comprises a prompt learning (PL) module and a restorer ($Res$). It iteratively replaces the previous less powerful fixed restorer $\overline{Res}$ in the PL module with a more powerful $Res$. The enhanced PL module generates better pseudo-degraded/clean image pairs, leading to a more powerful $Res$ for the next iteration. Our SC can significantly improve the $Res$ 's performance by over 1.5dB without adding extra parameters or computational complexity during inference. Meanwhile, existing self-ensemble (SE) and our SC strategies enhance the performance of pre-trained restorers from different perspectives. As SE increases computational complexity during inference, we propose a re-boosting module to the SC (Reb-SC) to improve the SC strategy further by incorporating SE into SC without increasing inference time. This approach further enhances the restorer's performance by approximately 0.3 dB. Additionally, we present a baseline framework that includes parallel generative adversarial branches with complementary “self-synthesis” and “unpaired-synthesis” constraints, ensuring the effectiveness of the training framework. Extensive experimental results on restoration tasks demonstrate that the proposed model performs favorably against existing state-of-the-art unsupervised restoration methods. Source code and trained models are publicly available at: https://github.com/linxin0/RSCP2GAN.},
  archive      = {J_TPAMI},
  author       = {Xin Lin and Yuyan Zhou and Jingtong Yue and Chao Ren and Kelvin C.K. Chan and Lu Qi and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2025.3589606},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Re-boosting self-collaboration parallel prompt GAN for unsupervised image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable random feature latent variable models. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3589728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random feature latent variable models (RFLVMs) are state-of-the-art tools for uncovering structure in high-dimensional, non-Gaussian data. However, their reliance on Monte Carlo sampling significantly limits scalability, posing challenges for large-scale applications. To overcome these limitations, we develop a scalable RFLVM framework based on variational Bayesian inference (VBI), a deterministic and optimization-based alternative to sampling methods. Applying VBI to RFLVMs is nontrivial due to two key challenges: (i) the lack of an explicit probability density function (PDF) for Dirichlet process (DP) mixing weights, and (ii) the inefficiency of existing VBI approaches when handling the high-dimensional variational parameters of RFLVMs. To address these issues, we adopt the stick-breaking construction for the DP, which provides an explicit and tractable PDF over mixing weights, and propose a novel inference algorithm, block coordinate descent variational inference (BCD-VI), which partitions variational parameters into blocks and applies tailored solvers to optimize them efficiently. The resulting scalable model, referred to as SRFLVM, supports various likelihoods; we demonstrate its effectiveness under Gaussian and logistic settings. Extensive experiments on diverse benchmark datasets show that SRFLVM achieves superior scalability, computational efficiency, and performance in latent representation learning and missing data imputation, consistently outperforming state-of-the-art latent variable models, including deep generative approaches.},
  archive      = {J_TPAMI},
  author       = {Ying Li and Zhidi Lin and Yuhao Liu and Michael Minyi Zhang and Pablo M. Olmos and Petar M. Djurić},
  doi          = {10.1109/TPAMI.2025.3589728},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scalable random feature latent variable models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAG3D++: A benchmark for 3D fitness activity comprehension with language instruction. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3590012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of general human action understanding. However, when applied to real-world applications such as sports analysis, most existing datasets are still unsatisfactory, because of the limitations in rich labels on multiple tasks, language instructions, high-quality 3D data, and diverse environments. In this paper, we present FLAG3D++, a large-scale benchmark for 3D fitness activity comprehension, which contains 180 K sequences of 60 activity categories with language instruction. FLAG3D++ features the following four aspects: 1) fine-grained annotations of the temporal intervals of actions in the untrimmed long sequences and how well these actions are performed, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 4) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. In light of the specified features, we present two new practical applications as language-guided repetition action counting (L-RAC) and language-guided action quality assessment (L-AQA), which aim to take the language descriptions as references to count the repetitive times of an action and assess the quality of action respectively. Furthermore, we propose a Hierarchical Language-Guided Graph Convolutional Network (HL-GCN) model to better fuse the language information and skeleton sequences for L-RAC and L-AQA. To be specific, the HL-GCN performs cross-modal alignments by the early fusion of the linguistic feature and the hierarchical node features of the skeleton-based sequences encoded by the multiple intermediate graph convolutional layers. Extensive experiments show the superiority of our HL-GCN on both L-RAC and L-AQA, as well as the great research value of FLAG3D++ for various challenges, such as dynamic human mesh recovery and cross-domain human action recognition. Our dataset, source code, and trained models are made publicly available at FLAG3D++.},
  archive      = {J_TPAMI},
  author       = {Yansong Tang and Aoyang Liu and Jinpeng Liu and Shiyi Zhang and Wenxun Dai and Jie Zhou and Xiu Li and Jiwen Lu},
  doi          = {10.1109/TPAMI.2025.3590012},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FLAG3D++: A benchmark for 3D fitness activity comprehension with language instruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inv-adapter: ID customization generation via image inversion and lightweight parameter adapter. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3590321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable advancement in text-to-image generation models significantly boosts the research in ID customization generation. However, existing personalization methods cannot simultaneously satisfy high-fidelity and low-costs requirements. Their main bottleneck lies in the additional prompt image encoder (i.e., CLIP vision encoder), which produces weak alignment signals with the text-to-image model that may lose face information and is not well ‘absorbed’ by the text-to-image model. Towards this end, we propose Inv-Adapter, which first introduces a more reasonable and efficient token representation of ID image features and introduces a lightweight parameter adaptor to inject ID features. Specifically, our Inv-Adapter extracts diffusion-domain representations of ID images utilizing a pre-trained text-to-image model via DDIM image inversion, without an additional image encoder. Benefiting from the high alignment of the extracted ID prompt features and the intermediate features of the text-to-image model, we then introduce a lightweight attention adapter to embed them efficiently into the base text-to-image model. We conduct extensive experiments on different text-to-image models to assess ID fidelity, generation loyalty, speed, training costs, model scale and generalization ability in scenarios of general object, all of which show that the proposed Inv-Adapter is highly competitive in ID customization generation and model scale.},
  archive      = {J_TPAMI},
  author       = {Peng Xing and Ning Wang and Jianbo Ouyang and Zechao Li},
  doi          = {10.1109/TPAMI.2025.3590321},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Inv-adapter: ID customization generation via image inversion and lightweight parameter adapter},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilingual-prompt-guided directional feature learning for weakly supervised video anomaly detection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3590242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video anomaly detection has gained attention for its effective performance and cost-efficient annotation, using video-level labels to distinguish between normal and abnormal patterns. However, challenges arise from the diversity and incompleteness of anomalous events, complicating feature learning. Vision-language models offer promising approaches, but designing precise prompts remains difficult. This is because accommodating the diverse range of normal and anomalous scenarios in real-world settings is challenging, and the workload is significant. To tackle these issues, we propose integrating multilingualism and multiple prompts to improve feature learning. By utilizing prompts in various languages to define “anomaly” and “normalcy,” we tackle these concepts across different linguistic domains. In each domain, multiple prompts are employed for adaptive top-K prompt selection of snippets. To enhance visual feature learning, a multi-granularity attention module combining Transformer and Mamba is designed. Mamba's long-range adaptation selection builds fine-grained temporal correlations among coarse-grained snippets, while Transformer enhances fine-grained information guided by coarse-grained information. Alongside a multilingual prompt guidance loss, we introduce a gradual directional loss to jointly optimize visual feature distribution and the top-K prompt selection. Our method demonstrates effectiveness on four video datasets and provides generalizability analyses on two medical datasets, including EMG and ECG temporal data.},
  archive      = {J_TPAMI},
  author       = {Chizhuo Xiao and Yang Xiao and Joey Tianyi Zhou and Zhiwen Fang},
  doi          = {10.1109/TPAMI.2025.3590242},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multilingual-prompt-guided directional feature learning for weakly supervised video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Another vertical view: A hierarchical network for heterogeneous trajectory prediction via spectrums. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3590487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast development of AI-related techniques, the applications of trajectory prediction are no longer limited to easier scenes and trajectories. More and more trajectories with different forms, such as coordinates, bounding boxes, and even high-dimensional human skeletons, need to be analyzed and forecasted. Among these heterogeneous trajectories, interactions between different elements within a frame of trajectory, which we call “Dimension-wise Interactions”, would be more complex and challenging. However, most previous approaches focus mainly on a specific form of trajectories, and potential dimension-wise interactions are less concerned. In this work, we expand the trajectory prediction task by introducing the trajectory dimensionality $M$, thus extending its application scenarios to heterogeneous trajectories. We first introduce the Haar transform as an alternative to the Fourier transform to better capture the time-frequency properties of each trajectory-dimension. Then, we adopt the bilinear structure to model and fuse two factors simultaneously, including the time-frequency response and the dimension-wise interaction, to forecast heterogeneous trajectories via trajectory spectrums hierarchically in a generic way. Experiments show that the proposed model outperforms most state-of-the-art methods on ETH-UCY, SDD, nuScenes, and Human3.6M with heterogeneous trajectories, including 2D coordinates, 2D/3D bounding boxes, and 3D human skeletons.},
  archive      = {J_TPAMI},
  author       = {Beihao Xia and Conghao Wong and Duanquan Xu and Qinmu Peng and Xinge You},
  doi          = {10.1109/TPAMI.2025.3590487},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Another vertical view: A hierarchical network for heterogeneous trajectory prediction via spectrums},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPIC-SOUNDS: A large-scale dataset of actions that sound. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3590390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations capturing temporal extents and class labels within the audio stream of the egocentric videos. We propose an annotation pipeline where annotators temporally label distinguishable audio segments and describe the action that could have caused this sound. We identify actions that can be discriminated purely from audio, through grouping these free-form descriptions of audio into classes. For actions that involve objects colliding, we collect human annotations of the materials of these objects (e.g. a glass object being placed on a wooden surface), which we verify from video, discarding ambiguities. Overall, EPIC-SOUNDS includes 78.4 k categorised segments of audible events and actions, distributed across 44 classes as well as 39.2 k non-categorised segments. We train and evaluate state-of-the-art audio recognition and detection models on our dataset, for both audio-only and audio-visual methods. We also conduct analysis on: the temporal overlap between audio events, the temporal and label correlations between audio and visual modalities, the ambiguities in annotating materials from audio-only input, the importance of audio-only labels and the limitations of current models to understand actions that sound.},
  archive      = {J_TPAMI},
  author       = {Jaesung Huh and Jacob Chalk and Evangelos Kazakos and Dima Damen and Andrew Zisserman},
  doi          = {10.1109/TPAMI.2025.3590390},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EPIC-SOUNDS: A large-scale dataset of actions that sound},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating zero-shot NAS with feature map-based proxy and operation scoring function. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3590342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has been extensively studied due to its ability in automatic architecture engineering. Existing NAS methods rely heavily on the gradients and data labels, which either incur immense computational costs or suffer from discretization discrepancy due to the supernet structure. Moreover, the majority of them are limited in generating diverse architectures. To alleviate these issues, in this paper, we propose a novel zero-cost proxy called $\mathsf {MeCo}$ based on the Pearson correlation matrix of the feature maps. Unlike the previous work, the computation of $\mathsf {MeCo}$ as well as its variant $\mathsf {MeCo_{opt}}$ requires only one random data for a single forward pass. Based on the proposed zero-cost proxy, we further craft a new zero-shot NAS scheme called $\mathsf {FLASH}$, which harnesses a new proxy-based operation scoring function and a greedy heuristic. Compared to the existing methods, $\mathsf {FLASH}$ is highly efficient and can construct diverse model architectures instead of repeated cells. We design comprehensive experiments and extensively evaluate our designs on multiple benchmarks and datasets. The experimental results show that our method is one to six orders of magnitudes more efficient than the state-of-the-art baselines with the highest model accuracy.},
  archive      = {J_TPAMI},
  author       = {Tangyu Jiang and Haodi Wang and Rongfang Bie and Chun Yuan},
  doi          = {10.1109/TPAMI.2025.3590342},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Accelerating zero-shot NAS with feature map-based proxy and operation scoring function},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-GAN: Data-efficient GANs training via architectural reconfiguration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3590650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of Generative Adversarial Networks (GANs) for high-fidelity images has predominantly relied on large-scale datasets. Emerging research, particularly on GANs ‘lottery tickets’, suggests that dense GANs models have sparse sub-networks capable of superior performance with limited data. However, the conventional process to uncover these ‘lottery tickets’ involves a resource-intensive train-prune-retrain cycle. Addressing this, our paper introduces Re-GAN, a novel, dataefficient approach for GANs training that dynamically reconfigures the GANs architecture during training. This method focuses on iterative pruning of non-important connections and regrowing them, thereby preventing premature loss of important features and maintaining the model's representational strength. Re-GAN provides a more stable and efficient solution for GANs models with limited data, offering an alternative to existing progressive growing methods and GANs tickets. While Re-GAN has already demonstrated its potential in image generation across diverse datasets, domains, and resolutions, in this paper, we significantly expand our study. We incorporate new applications, notably Image-to-Image translation, include additional datasets, provide in-depth analyses, and explore compatibility with data augmentation techniques. This expansion not only broadens the scope of Re-GAN but also establishes it as a generic training methodology, demonstrating its effectiveness and adaptability in different GANs scenarios. Code is available at https://github.com/IntellicentAI-lab/Re-GAN},
  archive      = {J_TPAMI},
  author       = {Divya Saxena and Jiannong Cao and Jiahao Xu and Tarun Kulshrestha},
  doi          = {10.1109/TPAMI.2025.3590650},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Re-GAN: Data-efficient GANs training via architectural reconfiguration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced dual-pattern matching with vision-language representation for out-of-distribution detection. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3590717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection presents a significant challenge in deploying pattern recognition and machine learning models, as they frequently fail to generalize to data from unseen distributions. Recent advancements in vision-language models (VLMs), particularly CLIP, have demonstrated promising results in OOD detection through their rich multimodal representations. However, current CLIP-based OOD detection methods predominantly rely on single-modality in-distribution (ID) data (e.g., textual cues), overlooking the valuable information contained in ID visual cues. In this work, we demonstrate that incorporating ID visual information is crucial for unlocking CLIP's full potential in OOD detection. We propose a novel approach, Dual-Pattern Matching (DPM), which effectively adapts CLIP for OOD detection by jointly exploiting both textual and visual ID patterns. Specifically, DPM refines visual and textual features through the proposed Domain-Specific Feature Aggregation (DSFA) and Prompt Enhancement (PE) modules. Subsequently, DPM stores class-wise textual features as textual patterns and aggregates ID visual features as visual patterns. During inference, DPM calculates similarity scores relative to both patterns to identify OOD data. Furthermore, we enhance DPM with lightweight adaptation mechanisms to further boost OOD detection performance. Comprehensive experiments demonstrate that DPM surpasses state-of-the-art methods on multiple benchmarks, highlighting the effectiveness of leveraging multimodal information for OOD detection. The proposed dual-pattern approach provides a simple yet robust framework for leveraging vision-language representations in OOD detection tasks.},
  archive      = {J_TPAMI},
  author       = {Xiang Xiang and Zhuo Xu and Zihan Zhang and Zhigang Zeng and Xilin Chen},
  doi          = {10.1109/TPAMI.2025.3590717},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced dual-pattern matching with vision-language representation for out-of-distribution detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning text-to-image diffusion models with constrained reinforcement learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3590730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reward finetuning has emerged as a powerful technique for aligning diffusion models with specific downstream objectives or user preferences. However, current approaches suffer from a persistent challenge of reward overoptimization, where models exploit imperfect reward feedback at the expense of overall performance. In this work, we identify three key contributors to overoptimization: (1) a granularity mismatch between the multi-step diffusion process and sparse rewards; (2) a loss of plasticity that limits the model's ability to adapt and generalize; and (3) an overly narrow focus on a single reward objective that neglects complementary performance criteria. Accordingly, we introduce Constrained Diffusion Policy Optimization (CDPO), a novel reinforcement learning framework that addresses reward overoptimization from multiple angles. Firstly, CDPO tackles the granularity mismatch through a temporal policy optimization strategy that delivers step-specific rewards throughout the entire diffusion trajectory, thereby reducing the risk of overfitting to sparse final-step rewards. Then we incorporate a neuron reset strategy that selectively resets overactive neurons in the model, preventing overoptimization induced by plasticity loss. Finally, to avoid overfitting to a narrow reward objective, we integrate constrained reinforcement learning with auxiliary reward objectives serving as explicit constraints, ensuring a balanced optimization across diverse performance metrics.},
  archive      = {J_TPAMI},
  author       = {Ziyi Zhang and Sen Zhang and Li Shen and Yibing Zhan and Yong Luo and Han Hu and Bo Du and Yonggang Wen and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3590730},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Aligning text-to-image diffusion models with constrained reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards OOD object detection with unknown-concept guided feature diffusion. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3590735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, learning plentiful knowledge corresponding to known objects is an important ability for humans. The unknown objects could be assumed to depart from the familiar knowledge. Inspired by this idea, we explore leveraging the extracted knowledge to reason a set of unknown concepts. And they could be used to address unsupervised out-of-distribution object detection (OOD-OD) that aims to detect unseen OOD objects without accessing any auxiliary OOD data during training. To this end, we propose a new approach, i.e., Unknown-Concept Guided Feature Diffusion (UCFD), including an object-related knowledge extractor and an unknown-concept guided diffusor for synthesizing virtual OOD features. Specifically, we define multiple learnable codewords to capture object-relevant visual knowledge from all object categories. To avoid the detection performance degradation of the in-distribution (ID) objects, these codewords are utilized to enhance object features. Next, an unknown-concept pool is constructed by mixing up these extracted codewords. Finally, to reduce the impact of lacking OOD data for supervision, we design an unknown-concept guided diffusor, which leverages the sampled unknown concepts from the pool to guide the reverse process to generate expected OOD features that deviate from the familiar knowledge. The significant performance gains on three different tasks demonstrate the superiorities of our method. Meanwhile, extensive visualization results show that our method could synthesize effective virtual OOD features.},
  archive      = {J_TPAMI},
  author       = {Aming Wu and Cheng Deng},
  doi          = {10.1109/TPAMI.2025.3590735},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards OOD object detection with unknown-concept guided feature diffusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marigold: Affordable adaptation of diffusion-based image generators for image analysis. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3591076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io.},
  archive      = {J_TPAMI},
  author       = {Bingxin Ke and Kevin Qu and Tianfu Wang and Nando Metzger and Shengyu Huang and Bo Li and Anton Obukhov and Konrad Schindler},
  doi          = {10.1109/TPAMI.2025.3591076},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Marigold: Affordable adaptation of diffusion-based image generators for image analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointLLM-v2: Empowering large language models to better understand point clouds. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3590784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, empowering LLMs to understand point clouds and offering a new avenue beyond 2D data. PointLLM understands colored object point clouds with human instructions, including coordinate-based part specifications, and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. To overcome the scarcity of point-text instruction following data, we developed an automated data generation pipeline, collecting a large-scale dataset of about 1.8M samples with 1M different 3D objects, which facilitates the adoption of the two-stage training strategy prevalent in MLLM development. Additionally, we address the absence of appropriate benchmarks and the limitations of current evaluation metrics by proposing two novel benchmarks: Generative 3D Object Classification and 3D Object Captioning, which are supported by new, comprehensive evaluation metrics derived from human and GPT analyses. Through exploring various training strategies, we develop PointLLM, significantly outperforming 2D and 3D baselines and achieving SOTA performance, with a notable achievement in object captioning tasks where it surpasses human annotators in over 50% of the samples. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/PointLLM.},
  archive      = {J_TPAMI},
  author       = {Runsen Xu and Shuai Yang and Xiaolong Wang and Tai Wang and Yilun Chen and Jiangmiao Pang and Dahua Lin},
  doi          = {10.1109/TPAMI.2025.3590784},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PointLLM-v2: Empowering large language models to better understand point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sel4FT: Annotation selection for pretraining-finetuning with distribution shift. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3591018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pretraining-finetuning paradigm has become dominant in computer vision, yet strategically exploiting limited annotation budgets during finetuning remains unexplored. We introduce active finetuning—a novel task for selecting the most informative samples to annotate within this paradigm. We propose Sel4FT, a unified annotation selection framework that optimizes a parametric model in continuous feature space to identify a subset preserving the entire pool's distribution while maintaining diversity. To address distribution shifts from data augmentation, we develop Sel4FT++ with augmentation-aware selection mechanisms. We theoretically prove our approach minimizes the Earth Mover's Distance between selected subset and full data pool. Our framework eliminates iterative retraining and annotation process during selection, providing an efficient solution for real-world deployment. Extensive experiments on image classification, long-tailed recognition, and semantic segmentation demonstrate state-of-the-art performance with over 100× speedup compared to existing methods. Code is released at https://github.com/yichen928/ActiveFT.},
  archive      = {J_TPAMI},
  author       = {Han Lu and Yichen Xie and Mingyu Ding and Wei Zhan and Xiaokang Yang and Masayoshi Tomizuka and Junchi Yan},
  doi          = {10.1109/TPAMI.2025.3591018},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sel4FT: Annotation selection for pretraining-finetuning with distribution shift},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monocular-to-3D virtual try-on with generative semantic articulated fields. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3591072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a monocular-to-3D virtual try-on network based on a conditional 3D-aware Generative Adversarial Network (3D-GAN) for synthesizing multi-view try-on results from single monocular images. In contrast to previous 3D virtual try-on methods that rely on costly scanned meshes or pseudo-depth maps for supervision, our approach utilizes a conditional 3D-GAN trained solely on 2D images, greatly simplifying dataset construction and enhancing model scalability. Specifically, we propose a Generative monocular-to-3D Virtual Try-ON network (G3D-VTON) that integrates a 3D-aware conditional Parsing Module (3DPM), a U-Net Refinement Module (URM), and a Flow-based 2D Virtual Try-On Module (FTM). In our framework, the 3DPM is designed to generate a 3D representation of the virtual try-on result, thereby enabling multi-view rendering. To accomplish this, it is implemented using conditional generative semantic articulated fields, which leverage the 3D SMPL prior via inverse skinning to learn the Signed Distance Function (SDF) of the try-on results in a canonical pose space. This learned SDF enables the rendering of both a coarse human parsing map and a preliminary try-on output with explicit camera control. Furthermore, within 3DPM, we introduce deferred pose guidance to decouple style and pose conditions during training, thereby facilitating view controllable generation during inference. However, the rendered human parsing and try-on results exhibit imprecise shapes and blurry textures. To address these issues, the URM subsequently refines these rendered outputs using a refinement U-Net, and the FTM integrates the refined results with the 2D warped garment to generate the final try-on output with more accurate and realistic appearance details. Extensive experiments demonstrate that the proposed G3D-VTON effectively manipulates and generates faithful 3D human appearances wearing the desired garment, outperforming both 3D-GAN and depth-based 3D approaches while delivering superior visual results in 2D.},
  archive      = {J_TPAMI},
  author       = {Zhenyu Xie and Fuwei Zhao and Jun Zheng and Xin Dong and Feida Zhu and Xiaodan Liang},
  doi          = {10.1109/TPAMI.2025.3591072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Monocular-to-3D virtual try-on with generative semantic articulated fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 360VOTS: Visual object tracking and segmentation in omnidirectional videos. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3591725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360$^{\circ }$ images. To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks. Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS). The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories. To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences. Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment. Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset.},
  archive      = {J_TPAMI},
  author       = {Yinzhe Xu and Huajian Huang and Yingshu Chen and Sai-Kit Yeung},
  doi          = {10.1109/TPAMI.2025.3591725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {360VOTS: Visual object tracking and segmentation in omnidirectional videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient image fusion network exploiting unifying language and mask guidance. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3591930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims to merge image pairs collected by different sensors over the same scene, preserving their distinct features. Recent works have often focused on designing various image fusion losses, developing different network architectures, and leveraging downstream tasks (e.g., object detection) for image fusion. However, a few studies have explored how language and semantic masks can serve as guidance to aid image fusion. In this paper, we investigate how the combination of language and masks can guide image fusion tasks, discarding the previously complex frameworks, which rely on downstream tasks, GAN-based cycle training, diffusion models, or deep image priors. Additionally, we exploit a recurrent neural network-like architecture to build a lightweight network that avoids the quadratic-cost of traditional attention mechanisms. To adapt the receptance weighted key value (RWKV) model to an image modality, we modify it into a bidirectional version using an efficient scanning strategy (ESS). To guide image fusion by language and mask features, we introduce a multi-modal fusion module (MFM) to facilitate information exchange. Comprehensive experiments show that the proposed framework achieved state-of-the-art results in various image fusion tasks (i.e., visible-infrared image fusion, multi-focus image fusion, multi-exposure image fusion, medical image fusion, hyperspectral and multispectral image fusion, and pansharpening). Code will be available at https://github.com/294coder/RWKVFusion.},
  archive      = {J_TPAMI},
  author       = {Zi-Han Cao and Yu-Jie Liang and Liang-Jian Deng and Gemine Vivone},
  doi          = {10.1109/TPAMI.2025.3591930},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An efficient image fusion network exploiting unifying language and mask guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M3DM-NR: RGB-3D noisy-resistant industrial anomaly detection via multimodal denoising. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3592089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images. Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios. To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP. M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference. Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations. Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection.},
  archive      = {J_TPAMI},
  author       = {Chengjie Wang and Haokun Zhu and Jinlong Peng and Yue Wang and Ran Yi and Yunsheng Wu and Lizhuang Ma and Jiangning Zhang},
  doi          = {10.1109/TPAMI.2025.3592089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {M3DM-NR: RGB-3D noisy-resistant industrial anomaly detection via multimodal denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards reliable and faithful explanations: A disentanglement-augmented approach for selective rationalization. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3592313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of model explainability has prompted the selective rationalization (aka, rationale extraction) which can identify important features (i.e., rationales) from the original input to support prediction results. Existing methods typically involve a cascaded approach with a selector responsible for extracting rationales from the input, followed by a predictor that makes predictions based on the selected rationales. However, these approaches often neglect the information contained in the non-rationales, underutilizing the input. Therefore, in our prior work, we introduce the Disentanglement-Augmented Rationale Extraction (DARE) method, which disentangles the input into rationale and non-rationale components, and enhances rationale representations by minimizing the mutual information between them. While DARE demonstrates strong performance in rationalization, it may still rely on shortcuts in the training distribution, leading to unfaithful rationales. To this end, in this paper, we propose Faith-DARE, an extension of DARE that aims to extract more reliable rationales by mitigating shortcut dependencies. Specifically, we treat the non-rationale features identified by DARE as environments that are decorrelated from the predictions. By shuffling and recombining these environments with rationales, we generate counterfactual samples and identify invariant rationales that remain predictive across shifted distributions. Extensive experiments on graph and textual datasets validate the effectiveness of Faith-DARE. Codes are available at https://github.com/yuelinan/DARE.},
  archive      = {J_TPAMI},
  author       = {Linan Yue and Qi Liu and YiChao Du and Li Wang and Yanqing An and Enhong Chen},
  doi          = {10.1109/TPAMI.2025.3592313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards reliable and faithful explanations: A disentanglement-augmented approach for selective rationalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial frequency modulation for semantic segmentation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3592621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. As a result, the proposed approach considerably enhances existing state-of-the-art segmentation models (e.g., Mask2Former-Swin-T +1.5 mIoU, InternImage-T +1.4 mIoU on ADE20K). Furthermore, ARS also boosts the performance of powerful Deformable Convolution (+0.8 mIoU on Cityscapes) by maintaining relative positional order during non-uniform sampling. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code will be released upon publication.},
  archive      = {J_TPAMI},
  author       = {Linwei Chen and Ying Fu and Lin Gu and Dezhi Zheng and Jifeng Dai},
  doi          = {10.1109/TPAMI.2025.3592621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spatial frequency modulation for semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GradBias: Unveiling word influence on bias in text-to-image generative models. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3592901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in Text-to-Image (T2I) generative models has enabled high-quality image generation. As performance and accessibility increase, these models are gaining significant attraction and popularity: ensuring their fairness and safety is a priority to prevent the dissemination and perpetuation of biases. However, existing studies in bias detection focus on closed sets of predefined biases (e.g., gender, ethnicity). In this paper, we propose a general framework to identify, quantify, and explain biases in an open set setting, i.e.,without requiring a predefined set. This pipeline leverages a Large Language Model (LLM) to propose biases starting from a set of captions. Next, these captions are used by the target generative model for generating a set of images. Finally, Vision Question Answering (VQA) is leveraged for bias evaluation. We show two variations of this framework: OpenBias and GradBias. OpenBias detects and quantifies biases, while GradBias determines the contribution of individual prompt words on biases. OpenBias effectively detects both well-known and novel biases related to people, objects, and animals and highly aligns with existing closed-set bias detection methods and human judgment. GradBias shows that neutral words can significantly influence biases and it outperforms several baselines, including state-of-the-art foundation models. Code available here: https://github.com/Moreno98/GradBias.},
  archive      = {J_TPAMI},
  author       = {Moreno D'Inca and Elia Peruzzo and Massimiliano Mancini and Xingqian Xu and Humphrey Shi and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3592901},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GradBias: Unveiling word influence on bias in text-to-image generative models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rotation- and permutation-equivariant quantum graph neural network for 3D graph data. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3593371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equivariant quantum graph neural networks (EQGNNs) offer a potentially powerful method to process graph data. However, existing EQGNN models only consider the permutation symmetry of graphs, and failing to fully exploit the geometric and non-geometric information in graphs, resulting in suboptimal performance when processing 3D graph data. To address these limitations, we derive constraints of rotation and permutation equivariance, and then propose a novel rotation- and permutation-equivariant quantum graph neural network (RP-EQGNN). An equivariant module is designed to extract the geometric information. Then, a convolution and entanglement module is constructed to extract non-geometric information. To improve performance of our model, an edge entanglement strategy is designed to perform distinguishable entanglement operations based on edge heterogeneity. The experiment results demonstrate that RP-EQGNN is significantly better for graph regression on the QM9 dataset and the OC20 dataset than Q3DGL and EQC in MAE and achieves results comparable to those of EquiformerV2 , Geoformer , SO3KRATES and HEGNN. It also has advantage for point cloud classification on the ModelNet40 dataset over quantum models, including sQCNN-3D and PI-QSVM. RP-EQGNN introduces an innovative approach for processing 3D graph data, establishing a basis for future investigations into symmetries within graph neural networks.},
  archive      = {J_TPAMI},
  author       = {Wenjie Liu and Yifan Zhu and Ying Zha and Qingshan Wu and Lei Jian and Zhihao Liu},
  doi          = {10.1109/TPAMI.2025.3593371},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rotation- and permutation-equivariant quantum graph neural network for 3D graph data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-inverted image pyramid networks for visual perception and multimodal understanding. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3593283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20 K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8 M training data. Our code is released at https://github.com/OpenGVLab/PIIP.},
  archive      = {J_TPAMI},
  author       = {Zhaokai Wang and Xizhou Zhu and Xue Yang and Gen Luo and Hao Li and Changyao Tian and Wenhan Dou and Junqi Ge and Lewei Lu and Yu Qiao and Jifeng Dai},
  doi          = {10.1109/TPAMI.2025.3593283},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parameter-inverted image pyramid networks for visual perception and multimodal understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HPformer: Low-parameter transformer with temporal dependency hierarchical propagation for health informatics. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3593657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers based on Self-Attention (SA) mechanism have demonstrated unrivaled superiority in numerous areas. Compared to RNN-based networks, Transformers can learn the temporal dependency representation of an entire sequence in parallel, while efficiently dealing with long-range dependencies. However, the $\mathcal {O}(L^{2})$ ($L$ denotes the length of the sequence) computational complexity of the SA mechanism and the high memory usage make the construction cost of the Transformer-based model prohibitively expensive. To address these challenges, we propose a Transformer-like model, HPformer: Low-Parameter Transformer with Temporal Dependency Hierarchical Propagation. HPformer first chunks the sequence into $K$ ($K = \left\lceil \log {L} \right\rceil + 1$, $\left\lceil \cdot \right\rceil$ denotes ceiling operation) sequence segments, then leverages the hierarchical propagation mechanism with $\mathcal {O}(L)$ computational complexity to learn the temporal dependencies between the segments and within the segments, and ultimately generates $K$ vectors as $Key$ matrices. This reduces the complexity of the SA mechanism from $\mathcal {O}(L^{2})$ to $\mathcal {O}(L\log {L})$. In addition, we employ a strategy of sharing $Key$ and $Value$ matrices between layers to build the HPformer, thus reducing memory usage. Extensive experiments based on public health informatics benchmark and Long-Range Arena (LRA) benchmark have demonstrated that HPformer has advantages over Transformer-based models in terms of memory usage and efficiency.},
  archive      = {J_TPAMI},
  author       = {Wu Lee and Yuliang Shi and Han Yu and Lin Cheng and Xinjun Wang and Zhongmin Yan and Fanyu Kong},
  doi          = {10.1109/TPAMI.2025.3593657},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HPformer: Low-parameter transformer with temporal dependency hierarchical propagation for health informatics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISEARLE: Improving textual inversion for zero-shot composed image retrieval. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3593539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a query consisting of a reference image and a relative caption, Composed Image Retrieval (CIR) aims to retrieve target images visually similar to the reference one while incorporating the changes specified in the relative caption. The reliance of supervised methods on labor-intensive manually labeled datasets hinders their broad applicability to CIR. In this work, we introduce a new task, Zero-Shot CIR (ZS-CIR), that addresses CIR without the need for a labeled training dataset. We propose an approach, named iSEARLE (improved zero-Shot composEd imAge Retrieval with textuaL invErsion), that involves mapping the visual information of the reference image into a pseudo-word token in the CLIP token embedding space and combining it with the relative caption. To foster research on ZS-CIR, we present an open-domain benchmarking dataset named CIRCO (Composed Image Retrieval on Common Objects in context), the first CIR dataset where each query is labeled with multiple ground truths and a semantic categorization. The experimental results illustrate that iSEARLE obtains state-of-the-art performance on three different CIR datasets – FashionIQ, CIRR, and the proposed CIRCO – and two additional evaluation settings, namely domain conversion and object composition. The dataset, code, and model are publicly available at https://github.com/miccunifi/SEARLE.},
  archive      = {J_TPAMI},
  author       = {Lorenzo Agnolucci and Alberto Baldrati and Alberto Del Bimbo and Marco Bertini},
  doi          = {10.1109/TPAMI.2025.3593539},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ISEARLE: Improving textual inversion for zero-shot composed image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative novel object discovery and box-guided cross-modal alignment for open-vocabulary 3D object detection. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3593580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary 3D Object Detection (OV-3DDet) addresses the detection of objects from an arbitrary list of novel categories in 3D scenes, which remains a very challenging problem. In this work, we propose CoDAv2, a unified framework designed to innovatively tackle both the localization and classification of novel 3D objects, under the condition of limited base categories. For localization, the proposed 3D Novel Object Discovery (3D-NOD) strategy utilizes 3D geometries and 2D open-vocabulary semantic priors to discover pseudo labels for novel objects during training. 3D-NOD is further extended with an Enrichment strategy that significantly enriches the novel object distribution in the training scenes, and then enhances the model's ability to localize more novel objects. The 3D-NOD with Enrichment is termed 3D-NODE. For classification, the Discovery-driven Cross-modal Alignment (DCMA) module aligns features from 3D point clouds and 2D/textual modalities, employing both class-agnostic and class-specific alignments that are iteratively refined to handle the expanding vocabulary of objects. Besides, 2D box guidance boosts the classification accuracy against complex background noises, which is coined as Box-DCMA. Extensive evaluation demonstrates the superiority of CoDAv2. CoDAv2 outperforms the best-performing method by a large margin ($\rm {AP}_{Novel}$ of 9.17vs. 3.61 on SUN-RGBD and 9.12vs. 3.74 on ScanNetv2). Source code and pre-trained models are available at the GitHub project page.},
  archive      = {J_TPAMI},
  author       = {Yang Cao and Yihan Zeng and Hang Xu and Dan Xu},
  doi          = {10.1109/TPAMI.2025.3593580},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Collaborative novel object discovery and box-guided cross-modal alignment for open-vocabulary 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTMamba++: Enhancing multi-task dense scene understanding via mamba-based decoders. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3593621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task dense scene understanding, which trains a model for multiple dense prediction tasks, has a wide range of application scenarios. Capturing long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba++, a novel architecture for multi-task scene understanding featuring with a Mamba-based decoder. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging state-space models, while CTM explicitly models task interactions to facilitate information exchange across tasks. We design two types of CTM block, namely F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic perspectives, respectively. Extensive experiments on NYUDv2, PASCAL-Context, and Cityscapes datasets demonstrate the superior performance of MTMamba++ over CNN-based, Transformer-based, and diffusion-based methods while maintaining high computational efficiency.},
  archive      = {J_TPAMI},
  author       = {Baijiong Lin and Weisen Jiang and Pengguang Chen and Shu Liu and Ying-Cong Chen},
  doi          = {10.1109/TPAMI.2025.3593621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MTMamba++: Enhancing multi-task dense scene understanding via mamba-based decoders},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards universal modal tracking with online dense temporal token learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3593543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a universal video-level modality-awareness tracking model with online dense temporal token learning (called UM-ODTrack). It is designed to support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and RGB+Event, utilizing the same model architecture and parameters. Specifically, our model is designed with three core goals: Video-level Sampling. We expand the model's inputs to a video sequence level, aiming to see a richer video context from an near-global perspective. Video-level Association. Furthermore, we introduce two simple yet effective online dense temporal token association mechanisms to propagate the appearance and motion trajectory information of target via a video stream manner. Modality Scalable. We propose two novel gated perceivers that adaptively learn cross-modal representations via a gated attention mechanism, and subsequently compress them into the same set of model parameters via a one-shot training manner for multi-task inference. This new solution brings the following benefits: (i) The purified token sequences can serve as temporal prompts for the inference in the next video frames, whereby previous information is leveraged to guide future inference. (ii) Unlike multi-modal trackers that require independent training, our one-shot training scheme not only alleviates the training burden, but also improves model representation. Extensive experiments on visible and multi-modal benchmarks show that our UM-ODTrack achieves a new SOTA performance.},
  archive      = {J_TPAMI},
  author       = {Yaozong Zheng and Bineng Zhong and Qihua Liang and Shengping Zhang and Guorong Li and Xianxian Li and Rongrong Ji},
  doi          = {10.1109/TPAMI.2025.3593543},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards universal modal tracking with online dense temporal token learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-task relation-aware consistency for weakly supervised temporal action detection. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3594178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection aims to predict temporal boundaries and category labels of actions in untrimmed videos. In the past years, many weakly supervised temporal action detection methods have been proposed to relieve the annotation cost of fully supervised methods. Due to the discrepancy between action localization and action classification, the two-branch structure is widely adopted by existing weakly supervised methods, where the classification branch is used to predict category-wise score and the localization branch is used to predict foreground score for each segment. Under the weakly supervised setting, the model training is mainly guided by the video-level or sparse segment-level annotations. As a result, the classification branch tends to focus on the most discriminative segments while ignore less discriminative ones so as to minimize the classification cost, and the localization branch may assign high foreground scores for some negative segments. This phenomenon can severely damage the action detection performance, because the foreground scores and classification scores are combined together in the testing stage for action detection. To deal with this problem, several methods have been proposed to encourage the consistency between the classification branch and localization branch. However, these methods only consider the video-level or segment-level consistency, without considering the relation among different segments to be consistent. In this paper, we propose a Cross-Task Relation-Aware Consistency (CRC) strategy for weakly supervised temporal action detection, including an intra-video consistency module and an inter-video consistency module. The intra-video consistency module can well guarantee the relationship among segments from the same video to be consistent, and the inter-video consistency module guarantees the relationship among segments from different videos to be consistent. These two modules are complementary to each other by combining both intra-video and inter-video consistency. Experimental results show that the proposed CRC strategy can consistently improve the performance of existing weakly supervised methods, including click-level supervised methods (e.g., LACP [1]), video-level supervised methods (e.g., DELU [2]) and unsupervised methods (e.g., BaS-Net [3]), verifying the generality and effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Wenfei Yang and Huan Ren and Tianzhu Zhang and Zhe Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2025.3594178},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-task relation-aware consistency for weakly supervised temporal action detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified random walk, its induced laplacians and spectral convolutions for deep hypergraph learning. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3593880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph-based modeling has gained significant attention for capturing complex higher-order interactions among vertices. While random walks serve as fundamental tools for analyzing hypergraphs, existing approaches either fail to fully leverage edge-dependent vertex weights (EDVWs) or lack sufficient expressiveness to model intricate hypergraph structures. To address these limitations, we propose a unified random walk framework that integrates hyperedge degrees and vertex weights, offering a more robust approach to hypergraph modeling. We establish equivalence conditions between hypergraph and graph random walks, leading to a novel unified random-walk-based hypergraph Laplacian that incorporates EDVWs, ensuring expressiveness and desirable spectral properties. Building on this foundation, we introduce the General Hypergraph Spectral Convolution (GHSC) framework, which extends existing Graph Convolutional Neural Networks (GCNNs) for effective hypergraph learning, supporting both edge-independent and edge-dependent vertex weights. Extensive experiments across diverse datasets, including citation networks, visual objects, and protein modeling tasks, demonstrate state-of-the-art performance, with notable improvements in protein structure modeling using EDVW-hypergraphs. This work advances the theoretical understanding of hypergraph random walks and spectral theory while providing a versatile framework for deep hypergraph learning. Code is available at https://github.com/youjibiying/GHSC_H-GNNs.},
  archive      = {J_TPAMI},
  author       = {Jiying Zhang and Fuyang Li and Xi Xiao and Guanzi Chen and Tingyang Xu and Yu Rong and Junzhou Huang and Yatao Bian},
  doi          = {10.1109/TPAMI.2025.3593880},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified random walk, its induced laplacians and spectral convolutions for deep hypergraph learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph spiking attention network: Sparsity, efficiency and robustness. <em>TPAMI</em>, 1-8. (<a href='https://doi.org/10.1109/TPAMI.2025.3593912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Graph Attention Networks (GATs) generally adopt the self-attention mechanism to learn graph edge attention, which usually return dense attention coefficients over all neighbors and thus are prone to be sensitive to graph edge noises. To overcome this problem, sparse GATs are desirable and have garnered increasing interest in recent years. However, existing sparse GATs usually suffer from high training complexity and are also not straightforward for inductive learning tasks. To address these issues, we propose to learn sparse GATs by exploiting spiking neuron (SN) mechanism, termed Graph Spiking Attention (GSAT). Specifically, it is known that spiking neuron can perform inexpensive information processing by transmitting the input data into discrete spike trains and return sparse outputs. Inspired by it, this work attempts to exploit spiking neuron to learn sparse attention coefficients, resulting in edge-sparsified graph for GNNs. Therefore, GSAT can perform message passing on the selective neighbors naturally, which makes GSAT perform compactly and robustly w.r.t graph noises. Moreover, GSAT can be used straightforwardly for inductive learning tasks. Extensive experiments on both transductive and inductive tasks demonstrate the effectiveness, robustness and efficiency of GSAT.},
  archive      = {J_TPAMI},
  author       = {Beibei Wang and Bo Jiang and Jin Tang and Lu Bai and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3593912},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph spiking attention network: Sparsity, efficiency and robustness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA3Det++: Side-aware quality estimation for semi-supervised 3D object detection. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3594086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised 3D object detection from point cloud aims to train a detector with a small number of labeled data and a large number of unlabeled data. Among existing methods, the pseudo-label based methods have achieved superior performance, and the core lies in how to select high-quality pseudo-labels with the designed quality evaluation criterion. Despite the success of these methods, they all consider the localization and classification quality estimation from a global perspective. For localization quality, they use a global score threshold to filter out low-quality pseudo-labels and assign equal importance to each side during training, ignoring the fact that sides with different localization quality should not be treat equally. Besides, a large number of pseudo-labels are discarded due to the high global threshold, which may also contain some correctly predicted sides that are helpful for model training. For the classification quality, they usually combine the objectness score and classification confidence score to filter out pseudo-labels. The main focus of them is designing effective classification confidence evaluation metrics, neglecting the importance of predicting better objectness score. In this paper, we propose SA3Det++, a side-aware quality estimation method for semi-supervised object detection, which consists of a probabilistic side localization strategy, a side-aware quality estimation strategy, and a soft pseudo-label selection strategy. Extensive results demonstrate that the proposed method consistently outperforms the baseline methods under different scenes and evaluation criterions. Code is avaliable at: https://github.com/OpenSpaceAI/Nesie.},
  archive      = {J_TPAMI},
  author       = {Wenfei Yang and Chuxin Wang and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2025.3594086},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SA3Det++: Side-aware quality estimation for semi-supervised 3D object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFC-RNN: Adaptive forgetting-controlled recurrent neural network for pedestrian trajectory prediction. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3594116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction plays a crucial and fundamental role in many computer vision tasks. Most existing works utilize recurrent neural networks to extract temporal features from trajectories because their recursive structure is inherently well-suited for time series data. However, previous methods overlook the forgetting characteristics of pedestrians when modeling historical trajectories, which may cause the model to focus on the wrong positions of historical information. In this paper, we propose a simple yet effective Adaptive Forgetting-Controlled Recurrent Neural Network (AFC-RNN) for pedestrian trajectory prediction. The core idea of AFC-RNN is a novel Adaptive Forgetting Controller (AFC), which controls the forgetting degree of the historical information at each time step explicitly and adaptively. Specifically, AFC first learns memory factors for each time step based on the temporal correlation of observed trajectories using the self-attention mechanism. Then, AFC-RNN applies these memory factors to regulate the forgetting degree of observed features at each time step from RNN. Extensive experiments and ablation studies on ETH, UCY, SDD, and NBA datasets demonstrate that our method outperforms existing state-of-the-art approaches. Additionally, we provide a mathematical analysis to demonstrate the superiority of our adaptive forgetting strategy in the AFC-RNN over traditional RNNs for trajectory forgetting modeling.},
  archive      = {J_TPAMI},
  author       = {Yonghao Dong and Le Wang and Sanping Zhou and Wei Tang and Gang Hua and Changyin Sun},
  doi          = {10.1109/TPAMI.2025.3594116},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AFC-RNN: Adaptive forgetting-controlled recurrent neural network for pedestrian trajectory prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust density peaks clustering for manifold data with multiple peaks. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is an excellent clustering algorithm that does not need any prior knowledge. However, DPC still has the following shortcomings: (1) The Euclidean distance used by it is not applicable to manifold data with multiple peaks. (2) The local density calculation for DPC is too simple, and the final results may fluctuate due to the cutoff-distance dc. (3) Manually selected centers by decision-graph may lead to a wrong number of clusters and poor performance. To address these shortcomings and improve the performance, a robust density peaks clustering algorithm for manifold data with multiple peaks (RDPCM) is proposed to reduce the sensitivity of clustering results to parameters. Motivated by DPC-GD, RDPCM replaces the Euclidean distance with geodesic distance, which is optimized by the improved mutual K-nearest neighbors. It better considers the local manifold structure of the datasets and obtains excellent results. In addition, the Davies-Bouldin Index based on Minimum Spanning Tree (MDBI) is proposed to select the ideal number of classes adaptively. Numerous experiments have established that RDPCM is more effective and superior than other advanced clustering algorithms.},
  archive      = {J_TPAMI},
  author       = {Ling Ding and Chao Li and Shifei Ding and Xiao Xu and Lili Guo and Xindong Wu},
  doi          = {10.1109/TPAMI.2025.3594121},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust density peaks clustering for manifold data with multiple peaks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay master: Automatic sample selection and effective memory utilization for continual semantic segmentation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3594040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Semantic Segmentation (CSS) extends static semantic segmentation by incrementally introducing new classes for training. To alleviate the catastrophic forgetting issue in this task, replay methods can be adopted, constructing a memory buffer that stores a small number of samples from previous classes for future replay. However, existing replay approaches in CSS often lack a thorough exploration of two critical issues: how to find the most suitable memory samples and how to utilize them for replay more effectively. Common strategies either randomly select samples or rely on hand-crafted, single-factor-driven methods that are hard to be optimal, and often employ conventional training techniques for replay that do not account for class imbalance problem resulting from limited memory capacity. In this work, we tackle these challenges by introducing a novel memory sample selection method that leverages a reinforcement learning framework with innovative state representations and a dual-stage action scheme to automatically learn a selection policy. Additionally, we propose an expert mechanism and a dual-phase training method to address the class imbalance issue, thereby enhancing the effectiveness of replay training by making better use of memory samples. Incorporating the proposed automatic sample selection and effective memory utilization methods, we develop a novel and effective replay-based pipeline for CSS. Our extensive experiments on Pascal VOC 2012 and ADE20K datasets demonstrate the effectiveness of our approach, which achieves state-of-the-art (SOTA) performance and outperforms previous advanced methods significantly.},
  archive      = {J_TPAMI},
  author       = {Lanyun Zhu and Tianrun Chen and Jianxiong Yin and Simon See and De Wen Soh and Jun Liu},
  doi          = {10.1109/TPAMI.2025.3594040},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Replay master: Automatic sample selection and effective memory utilization for continual semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection, ensemble, and adaptation: Advancing multi-source-free domain adaptation via architecture zoo. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3593943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional Multi-Source Free Domain Adaptation (MSFDA) assumes that each source domain provides a single source model, and all source models adopt a uniform architecture. This paper introduces Zoo-MSFDA, a more general setting that allows each source domain to offer a zoo of multiple source models with different architectures. While it enriches the source knowledge, Zoo-MSFDA risks being dominated by suboptimal/harmful models. To address this issue, we theoretically analyze the model selection problem in Zoo-MSFDA, and introduce two principles: transferability principle and diversity principle. Recognizing the challenge of measuring transferability, we subsequently propose a novel Source-Free Unsupervised Transferability Estimation (SUTE). It enables assessing and comparing transferability across multiple source models with different architectures under domain shift, without requiring target labels and source data. Based on above, we introduce a Selection, Ensemble, and Adaptation (SEA) framework to address Zoo-MSFDA, which consists of: 1) source models selection based on the proposed principles and SUTE; 2) ensemble construction based on SUTE-estimated transferability; 3) target-domain adaptation of the ensemble model. Evaluations demonstrate that our SEA framework, with the introduced Zoo-MSFDA setting, significantly improves adaptation performance in 2D image classification tasks. Additionally, our SUTE achieves state-of-the-art performance in transferability estimation. Codes are available at https://github.com/SPIresearch/Zoo-MSFDA.},
  archive      = {J_TPAMI},
  author       = {Jiangbo Pei and Ruizhe Li and Aidong Men and Yang Liu and Xiahai Zhuang and Qingchao Chen},
  doi          = {10.1109/TPAMI.2025.3593943},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Selection, ensemble, and adaptation: Advancing multi-source-free domain adaptation via architecture zoo},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEMI-CAVA: A causal variational approach to semi-supervised learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has advanced rapidly, but relies heavily on large-labeled datasets for effective training. This is particularly challenging in fields like medicine, where expert labeling is costly, labor-intensive, and prone to bias and error. Semi-supervised learning (SSL) addresses this challenge by reducing reliance on labeled data. SSL is closely tied to the concept of causation. However, recent works relating causality to SSL are limited by modeling only low-dimensional observations or designing a plug-in module to alleviate the class imbalance. In this paper, we take steps towards training causal generative models for semi-supervised learning, combining principles from causality and variational inference. We interpret the Mixup strategy as a stochastic intervention and introduce a consistency loss to promote coherent latent representations. Under reasonable assumptions, we provide theoretical guarantees that the learned latent representations align with true causal factors up to permissible ambiguities. The experimental results show the proposed approach achieves state-of-the-art performance on several medical datasets of different modalities. Additionally, we test our model on standard benchmarking datasets: CIFAR10, CIFAR100, and SVHN, where it achieves competitive performance.},
  archive      = {J_TPAMI},
  author       = {Saptarshi Saha and Pratyush Kumar Sahoo and Utpal Garain},
  doi          = {10.1109/TPAMI.2025.3594360},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SEMI-CAVA: A causal variational approach to semi-supervised learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic directed distance fields for ray-based shape representations. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3594225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern computer vision, the optimal representation of 3D shape remains task-dependent. One fundamental operation applied to such representations is differentiable rendering, which enables learning-based inverse graphics approaches. Standard explicit representations are often easily rendered, but can suffer from limited geometric fidelity, among other issues. On the other hand, implicit representations generally preserve greater fidelity, but suffer from difficulties with rendering, limiting scalability. In this work, we devise Directed Distance Fields (DDFs), which map a ray or oriented point (position and direction) to surface visibility and depth. This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as higher-order geometry with only additional backward passes. Using probabilistic DDFs (PDDFs), we can model the inherent discontinuities in the underlying field. We then apply DDFs to single-shape fitting, generative modelling, and 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation. Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency. We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing which shape the field is expressing.},
  archive      = {J_TPAMI},
  author       = {Tristan Aumentado-Armstrong and Stavros Tsogkas and Sven Dickinson and Allan Jepson},
  doi          = {10.1109/TPAMI.2025.3594225},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Probabilistic directed distance fields for ray-based shape representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPL++: Advancing the network performance via image and label perturbations. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3594149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in supervised learning have predominantly focused on regularizations, optimizers, and architectures, yet the potential of simultaneously optimizing data distributions and supervisory signals for training samples remains underexplored. In this paper, we propose a novel paradigm that leverages the benefits of image perturbations for rectifying data distributions. Our method, called DPL (Deep Perturbation Learning), introduces new insights into utilizing image perturbations and focuses on improving generalizability on normal samples, rather than resisting adversarial attacks. DPL formulates a differentiable function w.r.t. image perturbations and implements an alternative optimization process that seamlessly integrates with downstream tasks. However, the limitations of DPL stem from the inefficiency in employing differentiable targets caused by the exclusive optimization of image perturbations, while neglecting the critical role of supervisory signals in training effectiveness. These lead to the excessive necessity of DPL iterations and yield inferior performance-cost trade-off. To track this, we extend DPL to DPL++ with synchronous optimization for image perturbations and label perturbations. In our DPL++ paradigm, the post-hoc application of perturbations to images and labels endows amendments toward both data distributions and supervisory signals, significantly furthering the generalizability of models over various benchmarks. Crucially, the proposed synchronous optimization process shares key differentiable objectives to reduce computational complexity, thereby achieving enhanced effectiveness within fewer optimization iterations. Theoretically, as a generic and flexible approach, DPL++ can be applied to a variety of backbone architectures (e.g., ResNet, DenseNet, and ViT) and downstream tasks (e.g., image classification and object detection). To validate the efficacy of DPL++, we conduct extensive performance experiments and in-depth analytical studies on 2 visual tasks over 5 mainstream benchmarks across 13 backbone networks. The comprehensive results verify the superiority of DPL++ over DPL and demonstrate its promising capabilities for advancing decision-making capacity, risk minimization, class distinguishability, and training convergence.},
  archive      = {J_TPAMI},
  author       = {Zifan Song and Xiao Gong and Guosheng Hu and Shuguang Dou and Qingsong Zhao and Cairong Zhao},
  doi          = {10.1109/TPAMI.2025.3594149},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DPL++: Advancing the network performance via image and label perturbations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational and statistical guarantees for tensor-on-tensor regression with tensor train decomposition. <em>TPAMI</em>, 1-11. (<a href='https://doi.org/10.1109/TPAMI.2025.3593840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a tensor-on-tensor (ToT) regression model has been proposed to generalize tensor recovery, encompassing scenarios like scalar-on-tensor regression and tensor-on-vector regression. However, the exponential growth in tensor complexity poses challenges for storage and computation in ToT regression. To overcome this hurdle, tensor decompositions have been introduced, with the tensor train (TT)-based ToT model proving efficient in practice due to reduced memory requirements, enhanced computational efficiency, and decreased sampling complexity. Despite these practical benefits, a disparity exists between theoretical analysis and real-world performance. In this paper, we delve into the theoretical and algorithmic aspects of the TT-based ToT regression model. Assuming the regression operator satisfies the restricted isometry property (RIP), we conduct an error analysis for the solution to a constrained least-squares optimization problem. This analysis includes upper error bound and minimax lower bound, revealing that such error bounds polynomially depend on the order $N+M$. To efficiently find solutions meeting such error bounds, we propose two optimization algorithms: the iterative hard thresholding (IHT) algorithm (employing gradient descent with TT-singular value decomposition (TT-SVD)) and the factorization approach using the Riemannian gradient descent (RGD) algorithm. When RIP is satisfied, spectral initialization facilitates proper initialization, and we establish the linear convergence rate of both IHT and RGD. Notably, compared to the IHT, which optimizes the entire tensor in each iteration while maintaining the TT structure through TT-SVD and poses a challenge for storage memory in practice, the RGD optimizes factors in the so-called left-orthogonal TT format, enforcing orthonormality among most of the factors, over the Stiefel manifold, thereby reducing the storage complexity of the IHT. However, this reduction in storage memory comes at a cost: the recovery of RGD is worse than that of IHT, while the error bounds of both algorithms depend on $N+M$ polynomially. Experimental validation substantiates the validity of our theoretical findings.},
  archive      = {J_TPAMI},
  author       = {Zhen Qin and Zhihui Zhu},
  doi          = {10.1109/TPAMI.2025.3593840},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computational and statistical guarantees for tensor-on-tensor regression with tensor train decomposition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAC++: Towards 100X compression of 3D gaussian splatting. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3594066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) has emerged as a promising representation for novel view synthesis, boosting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. In this paper, we propose HAC++, which explicitly minimizes the representation's entropy during optimization, enabling efficient arithmetic coding after training for compressed storage. Specifically, to reduce entropy, HAC++ leverages the relationships between unorganized anchors and a structured hash grid, utilizing their mutual information for context modeling. Additionally, HAC++ captures intra-anchor contextual relationships to further enhance compression performance. To facilitate entropy coding, we utilize Gaussian distributions to precisely estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Moreover, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Overall, HAC++ achieves a remarkable size reduction of over $100\times$ compared to vanilla 3DGS when averaged on all datasets, while simultaneously improving fidelity. It also delivers more than $20\times$ size reduction compared to Scaffold-GS. Our code is available at https://github.com/YihangChen-ee/HAC-plus.},
  archive      = {J_TPAMI},
  author       = {Yihang Chen and Qianyi Wu and Weiyao Lin and Mehrtash Harandi and Jianfei Cai},
  doi          = {10.1109/TPAMI.2025.3594066},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HAC++: Towards 100X compression of 3D gaussian splatting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Out-of-distribution generalization on graphs: A survey. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3593897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem definition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions.},
  archive      = {J_TPAMI},
  author       = {Haoyang Li and Xin Wang and Ziwei Zhang and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2025.3593897},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Out-of-distribution generalization on graphs: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised risk control via prediction-powered inference. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3594263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The risk-controlling prediction sets (RCPS) framework is a general tool for transforming the output of any machine learning model to design a predictive rule with rigorous error rate control. The key idea behind this framework is to use labeled hold-out calibration data to tune a hyper-parameter that affects the error rate of the resulting prediction rule. However, the limitation of such a calibration scheme is that with limited hold-out data, the tuned hyper-parameter becomes noisy and leads to a prediction rule with an error rate that is often unnecessarily conservative. To overcome this sample-size barrier, we introduce a semi-supervised calibration procedure that leverages unlabeled data to rigorously tune the hyper-parameter without compromising statistical validity. Our procedure builds upon the prediction-powered inference framework, carefully tailoring it to risk-controlling tasks. We demonstrate the benefits and validity of our proposal through two real-data experiments: few-shot image classification and early time series classification.},
  archive      = {J_TPAMI},
  author       = {Bat-Sheva Einbinder and Liran Ringel and Yaniv Romano},
  doi          = {10.1109/TPAMI.2025.3594263},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised risk control via prediction-powered inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GetMesh: A controllable model for high-quality mesh generation and manipulation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3594478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meshes are essential for representing 3D assets across many industrial applications and serve as the standard input for graphics rendering pipelines. However, their irregular structure makes their creation and manipulation both time-consuming and labor-intensive. In this paper, we introduce GetMesh, a new generative model designed for mesh generation and manipulation across various categories. Utilizing a dynamic number of points as a latent representation, which is then organized into a triplane representation, GetMesh generates meshes with geometric details, sharp features as well as textures. This model significantly outperforms existing methods for both single-category and multi-category generation. Moreover, GetMesh offers unparalleled fine-grained control over the generation process, enabling intuitive, efficient, and robust modifications. These include changing global or local mesh topologies, adding or removing mesh parts, and combining mesh parts across categories through adjustments in the number, positions or features of the latent points.},
  archive      = {J_TPAMI},
  author       = {Ben Fei and Jinyi Wang and Lei Bai and Keyi Liu and Xudong Xu and Weidong Yang and Ya Zhang and Ying He and Dahua Lin and Zhaoyang Lyu and Bo Dai},
  doi          = {10.1109/TPAMI.2025.3594478},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GetMesh: A controllable model for high-quality mesh generation and manipulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human motion video generation: A survey. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3594034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository.},
  archive      = {J_TPAMI},
  author       = {Haiwei Xue and Xiangyang Luo and Zhanghao Hu and Xin Zhang and Xunzhi Xiang and Yuqin Dai and Jianzhuang Liu and Zhensong Zhang and Minglei Li and Jian Yang and Fei Ma and Zhiyong Wu and Changpeng Yang and Zonghong Dai and Fei Richard Yu},
  doi          = {10.1109/TPAMI.2025.3594034},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human motion video generation: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint-aware zero-shot vision-language navigation in continuous environments. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3594204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM's constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12% and 13% in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions.},
  archive      = {J_TPAMI},
  author       = {Kehan Chen and Dong An and Yan Huang and Rongtao Xu and Yifei Su and Yonggen Ling and Ian Reid and Liang Wang},
  doi          = {10.1109/TPAMI.2025.3594204},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Constraint-aware zero-shot vision-language navigation in continuous environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenPoly: Learning generalized and tessellated shape priors via 3D polymorphic evolving. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3593807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce GenPoly, a novel generalized 3D prior model designed for multiple 3D generation tasks, focusing on preserving fine details. While previous works learn generalizable representations by decomposing objects into coarse-grained components to reassemble a coherent global structure, this approach sacrifices small-scale details. In this paper, we take a different perspective, formulating 3D prior modeling as a bottom-up polymorphic evolving process. Our key insight is that, beyond global structures, intricate local geometry variations hold rich contextual information that should be incorporated into the modeling process to learn fine-grained, generalizable representations. This allows coarse shapes to progressively evolve through multi-granular local geometry refinements, enabling high-fidelity 3D generation. To this end, we first introduce a polymorphic variational autoencoder (PolyVAE), which constructs a versatile shape residual codebook via a polymorphic quantization mechanism. This codebook strategically encodes intricate local geometry representations from tesselated shapes within the latent space. Building on these representations, a 3D polymorphic evolving scheme is further developed to refine local details in a coarse-to-fine manner progressively. In this way, visually compelling 3D shapes with rich and complex details can be ultimately generated. The effectiveness of our method is demonstrated through extensive qualitative and quantitative evaluations, where GenPoly consistently surpasses state-of-the-art methods across various downstream tasks, particularly in local detail preservation. The code and more visualizations will be available on our project website.},
  archive      = {J_TPAMI},
  author       = {Bangzhen Liu and Yuyang Yu and Xuemiao Xu and Cheng Xu and Chenxi Zheng and Haoxin Yang and Shaoyu Huang and Shengfeng He},
  doi          = {10.1109/TPAMI.2025.3593807},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GenPoly: Learning generalized and tessellated shape priors via 3D polymorphic evolving},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised hypergraph training framework via structure-aware learning. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3594487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs, with their ability to model complex, beyond pair-wise correlations, presents a significant advancement over traditional graphs for capturing intricate relational data across diverse domains. However, the integration of hypergraphs into self-supervised learning (SSL) frameworks has been hindered by the intricate nature of high-order structural variations. This paper introduces the Self-Supervised Hypergraph Training Framework via Structure-Aware Learning (SS-HT), designed to enhance the perception and measurement of these variations within hypergraphs. The SS-HT framework employs a “Masking and Re-Masking” strategy to bolster feature reconstruction in Hypergraph Neural Networks (HGNNs), addressing the limitations of traditional SSL methods. It also introduces a metric strategy for local high-order correlation changes, streamlining the computational efficiency of structural distance calculations. Extensive experiments on 11 datasets demonstrate SS-HT's superior performance over existing SSL methods for both low-order and high-order data. Notably, the framework significantly reduces data labeling dependency, achieving a 32% improvement over HGNN in the downstream task fine-tuning phase under the 1% labeled data setting in the Cora-CC dataset. Ablation studies further validate SS-HT's scalability and its capacity to augment the performance of various HGNN methods, underscoring its robustness and applicability in real-world scenarios.},
  archive      = {J_TPAMI},
  author       = {Yifan Feng and Shiquan Liu and Shihui Ying and Shaoyi Du and Zongze Wu and Yue Gao},
  doi          = {10.1109/TPAMI.2025.3594487},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised hypergraph training framework via structure-aware learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RKHS-BA: A robust correspondence-free multi-view bundle adjustment framework for semantic point clouds. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3593521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work reports a novel multi-frame Bundle Adjustment (BA) framework called RKHS-BA. It uses continuous landmark representations that encode RGB-D/LiDAR and semantic observations in a Reproducing Kernel Hilbert Space (RKHS). With a correspondence-free pose graph formulation, the proposed system constructs a loss function that achieves more generalized convergence than classical point-wise convergence. We demonstrate its applications in multi-view point cloud registration, sliding-window odometry, and global LiDAR mapping on simulated and real data. It shows highly robust pose estimations in extremely noisy scenes and exhibits strong generalization with various types of semantic inputs. The open source implementation is released in https://github.com/UMich-CURLY/RKHS_BA.},
  archive      = {J_TPAMI},
  author       = {Ray Zhang and Jingwei Song and Xiang Gao and Junzhe Wu and Tiany Liu and Jinyuan Zhang and Ryan Eustice and Maani Ghaffari},
  doi          = {10.1109/TPAMI.2025.3593521},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RKHS-BA: A robust correspondence-free multi-view bundle adjustment framework for semantic point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based point cloud compression: An in-depth survey and benchmark. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3594355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturity of 3D capture technology, the explosive growth of point cloud data has burdened the storage and transmission process. Traditional hybrid point cloud compression (PCC) tools relying on handcrafted priors have limited compression performance and are increasingly weak in addressing the burden induced by data growth. Recently, deep learning-based PCC methods have been introduced to continue to push the PCC performance boundary. With the thriving of deep PCC, the community urgently demands a systematic overview to conclude the past progress and present future research directions. In this paper, we have a detailed review that covers popular point cloud datasets, algorithm evolution, benchmarking analysis, and future trends. Concretely, we first introduce several widely-used PCC datasets according to their major properties. Then the algorithm evolution of existing studies on deep PCC, including lossy ones and lossless ones proposed for various point cloud types, is reviewed. Apart from academic studies, we also investigate the development of relevant international standards (i.e., MPEG standards and JPEG standards). To help have an in-depth understanding of the advance of deep PCC, we select a representative set of methods and conduct extensive experiments on multiple datasets. Comprehensive benchmarking comparisons and analysis reveal the pros and cons of previous methods. Finally, based on the profound analysis, we highlight the challenges and future trends of deep learning-based PCC, paving the way for further study. Related source codes in this paper can be found at https://openi.pcl.ac.cn/OpenPointCloud.},
  archive      = {J_TPAMI},
  author       = {Wei Gao and Liang Xie and Songlin Fan and Ge Li and Shan Liu and Wen Gao},
  doi          = {10.1109/TPAMI.2025.3594355},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning-based point cloud compression: An in-depth survey and benchmark},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HL-HGAT: Heterogeneous graph attention network via hodge-laplacian operator. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3594226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. We design a novel Hodge-Laplacian heterogeneous graph attention network (HL-HGAT) to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatial localization properties. Additionally, we propose a pooling operator to coarsen $k$-simplices, combining features through simplicial attention mechanisms of self-attention and cross-attention via transformers and SP operators, capturing topological interconnections across multiple dimensions of simplices. The HL-HGAT is comprehensively evaluated across diverse graph applications, including NP-hard problems, graph multi-label and classification challenges, and graph regression tasks in logistics, computer vision, biology, chemistry, and neuroscience. The results demonstrate the model's efficacy and versatility in handling a wide range of graph-based scenarios.},
  archive      = {J_TPAMI},
  author       = {Jinghan Huang and Qiufeng Chen and Pengli Zhu and Yijun Bian and Nanguang Chen and Moo K. Chung and Anqi Qiu},
  doi          = {10.1109/TPAMI.2025.3594226},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HL-HGAT: Heterogeneous graph attention network via hodge-laplacian operator},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridge the intra-class gap: K-shot multi-scale intermediate prototype mining transformer for few-shot semantic segmentation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3593816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims to accurately segment target objects in a query image using only a limited number of annotated support images. Existing approaches typically follow a paradigm that directly leverages category information from the support set to identify target objects in the query. However, these methods often ignore the category information gap between query and support images, leading to suboptimal performance when faced with images containing objects exhibiting significant intra-class diversity. To address this issue, we propose a novel framework that introduces intermediate prototypes to capture both deterministic information from the support images and adaptive knowledge from the query at multiple scales. Our framework, named the K-shot Multi-scale Intermediate Prototype Mining Transformer (KMIPMT), is based on the Transformer architecture and learns intermediate prototypes in an iterative manner, where each KMIPMT layer propagates category information from both K-shot support features and multi-scale query features to intermediate prototypes. This information is then utilized to activate the query feature map. Through repeated iterations, both intermediate prototypes and the query feature are progressively enhanced, and the final refined query feature is used for generating precise segmentation predictions. Despite its simplicity, our method achieves remarkable performance gains on standard benchmarks, including PASCAL-$5^{i}$, COCO-$20^{i}$, and FSS-1000, setting new state-of-the-art results. Furthermore, we explore several practical and challenging extensions of our method, including 3D point cloud FSS, zero-shot segmentation, weak-label FSS, and cross-domain FSS. These extensions showcase the versatility and effectiveness of our proposed KMIPMT framework across different domains and scenarios. Code is available at https://github.com/LIUYUANWEI98/KMIPMT.},
  archive      = {J_TPAMI},
  author       = {Yuanwei Liu and Nian Liu and Tao Jiang and Xiwen Yao and Rao Muhammad Anwer and Hisham Cholakkal and Junwei Han},
  doi          = {10.1109/TPAMI.2025.3593816},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bridge the intra-class gap: K-shot multi-scale intermediate prototype mining transformer for few-shot semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gen-3Diffusion: Realistic image-to-3D generation via 2D & 3D diffusion synergy. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3577067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D avatars and objects with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on our https://yuxuan-xue.com/gen-3diffusion.},
  archive      = {J_TPAMI},
  author       = {Yuxuan Xue and Xianghui Xie and Riccardo Marin and Gerard Pons-Moll},
  doi          = {10.1109/TPAMI.2025.3577067},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gen-3Diffusion: Realistic image-to-3D generation via 2D & 3D diffusion synergy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIR: 3D gaussian inverse rendering for relightable scene factorization. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3575937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing 3D Gaussian representations to effectively factorize the scene into material properties, light, and geometry. The key contributions are three-fold. We compute the normal of each 3D Gaussian using the shortest eigenvector, with a directional masking scheme forcing accurate normal estimation without external supervision. We adopt an efficient voxel-based indirect illumination tracing scheme that stores direction-aware outgoing radiance in each 3D Gaussian to disentangle secondary illumination for approximating multi-bounce light transport. To further enhance the illumination disentanglement, we represent a high-resolution environmental map with a learnable low-resolution map and a lightweight, fully convolutional network. Our method achieves state-of-the-art performance in both relighting and novel view synthesis tasks among the recently proposed inverse rendering methods while achieving real-time rendering. This substantiates our proposed method's efficacy and broad applicability, highlighting its potential as an influential tool in various real-time interactive graphics applications such as material editing and relighting.},
  archive      = {J_TPAMI},
  author       = {Yahao Shi and Yanmin Wu and Chenming Wu and Xing Liu and Chen Zhao and Haocheng Feng and Jian Zhang and Bin Zhou and Errui Ding and Jingdong Wang},
  doi          = {10.1109/TPAMI.2025.3575937},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GIR: 3D gaussian inverse rendering for relightable scene factorization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physically based facial texture generation in the wild. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3580953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D facial texture generation has gained significant interest recently. However, existing approaches may lack compatibility with the widely used physically based rendering (PBR) pipeline or rely on 3D data captured by sophisticated systems such as Light Stage. In this paper, we propose a multistage framework to achieve text-driven physically based facial texture generation in the wild, which eliminates the reliance on expensive, controlled capture environments. It is based on FFHQUV to pave the way between the normalized UV texture space and facial images captured in unconstrained real-world settings and remove the influence of the background or hair in natural images on PBR texture generation. Specifically, we first integrate differentiable rendering techniques and carefully crafted texture disentanglement regularization to train a generative adversarial network for efficient PBR texture sampling. Then, the latent space of the network is aligned with the text embedding space for flexible text-guided generation. Besides, we design an edgeaware Score Distillation Sampling (EASDS) loss and introduce an EASDS-based PBR texture boosting scheme to achieve more diverse generation and efficient SDS optimization. Experiments demonstrate that our method outperforms existing PBR texture generation methods.},
  archive      = {J_TPAMI},
  author       = {Chi Wang and Junming Huang and Rong Zhang and Qi Wang and Haotian Yang and Pengfei Wan and Haibin Huang and Chongyang Ma and Weiwei Xu},
  doi          = {10.1109/TPAMI.2025.3580953},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Physically based facial texture generation in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond binary: Improving signed message passing in graph neural networks for multi-class graphs. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3581218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) exhibit satisfactory performance on homophilic networks, where most edges connect two nodes with the same label. However, their effectiveness diminishes as the graphs become heterophilic (low homophily), prompting the exploration of various message-passing schemes. In particular, assigning negative weights to heterophilic edges (signed propagation) for message-passing has gained significant attention, and some studies theoretically confirm its effectiveness. Nevertheless, prior theorems assume binary classification scenarios, which may not hold well for graphs with multiple classes. To solve this limitation, we offer new theoretical insights into GNNs in multi-class environments and identify the drawbacks of employing signed propagation from two perspectives: message-passing and parameter update. We found that signed propagation without considering feature distribution can degrade the separability of dissimilar neighbors, which also increases prediction uncertainty (e.g., conflicting evidence) that can cause instability. To address these limitations, we introduce two novel calibration strategies aiming to improve discrimination power while reducing entropy in predictions. Through theoretical and extensive experimental analysis, we demonstrate that the proposed schemes enhance the performance of both signed and general message-passing neural networks [1].},
  archive      = {J_TPAMI},
  author       = {Yoonhyuk Choi and Taewook Ko and Jiho Choi and Chong-Kwon Kim},
  doi          = {10.1109/TPAMI.2025.3581218},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Beyond binary: Improving signed message passing in graph neural networks for multi-class graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social reasoning-aware trajectory prediction via multimodal language model. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3582000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in language models have demonstrated its capacity of context understanding and generative representations. Leveraged by these developments, we propose a novel multimodal trajectory predictor based on a vision-language model, named VLMTraj, which fully takes advantage of the prior knowledge of multimodal large language models and the human-like reasoning across diverse modality information. The key idea of our model is to reframe the trajectory prediction task into a visual question answering format, using historical information as context and instructing the language model to make predictions in a conversational manner. Specifically, we transform all the inputs into a natural language style: historical trajectories are converted into text prompts, and scene images are described through image captioning. Additionally, visual features from input images are also transformed into tokens via a modality encoder and connector. The transformed data is then formatted to be used in a language model. Next, in order to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answers. For training, we first optimize a numerical tokenizer with the prompt data to effectively separate integer and decimal parts, allowing us to capture correlations between consecutive numbers in the language model. We then train our language model using all the visual question answering prompts. During model inference, we implement both deterministic and stochastic prediction methods through beam-search-based most-likely prediction and temperature-based multimodal generation. Our VLMTraj validates that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Extensive experiments show that VLMTraj can successfully understand social relationships and accurately extrapolate the multimodal futures on public pedestrian trajectory prediction benchmarks.},
  archive      = {J_TPAMI},
  author       = {Inhwan Bae and Junoh Lee and Hae-Gon Jeon},
  doi          = {10.1109/TPAMI.2025.3582000},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Social reasoning-aware trajectory prediction via multimodal language model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). V3D: Video diffusion models are effective 3D generators. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3581312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate $360^{\circ }$ orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency.},
  archive      = {J_TPAMI},
  author       = {Zilong Chen and Yikai Wang and Feng Wang and Zhengyi Wang and Fuchun Sun and Huaping Liu},
  doi          = {10.1109/TPAMI.2025.3581312},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {6},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {V3D: Video diffusion models are effective 3D generators},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Octree-GS: Towards consistent real-time rendering with LOD-structured 3D gaussians. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3568201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed 3D Gaussian Splatting (3D-GS) demonstrates superior rendering fidelity and efficiency compared to NeRF-based scene representations. However, it struggles in large-scale scenes due to the high number of Gaussian primitives, particularly in zoomed-out views, where all primitives are rendered regardless of their projected size. This often results in inefficient use of model capacity and difficulty capturing details at varying scales. To address this, we introduce Octree-GS, a Level-of-Detail (LOD) structured approach that dynamically selects appropriate levels from a set of multi-scale Gaussian primitives, ensuring consistent rendering performance. To adapt the design of LOD, we employ an innovative grow-and-prune strategy for densification and also propose a progressive training strategy to arrange Gaussians into appropriate LOD levels. Additionally, our LOD strategy generalizes to other Gaussian-based methods, such as 2D-GS and Scaffold-GS, reducing the number of primitives needed for rendering while maintaining scene reconstruction accuracy. Experiments on diverse datasets demonstrate that our method achieves real-time speeds, being up to 10× faster than state-of-the-art methods in large-scale scenes, without compromising visual quality. Project page: https://city-super.github.io/octree-gs/.},
  archive      = {J_TPAMI},
  author       = {Kerui Ren and Lihan Jiang and Tao Lu and Mulin Yu and Linning Xu and Zhangkai Ni and Bo Dai},
  doi          = {10.1109/TPAMI.2025.3568201},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Octree-GS: Towards consistent real-time rendering with LOD-structured 3D gaussians},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamComposer++: Empowering diffusion models with multi-view conditions for 3D content generation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3568190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in leveraging pre-trained 2D diffusion models achieve the generation of high-quality novel views from a single in-the-wild image. However, existing works face challenges in producing controllable novel views due to the lack of information from multiple views. In this paper, we present DreamComposer++, a flexible and scalable framework designed to improve current view-aware diffusion models by incorporating multi-view conditions. Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to extract 3D representations of an object from various views. These representations are then aggregated and rendered into the latent features of target view through the multi-view feature fusion module. Finally, the obtained features of target view are integrated into pre-trained image or video diffusion models for novel view synthesis. Experimental results demonstrate that DreamComposer++ seamlessly integrates with cutting-edge view-aware diffusion models and enhances their abilities to generate controllable novel views from multi-view conditions. This advancement facilitates controllable 3D object reconstruction and enables a wide range of applications.},
  archive      = {J_TPAMI},
  author       = {Yunhan Yang and Shuo Chen and Yukun Huang and Xiaoyang Wu and Yuan-Chen Guo and Edmund Y. Lam and Hengshuang Zhao and Tong He and Xihui Liu},
  doi          = {10.1109/TPAMI.2025.3568190},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {5},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamComposer++: Empowering diffusion models with multi-view conditions for 3D content generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gamba: Marry gaussian splatting with mamba for single-view 3D reconstruction. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3569596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the challenge of efficiently reconstructing a 3D asset from a single image at millisecond speed. In this work, we introduce Gamba, an end-to-end 3D reconstruction model from a single-view image, emphasizing two main insights: (1) Efficient Backbone Design: introducing a Mamba-based GambaFormer network to model 3D Gaussian Splatting (3DGS) reconstruction as sequential prediction with linear scalability of token length, thereby accommodating a substantial number of Gaussians; (2) Robust Gaussian Constraints: deriving radial mask constraints from multi-view masks to eliminate the need for warmup supervision of 3D point clouds in training. We trained Gamba on Objaverse and assessed it against existing optimization-based and feed-forward 3D reconstruction approaches on the GSO Dataset, among which Gamba is the only end-to-end trained single-view reconstruction model with 3DGS. Experimental results demonstrate its competitive generation capabilities both qualitatively and quantitatively and highlight its remarkable speed: Gamba completes reconstruction within 0.05 seconds on a single NVIDIA A100 GPU, which is about $1,000\times$ faster than optimization-based methods. Please see our project page at https://florinshen.github.io/gamba-project/.},
  archive      = {J_TPAMI},
  author       = {Qiuhong Shen and Zike Wu and Xuanyu Yi and Pan Zhou and Hanwang Zhang and Shuicheng Yan and Xinchao Wang},
  doi          = {10.1109/TPAMI.2025.3569596},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {5},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gamba: Marry gaussian splatting with mamba for single-view 3D reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EigenActor: Variant body-object interaction generation evolved from invariant action basis reasoning. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3573414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a cross-modality synthesis task that infers 3D human-object interactions (HOIs) from a given text-based instruction. Existing text-to-HOI synthesis methods mainly deploy a direct mapping from texts to object-specific 3D body motions, which may encounter a performance bottleneck since the huge cross-modality gap. In this paper, we observe that those HOI samples with the same interaction intention toward different targets, e.g., “lift a chair” and “lift a cup”, always encapsulate similar action-specific body motion patterns while characterizing different object-specific interaction styles. Thus, learning effective action-specific motion priors and object-specific interaction priors is crucial for a text-to-HOI model and dominates its performances on text-HOI semantic consistency and body-object interaction realism. In light of this, we propose a novel body pose generation strategy for the text-to-HOI task: infer object-agnostic canonical body action first and then enrich object-specific interaction styles. Specifically, the first canonical body action inference stage focuses on learning intra-class shareable body motion priors and mapping given text-based semantics to action-specific canonical 3D body motions. Then, in the object-specific interaction inference stage, we focus on object affordance learning and enrich object-specific interaction styles on an inferred action-specific body motion basis. Extensive experiments verify that our proposed text-to-HOI synthesis system significantly outperforms other SOTA methods on three large-scale datasets with better semantic consistency and interaction realism performances.},
  archive      = {J_TPAMI},
  author       = {Xuehao Gao and Yang Yang and Shaoyi Du and Yang Wu and Yebin Liu and Guo-Jun Qi},
  doi          = {10.1109/TPAMI.2025.3573414},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {5},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EigenActor: Variant body-object interaction generation evolved from invariant action basis reasoning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPS-gaussian+: Generalizable pixel-wise 3D gaussian splatting for real-time human-scene rendering from sparse views. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3561248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable rendering techniques have recently shown promising results for free-viewpoint video synthesis of characters. However, such methods, either Gaussian Splatting or neural implicit rendering, typically necessitate per-subject optimization which does not meet the requirement of real-time rendering in an interactive application. We propose a generalizable Gaussian Splatting approach for high-resolution image rendering under a sparse-view camera setting. To this end, we introduce Gaussian parameter maps defined on the source views and directly regress Gaussian properties for instant novel view synthesis without any fine-tuning or optimization. We train our Gaussian parameter regression module on human-only data or human-scene data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable with both depth and rendering supervision or with only rendering supervision. We further introduce a regularization term and an epipolar attention mechanism to preserve geometry consistency between two source views, especially when neglecting depth supervision. Experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed. Our project page is available at https://yaourtb.github.io/GPS-Gaussian+.},
  archive      = {J_TPAMI},
  author       = {Boyao Zhou and Shunyuan Zheng and Hanzhang Tu and Ruizhi Shao and Boning Liu and Shengping Zhang and Liqiang Nie and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3561248},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {4},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GPS-gaussian+: Generalizable pixel-wise 3D gaussian splatting for real-time human-scene rendering from sparse views},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video4DGen: Enhancing video and 4D generation through mutual optimization. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3550031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of 4D (i.e., sequential 3D) generation opens up new possibilities for lifelike experiences in various applications, where users can explore dynamic objects or characters from any viewpoint. Meanwhile, video generative models are receiving particular attention given their ability to produce realistic and imaginative frames. These models are also observed to exhibit strong 3D consistency, indicating the potential to act as world simulators. In this work, we present Video4DGen, a novel framework that excels in generating 4D representations from single or multiple generated videos as well as generating 4D-guided videos. This framework is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence. The 4D outputs generated by Video4DGen are represented using our proposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state. We design warped-state geometric regularization and refinements on Gaussian surfels, to preserve the structural integrity and fine-grained appearance details, respectively. Additionally, in order to perform 4D generation from multiple videos and effectively capture representation across spatial, temporal, and pose dimensions, we design multi-video alignment, root pose optimization, and pose-guided frame sampling strategies. The leveraging of continuous warping fields also enables a precise depiction of pose, motion, and deformation over per-video frames. Further, to improve the overall fidelity from the observation of all camera poses, Video4DGen performs novel-view video generation guided by the 4D content, with the proposed confidence-filtered DGS to enhance the quality of generated sequences. In summary, Video4DGen yields dynamic 4D generation with the ability to handle different subject movements, while preserving details in both geometry and appearance. The framework also generates 4D-guided videos with high spatial and temporal coherence. With the ability of 4D and video generation, Video4DGen offers a powerful tool for applications in virtual reality, animation, and beyond.},
  archive      = {J_TPAMI},
  author       = {Yikai Wang and Guangce Liu and Xinzhou Wang and Zilong Chen and Jiafang Li and Xin Liang and Fuchun Sun and Jun Zhu},
  doi          = {10.1109/TPAMI.2025.3550031},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video4DGen: Enhancing video and 4D generation through mutual optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instant gaussian splatting generation for high-quality and real-time facial asset rendering. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3550195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional and AI-driven modeling techniques enable high-fidelity 3D asset generation from scans, videos, or text prompts. However, editing and rendering these assets often involves a trade-off between quality and speed. In this paper, we propose GauFace, a novel Gaussian Splatting representation, tailored for efficient rendering of facial mesh with textures. Then, we introduce TransGS, a diffusion transformer that instantly generates the GauFace assets from mesh, textures and lightning conditions. Specifically, we adopt a patch-based pipeline to handle the vast number of Gaussian Points, a novel texel-aligned sampling scheme with UV positional encoding to enhance the throughput of generating GauFace assets. Once trained, TransGS can generate GauFace assets in 5 seconds, delivering high fidelity and real-time facial interaction of 30fps@1440p to a Snapdragon 8 Gen 2 mobile platform. The rich conditional modalities further enable editing and animation capabilities reminiscent of traditional CG pipelines. We conduct extensive evaluations and user studies, compared to traditional renderers, as well as recent neural rendering methods. They demonstrate the superior performance of our approach for facial asset rendering. We also showcase diverse applications of facial assets using our TransGS approach and GauFace representation, across various platforms like PCs, phones, and VR headsets.},
  archive      = {J_TPAMI},
  author       = {Dafei Qin and Hongyang Lin and Qixuan Zhang and Kaichun Qiao and Longwen Zhang and Jun Saito and Zijun Zhao and Jingyi Yu and Lan Xu and Taku Komura},
  doi          = {10.1109/TPAMI.2025.3550195},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Instant gaussian splatting generation for high-quality and real-time facial asset rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M2 diffuser: Diffusion-based trajectory optimization for mobile manipulation in 3D scenes. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3553454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation–a capability that requires the coordination of navigation and manipulation–remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2 Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2 Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2 Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution. Videos, code and more details are available at https://m2diffuser.github.io.},
  archive      = {J_TPAMI},
  author       = {Sixu Yan and Zeyu Zhang and Muzhi Han and Zaijin Wang and Qi Xie and Zhitian Li and Zhehan Li and Hangxin Liu and Xinggang Wang and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2025.3553454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {M2 diffuser: Diffusion-based trajectory optimization for mobile manipulation in 3D scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LFSRM: Few-shot diagram-sentence matching via local-feedback self-regulating memory. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3528723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-sentence matching that aims to understand the correspondence between vision and language, has achieved significant progress with various deep methods trained under large-scale supervision. Different from natural images taken by camera, diagrams in the textbooks contain more graphic objects, drawings, and natural objects, and the diagram-sentence matching plays an important role in textbook understanding and question answering. However, existing matching models are not suitable for the challenging task between diagrams and sentences, due to the more serious few-shot content and incomplete description problems. In this paper, we propose a novel local-feedback self-regulating memory framework (LFSRM) for diagram-sentence matching. On one hand, LFSRM includes an external memory to store the useful multi-modal information, especially uncommon ones, to overcome the few-shot content problem, where the memory is updated flexibly according to the local-feedback from visual-textual alignment scores. On the other hand, LFSRM designs an attention mechanism on local-level alignment scores and a strengthening factor impacted on sentence-to-diagram matching direction for alleviating the incomplete description problem. Extensive experiments on three datasets show that LFSRM achieves satisfactory results on conventional image-sentence matching, and outperforms SOTA methods on few-shot image/diagram-sentence matching by a large margin. The dataset for diagram-sentence matching called AI2D$^\#$ and the LFSRM code are opened on Github https://github.com/TeamResearchWork/LFSRM.},
  archive      = {J_TPAMI},
  author       = {Lingling Zhang and Wenjun Wu and Jun Liu and Xiaojun Chang and Xin Hu and Yuhui Zheng and Yaqiang Wu and Qinghua Zheng},
  doi          = {10.1109/TPAMI.2025.3528723},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LFSRM: Few-shot diagram-sentence matching via local-feedback self-regulating memory},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of deep learning for video captioning. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2024.3522295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning (VC) is a fast-moving, cross-disciplinary area of research that comprises contributions from domains such as computer vision, natural language processing, linguistics, and human-computer interaction. VC aims to understand a video and describe it through natural language descriptors. It plays a crucial role in various applications, from improving accessibility features such as low-vision navigation to advancing video question answering, video retrieval, and content generation. In this survey paper, we present a comprehensive review of deep learning-based VC methods. First, we provide an overview of VC, including the problem formulation, evaluation metrics, training losses, and attention-based architectures. Then, we categorize VC methods into several categories, including attention-based architectures graph networks, reinforcement learning, adversarial networks, and dense video captioning, and discuss each category in detail. In addition, we review existing data sets for VC methods and provide a discussion of research gaps and future research directions. We hope that this survey serves as a guide for researchers in relevant fields.},
  archive      = {J_TPAMI},
  author       = {Moloud Abdar and Meenakshi Kollati and Swaraja Kuraparthi and Farhad Pourpanah and Daniel McDuff and Mohammad Ghavamzadeh and Shuicheng Yan and Abduallah Mohamed and Abbas Khosravi and Erik Cambria and Fatih Porikli},
  doi          = {10.1109/TPAMI.2024.3522295},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {12},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A review of deep learning for video captioning},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonconvex zeroth-order stochastic ADMM methods with lower function query complexity. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2023.3347082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroth-order (a.k.a, derivative-free) methods are a class of effective optimization methods for solving complex machine learning problems, where gradients of the objective functions are not available or computationally prohibitive. Recently, although many zeroth-order methods have been developed, these approaches still have two main drawbacks: 1) high function query complexity; 2) not being well suitable for solving the problems with complex penalties and constraints. To address these challenging drawbacks, in this paper, we propose a class of faster zeroth-order stochastic alternating direction method of multipliers (ADMM) methods (ZO-SPIDER-ADMM) to solve the nonconvex finite-sum problems with multiple nonsmooth penalties. Moreover, we prove that the ZO-SPIDER-ADMM methods can achieve a lower function query complexity of $O(nd+dn^{\frac{1}{2}}\epsilon ^{-1})$ for finding an $\epsilon$ -stationary point, which improves the existing best nonconvex zeroth-order ADMM methods by a factor of $O(d^{\frac{1}{3}}n^{\frac{1}{6}})$ , where $n$ and $d$ denote the sample size and data dimension, respectively. At the same time, we propose a class of faster zeroth-order online ADMM methods (ZOO-ADMM+) to solve the nonconvex online problems with multiple nonsmooth penalties. We also prove that the proposed ZOO-ADMM+ methods achieve a lower function query complexity of $O(d\epsilon ^{-\frac{3}{2}})$ , which improves the existing best result by a factor of $O(\epsilon ^{-\frac{1}{2}})$ . Extensive experimental results on the structure adversarial attack on black-box deep neural networks demonstrate the efficiency of our new algorithms.},
  archive      = {J_TPAMI},
  author       = {Feihu Huang and Shangqian Gao and Jian Pei and Heng Huang},
  doi          = {10.1109/TPAMI.2023.3347082},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Nonconvex zeroth-order stochastic ADMM methods with lower function query complexity},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ego4D: Around the world in 3,000 hours of egocentric video. <em>TPAMI</em>, 1-32. (<a href='https://doi.org/10.1109/TPAMI.2024.3381075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/},
  archive      = {J_TPAMI},
  author       = {Kristen Grauman and Andrew Westbury and Eugene Byrne and Vincent Cartillier and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Devansh Kukreja and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh Kumar Ramakrishnan and Fiona Ryan and Jayant Sharma and Michael Wray and Mengmeng Xu and Eric Zhongcong Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Sean Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and Adriano Fragomeni and Qichen Fu and Abrham Gebreselasie and Cristina Gonzalez ´ and James Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and Jachym Kol ´ a´ˇr and Satwik Kottur and Anurag Kumar and Federico Landini and Chao Li and Yanghao Li and Zhenqiang Li and Karttikeya Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and Kiran Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Ziwei Zhao and Yunyi Zhu and Pablo Arbelaez and David Crandall and Dima Damen and Giovanni Maria Farinella and Christian Fuegen and Bernard Ghanem and Vamsi Krishna Ithapu and C. V. Jawahar and Hanbyul Joo and Kris Kitani and Haizhou Li and Richard Newcombe and Aude Oliva and Hyun Soo Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and Antonio Torralba and Lorenzo Torresani and Mingfei Yan and Jitendra Malik},
  doi          = {10.1109/TPAMI.2024.3381075},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1-32},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ego4D: Around the world in 3,000 hours of egocentric video},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistency-aware anchor pyramid network for crowd localization. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2024.3392013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd localization aims to predict the positions of humans in images of crowded scenes. While existing methods have made significant progress, two primary challenges remain: (i) a fixed number of evenly distributed anchors can cause excessive or insufficient predictions across regions in an image with varying crowd densities, and (ii) ranking inconsistency of predictions between the testing and training phases leads to the model being sub-optimal in inference. To address these issues, we propose a Consistency-Aware Anchor Pyramid Network (CAAPN) comprising two key components: an Adaptive Anchor Generator (AAG) and a Localizer with Augmented Matching (LAM). The AAG module adaptively generates anchors based on estimated crowd density in local regions to alleviate the anchor deficiency or excess problem. It also considers the spatial distribution prior to heads for better performance. The LAM module is designed to augment the predictions which are used to optimize the neural network during training by introducing an extra set of target candidates and correctly matching them to the ground truth. The proposed method achieves favorable performance against state-of-the-art approaches on five challenging datasets: ShanghaiTech A and B, UCF-QNRF, JHU-CROWD++, and NWPU-Crowd. The source code and trained models will be released at https://github.com/ucasyan/CAAPN .},
  archive      = {J_TPAMI},
  author       = {Xinyan Liu and Guorong Li and Yuankai Qi and Zhenjun Han and Anton van den Hengel and Nicu Sebe and Ming-Hsuan Yang and Qingming Huang},
  doi          = {10.1109/TPAMI.2024.3392013},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {4},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Consistency-aware anchor pyramid network for crowd localization},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable optimal transport methods in machine learning: A contemporary survey. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2024.3379571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade has been a witness to the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of scalable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain the optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the findings in a unified taxonomy. We conclude with presenting some open challenges and discussing potential future research directions. A live repository of related OT research papers is maintained in https://github.com/abdelwahed/OT_for_big_data.git},
  archive      = {J_TPAMI},
  author       = {Abdelwahed Khamis and Russell Tsuchida and Mohamed Tarek and Vivien Rolland and Lars Petersson},
  doi          = {10.1109/TPAMI.2024.3379571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scalable optimal transport methods in machine learning: A contemporary survey},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnyFace++: A unified framework for free-style text-to-face synthesis and manipulation. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2023.3345866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human faces contain rich semantic information that could hardly be described without a large vocabulary and complex sentence patterns. However, most existing text-to-image synthesis methods could only generate meaningful results based on limited sentence templates with words contained in the training set, which heavily impairs the generalization ability of these models. In this paper, we define a novel ‘free-style’ text-to-face generation and manipulation problem, and propose an effective solution, named AnyFace++, which is applicable to a much wider range of open-world scenarios. The CLIP model is involved in AnyFace++ for learning an aligned language-vision feature space, which also expands the range of acceptable vocabulary as it is trained on a large-scale dataset. To further improve the granularity of semantic alignment between text and images, a memory module is incorporated to convert the description with arbitrary length, format, and modality into regularized latent embeddings representing discriminative attributes of the target face. Moreover, the diversity and semantic consistency of generation results are improved by a novel semi-supervised training scheme and a series of newly proposed objective functions. Compared to state-of-the-art methods, AnyFace++ is capable of synthesizing and manipulating face images based on more flexible descriptions and producing realistic images with higher diversity.},
  archive      = {J_TPAMI},
  author       = {Jianxin Sun and Qiyao Deng and Qi Li and Muyi Sun and Yunfan Liu and Zhenan Sun},
  doi          = {10.1109/TPAMI.2023.3345866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AnyFace++: A unified framework for free-style text-to-face synthesis and manipulation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EPro-PnP: Generalized end-to-end probabilistic perspective-N-points for monocular object pose estimation. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2024.3354997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest interpreting PnP as a differentiable layer, allowing for partial learning of 2D-3D point correspondences by backpropagating the gradients of pose loss. Yet, learning the entire correspondences from scratch is highly challenging, particularly for ambiguous pose solutions, where the globally optimal pose is theoretically non-differentiable w.r.t. the points. In this paper, we propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation, which outputs a distribution of pose with differentiable probability density on the SE(3) manifold. The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution. The underlying principle generalizes previous approaches, and resembles the attention mechanism. EPro-PnP can enhance existing correspondence networks, closing the gap between PnP-based method and the task-specific leaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP helps to explore new possibilities of network design, as we demonstrate a novel deformable correspondence network with the state-of-the-art pose accuracy on the nuScenes 3D object detection benchmark. Our code is available at https://github.com/tjiiv-cprg/EPro-PnP-v2.},
  archive      = {J_TPAMI},
  author       = {Hansheng Chen and Wei Tian and Pichao Wang and Fan Wang and Lu Xiong and Hao Li},
  doi          = {10.1109/TPAMI.2024.3354997},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EPro-PnP: Generalized end-to-end probabilistic perspective-N-points for monocular object pose estimation},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Burst image restoration and enhancement. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2024.3356188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burst Image Restoration aims to reconstruct a high-quality image by efficiently combining complementary inter-frame information. However, it is quite challenging since individual burst images often have inter-frame misalignments that usually lead to ghosting and zipper artifacts. To mitigate this, we develop a novel approach for burst image processing named BIPNet that focuses solely on the information exchange between burst frames and filter-out the inherent degradations while preserving and enhancing the actual scene details. Our central idea is to generate a set of pseudo-burst features that combine complementary information from all the burst frames to exchange information seamlessly. However, due to inter-frame misalignment, the information cannot be effectively combined in pseudo-burst. Thus, we initially align the incoming burst features regarding the reference frame using the proposed edge-boosting feature alignment. Lastly, we progressively upscale the pseudo-burst features in multiple stages while adaptively combining the complementary information. Unlike the existing works, that usually deploy single-stage up-sampling with a late fusion scheme, we first deploy a pseudo-burst mechanism followed by the adaptive-progressive feature up-sampling. The proposed BIPNet significantly outperforms the existing methods on burst super-resolution, low-light image enhancement, low-light image super-resolution, and denoising tasks. The pre-trained models and source code are available at https://github.com/akshaydudhane16/BIPNet .},
  archive      = {J_TPAMI},
  author       = {Akshay Dudhane and Syed Waqas Zamir and Salman Khan and Fahad Shahbaz Khan and Ming-Husan Yang},
  doi          = {10.1109/TPAMI.2024.3356188},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Burst image restoration and enhancement},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ref-NeRF: Structured view-dependent appearance for neural radiance fields. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2024.3360018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.},
  archive      = {J_TPAMI},
  author       = {Dor Verbin and Peter Hedman and Ben Mildenhall and Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
  doi          = {10.1109/TPAMI.2024.3360018},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ref-NeRF: Structured view-dependent appearance for neural radiance fields},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-shutter optical vibration sensing. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2023.3344650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual vibrometry is a highly useful tool for remote capture of audio, as well as the physical properties of materials, human heart rate, and more. While visually-observable vibrations can be captured directly with a high-speed camera, minute imperceptible object vibrations can be optically amplified by imaging the displacement of a speckle pattern created by shining a laser beam on the vibrating surface. In this paper, we propose a novel method for sensing vibrations at high speeds (up to 63 kHz), for multiple scene sources at once, using sensors rated for only 130 Hz operation. Our method relies on simultaneously capturing the scene with two cameras equipped with rolling and global shutter sensors, respectively. The rolling shutter camera captures distorted speckle images that encode the high-speed object vibrations. The global shutter camera captures undistorted reference images of the speckle pattern, helping to decode the source vibrations. We demonstrate our method by capturing vibration caused by audio sources (e.g., speakers, human voice, and musical instruments) and analyzing the vibration modes of a tuning fork.},
  archive      = {J_TPAMI},
  author       = {Mark Sheinin and Dorian Chan and Matthew O'Toole and Srinivasa G. Narasimhan},
  doi          = {10.1109/TPAMI.2023.3344650},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {12},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual-shutter optical vibration sensing},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Light field neural rendering. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2023.3316992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical light field rendering for novel view synthesis can accurately reproduce view-dependent effects such as reflection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric reconstruction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional representation of the light field, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geometry is implicitly learned from a sparse set of views. Concretely, we introduce a two-stage transformer-based model that first aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Additionally, we propose modifications that allow the model to generalize to scenes without any fine-tuning. Our model outperforms the state-of-the-art on multiple forward-facing and 360 $^{\circ }$ datasets, with larger margins on scenes with severe view-dependent variations. Code and results can be found at https://light-field-neural-rendering.github.io/},
  archive      = {J_TPAMI},
  author       = {Mohammed Suhail and Carlos Esteves and Leonid Sigal and Ameesh Makadia},
  doi          = {10.1109/TPAMI.2023.3316992},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Light field neural rendering},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer-empowered invariant grounding for video question answering. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2023.3303451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Question Answering (VideoQA) is the task of answering questions about a video. At its core is the understanding of the alignments between video scenes and question semantics to yield the answer. In leading VideoQA models, the typical learning objective, empirical risk minimization (ERM), tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspecting the causal effect of question-critical scenes, which undermines the prediction with unreliable reasoning. In this work, we take a causal look at VideoQA and propose a modal-agnostic learning framework, named Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different interventions on the complement. With IGV, leading VideoQA models are forced to shield the answering from the negative influence of spurious correlations, which significantly improves their reasoning ability. To unleash the potential of this framework, we further provide a Transformer-Empowered Invariant Grounding for VideoQA (TIGV), a substantial instantiation of IGV framework that naturally integrates the idea of invariant grounding into a transformer-style backbone. Experiments on four benchmark datasets validate our design in terms of accuracy, visual explainability, and generalization ability over the leading baselines. Our code is available at https://github.com/yl3800/TIGV .},
  archive      = {J_TPAMI},
  author       = {Yicong Li and Xiang Wang and Junbin Xiao and Wei Ji and Tat-Seng Chua},
  doi          = {10.1109/TPAMI.2023.3303451},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transformer-empowered invariant grounding for video question answering},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to solve hard minimal problems. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2023.3307898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to solving hard geometric optimization problems in the RANSAC framework. The hard minimal problems arise from relaxing the original geometric optimization problem into a minimal problem with many spurious solutions. Our approach avoids computing large numbers of spurious solutions. We design a learning strategy for selecting a starting problem-solution pair that can be numerically continued to the problem and the solution of interest. We demonstrate our approach by developing a RANSAC solver for the problem of computing the relative pose of three calibrated cameras, via a minimal relaxation using four points in each view. On average, we can solve a single problem in under 70 μs. We also benchmark and study our engineering choices on the very familiar problem of computing the relative pose of two calibrated cameras, via the minimal case of five points in two views.},
  archive      = {J_TPAMI},
  author       = {Petr Hruby and Timothy Duff and Anton Leykin and Tomas Pajdla},
  doi          = {10.1109/TPAMI.2023.3307898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {8},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to solve hard minimal problems},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FINC: An efficient and effective optimization method for normalized cut. <em>TPAMI</em>, 1. (<a href='https://doi.org/10.1109/TPAMI.2022.3148812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization methods for solving the normalized cut model usually involve three steps, i.e., problem relaxation, problem solving and post-processing. However, these methods are problematic in both performance since they do not directly solve the original problem, and efficiency since they usually depend on the time-consuming eigendecomposition and k-means (or spectral rotation) for post-processing. In this paper, we propose a fast optimization method to speedup the classical normalized cut clustering process, in which an auxiliary variable is introduced and alternatively updated along with the cluster indicator matrix. The new method is faster than the conventional three-step optimization methods since it solves the normalized cut problem in one step. Theoretical analysis reveals that the new method is able to monotonically decrease the normalized cut objective function and converge in finite iterations. Moreover, we have proposed efficient methods for adjust two regularization parameters. Extensive experimental results show the superior performance of the new method. Moreover, it is faster than the existing methods for solving the normalized cut.},
  archive      = {J_TPAMI},
  author       = {Xiaojun Chen and Zhicong Xiao and Feiping Nie and Joshua Zhexue Huang},
  doi          = {10.1109/TPAMI.2022.3148812},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FINC: An efficient and effective optimization method for normalized cut},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geodesic multi-class SVM with stiefel manifold embedding. <em>TPAMI</em>, 1. (<a href='https://doi.org/10.1109/TPAMI.2021.3069498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold of geodesic plays an essential role in characterizing the intrinsic data geometry. However, the existing SVM methods have largely neglected the manifold structure. As such, functional degeneration may occur due to the potential polluted training. Even worse, the entire SVM model might collapse in the presence of excessive training contamination. To address these issues, this paper devises a manifold SVM method based on a novel $\xi$ -measure geodesic, whose primary design objective is to extract and preserve the data manifold structure in the presence of training noises. To further cope with overly contaminated training data, we introduce Kullback-Leibler (KL) regularization with steerable sparsity constraint. In this way, each loss weight is adaptively obtained by obeying the prior distribution and sparse activation during model training for robust fitting. Moreover, the optimal scale for Stiefel manifold can be automatically learned to improve the model flexibility. Accordingly, extensive experiments verify and validate the superiority of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Rui Zhang and Xuelong Li and Hongyuan Zhang and Ziheng Jiao},
  doi          = {10.1109/TPAMI.2021.3069498},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Geodesic multi-class SVM with stiefel manifold embedding},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FakeCatcher: Detection of synthetic portrait videos using biological signals. <em>TPAMI</em>, 1. (<a href='https://doi.org/10.1109/TPAMI.2020.3009287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent proliferation of fake portrait videos poses direct threats on society, law, and privacy [1]. Believing the fake video of a politician, distributing fake pornographic content of celebrities, fabricating impersonated fake videos as evidence in courts are just a few real world consequences of deep fakes. We present a novel approach to detect synthetic content in portrait videos, as a preventive solution for the emerging threat of deep fakes. In other words, we introduce a deep fake detector. We observe that detectors blindly utilizing deep learning are not effective in catching fake content, as generative models produce formidably realistic results. Our key assertion follows that biological signals hidden in portrait videos can be used as an implicit descriptor of authenticity, because they are neither spatially nor temporally preserved in fake content. To prove and exploit this assertion, we first engage several signal transformations for the pairwise separation problem, achieving 99.39% accuracy. Second, we utilize those findings to formulate a generalized classifier for fake content, by analyzing proposed signal transformations and corresponding feature sets. Third, we generate novel signal maps and employ a CNN to improve our traditional classifier for detecting synthetic content. Lastly, we release an "in the wild" dataset of fake portrait videos that we collected as a part of our evaluation process. We evaluate FakeCatcher on several datasets, resulting with 96%, 94.65%, 91.50%, and 91.07% accuracies, on Face Forensics [2], Face Forensics++ [3], CelebDF [4], and on our new Deep Fakes Dataset respectively. In addition, our approach produces a significantly superior detection rate against baselines, and does not depend on the source, generator, or properties of the fake content. We also analyze signals from various facial regions, under image distortions, with varying segment durations, from different generators, against unseen datasets, and under several dimensionality reduction techniques.},
  archive      = {J_TPAMI},
  author       = {Umur Aybars Ciftci and Ilke Demir and Lijun Yin},
  doi          = {10.1109/TPAMI.2020.3009287},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {7},
  pages        = {1},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FakeCatcher: Detection of synthetic portrait videos using biological signals},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2013). Contextualized trajectory parsing with spatio-temporal graph. <em>TPAMI</em>, 1. (<a href='https://doi.org/10.1109/TPAMI.2013.84'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates how to automatically parse object trajectories in surveillance videos, that aims to jointly solve three subproblems: i) spatial segmentation, ii) temporal tracking, and iii) object categorization. We present a novel representation spatio-temporal graph (ST-Graph), in which: i) graph nodes express the motion primitives, each representing a short sequence of small-size patches over consecutive images; and ii) every two neighbor nodes are linked with either a positive edge or a negative edge to describe their collaborative or exclusive relationship of belonging to the same object trajectory. Phrasing the trajectory parsing as a graph multi-coloring problem, we propose a unified probabilistic formulation to integrate various types of context knowledge as informative priors. An efficient composite cluster sampling algorithm is employed in search of the optimal solution by exploiting both the collaborative and the exclusive relationships between nodes. The proposed framework is evaluated over challenging videos from public datasets, and results show that it can achieve state-of-the-art tracking accuracy.},
  archive      = {J_TPAMI},
  author       = {Xiaobai Liu and Liang Lin and Hai Jin},
  doi          = {10.1109/TPAMI.2013.84},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {5},
  pages        = {1},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contextualized trajectory parsing with spatio-temporal graph},
  year         = {2013},
}
</textarea>
</details></li>
<li><details>
<summary>
(2009). A CNN-based face detector with a simple feature map and a coarse-to-fine classifier - Withdrawn. <em>TPAMI</em>, 1. (<a href='https://doi.org/10.1109/TPAMI.2007.70798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Withdrawn.},
  archive      = {J_TPAMI},
  author       = {Ying-Nong Chen and Chin-Chuan Han and Cheng-Tzu Wang and Bor-Shenn Jeng and Kuo-Chin Fan},
  doi          = {10.1109/TPAMI.2007.70798},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A CNN-based face detector with a simple feature map and a coarse-to-fine classifier - Withdrawn},
  year         = {2009},
}
</textarea>
</details></li>
</ul>

</body>
</html>
