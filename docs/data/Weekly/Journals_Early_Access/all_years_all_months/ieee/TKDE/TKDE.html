<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde">TKDE - 65</h2>
<ul>
<li><details>
<summary>
(2025). Semi-supervised short text stream classification based on drift-aware incremental deep learning. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3605389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world applications have produced massive short text streams. Contrary to the traditional normal texts, they present the characteristics such as short length, only having few labeled data, high-velocity, high-volume and dynamic data distributions, which deteriorate the issues of data sparseness, label missing and concept drift. Obviously, it is a huge challenge for existing short text (stream) classification algorithms due to the poor effectiveness, where they always assume all short texts are completely labeled and little attention is paid on the concept drift issue hidden in short text streams. Therefore, we propose a novel semi-supervised short text steam classification method based on the drift-aware incremental deep learning ensemble model. Specifically, with the sliding window mechanism, we firstly fuse three types of statistical, semantic and structure information to solve the data sparseness issue. Secondly, a semi-supervised incremental deep learning ensemble model based on GCN and the refined LSTM is developed to adapt to the high-volume, high-velocity and label missing short text streams. Thirdly, a label-probability distribution based concept drift detector is introduced to distinguish concept drifts. Finally, as compared with eleven well-known classification methods, extensive experiments demonstrate the effectiveness of the proposed method in the handling of short text streams with limited labeled data.},
  archive      = {J_TKDE},
  author       = {Peipei Li and Shiying Yu and Jiajun Li and Xuegang Hu},
  doi          = {10.1109/TKDE.2025.3605389},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised short text stream classification based on drift-aware incremental deep learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental multi-view clustering: Exploring stream-view correlations to learn consistency and diversity. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3605594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) has demonstrated impressive performance due to its ability to capture both consistency and diversity information among views. However, most existing techniques assume that all views are available in advance, making them inadequate for stream-view data, such as in intelligent transportation systems and medical imaging analysis, where memory constraints or privacy concerns prevent storing all previous views. Although some methods attempt to address this issue by capturing consistency information, they often fail to effectively extract diversity information and cross-view relationships. We argue that these limitations are inherent to incremental multi-view clustering (IMVC), as the inability to retain all previous views inevitably leads to insufficient information utilization, thereby compromising performance. To address these challenges, we propose a novel algorithm, termed Incremental Multi-View Clustering with Cross-View Correlation and Diversity (CDIMVC). Unlike existing methods that only retain consistency information, CDIMVC also preserves diversity information and utilizes similarity matrices to capture cross-view relationships. To implement this method, we develop three key modules: the dynamic view correlation analysis module (DVCAM), the knowledge extraction module (KEM), and the knowledge transfer module (KTM). When a new data view arrives, DVCAM first assesses its importance and correlation with historical views. Subsequently, KEM computes its consistency and diversity information by comparing it to those in the knowledge base. Finally, KTM facilitates the effective transmission of past knowledge, preventing the loss of historical information. By integrating these modules, CDIMVC can effectively capture cross-view relationships and diversity information, facilitating efficient knowledge updating and maintenance. An alternating procedure is also designed to optimize the resulting optimization problem. Experimental results show that CDIMVC exceeds state-of-the-art methods, demonstrating its effectiveness in handling stream-view data.},
  archive      = {J_TKDE},
  author       = {Yu Feng and Weixuan Liang and Xinhang Wan and Jiyuan Liu and Miaomiao Li and Xinwang Liu},
  doi          = {10.1109/TKDE.2025.3605594},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incremental multi-view clustering: Exploring stream-view correlations to learn consistency and diversity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized review summarization by using graph-based retrieval augmemted generation. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3605824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Review summarization aims to provide a summary that covers the main aspect of the product review and reflects personal preference. Existing methods employ the historical reviews of customer and product to provide useful clues for the target summary generation. However, most of the existing methods indiscriminately model the historical reviews of customer and product. Since the historical customer reviews provide the personal information while the historical product reviews provide the commonly focused aspect of the product, these two types of heterogeneous information should be separately modeled. Moreover, the review rating of the historical reviews can be seen as a high-level abstraction of the customer preference and product which have been ignored by most of the existing methods. In this paper, we propose the Heterogeneous Historical Review aware Review Summarization (HHRRS) which separately models the two types of historical reviews with the rating information by a graph reasoning module with a contrastive loss. We employ a multi-task paradigm that conducts the review sentiment classification and summarization (GRARS) to model the two types of heterogeneous information in a fine-grained manner. We conduct extensive experiments on four benchmark datasets, and demonstrate the superiority of HHRRS on both tasks.},
  archive      = {J_TKDE},
  author       = {Shuo Shang and Xin Cheng and Yiren Xiong and Feng Guo and Shen Gao and Xiuying Chen and Feng Wang and Yongbo Wang and Dongyan Zhao and Rui Yan},
  doi          = {10.1109/TKDE.2025.3605824},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Personalized review summarization by using graph-based retrieval augmemted generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A diffusive data augmentation framework for reconstruction of complex network evolutionary history. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3605795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolutionary dynamics of complex systems encode critical information about their functional organization. In particular, the generation times of edges reveal key aspects of historical development in networked systems such as protein-protein interaction networks, ecosystems, and social networks. Accurately recovering these temporal processes is of significant scientific value-for example, in elucidating the mechanisms underlying protein interaction evolution. However, existing methods typically assume access to partially time-stamped networks and often struggle to generalize across domains. They perform poorly in recovering edge generation times in static networks without temporal annotations. To address this challenge, we propose a comparative paradigm that enables cross-network learning by jointly training on multiple temporal networks. This framework captures structural-temporal correlations that generalize across networks and improves accuracy by 16.98% on average compared to separate training strategies. Furthermore, to mitigate the scarcity of real temporal data, we introduce a novel diffusion-based generative model for producing Augmented Temporal Networks (ATNs) . By integrating both real and generated samples during training, our joint strategy yields an additional 5.46% improvement in predictive accuracy, demonstrating the effectiveness of data augmentation in enhancing generalization.},
  archive      = {J_TKDE},
  author       = {En Xu and Can Rong and Jingtao Ding and Yong Li},
  doi          = {10.1109/TKDE.2025.3605795},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A diffusive data augmentation framework for reconstruction of complex network evolutionary history},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models meet causal inference: Semantic-rich dual propensity score for sequential recommendation. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3606149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommender systems (SRSs) are designed to suggest relevant items to users by analyzing their interaction sequences. However, SRSs often suffer from exposure bias in these sequences due to imbalanced item exposure and varied user activity levels, creating a self-reinforcing loop favoring popular items regardless of their true relevance. Most SRSs only focus on item dependencies to address exposure bias, while overlooking user-side exposure bias and the rich semantics behind interactions. These oversights result in a limited understanding of less active users' preferences and inaccurate preference capture for less exposed items, exacerbating exposure biases. Towards this end, we propose a novel method LLM-enhanced Dual Propensity Score Estimation (LDPE), which synergistically integrates Large Language Models (LLMs) and causal inference. First, LDPE leverages LLMs' superior ability in capturing rich semantics from textual data and then integrates collaborative information to generate debiased semantic-rich LLM-based user/item embeddings. With these debiased item/user embeddings, LDPE estimates time-aware debiased propensity scores from both the item and user sides. These dual propensity scores can fully mitigate exposure bias by considering item popularity, user activity levels, and temporal dynamics. Lastly, LDPE employs the transformer as the backbone of our method, incorporating estimated dual propensity scores for accurately predicting users' true preferences. Extensive experiments show that our LDPE outperforms state-of-the-art baselines in terms of recommendation performance.},
  archive      = {J_TKDE},
  author       = {Dianer Yu and Qian Li and Sirui Huang and Jie Cao and Guandong Xu},
  doi          = {10.1109/TKDE.2025.3606149},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large language models meet causal inference: Semantic-rich dual propensity score for sequential recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking oversampling with class alliance constraints from data complexity perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3606036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class overlap is a major factor of data complexity that hampers classifier performance, particularly in imbalanced learning scenarios. Most existing oversampling methods rely on conservative seed sample selection and decoupled synthesis strategies, which limit sample diversity and fail to effectively control overlap risk. This paper proposes a novel oversampling framework called TMACO (Class Alliance-Constrained Oversampling), which integrates data complexity considerations into both seed selection and sample generation. First, TMACO selects seed sample units using a class alliance constraint that jointly considers spatial geometry and class distribution to enhance diversity and representativeness. Second, it generates synthetic samples based on three-point units to ensure regional stability. Third, a region-level filtering mechanism is applied to prevent synthetic samples from intruding into majority class areas. Extensive experiments on benchmark and real-world datasets demonstrate that TMACO consistently improves minority class performance and overall classification accuracy compared to state-of-the-art oversampling techniques. The proposed method also offers interpretable parameter control and adapts well to varying task objectives.},
  archive      = {J_TKDE},
  author       = {Mingming Han and Husheng Guo and Gaoxia Jiang and Wenjian Wang},
  doi          = {10.1109/TKDE.2025.3606036},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rethinking oversampling with class alliance constraints from data complexity perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional shortest-path graph kernels. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3606566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional shortest-path graph kernels generate for each graph a histogram-like feature map, whose elements represent the number of occurrences of non-isomorphic shortest paths in this graph. The histogram-like feature map does not contain the distributions of the shortest paths within and across graphs, causing inaccurate graph similarities. To this end, we propose a novel graph kernel called the Distributional Shortest-Path (DSP) graph kernel to embrace both types of distribution information. Since the distribution of substructures (e.g., the shortest paths) follows a power law like that of words in natural language, we utilize neural language models to learn each node's distributional shortest-path feature map, encompassing the distributions and dependencies of the shortest paths in each graph. Moreover, we design the Partition Kernel (PK) to capture the dataset-wide distribution information of the shortest paths. PK projects similar (i.e., belonging to the same partition) distributional shortest-path node feature maps to the same point in the Reproducing Kernel Hilbert Space. Finally, Kernel Mean Embedding (KME) is applied to compute graph feature maps and efficiently construct the DSP graph kernel. Empirical experiments demonstrate that DSP outperforms state-of-the-art graph kernels on most benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Wei Ye and Wengang Guo and Shuhao Tang and Hao Tian and Xin Sun and Xiaofeng Cao and Heng Tao Shen},
  doi          = {10.1109/TKDE.2025.3606566},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distributional shortest-path graph kernels},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAQE: Complex logical query answering via semantic-aware representation learning. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3603877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing complex First-Order Logic (FOL) queries on knowledge graphs is crucial for advancing knowledge reasoning. Knowledge graphs encapsulate rich semantic interactions among entities, encompassing both explicit structural knowledge represented by triples $(e_{1}, r, e_{2})$ and implicit relational knowledge through multi-hop paths $(e_{1} \stackrel{r_{1}}{\rightarrow } \cdots e_{3} \cdots \stackrel{r_{2}}{\rightarrow } e_{2})$. Traditional models often focus solely on either triple-level or path-level knowledge, overlooking the benefits of integrating both to enhance logic query answering. This oversight leads to suboptimal representation learning and inefficient query reasoning. To overcome these challenges, we introduce a new Semantic-Aware representation learning model for Query-answering Embeddings (SAQE). Specifically, SAQE employs a joint learning approach that integrates triple-level and path-level knowledge semantics and captures both explicit and implicit contextual nuances within the knowledge graph, yielding more accurate and contextually relevant representations. To efficiently handle the large combinatorial search spaces in FOL reasoning, we propose a novel hierarchical reasoning optimization strategy by a multi-hop tree thus optimizing subqueries rooted at variable nodes in a divide-and-conquer manner. Theoretical analysis confirms that SAQE effectively supports various types of FOL reasoning and enhances generalizations for query answering. Extensive experiments demonstrate that our model achieves state-of-the-art performance across several established datasets.},
  archive      = {J_TKDE},
  author       = {Zongsheng Cao and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TKDE.2025.3603877},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SAQE: Complex logical query answering via semantic-aware representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized recommendation models in federated settings: A survey. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3606643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommender systems (FedRecSys) have emerged as a pivotal solution for privacy-aware recommendations, balancing growing demands for data security and personalized experiences. Current research efforts predominantly concentrate on adapting traditional recommendation architectures to federated environments, optimizing communication efficiency, and mitigating security vulnerabilities. However, user personalization modeling, which is essential for capturing heterogeneous preferences in this decentralized and non-IID data setting, remains underexplored. This survey addresses this gap by systematically exploring personalization in FedRecSys, charting its evolution from centralized paradigms to federated-specific innovations. We establish a foundational definition of personalization in a federated setting, emphasizing personalized models as a critical solution for capturing fine-grained user preferences. The work critically examines the technical hurdles of building personalized FedRecSys and synthesizes promising methodologies to meet these challenges. As the first consolidated study in this domain, this survey serves as both a technical reference and a catalyst for advancing personalized FedRecSys research.},
  archive      = {J_TKDE},
  author       = {Chunxu Zhang and Guodong Long and Zijian Zhang and Zhiwei Li and Honglei Zhang and Qiang Yang and Bo Yang},
  doi          = {10.1109/TKDE.2025.3606643},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Personalized recommendation models in federated settings: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COIN-GNN: Inductive spatial-temporal prediction for continuous distribution shifts via graph neural networks. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3606629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution shifts from external events and new entities can significantly compromise spatial-temporal prediction accuracy, potentially leading to severe outcomes like traffic accidents. Existing methods often fail under these conditions due to two main limitations: they focus on invariant patterns, missing the diversity required to capture the evolving dynamics of distribution shifts; they rely on often inaccessible future knowledge, such as spatial information of new entities, limiting their generalizability. To address these limitations, we formally define the problem of inductive spatial-temporal prediction under continuous distribution shifts and introduce the Contrastive Learning Based Inductive Graph Neural Network (COIN-GNN) as a solution. We develop a novel metric, Relation Importance (RI), to effectively select stable entities and distinct spatial relationships, forming an informative subgraph. Additionally, we construct an informative temporal memory buffer to store and review influential timestamps identified using influence functions. COIN-GNN then generates pseudo-observations for unstable and uninformative entities during these influential timestamps, simulating potential distribution shifts. By applying contrastive learning, the network learns stable and informative representations that can effectively counter distribution shifts without relying on future knowledge. Our extensive experiments on several real-world datasets-from traffic to weather-demonstrate COIN-GNN's superior performance across different domains without requiring future knowledge.},
  archive      = {J_TKDE},
  author       = {Jialun Zheng and Divya Saxena and Jiannong Cao},
  doi          = {10.1109/TKDE.2025.3606629},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {COIN-GNN: Inductive spatial-temporal prediction for continuous distribution shifts via graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Financial time series prediction with multi-granularity graph augmented learning. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3607005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series prediction is an important and challenging data mining task for quantitative investment. The inherent non-linearity, high noise, and susceptibility to various factors, such as macroeconomic conditions and market sentiment in the stock market, increase the difficulty of prediction. Existing financial industries mainly employ time series models or fundamental analysis methods for prediction. However, these methods fail to effectively capture the complex interrelationships between equity. In recent years, graph neural networks (GNNs), due to their powerful relational modeling capabilities, have been applied to stock prediction. However, with the advances of recent digital power, such as widely-used high-frequency trading techniques, existing graph-based methods still have shortcomings in effectively learning multi-granularity temporal relations as they cannot effectively learn the patterns in different frequencies, e.g., minute-level, daily, weekly, etc. Therefore, in this paper, we propose a multi-granularity graph augmented learning framework for interrelated financial time series forecasting. We first construct a temporal return relationship graph with multi-granularity financial time series, including weekly, daily, and minute-level, to comprehensively capture the dynamic relations of equities, including both medium-term trends and short-term fluctuations. Then, to further augment the node relations, we devise an attentional graph augment module to improve the graph learning with fundamental data, which are jointly optimized in the prediction layer. We conduct extensive empirical studies on multiple datasets from both the Chinese and U.S. stock markets. The results demonstrate that our proposed model consistently outperforms existing baseline methods across four key financial metrics, including ARR, ASR, CR, and IR, thereby validating its effectiveness and superiority. The model has been applied and empirically tested in commercial-grade trading platforms, further demonstrating its efficiency and robustness in real-world trading environments.},
  archive      = {J_TKDE},
  author       = {Peng Zhu and Yuante Li and Qinyuan Liu and Dawei Cheng and Changjun Jiang},
  doi          = {10.1109/TKDE.2025.3607005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Financial time series prediction with multi-granularity graph augmented learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge-guided pre-training temporal data analysis foundation model for urban computing. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3607026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal data analysis plays a pivotal role in applications such as weather forecasting, traffic flow management, energy consumption monitoring, and other areas of urban computing. In recent years, temporal data modeling has transitioned from traditional deep learning methods to pretrained models. However, existing approaches often exhibit significant task-specific limitations, requiring bespoke model designs and extensive domain data for training. To address these challenges, this study introduces KPT, a novel foundation model for temporal data analysis in urban computing. By leveraging temporal competitive attention and feature interaction attention mechanisms, KPT can effectively capture global context, integrate cross-variable features precisely, and achieve universal feature learning across diverse time series tasks. Additionally, the knowledge prompt network facilitates the deep fusion of crosslayer features via an intricate interaction mechanism, enabling the model to identify and align shared temporal patterns across different time series data. These patterns then transformed into knowledge prompts, thereby enhancing the universal feature learning capabilities of the pre-trained model. Experimental results demonstrate that KPT excels in four core temporal analysis tasks within urban computing, outperforming taskspecific models. This highlights KPT's ability to generalize across tasks and underscores its potential as a foundation model for multi-task scenarios in urban computing.},
  archive      = {J_TKDE},
  author       = {Haochen Shi and Shengdong Du and Yan Yang and Junbo Zhang and Tianrui Li and Yu Zheng},
  doi          = {10.1109/TKDE.2025.3607026},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A knowledge-guided pre-training temporal data analysis foundation model for urban computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing act: MDGAN for imbalanced tabular data synthesis. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3607862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the persistent challenge of learning from imbalanced datasets is crucial in advancing machine learning applications. Standard machine learning algorithms typically assume that the input data is balanced, and they often struggle to effectively learn the distribution of minority class data when dealing with imbalanced data. To address this, our study designed an improved Generative Adversarial Networks (GANs) model, named MDGAN, for tabular sample synthesis to augment samples and balance the data distribution. MDGAN employs a multi-generator and multi-discriminator structure to capture non-connected subspace manifolds, thereby better fitting the complete data distribution. To enhance the diversity among the multiple generators, an exclusive loss among generators was designed, ensuring that each generator produces data of different modalities. Additionally, a contrastive loss was introduced to ensure that the generated samples better fit the minority class distribution and are separated from the majority class distribution, preventing blurred classification boundaries. Qualitative and quantitative tests were conducted on 25 real datasets, and the experimental results indicate that MDGAN outperforms traditional classical models and current advanced oversampling models.},
  archive      = {J_TKDE},
  author       = {Hongwei Ding and Nana Huang and Qi Tao and Jiaqi Liang and Xiaohui Cui},
  doi          = {10.1109/TKDE.2025.3607862},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Balancing act: MDGAN for imbalanced tabular data synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and effective adaptation of multimodal foundation models in sequential recommendation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger text encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the MicroLens public benchmark. We will release our code and datasets to support future research.},
  archive      = {J_TKDE},
  author       = {Junchen Fu and Xuri Ge and Xin Xin and Alexandros Karatzoglou and Ioannis Arapakis and Kaiwen Zheng and Yongxin Ni and Joemon M. Jose Joemon},
  doi          = {10.1109/TKDE.2025.3608071},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and effective adaptation of multimodal foundation models in sequential recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiST: Efficient distributed spatio-temporal clustering with automatic parameter optimization. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3607744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancements in positioning technologies, the volume of spatio-temporal data has grown significantly. Analyzing the spatial and temporal characteristics of these data is imperative for uncovering underlying associations and deriving insights into natural and societal mechanisms. Clustering is a widely utilized technique for data analysis, which groups data with similar characteristics for further investigation. However, current clustering methodologies usually inadequately address temporal properties that are vital in numerous scenarios. Additionally, traditional spatio-temporal clustering approaches are constrained to standalone environments, which struggle to handle large-scale spatio-temporal datasets. To this end, we introduce DiST, the first distributed spatio-temporal clustering method, which simultaneously considers both temporal and spatial proximity. DiST comprises data partition, local clustering, and global merging stages, along with an auto-tuning framework for parameter optimization. DiST addresses key challenges, including the integration of temporal and spatial attributes, managing data duplication across distributed nodes, and selecting appropriate parameters for diverse data characteristics. Comparative experiments on two real-world datasets validate the performance and scalability of DiST, demonstrating its effectiveness in spatio-temporal data analysis. The source codes are released at https://github.com/Spatio-Temporal-Lab/stdbscan.},
  archive      = {J_TKDE},
  author       = {Jiajun Li and Shuxiang Gou and Ruiyuan Li and Huajun He and Wenhui Li and Yu Zheng},
  doi          = {10.1109/TKDE.2025.3607744},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DiST: Efficient distributed spatio-temporal clustering with automatic parameter optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated graph neural networks with equivalent hypergraph construction for traffic flow prediction. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3607895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is essential for intelligent transportation systems, yet privacy concerns and limited cross-regional data sharing hinder accurate modeling of global traffic patterns. This paper proposes a Federated Graph Neural Network with Equivalent Hypergraph (FGNNEH) framework to address these challenges by preserving privacy and enhancing cross-client collaboration. FGNNEH consists of two key stages. First, local traffic networks are transformed into high-dimensional hypernodes through an integrated process of backbone network extraction, kernel matrix analysis, and multilayer perceptrons. The backbone network extraction simplifies graph structures by isolating critical nodes and edges based on topological centrality, ensuring computational efficiency while retaining key spatial dependencies. Kernel matrix analysis captures complex nonlinear correlations among traffic flow features, including spatial-temporal dependencies and region-specific dynamics, enabling more effective feature representation. The multilayer perceptrons further fuse these features into robust hypernode embeddings that encapsulate both structural and traffic flow characteristics. Second, a global hypergraph construction mechanism is introduced to optimize inter-client collaboration. This mechanism employs an iterative performance feedback loop to dynamically add or remove edges between hypernodes, addressing the issue of lost inter-client connections and enabling effective cross-regional information exchange. Together, these components reconstruct a global traffic model that balances local privacy with holistic accuracy. Experiments on real-world traffic datasets, including PeMSD4, METR-LA and Guangzhou, demonstrate that FGNNEH outperforms existing methods in prediction accuracy, computational efficiency, and scalability.},
  archive      = {J_TKDE},
  author       = {Feng Wang and Yuhang Cao and Li Liu and Qi Kang and Jun Chen},
  doi          = {10.1109/TKDE.2025.3607895},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Federated graph neural networks with equivalent hypergraph construction for traffic flow prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SandwichSketch: A more accurate sketch for frequent object mining in data streams. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3607691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent object mining has gained considerable interest in the research community and can be split into frequent item mining and frequent set mining depending on the type of object. While existing sketch-based algorithms have made significant progress in addressing these two tasks concurrently, they also possess notable limitations. They either support only software platforms with low throughput or compromise accuracy for faster processing speed and better hardware compatibility. In this paper, we make a substantial stride towards supporting frequent object mining by designing SandwichSketch, which draws inspiration from sandwich making and proposes two techniques including the double fidelity enhancement and hierarchical hot locking to guarantee high fidelity on both two tasks. We implement SandwichSketch on three platforms (CPU, Redis, and FPGA) and show that it enhances accuracy by 38.4× and 5× for two tasks on three real-world datasets, respectively. Additionally, it supports a distributed measurement scenario with less than a 0.01% decrease in Average Relative Error (ARE) when the number of nodes increases from 1 to 16.},
  archive      = {J_TKDE},
  author       = {Zhuochen Fan and Ruixin Wang and Zihan Jiang and Ruwen Zhang and Tong Yang and Sha Wang and Yuhan Wu and Ruijie Miao and Kaicheng Yang and Bui Cui},
  doi          = {10.1109/TKDE.2025.3607691},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SandwichSketch: A more accurate sketch for frequent object mining in data streams},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KnobCF: Uncertainty-aware knob tuning. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3608030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knob tuning aims to optimize database performance by searching for the most effective knob configuration under a certain workload. Existing works suffer from two significant problems. First, there exist multiple useless evaluations of knob tuning even with diverse searching methods because of the different sensitivities of knobs on a certain workload. Second, the single evaluation of knob configurations may bring overestimation or underestimation because of query performance uncertainty. To solve the above problems, we propose a query uncertainty-aware knob classifier, called KnobCF, to enhance knob tuning. Our method has three contributions: (1) We propose uncertainty-aware configuration estimation to improve the tuning process. (2) We design a few-shot uncertainty estimator that requires no extra data collection, ensuring high efficiency in practical tasks. (3) We provide a flexible framework that can be integrated into existing knob tuners and DBMSs without modification. Our experiments on four open-source benchmarks demonstrate that our method effectively reduces useless evaluations and improves the tuning results. Especially in TPCC, our method achieves competitive tuning results with only 60% to 70% time consumption compared to the full workload evaluations.},
  archive      = {J_TKDE},
  author       = {Yu Yan and Junfang Huang and Hongzhi Wang and Jian Geng and Kaixin Zhang and Tao Yu},
  doi          = {10.1109/TKDE.2025.3608030},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {KnobCF: Uncertainty-aware knob tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey and experimental study on neural trajectory-user linking models. <em>TKDE</em>, 1-17. (<a href='https://doi.org/10.1109/TKDE.2025.3607902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of location-aware devices has boosted urban systems with massive volumes of anonymous trajectory data, presenting both challenges and opportunities for enhancing smart city initiatives through Trajectory-User Linking (TUL). Typically, TUL aims to match anonymous trajectories with specific users by exploring spatiotemporal patterns and insightful mobility behaviors. However, current TUL models face significant limitations due to their reliance on singular data sources and insufficient consideration of real-world scenarios. Furthermore, these models often lack evaluation in fair and comprehensive environments, hindering accurate assessment of their performance and applicability. This paper systematically investigates prevalent challenges encountered by existing TUL models, conducts a comprehensive review of state-of-the-art models, and proposes a structured framework that encompasses three core components: point-level representation learning, trajectory-level representation learning, and user linking. Through meticulously designed experiments, we examine the effectiveness and efficiency of leading TUL models in handling the complexities of real-world data, such as data imbalance, sparsity, new users, and scalability. This in-depth analysis uncovers limitations in existing methodologies and offers guidance for future advancements, contributing to the development of robust TUL solutions for urban mobility analysis and smart city technologies.},
  archive      = {J_TKDE},
  author       = {Hua Shi and Dan He and Fengmei Jin and Wen Hua and Jiwon Kim and Qilin Wang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2025.3607902},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey and experimental study on neural trajectory-user linking models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Document-level relation extraction with low entity redundancy feature map. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3607566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document-level relation extraction (RE) aims to determine the relations between entities scattered across different sentences through reading and reasoning. Existing methods use semantic segmentation to obtain global information among triples by analyzing entity-level matrices. However, complete document input may introduce certain interference, making it challenging to express the underlying relationships. To address this, we propose a novel approach introducing a low-entity redundancy feature map, achieved by removing certain entities. The proposed optimal path filtering (OPF) selects entity-related sentences using heuristic rules and formulates sentence selection as a set cover problem, solved via backtracking pruning. U-Net is then applied to obtain global features. Our experiment achieves state-of-the-art results on two common document-level RE datasets, Re-DocRED and CDR, outperforming previous methods. The code is available at https://github.com/dadadaray/document-RE-feature-map.},
  archive      = {J_TKDE},
  author       = {Rongen Yan and Depeng Dang and Keqin Peng and Yakun Li and Ye Tao and Lei Hou and Juanzi Li and Jie Tang},
  doi          = {10.1109/TKDE.2025.3607566},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Document-level relation extraction with low entity redundancy feature map},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating spherical K-means clustering for large-scale sparse document data. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an accelerated spherical Kmeans clustering algorithm for large-scale and high-dimensional sparse document data sets. We design an algorithm working in an architecture-friendly manner (AFM), which is a way of suppressing performance-degradation factors such as the numbers of instructions, branch mispredictions, and cache misses in CPUs of a computer system. For the AFM operation, we leverage universal characteristics (UCs) of the data, which are skewed distributions on data relationships. The UCs indicate that the most part of multiplications for similarity calculations is executed on high-document-frequency terms and the most part of a similarity is obtained by the multiplications regarding a few high mean-feature values. To extract the foregoing specific region on terms and mean-feature values, we construct a mean-inverted index partitioned into three regions by two structural parameters. Our algorithm optimizes the parameters by minimizing the approximate number of the multiplications corresponding to the instructions based on our efficient pruning method, reduces conditional branches by sharing the index structure with all the objects, and keeps in the caches the frequently used data in the foregoing specific region. We experimentally demonstrate that our algorithm efficiently achieves superior speed performance in large-scale documents compared with algorithms using the stateof-the-art techniques},
  archive      = {J_TKDE},
  author       = {Kazuo Aoyama and Kazumi Saito},
  doi          = {10.1109/TKDE.2025.3608264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Accelerating spherical K-means clustering for large-scale sparse document data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing robustness and generalization capability for multimodal recommender systems via sharpness-aware minimization. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3604242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal recommender systems utilize a variety of information types to model user preferences and item properties, aiding in the discovery of items that align with user interests. Rich multimodal information alleviates inherent challenges in recommendation systems, such as data sparsity and cold start problems. However, multimodal information further introduces challenges in terms of robustness and generalization capability. Regarding robustness, multimodal information magnifies the risks associated with information adjustment and inherent noise, posing severe challenges to the stability of recommendation models. For generalization capability, multimodal recommender systems are more complex and difficult to train, making it harder for models to handle data beyond the training set, posing significant challenges to model generalization capability. In this paper, we analyze the shortcomings of existing robustness and generalization capability enhancement strategies in the multimodal recommendation field. We propose a sharpness-aware minimization strategy focused on batch data (BSAM), which effectively enhances the robustness and generalization capability of multimodal recommender systems without requiring extensive hyper-parameter tuning. Furthermore, we introduce a mixed loss variant strategy (BSAM+), which accelerates convergence and achieves remarkable performance improvement. We provide rigorous theoretical proofs and conduct experiments with nine advanced models on five widely used datasets to validate the superiority of our strategies. Moreover, our strategies can be integrated with existing robust training and data augmentation strategies to achieve further improvement, providing a superior training paradigm for multimodal recommendations.},
  archive      = {J_TKDE},
  author       = {Jinfeng Xu and Zheyu Chen and Jinze Li and Shuo Yang and Wei Wang and Xiping Hu and Raymond Chi-Wing Wong and Edith C. H. Ngai},
  doi          = {10.1109/TKDE.2025.3604242},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing robustness and generalization capability for multimodal recommender systems via sharpness-aware minimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GOIO: Generative oversampling approach to class imbalance and overlap of tabular data. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance, which is common in real-world classification tasks, often leads to biased models favoring majority classes. Data oversampling is a widely used strategy to address this issue. However, traditional oversampling methods often generate incorrect or redundant instances when class overlap occurs, increasing decision boundary complexity. To this end, we propose a novel Generative Oversampling approach to addressing Class Imbalance and Overlap (GOIO) in the classification of tabular data. GOIO combines a Metric-Learning-based Variational Autoencoder (MLVAE) and a Conditional Latent Diffusion Model (CLDM) to handle class imbalance and overlap effectively. The MLVAE employs a triplet-center loss to the adverse effects of class overlap by transforming the data distribution into a more separable latent feature space. Following this, the CLDM is trained with class-center feature prompting and classifier-free guidance strategy to capture class-specific latent distributions accurately. Minority class samples are synthesized in the latent space using the CLDM and then reconstructed into the data space via the MLVAE decoder. Comprehensive experiments on 18 real-world and five synthetic datasets demonstrate that GOIO outperforms the state-of-the-art oversampling methods in F1-score, MCC, and Accuracy. Ablation studies further validate the effectiveness of the proposed contributions in addressing class imbalance and overlap.},
  archive      = {J_TKDE},
  author       = {Shiqi Ren and Jinliang Ding and Cuie Yang and Yiu-ming Cheung},
  doi          = {10.1109/TKDE.2025.3608246},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GOIO: Generative oversampling approach to class imbalance and overlap of tabular data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atomic fact decomposition helps attributed question answering. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrievalbased AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decompositionbased Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric Attrp for evaluating the precision of evidence attribution.},
  archive      = {J_TKDE},
  author       = {Zhichao Yan and Jiapu Wang and Jiaoyan Chen and Xiaoli Li and Jiye Liang and Ru Li and Jeff Z. Pan},
  doi          = {10.1109/TKDE.2025.3608716},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Atomic fact decomposition helps attributed question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale storage location assignment via hierarchical reinforcement learning: A rank and assign approach. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3609173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The storage location assignment problem for export containers (EC-SLAP) is crucial to the efficiency of cargo turnover in ports. Existing methods fall short in real-world applications due to the challenges of the unpredictability of container arrival sequences and the large-scale problem. We propose a new buffer-wise framework based on hierarchical reinforcement learning for EC-SLAP, aimed at optimizing container turnover efficiency. The framework comprises two processes: 1) Ranking Agent ranks containers in the buffer, reducing the uncertainty of random arrival sequences compared to the immediate assignment. 2) Assigning Agents assign storage locations in a two-step process by block and slot, diminishing the dimensionality of the large-scale discrete action space. We iteratively optimize agents by asynchronously obtaining rewards from the environment. In addition, to address the challenge of sparse rewards in long-sequence decision-making, we have developed a novel immediate reward function to enhance learning efficiency and accelerate convergence. We propose a new large-scale dataset, NZP-SLAD, collected from real-world historical data from the terminal operating system of Ningbo-Zhoushan Port and develop a realistic container terminal simulator. We conducted numerous offline simulations and tests with this dataset. The experimental results demonstrate that our proposed method achieves rapid convergence and significantly surpasses expert methods used in real-world production.},
  archive      = {J_TKDE},
  author       = {Weihang Pan and Minghao Chen and Binbin Lin and Yafei Wang and Xinkui Zhao and Xiaofei He and Jieping Ye},
  doi          = {10.1109/TKDE.2025.3609173},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large-scale storage location assignment via hierarchical reinforcement learning: A rank and assign approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective and transferable detection for multi-modal fake news in the social media stream. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3609045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of multimedia fake news on social media has raised significant concerns in recent years. Existing studies on fake news detection predominantly adopt an instance-based paradigm, where the detector evaluates a single post to determine its veracity. Despite notable advancements achieved in this domain, we argue that the instance-based approach is misaligned with real-world deployment scenarios. In practice, detectors typically operate on servers that process incoming posts in temporal order, striving to assess their authenticity promptly. Instance-based detectors lack awareness of temporal information and contextual relationships between surrounding posts, therefore fail to capture long-range dependencies from the timeline. To bridge this gap, we introduce a more practical stream-based multi-modal fake news detection paradigm, which assumes that social media posts arrive continuously over time and allows the utilization of previously seen posts to aid in the classification of incoming ones. To enable effective and transferable fake news detection under this novel paradigm, we propose maintaining historical knowledge as a collection of incremental high-level forgery patterns. Based on this principle, we design a novel framework called Incremental Forgery Pattern Learning and Clues Refinement (IPLCR). IPLCR incrementally learns high-level forgery patterns as the stream evolves, leveraging this knowledge to improve the detection of newly arrived posts. At the core of IPLCR is the Incremental Forgery Pattern Bank (IPB), which dynamically summarizes historical posts into a set of latent forgery patterns. IPB is designed to continuously incorporate timely knowledge and actively discard obsolete information, even during inference. When a new post arrives, IPLCR retrieves the most relevant forgery pattern knowledge from IPB and refines the clues for fake news detection. The refined clues are subsequently incorporated into IPB to enrich its knowledge base. Extensive experiments validate IPLCR's effectiveness as a robust stream-based detector. Moreover, IPLCR addresses several critical issues relevant to industrial applications, including seamless context transfer and efficient model upgrading, making it a practical solution for realworld deployment.},
  archive      = {J_TKDE},
  author       = {Jingyi Xie and Jiawei Liu and Zheng-Jun Zha},
  doi          = {10.1109/TKDE.2025.3609045},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards effective and transferable detection for multi-modal fake news in the social media stream},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertain priors for graphical causal models: A multi-objective optimization perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning graphical causal models from observational data can effectively elucidate the underlying causal mechanism behind the variables. In the context of limited datasets, modelers often incorporate prior knowledge, which is assumed to be correct, as a penalty in single-objective optimization. However, this approach struggles to adapt complex and uncertain priors effectively. This paper introduces UpCM, which tackles the issue from a multi-objective optimization perspective. Instead of focusing exclusively on the DAG as the optimization goal, UpCM methodically evaluate the effect of uncertain priors on specific structures, merging data-driven and knowledge-driven objectives. Utilizing the MOEA/D framework, it achieve a balanced tradeoff between these objectives. Furthermore, since uncertain priors may introduce erroneous constraints, resulting in PDAGs lacking consistent extensions, the minimal non-consistent extension is explored. This extension, which separately incorporates positive and negative constraints, aims to approximate the true causality of the PDAGs. Experimental results demonstrate that UpCM achieves significant structural accuracy improvements compared to baseline methods. It reduces the SHD by 7.94%, 13.23%, and 12.8% relative to PC stable, GES, and MAHC, respectively, when incorporating uncertain priors. In downstream inference tasks, UpCM outperforms domain-expert knowledge graphs, owing to its ability to learn explainable causal relationships that balance data-driven evidence with prior knowledge},
  archive      = {J_TKDE},
  author       = {Zidong Wang and Xiaoguang Gao and Qingfu Zhang},
  doi          = {10.1109/TKDE.2025.3608723},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Uncertain priors for graphical causal models: A multi-objective optimization perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible keyword-aware top-k route search. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3609302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-k best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-k Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.},
  archive      = {J_TKDE},
  author       = {Ziqiang Yu and Xiaohui Yu and Yueting Chen and Wei Liu and Anbang Song and Bolong Zheng},
  doi          = {10.1109/TKDE.2025.3609302},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Flexible keyword-aware top-k route search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling temporal dependencies within the target for long-term time series forecasting. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3609415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term time series forecasting (LTSF) is a critical task across diverse domains. Despite significant advancements in LTSF research, we identify a performance bottleneck in existing LTSF methods caused by the inadequate modeling of Temporal Dependencies within the Target (TDT). To address this issue, we propose a novel and generic temporal modeling framework, Temporal Dependency Alignment (TDAlign), that equips existing LTSF methods with TDT learning capabilities. TDAlign introduces two key innovations: 1) a loss function that aligns the change values between adjacent time steps in the predictions with those in the target, ensuring consistency with variation patterns, and 2) an adaptive loss balancing strategy that seamlessly integrates the new loss function with existing LTSF methods without introducing additional learnable parameters. As a plug-and-play framework, TDAlign enhances existing methods with minimal computational overhead, featuring only linear time complexity and constant space complexity relative to the prediction length. Extensive experiments on six strong LTSF baselines across seven real-world datasets demonstrate the effectiveness and flexibility of TDAlign. On average, TDAlign reduces baseline prediction errors by 1.47% to 9.19% and change value errors by 4.57% to 15.78%, highlighting its substantial performance improvements.},
  archive      = {J_TKDE},
  author       = {Qi Xiong and Kai Tang and Minbo Ma and Ji Zhang and Jie Xu and Tianrui Li},
  doi          = {10.1109/TKDE.2025.3609415},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling temporal dependencies within the target for long-term time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-generation database interfaces: A survey of LLM-based text-to-SQL. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating accurate SQL from users' natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restrict the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summary and discuss the remaining challenges in this field and suggest expectations for future research directions.},
  archive      = {J_TKDE},
  author       = {Zijin Hong and Zheng Yuan and Qinggang Zhang and Hao Chen and Junnan Dong and Feiran Huang and Xiao Huang},
  doi          = {10.1109/TKDE.2025.3609486},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Next-generation database interfaces: A survey of LLM-based text-to-SQL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A region-aware dual latent state mining framework for service recommendation in large-scale service networks. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3609553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale service networks, Quality of Service (QoS) data is vital for tasks such as resource provisioning, real-time service recommendation, and user experience optimization; yet effectively predicting QoS often faces two key obstacles: data sparsity and highly imbalanced distributions (where most response times cluster near small values while a minority grow disproportionately large). Existing approaches typically rely on individual object (user/service) features, overlooking the phenomenon that users or services within the same region (physical or virtual) exhibit similar network states. This paper presents a Region-Aware Dual-Latent State Learning (R2SL) framework that tackles these challenges by explicitly modeling regional network latent states. Specifically, we propose to learn physical-region (city-level) and virtual-region (AS-level) latent states from historical QoS records through a joint EM-gradient descent strategy, thereby alleviating data sparsity. Furthermore, to mitigate label imbalance in QoS data, we introduce a Smooth Huber (S-Huber) loss function that appropriately reweights extreme errors, preventing the training process from being dominated by outliers. We also develop a sparsely activated mixture-of-experts module, dynamically routing regional latent features based on each prediction task's context. Experiments on real-world QoS datasets show that R2SL substantially outperforms state-of-the-art baselines, including the newly introduced FRLN. On throughput tasks, R2SL reduces MAE by an average of 18.8% and RMSE by 12.9%, while on response time tasks, it achieves 26.4% lower MAE and 24.2% lower RMSE. These findings indicate that dual-latent state modeling, combined with a distribution-aware loss, effectively captures complex regional patterns and mitigates long-tail label effects, making R2SL a powerful and scalable framework for large-scale QoS data mining in service networks.},
  archive      = {J_TKDE},
  author       = {Ziliang Wang and Xiaohong Zhang and Ze Shi Li and Meng Yan},
  doi          = {10.1109/TKDE.2025.3609553},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A region-aware dual latent state mining framework for service recommendation in large-scale service networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRIME: A phishing detection framework with quantitative and fuzzy-based dual validation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3609320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phishing attacks continue to pose a significant cybersecurity threat, especially as social engineering (SE) tactics become more contextually embedded and difficult to detect. To address the limitations of traditional rule-based framework and AI-driven classifiers, we propose PRIME, a phishing email evaluation framework that leverages large language models (LLMs) to assess manipulative intent across six interpretable criteria. Three risk scoring strategies, namely equal weighting, semantically weighted scoring, and fuzzy logic-based classification, are applied to aggregate the criterion scores into multi-level risk assessments. Qualitative comparisons with established frameworks demonstrate PRIME's broad coverage and conceptual soundness. Quantitative experiments validate its effectiveness, with the fuzzy-based method achieving perfect recall on a phishing-only dataset and consistent performance across multiple years. An ablation study, where each criterion is removed in turn, highlights the critical role of the Context and Content dimension in detecting both explicit and subtle SE cues. By separating LLMs interpretation from final decision-making, PRIME enhances transparency, robustness, and adaptability in phishing detection systems.},
  archive      = {J_TKDE},
  author       = {Yicun Tian and Youyang Qu and Ming Ding and Shigang Liu and Pei-Wei Tsai and Jun Zhang},
  doi          = {10.1109/TKDE.2025.3609320},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PRIME: A phishing detection framework with quantitative and fuzzy-based dual validation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Similarity and dissimilarity guided co-association matrix construction for ensemble clustering. <em>TKDE</em>, 1-23. (<a href='https://doi.org/10.1109/TKDE.2025.3608721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering aggregates multiple weak clusterings to achieve a more accurate and robust consensus result. The Co-Association matrix (CA matrix) based method is the mainstream ensemble clustering approach that constructs the similarity relationships between sample pairs according the weak clustering partitions to generate the final clustering result. However, the existing methods neglect that the quality of cluster is related to its size, i.e., a cluster with smaller size tends to higher accuracy. Moreover, they also do not consider the valuable dissimilarity information in the base clusterings which can reflect the varying importance of sample pairs that are completely disconnected. To this end, we propose the Similarity and Dissimilarity Guided Co-association matrix (SDGCA) to achieve ensemble clustering. First, we introduce normalized ensemble entropy to estimate the quality of each cluster, and construct a similarity matrix based on this estimation. Then, we employ the random walk to explore high-order proximity of base clusterings to construct a dissimilarity matrix. Finally, the adversarial relationship between the similarity matrix and the dissimilarity matrix is utilized to construct a promoted CA matrix for ensemble clustering. We compared our method with 13 state-of-the-art methods across 12 datasets, and the results demonstrated the superior clustering ability and robustness of the proposed approach. The code is available at https://github.com/xuz2019/SDGCA.},
  archive      = {J_TKDE},
  author       = {Xu Zhang and Yuheng Jia and Mofei Song and Ran Wang},
  doi          = {10.1109/TKDE.2025.3608721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-23},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Similarity and dissimilarity guided co-association matrix construction for ensemble clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CryptIF: Towards cloud-based IoT anomaly detection over encrypted feature streams. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3609407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation and extensive use of the Internet of Things (IoT), it is vital to ensure the secure operation of IoT devices. However, due to limited computing power and lightweight system design, IoT devices often remain unprotected and are vulnerable to a myriad of attacks. Directly outsourcing the computationally intensive anomaly detection work to some middleboxes or cloud servers seems to solve this problem, but it can raise severe privacy concerns. To tackle these problems, we propose CryptIF, a scalable, privacy-preserving approach to detecting IoT anomalies from the cloud. By leveraging the Extended isolation forest (EIForest) and ciphertext comparison algorithms, CryptIF inspects features encrypted by fully homomorphic encryption (FHE) to detect various IoT anomalies. Furthermore, CryptIF parallelizes computing tasks by taking advantage of the single instruction, multiple data (SIMD) property of Cheon, Kim, Kim and Song (CKKS) homomorphic encryption to accelerate the detection process, thereby significantly increasing its scalability and operating efficiency. The evaluations demonstrate that CryptIF outperforms the state-of-the-art ciphertext-based anomaly detection approach in both detection accuracy and time efficiency. Additionally, CryptIF achieves comparable detection performance to plaintext-based IForest algorithms.},
  archive      = {J_TKDE},
  author       = {Teng Li and Zejian Lin and Yebo Feng and Chong Wang and Zhuo Ma and Bin Xiao and Jianfeng Ma and Yang Liu},
  doi          = {10.1109/TKDE.2025.3609407},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CryptIF: Towards cloud-based IoT anomaly detection over encrypted feature streams},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orthogonal keys high precision and recall for mining database keys from inconsistent and incomplete relations. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3608680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of orthogonality between column sets as a means to eliminate accidental keys during data driven key discovery. This becomes particularly important for inconsistent relations, where keys that almost hold need to be considered as well. Here we employ the classic $g_{3}$ metric for dirtiness, which can be applied to a variety of key semantics for incomplete relations, although the difficulty in computing the metric under different semantics varies greatly. Efficient algorithms for orthogonal mining and measuring dirtiness are proposed and evaluated on a real-world database. Additionally, we propose the notion of partial key, which turns out to be particularly useful for dealing with incomplete data in a key discovery context.},
  archive      = {J_TKDE},
  author       = {Henning Koehler and Sebastian Link},
  doi          = {10.1109/TKDE.2025.3608680},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Orthogonal keys high precision and recall for mining database keys from inconsistent and incomplete relations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive information fusion-based concept drift learning for evolving multiple data streams. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3610184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift arises from unpredictable data distribution shifts, degrading model performance. In evolving multiple data streams, these drifts pose greater challenges due to dynamic changes and uncertain inter-stream correlations, demanding robust accuracy and generalization. To address this issue, in this article, we propose a novel multiple data stream learning method, called the adaptive information fusion-based concept drift learning (AIF-CD) method, to adaptively handle multiple data streams with heterogeneous feature spaces and complex drift situations. First, a real-time learning method with a cooperation scheme is proposed to handle multiple data streams. Second, an information fusion-based augmentation process is designed to help enhance the learning efficiency of each stream. Next, a drift severity identification-based adaptation strategy and a process to selectively use the previous timestamps' data are introduced to enhance learning robustness in both synchronous and asynchronous scenarios. Moreover, a detailed runtime complexity and theoretical analysis further explains the learning efficiency of our method. Our key innovation combines real-time adaptation with theoretical guarantees for complex, evolving multi-stream learning. The experiment results in various scenarios under synchronous and asynchronous settings show that the proposed method is more efficient than other benchmark methods.},
  archive      = {J_TKDE},
  author       = {Kun Wang and Jie Lu and Anjin Liu},
  doi          = {10.1109/TKDE.2025.3610184},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive information fusion-based concept drift learning for evolving multiple data streams},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering explainable artificial intelligence through case-based reasoning: A comprehensive exploration. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) advancements have significantly broadened its application across various sectors, simultaneously elevating concerns regarding the transparency and understandability of AI-driven decisions. Addressing these concerns, this paper embarks on an exploratory journey into Case-Based Reasoning (CBR) and Explainable Artificial Intelligence (XAI), critically examining their convergence and the potential this synergy holds for demystifying the decision-making processes of AI systems. We employ the concept of Explainable CBR (XCBR) system that leverages CBR to acquire case-based explanations or generate explanations using CBR methodologies to enhance AI decision explainability. Though the literature has few surveys on XCBR, recognizing its potential necessitates a detailed exploration of the principles for developing effective XCBR systems. We present a cycle-aligned perspective that examines how explainability functions can be embedded throughout the classical CBR phases: Retrieve, Reuse, Revise, and Retain. Drawing from a comprehensive literature review, we propose a set of six functional goals that reflect key explainability needs. These goals are mapped to six thematic categories, forming the basis of a structured XCBR taxonomy. The discussion extends to the broader challenges and prospects facing the CBR-XAI arena, setting the stage for future research directions. This paper offers design guidance and conceptual grounding for future XCBR research and system development.},
  archive      = {J_TKDE},
  author       = {Preeja Pradeep and Marta Caro-Martínez and Anjana Wijekoon},
  doi          = {10.1109/TKDE.2025.3609825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Empowering explainable artificial intelligence through case-based reasoning: A comprehensive exploration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEFS: Adaptive early feature selection for deep recommender systems. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3610351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of features plays an important role in the performance of recommender systems. Recognizing this, feature selection has emerged as a crucial technique in refining recommender systems. Recent advancements leveraging Automated Machine Learning (AutoML) has drawn significant attention, particularly in two main categories: early feature selection and late feature selection, differentiated by whether the selection occurs before or after the embedding layer. The early feature selection selects a fixed subset of features and retrains the model, while the late feature selection, known as adaptive feature selection, dynamically adjusts feature choices for each data instance, recognizing the variability in feature significance. Although adaptive feature selection has shown remarkable improvements in performance, its main drawback lies in its post-embedding layer feature selection. This process often becomes cumbersome and inefficient in large-scale recommender systems with billions of ID-type features, leading to a highly sparse and parameter-heavy embedding layer. To overcome this, we introduce Adaptive Early Feature Selection (AEFS), a very simple method that not only adaptively selects informative features for each instance, but also significantly reduces the activated parameters of the embedding layer. AEFS employs a dual-model architecture, encompassing an auxiliary model dedicated to feature selection and a main model responsible for prediction. To ensure effective alignment between these two models, we incorporate two collaborative training loss constraints. Our extensive experiments on three benchmark datasets validate the efficiency and effectiveness of our approach. Notably, AEFS matches the performance of current state-of-theart Adaptive Late Feature Selection methods while achieving a significant reduction of 37. 5% in the activated parameters of the embedding layer. We believe that this work opens up new possibilities for feature selection.},
  archive      = {J_TKDE},
  author       = {Fan Hu and Gaofeng Lu and Jun Chen and Channan Guo and Yuekui Yang and Xirong Li},
  doi          = {10.1109/TKDE.2025.3610351},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AEFS: Adaptive early feature selection for deep recommender systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDiffMAE: Guided diffusion enhanced mask graph AutoEncoder for recommendation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3611270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advancements using graph neural networks (GNNs) to capture complex user-item interactions, challenges persist due to data sparsity and noise. To address these, self-supervised learning (SSL) methods, particularly recent generative approaches, have gained attention due to their ability to augment graph data without requiring complex view constructions and unstable negative sampling. However, existing generative SSL solutions often focus on structural rather than semantic (refer to collaborative signals in recommendation scenarios) reconstruction, limiting their potential as comprehensive recommender. This paper explores the untapped potential of generative SSL for graph-based recommender systems. We highlight two critical challenges: firstly, designing effective diffusion mechanisms to enhance semantic information and collaborative signals while avoiding optimization biases; and secondly, developing adaptive structural masking mechanisms within graph diffusion to improve overall model performance. Motivated by these challenges, we propose a novel approach: the Guided Diffusion enhanced Mask graph AutoEncoder (GDiffMAE). GDiffMAE integrates an adaptive mask encoder for structural reconstruction and a guided diffusion model for semantic reconstruction, addressing the limitations of current methods. Experimental results on diverse datasets demonstrate that GDiffMAE consistently outperforms powerful baseline models, particularly in handling noisy data scenarios. By enhancing both structural and semantic dimensions through guided diffusion, our model advances the state-of-the-art in graph-based recommender systems.},
  archive      = {J_TKDE},
  author       = {Lei Zhang and Zihao Chen and Wuji Zhang and Hongke Zhao and Likang Wu},
  doi          = {10.1109/TKDE.2025.3611270},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GDiffMAE: Guided diffusion enhanced mask graph AutoEncoder for recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical data collection under input-discriminative local differential privacy. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input-discriminative local differential privacy (ID-LDP) protects user data with a different range of values, which improves the utility of the estimated data compared to traditional LDP. However, the existing ID-LDP methods are used for categorical data and cannot be directly applied to numerical data. In this paper, we propose a numerical data collection (NDC) framework with ID-LDP to provide discriminative protection for the data with different inputs. This framework uses a piecewise mechanism to divide the numerical data into several segments and designs two perturbation methods to minimize the mean value of numerical data based on values submitted by users. We first create an NDC-UE method that encodes the raw data into a binary vector. This method sets the uploaded data bit as 1 and the rest as zero and perturbs each bit with a given probability. We further propose an NDC-GRR algorithm to perturb the numerical data with an optimal privacy budget. To reduce the complexity of NDC-GRR, we apply a greedy algorithm-based spanner to shorten the computation time and improve the accuracy. Theoretical analysis proves that our schemes satisfy the definition of ID-LDP. Experimental results based on two real-world datasets and a synthetic dataset show that the proposed schemes have less mean square error compared with the benchmarks.},
  archive      = {J_TKDE},
  author       = {Youwen Zhu and Shibo Dai and Pengfei Zhang and Xiqi Kuang},
  doi          = {10.1109/TKDE.2025.3610932},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Numerical data collection under input-discriminative local differential privacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective explanation framework for graph neural networks. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3611170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) hold promise in various application domains, but their limited explainability hinders widespread adoption, impacting customer satisfaction and loyalty. This issue intensifies when addressing diverse explanation needs of different user groups. Current GNN explanation models focus on a single objective, neglecting varied and potential conflicting user requirements, resulting in suboptimal outcomes. Moreover, existing models prioritize explanation objectives during multi-objective explanations, disrupting the intrinsic hierarchical structures and distant relationships within the graphs, further diminishing their effectiveness. To tackle these challenges, this paper introduces a novel multi-objective explanatory framework with hierarchical structure attribution for GNNs, termed HM-Explainer. This framework constructs a multi-objective explanation generation module based on Pareto theory to balance different and potentially conflicting explanatory objectives. Additionally, to embed hierarchical information into explanations, HM-Explainer designs node-level and cluster-level attribution modules to analyze the impact of input data on GNN decisions hierarchically. Furthermore, a self-attention mechanism is integrated into the node-level attribution module to account for the influence of distant neighbors. Ultimately, the efficacy of HM-Explainer is validated across multiple datasets for different GNN models through experimentation.},
  archive      = {J_TKDE},
  author       = {Yibowen Zhao and Yonghui Xu and Di Wang and Yixin Zhang and Qingzhong Li and Lizhen Cui},
  doi          = {10.1109/TKDE.2025.3611170},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A multi-objective explanation framework for graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSC-DOLES: Multi-view subspace clustering in diverse orthogonal latent embedding spaces. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3610659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of Multi-view Subspace Clustering (MSC) in Latent Embedding Space (LES), existing methods aim to capture and leverage critical multi-view information by mapping it into a low-dimensional LES. However, several aspects can be further improved: (i) Fusion Strategy: Existing methods adopt either early fusion or late fusion to integrate multi-view information, limiting the effectiveness of the fusion. (ii) Diversity: Current methods often overlook the inherent diversity in the multi-view data by focusing on a single LES. (iii) Efficiency: LES-based methods exhibit high computational complexity, with cubic time and quadratic space requirements based on the number of samples. To address these issues, we propose a novel framework called MSC-DOLES (Multi-view Subspace Clustering in Diverse Orthogonal Latent Embedding Spaces), a novel framework designed to tackle these challenges. MSC-DOLES incorporates a two-stage fusion approach that generates and learns from multiple LES to maximize cross-view diversity. Orthogonality constraints on individual LES ensure view-internal diversity, resulting in a set of Diverse Orthogonal Latent Embedding Spaces (DOLES). The DOLES are then fused into a consensus anchor graph using learnable anchors. The final clustering is induced by partitioning the obtained graph without pre-processing. We develop an eight-step optimization algorithm for MSC-DOLES, which exhibits nearly linear time and space complexities relative to the number of samples. Extensive experiments demonstrate the superiority of MSC-DOLES over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yuan Fang and Geping Yang and Ruichu Cai and Yiyang Yang and Zhiguo Gong and Can Chen and Zhifeng Hao},
  doi          = {10.1109/TKDE.2025.3610659},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MSC-DOLES: Multi-view subspace clustering in diverse orthogonal latent embedding spaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive and dual adversarial representation learning for multi-view clustering. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3611368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-View Clustering (MVC) has gained increasing attention due to its ability to effectively leverage the complementary information of multi-view data. Despite the success of existing MVC methods in many real-world applications, they often overlook the discrepancy of view-specific latent distribution and struggle to ensure the completeness of the multi-view data. To address these challenges and harness the powerful feature extraction capability of deep networks, we propose a novel Contrastive and Dual Adversarial Representation Learning method for Multi-view Clustering, termed as CDARL, to solve multi-view clustering problems with both complete and incomplete multi-view data. Specifically, CDARL employs alternating adversarial and contrastive learning to align the view-specific representations, driving them into the same semantic latent space to minimize the discrepancy in view-specific distributions. In addition, a consensus latent representation is learned by an adaptive fusion block that integrates information from multiple views. The consensus representation is further refined through adversarial learning modeling the transformation of the standard Gaussian distribution to the original data distribution. Moreover, the proposed method incorporates an imputation strategy designed to handle the incomplete multi-view data clustering task. This strategy utilizes both reconstructed samples and cross-view neighbors to impute missing views from the latent space and the original space, thereby preserving clustering information, which ensures the quality and feasibility of the imputed samples. Experimental results on six widely used datasets have verified the competitiveness of the proposed CDARL method against state-of-the-art methods in MVC problems with complete and incomplete multi-view data. Code is available at https://github.com/xywy220/CDARL-MVC.},
  archive      = {J_TKDE},
  author       = {Yanwanyu Xi and Chang Tang and Jun-Jie Huang and Xingchen Hu and Yuanyuan Liu and Xinwang Liu},
  doi          = {10.1109/TKDE.2025.3611368},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive and dual adversarial representation learning for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversifying graph augmentation for learning to solve graph optimization problems. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3611663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many machine learning-based approaches that effectively solve graph optimization problems have been proposed. The graph optimization problem is the problem that aims to optimize (maximize or minimize) a quantity that is associated with a graph, such as the Minimum Vertex Cover (MVC) and Maximum Independent Set (MIS) problems. These approaches are usually trained on graphs randomly generated with graph generators or sampled from existing datasets. However, we observe that such training graphs lead to poor testing performance if the testing graphs are not generated analogously, i.e., the generalizability of the models trained on those randomly generated training graphs is very limited. To address this critical issue, in this paper, we propose a new framework, named Learning with Iterative Graph Diversification (LIGD), and formulate a new research problem, named Diverse Graph Modification Problem (DGMP), that iteratively generate diversified training graphs and train the models that solve graph optimization problems to improve their performance significantly. We propose three approaches to solve DGMP by considering both the performance of the machine learning approaches and the structural properties of the training graphs. In addition, we study a practical case of DGMP, named Diverse Graph Modification Problem with XOR Diversity (DGMP-XDiv), which considers an XOR-based diversity function. We propose a polynomial-time algorithm named Structure Diversifying Modification on Edge Score (DMES) to obtain the optimal solution. We also propose DMES with Efficiency-Boosting Strategies (DMES-EB) to enhance the efficiency of DMES significantly. Experimental results on well-known problems show that our proposed approaches significantly boost the performance of both supervised and reinforcement learning approaches. They produce near-optimal results and significantly outperform the baseline approaches, such as graph augmentation and diffusion-based approaches.},
  archive      = {J_TKDE},
  author       = {Bay-Yuan Hsu and Chen-Hsu Yang and Chia-Hsun Lu and Ming-Yi Chang and Lo-Yao Yeh and Chih-Ya Shen},
  doi          = {10.1109/TKDE.2025.3611663},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diversifying graph augmentation for learning to solve graph optimization problems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiTruss community search for multilayer graphs. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community search on multilayer graphs has significant applications in fields such as bioinformatics, social network analysis, and financial fraud detection, offering deeper insights compared to traditional community search on single-layer graphs. However, existing approaches often suffer from several key limitations, including inefficiency and a lack of flexibility in accommodating query requirements. To address these challenges, we investigate the problem of community search over large multilayer graphs. Specifically, we introduce a novel multilayer community model called PivotTruss Community (PiTC) with provably nice structural guarantees. We formalize the PiTC search (PiTCS) problem, which aims to efficiently identify personalized PiTCs for a given query vertex. To solve the PiTCS problem, we propose an efficient algorithm and design an elegant index to accelerate the search process. In addition, we propose a parameter recommendation method to improve the usability of PiTCS. To further optimize performance, we introduce a method to compact the index by making a trade-off between search time and index size. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Run-An Wang and Zhaonian Zou and Dandan Liu and Xudong Liu},
  doi          = {10.1109/TKDE.2025.3610998},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PiTruss community search for multilayer graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term urban flow prediction against data distribution shift: A causal perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3612033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for more precise and timely urban resource allocation and management has driven the extension of urban flow prediction from short-term to long-term horizons. As the time scale expands, the issue of urban flow distribution shift becomes increasingly prominent due to various impact factors, such as weather, events, city changes, etc. Traditionally, comprehensively analyzing and addressing the causal relationships underlying the distribution shift caused by these factors has been challenging. In this paper, we propose that these impact factors can be partitioned in two major types, i.e., context factors and structural factors. We then present a decomposition-based model for long-term urban flow prediction from a causal perspective, named DeCau, which can discriminate between the two types of factors for effectively solving the problem of urban flow distribution shift. First, we employ a decomposition module to decompose urban flow into seasonal part and trend part. The seasonal part contains high frequency irregular variations caused by context factors. We advise a shared distribution estimator to approximate the unavailable prior distributions of context factors, and then apply causal intervention to mitigate the confounding impact of context factors. The distribution shift in the trend part is induced by structural factors. We design a dual causal dependency extractor to model the causality between POIs distribution and urban flow, and then eliminate spurious correlations through causal adjustment. Finally, we design an end-to-end framework for long-term urban flow prediction by combining the embeddings from two parts, enabling the model to generalize to unseen distribution. Extensive experimental results demonstrate DeCau outperforms state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yuting Liu and Qiang Zhou and Hanzhe Li and Fuzhen Zhuang and Jingjing Gu},
  doi          = {10.1109/TKDE.2025.3612033},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Long-term urban flow prediction against data distribution shift: A causal perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural clustering for bipartite graphs. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3612290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graphs are widely used in many real-world applications, where discovering clusters is crucial for understanding their underlying structure. However, most existing clustering methods for bipartite graphs enforce the assignment of all vertices to clusters, often neglecting the important roles of outliers and hubs. To address this limitation, we plan to extend the structural clustering model from unipartite to bipartite graphs. This extension is non-trivial due to the lack of common neighbors in bipartite graphs, which renders traditional similarity measures less effective. Recognizing that similarity is key to structural clustering, we resort to butterflies—the fundamental building blocks of bipartite graphs—to define a more effective similarity measure. Building on this, we further propose a novel structural clustering model, SBC, tailored for bipartite graphs. To enable clustering under this model, we develop efficient online and index-based methods, along with a dynamic maintenance method to accommodate graph updates over time. Extensive experiments on real-world bipartite graphs demonstrate that: (1) The SBC model greatly enhances clustering quality, achieving higher modularity while effectively identifying outliers and hubs. (2) Our proposed clustering methods are highly scalable, enabling the processing of graphs with up to 12.2 million edges within 2 seconds},
  archive      = {J_TKDE},
  author       = {Mingyu Yang and Wentao Li and Wei Wang and Dong Wen and Min Gao and Lu Qin},
  doi          = {10.1109/TKDE.2025.3612290},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structural clustering for bipartite graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards balanced denoising: Building a structural and textual denoiser for table understanding. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3612217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large language models (LLMs) have made remarkable progress in table understanding, yet they remain vulnerable to the structural noise (SN) and the textual noise (TN). Existing methods usually employ biased denoising strategies such as structural matching and textual filtering, or overzealous denoising strategies such as introducing supplementary tasks like text-to-SQL and table-to-text to reduce these two types of noise. However, these methods either neglect one type of noise or introduce substantial external noise. Therefore, how to simultaneously mitigate the structural and textual noise without introducing extra noise and improve the performance of LLMs in table understanding is still an unresolved issue. In this paper, we rethink the bottlenecks in table understanding from the perspective of noise reduction and propose a novel dual-denoiser-reasoner model, called TabDDR, for balanced and effective denoising. Specially, our model consists of a structural-and-textual denoiser and a task-adaptive reasoner. The former removes two types of noise via triplet alignment and planning extraction to seek an interpretable balance between breaking structural barriers and preserving structural characteristics, eliminating textual noise and retaining maximal information; the latter ensures a simple but effective reasoning process which can adapt to various downstream tasks. To highlight the presence and impact of the structural and textual noise, we construct the WTQ-SN and WTQ-TN datasets based on the WikiTableQuestion (WTQ) dataset. Extensive experiments on these self-constructed datasets and two other public datasets demonstrate that our proposed method performs better than state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Shu-Xun Yang and Xian-Ling Mao and Yu-Ming Shang and Heyan Huang},
  doi          = {10.1109/TKDE.2025.3612217},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards balanced denoising: Building a structural and textual denoiser for table understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RexUniNLU: Recursive method with explicit schema instructor for universal natural language understanding. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3595143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information Extraction (IE) and Text Classification (CLS) serve as the fundamental pillars of NLU, with both disciplines relying on analyzing input sequences to categorize outputs into pre-established schemas. However, there is no existing encoder-based model that can unify IE and CLS tasks from this perspective. To fully explore the foundation shared within NLU tasks, we have proposed a recursive method with explicit schema instructor for universal NLU. Specifically, we firstly redefine the true universal information extraction (UIE) with a formal formulation that covers almost all extraction schemas, including quadruples and quintuples which remain unsolved for previous UIE models. Then, we expands the formulation to all CLS and multi-modal NLU tasks. Based on that, we introduce RexUniNLU, an universal NLU solution that employs explicit schema constraints for IE and CLS, which encompasses all IE and CLS tasks and prevent incorrect connections between schema and input sequence. To avoid interference between different schemas, we reset the position ids and attention mask matrices. Extensive experiments are conducted on IE, CLS in both English and Chinese, and multi-modality, revealing the effectiveness and superiority. Our codes are publicly released at https://modelscope.cn/models/iic/nlp_deberta_rex-uninlu_chinese-base.},
  archive      = {J_TKDE},
  author       = {Chengyuan Liu and Shihang Wang and Yangyang Kang and Fubang Zhao and Kun Kuang and Weiming Lu and Changlong Sun and Fei Wu},
  doi          = {10.1109/TKDE.2025.3595143},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RexUniNLU: Recursive method with explicit schema instructor for universal natural language understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is precise recovery necessary? a task-oriented imputation approach for time series forecasting on variable subset. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3594032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable Subset Forecasting (VSF) refers to a unique scenario in multivariate time series forecasting, where available variables in the inference phase are only a subset of the variables in the training phase. VSF presents significant challenges as the entire time series may be missing, and neither inter- nor intra-variable correlations persist. Such conditions impede the effectiveness of traditional imputation methods, primarily focusing on filling in individual missing data points. Inspired by the principle of feature engineering that not all variables contribute positively to forecasting, we propose Task-Oriented Imputation for VSF (TOI-VSF), a novel framework shifts the focus from accurate data recovery to directly support the downstream forecasting task. TOI-VSF incorporates a self-supervised imputation module, agnostic to the forecasting model, designed to fill in missing variables while preserving the vital characteristics and temporal patterns of time series data. Additionally, we implement a joint learning strategy for imputation and forecasting, ensuring that the imputation process is directly aligned with and beneficial to the forecasting objective. Extensive experiments across four datasets demonstrate the superiority of TOI-VSF, outperforming baseline methods by 15% on average.},
  archive      = {J_TKDE},
  author       = {Qi Hao and Runchang Liang and Yue Gao and Hao Dong and Wei Fan and Lu Jiang and Pengyang Wang},
  doi          = {10.1109/TKDE.2025.3594032},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Is precise recovery necessary? a task-oriented imputation approach for time series forecasting on variable subset},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authenticated subgraph matching on large-scale graphs in hybrid-storage blockchains. <em>TKDE</em>, 1-18. (<a href='https://doi.org/10.1109/TKDE.2025.3598850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs serve as an essential data structure to model complex relationships in a variety of applications, such as social networks, web graphs, and chemical informatics. Due to the high cost of maintaining large-scale graph data and executing graph queries, data owners often outsource their graph data to a third-party service provider for graph processing. In this scenario, it is crucial to ensure the integrity of query results, as the provider may have the incentive to return only partial or tampered results to save computing resources or serve their own interests. Blockchain, as a promising solution for secure data storage and retrieval, opens up new opportunities for data management in such scenarios. To scale the blockchain, existing studies have concentrated on using off-chain storage while ensuring the integrity of query results for key-value data in hybrid-storage blockchain architectures. To the best of our knowledge, there is no work to enable the blockchain to support subgraph matching queries. In this paper, we first study the problem of authenticated subgraph matching queries. Traditional subgraph matching algorithms follow the filtering-searching paradigm. The main challenge is to design an Authenticated Data Structure (ADS) and aggregation algorithm that efficiently aggregates non-results for verification during the filtering-searching process. We first propose a vertex-based scheme - the novel ADS MELTree can generate candidate vertices and aggregate non-resulting vertices in the filtering phase, while the aggregation algorithm AMatching can aggregate invalid partial results in the search phase. Furthermore, we propose the bidirectional search aggregation algorithm AMatching* and ADS MVPTree to reduce the computational cost in the search phase and to reduce the on-chain storage cost. In addition, we propose a novel path-based scheme to enhance the aggregation of non-results and accelerate the processing. We design the path-based ADS MPETree for generating candidate paths and aggregating non-resulting paths, and the aggregation algorithm PMatching for efficiently aggregating invalid partial results one path at a time. The results of extensive experiments on five real-world graphs demonstrate the efficiency of our proposed ADSs and aggregation algorithms.},
  archive      = {J_TKDE},
  author       = {Siyu Li and Zhiwei Zhang and Kai Zhong and Kangfei Zhao and Meihui Zhang and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2025.3598850},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Authenticated subgraph matching on large-scale graphs in hybrid-storage blockchains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient algorithms for influence maximization in hypergraphs by stratified sampling. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3599790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) aims to identify $k$ vertices that maximize influence spread across a network. While well-studied in regular graphs, IM in hypergraphs presents unique challenges: conventional graph-based IM methods fail to capture hypergraph-specific structural properties, and existing hypergraph IM algorithms lack theoretical guarantees for time complexity and approximation quality. We address these gaps with HyperIM, a novel algorithm leveraging stratified sampling to generate random reversible reachable sets for efficient seed selection. Our key innovation lies in dual-perspective stratified sampling: assigning sampling probabilities based on vertex structural properties while applying size-adaptive sampling strategies. This approach optimizes seed selection, reduces computational costs, and provides rigorous theoretical guarantees. We further propose HyperIM_BRR, which optimizes the required number of reversible reachable sets, achieving substantial cost reduction without sacrificing accuracy. Extensive experiments on real-world hypergraphs demonstrate that our algorithms significantly outperform state-of-the-art methods, delivering faster execution times and superior influence spread.},
  archive      = {J_TKDE},
  author       = {Lingling Zhang and Tiancheng Lu and Zhiping Shi and Zhiwei Zhang and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2025.3599790},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient algorithms for influence maximization in hypergraphs by stratified sampling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-based generative diffusion models for social recommendations. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3600103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of social networks on online platforms, social recommendation has become a vital technique for enhancing personalized recommendations. The effectiveness of social recommendations largely relies on the social homophily assumption, which presumes that individuals with social connections often share similar preferences. However, this foundational premise has been recently challenged due to the inherent complexity and noise present in real-world social networks. In this paper, we tackle the low social homophily challenge from an innovative generative perspective, directly generating optimal user social representations that maximize consistency with collaborative signals. Specifically, we propose the Score-based Generative Model for Social Recommendation (SGSR), which effectively adapts the Stochastic Differential Equation (SDE)-based diffusion models for social recommendations. To better fit the recommendation context, SGSR employs a joint curriculum training strategy to mitigate challenges related to missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Extensive experiments on realworld datasets demonstrate the effectiveness of our approach in filtering redundant social information and improving recommendation performance. Our codes are available at https://github.com/Anonymous-CodeRepository/Score-based- Generative-Diffusion-Models-for-Social-Recommendations- SGSR},
  archive      = {J_TKDE},
  author       = {Chengyi Liu and Jiahao Zhang and Shijie Wang and Wenqi Fan and Qing Li},
  doi          = {10.1109/TKDE.2025.3600103},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Score-based generative diffusion models for social recommendations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TensorMon: A breakthrough in sparse data gathering leveraging tensor-enhanced techniques for system and network monitoring. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3601198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse data gathering has become a promising solution for reducing measurement costs by leveraging the inherent sparsity of data. However, most existing approaches rely on low-dimensional models such as compressive sensing or matrix completion, which are limited in capturing complex high-dimensional structures. To overcome these limitations, we propose TensorMon, a novel tensor-based sparse data gathering framework that introduces a cuboid sampling strategy to more effectively exploit multidimensional correlations. Unlike traditional entry-based or tube-based sampling, TensorMon introduces the innovative concept of cuboid sampling. We further develop a lightweight sampling scheduling algorithm and a non-iterative inference algorithm to ensure efficient measurement planning and accurate reconstruction of unmeasured data. Theoretical analysis establishes a new performance bound for our sampling strategy, which is significantly lower than those in existing literature. To validate our theoretical findings, we conduct extensive experiments on four real-world datasets: two network monitoring datasets, a city-scale crowd flow dataset, and a road traffic speed dataset. Experimental results demonstrate that TensorMon achieves substantial reductions in measurement cost, delivers high inference accuracy, and ensures rapid data recovery, highlighting its effectiveness and practicality across diverse application scenarios.},
  archive      = {J_TKDE},
  author       = {Jiazheng Tian and Kun Xie and Xin Wang and Jigang Wen and Gaogang Xie and Wei Liang and Dafang Zhang and Kenli Li},
  doi          = {10.1109/TKDE.2025.3601198},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TensorMon: A breakthrough in sparse data gathering leveraging tensor-enhanced techniques for system and network monitoring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view clustering via high-order bipartite graph learning and tensor low-rank representation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3603594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view clustering methods have demonstrated satisfying performance by effectively capturing relationships among data samples. However, most existing methods primarily emphasize direct pairwise relationships, neglecting the exploration of high-order correlations present within each view. To this end, a novel approach, called multiview clustering via high-order bipartite graph learning and tensor low-rank representation (HBGTLRR), is proposed. Specifically, we first construct high-order bipartite graphs to capture latent relationships and concatenate them into a tensor. By applying tensor nuclear norm (TNN) minimization, we obtain a low-rank representation that reduces noise and preserves high-order consistency. Subsequently, a consensus graph is constructed by adaptively fusing the high-order bipartite graphs with corresponding weights, and then a Laplacian low-rank constraint is imposed on it to effectively capture the intrinsic data structure. Finally, extensive experimental results show that HBGTLRR significantly outperforms existing methods, thereby validating the effectiveness of our proposed method.},
  archive      = {J_TKDE},
  author       = {Chuan Tang and Miaomiao Li and Jun Wang and Chang Tang and Jiahe Jiang and Tianyi Wang and En Zhu and Xinwang Liu},
  doi          = {10.1109/TKDE.2025.3603594},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view clustering via high-order bipartite graph learning and tensor low-rank representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential privacy configurations in the real world: A comparative analysis. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3603731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of technologies depend on the large-scale collection of individual-level data, whether for gathering statistical insights from billions of users or training AI models. However, reliance on personal data raises privacy concerns that, in turn, limit the collection and analysis essential to these technologies. Differential Privacy (DP) has gained traction in both academia and industry, ensuring privacy by adding carefully crafted noise to data or its outputs based on a pre-defined DP parameter $\varepsilon$. As real-world implementations emerge, we can examine how DP is practically used beyond academic settings, supporting industry adoption and expanding knowledge on DP applications. Using a systematic process, we comprehensively surveyed the deployed parameters of DP configurations in both commercial and governmental implementations ($n=140$) and compared them to those employed in academic research. We also propose a high-level taxonomy for DP configuration, capturing practical implementations of differentially private Machine Learning (ML) and Federated Learning (FL) applications, highlighting key factors, including the privacy unit and $\varepsilon$. Our results show that, on average, $\varepsilon$ values utilized in the industry span a wider range than those in academic research, with distinct configuration policies for governmental and commercial organizations. Moreover, we identified contrasting reasoning behind $\varepsilon$ selection across deployment environments, alongside insufficient transparency in industry disclosures of DP parameters and limited support for user-oriented configuration. Finally, we discuss how the collected knowledge can be used to create methodological guidelines for the configuration of DP in real-world environments, supporting the vision of an Epsilon Registry.},
  archive      = {J_TKDE},
  author       = {Michael Khavkin and Eran Toch},
  doi          = {10.1109/TKDE.2025.3603731},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Differential privacy configurations in the real world: A comparative analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-tolerant feature selections based on two-type weight-fuzzy granulations and three-view uncertainty measures. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3604005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise-tolerant feature selections are valuable for data learning; they can resort to efficient fuzzy granulations and uncertainty measures, and a fundamental model concerns weighted kernel fuzzy rough sets (WKFRSs) which consider data distributions and uncertainty. In terms of current WKFRSs, fuzzy granulations adopt k-nearest neighbors for weighted optimization, while uncertainty measures consider single algebraic and informational views; corresponding feature selection algorithms have made achievements of noisy processing, but still exist advancement space from granulation deepening and measurement reinforcement. In this paper embracing WKFRSs, two-type weight-fuzzy granulations are defined by using self-adapting radius neighborhoods, three-view uncertainty measures are comprehensively constructed from uncertainty mechanisms, so 2 × (1 + 1 + 2) = 8 heuristic algorithms of feature selections are systematically established for better noise-aware learning. At first, two improved factors of local density and boundary influence are proposed by general neighborhood characterization and statistical radius determination, and thus two sample weights emerge to adjust Gaussian-kernel fuzzy relations to induce two weight-fuzzy granulations. Then, the fuzzy precision and fuzzycomplementary mutual information are respectively proposed from algebraic and informational views, and the two are combined into two fused measures via arithmetic and geometric means. Furthermore, the above two-type granulations and threeview measures two-dimensionally generate 2×(1+1+2) = 8 new heuristic selection algorithms via feature significances. Finally by data experiments, constructional fuzzy granulations, uncertainty measures, feature selections are validated to have anti-noise characteristics and corresponding robustness, while new selection algorithms acquire better performances of classification learning than multiple contrast algorithms.},
  archive      = {J_TKDE},
  author       = {Xin Xie and Xianyong Zhang and Xiaoling Yang and Jiling Yang},
  doi          = {10.1109/TKDE.2025.3604005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Noise-tolerant feature selections based on two-type weight-fuzzy granulations and three-view uncertainty measures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing global path planning via simple queries across multiple platforms. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3591460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of AI, big data, and mobile communication, intelligent transportation has become popular in recent years. Path planning is a typical topic of intelligent transportation, attracting significant attention from researchers. However, existing studies only focus on the path planning of a single platform, which may lead to unexpected traffic congestion. This is because multiple platforms can provide route planning services, the optimal planning calculated by one single platform may be not good in practice, since multiple platforms may lead the users to the same roads, which causes unexpected traffic congestion. Although in the view of each platform, the planning is optimal. Fortunately, with the rise of data sharing and cross-platform cooperation, the data silos between different platforms are gradually being broken. Based on this, we propose Cooperative Global Path Planning(CGPP) framework to overcome the above shortcoming. CGPP allows the path planning request target platform to send some queries to cooperative platforms to optimize its path planning results. Such queries should be “easy” enough to answer, and the query frequency should be small. Based on the above principle, we design a query decision model based on multi-agent reinforcement learning in CGPP framework to decide the query range and query frequency. We design action and reward specifically for the CGPP problem. Furthermore, we propose mechanisms to enhance query precision and reduce query overhead. Specifically, the Self-adjusting Query Area(SQA) concept allows refining query parameters, while the Query Reuse Optimization(QRO) algorithm aims to minimize the number of queries. To solve potential overestimation problems in queries, we propose a Distance-based Outer Query (DB-oq) and Distance-Based Vehicle Count Estimation (DB-VCE) Model. To address the issue that the time interval computed by the QRO algorithm might not fully adapt to dynamic traffic environments, we propose the Temporal Sequence Historical Integration for Time Interval Prediction(TSHI-TIP) algorithm. Extensive experiments on real and synthetic datasets confirm the effectiveness and efficiency of our algorithms.},
  archive      = {J_TKDE},
  author       = {Yurong Cheng and Xiaoxi Cui and Ye Yuan and Xiangmin Zhou and Guoren Wang},
  doi          = {10.1109/TKDE.2025.3591460},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing global path planning via simple queries across multiple platforms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label propagation-based membership degree computation towards a computationally efficient fuzzy community detection approach. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3593203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel fuzzy community detection (FCD) approach, which we term as ‘Label Propagation-Based Fuzzy Community (LaProFC)’, and shows that it has the ability to outperform the existing FCD approaches. While designing the proposed FCD approach, we introduce a new compound type similarity metric termed ‘proportion of common neighbors and edges-based similarity (CCS)’ to compute similarity between two neighboring nodes. By executing local exploration on graphs with modified local random walk (mLRW), most similar neighbors of each node are identified; and based on the directions of most similar neighbors some tentative communities are generated. Afterward, these tentative communities are corrected and stabilized by iteratively computing membership degrees of each node using a novel label propagation-based membership computation function. We also propose a novel edge-density-based technique called ‘community-weight based tie-breaking (CTB)’, which is incorporated with the membership degree computation function. We conduct extensive experiments with both real-life and synthetic datasets and show the working of the proposed approach. Our Proposed LaProFC approach outperforms baseline approaches in terms of popular quality and accuracy metrices including modularity and normalized mutual information. Further, popular multi-criteria decision making (MCDM) tools are used to show supremacy of the proposed approach by computing the ranks of different approaches through two sets of accuracy and quality metrices. Our proposed LaProFC approach supersedes other approaches in terms of faster computations and asymptotic time complexity.},
  archive      = {J_TKDE},
  author       = {Uttam K. Roy and Pranab K. Muhuri},
  doi          = {10.1109/TKDE.2025.3593203},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Label propagation-based membership degree computation towards a computationally efficient fuzzy community detection approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing domain generalization for robust machine-generated text detection. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3581694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have revolutionized text generation, offering significant benefits while also posing threats to society, such as copyright infringement and misinformation. To prevent harmful use, the task of detecting machine-generated content has become an important research topic, though it remains particularly challenging across diverse content domains. This paper presents DGRM, an innovative add-on module designed to improve the domain generalization capability of existing machine-generated text detectors. Our model consists of two training components. (1) Feature disentanglement separates a text's embedding into target-specific and common attributes, thereby enhancing semantic domain generalization across different content domains. (2) Feature regularization applies constraints to these attributes to extract additional target-relevant information and ensure detection consistency under syntactic perturbations—thus achieving syntactic domain generalization. Evaluation over multiple datasets demonstrates that incorporating our module substantially improves the detection of machine-generated text across semantically and syntactically diverse domains. We hope our work contributes to mitigating the harmful use of language models.},
  archive      = {J_TKDE},
  author       = {Sungwon Park and Sungwon Han and Meeyoung Cha},
  doi          = {10.1109/TKDE.2025.3581694},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing domain generalization for robust machine-generated text detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining triangle-dense subgraphs of a fixed size: Hardness, lovasz extension and ´ applications. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2024.3444608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the triangle-densest-k-subgraph problem (TDkS) for undirected graphs: given a size parameter k, compute a subset of k vertices that maximizes the number of induced triangles. The problem corresponds to the simplest generalization of the edge-based densest-k-subgraph problem (DkS) to the case of higher-order network motifs. We prove that TDkS is NP-hard and is not amenable to efficient approximation, in the worst-case. By judiciously exploiting the structure of the problem, we propose a relaxation algorithm for the purpose of obtaining high-quality, sub-optimal solutions. Our approach utilizes the fact that the cost function of TDkS is submodular to construct a convex relaxation for the problem based on the Lovasz extension for submodular functions. We ´ demonstrate that our approaches attain state-of-the-art performance on real-world graphs and can offer substantially improved exploration of the optimal density-size curve compared to sophisticated approximation baselines for DkS. We use document summarization to showcase why TDkS is a useful generalization of DkS},
  archive      = {J_TKDE},
  author       = {Aritra Konar and Nicholas D. Sidiropoulos},
  doi          = {10.1109/TKDE.2024.3444608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mining triangle-dense subgraphs of a fixed size: Hardness, lovasz extension and ´ applications},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On regularization for explaining graph neural networks: An information theory perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2024.3422328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the explainability of graph neural networks (GNNs), which is important for the credibility of GNNs in practical usage. Existing mask-based explanation methods mostly follow the two-phase paradigm to interpret a prediction: feature attribution and selection . However, another important component — regularization, which is crucial to facilitate the above paradigm-has been seldom studied. Regularization is pivotal in mask-based methods, serving to refine explanatory subgraphs through constraints imposed on mask values. Hence, in this work, we endevour to explore the role of regularization in GNNs explainability. As theoretical groundwork inspired by Graph Information Bottleneck (GIB), we first introduce an innovative principle, GIB tailored for explainability, termed GIBE. GIBE serves to consolidate existing mask-based explanation methods by establishing a unified optimization objective for both feature attribution and selection processes. Then, based on GIBE, our main findings can be summarized as: 1) regularization is essentially pursuing the balance between two phases, 2) its optimal coefficient is proportional to the sparsity of explanations, 3) existing methods imply an implicit regularization effect of stochastic mechanism, and 4) its contradictory effects on two phases are responsible for the out-of-distribution (OOD) issue in post-hoc explainability. Based on these findings, we propose two common optimization methods, which can bolster the performance of the current explanation methods via sparsity-adaptive and OOD-resistant regularization schemes. Extensive empirical studies validate our findings and proposed methods.},
  archive      = {J_TKDE},
  author       = {Junfeng Fang and Guibin Zhang and Kun Wang and Wenjie Du and Yifan Duan and Yuankai Wu and Roger Zimmermann and Xiaowen Chu and Yuxuan Liang},
  doi          = {10.1109/TKDE.2024.3422328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On regularization for explaining graph neural networks: An information theory perspective},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperbolic contrastive graph representation learning for session-based recommendation. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2023.3295063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) learns users' preferences by capturing the short-term and sequential patterns from the evolution of user behaviors. Among the studies in the SBR field, graph-based approaches are a relatively powerful technique, which generally extract item information by message aggregation under Euclidean space. However, such methods cannot effectively extract the hierarchical information contained among consecutive items in a session, which is critical to represent users' preferences. In this paper, we present a hyperbolic contrastive graph recommender (HCGR), a principled session-based recommendation framework involving Lorentz hyperbolic space to efficiently capture the coherence and hierarchical representations of the items. Within this framework, an adaptive hyperbolic attention computation is designed to aggregate the graph message of each user's preference in a session-based behavior sequence. In addition, a contrastive ranking loss with the hyperbolic distance is used to separate the positive and negative items. Compared with the Euclidean distance, the advantage of the hyperbolic distance in the contrastive ranking loss is analyzed. The results of extensive experiments on four real-world datasets demonstrate that HCGR consistently outperforms state-of-the-art baselines by 0.43 $\%$ -14.44 $\%$ in terms of $HitRate$ , $NDCG$ and $MRR$ .},
  archive      = {J_TKDE},
  author       = {Naicheng Guo and Xiaolei Liu and Shaoshuai Li and Mingming Ha and Qiongxu Ma and Binfeng Wang and Yunan Zhao and Linxun Chen and Xiaobo Guo},
  doi          = {10.1109/TKDE.2023.3295063},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic contrastive graph representation learning for session-based recommendation},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Notice of retraction: CHN: An efficient algorithm for mining closed high utility itemsets with negative utility. <em>TKDE</em>, 1. (<a href='https://doi.org/10.1109/TKDE.2018.2882421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retracted.},
  archive      = {J_TKDE},
  author       = {Kuldeep Singh and Shashank Sheshar Singh and Ajay Kumar and Harish Kumar Shakya and Bhaskar Biswas},
  doi          = {10.1109/TKDE.2018.2882421},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Notice of retraction: CHN: An efficient algorithm for mining closed high utility itemsets with negative utility},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2009). Cost optimization of the supply chain network using genetic algorithms - Withdrawn. <em>TKDE</em>, 1. (<a href='https://doi.org/10.1109/TKDE.2009.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Withdrawn.},
  archive      = {J_TKDE},
  author       = {Henry C. W. Lau and T. M. Chan and Wan Ting Tsui and G.T.S. Ho},
  doi          = {10.1109/TKDE.2009.20},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost optimization of the supply chain network using genetic algorithms - Withdrawn},
  year         = {2009},
}
</textarea>
</details></li>
</ul>

</body>
</html>
