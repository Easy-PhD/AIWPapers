<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde">TKDE - 62</h2>
<ul>
<li><details>
<summary>
(2025). Approximately unimodal likelihood models for ordinal regression. <em>TKDE</em>, 1-11. (<a href='https://doi.org/10.1109/TKDE.2025.3617386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression (OR, also called ordinal classification) is classification of ordinal data, in which the underlying target variable is categorical and considered to have a natural ordinal relation for the underlying explanatory variable. A key to successful OR models is to find a data structure ‘natural ordinal relation’ common to many ordinal data and reflect that structure into the design of those models. A recent OR study found that many real-world ordinal data show a tendency that the conditional probability distribution (CPD) of the target variable given a value of the explanatory variable will often be unimodal. Several previous studies thus developed unimodal likelihood models, in which a predicted CPD is guaranteed to become unimodal. However, it was also observed experimentally that many real-world ordinal data partly have values of the explanatory variable where the underlying CPD will be non-unimodal, and hence unimodal likelihood models may suffer from a bias for such a CPD. Therefore, motivated to mitigate such a bias, we propose approximately unimodal likelihood models, which can represent up to a unimodal CPD and a CPD that is close to be unimodal. We also verify experimentally that a proposed model can be effective for statistical modeling of ordinal data and OR tasks.},
  archive      = {J_TKDE},
  author       = {Ryoya Yamasaki},
  doi          = {10.1109/TKDE.2025.3617386},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Approximately unimodal likelihood models for ordinal regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSM: Adaptive subject-focused modeling for multimodal summarization via semantic matching. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Summarization aims to use multimodal data to generate accurate and concise summaries for long sentences. While previous work has achieved promising success, they have overlooked the mismatching among multimodal semantics and lacked subject information guidance for adaptive referential images. Motivated by this observation, we propose ASSM, an Adaptive Subject-focused modeling for multimodal summarization via Semantic Matching. The novelty of ASSM lies in two aspects. First, we propose a multimodal semantic matching module that projects multimodal inputs into a shared joint embedding semantic space to determine whether the semantics between multimodalities are mismatching. Second, we propose an adaptive subject-focused guide module, which adaptively references images to learn subject tokens based on the multimodal semantic matching results. With these subject tokens, we are able to focus on the subject information, providing precise guidance for summary generation. We conduct extensive experiments on two standard benchmarks and compare ASSM with 17 existing models. The experimental results regarding ROUGE, BERTScore, and MoverScore show that the proposed ASSM model outperforms all competitors, achieving state-of-the-art performance and suggesting the effectiveness of our proposal. In addition, we provide a case study to further demonstrate the usability of ASSM.},
  archive      = {J_TKDE},
  author       = {Xujian Zhao and Chuanpeng Deng and Peiquan Jin},
  doi          = {10.1109/TKDE.2025.3610544},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ASSM: Adaptive subject-focused modeling for multimodal summarization via semantic matching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of large language models on generative graph analytics: Query, learning, and applications. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various natural language processing tasks to answer users' arbitrary questions and generate specific-domain content. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. However, LLMs are sequential models for textual data, but graphs are non-sequential topological data. It is challenging to adapt LLMs to tackle graph analytics tasks. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) in terms of three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graphs and LLMs, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning, and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.},
  archive      = {J_TKDE},
  author       = {Wenbo Shang and Xin Huang},
  doi          = {10.1109/TKDE.2025.3609877},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of large language models on generative graph analytics: Query, learning, and applications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive hyper-box granulation with justifiable granularity for feature selection. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3617583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering as a fundamental technique in data mining and machine learning, aims to partition data into meaningful groups based on the inherent relationships among data. However, traditional clustering algorithms typically assume convex hyperspherical geometry of data, where the clusters have clearly defined boundaries and do not overlap. In contrast, real-world data often exhibits complex and non-convex geometries, which makes these assumptions ineffective and lead to inaccurate clustering results that fail to capture the intrinsic structure. To address this challenge, the paper proposes a novel granular clustering based on an enhanced granularity representation, which further refines the principle of justifiable granularity. By introducing a more precise and flexible hyper-box granulation mechanism, the method dynamically adapts to the topology of data, thereby improving clustering accuracy. By defining the degree of aggregation and discreteness between data points, the importance of attributes in the feature space is quantified, leading to the design of a novel hyper-box feature selection (HBFS) algorithm. This algorithm integrates the granular clustering principle to optimize the feature selection process, reducing the impact of redundant features and noise, thus improving clustering efficiency and interpretability. To validate the superiority and effectiveness of the proposed method, extensive experiments were conducted on fifteen publicly available datasets, comparing the performance of HBFS algorithm with classical and state-of-art feature selection methods. The results and the statistical significance tests show that HBFS significantly outperforms existing feature selection methods across various evaluation metrics.},
  archive      = {J_TKDE},
  author       = {Wentao Li and Bowen Yang and Witold Pedrycz and Chao Zhang and Tao Zhan},
  doi          = {10.1109/TKDE.2025.3617583},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive hyper-box granulation with justifiable granularity for feature selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph2Region: Efficient graph similarity learning with structure and scale restoration. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3617461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph similarity is critical in graph-related tasks such as graph retrieval, where metrics like maximum common subgraph (MCS) and graph edit distance (GED) are commonly used. However, exact computations of these metrics are known to be NP-Hard. Recent neural network-based approaches approximate the similarity score in embedding spaces to alleviate the computational burden, but they either involve expensive pairwise node comparisons or fail to effectively utilize structural and scale information of graphs. To tackle these issues, we propose a novel geometric-based graph embedding method called Graph2Region (G2R). G2R represents nodes as closed regions and recovers their adjacency patterns within graphs in the embedding space. By incorporating the node features and adjacency patterns of graphs, G2R summarizes graph regions, i.e., graph embeddings, where the shape captures the underlying graph structures and the volume reflects the graph size. Consequently, the overlap between graph regions can serve as an approximation of MCS, signifying similar node regions and adjacency patterns. We further analyze the relationship between MCS and GED and propose using disjoint parts as a proxy for GED similarity. This analysis enables concurrent computation of MCS and GED, incorporating local and global structural information. Experimental evaluation highlights G2R's competitive performance in graph similarity computation. It achieves up to a 60.0% relative accuracy improvement over state-of-the-art methods in MCS similarity learning, while maintaining efficiency in both training and inference. Moreover, G2R showcases remarkable capability in predicting both MCS and GED similarities simultaneously, providing a holistic assessment of graph similarity.},
  archive      = {J_TKDE},
  author       = {Zhouyang Liu and Yixin Chen and Ning Liu and Jiezhong He and Dongsheng Li},
  doi          = {10.1109/TKDE.2025.3617461},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph2Region: Efficient graph similarity learning with structure and scale restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning accurate representation to nonstandard tensors via a mode-aware tucker network. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3617894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonstandard tensor is frequently adopted to model a large-sale complex dynamic network. A Tensor Representation Learning (TRL) model enables extracting valuable knowledge form a dynamic network via learning low-dimensional representation of a target nonstandard tensor. Nevertheless, the representation learning ability of existing TRL models are limited for a nonstandard tensor due to its inability to accurately represent the specific nature of the nonstandard tensor, i.e., mode imbalance, high-dimension, and incompleteness. To address this issue, this study innovatively proposes a Mode-Aware Tucker Networkbased Tensor Representation Learning (MTN-TRL) model with three-fold ideas: a) designing a mode-aware Tucker network to accurately represent the imbalanced mode of a nonstandard tensor, b) building an MTN-based high-efficient TRL model that fuses both data density-oriented modeling principle and adaptive parameters learning scheme, and c) theoretically proving the MTN-TRL model's convergence. Extensive experiments on eight nonstandard tensors generating from real-world dynamic networks demonstrate that MTN-TRL significantly outperforms state-of-the-art models in terms of representation accuracy.},
  archive      = {J_TKDE},
  author       = {Hao Wu and Qu Wang and Xin Luo and Zidong Wang},
  doi          = {10.1109/TKDE.2025.3617894},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning accurate representation to nonstandard tensors via a mode-aware tucker network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A geometric approach to $k$-means clustering. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3616858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$k$-meansclustering is a fundamental problem in many scientific and engineering domains. The optimization problem associated with $k$-means clustering is nonconvex, for which standard algorithms are only guaranteed to find a local optimum. Leveraging the hidden structure of local solutions, we propose a general algorithmic framework for escaping undesirable local solutions and recovering the global solution or the ground truth clustering. This framework consists of iteratively alternating between two steps: (i) detect mis-specified clusters in a local solution, and (ii) improve the local solution by non-local operations. We discuss specific implementation of these steps, and elucidate how the proposed framework unifies many existing variants of $k$-means algorithms through a geometric perspective. We also present two natural variants of the proposed framework, where the initial number of clusters may be over- or under-specified. We provide theoretical justifications and extensive experiments to demonstrate the efficacy of the proposed approach.},
  archive      = {J_TKDE},
  author       = {Jiazhen Hong and Wei Qian and Yudong Chen and Yuqian Zhang},
  doi          = {10.1109/TKDE.2025.3616858},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A geometric approach to $k$-means clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from graph-graph relationship: A new perspective on graph-level anomaly detection. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3618929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level anomaly detection (GLAD) aims to distinguish anomalous graphs that exhibit significant deviations from others. The graph-graph relationship, revealing the deviation and similarity between graphs, offers global insights into the entire graph level for highlighting the anomalies' divergence from normal graph patterns. Thus, understanding graph-graph relationships is critical to boosting models on GLAD tasks. However, existing deep GLAD algorithms heavily rely on Graph Neural Networks that primarily focus on analyzing individual graphs. These methods overlook the significance of graph-graph relationships in telling anomalies from normal graphs. In this paper, we propose a novel model for Graph-level Anomaly Detection using the Transformer technique, namely GADTrans. Specifically, GADTrans build the transformer upon crucial subgraphs mined by a parametrized extractor, for modeling precise graph-graph relationships. The learned graph-graph relationships put effort into distinguishing normal and anomalous graphs. In addition, a specific loss is introduced to guide GADTrans in highlighting the deviation between anomalous and normal graphs while underlining the similarities among normal graphs. GADTrans achieves model interpretability by delivering human-interpretable results, which are learned graph-graph relationships and crucial subgraphs. Extensive experiments on six real-world datasets verify the effectiveness and superiority of GADTrans for GLAD tasks.},
  archive      = {J_TKDE},
  author       = {Zhenyu Yang and Ge Zhang and Jia Wu and Jian Yang and Hao Peng and Pietro Liò},
  doi          = {10.1109/TKDE.2025.3618929},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning from graph-graph relationship: A new perspective on graph-level anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A universal physics-informed neural network framework for predicting network dynamics: From lower-order to higher-order. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3618834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the complex networks dynamics is a challenging task. Many studies have shown that data-driven frameworks offer promising solutions to this issue. However, existing approaches still face significant limitations, particularly when network structures evolve from lower-order to higher-order networks, or when the dynamical equation of the network is governed by multiple dynamical terms, such as local self-dynamics, lower-order and higher-order coupling dynamics. To this end, we propose a universal physics-informed neural network framework capable of predicting various types of dynamics on both lower- and higher-order networks. First, the framework captures and integrates more nonlinear features through the higher-order term expansion module. Second, we design a hybrid neural network module to differentially learn each dynamical term to comprehensively capture network dynamics. Finally, a physics-informed loss function construction module is introduced to integrate differential loss with prediction loss, improving the accuracy of network dynamical prediction. Experimental results indicate that our method outperforms the state-of-the-art approaches in predicting network dynamics on both lower- and higher-order networks. Ablation studies confirm the critical role of each module. In addition, our method also performs well on real-world dynamical processes, which shows that it remains robust to real complex scenarios.},
  archive      = {J_TKDE},
  author       = {Xiao Ding and Xingyi Zhang and Hai-Feng Zhang},
  doi          = {10.1109/TKDE.2025.3618834},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A universal physics-informed neural network framework for predicting network dynamics: From lower-order to higher-order},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking information granularity heterogeneity: A mutual information-inspired causal discovery framework for multi-rate time series. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3618763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery in multi-rate time series encounters greater challenges compared to regular time series. This stems from a potential problem that has not been noticed and explored in existing studies: information granularity heterogeneity, which refers to the natural difference in information granularity between fast sampling rate data (high information granularity) and slow sampling rate data (low information granularity). Such an imbalance in information granularity can hinder forecasting relationships modeling and induce biased causal learning. Therefore, we propose a Mutual Information-iNspired causal Discovery framework (MIND), aiming to derive rate-agnostic features with consistent information granularity to alleviate information granularity heterogeneity problem. Technically, MIND comprises Stage 1 (pre-training) and Stage 2 (fine-tuning and causal discovery). In Stage 1, empowered by pseudo-slow sampling rate data (generated through the interleaved down sampling strategy) and mutual information, we can eliminate the influence of sampling rates and drive rate-aware encoders (RAEs) to sense key information (i.e., rate-agnostic) that remains unchanged across varying sampling rates. In Stage 2, the well-trained RAEs can extract rate-agnostic features from real multi-rate time series, thus facilitating effective forecasting relationships modeling and yield accurate causal discovery. Empirically, MIND realizes superior performance on various multi-rate scenarios, including four simulation datasets and one real-world dataset.},
  archive      = {J_TKDE},
  author       = {Kun Zhu and Chunhui Zhao and Biao Huang},
  doi          = {10.1109/TKDE.2025.3618763},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Breaking information granularity heterogeneity: A mutual information-inspired causal discovery framework for multi-rate time series},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBCGM: A granular model for big data classification based on data bisection and cascade weighted clustering. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3618784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers confront significant challenges when classifying and modeling data due to the growing system complexity, data volume, and requirement for accurate and reliable models. Information granules, a fundamental component of Granular Computing (GrC), play a crucial role in human cognition. In this study, we develop a granular classifier called DBCGM, which facilitates delivering models with improved accuracy and efficiency in big data classification. In particular, DBCGM achieves the goal through the following four steps. First, we construct a 1-D index for each point and then utilize a context-based data bisection method to obtain non-overlapping subsets. These disjoint subsets enhance both the quality and efficiency of big data clustering and make it possible to process the entire dataset simultaneously. Next, we propose a cascade weighted clustering (CWC) algorithm to generate numeric prototypes from the obtained subsets. Then, following the principle of justification granularity (PJG), the numeric prototypes are refined into information granules. Finally, the classifier can be regarded as the weighted sum of all the values of the spatial relationship between the input instance and the information granules. We evaluate the performance of DBCGM in terms of accuracy, V-measure, and execution time. We compare DBCGM with benchmark classifiers and three big data granulating methods. Experimental results on both synthetic and public datasets show that DBCGM outperforms the existing methods. In particular, compared with the state-of-the-art method, DBCGM reduces the running time by an average of 2.55%, and improves the V-measure and accuracy by an average of 5.71% and 2.32%, respectively.},
  archive      = {J_TKDE},
  author       = {Jiande Huang and Yuhui Deng and Yi Zhou and Shujie Pang and Qifen Yang and Geyong Min},
  doi          = {10.1109/TKDE.2025.3618784},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DBCGM: A granular model for big data classification based on data bisection and cascade weighted clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An event-centric framework for predicting crime hotspots with flexible time intervals. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3618389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting crime hotspots in a city is a complex and critical task with significant societal implications. Numerous spatiotemporal correlations and irregularities pose substantial challenges to this endeavor. Existing methods commonly employ fixed-time granularities and sequence prediction models. However, determining appropriate time granularities is difficult, leading to inaccurate predictions for specific time windows. For example, users might ask: What are the crime hotspots during [12:00-20:00]? To address this issue, we introduce ${\sf FlexiCrime}$, a novel event-centric framework for predicting crime hotspots with flexible time intervals. ${\sf FlexiCrime}$ incorporates a continuous-time attention network to capture correlations between crime events, which learns crime context features, representing general crime patterns across time points and locations. Furthermore, we introduce a type-aware spatiotemporal point process that learns crime-evolving features, measuring the risk of specific crime types at a given time and location by considering the frequency of past crime events. Together, the crime context and evolving features allow us to predict whether an urban area is a crime hotspot given a future time interval. To evaluate ${\sf FlexiCrime}$'s effectiveness, we conducted experiments using real-world datasets from two cities, covering twelve crime types. The results show that our model outperforms baseline techniques in predicting crime hotspots over flexible time intervals.},
  archive      = {J_TKDE},
  author       = {Jiahui Jin and Yi Hong and Guandong Xu and Jinghui Zhang and Jun Tang and Hancheng Wang},
  doi          = {10.1109/TKDE.2025.3618389},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An event-centric framework for predicting crime hotspots with flexible time intervals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior merging graph convolution network for multi-behavior recommendation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3618466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encoding multi-behavior information into a single graph collaborative filtering vector is an emerging challenge, as different behaviors generate distinct graphs, each with its own embedding vector. To address this problem, recent approaches typically designate the embedding of some behaviors as primary embeddings and use the embeddings of other behaviors to enhance the primary behavior recommendation. However, these models may excel in recommending primary behaviors at the expense of degrading the performance of auxiliary behaviors. As a result, modern recommender systems often need to maintain multiple sets of collaborative filtering embeddings to achieve satisfactory recommendation performance across all behaviors. To alleviate this issue, we introduce the Behavior Merging Graphs. Instead of modeling each behavior separately, BMG uses a joint graph to capture potential behavior merging sets between nodes and applies the partial order theory to model the intricate structures and relational order among behavior merging sets. Based on BMG, we introduce the Behavior Merging Graphs Convolutional Networks (BMGCN), which aggregates neighbor information by integrating convolutional weights that account for the rank transformation of Behavior Merging Order across various behavior merging sets. Furthermore, BMGCN employs behavior merging-based sampling to guide the traditional BPR sampling process, enhancing embedding training. Experiments on three widely used datasets demonstrate that BMGCN achieves superior multi-behavior recommendation performance compared to state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Hao Chen and Zhiqing Li and Yuanchen Bei and Kai Xu and Yijie Zhang and Feiran Huang and Yu Yang and Huan Gong and Fakhri Karray},
  doi          = {10.1109/TKDE.2025.3618466},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Behavior merging graph convolution network for multi-behavior recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private-set-intersection-based medical data sharing scheme with integrity auditing for IoMT cloud storage systems. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3619426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the medical industry is generating a large amount of data. How to securely store and reliably share these medical data has been a hot research topic. Cloud storage technology can be applied to the medical industry to adapt to the rapid growth of medical data. However, cloud-based data storage and sharing systems face a series of security issues: whether the integrity of outsourced medical data can be guaranteed, and malicious access between different medical institutions may leak user's privacy. This article proposes a system that simultaneously solves the integrity auditing of medical data and securely data sharing between different medical institutions under the terminal-edge-cloud framework. Specifically, patients/doctors are treated as terminal users, medical institutions are viewed as edge nodes, and medical clouds form the central storage layer. In the process of data auditing, third-party auditor can achieve integrity auditing of medical cloud storage data. Moreover, different medical institutions use private-set-intersection technology to share the common user's electronic medical data, while for other users not in intersection set, their data does not need to be shared. Finally, security and performance analyses show that our proposed system is provable secure and has high computational and communication efficiency.},
  archive      = {J_TKDE},
  author       = {Zekun Li and Jinyong Chang and Bei Liang and Kaijing Ling and Yifan Dong and Yanyan Ji and Maozhi Xu},
  doi          = {10.1109/TKDE.2025.3619426},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Private-set-intersection-based medical data sharing scheme with integrity auditing for IoMT cloud storage systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMCR: An online multivariate forecaster for cloud resource management. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3619097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A precise workload forecaster is the key to effective resource management, system scalability, and overall operational efficiency in cloud environments. However, real-world cloud systems frequently operate in dynamic and unpredictable settings, causing workloads that exhibit significant diversity and fluctuations. To address these problems, we introduce OMCR, a novel online multivariate forecaster for cloud resource management, that overcomes the limitations of existing static forecasting methods through online learning. OMCR integrates long-term memory with a rapid response mechanism to short-term changes in cloud systems, while also considering the impact of multivariate relationships on workload prediction. OMCR minimizes its reliance on historical data, thereby reducing training difficulty and maintaining lower prediction loss in the long run. OMCR also offers an adaptive approach to forecasting peak workloads in a certain time span, which helps cloud resource management. Experimental results demonstrate the superior performance of our proposed framework compared to state-of-the-art methods in MAE and MSE metrics when forecasting cloud workloads.},
  archive      = {J_TKDE},
  author       = {Xu Gao and Xiu Tang and Chang Yao and Sai Wu and Gongsheng Yuan and Wenchao Zhou and Feifei Li and Gang Chen},
  doi          = {10.1109/TKDE.2025.3619097},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OMCR: An online multivariate forecaster for cloud resource management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attentive continuous-time generative adversarial networks for irregular time series imputation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3617659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are widely used in many classification and regression tasks. However, numerous time series contain unavoidable missing data, making it challenging to model the temporal dynamics of sequential data. Various data imputation methods have been proposed to infer missing values in time series. Although sequences recorded at fixed time intervals are presented in discrete form, they possess an inherent temporal continuity, which is ignored in most existing approaches. In this paper, we propose an end-to-end Attentive Continuous-Time Generative Adversarial Network (ACGANet) to estimate unobserved values in irregular sequences. ACGANet captures the temporal dynamics by transforming the discrete sequence into the continuous-time flow, thereby modeling the underlying distribution of the real data. Furthermore, ACGANet employs an adversarial learning strategy to alleviate the error introduced by imputed values, with the discriminator distinguishing between real and generated samples. Additionally, ACGANet introduces the log-density of hidden temporal states as an auxiliary loss to further optimize the generator. This allows the model to simultaneously focus on the overall temporal dynamics of the time series and the underlying distribution of the missing data. Extensive experiments on three publicly available real-world datasets demonstrate that ACGANet achieves state-of-the-art performance in imputing incomplete time series. Moreover, both qualitative and quantitative analyses validate the effectiveness of the proposed model.},
  archive      = {J_TKDE},
  author       = {Yakun Wang and Yisheng Zou and Songlin He and Linfu Sun and Gang Wang},
  doi          = {10.1109/TKDE.2025.3617659},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Attentive continuous-time generative adversarial networks for irregular time series imputation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EIGA: A novel genetic algorithm based on edge information for community detection in weighted social networks. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3619494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community Detection (CD) in weighted social networks is a highly active research field, celebrated for its profound practical implications across a multitude of disciplines. Genetic algorithms (GAs) are frequently explored to tackle CD problems, leveraging their capability to navigate the extensive discrete search space effectively. Throughout the evolutionary process, genetic operators such as crossover and mutation assume pivotal roles in effectively exploring the vast solution space. Nonetheless, prevailing GA-based approaches often ignore crucial topology information, particularly information regarding edge weights, resulting in compromised algorithm performance. In light of this, this paper introduces Edge Information-based GA (EIGA) to effectively solve CD problems in weighted networks. This is achieved specifically through the innovative designs of edgeweight-aware crossover and mutation operators. These novel edge-weight-aware operators improve the extraction of meaningful community structures, advancing knowledge discovery from social networks. Empirical findings demonstrate the superior performance of EIGA over numerous state-of-the-art algorithms across various real-world and synthetic benchmark networks.},
  archive      = {J_TKDE},
  author       = {Anjali de Silva and Gang Chen and Hui Ma and Seyed Mohammad Nekooei},
  doi          = {10.1109/TKDE.2025.3619494},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EIGA: A novel genetic algorithm based on edge information for community detection in weighted social networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BPL: Bias-adaptive preference distillation learning for recommender system. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3619575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems suffer from biases that cause the collected feedback to incompletely reveal user preference. While debiasing learning has been extensively studied, they mostly focused on the specialized (called counterfactual) test environment simulated by random exposure of items, significantly degrading accuracy in the typical (called factual) test environment based on actual user-item interactions. In fact, each test environment highlights the benefit of a different aspect: the counterfactual test emphasizes user satisfaction in the long-terms, while the factual test focuses on predicting subsequent user behaviors on platforms. Therefore, it is desirable to have a model that performs well on both tests rather than only one. In this work, we introduce a new learning framework, called Bias-adaptive Preference distillation Learning (BPL), to gradually uncover user preferences with dual distillation strategies. These distillation strategies are designed to drive high performance in both factual and counterfactual test environments. Employing a specialized form of teacher-student distillation from a biased model, BPL retains accurate preference knowledge aligned with the collected feedback, leading to high performance in the factual test. Furthermore, through self-distillation with reliability filtering, BPL iteratively refines its knowledge throughout the training process. This enables the model to produce more accurate predictions across a broader range of user-item combinations, thereby improving performance in the counterfactual test. Comprehensive experiments validate the effectiveness of BPL in both factual and counterfactual tests.},
  archive      = {J_TKDE},
  author       = {SeongKu Kang and Jianxun Lian and Dongha Lee and Wonbin Kweon and Sanghwan Jang and Jaehyun Lee and Jindong Wang and Xing Xie and Hwanjo Yu},
  doi          = {10.1109/TKDE.2025.3619575},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BPL: Bias-adaptive preference distillation learning for recommender system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven scale-adaptive time-frequency convolutional network for long sequence time-series forecasting. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3619521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models based on Transformer variants have consistently demonstrated leading performance in long sequence time series forecasting. However, in some complex application scenarios, Transformers tend to capture low-frequency information in the data while overlooking high-frequency information, which often contains rich non-stationary features. This unbalanced feature extraction approach limits the model's ability to effectively handle real-world time series data. To address this issue, we explicitly represent both low-frequency and high-frequency information and propose a model called STCNet, a data-driven scale-adaptive convolutional network that aims to extract diverse features and patterns from the data by learning features across different frequency bands in a balanced manner. Specifically, we propose an entropy-based adaptive wavelet basis selection algorithm, which can adaptively select appropriate wavelet bases based on the data distribution to achieve effective multi-frequency decomposition of complex time series. In addition, we designed a hierarchical scale-adaptive factor that allows for dynamic adjustment of feature weights according to different time scales through refined layered weight adjustment, significantly enhancing the model's capability in handling non-stationary time series features. To further optimize the output features of the model, we introduce a test-time training mechanism, combined with a fast weight update strategy and a weight-sharing strategy to reduce the number of model parameters, effectively mitigating the risk of overfitting. Experimental results on nine datasets demonstrate that STCNet outperforms the current state-of-the-art models in both effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Zhiqiang Zhang and Weiqing Wang and Xin Zhou and Yu Bai and Hongzhi Yin},
  doi          = {10.1109/TKDE.2025.3619521},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A data-driven scale-adaptive time-frequency convolutional network for long sequence time-series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot knowledge graph completion with adaptive negative sampling mechanism. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3614171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph completion (few-shot KGC) mines unseen knowledge by leveraging meta-learning and contrastive learning to achieve accurate predictions with limited triples. Recent studies have focused on designing distance or similarity metrics to provide better knowledge representation between entities and relations. However, three issues with negative sampling remain unexplored: 1) the construction of negative queries heavily relies on manual experience in selecting candidate tail entities, 2) the constructed negative queries may mislabel potential true facts, and 3) the varying difficulties of negative queries are ignored. To solve the above issues, in this paper, we introduce curriculum learning into few-shot KGC and propose a novel few-shot KGC framework empowered by an adaptive negative sampling mechanism, which can eliminate the dependence on any additional manual experience, reduce mislabeling, and generate negative queries with appropriate difficulty. Specifically, the proposed framework includes two alternating phases. In the negative sampling phase, we first design a novel positive-unlabeled learning based scoring function with a type-related candidates encoder and then build a variable-speed sliding window based pacing function to select negative queries with appropriate learning difficulty under current training step. In the meta-training phase, we develop an adapted triple-oriented knowledge encoder to provide accurate representation for queries. Experimental results demonstrate that the proposed framework outperforms the state-of-the-art baselines and provides negative queries with appropriate difficulty in few-shot KGC.},
  archive      = {J_TKDE},
  author       = {Jiaao Yu and Lanlan Rui and Yijing Lin and Zhipeng Gao and Xuesong Qiu and Shaoyong Guo},
  doi          = {10.1109/TKDE.2025.3614171},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Few-shot knowledge graph completion with adaptive negative sampling mechanism},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental multi-view clustering: Exploring stream-view correlations to learn consistency and diversity. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3605594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) has demonstrated impressive performance due to its ability to capture both consistency and diversity information among views. However, most existing techniques assume that all views are available in advance, making them inadequate for stream-view data, such as in intelligent transportation systems and medical imaging analysis, where memory constraints or privacy concerns prevent storing all previous views. Although some methods attempt to address this issue by capturing consistency information, they often fail to effectively extract diversity information and cross-view relationships. We argue that these limitations are inherent to incremental multi-view clustering (IMVC), as the inability to retain all previous views inevitably leads to insufficient information utilization, thereby compromising performance. To address these challenges, we propose a novel algorithm, termed Incremental Multi-View Clustering with Cross-View Correlation and Diversity (CDIMVC). Unlike existing methods that only retain consistency information, CDIMVC also preserves diversity information and utilizes similarity matrices to capture cross-view relationships. To implement this method, we develop three key modules: the dynamic view correlation analysis module (DVCAM), the knowledge extraction module (KEM), and the knowledge transfer module (KTM). When a new data view arrives, DVCAM first assesses its importance and correlation with historical views. Subsequently, KEM computes its consistency and diversity information by comparing it to those in the knowledge base. Finally, KTM facilitates the effective transmission of past knowledge, preventing the loss of historical information. By integrating these modules, CDIMVC can effectively capture cross-view relationships and diversity information, facilitating efficient knowledge updating and maintenance. An alternating procedure is also designed to optimize the resulting optimization problem. Experimental results show that CDIMVC exceeds state-of-the-art methods, demonstrating its effectiveness in handling stream-view data.},
  archive      = {J_TKDE},
  author       = {Yu Feng and Weixuan Liang and Xinhang Wan and Jiyuan Liu and Miaomiao Li and Xinwang Liu},
  doi          = {10.1109/TKDE.2025.3605594},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incremental multi-view clustering: Exploring stream-view correlations to learn consistency and diversity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A diffusive data augmentation framework for reconstruction of complex network evolutionary history. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3605795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolutionary dynamics of complex systems encode critical information about their functional organization. In particular, the generation times of edges reveal key aspects of historical development in networked systems such as protein-protein interaction networks, ecosystems, and social networks. Accurately recovering these temporal processes is of significant scientific value-for example, in elucidating the mechanisms underlying protein interaction evolution. However, existing methods typically assume access to partially time-stamped networks and often struggle to generalize across domains. They perform poorly in recovering edge generation times in static networks without temporal annotations. To address this challenge, we propose a comparative paradigm that enables cross-network learning by jointly training on multiple temporal networks. This framework captures structural-temporal correlations that generalize across networks and improves accuracy by 16.98% on average compared to separate training strategies. Furthermore, to mitigate the scarcity of real temporal data, we introduce a novel diffusion-based generative model for producing Augmented Temporal Networks (ATNs) . By integrating both real and generated samples during training, our joint strategy yields an additional 5.46% improvement in predictive accuracy, demonstrating the effectiveness of data augmentation in enhancing generalization.},
  archive      = {J_TKDE},
  author       = {En Xu and Can Rong and Jingtao Ding and Yong Li},
  doi          = {10.1109/TKDE.2025.3605795},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A diffusive data augmentation framework for reconstruction of complex network evolutionary history},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and effective adaptation of multimodal foundation models in sequential recommendation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger text encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the MicroLens public benchmark. We will release our code and datasets to support future research.},
  archive      = {J_TKDE},
  author       = {Junchen Fu and Xuri Ge and Xin Xin and Alexandros Karatzoglou and Ioannis Arapakis and Kaiwen Zheng and Yongxin Ni and Joemon M. Jose Joemon},
  doi          = {10.1109/TKDE.2025.3608071},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and effective adaptation of multimodal foundation models in sequential recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KnobCF: Uncertainty-aware knob tuning. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3608030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knob tuning aims to optimize database performance by searching for the most effective knob configuration under a certain workload. Existing works suffer from two significant problems. First, there exist multiple useless evaluations of knob tuning even with diverse searching methods because of the different sensitivities of knobs on a certain workload. Second, the single evaluation of knob configurations may bring overestimation or underestimation because of query performance uncertainty. To solve the above problems, we propose a query uncertainty-aware knob classifier, called KnobCF, to enhance knob tuning. Our method has three contributions: (1) We propose uncertainty-aware configuration estimation to improve the tuning process. (2) We design a few-shot uncertainty estimator that requires no extra data collection, ensuring high efficiency in practical tasks. (3) We provide a flexible framework that can be integrated into existing knob tuners and DBMSs without modification. Our experiments on four open-source benchmarks demonstrate that our method effectively reduces useless evaluations and improves the tuning results. Especially in TPCC, our method achieves competitive tuning results with only 60% to 70% time consumption compared to the full workload evaluations.},
  archive      = {J_TKDE},
  author       = {Yu Yan and Junfang Huang and Hongzhi Wang and Jian Geng and Kaixin Zhang and Tao Yu},
  doi          = {10.1109/TKDE.2025.3608030},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {KnobCF: Uncertainty-aware knob tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey and experimental study on neural trajectory-user linking models. <em>TKDE</em>, 1-17. (<a href='https://doi.org/10.1109/TKDE.2025.3607902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of location-aware devices has boosted urban systems with massive volumes of anonymous trajectory data, presenting both challenges and opportunities for enhancing smart city initiatives through Trajectory-User Linking (TUL). Typically, TUL aims to match anonymous trajectories with specific users by exploring spatiotemporal patterns and insightful mobility behaviors. However, current TUL models face significant limitations due to their reliance on singular data sources and insufficient consideration of real-world scenarios. Furthermore, these models often lack evaluation in fair and comprehensive environments, hindering accurate assessment of their performance and applicability. This paper systematically investigates prevalent challenges encountered by existing TUL models, conducts a comprehensive review of state-of-the-art models, and proposes a structured framework that encompasses three core components: point-level representation learning, trajectory-level representation learning, and user linking. Through meticulously designed experiments, we examine the effectiveness and efficiency of leading TUL models in handling the complexities of real-world data, such as data imbalance, sparsity, new users, and scalability. This in-depth analysis uncovers limitations in existing methodologies and offers guidance for future advancements, contributing to the development of robust TUL solutions for urban mobility analysis and smart city technologies.},
  archive      = {J_TKDE},
  author       = {Hua Shi and Dan He and Fengmei Jin and Wen Hua and Jiwon Kim and Qilin Wang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2025.3607902},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey and experimental study on neural trajectory-user linking models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating spherical K-means clustering for large-scale sparse document data. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an accelerated spherical Kmeans clustering algorithm for large-scale and high-dimensional sparse document data sets. We design an algorithm working in an architecture-friendly manner (AFM), which is a way of suppressing performance-degradation factors such as the numbers of instructions, branch mispredictions, and cache misses in CPUs of a computer system. For the AFM operation, we leverage universal characteristics (UCs) of the data, which are skewed distributions on data relationships. The UCs indicate that the most part of multiplications for similarity calculations is executed on high-document-frequency terms and the most part of a similarity is obtained by the multiplications regarding a few high mean-feature values. To extract the foregoing specific region on terms and mean-feature values, we construct a mean-inverted index partitioned into three regions by two structural parameters. Our algorithm optimizes the parameters by minimizing the approximate number of the multiplications corresponding to the instructions based on our efficient pruning method, reduces conditional branches by sharing the index structure with all the objects, and keeps in the caches the frequently used data in the foregoing specific region. We experimentally demonstrate that our algorithm efficiently achieves superior speed performance in large-scale documents compared with algorithms using the stateof-the-art techniques},
  archive      = {J_TKDE},
  author       = {Kazuo Aoyama and Kazumi Saito},
  doi          = {10.1109/TKDE.2025.3608264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Accelerating spherical K-means clustering for large-scale sparse document data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atomic fact decomposition helps attributed question answering. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrievalbased AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decompositionbased Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric Attrp for evaluating the precision of evidence attribution.},
  archive      = {J_TKDE},
  author       = {Zhichao Yan and Jiapu Wang and Jiaoyan Chen and Xiaoli Li and Jiye Liang and Ru Li and Jeff Z. Pan},
  doi          = {10.1109/TKDE.2025.3608716},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Atomic fact decomposition helps attributed question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertain priors for graphical causal models: A multi-objective optimization perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3608723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning graphical causal models from observational data can effectively elucidate the underlying causal mechanism behind the variables. In the context of limited datasets, modelers often incorporate prior knowledge, which is assumed to be correct, as a penalty in single-objective optimization. However, this approach struggles to adapt complex and uncertain priors effectively. This paper introduces UpCM, which tackles the issue from a multi-objective optimization perspective. Instead of focusing exclusively on the DAG as the optimization goal, UpCM methodically evaluate the effect of uncertain priors on specific structures, merging data-driven and knowledge-driven objectives. Utilizing the MOEA/D framework, it achieve a balanced tradeoff between these objectives. Furthermore, since uncertain priors may introduce erroneous constraints, resulting in PDAGs lacking consistent extensions, the minimal non-consistent extension is explored. This extension, which separately incorporates positive and negative constraints, aims to approximate the true causality of the PDAGs. Experimental results demonstrate that UpCM achieves significant structural accuracy improvements compared to baseline methods. It reduces the SHD by 7.94%, 13.23%, and 12.8% relative to PC stable, GES, and MAHC, respectively, when incorporating uncertain priors. In downstream inference tasks, UpCM outperforms domain-expert knowledge graphs, owing to its ability to learn explainable causal relationships that balance data-driven evidence with prior knowledge},
  archive      = {J_TKDE},
  author       = {Zidong Wang and Xiaoguang Gao and Qingfu Zhang},
  doi          = {10.1109/TKDE.2025.3608723},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Uncertain priors for graphical causal models: A multi-objective optimization perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible keyword-aware top-k route search. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3609302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-k best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-k Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.},
  archive      = {J_TKDE},
  author       = {Ziqiang Yu and Xiaohui Yu and Yueting Chen and Wei Liu and Anbang Song and Bolong Zheng},
  doi          = {10.1109/TKDE.2025.3609302},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Flexible keyword-aware top-k route search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling temporal dependencies within the target for long-term time series forecasting. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3609415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term time series forecasting (LTSF) is a critical task across diverse domains. Despite significant advancements in LTSF research, we identify a performance bottleneck in existing LTSF methods caused by the inadequate modeling of Temporal Dependencies within the Target (TDT). To address this issue, we propose a novel and generic temporal modeling framework, Temporal Dependency Alignment (TDAlign), that equips existing LTSF methods with TDT learning capabilities. TDAlign introduces two key innovations: 1) a loss function that aligns the change values between adjacent time steps in the predictions with those in the target, ensuring consistency with variation patterns, and 2) an adaptive loss balancing strategy that seamlessly integrates the new loss function with existing LTSF methods without introducing additional learnable parameters. As a plug-and-play framework, TDAlign enhances existing methods with minimal computational overhead, featuring only linear time complexity and constant space complexity relative to the prediction length. Extensive experiments on six strong LTSF baselines across seven real-world datasets demonstrate the effectiveness and flexibility of TDAlign. On average, TDAlign reduces baseline prediction errors by 1.47% to 9.19% and change value errors by 4.57% to 15.78%, highlighting its substantial performance improvements.},
  archive      = {J_TKDE},
  author       = {Qi Xiong and Kai Tang and Minbo Ma and Ji Zhang and Jie Xu and Tianrui Li},
  doi          = {10.1109/TKDE.2025.3609415},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling temporal dependencies within the target for long-term time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-generation database interfaces: A survey of LLM-based text-to-SQL. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating accurate SQL from users' natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restrict the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summary and discuss the remaining challenges in this field and suggest expectations for future research directions.},
  archive      = {J_TKDE},
  author       = {Zijin Hong and Zheng Yuan and Qinggang Zhang and Hao Chen and Junnan Dong and Feiran Huang and Xiao Huang},
  doi          = {10.1109/TKDE.2025.3609486},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Next-generation database interfaces: A survey of LLM-based text-to-SQL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CryptIF: Towards cloud-based IoT anomaly detection over encrypted feature streams. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3609407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation and extensive use of the Internet of Things (IoT), it is vital to ensure the secure operation of IoT devices. However, due to limited computing power and lightweight system design, IoT devices often remain unprotected and are vulnerable to a myriad of attacks. Directly outsourcing the computationally intensive anomaly detection work to some middleboxes or cloud servers seems to solve this problem, but it can raise severe privacy concerns. To tackle these problems, we propose CryptIF, a scalable, privacy-preserving approach to detecting IoT anomalies from the cloud. By leveraging the Extended isolation forest (EIForest) and ciphertext comparison algorithms, CryptIF inspects features encrypted by fully homomorphic encryption (FHE) to detect various IoT anomalies. Furthermore, CryptIF parallelizes computing tasks by taking advantage of the single instruction, multiple data (SIMD) property of Cheon, Kim, Kim and Song (CKKS) homomorphic encryption to accelerate the detection process, thereby significantly increasing its scalability and operating efficiency. The evaluations demonstrate that CryptIF outperforms the state-of-the-art ciphertext-based anomaly detection approach in both detection accuracy and time efficiency. Additionally, CryptIF achieves comparable detection performance to plaintext-based IForest algorithms.},
  archive      = {J_TKDE},
  author       = {Teng Li and Zejian Lin and Yebo Feng and Chong Wang and Zhuo Ma and Bin Xiao and Jianfeng Ma and Yang Liu},
  doi          = {10.1109/TKDE.2025.3609407},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CryptIF: Towards cloud-based IoT anomaly detection over encrypted feature streams},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive information fusion-based concept drift learning for evolving multiple data streams. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3610184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift arises from unpredictable data distribution shifts, degrading model performance. In evolving multiple data streams, these drifts pose greater challenges due to dynamic changes and uncertain inter-stream correlations, demanding robust accuracy and generalization. To address this issue, in this article, we propose a novel multiple data stream learning method, called the adaptive information fusion-based concept drift learning (AIF-CD) method, to adaptively handle multiple data streams with heterogeneous feature spaces and complex drift situations. First, a real-time learning method with a cooperation scheme is proposed to handle multiple data streams. Second, an information fusion-based augmentation process is designed to help enhance the learning efficiency of each stream. Next, a drift severity identification-based adaptation strategy and a process to selectively use the previous timestamps' data are introduced to enhance learning robustness in both synchronous and asynchronous scenarios. Moreover, a detailed runtime complexity and theoretical analysis further explains the learning efficiency of our method. Our key innovation combines real-time adaptation with theoretical guarantees for complex, evolving multi-stream learning. The experiment results in various scenarios under synchronous and asynchronous settings show that the proposed method is more efficient than other benchmark methods.},
  archive      = {J_TKDE},
  author       = {Kun Wang and Jie Lu and Anjin Liu},
  doi          = {10.1109/TKDE.2025.3610184},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive information fusion-based concept drift learning for evolving multiple data streams},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering explainable artificial intelligence through case-based reasoning: A comprehensive exploration. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) advancements have significantly broadened its application across various sectors, simultaneously elevating concerns regarding the transparency and understandability of AI-driven decisions. Addressing these concerns, this paper embarks on an exploratory journey into Case-Based Reasoning (CBR) and Explainable Artificial Intelligence (XAI), critically examining their convergence and the potential this synergy holds for demystifying the decision-making processes of AI systems. We employ the concept of Explainable CBR (XCBR) system that leverages CBR to acquire case-based explanations or generate explanations using CBR methodologies to enhance AI decision explainability. Though the literature has few surveys on XCBR, recognizing its potential necessitates a detailed exploration of the principles for developing effective XCBR systems. We present a cycle-aligned perspective that examines how explainability functions can be embedded throughout the classical CBR phases: Retrieve, Reuse, Revise, and Retain. Drawing from a comprehensive literature review, we propose a set of six functional goals that reflect key explainability needs. These goals are mapped to six thematic categories, forming the basis of a structured XCBR taxonomy. The discussion extends to the broader challenges and prospects facing the CBR-XAI arena, setting the stage for future research directions. This paper offers design guidance and conceptual grounding for future XCBR research and system development.},
  archive      = {J_TKDE},
  author       = {Preeja Pradeep and Marta Caro-Martínez and Anjana Wijekoon},
  doi          = {10.1109/TKDE.2025.3609825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Empowering explainable artificial intelligence through case-based reasoning: A comprehensive exploration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEFS: Adaptive early feature selection for deep recommender systems. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3610351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of features plays an important role in the performance of recommender systems. Recognizing this, feature selection has emerged as a crucial technique in refining recommender systems. Recent advancements leveraging Automated Machine Learning (AutoML) has drawn significant attention, particularly in two main categories: early feature selection and late feature selection, differentiated by whether the selection occurs before or after the embedding layer. The early feature selection selects a fixed subset of features and retrains the model, while the late feature selection, known as adaptive feature selection, dynamically adjusts feature choices for each data instance, recognizing the variability in feature significance. Although adaptive feature selection has shown remarkable improvements in performance, its main drawback lies in its post-embedding layer feature selection. This process often becomes cumbersome and inefficient in large-scale recommender systems with billions of ID-type features, leading to a highly sparse and parameter-heavy embedding layer. To overcome this, we introduce Adaptive Early Feature Selection (AEFS), a very simple method that not only adaptively selects informative features for each instance, but also significantly reduces the activated parameters of the embedding layer. AEFS employs a dual-model architecture, encompassing an auxiliary model dedicated to feature selection and a main model responsible for prediction. To ensure effective alignment between these two models, we incorporate two collaborative training loss constraints. Our extensive experiments on three benchmark datasets validate the efficiency and effectiveness of our approach. Notably, AEFS matches the performance of current state-of-theart Adaptive Late Feature Selection methods while achieving a significant reduction of 37. 5% in the activated parameters of the embedding layer. We believe that this work opens up new possibilities for feature selection.},
  archive      = {J_TKDE},
  author       = {Fan Hu and Gaofeng Lu and Jun Chen and Channan Guo and Yuekui Yang and Xirong Li},
  doi          = {10.1109/TKDE.2025.3610351},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AEFS: Adaptive early feature selection for deep recommender systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDiffMAE: Guided diffusion enhanced mask graph AutoEncoder for recommendation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3611270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advancements using graph neural networks (GNNs) to capture complex user-item interactions, challenges persist due to data sparsity and noise. To address these, self-supervised learning (SSL) methods, particularly recent generative approaches, have gained attention due to their ability to augment graph data without requiring complex view constructions and unstable negative sampling. However, existing generative SSL solutions often focus on structural rather than semantic (refer to collaborative signals in recommendation scenarios) reconstruction, limiting their potential as comprehensive recommender. This paper explores the untapped potential of generative SSL for graph-based recommender systems. We highlight two critical challenges: firstly, designing effective diffusion mechanisms to enhance semantic information and collaborative signals while avoiding optimization biases; and secondly, developing adaptive structural masking mechanisms within graph diffusion to improve overall model performance. Motivated by these challenges, we propose a novel approach: the Guided Diffusion enhanced Mask graph AutoEncoder (GDiffMAE). GDiffMAE integrates an adaptive mask encoder for structural reconstruction and a guided diffusion model for semantic reconstruction, addressing the limitations of current methods. Experimental results on diverse datasets demonstrate that GDiffMAE consistently outperforms powerful baseline models, particularly in handling noisy data scenarios. By enhancing both structural and semantic dimensions through guided diffusion, our model advances the state-of-the-art in graph-based recommender systems.},
  archive      = {J_TKDE},
  author       = {Lei Zhang and Zihao Chen and Wuji Zhang and Hongke Zhao and Likang Wu},
  doi          = {10.1109/TKDE.2025.3611270},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GDiffMAE: Guided diffusion enhanced mask graph AutoEncoder for recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical data collection under input-discriminative local differential privacy. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input-discriminative local differential privacy (ID-LDP) protects user data with a different range of values, which improves the utility of the estimated data compared to traditional LDP. However, the existing ID-LDP methods are used for categorical data and cannot be directly applied to numerical data. In this paper, we propose a numerical data collection (NDC) framework with ID-LDP to provide discriminative protection for the data with different inputs. This framework uses a piecewise mechanism to divide the numerical data into several segments and designs two perturbation methods to minimize the mean value of numerical data based on values submitted by users. We first create an NDC-UE method that encodes the raw data into a binary vector. This method sets the uploaded data bit as 1 and the rest as zero and perturbs each bit with a given probability. We further propose an NDC-GRR algorithm to perturb the numerical data with an optimal privacy budget. To reduce the complexity of NDC-GRR, we apply a greedy algorithm-based spanner to shorten the computation time and improve the accuracy. Theoretical analysis proves that our schemes satisfy the definition of ID-LDP. Experimental results based on two real-world datasets and a synthetic dataset show that the proposed schemes have less mean square error compared with the benchmarks.},
  archive      = {J_TKDE},
  author       = {Youwen Zhu and Shibo Dai and Pengfei Zhang and Xiqi Kuang},
  doi          = {10.1109/TKDE.2025.3610932},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Numerical data collection under input-discriminative local differential privacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective explanation framework for graph neural networks. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3611170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) hold promise in various application domains, but their limited explainability hinders widespread adoption, impacting customer satisfaction and loyalty. This issue intensifies when addressing diverse explanation needs of different user groups. Current GNN explanation models focus on a single objective, neglecting varied and potential conflicting user requirements, resulting in suboptimal outcomes. Moreover, existing models prioritize explanation objectives during multi-objective explanations, disrupting the intrinsic hierarchical structures and distant relationships within the graphs, further diminishing their effectiveness. To tackle these challenges, this paper introduces a novel multi-objective explanatory framework with hierarchical structure attribution for GNNs, termed HM-Explainer. This framework constructs a multi-objective explanation generation module based on Pareto theory to balance different and potentially conflicting explanatory objectives. Additionally, to embed hierarchical information into explanations, HM-Explainer designs node-level and cluster-level attribution modules to analyze the impact of input data on GNN decisions hierarchically. Furthermore, a self-attention mechanism is integrated into the node-level attribution module to account for the influence of distant neighbors. Ultimately, the efficacy of HM-Explainer is validated across multiple datasets for different GNN models through experimentation.},
  archive      = {J_TKDE},
  author       = {Yibowen Zhao and Yonghui Xu and Di Wang and Yixin Zhang and Qingzhong Li and Lizhen Cui},
  doi          = {10.1109/TKDE.2025.3611170},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A multi-objective explanation framework for graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSC-DOLES: Multi-view subspace clustering in diverse orthogonal latent embedding spaces. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3610659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of Multi-view Subspace Clustering (MSC) in Latent Embedding Space (LES), existing methods aim to capture and leverage critical multi-view information by mapping it into a low-dimensional LES. However, several aspects can be further improved: (i) Fusion Strategy: Existing methods adopt either early fusion or late fusion to integrate multi-view information, limiting the effectiveness of the fusion. (ii) Diversity: Current methods often overlook the inherent diversity in the multi-view data by focusing on a single LES. (iii) Efficiency: LES-based methods exhibit high computational complexity, with cubic time and quadratic space requirements based on the number of samples. To address these issues, we propose a novel framework called MSC-DOLES (Multi-view Subspace Clustering in Diverse Orthogonal Latent Embedding Spaces), a novel framework designed to tackle these challenges. MSC-DOLES incorporates a two-stage fusion approach that generates and learns from multiple LES to maximize cross-view diversity. Orthogonality constraints on individual LES ensure view-internal diversity, resulting in a set of Diverse Orthogonal Latent Embedding Spaces (DOLES). The DOLES are then fused into a consensus anchor graph using learnable anchors. The final clustering is induced by partitioning the obtained graph without pre-processing. We develop an eight-step optimization algorithm for MSC-DOLES, which exhibits nearly linear time and space complexities relative to the number of samples. Extensive experiments demonstrate the superiority of MSC-DOLES over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yuan Fang and Geping Yang and Ruichu Cai and Yiyang Yang and Zhiguo Gong and Can Chen and Zhifeng Hao},
  doi          = {10.1109/TKDE.2025.3610659},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MSC-DOLES: Multi-view subspace clustering in diverse orthogonal latent embedding spaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive and dual adversarial representation learning for multi-view clustering. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3611368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-View Clustering (MVC) has gained increasing attention due to its ability to effectively leverage the complementary information of multi-view data. Despite the success of existing MVC methods in many real-world applications, they often overlook the discrepancy of view-specific latent distribution and struggle to ensure the completeness of the multi-view data. To address these challenges and harness the powerful feature extraction capability of deep networks, we propose a novel Contrastive and Dual Adversarial Representation Learning method for Multi-view Clustering, termed as CDARL, to solve multi-view clustering problems with both complete and incomplete multi-view data. Specifically, CDARL employs alternating adversarial and contrastive learning to align the view-specific representations, driving them into the same semantic latent space to minimize the discrepancy in view-specific distributions. In addition, a consensus latent representation is learned by an adaptive fusion block that integrates information from multiple views. The consensus representation is further refined through adversarial learning modeling the transformation of the standard Gaussian distribution to the original data distribution. Moreover, the proposed method incorporates an imputation strategy designed to handle the incomplete multi-view data clustering task. This strategy utilizes both reconstructed samples and cross-view neighbors to impute missing views from the latent space and the original space, thereby preserving clustering information, which ensures the quality and feasibility of the imputed samples. Experimental results on six widely used datasets have verified the competitiveness of the proposed CDARL method against state-of-the-art methods in MVC problems with complete and incomplete multi-view data. Code is available at https://github.com/xywy220/CDARL-MVC.},
  archive      = {J_TKDE},
  author       = {Yanwanyu Xi and Chang Tang and Jun-Jie Huang and Xingchen Hu and Yuanyuan Liu and Xinwang Liu},
  doi          = {10.1109/TKDE.2025.3611368},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive and dual adversarial representation learning for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversifying graph augmentation for learning to solve graph optimization problems. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3611663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many machine learning-based approaches that effectively solve graph optimization problems have been proposed. The graph optimization problem is the problem that aims to optimize (maximize or minimize) a quantity that is associated with a graph, such as the Minimum Vertex Cover (MVC) and Maximum Independent Set (MIS) problems. These approaches are usually trained on graphs randomly generated with graph generators or sampled from existing datasets. However, we observe that such training graphs lead to poor testing performance if the testing graphs are not generated analogously, i.e., the generalizability of the models trained on those randomly generated training graphs is very limited. To address this critical issue, in this paper, we propose a new framework, named Learning with Iterative Graph Diversification (LIGD), and formulate a new research problem, named Diverse Graph Modification Problem (DGMP), that iteratively generate diversified training graphs and train the models that solve graph optimization problems to improve their performance significantly. We propose three approaches to solve DGMP by considering both the performance of the machine learning approaches and the structural properties of the training graphs. In addition, we study a practical case of DGMP, named Diverse Graph Modification Problem with XOR Diversity (DGMP-XDiv), which considers an XOR-based diversity function. We propose a polynomial-time algorithm named Structure Diversifying Modification on Edge Score (DMES) to obtain the optimal solution. We also propose DMES with Efficiency-Boosting Strategies (DMES-EB) to enhance the efficiency of DMES significantly. Experimental results on well-known problems show that our proposed approaches significantly boost the performance of both supervised and reinforcement learning approaches. They produce near-optimal results and significantly outperform the baseline approaches, such as graph augmentation and diffusion-based approaches.},
  archive      = {J_TKDE},
  author       = {Bay-Yuan Hsu and Chen-Hsu Yang and Chia-Hsun Lu and Ming-Yi Chang and Lo-Yao Yeh and Chih-Ya Shen},
  doi          = {10.1109/TKDE.2025.3611663},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diversifying graph augmentation for learning to solve graph optimization problems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiTruss community search for multilayer graphs. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community search on multilayer graphs has significant applications in fields such as bioinformatics, social network analysis, and financial fraud detection, offering deeper insights compared to traditional community search on single-layer graphs. However, existing approaches often suffer from several key limitations, including inefficiency and a lack of flexibility in accommodating query requirements. To address these challenges, we investigate the problem of community search over large multilayer graphs. Specifically, we introduce a novel multilayer community model called PivotTruss Community (PiTC) with provably nice structural guarantees. We formalize the PiTC search (PiTCS) problem, which aims to efficiently identify personalized PiTCs for a given query vertex. To solve the PiTCS problem, we propose an efficient algorithm and design an elegant index to accelerate the search process. In addition, we propose a parameter recommendation method to improve the usability of PiTCS. To further optimize performance, we introduce a method to compact the index by making a trade-off between search time and index size. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Run-An Wang and Zhaonian Zou and Dandan Liu and Xudong Liu},
  doi          = {10.1109/TKDE.2025.3610998},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PiTruss community search for multilayer graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term urban flow prediction against data distribution shift: A causal perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3612033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for more precise and timely urban resource allocation and management has driven the extension of urban flow prediction from short-term to long-term horizons. As the time scale expands, the issue of urban flow distribution shift becomes increasingly prominent due to various impact factors, such as weather, events, city changes, etc. Traditionally, comprehensively analyzing and addressing the causal relationships underlying the distribution shift caused by these factors has been challenging. In this paper, we propose that these impact factors can be partitioned in two major types, i.e., context factors and structural factors. We then present a decomposition-based model for long-term urban flow prediction from a causal perspective, named DeCau, which can discriminate between the two types of factors for effectively solving the problem of urban flow distribution shift. First, we employ a decomposition module to decompose urban flow into seasonal part and trend part. The seasonal part contains high frequency irregular variations caused by context factors. We advise a shared distribution estimator to approximate the unavailable prior distributions of context factors, and then apply causal intervention to mitigate the confounding impact of context factors. The distribution shift in the trend part is induced by structural factors. We design a dual causal dependency extractor to model the causality between POIs distribution and urban flow, and then eliminate spurious correlations through causal adjustment. Finally, we design an end-to-end framework for long-term urban flow prediction by combining the embeddings from two parts, enabling the model to generalize to unseen distribution. Extensive experimental results demonstrate DeCau outperforms state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yuting Liu and Qiang Zhou and Hanzhe Li and Fuzhen Zhuang and Jingjing Gu},
  doi          = {10.1109/TKDE.2025.3612033},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Long-term urban flow prediction against data distribution shift: A causal perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural clustering for bipartite graphs. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3612290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graphs are widely used in many real-world applications, where discovering clusters is crucial for understanding their underlying structure. However, most existing clustering methods for bipartite graphs enforce the assignment of all vertices to clusters, often neglecting the important roles of outliers and hubs. To address this limitation, we plan to extend the structural clustering model from unipartite to bipartite graphs. This extension is non-trivial due to the lack of common neighbors in bipartite graphs, which renders traditional similarity measures less effective. Recognizing that similarity is key to structural clustering, we resort to butterflies—the fundamental building blocks of bipartite graphs—to define a more effective similarity measure. Building on this, we further propose a novel structural clustering model, SBC, tailored for bipartite graphs. To enable clustering under this model, we develop efficient online and index-based methods, along with a dynamic maintenance method to accommodate graph updates over time. Extensive experiments on real-world bipartite graphs demonstrate that: (1) The SBC model greatly enhances clustering quality, achieving higher modularity while effectively identifying outliers and hubs. (2) Our proposed clustering methods are highly scalable, enabling the processing of graphs with up to 12.2 million edges within 2 seconds},
  archive      = {J_TKDE},
  author       = {Mingyu Yang and Wentao Li and Wei Wang and Dong Wen and Min Gao and Lu Qin},
  doi          = {10.1109/TKDE.2025.3612290},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structural clustering for bipartite graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards balanced denoising: Building a structural and textual denoiser for table understanding. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3612217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large language models (LLMs) have made remarkable progress in table understanding, yet they remain vulnerable to the structural noise (SN) and the textual noise (TN). Existing methods usually employ biased denoising strategies such as structural matching and textual filtering, or overzealous denoising strategies such as introducing supplementary tasks like text-to-SQL and table-to-text to reduce these two types of noise. However, these methods either neglect one type of noise or introduce substantial external noise. Therefore, how to simultaneously mitigate the structural and textual noise without introducing extra noise and improve the performance of LLMs in table understanding is still an unresolved issue. In this paper, we rethink the bottlenecks in table understanding from the perspective of noise reduction and propose a novel dual-denoiser-reasoner model, called TabDDR, for balanced and effective denoising. Specially, our model consists of a structural-and-textual denoiser and a task-adaptive reasoner. The former removes two types of noise via triplet alignment and planning extraction to seek an interpretable balance between breaking structural barriers and preserving structural characteristics, eliminating textual noise and retaining maximal information; the latter ensures a simple but effective reasoning process which can adapt to various downstream tasks. To highlight the presence and impact of the structural and textual noise, we construct the WTQ-SN and WTQ-TN datasets based on the WikiTableQuestion (WTQ) dataset. Extensive experiments on these self-constructed datasets and two other public datasets demonstrate that our proposed method performs better than state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Shu-Xun Yang and Xian-Ling Mao and Yu-Ming Shang and Heyan Huang},
  doi          = {10.1109/TKDE.2025.3612217},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards balanced denoising: Building a structural and textual denoiser for table understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Answering subset query over multi-attribute data streams using hyper-USS. <em>TKDE</em>, 1-18. (<a href='https://doi.org/10.1109/TKDE.2025.3611502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate queries offer an efficient means of analyzing massive data streams under acceptable errors. Among these, subset queries over multiple attributes are common in many real-world applications. While sketches offer promising approximate solutions for massive data streams, efficiently supporting subset queries over multiple statistical attributes remains a significant challenge. To address this, we propose Hyper-USS, a novel sketching solution that accurately and efficiently supports subset queries over data streams involving multiple statistical attributes. With Joint Variance Optimization, Hyper-USS provides unbiased estimation and optimizes estimation variance jointly, addressing the challenge of accurately estimating multiple statistical attributes in the sketch design. The algorithm records the information of keys and all attributes in one sketch, ensuring high insertion efficiency. Furthermore, its three speed-optimized versions are introduced to handle the growing number of statistical attributes in data streams. Experimental results show that Hyper-USS and its three speed-optimized versions consistently surpass state-of-the-art methods that support subset queries in both estimation accuracy and insertion throughput. Specifically, Hyper-USS improves accuracy by at least 38%, while the algorithm and its three speed-optimized versions achieve throughput improvements of up to $31.90\times$, $45.31\times$, $49.21\times$, and $58.03\times$, respectively.},
  archive      = {J_TKDE},
  author       = {Ce Zheng and Zhouran Shi and Ruijie Miao and Wenpu Liu and Tong Yang and Bin Cui and Steve Uhlig},
  doi          = {10.1109/TKDE.2025.3611502},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Answering subset query over multi-attribute data streams using hyper-USS},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised entity alignment based on personalized discriminative rooted tree. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3607765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity Alignment (EA) is to link potential equivalent entities across different knowledge graphs (KGs). Most existing EA methods are supervised as they require the supervision of seed alignments, i.e., manually specified aligned entity pairs. Very recently, several EA studies have made some attempts to get rid of seed alignments. Despite achieving preliminary progress, they still suffer two limitations: (1) The entity embeddings produced by their GNN-like encoders lack personalization since some of the aggregation subpaths are shared between different entities. (2) They cannot fully alleviate the distribution distortion issue between candidate KGs due to the absence of supervised signals. In this work, we propose a novel unsupervised entity alignment approach called UNEA to address the above two issues. First, we parametrically sample a tree neighborhood rooted at each entity, and accordingly develop a tree attention aggregation mechanism to extract a personalized embedding for each entity. Second, we introduce an auxiliary task of maximizing the mutual information between the input and the output of the KG encoder, which serves as a regularization to prevent the distribution distortion. Extensive experiments show that our UNEA achieves a new state-of-the-art for the unsupervised EA task, and can even outperform many existing supervised EA baselines.},
  archive      = {J_TKDE},
  author       = {Yaming Yang and Zhe Wang and Ziyu Guan and Wei Zhao and Xinyan Huang and Xiaofei He},
  doi          = {10.1109/TKDE.2025.3607765},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised entity alignment based on personalized discriminative rooted tree},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online billboard auction with social welfare maximization. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3613148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor billboard advertising has proven effective for commercial promotions, attracting potential customers, and boosting product sales. Auction serves as a popular method for leasing billboard usage rights, enabling a seller to rent billboards to winning users for predefined periods according to their bids. An effective auction algorithm is of great significance to maximize the efficiency of the billboard ecosystem. In contrast to a rich literature on Internet advertising auctions, well-crafted algorithms tailored for outdoor billboard auctions remain rare. In this work, we investigate the problem of outdoor billboard auctions, in the practical setting where bids are received and processed on the fly. Our goal is to maximize social welfare, namely the total benefits of auction participants, including the billboard service provider and the bidding users. To this end, we first formulate the billboard social welfare maximization problem into an Integer Linear Problem (ILP), and then reformulate the ILP into a compact form with a reduced size of constraints (at the cost of involving exponentially many primal variables), based on which we derive the dual problem. Furthermore, we design a dual oracle to handle the exponentially many dual constraints, avoiding exhaustive enumeration. We present a primal-dual online algorithm with an incentive-compatible pricing mechanism. Theoretical analysis proves the individual rationality, incentive compatibility, and computational efficiency of our online algorithm. Extensive experimental results show that the online algorithm is both effective and efficient, and achieves a good competitive ratio.},
  archive      = {J_TKDE},
  author       = {Hao Huang and Mingxin Wang and Mengqi Shan and Zhigao Zheng and Ting Gan and Jiawei Jiang and Zongpeng Li},
  doi          = {10.1109/TKDE.2025.3613148},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online billboard auction with social welfare maximization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference aware item cold-start recommendation with hierarchical item alignment. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3613263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cold-start recommendation methods typically use item-level alignment strategies to align the content feature and collaborative feature of warm items during model training. However, these methods are less effective for cold items with low semantic similarity to the warm items when they first appear in the test stage, as they have no historical interactions to obtain the collaborative feature. In this paper, we propose a preference aware recommendation (PARec) model with hierarchical item alignment to solve the item cold-start issue. Our approach exploits user preference from historical records to achieve group-level alignment with item content feature, enhancing recommendation performance. Specifically, our hierarchical item alignment strategy improves recommendations for both high and low similarity cold items by using item-level alignment for high similarity cold items and introducing group-level alignment for low similarity cold items. Low similarity cold items can be successfully recommended through relationships among items, captured by our group-level alignment, based on their co-occurrence possibilities and semantic similarities. For model training, a hierarchical contrastive objective function is presented to balance the performance of warm and cold items, achieving better overall performance. Extensive experiments demonstrate the effectiveness of our method, with results showing its superiority compared to state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Wenbo Wang and Ben Chen and Bingquan Liu and Lili Shan and Chengjie Sun and Qian Chen and Feiyang Xiao and Jian Guan},
  doi          = {10.1109/TKDE.2025.3613263},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Preference aware item cold-start recommendation with hierarchical item alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FELEMN: Toward efficient feature-level machine unlearning for exact privacy protection. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3613659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data privacy protection legislation around the world has increasingly enforced the “right to be forgotten” regulation, generating a surge in research interest in machine unlearning (MU), which aims to remove the impact of training data from machine learning models upon receiving revocation requests from data owners. There exist two major challenges for the performance of MU: the execution efficiency and the inference interference. The former requires minimizing the computational overhead for each execution of the MU mechanism, while the latter calls for reducing the execution frequency to minimize interference with normal inference services. Nowadays most MU studies focus on the sample-level unlearning setting, leaving the other paramount feature-level setting under-explored. Adapting these existing techniques to the latter turns out to be non-trivial. The only known feature-level work achieves an approximate unlearning guarantee, but suffers from degraded model accuracy and still leaves the inference interference challenge unsolved. We are therefore motivated to propose FELEMN, the first FEature-Level Exact Machine uNlearning method that overcomes both of the above-mentioned hurdles. For the MU execution efficiency challenge, we explore the impact of different feature partitioning strategies on the preservation of semantic relationships for maintaining model accuracy and MU efficiency. For the inference interference challenge, we propose two batching mechanisms to combine as many individual unlearning requests to be processed together as possible, while avoiding potential privacy issues coming with falsely postponing unlearning requests, which is grounded on theoretical analysis. Experiments on five real datasets show that our FELEMN outperforms up-to-date competitors with up to $3\times$ speedup for each MU execution, and 50% runtime reduction by mitigating inference interference.},
  archive      = {J_TKDE},
  author       = {Zhigang Wang and Yizhen Yu and Mingxin Li and Jian Lou and Ning Wang and Yu Gu and Shen Su and Yuan Liu and Hui Jiang and Zhihong Tian},
  doi          = {10.1109/TKDE.2025.3613659},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FELEMN: Toward efficient feature-level machine unlearning for exact privacy protection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBF-VQA: Towards language bias-free visual question answering with multi-space collaborative debiasing. <em>TKDE</em>, 1-16. (<a href='https://doi.org/10.1109/TKDE.2025.3613421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA), aimed at improving AI-driven interactions and solving complex visual-linguistic tasks, has increasingly garnered attention as a pivotal research domain in both academic and industrial spheres. Despite progress in VQA, current studies still suffer from the challenge of language bias posed by spurious semantic correlations and minority class collapse, leading to semantic ambiguities and distribution shifts that hinder robust performance across challenging scenarios. To address these challenges, we propose a robust multi-space collaborative debiasing paradigm, termed “LBF-VQA” which systematically leverages multi-space collaborative debiasing strategies to achieve language bias-free VQA, encompassing both Euclidean space debiasing (ESD) and Spherical space debiasing (SSD). By strategically introducing bias-examples and their corresponding counter-examples, the ESD strategy focuses on uncovering hidden prior correlations and the complex interactions between modality and semantics within the Euclidean space. Benefiting from the infinite contrastive and distribution debiasing learning mechanisms, the SSD strategy is devoted to effectively preventing the collapse of minority classes while enhancing the manifold representations of instance de-bias and distribution de-dependence in the Spherical space. Furthermore, we meticulously constructed a specialized medical dataset intentionally embedded with deliberate language bias to comprehensively examine the negative effects of language bias on medical VQA systems. Extensive experiments on multiple general and medical VQA benchmarks consistently verify the effectiveness and generalizability of our LBF-VQA in handling various complex VQA scenarios than state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yishu Liu and Huanjia Zhu and Bingzhi Chen and Xiaozhao Fang and Guangming Lu and Shengli Xie},
  doi          = {10.1109/TKDE.2025.3613421},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LBF-VQA: Towards language bias-free visual question answering with multi-space collaborative debiasing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-task collaborative meta-learning for cold-start recommendations. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3613366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizer-based meta-learning, specifically model-agnostic meta-learning (MAML), has emerged as a powerful tool for tackling the cold-start recommendation problem. In these meta-learning-based methods, recommendations for individual users are typically treated as separate tasks and learned independently. However, this task-by-task learning paradigm presents several observable limitations. First, learning one task at a time ignores inter-task correlations, i.e., collaborative signals, which limits the meta-model's receptive field and prevents it from leveraging valuable shared information, ultimately leading to subpar performance. Second, the meta-model is susceptible to the task distribution, i.e., the varied preference distributions among different users, which in turn introduces biases and inconsistencies, resulting in a less robust model that may perform well on certain user groups while underperforming on others. In this paper, we explore the correlations among different tasks in cold-start recommendations and develop a novel strategy termed cross-task collaborative meta-learning (CCML). More specifically, we propose a collaborative task sampling module designed to mitigate the adverse impact of irrelevant tasks during meta-model learning. This module adaptively identifies tasks that are both similar and beneficial to the primary task, ensuring that the meta-model learns from relevant and supportive information. Additionally, to harness collaborative information across relevant tasks, we introduce a bi-level cross-task meta-training strategy. This strategy leverages multi-task learning to capture collaborative knowledge simultaneously and enhance user profiling with pertinent information. Extensive experiments on four public benchmark datasets demonstrate the advantages of CCML over many state-of-the-art cold-start recommendation methods. Our results show significant improvements in recommendation accuracy and robustness, highlighting the potential of cross-task collaboration in enhancing meta-learning-based recommender systems. The code is available at https://anonymous.4open.science/r/CCML-F064.},
  archive      = {J_TKDE},
  author       = {Yantong Du and Rui Chen and Qiaoyu Tan and Qilong Han and Shenjie Wang and Xiangyu Zhao},
  doi          = {10.1109/TKDE.2025.3613366},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-task collaborative meta-learning for cold-start recommendations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient distributed graph neural network training with source chunking and moving aggregation. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3613787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are effective models for analyzing graph-structured data, but encounter challenges when training on large distributed graphs. Existing GNN training frameworks use sampling parallelism and historical embedding methods to support distributed training and enhance efficiency. However, these methods suffer from issues like stale historical embeddings, imbalanced communication messages, and redundant storage and computation costs. In this paper, we present Emma, a distributed GNN training framework that incorporates source node centric chunking for frequent updates of embeddings and balanced communication, as well as a moving message aggregation technique to boost training efficiency and reduce storage costs. Experimental results show that Emma significantly enhances training efficiency by reducing computation and communication overhead, leading to a notable speedup while maintaining convergence accuracy compared to state-of-the-art distributed GNN training methods.},
  archive      = {J_TKDE},
  author       = {Wenjie Huang and Tongya Zheng and Rui Wang and Tongtian Zhu and Bingde Hu and Shuibing He and Mingli Song and Xinyu Wang and Sai Wu and Chun Chen},
  doi          = {10.1109/TKDE.2025.3613787},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient distributed graph neural network training with source chunking and moving aggregation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Address anomalies at critical crossroads for graph anomaly detection. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3613344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph anomaly detection (GAD) on attributed networks aims to capture abnormal nodes whose attributes or structures differ significantly from most nodes. The existing GAD models amplify the representation differences between normal and abnormal nodes to identify anomalies via carefully designed feature extraction modules. However, these models ignore the bottlenecks encountered by abnormal nodes in message passing. In particular, when the anomalies occurs at critical crossroads, the information of multiple nodes is compressed into a fixed-length representation, and the resulting over-squashing weakens the abnormal information. To address this, we propose an unsupervised STructural optimization model guided by sIMilarity reconstruction (STIM). Specifically, we define redundant edges that cause over-squashing, design the Neighbor-Structure Optimization module to filter redundant edges through the edge-dropping strategy based on critical crossroads, and optimize the graph structure to alleviate over-squashing. In addition, to alleviate the over-smoothing caused by the high inter-class node similarity of the data itself and the edge-dropping strategy, we design the Neighbor-Similarity Reconstruction module based on similarity calculation, which guides the model to expand inter-class variation. Extensive experiments on benchmark datasets show that STIM can effectively optimize message passing and improve anomaly detection performance. The source code is available at https://github.com/Junyi-Yan/STIM.},
  archive      = {J_TKDE},
  author       = {Junyi Yan and Enguang Zuo and Ke Liang and Meng Liu and Miaomiao Li and Xinwang Liu and Xiaoyi Lv and Kai Lu},
  doi          = {10.1109/TKDE.2025.3613344},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Address anomalies at critical crossroads for graph anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex formulation for fast semi-supervised learning. <em>TKDE</em>, 1-12. (<a href='https://doi.org/10.1109/TKDE.2025.3609886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a compromise between supervised and unsupervised learning, semi-supervised learning (SSL) harnesses both labeled and unlabeled data to enhance learning performance. Graph-based semi-supervised learning (GSSL) has emerged as a prominent approach owing to its versatility in representing sample interdependencies via graph structures. However, traditional GSSL methods face high time cost when computing matrix inverses, making them inefficient for large datasets. To address this, some researchers have introduced anchors as a bridge to accelerate the process. Nevertheless, most anchor-based models suffer from one or more of the following issues: (1) The anchor graph-based construction of the adjacency matrix has limitations; (2) The objective functions are typically non-convex, leading to local optima and requiring multiple runs to achieve good performance. To tackle these challenges, we develop a probability-driven approach to build the adjacency matrix, defining sample similarity as the probability of sharing the same anchor. Based on this strategy, we design a model (CFSL) with a strictly convex objective function, guaranteeing a globally optimal solution without iterative optimization. Experiments on multiple datasets indicate that our algorithm yields strong performance.},
  archive      = {J_TKDE},
  author       = {Xinyi Fan and Weizhong Yu and Feiping Nie and Zongcheng Miao and Xuelong Li},
  doi          = {10.1109/TKDE.2025.3609886},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A convex formulation for fast semi-supervised learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing global path planning via simple queries across multiple platforms. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3591460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of AI, big data, and mobile communication, intelligent transportation has become popular in recent years. Path planning is a typical topic of intelligent transportation, attracting significant attention from researchers. However, existing studies only focus on the path planning of a single platform, which may lead to unexpected traffic congestion. This is because multiple platforms can provide route planning services, the optimal planning calculated by one single platform may be not good in practice, since multiple platforms may lead the users to the same roads, which causes unexpected traffic congestion. Although in the view of each platform, the planning is optimal. Fortunately, with the rise of data sharing and cross-platform cooperation, the data silos between different platforms are gradually being broken. Based on this, we propose Cooperative Global Path Planning(CGPP) framework to overcome the above shortcoming. CGPP allows the path planning request target platform to send some queries to cooperative platforms to optimize its path planning results. Such queries should be “easy” enough to answer, and the query frequency should be small. Based on the above principle, we design a query decision model based on multi-agent reinforcement learning in CGPP framework to decide the query range and query frequency. We design action and reward specifically for the CGPP problem. Furthermore, we propose mechanisms to enhance query precision and reduce query overhead. Specifically, the Self-adjusting Query Area(SQA) concept allows refining query parameters, while the Query Reuse Optimization(QRO) algorithm aims to minimize the number of queries. To solve potential overestimation problems in queries, we propose a Distance-based Outer Query (DB-oq) and Distance-Based Vehicle Count Estimation (DB-VCE) Model. To address the issue that the time interval computed by the QRO algorithm might not fully adapt to dynamic traffic environments, we propose the Temporal Sequence Historical Integration for Time Interval Prediction(TSHI-TIP) algorithm. Extensive experiments on real and synthetic datasets confirm the effectiveness and efficiency of our algorithms.},
  archive      = {J_TKDE},
  author       = {Yurong Cheng and Xiaoxi Cui and Ye Yuan and Xiangmin Zhou and Guoren Wang},
  doi          = {10.1109/TKDE.2025.3591460},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing global path planning via simple queries across multiple platforms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing domain generalization for robust machine-generated text detection. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3581694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have revolutionized text generation, offering significant benefits while also posing threats to society, such as copyright infringement and misinformation. To prevent harmful use, the task of detecting machine-generated content has become an important research topic, though it remains particularly challenging across diverse content domains. This paper presents DGRM, an innovative add-on module designed to improve the domain generalization capability of existing machine-generated text detectors. Our model consists of two training components. (1) Feature disentanglement separates a text's embedding into target-specific and common attributes, thereby enhancing semantic domain generalization across different content domains. (2) Feature regularization applies constraints to these attributes to extract additional target-relevant information and ensure detection consistency under syntactic perturbations—thus achieving syntactic domain generalization. Evaluation over multiple datasets demonstrates that incorporating our module substantially improves the detection of machine-generated text across semantically and syntactically diverse domains. We hope our work contributes to mitigating the harmful use of language models.},
  archive      = {J_TKDE},
  author       = {Sungwon Park and Sungwon Han and Meeyoung Cha},
  doi          = {10.1109/TKDE.2025.3581694},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing domain generalization for robust machine-generated text detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining triangle-dense subgraphs of a fixed size: Hardness, lovasz extension and ´ applications. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2024.3444608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the triangle-densest-k-subgraph problem (TDkS) for undirected graphs: given a size parameter k, compute a subset of k vertices that maximizes the number of induced triangles. The problem corresponds to the simplest generalization of the edge-based densest-k-subgraph problem (DkS) to the case of higher-order network motifs. We prove that TDkS is NP-hard and is not amenable to efficient approximation, in the worst-case. By judiciously exploiting the structure of the problem, we propose a relaxation algorithm for the purpose of obtaining high-quality, sub-optimal solutions. Our approach utilizes the fact that the cost function of TDkS is submodular to construct a convex relaxation for the problem based on the Lovasz extension for submodular functions. We ´ demonstrate that our approaches attain state-of-the-art performance on real-world graphs and can offer substantially improved exploration of the optimal density-size curve compared to sophisticated approximation baselines for DkS. We use document summarization to showcase why TDkS is a useful generalization of DkS},
  archive      = {J_TKDE},
  author       = {Aritra Konar and Nicholas D. Sidiropoulos},
  doi          = {10.1109/TKDE.2024.3444608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mining triangle-dense subgraphs of a fixed size: Hardness, lovasz extension and ´ applications},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On regularization for explaining graph neural networks: An information theory perspective. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2024.3422328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the explainability of graph neural networks (GNNs), which is important for the credibility of GNNs in practical usage. Existing mask-based explanation methods mostly follow the two-phase paradigm to interpret a prediction: feature attribution and selection . However, another important component — regularization, which is crucial to facilitate the above paradigm-has been seldom studied. Regularization is pivotal in mask-based methods, serving to refine explanatory subgraphs through constraints imposed on mask values. Hence, in this work, we endevour to explore the role of regularization in GNNs explainability. As theoretical groundwork inspired by Graph Information Bottleneck (GIB), we first introduce an innovative principle, GIB tailored for explainability, termed GIBE. GIBE serves to consolidate existing mask-based explanation methods by establishing a unified optimization objective for both feature attribution and selection processes. Then, based on GIBE, our main findings can be summarized as: 1) regularization is essentially pursuing the balance between two phases, 2) its optimal coefficient is proportional to the sparsity of explanations, 3) existing methods imply an implicit regularization effect of stochastic mechanism, and 4) its contradictory effects on two phases are responsible for the out-of-distribution (OOD) issue in post-hoc explainability. Based on these findings, we propose two common optimization methods, which can bolster the performance of the current explanation methods via sparsity-adaptive and OOD-resistant regularization schemes. Extensive empirical studies validate our findings and proposed methods.},
  archive      = {J_TKDE},
  author       = {Junfeng Fang and Guibin Zhang and Kun Wang and Wenjie Du and Yifan Duan and Yuankai Wu and Roger Zimmermann and Xiaowen Chu and Yuxuan Liang},
  doi          = {10.1109/TKDE.2024.3422328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On regularization for explaining graph neural networks: An information theory perspective},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperbolic contrastive graph representation learning for session-based recommendation. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2023.3295063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) learns users' preferences by capturing the short-term and sequential patterns from the evolution of user behaviors. Among the studies in the SBR field, graph-based approaches are a relatively powerful technique, which generally extract item information by message aggregation under Euclidean space. However, such methods cannot effectively extract the hierarchical information contained among consecutive items in a session, which is critical to represent users' preferences. In this paper, we present a hyperbolic contrastive graph recommender (HCGR), a principled session-based recommendation framework involving Lorentz hyperbolic space to efficiently capture the coherence and hierarchical representations of the items. Within this framework, an adaptive hyperbolic attention computation is designed to aggregate the graph message of each user's preference in a session-based behavior sequence. In addition, a contrastive ranking loss with the hyperbolic distance is used to separate the positive and negative items. Compared with the Euclidean distance, the advantage of the hyperbolic distance in the contrastive ranking loss is analyzed. The results of extensive experiments on four real-world datasets demonstrate that HCGR consistently outperforms state-of-the-art baselines by 0.43 $\%$ -14.44 $\%$ in terms of $HitRate$ , $NDCG$ and $MRR$ .},
  archive      = {J_TKDE},
  author       = {Naicheng Guo and Xiaolei Liu and Shaoshuai Li and Mingming Ha and Qiongxu Ma and Binfeng Wang and Yunan Zhao and Linxun Chen and Xiaobo Guo},
  doi          = {10.1109/TKDE.2023.3295063},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic contrastive graph representation learning for session-based recommendation},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2018). Notice of retraction: CHN: An efficient algorithm for mining closed high utility itemsets with negative utility. <em>TKDE</em>, 1. (<a href='https://doi.org/10.1109/TKDE.2018.2882421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retracted.},
  archive      = {J_TKDE},
  author       = {Kuldeep Singh and Shashank Sheshar Singh and Ajay Kumar and Harish Kumar Shakya and Bhaskar Biswas},
  doi          = {10.1109/TKDE.2018.2882421},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  pages        = {1},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Notice of retraction: CHN: An efficient algorithm for mining closed high utility itemsets with negative utility},
  year         = {2018},
}
</textarea>
</details></li>
<li><details>
<summary>
(2009). Cost optimization of the supply chain network using genetic algorithms - Withdrawn. <em>TKDE</em>, 1. (<a href='https://doi.org/10.1109/TKDE.2009.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Withdrawn.},
  archive      = {J_TKDE},
  author       = {Henry C. W. Lau and T. M. Chan and Wan Ting Tsui and G.T.S. Ho},
  doi          = {10.1109/TKDE.2009.20},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  pages        = {1},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cost optimization of the supply chain network using genetic algorithms - Withdrawn},
  year         = {2009},
}
</textarea>
</details></li>
</ul>

</body>
</html>
