<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds">TCDS - 4</h2>
<ul>
<li><details>
<summary>
(2025). Graph reinforcement learning-based reachability map for generalized mobile manipulation. <em>TCDS</em>, 1-14. (<a href='https://doi.org/10.1109/TCDS.2025.3605388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile manipulators need to determine feasible navigation positions before manipulation tasks. Real-world environments, with varying obstacles and objects, pose significant challenges for computing optimal navigation positions due to their variability. In this work, a novel method named Graph Reinforcement Learning-based Reachability Map (GRAM) is proposed. First, GRAM uses a graph attention network (GAT) to capture the spatial relationships between objects. Then, it leverages the Q-value from the pre-trained critic network to generate the reachability map. The reachability map is integrated into navigation policies for long-horizon tasks, effectively solving the skill transition problems. Extensive simulation and real-world experiments were conducted on the Fetch mobile robot platform. The results demonstrate the superiority of GRAM, with simulation results showing an average 16.3% performance improvement over the baseline in four flexible environments. In long-horizon tasks, GRAM’s overall task success rate improved by 4.2%. The project is open-source at https://github.com/nubotnudt/Grand RM.},
  archive      = {J_TCDS},
  author       = {Lu Jiang and Junkai Ren and Zhiqian Zhou and Yuke Qu and Huimin Lu and Meiping Wu},
  doi          = {10.1109/TCDS.2025.3605388},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph reinforcement learning-based reachability map for generalized mobile manipulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic and instance information interactive mining for semi-supervised deep facial expression recognition. <em>TCDS</em>, 1-10. (<a href='https://doi.org/10.1109/TCDS.2025.3605921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Deep Facial Expression Recognition (SS-DFER) has gained significant attention due to its utilization of large amounts of unlabeled data. However, SS-DFER faces two main problems, ambiguity in facial semantics caused by noisy labels and poor feature representation capabilities of models. In this paper, we propose a novel SS-DFER method based on Semantic-level and Instance-Level information Interactive Mining, namely SILIM, to simultaneously address both problems. Specifically, the model generates pseudo-labels for semantics and instances through the interaction of semantic and instance information, achieving matching at both levels to fully exploit image feature information. In addition, we construct a memory buffer that stores all instances of labeled data, enabling interaction between semantic pseudo-labels and instance pseudo-labels. For this, we design a neighbor-node-based instance space optimization strategy. This prevents degradation in module’s feature representation ability caused by pushing away instances of the same image category, thereby optimizing decision boundaries. Experiments on four challenging facial expression datasets show that our method significantly outperforms the second-best state-of-the-art SS-DFER method and surpasses the fully-supervised baselines.},
  archive      = {J_TCDS},
  author       = {Qi Zhou and Yuanyuan Xu and Deng Xiong and Xi Wu and Jiliu Zhou and Yan Wang},
  doi          = {10.1109/TCDS.2025.3605921},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Semantic and instance information interactive mining for semi-supervised deep facial expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PIT-NBV: Poisson-informed transformer for 6-DOF next best view planning in 3D object reconstruction with narrow field of view. <em>TCDS</em>, 1-14. (<a href='https://doi.org/10.1109/TCDS.2025.3606221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object reconstruction utilizes multi-view information to capture the three-dimensional geometry of objects. High-precision scanners used in object reconstruction face challenges, including a narrow field of view, shallow sensing depth, and self-occlusion. Hence, this study proposes a novel Next-Best-View (NBV) selection algorithm called a Poisson-informed Transformer for NBV (PIT-NBV). The algorithm combines the strengths of both Poisson reconstruction-based and deep learning-based approaches. Our framework introduces a 6-Degree of Freedom (DOF) NBV selection mechanism, designed to enhance surface detail capture and mitigate self-occlusion, unlike previous deep learning approaches that operate within 2-DOF spherical view spaces. The proposed method incorporates a View Constraint Block (VCB) to ensure collision-free 6-DOF viewpoint selection and high-quality data for sensors with limited sensing depths. Additionally, we introduce Point Cloud Transformer-View (PCTV), an enhanced PCT specifically tailored for efficient NBV search. Experimental evaluations conducted on the ShapeNet, Stanford, and MIT CSAIL datasets demonstrate the superior performance of our approach. The proposed method achieved reconstruction quality comparable to Poisson reconstructionbased approaches, offering inference speeds more than 50 times faster. In addition to synthetic benchmarks, real-world experiments using a high-precision structured light scanner and robotic manipulator demonstrate the feasibility of deploying PIT-NBV in real-world applications. These results suggest that PIT-NBV has the potential for applications in robotic vision, automated inspection, and digital archiving, where rapid and accurate 3D reconstruction is essential.},
  archive      = {J_TCDS},
  author       = {Doyu Lim and Chaewon Park and Joonhee Kim and Junwoo Hong and Soohee Han},
  doi          = {10.1109/TCDS.2025.3606221},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {PIT-NBV: Poisson-informed transformer for 6-DOF next best view planning in 3D object reconstruction with narrow field of view},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPECI: Skill prompts based hierarchical continual imitation learning for robot manipulation. <em>TCDS</em>, 1-15. (<a href='https://doi.org/10.1109/TCDS.2025.3610492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world robot manipulation in dynamic unstructured environments requires lifelong adaptability to evolving objects, scenes and tasks. Traditional imitation learning relies on static training paradigms, which are ill-suited for lifelong adaptation. Although Continual Imitation Learning (CIL) enables incremental task adaptation while preserving learned knowledge, current CIL methods primarily overlook the intrinsic skill characteristics of robot manipulation or depend on manually defined and rigid skills, leading to suboptimal cross-task knowledge transfer. To address these issues, we propose Skill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel end-to-end hierarchical CIL policy architecture for robot manipulation. The SPECI framework consists of a multimodal perception and fusion module for heterogeneous sensory information encoding, a high-level skill inference module for dynamic skill extraction and selection, and a low-level action execution module for precise action generation. To enable effective knowledge transfer on both skill and task levels, SPECI performs continual implicit skill acquisition and reuse via an expandable skill codebook and an attention-driven skill selection mechanism. Furthermore, we introduce Mode Approximation to augment the last two modules with task-specific and task-shared parameters, thereby enhancing task-level knowledge transfer. Extensive experiments on diverse manipulation task suites demonstrate that SPECI consistently outperforms state-of-the-art CIL methods across all evaluated metrics, revealing exceptional bidirectional knowledge transfer and superior overall performance. Implementation code are available at: https://github.com/Triumphant-strain/SPECI.},
  archive      = {J_TCDS},
  author       = {Jingkai Xu and Xiangli Nie},
  doi          = {10.1109/TCDS.2025.3610492},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SPECI: Skill prompts based hierarchical continual imitation learning for robot manipulation},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
