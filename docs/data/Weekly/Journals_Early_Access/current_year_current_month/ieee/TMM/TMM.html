<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm">TMM - 136</h2>
<ul>
<li><details>
<summary>
(2025). Instructive probabilistic transformer for complex action recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex action recognition aims to identify multiple actions over a long time. Multiple actions may occur at the same time (defined as simultaneous actions), and may occur after each other (defined as each action) Complex action recognition may suffer from two challenges. (1) Temporal repeated bias. The same action may repeat in a temporal duration. In this duration, the prediction may be biased to the majority of actions, which occur repeatedly in the past temporal frames. (2) Epistemic uncertainty of multiple actions. When there are multiple simultaneous actions in one frame, this frame's feature may result in the distribution of multiple actions overlapping each other. Without modeling proper relations between actions, the model may hinder accurately explaining certain categories in multiple actions (defined as the model's epistemic uncertainty). In this work, we propose an Instructive Probabilistic Transformer, which contains a probabilistic temporal memorizer, and a probabilistic prototype Transformer. First, to alleviate temporal repeated bias, we design a probabilistic temporal memory module, which learns probabilistic temporal gates to localize each action. The probabilistic gates instruct the selective memory of each action in long-term frames. Second, we cluster features to capture common action semantics among features (defined as action prototypes). To alleviate the epistemic uncertainty of multiple actions, we design a probabilistic prototype Transformer module. This module learns probabilistic relations depending on each prototype, which can ensure the separation between different prototypes. Third, to ensure the proper probabilistic relations depending on each prototype, we extend action loss with distribution loss to learn uncertainty-aware action loss. In uncertainty-aware action loss, the distribution loss measures the consistency between probabilistic relations and prototype relation distribution. The prediction uncertainty is learned by analyzing the entropy of multiple predictions, and helps to ensure the effect between action loss and distribution loss. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Charades, Breakfast Actions, and MultiTHUMOS.},
  archive      = {J_TMM},
  author       = {Zhao Xie and Longsheng Lu and Kewei Wu and Zhehan Kan and Xingming Yang and Dan Guo},
  doi          = {10.1109/TMM.2025.3599089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instructive probabilistic transformer for complex action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AS-GCL: Asymmetric spectral augmentation on graph contrastive learning. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.},
  archive      = {J_TMM},
  author       = {Ruyue Liu and Rong Yin and Yong Liu and Xiaoshuai Hao and Haichao Shi and Can Ma and Weiping Wang},
  doi          = {10.1109/TMM.2025.3604953},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AS-GCL: Asymmetric spectral augmentation on graph contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards semi-supervised dual-modal semantic segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D and 2D data acquisition techniques, it has become easy to obtain point clouds and images of scenes simultaneously, which further facilitates dual-modal semantic segmentation. Most existing methods for simultaneously segmenting point clouds and images rely heavily on the quantity and quality of the labeled training data. However, massive point-wise and pixel-wise labeling procedures are time-consuming and labor-intensive. To address this issue, we propose a parallel dual-stream network to handle the semi-supervised dual-modal semantic segmentation task, called PD-Net, by jointly utilizing a small number of labeled point clouds, a large number of unlabeled point clouds, and unlabeled images. The proposed PD-Net consists of two parallel streams (called original stream and pseudo-label prediction stream). The pseudo-label prediction stream predicts the pseudo labels of unlabeled point clouds and their corresponding images. Then, the unlabeled data is sent to the original stream for self-training. Each stream contains two encoder-decoder branches for 3D and 2D data respectively. In each stream, multiple dual-modal fusion modules are explored for fusing the dual-modal features. In addition, a pseudo-label optimization module is explored to optimize the pseudo labels output by the pseudo-label prediction stream. Experimental results on two public datasets demonstrate that the proposed PD-Net not only outperforms the comparative semi-supervised methods but also achieves competitive performances with some fully-supervised methods in most cases.},
  archive      = {J_TMM},
  author       = {Qiulei Dong and Jianan Li and Shuang Deng},
  doi          = {10.1109/TMM.2025.3604939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards semi-supervised dual-modal semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high resolution (UHR) image segmentation is a challenging task that requires efficient processing of large images while maintaining high accuracy. Existing approaches usually employ both shallow and deep networks to extract high-resolution details and global context from different-resolution inputs, achieving a balance between performance, memory, and speed. However, these methods still rely on preserving relatively high-resolution features within the deep network, leading to increased time and memory costs. This also indicates that the full potential of the high-resolution information from the shallow network remains underexplored. To address this, we propose a novel framework called the Hierarchical Grafting Network (HGN), wherein the shallow network is hierarchically grafted to the deep network from multiple perspectives, enabling comprehensive utilization of the features from the shallow network. Our framework involves carefully designed global structure aggregated grafting and local structure aligned grafting mechanism, which progressively integrate semantic details and spatial structure from the shallow network to the deep network. In addition, to enhance the discriminative power of the high-resolution local features extracted by the shallow network, we introduce a shallow-deep contrastive loss to encourage the shallow network to learn semantically similar features to those of the deep network. Extensive experiments on several UHR image segmentation datasets demonstrate that our approach outperforms state-of-the-art UHR methods. The results demonstrate an overall improvement in terms of memory efficiency, accuracy, and speed.},
  archive      = {J_TMM},
  author       = {Ting Liu and Jing Yang and Shikui Wei and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3604913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crafting more transferable adversarial examples via quality-aware transformation combination. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input diversity is an effective technique for crafting transferable adversarial examples that can deceive unknown AI models. Existing input-diversity-based methods typically use single input transformation, limiting targeted transferability and defense robustness. Combining different transformation types is challenging, as keeping increasing types would degrade semantic information and targeted transferability. This paper proposes a quality-aware transformation combination attack (TCA) that selects high-quality transformation combinations. The quality-aware selection enables expansion of transformation types, enhances input diversity, and hence improves targeted transferability and defense robustness. We first design a quality-evaluation framework to quantify the effectiveness of transformation combinations, which jointly considers convergence, transferability, and robustness. Only a small group (up to 10) of images are required for computation-efficient quality evaluation. Experiments validate TCA's superiority over state-of-the-art baselines in adversarial transferability and robustness. When defenses are secured, the average targeted success rate of TCA with four transformation types (i.e., TCA-t4) outperforms the best baseline by 26%$\sim$42% on ImageNet.},
  archive      = {J_TMM},
  author       = {Junlin Liu and Xinchen Lyu and Chenshan Ren and Qimei Cui},
  doi          = {10.1109/TMM.2025.3604967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Crafting more transferable adversarial examples via quality-aware transformation combination},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code is made publicly available at https://github.com/hangyu94/KE2NT.},
  archive      = {J_TMM},
  author       = {Hangyu Li and Yihan Xu and Jiangchao Yao and Nannan Wang and Xinbo Gao and Bo Han},
  doi          = {10.1109/TMM.2025.3604916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement-based equivariant learning for compositional VQA. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional visual question answering (VQA) represents a challenging yet fundamental task that requires models to comprehend novel combinations of previously learned concepts. The current methods often overlook the disentanglement of underlying concepts and are restricted in terms of their ability to effectively capture the compositional variation mechanism. Moreover, the state-of-the-art techniques depend on additional clues for training, which is not feasible in real-world VQA scenarios. To address these issues, in this paper, we introduce a novel Disentanglement-based EquivAriant Learning (DEAL) framework for compositional VQA, which is guided exclusively by ground-truth answers. In DEAL, we employ causality-inspired interventions to disentangle concepts derived from visual and textual inputs within a re-encoding framework. Based on the principle of equivariance, we subsequently perform a compositional transformation on the inference input and impose the equivariant constraint on the output to augment the compositional reasoning capacity of the model. Comprehensive experiments conducted on the benchmark CLEVR-CoGenT and GQA-SGL datasets validate the superiority of our proposed DEAL approach over the existing state-of-the-art methods for compositional VQA tasks in both visual and linguistic generalization settings.},
  archive      = {J_TMM},
  author       = {Zhou Du and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604897},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentanglement-based equivariant learning for compositional VQA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient SDRTV-to-HDRTV by learning from image formation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary display enables video content rendering with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of existing content remains in standard dynamic range (SDR) format. Therefore, the conversion of SDR content to HDRTV standards holds significant value. This paper delineates and analyzes the SDRTV-to-HDRTV conversion by modeling the formation of SDRTV/HDRTV content. The findings reveal that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this, we propose a new three-step solution called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step utilizes global statistics for image-adaptive color adjustments, followed by a local enhancement network for detail improvement. These two components are integrated as a generator, with GAN-based joint training ensuring highlight consistency. Our method, tailored for ultra-high-definition TV content, offers both effectiveness and computational efficiency in processing 4K resolution images. We also construct HDRTV1K, a dataset comprising HDR videos adhering to the HDR10 standard, featuring 1235 training and 117 testing images at 4K resolution. Furthermore, we employ five metrics to assess SDRTV-to-HDRTV performance. Our results demonstrate state-of-the-art performance both quantitatively and visually. The codes and models are available at https://github.com/xiaom233/HDRTVNet-plus.},
  archive      = {J_TMM},
  author       = {Xiangyu Chen and Zheyuan Li and Zhengwen Zhang and Jimmy S. Ren and Yihao Liu and Jingwen He and Yu Qiao and Jiantao Zhou and Chao Dong},
  doi          = {10.1109/TMM.2025.3604961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient SDRTV-to-HDRTV by learning from image formation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised hyperspectral image (HSI) clustering remains a fundamental yet challenging task due to the absence of labeled data and the inherent complexity of spatial-spectral interactions. While recent advancements have explored innovative approaches, existing methods face critical limitations in clustering accuracy, feature discriminability, computational efficiency, and robustness to noise, hindering their practical deployment. In this paper, a self-supervised efficient low-pass contrastive graph clustering (SLCGC) is introduced for HSIs. Our approach begins with homogeneous region generation, which aggregates pixels into spectrally consistent regions to preserve local spatial-spectral coherence while drastically reducing graph complexity. We then construct a structural graph using an adjacency matrix A and introduce a low-pass graph denoising mechanism to suppress high-frequency noise in the graph topology, ensuring stable feature propagation. A dual-branch graph contrastive learning module is developed, where Gaussian noise perturbations generate augmented views through two multilayer perceptrons (MLPs), and a cross-view contrastive loss enforces structural consistency between views to learn noise-invariant representations. Finally, latent embeddings optimized by this process are clustered via K-means. Extensive experiments and repeated comparative analysis have verified that our SLCGC contains high clustering accuracy, low computational complexity, and strong robustness. The code source will be available at https://github.com/DY-HYX.},
  archive      = {J_TMM},
  author       = {Yao Ding and Zhili Zhang and Aitao Yang and Yaoming Cai and Xiongwu Xiao and Danfeng Hong and Junsong Yuan},
  doi          = {10.1109/TMM.2025.3604954},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional quality assessment for text-to-3D assets: Dataset and model. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively. Specifically, the proposed method utilizes the projection videos of text-to-3D assets to extract 3D shape, texture and text-asset correspondence features, then fuses them to calculate the final three preference scores respectively. Extensive experimental results demonstrate the effectiveness of the proposed T23DAQA method in evaluating the quality of AI generated 3D asset, which is more consistent with human perception. To the best of our knowledge, this is the first work that studies the problem of text-guided 3D generation quality assessment, and our database and codes will be released to facilitate future research.},
  archive      = {J_TMM},
  author       = {Kang Fu and Huiyu Duan and Zicheng Zhang and Xiaohong Liu and Xiongkuo Min and Jia Wang and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3604905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-dimensional quality assessment for text-to-3D assets: Dataset and model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of ‘Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in $2.55\% / 5.7\%$ under different settings, compared with the existing methods.},
  archive      = {J_TMM},
  author       = {Ming-Hui Liu and Xiao-Qian Liu and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TMM.2025.3604932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a weakly supervised referring image segmentation method, named CollabLearn, that segments objects described by free-form referring expression utilizing solely image-text pairs. Existing methods suffer from incorrect localization of referring expressions due to the lack of high-level semantics in cross-modal alignment or rough segmentation of referenced objects stemming from the absence of low-level details. To address these issues, we propose an innovative framework for generating cross-modal features encompassing both high-level semantics and low-level details via two fusion modules: a semantic awareness module and a detail cognition module. Each of these modules generates an activation map, and they mutually correct each other through a collaborative learning strategy. Specifically, the semantic awareness module performs in-depth cross-modal interaction and achieves accurate localization in a top-down manner. The detail cognition module facilitates the segmentation of entire objects in a bottom-up manner. A collaborative learning strategy is designed to enable interaction between these two modules, enforcing sufficient vision-language alignment. Experiments on three benchmarks demonstrate that CollabLearn consistently outperforms state-of-the-art weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Chao Jiang and Yuqiu Kong and Mengnan Zhao and Lihe Zhang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3604944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced head: Exploring strong detection heads with vision transformer. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial component of object detectors, current detection heads often lack the capability to effectively utilize contextual information, adapt to deformable objects, and align features and tasks. However, most existing methods prioritize a single capability, lacking comprehensive approaches to introduce them simultaneously. In this paper, we propose the Enhanced Head to integrate the above three capabilities into the detectors concurrently. Specifically, we propose three attention blocks with linear complexity: Global Concentrated Attention (GCA), Local Deformable Cross-Task Attention (LDCA), and Boundary-Aware Cross-Task Attention (BACA). The GCA captures long-range dependencies efficiently by employing Spatial Information Concentration (SIC). The LDCA improves feature alignment and deformation adaptability by enabling local deformable cross-task feature interactions. The BACA aligns classification features with localization results, enhancing task alignment and further improving deformation adaptability through a region-deformable interaction scheme. We implement Enhanced Head as a plug-and-play detection head and evaluate its effectiveness through extensive experiments on the MS COCO and VisDrone datasets. For instance, on the COCO detection benchmark, our Enhanced Head achieves +3.6 AP gain for FSAF, +3.3 AP for RetinaNet, and +2.9 AP for ATSS while reducing the FLOPs.},
  archive      = {J_TMM},
  author       = {Zewen Du and Zhenjiang Hu and Guiyu Zhao and Ying Jin and Hongbin Ma},
  doi          = {10.1109/TMM.2025.3604917},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced head: Exploring strong detection heads with vision transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual high-order total variation model for underwater image restoration. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are typically characterized by color cast, haze, blurring, and uneven illumination due to the selective absorption and scattering when light propagates through the water, which limits their practical applications. Underwater image enhancement and restoration (UIER) is one crucial mode to improve the visual quality of underwater images. However, most existing UIER methods concentrate on enhancing contrast and dehazing, rarely pay attention to the local illumination differences within the image caused by illumination variations, thus introducing some undesirable artifacts and unnatural color. To address this issue, an effective variational framework is proposed based on an extended underwater image formation model (UIFM). Technically, dual high-order regularizations are successfully integrated into the variational model to acquire smoothed local ambient illuminance and structure-revealed reflectance in a unified manner. In our proposed framework, the weight factors-based color compensation is combined with the color balance to compensate for the attenuated color channels and remove the color cast. In particular, the local ambient illuminance with strong robustness is acquired by performing the local patch brightest pixel estimation and an improved gamma correction. Additionally, we design an iterative optimization algorithm relying on the alternating direction method of multipliers (ADMM) to accelerate the solution of the proposed variational model. Considerable experiments on three real-world underwater image datasets demonstrate that the proposed method outperforms several state-of-the-art methods with regard to visual quality and quantitative assessments. In the quantitative assessments, the proposed method achieves average scores of 0.205 FADE, 7.688 Entropy, 0.628 UCIQE, and 0.775 FDUM across the UIEB and UIQS datasets. Moreover, the proposed method can also be extended to outdoor image dehazing and low-light image enhancement tasks. The code is available at https://github.com/HouGuojia/UDHTV.},
  archive      = {J_TMM},
  author       = {Yuemei Li and Guojia Hou and Peixian Zhuang and Zhenkuan Pan},
  doi          = {10.1109/TMM.2025.3604900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual high-order total variation model for underwater image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FontGuard: A robust font watermarking approach leveraging deep font knowledge. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard.},
  archive      = {J_TMM},
  author       = {Kahim Wong and Jicheng Zhou and Kemou Li and Yain-Whar Si and Xiaowei Wu and Jiantao Zhou},
  doi          = {10.1109/TMM.2025.3604908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FontGuard: A robust font watermarking approach leveraging deep font knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the behavior of contrastive regularization in improving chinese text recognizer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense representation space in Chinese scene text recognition (STR) makes discriminating between categories highly challenging, because of the large candidate category set. Mainstream STR methods have achieved remarkable advancements by leveraging linguistic knowledge to implicitly address this challenge. In this paper, inspired by the correlation between recognizer performance and the distributional properties of character representations, as well as the inherent consistency between this correlation and supervised contrastive learning (SupCon), we thoroughly investigate how to integrate SupCon with an STR model to alleviate this challenge, and elucidate some dynamic behaviors underlying the performance improvements. Specifically, we analyze the SupCon-STR models instantiated with different projectors and evaluate their distributional properties through metrics, including intra-class compactness, inter-class separability, and feature redundancy, while assessing performances that involve in-domain accuracy and cross-domain recognition generalization. The main results reveal how the temperature $\tau$ and projectors affect the representation distribution, and highlight that suitable intra-class compactness and sufficient inter-class separability are key factors for delivering competitive performances in both in-domain and cross-domain STR scenarios. Moreover, these results also provide valuable insights into the design of SupCon-STR architectures for diverse resource constraints. Taking existing Chinese STR models as baselines, and combining SupCon-STR with them, the average improvements in cross-domain recognition performance are over 5% across 7 testing datasets. A new state-of-the-art accuracy of 77.19% on the Chinese Scene benchmark is also established.},
  archive      = {J_TMM},
  author       = {Dekang Liu and Tianlei Wang and Huanqiang Zeng and Jiuwen Cao},
  doi          = {10.1109/TMM.2025.3604892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {On the behavior of contrastive regularization in improving chinese text recognizer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep frequency-separable temporal network for efficient video denoising. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively explore inter-frame information is critical for video denoising. Existing methods often rely on complex architectures, such as optical flow estimation and cross-frame self-attention, which introduce high computational costs and limit their practicality in real-world scenarios. To address this limitation, we propose a simple yet efficient deep Frequency-Separable Temporal Network (FSTN) for video denoising. FSTN utilizes the multi-scale analysis capability of wavelet transform to extract high-frequency and low-frequency information at the feature level, enabling faster processing while maintaining high-quality reconstruction. To further reduce computational complexity and enhance detail preservation, we develop a learnable high-frequency processing module that adaptively filters noise and recovers edge details. Additionally, to effectively utilize information from long-range frames, we propose a low-frequency propagation method equipped with a temporal feature alignment module. This method enables the efficient transfer of structural information from distant frames, ensuring temporal consistency and enhancing denoising performance. Extensive experiments demonstrate that our method has 1.28× fewer network parameters than state-of-the-art efficient video denoising methods, such as BasicVSR++, and requires less computational cost while achieving comparable performance.},
  archive      = {J_TMM},
  author       = {Zhulin Tao and Jinjuan Wang and Lifang Yang and Jinshan Pan and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3604914},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep frequency-separable temporal network for efficient video denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BASNet: Boundary assisted network for image splicing forgery detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing is a common technique used in image forgery. With the rapid development of digital image processing technology, detecting image splicing forgery has become increasingly challenging. Existing splicing forgery localization methods lack exploration in effectively utilizing tampered region boundary information. To address this issue, we propose a novel model for detecting image splicing forgery called boundary-assisted network (BASNet). We introduce a boundary-motivated module (BMM) to explore valuable and additional boundary features related to tampered regions, enhancing representation learning for detecting tampered regions. Additionally, we present a boundary-enhanced module (BEM) to enhance boundary information using the cross-channel attention mechanism. To efficiently merge features from various levels and boundary features, we further present the feature fusion module (FFM). To optimize performance, the BASNet incorporates weighted binary cross-entropy loss, dice loss, and boundary loss, which can effectively leverage edge supervision while mitigating imbalance between positive and negative samples. Evaluation of five widely-used forgery detection datasets demonstrates the state-of-the-art performance of the BASNet. Robustness experiments verify that the BASNet is robust enough to detect image splicing forgery across various common attacks.},
  archive      = {J_TMM},
  author       = {Enji Liang and Kuiyuan Zhang and Zhongyun Hua and Xiaohua Jia},
  doi          = {10.1109/TMM.2025.3604911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BASNet: Boundary assisted network for image splicing forgery detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy protection based on hopfield cross neural network in WBANs for medical images. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable medical data collection and surveillance devices provides real-time guarantees for the whole process of a patient's medical treatment, especially medical image data plays a key role. However, existing medical images face data leakage, pollution and vulnerability to attacks during transmission over wireless body area networks(WBANs). To address these issues, a privacy protection algorithm based on Hopfield cross neural network (HCNN) for medical data is proposed. Specifically, the HCNN model is first constructed and its dynamic behavior is analyzed, which is suitable for application to image encryption. Then, a confusion method of NZ fractal curve sorting matrix (NZ-FCSM) is designed to achieve good encryption effect. Subsequently, the secret image sharing (SIS) technique based on sharing matrix is introduced to enhance the algorithm robustness. Finally, an alignment embedding of double diamond prediction (AEDDP) method is proposed to implement lossless hiding of private information. The present issues in medical image protection include ensuring the security and effectiveness of encryption algorithms while maintaining the robustness and concealment of ciphertext data, and balancing the need for preservation with the limited resources of complex work environment. Experimental results show that the proposed algorithm achieves PSNR of 53 dB for the cipher image, more than 36 dB for the reconstructed image, and the information entropy of the secret image is over 7.99, and displays good robustness. These findings highlight the validity of the algorithm in medical image data privacy preserving applications that ensure confidentiality and extend to practical applications of concealed transmission of confidential information and secure multi-party transactions.},
  archive      = {J_TMM},
  author       = {Xiuli Chai and Guoqiang Long and Yakun Ma and Changbo Li and Zhihua Gan and Yushu Zhang},
  doi          = {10.1109/TMM.2025.3604898},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy protection based on hopfield cross neural network in WBANs for medical images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification via hölder divergence for multi-view representation learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the “distance” between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpasses the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.},
  archive      = {J_TMM},
  author       = {Yan Zhang and Ming Li and Chun Li and Zhaoxia Liu and Ye Zhang and F. Yu},
  doi          = {10.1109/TMM.2025.3604966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty quantification via hölder divergence for multi-view representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral pedestrian detection has been shown to be effective in improving performance in complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To efficiently compress multispectral object detection networks, we propose a novel distillation method, the Adaptive Modal Fusion Distillation (AMFD) framework. Unlike traditional distillation methods, the AMFD framework fully leverages the original modal features from the teacher network, thereby significantly enhancing the performance of the student network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP, SUNRGB-D and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.},
  archive      = {J_TMM},
  author       = {Zizhao Chen and Yeqiang Qian and Xiaoxiao Yang and Chunxiang Wang and Ming Yang},
  doi          = {10.1109/TMM.2025.3604937},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic frame aggregation-based transformer for live video comment generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments that align with viewer interests. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with cross-attention mechanism to attend to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large-scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.},
  archive      = {J_TMM},
  author       = {Anam Fatima and Yi Yu and Janak Kapuriya and Julien Lalanne and Jainendra Shukla},
  doi          = {10.1109/TMM.2025.3604921},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic frame aggregation-based transformer for live video comment generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of repetitive actions in videos, a task commonly referred to as Video Action Counting (VAC), is a critical challenge in understanding and analyzing content in sports, fitness, and daily activities. Traditional approaches to VAC have largely overlooked the nuanced irregularities inherent in action repetitions, such as interruptions and variable lengths between cycles. Addressing this gap, our study introduces a novel perspective on VAC, focusing on Irregular Video Action Counting (IVAC), which emphasizes the importance of modeling the irregular repetition priors present in video content. We conceptualize these priors through two key aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures that the spatiotemporal representations across all cycle segments in a video remain homogeneous, thereby reflecting the uniformity of actions between different cycle segments. In contrast, Cycle-interval Inconsistency mandates a clear semantic distinction between the representations of cycle segments and intervals, acknowledging the inherent dissimilarities in content. To effectively encapsulate these priors, we introduce a novel methodology consisting of consistency and inconsistency modules, underpinned by a tailored pull-push loss ($\mathrm {P^{2}~L}$) mechanism. This approach employs a pull loss to enhance the cohesion among cycle segment features and a push loss to distinctly differentiate between cycle and interval segment features. Empirical evaluations on the RepCount dataset illustrate that our IVAC-$\mathrm {P^{2}~L}$ model sets a new benchmark in state-of-the-art performance for the VAC task. Moreover, our model demonstrates adaptability and generalization across diverse video content, achieving superior performance on two additional datasets, UCFRep and Countix, without necessitating dataset-specific fine-tuning. These findings not only validate the effectiveness of our approach in addressing the complexities of irregular repetitions in videos but also open new avenues for future research in video understanding and analysis.},
  archive      = {J_TMM},
  author       = {Hang Wang and Zhi-Qi Cheng and Youtian Du and Lei Zhang},
  doi          = {10.1109/TMM.2025.3604935},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception-distortion- tradeoff reveals the limitation of current low-level deep learning paradigms, i.e., minimizing reconstruction distortion does not guarantee improved perceptual quality. Acknowledging the lack of a reliable perception-oriented optimization function, we are motivated to explore a flexible approach for enhancing perceptual quality by steering the tradeoff to prioritize perception. To this end, we reconsider the perception-distortion function by incorporating the Just-Noticeable-Distortion (JND) mechanism. We mathematically demonstrate that in the common image restoration process, altering the optimization target from natural images to distorted images—where the distortion intensity is constrained by the JND threshold and the distortion type aligns with that arising from the restorer itself—effectively obtained improved perception indices without any changes to the restorer or optimization function. Accordingly, to facilitate various low-level learning models, we are motivated to construct the first large-scale CNN-oriented JND image dataset. Our dataset comprises 500 natural images and 4,500 degraded versions generated by a series of autoencoders, as well as the actual JND judgment results collected through rigorous subjective testing from twenty volunteers. Finally, a learning-based JND inference model is established on the proposed dataset and employed in the proposed JND-based adaptation scheme, where the inferred JND images serve as pseudo-ground truth for the training or fine-tuning processes of low-level vision models. Extensive experiments on image super-resolution and end-to-end image compression across multiple models have shown encouraging improvements in perceptual quality, demonstrating the effectiveness of the proposed scheme. Our dataset is available at: https://github.com/ohq17/CNN-Oriented-JND-Dataset.},
  archive      = {J_TMM},
  author       = {Xuelin Shen and Haoqiao Ou and Zhangkai Ni and Wenhan Yang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TMM.2025.3604973},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RandomViG: Random vision graph neural network for image classification. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Graph Neural Network (ViG) is the first graph neural network model capable of directly processing image data. The community primarily focuses on the model structures to improve ViG's performance but lacks attention to its graph construction method. To avoid quadratic computational complexity, ViG uses clustering algorithms (K-nearest neighbor) to construct graph structures. Nevertheless, clustering algorithms introduce biases, which limit ViG's ability to obtain global information. To address this problem, we propose RandomViG, which abandons clustering algorithms and uses a random manner to obtain relationships between nodes. Our RandomViG is sparse in computation and can approximate a complete graph, enabling ViG to gain global interaction capability. In order to obtain the local dependence, we design a local feature extraction module for RandomViG. In addition, to alleviate the over-smoothing problem, we propose a novel method called MRN (maintaining relationships among nodes). Considering that the increased feature diversity does not necessarily lead to better performance, MRN does not aim to maximize the feature diversity of the model but instead strives to maintain consistency between the feature similarity and the inherent similarity of the original image. We validate our proposal in three major computer visual tasks, including image classification, object detection, and instance segmentation. Without extra data, RandomViG-Ti achieves 79.4% ImageNet-1 K top-1 accuracy, outperforming the baseline (ViG) by 1.2%. Under the same model scale, our RandomViG performs better with fewer FLOPs compared with existing state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Xun Gong and Daisong Yan and Zhemin Zhang},
  doi          = {10.1109/TMM.2025.3604948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RandomViG: Random vision graph neural network for image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLE: Out-of-distribution detection with shallow layer-driven enhancement. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3604940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution detection aims to protect models against overconfidently categorizing samples from unknown categories, i.e., out-of-distribution data (OOD), into known categories, i.e., in-distribution data (ID). From the perspective of feature distribution, the difference between OOD samples and ID samples can be decomposed into semantic shifts and covariate shifts. Most DL-based methods only extract deeper features, which represent semantic shifts, to discern feature variances in the data, ignoring the exploration of covariance shifts. In this paper, we propose a Shallow Layer-driven Enhanced OOD detection method (SLE), which enhances the difference of OOD samples by exploiting covariate shifts in shallow features. Specifically, it contains three main components: Hierarchical Feature Extractor (HFE), Adaptive Dimensionality Reduction Strategy (ADR), Cross-layer Score Aggregator (CSA). HFE is responsible for extracting both deeper and shallow features from the deep network. ADR adaptively reduces all hierarchical feature dimensionality according to sample characteristics, avoiding feature redundancy. CSA defines a novel confidence score for OOD samples, that effectively prevents confusion in the feature representation space at each layer. In SLE, these three closely related components cooperate with each other to effectively enhance the representation ability of OOD samples and divide OOD data better. We conduct extensive experiments to examine the performance of SLE in four benchmarks and discuss its individual components. This method performs well on the OOD datasets.},
  archive      = {J_TMM},
  author       = {Zhenni Yang and Chengxu Liu and Xueming Qian},
  doi          = {10.1109/TMM.2025.3604940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLE: Out-of-distribution detection with shallow layer-driven enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active cross-modal domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most cross-modal methods assume that training and testing data come from the same domain, which is often not the case in real-world scenarios due to cross-modal domain shifts and potential unknown concepts. Moreover, cross-modal shifts hinder the capture of unknown concepts, and the presence of unknown concepts can in turn exacerbate the cross-modal shifts. To address these challenges, this paper proposes a new paradigm called Active Cross-Modal Domain Adaptation (ACM-DA), wherein only cross-modal data from the source domain and uni-modal data from the target domain are utilized. To concurrently mitigate the adverse effects of both cross-modal domain shifts and unknown concepts, we propose a Curiosity-Driven Active Adaptation Network (CD-A2N), selectively annotating samples to maximize performance gain. First, we present Curiosity Arousal within Cross-modal Domain Adaptation (CA-CDA) to explore the complexity and novelty characteristics of target samples, while reducing cross-modal discrepancy and aligning source and target domains. Second, Curiosity-driven Active Learning (CAL) is devised to strategically select a subset of target samples for annotation, aiming to achieve more valuable data selection at a small labeling cost. Finally, we jointly train CA-CDA and CAL with the newly labeled target domain sub-dataset to alleviate the above issues. Extensive experiments demonstrate that CD-A2N provides an effective solution for achieving ACM-DA. Code will be available at https://github.com/Feliciaxyao/ACM-DA.},
  archive      = {J_TMM},
  author       = {Xuan Yao and Xiao Peng and Junyu Gao and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604968},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Active cross-modal domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards multimodal emotional support conversation systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support. This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities. This dataset captures the intricate interplay of user emotions, system strategies, system emotions, and system responses, setting a new precedent in the field. Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. This work not only pushes the boundaries of AI's role in mental health care but also establishes a foundation for developing conversational agents that can provide more empathetic and effective emotional support.},
  archive      = {J_TMM},
  author       = {Yuqi Chu and Lizi Liao and Zhiyuan Zhou and Chong-Wah Ngo and Richang Hong},
  doi          = {10.1109/TMM.2025.3604951},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards multimodal emotional support conversation systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HARG: Hierarchical adaptive reasoning graph for activity parsing. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a video understanding task, activity parsing aims at encompassing actions into multiple levels of activity components, including activity, sub-activity and atomic action, enabling understanding of complex video scenes within multimedia systems. Existing methods form activity parsing as a multi-task learning problem to predict multi-granular activity labels simultaneously, which ignores modeling the hierarchical structure and the fine-grained transitions of activity components at different levels. In this paper, we propose a Hierarchical Adaptive Reasoning Graph (HARG) to model the hierarchical structure (i.e., object level $\rightarrow$ atomic action level $\rightarrow$ activity level) dynamically and precisely. To achieve that, an object reasoning graph (ORG) and an atomic action reasoning graph (ARG) are designed to reason fine-grained information transitions between multiple actors at different levels. In addition, an adaptive segmentation module (ASM) is investigated for bridging the gap among different levels, permitting step-by-step reasoning from the object level to the atomic action level. Experimental results show our method outperforms state-of-the-art methods on two activity parsing datasets, achieving hierarchical modeling and fine-grained reasoning for activity understanding. The code is available on GitHub: https://github.com/whuoyj/HARG.},
  archive      = {J_TMM},
  author       = {Yangjun Ou and Li Mi and Zhenzhong Chen},
  doi          = {10.1109/TMM.2025.3604927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HARG: Hierarchical adaptive reasoning graph for activity parsing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware wavelet transformer for pyramid learning object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer displays the impressive capabilities on vision tasks. The built-in self-attention retains the quadratic computation burden in respect of the spatial resolution of image features. The traditional downsampling (e.g., average pooling) can reduce the resolution. Nonetheless, it may suffer from the dropping of detailed information. In this work, we propose an Efficient Wavelet Attention (EWA), which injects the wavelet transform and a Mean GELU (MGELU) function. Firstly, the wavelet transform enables the detailed information to participate in the efficient interaction modeling. Secondly, MGELU regards the statistical mean as reference and loosely passes the high relative responses. Building upon EWA, we present an effective Semantic-aware Wavelet Transformer (SWFormer), which is then employed for pyramid learning, including CNN feature hierarchy or Region of Interest (RoI) features. For the feature hierarchy, a Pyramid SWFormer (PSWFormer) incorporates SWFormer at each level to fit the bidirectional features. For RoIs, a Recognition-Localization SWFormer (RLSWFormer) is inserted into the head to fit their features from all levels. The effectiveness of our SWFormer is displayed experimentally on the MS COCO detection dataset and the Pascal VOC dataset. When exploiting Swin-small backbone, our SWFormer-based method acquires AP of 52.1 in the single-scale evaluation on the COCO test-dev set. This work will have the codes at https://github.com/TimeIsFuture/Dt2_SWFormer.},
  archive      = {J_TMM},
  author       = {Yang Li and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Puhua Chen},
  doi          = {10.1109/TMM.2025.3604963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-aware wavelet transformer for pyramid learning object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-contact respiratory monitoring, reducing motion artifact and selecting the appropriate Region of Interest (ROI) pose significant challenges. Most motion artifact removal methods rely on signal periodicity assumptions, while respiratory signals usually are non-periodic in real-world scenarios. Existing automated ROI selection approaches are mostly primarily impacted by the texture of clothing, absence of chest landmarks, and obstruction of face. To improve the quality of respiratory signals, in this study, we propose a framework for automatic respiratory ROI selection based on video, namely, Optimizing Video-based Respiration Monitoring (OVRM), which consists of peak-trough adaptive motion artifact removal and characteristic-driven adaptive ROI selection. This motion artifact removal strategy removes motion artifacts by using a dynamic ratio-based judgment mechanism, and reconstructs signals using sinusoidal interpolation. The adaptive ROI method scores signals based on periodicity, similarity, smoothness, and energy, selecting the highest-scoring blocks as the ROIs to match respiratory signals efficiently. Experimental results, validated across four datasets, demonstrate that OVRM effectively reduces signal noise caused by subject movement and outperforms state-of-the-art non-contact respiratory monitoring algorithms. The dataset and code are publicly available at: https://github.com/zxx5058/OVRM.},
  archive      = {J_TMM},
  author       = {Xinxin Zhang and Xudong Tan and Yan Zhu and Mei Zhou and Menghan Hu and Zhanzhan Cheng and Nengfeng Qian and Changyin Wu and Guangtao Zhai and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3604970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action-responsive contrastive network for fine-grained skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, fine-grained skeleton action recognition based on graph convolutional networks (GCNs) has become an important research focus. Fine-grained action recognition refers to the accurate recognition of subtle, complex or detailed actions. This task is particularly challenging due to the limited appearance information in skeleton data and the limitations of predefined single-topology skeleton structures. To address these challenges, we propose an action-responsive contrastive network (ARCN). The network consists of two main components: an action-responsive graph convolutional network (ARGCN) with enhanced skeleton topology and a fine-grained action comparator (FAC) that uses feature contrastive learning to explore the latent space of motion features. The ARGCN contains two specialized modules: the action-responsive topology (ART) module, which captures important motion features through the learned action-specific topology structure matrix and multiscale temporal features; and the action-responsive attention (ARA) module, which learns complex spatiotemporal skeleton attention information. These modules jointly generate a multichannel cross-temporal dynamic skeleton joint attention topology map tailored for the specific action being analysed. To further clarify the fine-grained action feature differences, the FAC is integrated in some stages of the ARGCN. The FAC performs spatiotemporal decoupling of feature maps, classifies and contrasts similar and different fine-grained motion features, and builds a learnable latent space for fine-grained motion, thereby improving classification performance. Our model is evaluated on six public datasets: NTU RGB+D, NTU RGB+D 120, NW-UCLA, UAV-Human, Finegym, and Diving48. It achieves 91.2% accuracy on the NTU RGB+D 120 dataset X-Set, 97.2% accuracy on the NW-UCLA dataset, 44.6% accuracy on the UAV-Human dataset CSv1, 72.0% accuracy on the UAV-Human dataset CSv2, 95.3% accuracy on the Finegym dataset, and 54.3% accuracy on the Diving48 dataset, which are competitive results compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hongjun Li and Tian Bai},
  doi          = {10.1109/TMM.2025.3604906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Action-responsive contrastive network for fine-grained skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed AutoRegressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed AutoRegressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can achieve over a magnitude acceleration in decoding time without industrial level optimization or achieve state-of-the-art reconstruction quality compared with other INR codecs. To the best of our knowledge, our method is the first INR-based codec comparable with Ballé et al. [1] in both decoding speed and quality while maintaining low complexity.},
  archive      = {J_TMM},
  author       = {Xiang Liu and Jiahong Chen and Bin Chen and Zimo Liu and Baoyi An and Shu-Tao Xia and Zhi Wang},
  doi          = {10.1109/TMM.2025.3604982},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To counter the security threats posed by the realism of image inpainting generated through diffusion models and GANs, in this paper, we propose a texture enhancement and progressive refinement network (TEPR-Net) for image inpainting localization (IIL). The IIL task is divided into two phases: coarse and fine locating. In the coarse locating phase, we utilize an anomaly texture encoder to capture tampering traces in textures, employ a texture–context feature interaction strategy to effectively integrate texture features with contextual features, and utilize a pixel-level contrastive learning strategy to enhance feature clustering and model generalization. In the fine locating phase, we first enhance the receptive field features in the frequency domain by transforming the features and separately enhancing the low- and high-frequency components. Then, we utilize the coarse localization result to augment the model's sensitivity to tampered regions. Additionally, we introduce a progressive edge distribution guidance and reconstruction strategy that progressively refines the edges of the tampered regions at each level, ultimately generating refined localization results. To support the research and evaluation of the IIL task, we create the Inpaint32K dataset, which is characterized by its large scale, diversity, comprehensiveness, high quality, and authenticity. Finally, extensive experiments demonstrate that TEPR-Net has significant advantages in terms of localization performance, generalizability, extensibility, and robustness.},
  archive      = {J_TMM},
  author       = {Qixian Hao and Kai Wang and Haoliang Cui and Jiwei Zhang and Shaozhang Niu},
  doi          = {10.1109/TMM.2025.3604965},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and robust 3D panoptic segmentation is crucial for scene perception in autonomous driving. Modern methods widely adopt multi-modal fusion based simple feature concatenation to enhance 3D scene understanding, resulting in generated multi-modal representations typically lack comprehensive semantic and geometry information. These methods focused on panoptic prediction in a single step also limit the capability to progressively refine panoptic predictions under varying noise levels, which is essential for enhancing model robustness. To address these limitations, we first utilize BEV space to unify semantic-geometry perceptual representation, allowing for a more effective integration of LiDAR and camera data. Then, we propose PrimePSegter, a progressively combined diffusion 3D panoptic segmentation model that is conditioned on BEV maps to iteratively refine predictions by denoising samples generated from Gaussian distribution. PrimePSegter adopts a conditional encoder-decoder architecture for fine-grained panoptic predictions. Specifically, a multi-modal conditional encoder is equipped with BEV fusion network to integrate semantic and geometric information from LiDAR and camera streams into unified BEV space. Additionally, a diffusion transformer decoder operates on multi-modal BEV features with varying noise levels to guide the training of diffusion model, refining the BEV panoptic representations enriched with semantics and geometry in a progressive way. PrimePSegter achieves state-of-the-art performance on the nuScenes and competitive results on the SemanticKITTI, respectively. Moreover, PrimePSegter demonstrates superior robustness towards various scenarios, outperforming leading methods.},
  archive      = {J_TMM},
  author       = {Hongqi Yu and Sixian Chan and Xiaolong Zhou and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3604903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided adversarial attack in the low-frequency space. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3604964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples can assess the robustness of machine learning models, which has attracted the attention of many researchers to adversarial example generation methods. Transferability and imperceptibility stand out as two crucial metrics for evaluating the quality of adversarial examples. However, achieving a balance between these two indicators poses a formidable challenge. In this paper, we propose a low-frequency guided adversarial attack method (LGA) to generate adversarial examples with strong transferability and good imperceptibility. Specifically, we enhance the transferability of adversarial examples by increasing the diversity of attack algorithms, and introduce the guiding principle and the triplet loss constraint to ensure that the generated adversarial examples are optimized away from the class regions of the clean examples. We find that the low-frequency component in the frequency domain of the image contains the vast majority of the semantic information of the image. Therefore, we constrain the attack perturbations to low-frequency component space to enhance the covert nature while maintaining visual coherence, rendering the adversarial examples more difficult to perceive. We conduct extensive experiments on various models with different network structures and multiple defense strategies, and the experimental results demonstrate that our method outperforms existing methods in the tradeoff between transferability and imperceptibility, achieving the SOTA performance.},
  archive      = {J_TMM},
  author       = {Jiang Zhu and Lingping Tan and Yanchun Li and Shujuan Tian and Jianqi Li and Yaonan Wang},
  doi          = {10.1109/TMM.2025.3604964},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guided adversarial attack in the low-frequency space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view clustering via mutual information. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering focus on mining useful information from low-quality multiple sources, such as missing and distorted data that are prevalent in real life. However, after representation learning and the processing of incomplete information, existing methods often leave representations containing information task-irrelevant information. In addition, the separation between missing data imputation and clustering tasks leads to sub-optimal multi-view clustering performance. To address these issues, we propose an incomplete multi-view clustering method based on mutual information. For the problem of task-irrelevant information, we use incomplete view prediction to extract sufficient and minimal task-relevant information and provide theoretical proof from the perspective of mutual information. For the problem of separation between missing data imputation and clustering tasks, we integrate incomplete-view prediction with contrastive clustering, collaboratively enhancing the clustering performance. Comparative experiments on five public datasets, under both complete and incomplete scenarios, reveal that our method outperforms nine other competing approaches, demonstrating its effectiveness and robustness in handling multi-view data.},
  archive      = {J_TMM},
  author       = {Xuejiao Yu and Guoqing Chao and Yi Jiang and Guanzhou Ke and Dianhui Chu},
  doi          = {10.1109/TMM.2025.3604942},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incomplete multi-view clustering via mutual information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Like humans to few-shot learning through knowledge permeation of visual and language. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario. With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes. However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge. In this paper, we set out to fill this gap via a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in human intuition: a class name description offers a more general representation, whereas an image captures the specificity of individuals. BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation. On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information. Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop, particularly outperforming previous methods by a substantial margin in the 1-shot setting (improving the accuracy by 7.58% on miniImageNet).},
  archive      = {J_TMM},
  author       = {Yuyu Jia and Qing Zhou and Junyu Gao and Qiang Li and Qi Wang},
  doi          = {10.1109/TMM.2025.3604977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Like humans to few-shot learning through knowledge permeation of visual and language},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCPoint: Contrasting corrupted point clouds for self-supervised representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised Learning (SSL), including mainstream contrastive learning, has achieved significant success in learning visual representations without the need for data annotations in 3D vision. While most contrastive learning methods focus on instance-level information through random affine transformations, they pay limited attention to the intrinsic structures within point clouds. In this work, we propose a novel SSL paradigm for point cloud representation learning, called CCPoint, which incorporates a novel form of data corruption as a negative augmentation strategy. Specifically, we degrade the input point cloud with various corruptions and conduct contrastive learning among the augmented, raw, and corrupted points to learn robust and discriminative representations. To preserve the semantic structure of the point cloud even under heavy degradation, an auxiliary reconstruction decoder is introduced into the corruption branch to provide an additional supervision signal. We explore four families of corruptions—affine, noise, masking, and combined transformations. Different from previous methods that rely on multi-modal data or complex network architectures, CCPoint achieves state-of-the-art performance on three widely used datasets (ModelNet40, ScanObjectNN, and ShapeNetPart) with a lightweight and efficient structure, reaching top linear accuracies of 92.4% and 86.2% on ModelNet40 and ScanObjectNN, respectively.},
  archive      = {J_TMM},
  author       = {Xiaoyang Xiao and Shaoyi Du and Zhiqiang Tian and Meiqin Liu and Xinhu Zheng},
  doi          = {10.1109/TMM.2025.3604890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCPoint: Contrasting corrupted point clouds for self-supervised representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive gradient-guided self-distillation keypoint detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of autonomous driving and 3D reconstruction, keypoint detection, as a key link in visual localization, has become a hot topic in current research. However, existing keypoint detection methods rarely pay attention to the difficulty differences of samples and lack a progressive learning mechanism, which often leads to overfitting for simple samples and underfitting for complex samples, limiting the overall performance of the model. To address these issues, we propose a novel progressive gradient-guided self-distillation method (PG2 SD) for keypoint detection, which possesses self-evolutionary learning capabilities. Specifically, we propose a progressive gradient constraint strategy (PGCS) that dynamically adjusts the gradient contributions of different samples, enabling the model to adapt to the evolving learning capability during training. On this basis, we propose a gradient-guided self-distillation strategy (G2 SDS), which integrates seamlessly with PGCS to alleviate the insufficient feature representation of hard samples in the early training stage. We further design a novel loss function to achieve dynamic collaboration between PGCS and G2 SDS, allowing G2 SDS to adaptively adjust the self-distillation parameters through the PGCS. Experimental results on multiple benchmark datasets show that our method achieves state-of-theart performance on image matching, visual localization, and 3D reconstruction tasks without designing a proprietary network, indicating broad application prospects.},
  archive      = {J_TMM},
  author       = {Zhaoyang Li and Jie Cao and Qun Hao and Haifeng Yao and Yingbo Wang},
  doi          = {10.1109/TMM.2025.3607731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive gradient-guided self-distillation keypoint detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-Based Signature (ABS) provides a critical solution for ensuring data integrity, fine-grained access control, and anonymous authentication in security-sensitive systems such as the Multimedia Internet of Things (MIoT) and multimedia streaming platforms. However, practical adoption of ABS faces three fundamental challenges: vulnerability to key exposure and escrow risks, linear growth of computational cost, and insufficient robustness in multi-authority environments. To address these issues, we propose a forward secure and threshold authorized multi-authority ABS scheme called FORT in this paper. By employing a binary tree structure to divide multiple time periods, historical signatures remain valid even in the event of key exposure. Furthermore, to balance robustness and resistance to corruption while mitigating the key escrow problem, we construct a threshold authorized multi-authority structure based on Lagrange interpolation. This structure effectively reduces the impact of a single authority on the MIoT. Additionally, through the adoption of outsourced computation technology, which offloads complex computations in the signature and verification phases to the edge server, the computational burden for both the signer and verifier is significantly reduced to a small constant. Rigorous security analysis demonstrates that the FORT scheme achieves forward security, collusion attack resistance, corrupt authority resistance and anonymity. Theoretical comparisons and simulation experiments demonstrate the lightweight nature of the FORT scheme in terms of computation and communication.},
  archive      = {J_TMM},
  author       = {Chong Guo and Bei Gong and Zhe Li and Mowei Gong and Haotian Zhu},
  doi          = {10.1109/TMM.2025.3607696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foodfusion: A novel approach for food image composition via diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image composition requires the use of existing dish images and background images to synthesize a natural new image, while diffusion models have made significant advancements in image generation, enabling the construction of end-to-end architectures that yield promising results. However, existing diffusion models face challenges in processing and fusing information from multiple images and lack access to high-quality publicly available datasets, which prevents the application of diffusion models in food image composition. In this paper, we introduce a large-scale, high-quality food image composite dataset, FC22k, which comprises 22,000 foreground, background, and ground truth ternary image pairs. Additionally, we propose a novel food image composition method, Foodfusion, which leverages the capabilities of the pre-trained diffusion models and incorporates a Fusion Module for processing and integrating foreground and background information. This fused information aligns the foreground features with the background structure by merging the global structural information at the cross-attention layer of the denoising UNet. To further enhance the content and structure of the background, we also integrate a Content-Structure Control Module. Extensive experiments demonstrate the effectiveness and scalability of our proposed method.},
  archive      = {J_TMM},
  author       = {Chaohua Shi and Xuan Wang and Si Shi and Xule Wang and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3607683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Foodfusion: A novel approach for food image composition via diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have demonstrated remarkable abilities in image-language reasoning. However, they deal with Video Question Answering (VideoQA) insufficiently, especially for questions demanding causal-temporal reasoning. Typically, they directly concatenate features of uniformly sampled frames as visual inputs for VideoQA. This gives rise to two challenges. For one thing, uniformly sampled frames are discrete and separately distributed across different timestamps, disrupting the coherence of question-critical events or actions. For another, it considers every scene within videos equally and introduces redundant frames that may distract the model from discovering the truth. Towards this, we highlight the importance of identifying continuous frames that are crucial for answering the questions, and propose a lightweight and differentiable Coherence Recognizer (CoRe) to achieve this. Guided by the semantics of questions, CoRe computes scores recording the relevance between each frame and the question, and selects a set of continuous frames with the highest scores for answer prediction. Additionally, CoRe encodes the unselected frames into a short and coarse-grained representation as a completion of the general context. Equipped with CoRe, we can efficiently fine-tune the current MLLMs for VideoQA in an end-to-end manner, without suffering from the problems of incoherence or distraction. Extensive experiments demonstrate that our method achieves substantial improvements on several VideoQA benchmarks.},
  archive      = {J_TMM},
  author       = {Haibo Wang and Chenghang Lai and Weifeng Ge},
  doi          = {10.1109/TMM.2025.3607780},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage causal intervention framework for long-tailed SAR target recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of SAR targets generally conforms to a long-tailed distribution. Due to the existence of sample distribution bias and sample selection bias, training classifiers on this distribution of data often introduces spurious correlations between samples and classes. To address this issue, we propose a two-stage causal intervention framework. The core is that structural causality allows for independent interventions on multiple biases, thereby ensuring high-quality tail class predictions while maintaining unbiased performance for head classes. Firstly, we construct a structural causal graph for the long-tailed recognition task from causal perspective. Based on this graph, the causal paths underlying the two types of biases are identified. Secondly, we design a data augmentation method named DiagPatch-M, which identifies causal features within samples. In this process, these generated patches randomly integrate causal and non-causal features from two different samples, disrupting the original recognition process and effectively eliminating biases induced by sample selection. Thirdly, we design an unbiased structural risk minimization (USRM) optimization strategy, which eliminates the “head preference” of conventional models and the “tail preference” of modified models. This strategy reduces the bias introduced by the model's dependence on the original sample distribution, and achieves stable recognition under different sample distributions. Experimental results on two long-tailed and two balanced datasets demonstrate that the effectiveness of our model surpasses the state-of-the-art (SOTA) methods, indicating the efficacy of our proposed framework in tackling the challenges posed by the long-tailed distribution in SAR target recognition.},
  archive      = {J_TMM},
  author       = {Jiaxiang Liu and Zhunga Liu and Longfei Wang and Zuowei Zhang},
  doi          = {10.1109/TMM.2025.3607804},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A two-stage causal intervention framework for long-tailed SAR target recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTSNet: Dynamic transformer slimming for efficient vision recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have recently adopted increasingly complex structure (e.g., deeper or wider stacked network) to promote the representation learning capabilities of vision recognition. However, progressively deeper or wider stacked network cause the expensive computation cost, which hinders their effective deployment in resource-constrained edge clouds or end devices. In this paper, we propose DTSNet, a dynamic transformer slimming model, which scales vision transformers (ViTs) down across layers from both of the model depth and input width. This is the first time to explore the joint reduction of input tokens and model parameters for ViTs under maintaining performance. Specifically, DTSNet adopts a diversity-enhanced weight sharing module to reduce network parameters, where the weight knowledge of multiple adjacent blocks is effectively integrated into one block. Furthermore, DTSNet designs a unified and massively scalable token pruning mechanism that dynamically discarding less important tokens with a model-driven manner, by introducing a series of discriminant parameters, which is a simple change to the common architecture of vision transformers. Extensive experiments are conducted to verify that DTSNet is able to yield high efficacy in compressing parameter space and accelerating model inference. DTSNet-T/-S/-B on ImageNet achieves 3.0M/11.1M/42.9M parameters and 0.8/2.9/13.7 GFLOPs, where number of parameters are reduced by 48%$\sim$51% and inference speed are improved by 1.3$\times \sim 1.5\times$. Experiments results on semantic segmentation and object detection dataset further demonstrate the potential of DTSNet on complex dense prediction tasks. Code will be available upon publication.},
  archive      = {J_TMM},
  author       = {Wenjing Xiao and Xianzhi Li and Long Hu and Yixue Hao and Min Chen},
  doi          = {10.1109/TMM.2025.3607796},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTSNet: Dynamic transformer slimming for efficient vision recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.},
  archive      = {J_TMM},
  author       = {Jianwu Fang and Lei-Lei Li and Zhedong Zheng and Hongkai Yu and Jianru Xue and Zhengguo Li and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3607808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comp-diff: A unified pruning and distillation framework for compressing diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, generative models such as diffusion models (DMs) have gained prominence in various applications, and there is a growing demand for their deployment on resourceconstrained devices. Model pruning provides an effective solution by reducing the model redundancy without significantly impacting performance. However, most existing model pruning methods are designed for classification models and often lead to substantial performance degradation when applied to generative models. To address this issue, we propose Comp-Diff, a novel two stage framework of pruning and knowledge distillation tailored for diffusion models. In the pruning stage, we propose a new structured content-aware pruning (CaP) method within CompDiff to identify and preserve informative units (filters/channels) that actually contribute to the generative capability of the model. Specifically, we introduce input perturbations to the pre-trained model and measure each unit's importance score using gradients induced by these perturbations. Units with higher importance scores are considered more informative and are retained to maintain the model's generative power. In the fine-tuning stage of Comp-Diff, we propose the distribution-aware knowledge distillation (DaKD) method, which effectively transfers finegrained knowledge from the original model to the pruned one on both attention and noise distribution levels. In addition, DaKD includes an adversarial loss to improve the quality and diversity of generated outputs. To verify and evaluate our method, we apply the proposed Comp-Diff on three representative tasks: unconditional image generation, conditional image generation, and text-to-image generation. Extensive experiments on both multi-step and one-step diffusion models demonstrate that the proposed framework consistently yields compact models and outperforms existing pruning techniques by a large margin.},
  archive      = {J_TMM},
  author       = {Lu Yu and Wei Xiang and Kang Han and Gaowen Liu and Ramana Kompella},
  doi          = {10.1109/TMM.2025.3607799},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comp-diff: A unified pruning and distillation framework for compressing diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online video super-resolution methods utilize implicit memories of previous frames to provide reference information, which have a single memory stream path and are highly dependent on the continuous memory stream. However, video capture in real-world scenes is typically affected by abnormal exposures resulting in sudden changes of lightness thus interrupting the memory stream, while long-term memories suffer from memory vanishing problems during transmission. To address this problem, we propose a novel multi-memory streams based online video super-resolution paradigm that adaptively corrects for abnormal exposures and creates multi-memory streams to accurately converge long-term memories. Specifically, we first propose an exposure detection-correction module, which utilizes optical flow overfitting property and temporal lightness information to detect and correct abnormal exposures to avoid interruption of memory streams. In addition, we propose a dynamic-static decoupled alignment strategy, which can adaptively select the alignment method based on pixel displacement, thus accurately aggregating past long-term memories to create multiple memory streams. Further, we propose an adaptive memory fusion module to mine complementary information between multiple memory streams to solve the memory vanishing problem. Extensive experimental results show that our method outperforms existing video super-resolution methods on complex exposure datasets. We also conduct detailed ablation experiments to analyze and validate our contributions. The implementation code is available at https://github.com/GZ-T/MMVSR.},
  archive      = {J_TMM},
  author       = {Guozhi Tang and Hongwei Ge and Yong Luo and Bo Li and Chunguo Wu},
  doi          = {10.1109/TMM.2025.3607758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clean image may be dangerous: Data poisoning attacks against deep hashing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image retrieval using deep hashing has become increasingly popular due to the exponential growth of image data and the remarkable feature extraction capabilities of deep neural networks (DNNs). However, deep hashing methods are vulnerable to malicious attacks, including adversarial and backdoor attacks. It is worth noting that these attacks typically involve altering the query images, which is not a practical concern in real-world scenarios. In this paper, we point out that even clean query images can be dangerous, inducing malicious target retrieval results, like undesired or illegal images. To the best of our knowledge, we are the first to study data poisoning attacks against deep hashing (PADHASH). Specifically, we first train a surrogate model to simulate the behavior of the target deep hashing model. Then, a strict gradient matching strategy is proposed to generate the poisoned images. Extensive experiments on different models, datasets, hash methods, and hash code lengths demonstrate the effectiveness and generality of our attack method.},
  archive      = {J_TMM},
  author       = {Shuai Li and Jie Zhang and Yuang Qi and Kejiang Chen and Tianwei Zhang and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2025.3607774},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clean image may be dangerous: Data poisoning attacks against deep hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale invertible neural network for wide-range variable-rate learned image compression. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder-based structures have dominated recent learned image compression methods. However, the inherent information loss associated with autoencoders limits their rate-distortion performance at high bit rates and restricts their flexibility of rate adaptation. In this paper, we present a variable-rate image compression model based on invertible transform to overcome these limitations. Specifically, we design a lightweight multi-scale invertible neural network, which bijectively maps the input image into multi-scale latent representations. To improve the compression efficiency, a multi-scale spatial-channel context model with extended gain units is devised to estimate the entropy of the latent representation from high to low levels. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared to existing variable-rate methods, and remains competitive with recent multi-model approaches. Notably, our method is the first learned image compression solution that outperforms VVC across a very wide range of bit rates using a single model, especially at high bit rates.},
  archive      = {J_TMM},
  author       = {Hanyue Tu and Siqi Wu and Li Li and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2025.3607748},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale invertible neural network for wide-range variable-rate learned image compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIFormer+: Rethinking rotation-invariant feature learning in transformer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved remarkable success in the field of computer vision due to their advantage in capturing the global information of images. However, they fail to model the variance of rotation, resulting in significant performance loss in target detection in remote sensing imagery. In this paper, a rotation-invariant transformer plus model, namely RIFormer+ is proposed to enhance the capabilities of transformers in rotation-invariant feature learning at both long-overlooked local-level and the acknowledged global-level. At the local-level, a rotation-invariant cross-patch embedding (RICPE) module is designed to generate dense patches, which handles encoding inconsistency of tokens with similar semantic information before and after rotation. Moreover, response-enhanced attention (REA) is proposed to extract more rotation-robust global features, which highlights overly dispersed responses ensure sustained attention on discriminative regions. Extensive experiments on three datasets demonstrate the effectiveness of RIFormer+. Without bells and whistles, RIFormer+ increases the classification accuracy by an average of 10% and improves the accuracy on rotated datasets by 20% compared with some state-of-the-art transformers. The code of this paper is available at: https://github.com/psychAo/RIFormerPlus.},
  archive      = {J_TMM},
  author       = {Chao Song and Yifan Zhang and Mingyang Ma and Shaohui Mei},
  doi          = {10.1109/TMM.2025.3607728},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RIFormer+: Rethinking rotation-invariant feature learning in transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Unmanned Aerial Vehicles (UAVs) and their unique vantage points present both new opportunities and challenges for person Re-Identification (ReID). Uncertain rotations and scale variations of targets in UAV images, coupled with complex environmental factors, hinder existing methods from extracting robust feature representations. Some methods either make minor modifications to the traditional model architecture or apply simple image rotations but still fail to effectively address the challenges of UAV person ReID. To overcome these limitations, we propose a novel Data Augmentation and Rotation Invariance (DARI) algorithm. First, rotation-invariant convolution is introduced to adaptively extract features, mitigating the uncertainty caused by target rotation. Second, a refined data augmentation correction strategy is employed to reduce noise interference by increasing the richness of global features at different stages. Additionally, considering that multiple features of the same identity should yield consistent recognition result, invariant constraints are designed to enhance the clustering effect. We conducted extensive experiments on both UAV and fixed-camera datasets. The results on PRAI-1581 demonstrate a 5.6% and 6.1% improvement in mAP and Rank-1, respectively, compared to baseline. These findings highlight the model's effectiveness in addressing the challenges of UAV ReID, demonstrating its robustness and superiority. The source code will be released at https://github.com/ZFZ314/DARI.},
  archive      = {J_TMM},
  author       = {Fuzeng Zhang and Eksan Firkat and Hongbing Ma and Jihong Zhu and Bin Zhu and Askar Hamdulla},
  doi          = {10.1109/TMM.2025.3607835},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential of diffusion-generated data on salient object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, salient object detection (SOD) has made significant progress. However, this advancement is often constrained by the requirement for extensive training data and expensive manual annotation. To eliminate the laborious cost of dataset collection and pixel-level annotation, in this work, we employ Stable Diffusion to synthesize data and subsequently automate annotation for the SOD task. Firstly, we design a unified prompt and ChatGPT4 driven diverse prompts, which guide generating images with simple and complex scenes using Stable Diffusion. Secondly, the reliable pseudo-labels of these synthetic images are generated. For simple images, we propose the simple pseudo-label generation (SPLG) strategy which combines SAM segmentation and CLIP classifier, then train the initial SOD model. For complex images, we utilize the inference capability of the initial SOD model to generate pseudo-labels using the complex pseudo-label generation (CPLG) strategy, and employ iterative training to dynamically update the pseudo-labels. Finally, we design a simple yet effective SOD model which combines a feature fusion module (FFM) and an edge enhancement module (EEM), the former is employed to extract saliency via fusing high-level features, and the latter extracts spatial positional information from low-level features to enhance the edges of saliency results. Experiments on five benchmarks show that our method outperforms the unannotated methods, and also demonstrates better or comparable performance than weak annotation based methods. Our code will be published at https://github.com/FangWenRE/PotentialOfSDSOD.},
  archive      = {J_TMM},
  author       = {Xiangquan Liu and Xianlong Luo and Ying Ye and Xiaoming Huang},
  doi          = {10.1109/TMM.2025.3607734},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Potential of diffusion-generated data on salient object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced subspace clustering network with information bottleneck. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data mining, subspace clustering is a crucial technique which determines the union of the underlying subspace to cluster data points in an unsupervised manner. Although deep-learning-based subspace clustering, typically referred to as deep subspace clustering (DSC), has significantly improved clustering accuracy, existing DSC models still struggle to capture a comprehensive and compact latent representation as they generally explore the spatial domain to extract useful information and face difficulty in balancing the high mutual and low redundant information between the original input space and latent subspace. This leads to the performance of the model being dependent on initialization, resulting in a lack of stability. In this study, a novel network is proposed to extract features in both the frequency domain and spatial domain. We introduce three types of ResBlocks in the discrete Fourier transform (DFT), discrete cosine transform (DCT), or discrete wavelet transform (DWT) frequency domains separately to learn both the low-frequency and high-frequency information in the proposed networks. Additionally, to extract concise and rich latent representations, IB loss is employed by deriving a variational lower bound on the IB objective. Extensive experiments on several benchmark datasets verify the effectiveness of our networks compared to state-of-the-art models. In addition, detailed ablation studies are performed to demonstrate the advantages of the two introduced components.},
  archive      = {J_TMM},
  author       = {Mengran Hou and Mengyao Li and Chengli Tan and Junmin Liu and Jinhai Li and Huirong Li},
  doi          = {10.1109/TMM.2025.3607797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-enhanced subspace clustering network with information bottleneck},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local prompts distillation for domain generalization. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has been proven effective for Domain Generalization (DG) by enhancing the generalization capability of visual-language models with fewer learnable tokens. Existing methods adopt mostly inferring global-level individual prompts for the whole dataset to capture domain-invariant knowledge across different domains. However, since domain shifts exist, a single global-level individual prompt is easily overfitted to source domain datasets, thus lacking generalizability to the whole dataset's feature distribution. Moreover, fluctuations in the generalization performance during the training process in DG problems often pose significant challenges to model selection strategies. To address the aforementioned problems, inspired by the Mixture-of-Expert (MOE) and knowledge distillation, we propose a novel Multiple Local Prompts Distillation (MLPD) method to inject the knowledge of multiple local prompts into a unique global prompt, improving both the generalization and discriminative ability. To ensure the diversity of local prompts, we split the whole dataset into several subsets to infer the discriminative local prompts for each subset, which is further applied to generate the generability global prompt. Formally, for each subset, Meta Prompt Tuning (MPT) is proposed to constrain each local prompt to capture both the domain-specific and domain-shared generalization knowledge on the basis of the domain label and meta-learning mechanism. After that, Prompt Knowledge Distillation (PKD) is proposed to distill the knowledge captured in the local-level prompts into the global-level prompt with prompt-level and feature-level knowledge distillations. The final evaluation on multiple benchmarks underscores the effectiveness of the proposed MLPD, e.g., achieving mAPs of 97.3%, 84.8%, 85.2%, 57.3%, and 60.7% on PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, respectively.},
  archive      = {J_TMM},
  author       = {Huaihai Lyu and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607719},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple local prompts distillation for domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuInNet: Dual-modality feature interaction for point cloud completion. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further promote the development of multimodal point cloud completion, we contribute a large-scale multimodal point cloud completion benchmark ModelNet-MPC with richer shape categories and more diverse test data, which contains nearly 400,000 pairs of high-quality point clouds and rendered images of 40 categories. Besides the fully supervised point cloud completion task, two additional tasks including denoising completion and zero-shot learning completion are proposed in ModelNet-MPC, to simulate real-world scenarios and verify the robustness to noise and the transfer ability across categories of current methods. Meanwhile, considering that existing multimodal completion pipelines usually adopt a unidirectional fusion mechanism and ignore the shape prior contained in the image modality, we propose a Dual-Modality Feature Interaction Network (DuInNet) in this paper. DuInNet iteratively interacts features between point clouds and images to learn both geometric and texture characteristics of shapes with the dual feature interactor. To adapt to specific tasks such as fully supervised, denoising, and zero-shot learning point cloud completions, an adaptive point generator is proposed to generate complete point clouds in blocks with different weights for these two modalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC benchmarks demonstrate that DuInNet exhibits superiority, robustness and transfer ability in all completion tasks over state-of-the-art methods. The code and dataset will be available at https://github.com/xinpuliu/DuInNet.},
  archive      = {J_TMM},
  author       = {Xinpu Liu and Baolin Hou and Hanyun Wang and Ke Xu and Jianwei Wan and Yulan Guo},
  doi          = {10.1109/TMM.2025.3607739},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DuInNet: Dual-modality feature interaction for point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module-a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: (i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and (ii) Attribute binding, ensuring that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models like Stable Diffusion and Gligen, markedly enhancing models' performance in addressing these key challenges. We assess our technique on the well-established CompBench and TIFA score benchmarks, and HRS dataset where B2B not only surpasses methods specialized in either attribute binding or layout guidance but also uniquely excels by integrating these capabilities to deliver enhanced overall performance.},
  archive      = {J_TMM},
  author       = {Ashkan Taghipour and Morteza Ghahremani and Mohammed Bennamoun and Aref Miri Rekavandi and Hamid Laga and Farid Boussaid},
  doi          = {10.1109/TMM.2025.3607759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in point cloud compression (PCC) is how to evaluate the perceived distortion so that the codec can be optimized for perceptual quality. Current standard practices in PCC highlight a primary issue: while single-feature metrics are widely used to assess compression distortion, the classic method of searching point-to-point nearest neighbors frequently fails to adequately build precise correspondences between point clouds, resulting in an ineffective capture of human perceptual features. To overcome the related limitations, we propose a novel assessment method called RBFIM, utilizing radial basis function (RBF) interpolation to convert discrete point features into a continuous feature function for the distorted point cloud. By substituting the geometry coordinates of the original point cloud into the feature function, we obtain the bijective sets of point features. This enables an establishment of precise corresponding features between distorted and original point clouds and significantly improves the accuracy of quality assessments. Moreover, this method avoids the complexity caused by bidirectional searches. Extensive experiments on multiple subjective quality datasets of compressed point clouds demonstrate that our RBFIM excels in addressing human perception tasks, thereby providing robust support for PCC optimization efforts.},
  archive      = {J_TMM},
  author       = {Zhang Chen and Shuai Wan and Siyu Ren and Fuzheng Yang and Mengting Yu and Junhui Hou},
  doi          = {10.1109/TMM.2025.3607782},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRFCNet: Feature refinement and flexible concatenation for object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art YOLO detection algorithms still suffer from the issue of redundant extraction of similar features during feature propagation, and the simplistic stacking approach of connecting different features limits the flexibility of feature fusion. We propose a new feature recombination mechanism involving refining feature extraction and flexible concatenation. It includes the HFConv (Hybrid Flexibility Convolution) module, the MFD (Multivariate Flexibility Downsampling) module, and the DFSPP (Deformable and Flexible Spatial Pyramid Pooling) module. Specifically, the HFConv module employs feature refinement and flexible connection strategies to optimize feature representation and reduce redundancy in a dynamic way, acquiring diverse feature information from local and surrounding regions. The MFD module leverages multiple downsampling methods to address the issue of feature redundancy that may arise from a single downsampling method, thereby enhancing feature diversity. The DFSPP module learns an offset corresponding to the pooling kernel size, allowing for the extraction of the most critical information in a dynamic manner. By incorporating these modules into the YOLO architecture, we develop a more robust network called FRFCNet, and the experimental results show a notable 4.1% and 2.8% improvement in AP values on the VOC2012 and COCO2017 datasets, respectively, compared to the baseline (YOLOV7-Tiny-SiLu), outperforming current one-stage detectors.},
  archive      = {J_TMM},
  author       = {Tao Zhang and Zhiheng Wu and Xiangjian He and Qiang Wu},
  doi          = {10.1109/TMM.2025.3607701},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FRFCNet: Feature refinement and flexible concatenation for object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrackletGait: A robust framework for gait recognition in the wild. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8% and 80.4% rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.},
  archive      = {J_TMM},
  author       = {Shaoxiong Zhang and Jinkai Zheng and Shangdong Zhu and Chenggang Yan},
  doi          = {10.1109/TMM.2025.3607705},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TrackletGait: A robust framework for gait recognition in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriPHiT: Privacy-preserving hierarchical training of deep neural networks. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training phase of deep neural networks requires substantial resources and as such is often performed on cloud servers. However, this raises privacy concerns when the training dataset contains sensitive content, e.g., facial or medical images. In this work, we propose a method to perform the training phase of a deep learning model on both an edge device and a cloud server that prevents sensitive content being transmitted to the cloud while retaining the desired information. The proposed privacy-preserving method uses adversarial early exits to suppress the sensitive content at the edge and transmits the task-relevant information to the cloud. This approach incorporates noise addition during the training phase to provide a differential privacy guarantee. We extensively test our method on different facial and medical datasets with diverse attributes using various deep learning architectures, showcasing its outstanding performance. We also demonstrate the effectiveness of privacy preservation through successful defenses against different white-box, deep and GAN-based reconstruction attacks. This approach is designed for resource-constrained edge devices, ensuring minimal memory usage and computational overhead.},
  archive      = {J_TMM},
  author       = {Yamin Sepehri and Pedram Pad and Pascal Frossard and L. Andrea Dunbar},
  doi          = {10.1109/TMM.2025.3607801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PriPHiT: Privacy-preserving hierarchical training of deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screenshot, which is a common tool in office work, has become a significant threat to organizations like companies and research institutions. Malicious users can easily leak sensitive information like business secrets and research data by taking a screenshot and spreading onto the Internet. While existing watermarking schemes serve as useful tools for leakage tracing, they fall short in the scenario of arbitrary screenshot. Most current methods are file-targeted, focusing on embedding watermark for a single file of one type at a time, making it hard to handle arbitrary content on screen. To address the issues above and better satisfy the need of the scenario, we propose ScreenGuard, a novel watermarking scheme targeted for the screen itself to protect arbitrary screen content shown on it. Unlike previous watermarking schemes, ScreenGuard does not modify the content itself. Instead, we generate a transparent mask template based on the watermark, tile it to the size of the screen to form a complete transparent mask, and overlay this mask onto the screen. This ensures that any screenshots taken will contain our watermark. We then train a locator and a decoder to extract watermarks from suspected leaked screenshots to trace leaks to their source. We summarized five properties that needs to be satisfied in the scenario of arbitrary screenshot (Generalizable, Unseeable, Adaptable, Robust, Dynamic) and evaluate our method on these criteria. Extensive experiments demonstrate that ScreenGuard meets these five properties effectively, showcasing its superiority and broad practical applications.},
  archive      = {J_TMM},
  author       = {Gaozhi Liu and Xiujian Liang and Xiaoxiao Hu and Yichao Si and Xinpeng Zhang and Zhenxing Qian},
  doi          = {10.1109/TMM.2025.3607779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical semantic distillation framework for open-vocabulary object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, e.g., CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin.},
  archive      = {J_TMM},
  author       = {Shenghao Fu and Junkai Yan and Qize Yang and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3607729},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hierarchical semantic distillation framework for open-vocabulary object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-attention transformers for class-incremental learning: A tale of two memories. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (Class-IL) aims to continuously learn a model from a sequence of tasks, which suffers from the issue of catastrophic forgetting. Recently, a few transformer based methods are proposed to address this issue by transferring self-attention into task-specific attention. However, these methods utilize shared task-specific attention modules across the whole incremental learning process, and are unable to achieve the balance between consolidation and plasticity, i.e., to remember the knowledge learned from previous tasks and absorb the knowledge from the current task simultaneously. Motivated by the mechanism of LSTM and hippocampus memory, we point out that dual attention on long and short-term memories can handle the consolidation-plasticity dilemma of Class-IL. Typically, we propose Dual-Attention Transformers (DAFormer) to learn external attention and internal attention. The former utilizes sample-dependent keys which exclusively focused on the new tasks, while the latter consolidates the knowledge from previous tasks by using sample-agnostic keys. We present two editions of DAFormer: DAFormer-S and DAFormer-M: the former utilizes shared external keys and maintains a small parameter size, while the latter utilizes multiple external keys and enhances the long-term memory. Furthermore, we propose the $K$-nearest neighbor invariant based distillation scheme, which distills knowledge from previous tasks to current task by maintaining the same neighborhood relationship of each sample over old and new models. Experimental results on CIFAR-100, ImageNet-subset and ImageNet-full demonstrate that DAFormer significantly outperforms all the state-of-the-art parameter-static and parameter-growing methods.},
  archive      = {J_TMM},
  author       = {Shaofan Wang and Weixing Wang and Yanfeng Sun and Zhiyong Wang and Boyue Wang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3607800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-attention transformers for class-incremental learning: A tale of two memories},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-aware dynamic label assignment with high-quality proposals for oriented object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented objects in images are typically embedded in complex backgrounds and exhibit arbitrary orientations. When using oriented bounding boxes (OBBs) to represent these objects, the periodicity of the angles and associated variations in side lengths lead to discontinuities in the angle loss. This paper fundamentally addresses this problem by proposing a trigonometric loss function in the complex plane. Moreover, a conformer RPN head is designed with convolution and multi-head self-attention, which can dynamically capture angular and classification information. The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals. A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for oriented proposal label assignment. This method makes negative sample selection more representative, ensuring consistency between classification and regression features. Experiments were conducted on five realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs. Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, 46.45%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, STAR, and HRSC2016 datasets, respectively.},
  archive      = {J_TMM},
  author       = {Mingkui Feng and Hancheng Yu and Xiaoyu Dang and Ming Zhou},
  doi          = {10.1109/TMM.2025.3607785},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-aware dynamic label assignment with high-quality proposals for oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively representing and transferring user preferences across various domains presents a significant challenge in cross-domain recommendation (CDR). Some approaches utilize graph neural networks that use interaction behavior to establish relationships between entities, providing a comprehensive understanding of user interests. However, the impact of consistent semantics across various types, fields, and perspectives of social media information on user preferences is overlooked, i.e. the multidimensional consistency of user preferences. This oversight results in graph node representations that inadequately reflect user preferences. To address these limitations, we propose a multi-layer transfer learning network (MTLG) for CDR based on graph node representation enhancement via multi-dimensional consistent user preferences. Firstly, the model introduces a set of globally shared semantic units to perform different-grained semantic alignment of multiple media information without clear alignment boundaries, thereby modeling multi-dimensional consistent user preference features. These features are then seamlessly integrated with the initial high-order graph structure embedding features, thus significantly improving the quality of graph node representation. Secondly, the model innovatively designs a multi-layer transfer learning network that hierarchically aligns the domain distribution differences. It calculates the similarity between domains to derive layer weights for more precise transfer learning, thereby mitigating the possibility of information error accumulation resulting from inaccurate feature aggregation processes. We conducted numerous experiments on 3 scenarios, including 7,954,943 rating information from the Amazon dataset. The results indicate that MTLG's recommendation accuracy surpasses those of state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xin Ni and Jie Nie and Niantai Jing and Jianliang Xu and Xiaodong Wang and Xuesong Gao and MingXing Jiang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3607706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth map super-resolution via deep cross-modality and cross-scale guidance. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided depth super-resolution is essential in many applications, which enhances low-resolution (LR) depth maps using high-resolution (HR) RGB images from the same scene. However, the challenge lies in avoiding the texture-copy artifacts issue caused by structural inconsistencies between two modalities. To mitigate, we propose a cross-modality and cross-scale guided depth super-resolution network (D2CNet). We first design a novel two-stage feature integration module to effectively fuse multi-modal RGB and depth while minimizing texture-copy artifacts. That is, a cross-modality fusion stage transfers consistent structures from RGB to depth in a multi-scale manner, and a cross-scale refinement stage mitigates inconsistent structures across modalities. In addition, we design a convolution group as the basic module to well extract high-frequency features and an LR and HR domain projection strategy to enrich features between the fusion and refinement stages. We then develop a new network architecture by progressively repeating the feature integration module and the convolution group, which is flexibly controllable to strike a balance between accuracy and cost for easy implementation in real world. Extensive experiments on multiple benchmarks demonstrate that our D2CNet consistently achieves superior accuracy and generalization ability across sampling scales in both qualitative and quantitative evaluations, when compared to state-of-the-art baselines. The code is at https://github.com/suzdl/d2cnet},
  archive      = {J_TMM},
  author       = {Shuzhe Liu and Delong Suzhang and Meng Yang and Xinhu Zheng and Ce Zhu},
  doi          = {10.1109/TMM.2025.3607763},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Depth map super-resolution via deep cross-modality and cross-scale guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to identify novel compositions formed by known primitives (attributes and objects). Motivated by recent advancements in pre-trained vision-language models such as CLIP, many methods attempt to fine-tune CLIP for CZSL and achieve remarkable performance. However, the existing CLIP-based CZSL methods focus mainly on text prompt tuning, which lacks the flexibility to dynamically adapt both modalities. To solve this issue, an intuitive solution is to additionally introduce visual prompt tuning. This insight is not trivial to achieve because effectively learning prompts for CZSL involves the challenge of entanglement between visual primitives as well as appearance shifts in different compositions. In this paper, we propose a novel Synergetic Prompts as Disentanglement Queries (SPDQ) framework for CZSL. It can disentangle primitive features based on synergetic prompts to jointly alleviate these challenges. Specifically, we first design a low-rank primitive modulator to produce synergetic adaptive attribute and object prompts based on prior knowledge of each instance for model adaptation. Then, we additionally utilize text prefix prompts to construct synergetic prompt queries, which are used to resample corresponding visual features from local visual patches. Comprehensive experiments conducted on three benchmarks demonstrate that our SPDQ approach achieves state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Han Jiang and Xiaoshan Yang and Chaofan Chen and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607726},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning trimaps via clicks for image matting. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant advancements achieved in image matting, the existing models heavily depend on manually drawn trimaps to produce accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming and lacks user-friendliness and device compatibility. This greatly limits the practical applicability of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model that is capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. By analyzing real users' behavioral logic and the characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated simulation function, making Click2Trimap exhibit versatility across various scenarios. Compared with all existing trimap-free matting methods, Click2Trimap achieves superior performance in quantitative and qualitative assessments conducted on synthetic and real-world matting datasets. In particular, in a user study, Click2Trimap yields high-quality trimap and matting predictions in just 5 seconds per image on average, demonstrating its substantial practical value for use in real-world applications.},
  archive      = {J_TMM},
  author       = {Chenyi Zhang and Yihan Hu and Henghui Ding and Humphrey Shi and Yao Zhao and Yunchao Wei},
  doi          = {10.1109/TMM.2025.3607710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning trimaps via clicks for image matting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although screen-camera resilient watermarking addresses issues such as privacy leakage and copyright infringement in digital images to some extent during screen-camera communication. However, in screen-camera scenarios, uncontrolled shooting environments, various display devices, and different lens types introduce more complex noise into the watermarked images. Because some noise generated during the screen-camera process cannot be quantitatively analyzed, the integrity of the embedded watermark is compromised, making copyright verification and information acquisition still difficult. To solve this problem, we establish a large-scale screen-camera image dataset (SCISet) and propose a noise simulation network (NoS-Net). Specifically, we obtain 36,000 screen-camera images under various shooting environments with multiple types of screens and cameras. Then, we use SCISet to train the proposed NoS-Net based on the U-Net architecture, which can learn multi-level and complementary feature information of screen-camera images, enhancing its ability to simulate complex noise. Experimental results show that integrating the proposed NoS-Net into mainstream screen-camera resilient watermarking methods significantly improves their ability to resist screen-camera noise attacks. Furthermore, the diversity of SCISet plays an important role in advancing robust watermarking research.},
  archive      = {J_TMM},
  author       = {Daidou Guo and Chuan Qin and Fengyong Li and Heng Yao and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3607783},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD). However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture. To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and total Floating Point Operations (FLOPs). Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework. Simulations show that the proposed model is robust against noise and negative samples. Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest parameters and FLOPs. Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities.},
  archive      = {J_TMM},
  author       = {Ruituo Wu and Yang Chen and Jian Xiao and Bing Li and Jicong Fan and Frédéric Dufaux and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TMM.2025.3607708},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oversampling with GAN via meta-learning for imbalanced data. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing generative adversarial networks (GANs) for oversampling imbalanced data has demonstrated its effectiveness. However, many GAN-based oversampling methods are confronted with a significant challenge, namely, mode collapse, especially when dealing with tabular imbalanced data. In this paper, two unique penalty terms are respectively incorporated into the loss functions of the discriminator and the generator of GAN to promote the generated samples to exhibit not just statistical but also spatial information consistency with the minority samples, thereby alleviating the issue of mode collapse. In contrast to other studies that fix the coefficient of the penalty terms, the optimal coefficients of the penalty terms are adaptively searched using a meta-learning approach, where Bayesian optimization is firstly employed to effectively handle situations involving small size of minority samples in the imbalanced data. We call the proposed model as META_GAN. Experimental results demonstrate that META_GAN outperforms alternative oversampling methods on general tabular and image imbalanced datasets and long-tailed datasets in terms of different metrics.},
  archive      = {J_TMM},
  author       = {Yueqi Chen and Witold Pedrycz and Chao Zhang and Jian Wang and Jie Yang},
  doi          = {10.1109/TMM.2025.3607712},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oversampling with GAN via meta-learning for imbalanced data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting long-tailed recognition with label descriptor and beyond. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Tailed Recognition (LTR) poses significant challenges due to the heavily imbalanced nature of real-world data, which severely skews data-driven deep neural networks. Despite the rapid progress of Vision-Language Models (VLMs), they still face challenges in effectively learning from long-tailed visual data. In this paper, we present a comprehensive analysis of the reasons behind the underperformance of VLMs and propose a hierarchical inference framework to address this issue. Specifically, we prompt the large language models to generate sentencelevel descriptors for class labels and conduct the open vocabulary classification by computing the average similarity between the image and each descriptor. A reweighting mechanism is further proposed to filter out uninformative descriptors. To mitigate model bias incurred by the long-tail distribution, we propose a feature adapter with the logit adjustment technique and finetune the CLIP model via visual prompt tokens. We introduce the Shared Feature space Mixup (SFM) to enhance the interaction between modalities to address tail visual feature insufficiency. Finally, we propose a hierarchical inference manner to combine the aforementioned proposals. Extensive evaluations demonstrate that our approach achieves state-of-the-art performance by finetuning only a few parameters on the Places-LT, ImageNet-LT, and iNaturalist 2018 benchmarks.},
  archive      = {J_TMM},
  author       = {Zhengzhuo Xu and Ruikang Liu and Zenghao Chai and Yiyan Qi and Lei Li and Haiqin Yang and Chun Yuan},
  doi          = {10.1109/TMM.2025.3607812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting long-tailed recognition with label descriptor and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive action quality assessment through multi-branch modeling. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA) aims to evaluate and score human actions in videos accurately. Existing approaches involve extracting features from the input video and implementing regression based on those features. However, representations derived from a single branch often lack the necessary diversity and flexibility to capture the complexity of human actions effectively. This work addresses these limitations by introducing a multi-branch architecture designed to capture a broad spectrum of video dynamics at varying levels of granularity. Specifically, we enhance video representation in the flow-guided branch by integrating optical flow with video features. This combination of multimodal features offers a more comprehensive context of global motion. Meanwhile, the momentfocused branch is tailored to extract frame-specific features, constructing two distinct quality-based representations with different focuses on moments, which achieves adaptive clues aggregation. Furthermore, the detail-aware branch leverages multiscale deep embeddings from a hierarchy convolutional neural network to capture fine-grained spatial information, which is useful when objects have complex spatial changes. Finally, a post-fusion strategy is employed to merge outputs from all branches, contributing to the comprehensive action quality assessment. Experimental evaluations on three benchmark datasets, FineDiving, MTLAQA, and AQA-7, demonstrate the superiority of our model in providing reliable assessments of action quality.},
  archive      = {J_TMM},
  author       = {Siyuan Xu and Peilin Chen and Yue Liu and Meng Wang and Shiqi Wang and Hong Yan and Sam Kwong},
  doi          = {10.1109/TMM.2025.3607713},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comprehensive action quality assessment through multi-branch modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the optimization of quality of experience (QoE) through learning adaptive bitrate (ABR) algorithms from internet video streams. However, the complex nature of the real-world Internet, characterized by heavy-tailed behavior, diversity, and unpredictability, hinder the effective learning of off-the-shelf reinforcement learning (RL)-based ABR algorithms. As a result, existing methods inevitably fail to achieve optimal performance under various network conditions and user QoE objectives. We propose Fortuna, a novel offline meta RL ABR algorithm that can effectively learn from these heavy-tailed internet data features and become more practical. Fortuna is primarily divided into two phases. In the offline phase, Fortuna utilizes diverse offline data for learning to reduce the costly online RL interaction expense, while in the online phase, we gradually increase video streaming sessions complexity through curriculum learning to quickly adapt to specific network conditions. Fortuna then utilizes meta-learning to optimize ABR policies and enhance generalization. Additionally, to better learn network features, Fortuna further optimizes QoE by learning low-level TCP congestion control information. Experimental results from trace-driven and real-world scenarios demonstrate that Fortuna enhances learning efficiency by more than 7.5%-4 ×, reduces stall time by 4.6%-14.2%, and generalizes to different network conditions and video streams.},
  archive      = {J_TMM},
  author       = {Ling Yi and Yongbin Qin and Ruizhang Huang},
  doi          = {10.1109/TMM.2025.3604930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal transformer for cross-modal long document classification. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3608295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.},
  archive      = {J_TMM},
  author       = {Tengfei Liu and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1109/TMM.2025.3608295},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-modal transformer for cross-modal long document classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USTC-TD: A test dataset and benchmark for image and video coding in 2020s. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3608643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image/video coding has been a remarkable research area for both academia and industry for many years. Testing datasets, especially high-quality image/video datasets, are desirable for the justified evaluation of coding-related research, practical applications, and standardization activities. We put forward a test dataset, namely USTC-TD, which has been successfully adopted in the practical end-to-end image/video coding challenge of IEEE International Conference on Visual Communications and Image Processing (VCIP) in 2022 and 2023. USTC-TD contains 40 images at 4K spatial resolution and 10 video sequences at 1080p spatial resolution, featuring various content due to the diverse environmental factors (e.g., scene type, texture, motion, view) and the designed imaging factors (e.g., illumination, lens, shadow). We quantitatively evaluate USTC-TD on different image/video features (spatial, temporal, color, lightness), and compare it with the previous image/video test datasets, which verifies its excellent compensation for the shortcomings of existing datasets. We also evaluate both classic standardized and recently learned image/video coding schemes on USTC-TD using objective quality metrics (PSNR, MS-SSIM, VMAF) and subjective quality metric (MOS), providing an extensive benchmark for these evaluated schemes. Based on the characteristics and specific design of the proposed test dataset, we analyze the benchmark performance and shed light on the future research and development of image/video coding. All the data are released online: https://esakak.github.io/USTC-TD.},
  archive      = {J_TMM},
  author       = {Zhuoyuan Li and Junqi Liao and Chuanbo Tang and Haotian Zhang and Yuqi Li and Yifan Bian and Xihua Sheng and Xinmin Feng and Yao Li and Changsheng Gao and Li Li and Dong Liu and Feng Wu},
  doi          = {10.1109/TMM.2025.3608643},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {USTC-TD: A test dataset and benchmark for image and video coding in 2020s},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JPEG AI compressed domain face detection: A multi-scale bridging perspective. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3609179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image coding is showing improved compression efficiency, while also offering a novel advantage in enabling computer vision tasks directly within the compressed domain. The latent representation created by deep learning methods inherently contains all visual features, without a computationally expensive synthesis process at the decoder. This paper is an invited extension of a previous solution for JPEG AI compressed domain face detection that adapts a RetinaFace-based detector to operate directly on the latent tensor. In addition to a former single-scale bridging solution, this work provides a novel multi-scale bridging architecture to enable a more effective multi-scale compressed domain face detection. The results show a significant performance gain, improving accuracy up to 20% for detection of tiny faces on the WIDER FACE dataset compared to single-scale bridging, and further narrowing the gap when compared to detection on uncompressed or JPEG AI decoded images. Furthermore, since the computationally expensive decoding step is bypassed and since the bridges consist of lower-complexity networks, the overall processing cost is significantly reduced. Single and multi-scale bridging, respectively, have about 10% and 32% the complexity of applying pixel domain face detection on decoded images. The proposed architecture is expected to be extended to other multiscale sensitive vision tasks, as JPEG AI is not specifically designed for any single downstream application.},
  archive      = {J_TMM},
  author       = {Ayman Alkhateeb and Alessandro Gnutti and Fabrizio Guerrini and Riccardo Leonardi and João Ascenso and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3609179},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {JPEG AI compressed domain face detection: A multi-scale bridging perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-led prompting learning caption generating model and benchmark. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) is an important intelligent system application, but most current research views it as a coarse binary classification task that lacks a fine-grained understanding of abnormal video sequences. We explore a new task for video anomaly analysis called Comprehensive Video Anomaly Caption (CVAC), which aims to generate comprehensive textual captions (containing scene information such as time, location, anomalous subject, anomalous behavior, etc.) for surveillance videos. CVAC is more consistent with human understanding than VAD, but it has not been well explored. We constructed a large-scale benchmark CVACBench to lead this research. For each video clip, we provide 6 fine-grained annotations, including scene information and abnormal keywords. A new evaluation metric Abnormal-F1 (A-F1) is also proposed to more accurately evaluate the caption generation performance of the model. We also designed a method called Anomaly-Led Generating Prompting Transformer (AGPFormer) as a baseline. In AGPFormer, we introduce an anomaly-led language modeling mechanism (Anomaly-Led MLM, AMLM) to focus on anomalous events in videos. To achieve more efficient cross-modal semantic understanding, we design the Interactive Generating Prompting (IGP) module and Scene Alignment Prompting (SAP) module to explore the divide between video and text modalities from multiple perspectives, and to improve the model's performance in understanding and reasoning about the complex semantics of videos. We conducted experiments on CVACBench by using traditional caption metrics and the proposed metrics, and the experimental results demonstrate the effectiveness of AGPFormer in the field of anomaly caption.},
  archive      = {J_TMM},
  author       = {Qianyue Bao and Fang Liu and Licheng Jiao and Yang Liu and Shuo Li and Lingling Li and Xu Liu and Xinyi Wang and Baoliang Chen},
  doi          = {10.1109/TMM.2025.3607837},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anomaly-led prompting learning caption generating model and benchmark},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3608947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised text-based person re-identification (Text-ReID) confronts the challenge of matching target person images with textual descriptions, hindered by the absence of identity annotations during training. Traditional approaches, which rely solely on global features, overlook the rich, fine-grained information within both text and image modalities. Besides, merely aligning features at the semantic level is insufficient due to the significant differences in feature representation spaces between the two modalities. Existing methods also neglect the information inequality caused by person-irrelevant factors in images. In this paper, we introduce a novel framework called Attribute-Centric Cross-modal Alignment (ACCA), specifically designed to overcome these issues. Our approach concentrates on two main aspects: visual-text attribute alignment and prediction distribution alignment. To effectively capture fine-grained information without identity labels, we implement a visual-text attribute alignment method based on momentum contrastive learning to synchronize visual and textual attribute features within a unified embedding space. We also propose a unique strategy for negative sample filtering and enrichment, creating robust and comprehensive negative attribute sample spaces to support the attribute alignment. Additionally, we establish two methods of label-free prediction distribution alignment to encourage the learning of invariant feature representations across modalities. The first method, bias-reduction distribution alignment, aligns features and predictions within each text-image pair by utilizing semantic information from the text and reduces the impact of person-irrelevant factors in images. The second method, global-attribute distribution alignment, enhances the interaction between global and local prediction distributions across visual and textual modalities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets validate our superior performances across all standard benchmarks.},
  archive      = {J_TMM},
  author       = {Jiajia Xu and Weiwei Cai and Xuemiao Xu and Yi Xie and Huaidong Zhang and Shengfeng He},
  doi          = {10.1109/TMM.2025.3608947},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSG: Multi-scale speaker graph network for active speaker detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3608949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The active speaker detection task is to determine whether a person is speaking or not across a series of video frames. Existing methods heavily rely on facial information within the annotated face bounding boxes for cross-modal learning with audio. This leads to a substantial decline in detection performance when facial cues are unclear, such as in cases of face occlusion or low-resolution facial appearances. In this paper, we extend the perception scale using only face bounding box annotations to model both facial and gestural cues, addressing the over-reliance on facial cues in active speaker detection. We propose a novel graph neural network that models inter-speaker interactions and integrates various cues from individual speakers. The final detection results are obtained through a binary graph node classification task. Our method achieves state-of-the-art performance on the AVA-ActiveSpeaker dataset (mAP: 95.6%) and the ASW dataset (mAP: 99.4%), with a model size only 21% that of the second-best method. Additionally, when facial cues are of poor quality, our method demonstrates a significant performance advantage over existing approaches. The code and model weights will be available at https://github.com/sdqdlgj/MSSG.},
  archive      = {J_TMM},
  author       = {Guanjun Li and Jiangyan Yi and Zhengqi Wen and Ruibo Fu and Yuwang Wang and Jianhua Tao},
  doi          = {10.1109/TMM.2025.3608949},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSSG: Multi-scale speaker graph network for active speaker detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning orthogonal latent representations for multi-view clustering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view clustering, latent representations are often employed to address the challenge posed by low-quality data. Traditional approaches typically assume that multiple views are fully dependent, directly learning a common latent representation from the observed data. However, this assumption is overly restrictive in real-world scenarios and may overlook valuable information, as the independence of different views can reveal critical view-specific characteristics. To overcome this limitation, we propose learning Orthogonal Latent Representations for Multi-View Clustering (OLR-MVC), which jointly captures both cross-view dependence and independence. Specifically, our model maps multi-view data into shared and private latent spaces using distinct projection bases. To accurately capture both dependence and independence, we enforce orthogonality between the shared and private latent representations while also encouraging pairwise orthogonality among private representations. Furthermore, we leverage the self-expressive property of these latent representations to capture global data structures. Extensive experimental evaluations demonstrate that OLR-MVC outperforms state-of-the-art multi-view clustering methods.},
  archive      = {J_TMM},
  author       = {Xiaolin Xiao and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TMM.2025.3607704},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning orthogonal latent representations for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dehazing approach: Recovery of color and polarization information using polarized characteristics. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarization provides valuable physical information, making it beneficial for various computer vision tasks. However, haze reduces both the color and polarization information of a scene. While existing single-image dehazing methods can restore color information, they are poor at recovering polarization information. Furthermore, current polarization-based dehazing approaches neglect the physical mechanisms of polarization degradation, resulting in inaccurate reconstruction of polarization information. In this paper, we propose a novel polarization dehazing algorithm, along with a polarization degradation model, to accurately recover both polarization and color information. First, we combine two key characteristics (the polarization achromatism prior and polarization attenuation prior) with the polarization degradation model to precisely reconstruct the scene's polarization. Then, we utilize the reconstructed polarization information to recover the color information of the scene. Finally, a multi-scale fusion optimization framework is introduced to further enhance the image quality. Our method shows excellent performance on both real-world indoor and outdoor polarized images, outperforming existing dehazing algorithms in both objective evaluation metrics and subjective visual assessment.},
  archive      = {J_TMM},
  author       = {Zhenshuo Yang and Chunhui Hao and Yang Lu and Yiming Su and Yukuan Zhang and Junchao Zhang and Jiandong Tian},
  doi          = {10.1109/TMM.2025.3607694},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel dehazing approach: Recovery of color and polarization information using polarized characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained domain generalization with feature structuralization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained domain generalization (FGDG) is a more challenging task than traditional DG tasks due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the vulnerability of subtle features leads to a severe deterioration in model performance. Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning the commonality and specificity within categories. Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is accomplished through joint optimization of five constraints: a decorrelation function applied to disentangled segments, three constraints ensuring common feature consistency and specific feature distinctiveness, and a prediction calibration term. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.2% in FGDG performance. Beyond that, the explainability analysis on explicit concept matching intensity between the shared concepts among categories and the model channels, along with experiments on various mainstream model architectures, substantiates the validity of FS.},
  archive      = {J_TMM},
  author       = {Wenlong Yu and Dongyue Chen and Qilong Wang and Qinghua Hu},
  doi          = {10.1109/TMM.2025.3607716},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained domain generalization with feature structuralization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detailed object description with controllable dimensions. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object description plays an important role for visually impaired individuals to understand and compare the differences between objects. Recent multimodal large language models (MLLMs) exhibit powerful perceptual abilities and demonstrate impressive potential for generating object-centric descriptions. However, the descriptions generated by such models may still usually contain a lot of content that is not relevant to the user intent or miss some important object dimension details. Under special scenarios, users may only need the details of certain dimensions of an object. In this paper, we propose a training-free object description refinement pipeline, Dimension Tailor, designed to enhance user-specified details in object descriptions. This pipeline includes three steps: dimension extracting, erasing, and supplementing, which decompose the description into user-specified dimensions. Dimension Tailor can not only improve the quality of object details but also offer flexibility in including or excluding specific dimensions based on user preferences. We conducted extensive experiments to demonstrate the effectiveness of Dimension Tailor on controllable object descriptions. Notably, the proposed pipeline can consistently improve the performance of the recent MLLMs. The code is currently accessible at https://github.com/PRIS-CV/ControllableObjectDescription.},
  archive      = {J_TMM},
  author       = {Xinran Wang and Haiwen Zhang and Baoteng Li and Kongming Liang and Hao Sun and Zhongjiang He and Zhanyu Ma and Jun Guo},
  doi          = {10.1109/TMM.2025.3607747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detailed object description with controllable dimensions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StableIdentity: Inserting anybody into anywhere at first sight. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in large pretrained text-to-image generation models have shown unprecedented capabilities for high-quality human-centric generation, however, customizing face identity is still an intractable problem. Existing methods cannot ensure stable identity preservation and flexible editability, even with several images for each subject during training. In this work, we propose StableIdentity, which allows identity-consistent recontextualization with just one face image from a person seen for the first time. More specifically, we employ a face encoder with the identity prior to encode the input face, and then calibrate the face representation to align the distribution of a space with the editability prior, which is constructed from celeb names. By incorporating identity prior and editability prior, the learned identity can be injected anywhere with various contexts. In addition, we design a masked two-phase diffusion loss to boost the pixel-level perception of the input face and maintain the diversity of generation. Extensive experiments demonstrate our method outperforms previous customization methods. In addition, the learned identity can be flexibly combined with the off-theshelf modules such as ControlNet. Notably, to the best of our knowledge, we are the first to directly inject the identity learned from a single image into video/3D generation without finetuning. We believe that the proposed StableIdentity is an important step to unify image, video, and 3D customized generation models. The code is available: https://github.com/qinghew/StableIdentity.},
  archive      = {J_TMM},
  author       = {Qinghe Wang and Xu Jia and Xiaomin Li and Taiqing Li and Liqian Ma and Yunzhi Zhuge and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3613113},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StableIdentity: Inserting anybody into anywhere at first sight},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised contrastive learning for indoor point cloud oversegmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud oversegmentation method can obtain a series of superpoints by grouping points that are semantically and geometrically consistent. The generated superpoints can be treated as the basic processing units in various downstream tasks to improve task performance and processing efficiency. However, due to the high semantic and geometric complexity of point cloud scenes, obtaining high-quality superpoints is still challenging. Aiming to generate high-quality indoor superpoints, we propose an end-to-end supervised contrastive learning framework SCL-OverSeg for indoor point cloud oversegmentation. Firstly, to solve the challenge of balancing the importance of geometric similarity and spatial proximity constraint between points and superpoints in indoor scenes, we integrate the geometric similarity and spatial proximity constraint into the supervision signal by generating the superpoint ground truth. To solve the challenge of superpoints crossing objects, we propose to utilize instance labels rather than semantic labels to generate the ideal superpoint ground truth as the object-level supervision signal. Secondly, to construct the distinguishable embedding space facilitating to the assignments of points to superpoints, we propose point-superpoint contrastive learning to compel the network to project each point to be closer to the reasonable superpoint in embedding space. Besides, with the instance labels, to improve the superpoint performance on object boundaries, we propose the object boundary contrastive learning to enhance the feature distinguishability between tough points across the object boundaries. Extensive experiments demonstrate that SCL-OverSeg can effectively improve indoor oversegmentation performance, especially on object boundaries. The relevant codes will be available on https://github.com/sssssyf/SCL-OverSeg.},
  archive      = {J_TMM},
  author       = {Yifan Sun and Chenguang Dai and Wenke Li and Yongsheng Zhang and Song Ji and Anzhu Yu and Yiping Chen and Hanyun Wang},
  doi          = {10.1109/TMM.2025.3613177},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Supervised contrastive learning for indoor point cloud oversegmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-driven sampling for efficient pairwise comparison subjective assessment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing image quality is crucial in image processing tasks such as compression, super-resolution, and denoising. While subjective assessments involving human evaluators provide the most accurate quality scores, they are impractical for large-scale or continuous evaluations due to their high cost and time requirements. Pairwise comparison subjective assessment tests, which rank image pairs instead of assigning scores, offer more reliability and accuracy but require numerous comparisons, leading to high costs. Although objective quality metrics are more efficient, they lack the precision of subjective tests, which are essential for benchmarking and training learning-based quality metrics. This paper proposes an uncertainty-based sampling method to optimize the pairwise comparison subjective assessment process. By utilizing deep learning models to estimate human preferences and identify pairs that need human labeling, the approach reduces the number of required comparisons while maintaining high accuracy. The key contributions include modeling uncertainty for accurate preference predictions and for pairwise sampling. The experimental results demonstrate superior performance of the proposed approach compared to traditional active sampling methods. An implementation of the pairwise sampling method is publicly available at https://github.com/shimamohammadi/LBPS-EIC},
  archive      = {J_TMM},
  author       = {Shima Mohammadi and João Ascenso},
  doi          = {10.1109/TMM.2025.3613160},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-driven sampling for efficient pairwise comparison subjective assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PADNet: Progressive-difference-aware feature reconstruction mechanism for anomaly detection. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection generally aims to identify irregularities using only normal samples, where feature reconstruction-based methods demonstrate greater robustness to noise by comparing reconstructed results with original data. However, they encounter issues with detailed information loss and insufficient anomaly discriminability. To address these challenges, we propose a progressive-difference-aware feature reconstruction network for image anomaly detection, named PADNet. To enhance context interaction, we develop a harmonic symmetric reconstruction framework integrated with a progressive feature harmonizer (PFH). The PFH mitigates detailed information loss to reduce undesired reconstruction errors through the progressive fusion of information flows. To enhance anomaly discriminability, we introduce the neighbor-aided residual feature representation module (NRFR) to strengthen difference-aware feature representations. The NRFR innovatively captures discriminative cues by interacting with neighboring reference samples in the feature cache pool. Experimental results on the MVTec, Visa, and BTAD datasets demonstrate that our method achieves superior performance while requiring only 25.3% of the parameters compared to the state-of-the-art baseline. Code is available at: https://github.com/haaloowo/PADNet.},
  archive      = {J_TMM},
  author       = {Fan Yang and Peiguang Jing and Weiming Wang and Fu Lee Wang and Yuting Su},
  doi          = {10.1109/TMM.2025.3613127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PADNet: Progressive-difference-aware feature reconstruction mechanism for anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GrabDAE: An innovative framework for unsupervised domain adaptation utilizing grab-mask and denoise auto-encoder. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Unsupervised Domain Adaptation (UDA) methods often fall short in fully leveraging contextual information from the target domain, leading to suboptimal decision boundary separation during source and target domain alignment. To address this, we introduce GrabDAE, an innovative UDA framework designed to tackle domain shift in visual classification tasks. GrabDAE incorporates two key innovations: the Grab-Mask module, which blurs background information in target domain images, enabling the model to focus on essential, domain-relevant features through contrastive learning; and the Denoising Auto-Encoder (DAE), which enhances feature alignment by reconstructing features and filtering noise, ensuring a more robust adaptation to the target domain. These components empower GrabDAE to effectively handle unlabeled target domain data, significantly improving both classification accuracy and robustness. Extensive experiments on benchmark datasets, including VisDA-2017, Office-Home, and Office31, demonstrate that GrabDAE consistently surpasses state-of-the-art UDA methods, setting new performance benchmarks. By tackling UDA's critical challenges with its novel feature masking and denoising approach, GrabDAE offers both significant theoretical and practical advancements in domain adaptation.},
  archive      = {J_TMM},
  author       = {Junzhou Chen and Xuan Wen and Ronghui Zhang and Bingtao Ren and Di Wu and Zhigang Xu and Danwei Wang},
  doi          = {10.1109/TMM.2025.3613149},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GrabDAE: An innovative framework for unsupervised domain adaptation utilizing grab-mask and denoise auto-encoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIRE: Enhancing facial expression recognition through domain-invariant representation learning for robust generalization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose DIRE (Domain-Invariant Representation Learning for Expression), a novel approach to enhance the generalizability of facial expression recognition (FER) models in unseen domains. Traditional FER models often struggle with distribution shifts between training and test datasets, leading to significant performance drops. Based on the concept of Single-Source Domain Generalization, we introduce a novel domain augmentation technique that applies pixel-level and feature-level perturbations to domain-variant regions while preserving semantic consistency. Additionally, we incorporate semantic alignment regularization and domain information minimization loss so that domain-invariant features effectively represent facial expressions. Extensive experiments on multiple FER datasets demonstrate that our method significantly improves generalization across diverse target domains, even when trained on a single source domain. The proposed DIRE approach offers a robust solution to real-world FER tasks, where unseen domain generalizability is crucial.},
  archive      = {J_TMM},
  author       = {Heeje Kim and Yoojin Jung and Byung Cheol Song},
  doi          = {10.1109/TMM.2025.3613175},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DIRE: Enhancing facial expression recognition through domain-invariant representation learning for robust generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VQM4HAS: A real-time quality metric for HEVC videos in HTTP adaptive streaming. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In HTTP Adaptive Streaming (HAS), a video is encoded at various bitrate-resolution pairs, collectively known as the bitrate ladder, allowing users to select the most suitable representation based on their network conditions. Optimizing this set of pairs to enhance the Quality of Experience (QoE) requires accurately measuring the quality of these representations. VMAF and ITU-T's P.1204.3 are highly reliable metrics for assessing the quality of representations in HAS. However, in practice, using these metrics for optimization is often impractical for live streaming applications due to their high computational costs and the large number of bitrate-resolution pairs in the bitrate ladder that need to be evaluated. To address their high complexity, our paper introduces a new method called VQM4HAS, which extracts low-complexity features, including ($i$) video complexity features, ($ii$) frame-level encoding statistics logged during the encoding process, and ($iii$) lightweight video quality metrics. These extracted features are then fed into a regression model to predict VMAF or P.1204.3. The VQM4HAS model is designed to operate on a per bitrate-resolution pair, per-resolution, and cross-representation basis, optimizing quality predictions across different scenarios. Our experimental results demonstrate that VQM4HAS achieves a high correlation with VMAF and P.1204.3, with Pearson correlation coefficients (PCC) ranging from 0.95 to 0.96 for VMAF and 0.97 to 0.99 for P.1204.3, depending on the resolution. Despite achieving a high correlation with VMAF and P.1204.3, VQM4HAS exhibits significantly less complexity than both metrics, with 98% and 99% less complexity for VMAF and P.1204.3, respectively, making it suitable for live streaming scenarios. We also conduct a feature importance analysis to further reduce the complexity of the proposed method. Furthermore, we evaluate the effectiveness of our method by using it to predict subjective quality scores. The results show that VQM4HAS achieves a higher correlation with subjective scores at various resolutions despite its minimal complexity. The source code is available at https://github.com/cd-athena/VQM4HAS.},
  archive      = {J_TMM},
  author       = {Hadi Amirpour and Jingwen Zhu and Wei Zhou and Patrick Le Callet and Christian Timmerer},
  doi          = {10.1109/TMM.2025.3613110},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VQM4HAS: A real-time quality metric for HEVC videos in HTTP adaptive streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the model's feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused image's brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at https://github.com/HeDan-11/DM-FNet.},
  archive      = {J_TMM},
  author       = {Dan He and Weisheng Li and Guofen Wang and Yuping Huang and Shiqiang Liu},
  doi          = {10.1109/TMM.2025.3613156},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUGS-net: A lightweight and unified network for adverse weather image denoising. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising under adverse weather conditions aims to eliminate multiple weather-related noises and restore bright and clear images. Until now, most methods are task-specific while all-in-one algorithms often require a large number of parameters, limiting their model efficiency. Our theoretical analysis and statistical experiments reveal that adverse weather images in Hue channel contain rich contextual information for further processing. With this observation, we propose a novel lightweight HUe-Guided Synergistic Network (HUGS-Net) with multi-scale detail refinement. First, we design a Fourier interaction and evolution module to capture global information from Hue channel without introducing excessive network parameters. Second, we develop a lightweight residue group convolution block to model local texture features, incorporating them with global information to guide noises removal. Third, we introduce a multi-scale fusion module to enhance high-frequency details at a small feature resolution in RGB color space. With the above design, HUGS-Net further supervises and supplements refined background information. Comprehensive experiments showcase the superiority of HUGS-Net across various adverse weather datasets (e.g., image deraining, desnowing, dehazing) with the least parameter size and fast running speed.The source code will be made public after peer review process.},
  archive      = {J_TMM},
  author       = {Ting Zhang and Runjie Wang and Yuzhen Niu and Zuoyong Li and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3613104},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HUGS-net: A lightweight and unified network for adverse weather image denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted polygonal faces using vertex index similarity. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding techniques serve as a cornerstone in the protection of embedded information across diverse media. Traditionally, methods applied to 3D models have primarily focused on modifying vertex coordinates. However, this approach neglects the untapped potential of polygonal faces, which, being more abundant than vertices, offer a scalable and efficient avenue for data embedding. By leveraging polygon indices for reversible data hiding—particularly when integrated with encryption—it becomes possible to randomize the model's structure, facilitating secure modifications while preserving geometric integrity. This study introduces an innovative reversible data hiding algorithm that embeds messages within the polygon indices of encrypted 3D models. The algorithm harnesses the inherent similarities among vertex indices to conceal additional information. To maintain the consistency of polygon normal vectors, we implement a right circular shifting mechanism that systematically reorganizes the indices, ensuring that the smallest value consistently occupies the initial position. Additionally, we incorporate techniques such as leading zero count and multi-MSB prediction to enhance embedding capacity while keeping index values within permissible ranges. Experimental results demonstrate that our approach significantly outperforms conventional vertex-based methods, yielding substantial improvements in embedding efficiency. Crucially, the reversible nature of the proposed technique ensures the exact restoration of the original 3D model upon data extraction, guaranteeing zero information loss and no compromise in quality. Moreover, the algorithm is designed to integrate with vertex-based reversible data hiding techniques for encrypted 3D models, potentially enhancing data embedding capacity under compatible conditions.},
  archive      = {J_TMM},
  author       = {Yuan-Yu Tsai},
  doi          = {10.1109/TMM.2025.3613172},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reversible data hiding in encrypted polygonal faces using vertex index similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric continuity and consistency learning for self-supervised point cloud completion. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to infer the complete point clouds from incomplete ones. In real-world scenarios, where the paired data is absent, self-supervised methods have emerged as a promising solution. Although existing self-supervised methods perform well at relatively low resolutions, they suffer significant performance degradation at higher resolution primarily because they focus on point cloud reconstruction at patch-level or point-level. In this paper, we propose a self-supervised method based on Geometric Continuity and Consistency Learning (GCCL) at multi-scale level to improve the accuracy of predicting local details and global shapes of point clouds. Specifically, to capture local details, we employ a patch-topoint strategy and a coarse-fine manner for geometric continuity learning. To constrain the global shapes, we construct multiple branches for mutual supervision and utilize class priors to build a memory queue for contrasting current features, enhancing the network focus on geometric consistency learning. We evaluate GCCL on multiple datasets, and the results show that our method outperforms existing self-supervised methods by a 4.4 improvement in CD-$\ell_2$ on the synthetic PCN dataset and can generate more uniformly distributed completion results on realworld datasets},
  archive      = {J_TMM},
  author       = {Junkang Ma and Shuoyao Wang and Qi Zheng and Xiaochun Mai},
  doi          = {10.1109/TMM.2025.3613154},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geometric continuity and consistency learning for self-supervised point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RUL: Region uncertainty learning for robust face recognition. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data uncertainty refers to the degree of uncertainty in model predictions caused by data variability, challenging the model robustness in multimedia applications. The increased variability of uncontrolled face images, such as mask occlusion and image blur, increases the intra-class differences and inter-class similarities. As a result, the ambiguity in the learned features exacerbates the uncertainty of sample-to-class membership. Traditional face recognition models are deterministic point embedding models that fail to measure data uncertainty. Probabilistic embedding models, such as advanced Data Uncertainty Learning (DUL), represent each face image as a Gaussian distribution to measure data uncertainty. However, these models perform random sampling from the distribution once per training, and the sampled points may fall into different class regions, leading to training oscillation. Therefore, we propose a robust Region Uncertainty Learning (RUL) method, which adopts the entire Gaussian distribution of each sample during each training epoch, and estimates the region relations between the sample distribution region and the class region to measure the sample-to-class membership. In fact, DUL is a special case of the proposed RUL. Specifically, DUL estimates point-with-region relations and only represents absolute membership and non-membership. In contrast, RUL estimates region-with-region relations, enabling it to additionally represent incomplete membership. This more comprehensive membership measurement fully represents the uncertainty of membership, enhancing the model performance and robustness in uncontrolled scenes. Furthermore, for robust face recognition, we propose two RUL-based angular margin losses, AngleFace and RegionFace, to adaptively adjust the learning weights according to the uncertainty of membership. Finally, we comprehensively evaluate the effectiveness of RUL on various face datasets, and profoundly analyze the role of region relations. In future, we will explore the applicability of RUL in other tasks.},
  archive      = {J_TMM},
  author       = {Weiming Xiong and Mingyang Zhong and Shenglin Li and Guojun Huang and Libo Zhang},
  doi          = {10.1109/TMM.2025.3613164},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RUL: Region uncertainty learning for robust face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mix-based training strategies for learning implicit neural representations. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With coordinates as the input and RGB pixel values as the output, a neural network can be used to represent an image, which is widely known as Implicit neural representations (INRs). Previous works on INR have mainly focused on learning an invariant image target without exploring the impact of learning strategies on learning INR. It is observed that there is a substantial variation in PSNR among different images, and our preliminary investigation shows that, in the early training stage, learning complex image content yields significantly better performance than simple image content. Inspired by this finding, we conjecture that increasing INR task complexity in the early stage of training might boost INR performance and thus propose to intentionally contaminate the target image with another complex image. Our proposed method is called Mix-INR, which adopts a two-stage training to first learn a pseudo-target image (contaminated target) and then learn the real-target image (uncontaminated target). To generate the pseudo-target image, we experiment with two contamination methods (blending and replacement), both of which show superior performance and verify our conjecture. INRs have gained popularity as a promising approach for representing a variety of data types, including images of the task complexity of the pseudo-target image, we set the contamination image from a complex natural image to a random-noise image. Moreover, we propose a dynamic contamination method to smoothly transition from the pseudo-target image to the real-target image. Experimental results demonstrate that our proposed method achieves competitive performance, which suggests that INR can be improved by manipulating the task complexity in the early stage of training.},
  archive      = {J_TMM},
  author       = {Dongshen Han and Chaoning Zhang and Sheng Zheng and Fachrina Dewi Puspitasari and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3613162},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mix-based training strategies for learning implicit neural representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local and global structure-guided no-reference point cloud quality assessment. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial representation of 3D data, a point cloud (PC) can accurately capture the geometry, structure, and color information of objects. However, various quality problems arise owing to device noise, data acquisition errors, and compression algorithms, limiting the application of PCs. Therefore, assessing PC quality to determine its suitability for applications is a challenging task. In this work, a local and global structure-guided feature extraction and attention network (LGS-Net) is introduced for no-reference PC quality assessment (PCQA). This approach incorporates cluster construction (CC), local structure-guided cluster feature extraction (LSFE), and global structure-guided attention (GSA) modules. First, owing to the heightened sensitivity of the human visual system (HVS) to structural information, a graph filter is employed to identify high-frequency clusters. Within the LSFE module, a multiscale strategy is employed to ensure that structural information effectively influences both the geometry and color information. Simultaneously, the multiscale features within the cluster are dynamically fine-tuned using feature channel weight reassignment. To account for the impact of interclusters on overall quality, a GSA module is introduced to establish global dependencies between local clusters. This approach enables the extraction of final geometry, color, and structure information, which are ultimately used for accurate quality assessment. Extensive experimental results show that the proposed method outperforms the existing state-of-the-art PCQA methods using two publicly available subjective datasets.},
  archive      = {J_TMM},
  author       = {Zhouyan He and Qihao Liang and Gangyi Jiang and Mei Yu and Yeyao Chen and Ting Luo and Wujie Zhou},
  doi          = {10.1109/TMM.2025.3613114},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Local and global structure-guided no-reference point cloud quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and robust video virtual try-on via enhanced multi-garment alignment. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video virtual try-on aims to generate realistic sequences where garments maintain their identity and adapt accurately to a person's pose and body shape in source video. This task can be regarded as video inpainting, whereas previous methods focus primarily on the specific try-on region while simply “copying” the remaining parts of the person. However, this approach limits the degrees of freedom and heavily relies on precise human parsing. In complex in-the-wild scenarios, dynamic blurring and limb occlusions can introduce errors and discontinuities in the inpainting regions, adversely affecting the video try-on results. Our solution, VidClothEditor, adopts a relaxed editing approach that allows for full-body inpainting and treats non-edited regions as a reconstruction task. It utilizes multiple garment alignment with a proposed region guidance to enhance the naturalness of video try-on results. Additionally, we employ garment-augmented video consistency learning, which significantly reduces the inference time and increases the practical potential for video editing. Comprehensive experiments on the VITON-HD and TikTok datasets confirm VidClothEditor's ability to generate high-quality images and smooth videos. The project website is at video-tryon.github.io.},
  archive      = {J_TMM},
  author       = {Zijian He and Peixin Chen and Guolin Zheng and Guangrun Wang and Xiaonan Luo and Liang Lin and Guanbin Li},
  doi          = {10.1109/TMM.2025.3613169},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient and robust video virtual try-on via enhanced multi-garment alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic dual-adversarial network for blended-target domain adaptation. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of blended-target domain adaptation (BTDA) is growing since target data in the real world often come from multiple domains with different data distributions. Most BTDA studies adapt directly from the source domain to the target domains without considering which kinds of semantic information embedded in images should be explored. Therefore, some irrelevant semantic information is inevitably used, which leads to negative transfer. To address these issues, we propose a semantic dual-adversarial network (SDN) method for BTDA. Specifically, to suppress irrelevant semantic information, we adopt a min-max game strategy between the classifier and the feature extractor. The classifier tries to maximize the prediction distribution discrepancy, whereas the extractor endeavors to minimize this discrepancy. In this process, irrelevant semantic information is suppressed and the principal semantic information is emphasized. To align the categorical distributions, we train a category-aware domain discriminator and a feature extractor with category labels. In addition, we introduce a random ratio-based feature fusion scheme to augment the source domain, which can decrease domain gaps. At last, we propose a weighted negative self-supervised learning method to enhance the model's generalization. Extensive experiments on multiple benchmarks showcase that our method significantly outperforms the prior state-of-the-art methods in BTDA.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Xue Hu and Haoyu Huang and Zhihui Lai and Xuelong Li},
  doi          = {10.1109/TMM.2025.3613144},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic dual-adversarial network for blended-target domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-guided diffusion transformer for low-light image enhancement. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the diffusion transformer (DiT) has become a focal point of interest in recent years, its application in low-light image enhancement remains a blank area for exploration. Current methods recover the details from low-light images while inevitably amplifying the noise in images, resulting in poor visual quality. In this paper, we firstly introduce DiT into the low-light enhancement task and design a novel Structure-guided Diffusion Transformer based Low-light image enhancement (SDTL) framework. We compress the feature through wavelet transform to improve the inference efficiency of the model and capture the multi-directional frequency band. Then we propose a Structure Enhancement Module (SEM) that uses structural prior to enhance the texture and leverages an adaptive fusion strategy to achieve more accurate enhancement effect. In Addition, we propose a Structure-guided Attention Block (SAB) to pay more attention to texture-riched tokens and avoid interference from noisy areas in noise prediction. Extensive qualitative and quantitative experiments demonstrate that our method achieves SOTA performance on several popular datasets, validating the effectiveness of SDTL in improving image quality and the potential of DiT in low-light enhancement tasks.},
  archive      = {J_TMM},
  author       = {Xiangchen Yin and Zhenda Yu and Longtao Jiang and Xin Gao and Xiao Sun and Zhi Liu and Xun Yang},
  doi          = {10.1109/TMM.2025.3613117},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Structure-guided diffusion transformer for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALCER3D: Adaptive learning constraints for enhanced retrieval of complex indoor 3D scenarios. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse is growing rapidly, resulting in thousands of rich virtual universes. This results in a difficult search process for the user, making advanced search tools a necessity. Existing methods leverage contrastive learning to obtain a function mapping a 3D scene and its textual descriptions into similar representations. However, Metaverse scenarios are complex, multimedia-rich 3D scenes containing many elements, making cross-modal alignment difficult. For instance, a museum dedicated to Van Gogh is unrelated to Warhol, yet it shares similarities with Matisse or Monet. To make the mapping functions aware of these nuances, we propose a novel learning strategy to integrate Adaptive Optimization Constraints, computing data-dependent distances using a language-based method we design and enforcing them between the representations at training time. This novelty sets our approach apart from standard procedures enforcing the same distance. We validate the effectiveness of two datasets, one including 6000 apartments, and a novel dataset of 3000 museums that we collect. We observe consistent improvements compared to existing methods. Moreover, we obtain better generalization when with very complex scenarios, e.g. on the museums dataset it obtains an average R@1 of 5.2% compared to 1.2% obtained by existing methods. Finally, the source code is available at https://github.com/aliabdari/ALCER3D.},
  archive      = {J_TMM},
  author       = {Alex Falcon and Ali Abdari and Giuseppe Serra},
  doi          = {10.1109/TMM.2025.3613176},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ALCER3D: Adaptive learning constraints for enhanced retrieval of complex indoor 3D scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust noisy label learning via two-stream sample distillation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy label learning aims to learn robust networks under the supervision of noisy labels, which plays a critical role in deep learning. Existing work either conducts sample selection or label correction to deal with noisy labels during the model training process. In this paper, we design a simple yet effective sample selection framework, termed Two-Stream Sample Distillation (TSSD), for noisy label learning, which can extract more high-quality samples with clean labels to improve the robustness of network training. Firstly, a novel Parallel Sample Division (PSD) module is designed to generate a certain training set with sufficient reliable positive and negative samples by jointly considering the sample structure in feature space and the human prior in loss space. Secondly, a novel Meta Sample Purification (MSP) module is further designed to mine adequate semi-hard samples from the remaining uncertain training set by learning a strong meta classifier with extra golden data. As a result, more and more high-quality samples will be distilled from the noisy training set to train networks robustly in every iteration. Extensive experiments on four benchmark datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet and Clothing-1M, show that our method has achieved state-of-the-art results over its competitors.},
  archive      = {J_TMM},
  author       = {Sihan Bai and Sanping Zhou and Zheng Qin and Le Wang and Nanning Zheng},
  doi          = {10.1109/TMM.2025.3613115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust noisy label learning via two-stream sample distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-reflection neural network for class-incremental object counting. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowded scenarios, achieving the counting task of dynamically evolving categories is extremely challenging. In addition to grappling with challenges such as scale variations, severe occlusion and complex backgrounds, it is imperative to mitigate the issue of catastrophic forgetting. Previous approaches have heavily relied on leveraging historical data for knowledge distillation to tackle these difficulties. However, this strategy encounters two prominent obstacles: 1) Employing the teacher network from the previous stage for distillation incurs additional computational overhead during the training stage. 2) Although knowledge distillation can facilitate effective knowledge transfer, some inaccurate predictions from the teacher network may affect the knowledge acquisition in the current stage. To overcome these issues, we introduce a novel solution: a self-reflection neural network for class-incremental object counting. First, we construct a global-aware incremental regression branch that uses stacked transformer layers as backends to capture global information, while the final regression layers dynamically expand as categories increase. Furthermore, we introduce an uncertain estimation branch that selectively isolates certain feature maps to avoid some neurons updated with excessive gradient information, thereby enhancing the network plasticity while preserving stability. The output of this branch functions as a regularization signal, steering the learning process of the incremental regression branch. To foster a more robust retention of past knowledge, we propose a self-reflection loss. It employs the rectified outputs of global-aware incremental regression branch to encourage the network to reflect upon and refine its grasp of historical knowledge, effectively averting the pitfalls of inaccurate information. Our extensive experiments validate the effectiveness of our proposed method, achieving state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Shengqin Jiang and Linfei Li and Fengna Cheng and Yuankai Qi and Qingshan Liu},
  doi          = {10.1109/TMM.2025.3613145},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-reflection neural network for class-incremental object counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEVC video steganalysis based on centralized error and attention mechanism. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With high embedding capacity and security, transform coefficient-based video steganography has become an important branch of video steganography. However, existing steganalysis methods against transform coefficient-based steganography provide insufficient consideration to the prediction process of HEVC compression, which results in steganalysis that is not straightforward and fail to effectively detect adaptive steganography methods in low embedding rate scenarios. In this paper, an HEVC video steganalysis method based on centralized error and attention mechanism against transform coefficient-based steganography is proposed. Firstly, the centralized error phenomenon brought by distortion compensation-based steganography is analyzed, and prediction error maps is constructed for steganalysis to achieve higher SNR(signal-to-noise ratio). Secondly, a video steganalysis network called CESNet (Centralized Error Steganalysis Network) is proposed. The network takes the prediction error maps as input and four types of convolutional modules are designed to adapt to different stages of feature extraction. To address the intra-frame sparsity of adaptive steganography, CEA (Centralized Error Attention) modules based on spatial and channel attention mechanisms are proposed to adaptively enhance the steganographic region. Finally, after extracting the feature vectors of each frame, the detection of steganographic video is completed using the self-attention mechanism. Experimental results show that compared with the existing transform coefficient-based video steganalysis methods, the proposed method can effectively detect multiple transform coefficient-based steganography algorithms and achieve higher detection performance in low payload scenarios.},
  archive      = {J_TMM},
  author       = {Haojun Dai and Dawen Xu and Lin Yang and Rangding Wang},
  doi          = {10.1109/TMM.2025.3613171},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HEVC video steganalysis based on centralized error and attention mechanism},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StegFlow: Flow-based high-frequency distribution mapping network for multi-image steganography. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-image steganography refers to the technique of embedding multiple secret images into a single cover image while ensuring that the secret images remain imperceptible and can be perfectly recovered by the recipient. Traditional single-image-based steganography often leads to noticeable contour shadows or color distortions in the cover image, making the hidden image more detectable. In contrast, cascaded invertible neural network-based steganography introduces a large number of parameters, complicating the network structure and resulting in a time-consuming learning and training process. To address the above problems, this paper proposes a novel flow-based, end-to-end multi-image invertible steganography framework (StegFlow), which effectively integrates forward and backward data flows for image hiding and recovery. The framework employs cascading operations to enable deep hiding of multiple secret images. To enhance the coupling capabilities, we introduce an invertible permutation layer that disrupts the channel arrangement order, allowing the coupling layer to more accurately guide the embedding of secret information into regions of the image that are easy to hide and recover. In addition, a high-frequency distribution mapping (HFDM) is designed to model the lost high-frequency information during image hiding process, significantly improving the recovery performance of the secret images. Extensive experiments are performed over multiple classical datasets, and the results demonstrate that compared to state-of-the-art (SOTA) models, the proposed framework can achieve a superior overall performance in terms of visual quality and anti-steganalysis capability. Specifically, our scheme can improve the hiding accuracy (measured by PSNR) by over 3 dB and the recovery accuracy by over 1 dB when hiding two secret images.},
  archive      = {J_TMM},
  author       = {Fengyong Li and Hao Liu and Xinpeng Zhang and Chuan Qin},
  doi          = {10.1109/TMM.2025.3613124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StegFlow: Flow-based high-frequency distribution mapping network for multi-image steganography},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor completion framework by graph refinement for incomplete multi-view clustering. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-view Clustering (IMVC) endeavors to harness information from multiple incomplete views to partition multi-view data into their respective clusters. How to recover missing information with lossless fidelity is the core of IMVC, which is of vital importance but challenging. Most of the existing methods include a feature recovery step to mitigate the negative impact of missing samples on the feature graph, however, these IMVC algorithms simply utilize the correlation between samples to recover the relationship between the unmissing instances and the missing instances while ignoring the consistency between views, which leads to often unsatisfactory recovery results. In addition, previous IMVC algorithms focus more on the recovery of incomplete data, ignoring the effect of the error term on incomplete graphs. This can mislead the recovery process of IMVC algorithm and the feature graph can be affected by anomalous information, which leads to degradation of clustering performance. To address this gap, this paper introduces the Tensor Completion Framework by Graph Refinement for Incomplete Multi-view Clustering (IMVC-TGR). IMVC-TGR separates the redundant information in each affine graph by graph refinement operation, aiming to mitigate the negative impact of error terms and redundant information on the feature graph during the recovery process. Meanwhile, IMVC-TGR stacks the feature graphs into tensors to explore intra-view correlation and inter-view consistency, so as to recover the relationship between missing samples and non-missing samples, and improve the quality of the feature graphs. Finally, IMVC-TGR introduces semantic consistency constraints and self-weighted fusion strategies into the high-quality feature graphs, aiming at preserving the complementary information between different views while balancing the contributions of the refined representation matrices of different views. The experimental results on multiple different datasets indicate that IMVC-TGR can achieve state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Huibing Wang and Yawei Chen and Mingze Yao and Wenzhe Liu and Jinjia Peng and Xianping Fu},
  doi          = {10.1109/TMM.2025.3613125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tensor completion framework by graph refinement for incomplete multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Watch where you move: Region-aware dynamic aggregation and excitation for gait recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based gait recognition has achieved great success in various applications. The key to accurate gait recognition lies in considering the unique and diverse behavior patterns in different motion regions, especially when covariates affect visual appearance. However, existing methods typically use predefined regions for temporal modeling, with fixed or equivalent temporal scales assigned to different types of regions, which makes it difficult to model motion regions that change dynamically over time and adapt to their specific patterns. To tackle this problem, we introduce a Region-aware Dynamic Aggregation and Excitation framework (GaitRDAE) that automatically searches for motion regions, assigns adaptive temporal scales and applies corresponding attention. Specifically, the framework includes two core modules: the Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the optimal temporal receptive field for each region, and the Region-aware Dynamic Excitation (RDE) module, which emphasizes the learning of motion regions containing more stable behavior patterns while suppressing attention to static regions that are more susceptible to covariates. Experimental results show that GaitRDAE achieves state-of-the-art performance on several benchmark datasets. The source code will be published at https://github.com/HUAFOR/GaitRDAE.},
  archive      = {J_TMM},
  author       = {Binyuan Huang and Yongdong Luo and Xianda Guo and Xiawu Zheng and Zheng Zhu and Jiahui Pan and Chengju Zhou},
  doi          = {10.1109/TMM.2025.3613158},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Watch where you move: Region-aware dynamic aggregation and excitation for gait recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeskTransfer: Predicting multi-scenario video stream throughput in cloud desktop based on transfer autoencoder. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3613109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of cloud services has provided a new medium for video streams transmission. Cloud desktops, as a representative multimedia application, facilitate interaction between users and cloud via video streams, garnering widespread adoption in various fields. The network condition directly affects the transmission. Therefore, accurate throughput prediction helps guide the allocation of network resources, avoiding a decline in user experience due to insufficient resources and waste caused by excessive resources. Recent works focus more on the temporal characteristics of throughput. However, we believe that throughput of video streaming is significantly influenced by usage scenario. In this paper, we propose a transfer-based autoencoder framework DeskTransfer for throughput prediction in frequent switching cloud desktop scenarios. Specifically, we construct the Scenario Autoencoder and Throughput Autoencoder to respectively learn the scenario and throughput features from historical usage records. By adopting an adversarial mechanism, we design transfer algorithm using latent vectors, enabling the model suitable for multiple scenarios. We collect real-world data from a project cooperated with Lenovo Research for experiment and compare our solution with leading methods on public datasets to validate its effectiveness.},
  archive      = {J_TMM},
  author       = {Zuodong Jin and Peng Qi and Ruipeng Gao and Yanzhe Jing and Dan Tao},
  doi          = {10.1109/TMM.2025.3613109},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeskTransfer: Predicting multi-scenario video stream throughput in cloud desktop based on transfer autoencoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving infrared small target detection with GAN-driven data augmentation. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection (IRSTD) based on deep learning has received extensive research and application. However, deep learning models require a large amount of data to perform well, and the collection and standardization of infrared small target data is challenging, limiting the applicability of such models. To address this issue, this study proposes a data augmentation scheme for infrared small targets based on Generative Adversarial Networks (GANs). The proposed method is a two-step approach: the first step is the generation of clean backgrounds, and the second is the adaptive fusion of targets and backgrounds. In the background generation stage, we first use the Fast Marching Method (FMM) to fill background targets and obtain clean backgrounds. Then, we design a multi-generator and multi-discriminator GAN model (MGD-GAN) to generate high-quality and diverse background images. In the adaptive target-background fusion stage, we propose a dual-discriminator GAN network (FusionGAN), which allows the target mask to be adaptively fused with the background pixels. By combining real targets with generated backgrounds, new infrared small target images are generated, achieving the goal of data augmentation. Experiments conducted across three different scenarios demonstrate that the proposed data augmentation scheme effectively enhances the performance of both traditional and advanced detection models.},
  archive      = {J_TMM},
  author       = {Hongwei Ding and Nana Huang and Yaoxin Wu and Xiaohui Cui},
  doi          = {10.1109/TMM.2025.3613079},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving infrared small target detection with GAN-driven data augmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked text pre-training for scene text detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training has greatly boosted scene text detection methods by learning the representation of text. However, they still suffer from two drawbacks: 1) The learned representation for text is not discriminative due to the insufficient annotated real data and the domain gap between synthetic data. 2) Existing methods perform poorly on text lacking of visual information (e.g. occluded text). To address them, this paper explores the potential of the CLIP model and proposes a novel self-supervised pre-training network with masked text modeling (MTM) and text knowledge distillation (TKD), which aims at obtaining discriminative representation for text. First, a Text Perception Module is proposed to perceive coarse text area under an unsupervised manner. Second, we design a Text-aware Masking Strategy to mask the text area with a certain ratio and reconstruct the masked texts by the MTM Module. Compared to randomly pixel-level masking in classic masked image modeling, we perform a targeted text-aware masking and reconstruction. MTM obtains linguistic reasoning ability of text occlusion with reconstruction of masked text. Besides, to better utilize the multimodal knowledge of text in CLIP model, this paper devises a TKD Module to guide the representation learning of masked texts in semantic level. This robust feature extraction learned by reconstructing masked text and knowledge distillation ensures a more discriminative representation for text. Extensive experiments on four challenging datasets verify the effectiveness and superiority of our pre-training method. Specifically, our method achieves F-measure of $\bf{86.5\%}$, $\bf{87.1\%}$ and $\bf{88.5\%}$ for DBNet++ on CTW1500, Total-Text and MSRA-TD500 respectively. Dataset and code are available at https://github.com/rangek/MTP.},
  archive      = {J_TMM},
  author       = {Hongtao Xie and Keran Wang and Bangbang Zhou and Yuxin Wang and Weigang Qi and Jie Wu and Yadong Qu and Zuan Gao and Dongming Zhang and Yizhi Liu},
  doi          = {10.1109/TMM.2025.3613181},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Masked text pre-training for scene text detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware diffusion-enhanced multimedia recommendation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia recommendations aim to use rich multimedia content to enhance historical user-item interaction information, which can not only indicate the content relatedness among items but also reveal finer-grained preferences of users. In this paper, we propose a Knowledge-aware Diffusion-Enhanced architecture using contrastive learning paradigms (KDiffE) for multimedia recommendations. Specifically, we first utilize original user-item graphs to build an attention-aware matrix into graph neural networks, which can learn the importance between users and items for main view construction. The attention-aware matrix is constructed by adopting a random walk with a restart strategy, which can preserve the importance between users and items to generate aggregation of attention-aware node features. Then, we propose a guided diffusion model to generate strongly task-relevant knowledge graphs with less noise for constructing a knowledge-aware contrastive view, which utilizes user embeddings with an edge connected to an item to guide the generation of strongly task-relevant knowledge graphs for enhancing the item's semantic information. We perform comprehensive experiments on three multimedia datasets that reveal the effectiveness of our KDiffE and its components on various state-of-the-art methods. Our source codes are available.},
  archive      = {J_TMM},
  author       = {Xian Mo and Fei Liu and Rui Tang and Jintao Gao and Hao Liu},
  doi          = {10.1109/TMM.2025.3613108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-aware diffusion-enhanced multimedia recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tackling ambiguity from perspectives of uncertainty inference and affinity diversification for weakly supervised semantic segmentation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve dense predictions without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, particularly during the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, is widely hindered by ambiguity. Despite this, this issue has received little attention in previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspectives of uncertainty inference and affinity diversification. When activating class objects, we argue that the false activation stems from the bias to ambiguous regions during the feature extraction. Therefore, we formulate a robust feature representation with a Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be overly similar among ambiguities. To this end, we design an affinity diversification module to promote diversity among semantics. A mutual complementing refinement is first proposed to statically rectify the ambiguous affinity with multiple inferred pseudo labels. Then a contrastive affinity loss is further designed to dynamically diversify the relations among unrelated semantics. It stably propagates the diversity into the feature representation and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and its superiority over recent single-staged or even most multi-staged competitors. Code is publicly available at https://github.com/zwyang6/UniA.},
  archive      = {J_TMM},
  author       = {Zhiwei Yang and Yucong Meng and Kexue Fu and Shuo Wang and Zhijian Song},
  doi          = {10.1109/TMM.2025.3613165},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tackling ambiguity from perspectives of uncertainty inference and affinity diversification for weakly supervised semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A UNet-like transformer network for camouflaged object detection. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of Camouflaged Object Detection (COD) is to identify the objects that integrate seamlessly with the surrounding environment. Due to the high intrinsic similarity between the objects and their background, this task presents greater challenges than traditional object detection. Most existing COD methods often have a large number of parameters and high computational complexity in the pursuit of detection accuracy, which hinders the application of COD in practical scenarios. To address this issue, we propose a UNet-like Transformer Network for COD, termed UTNet, which achieves competitive detection accuracy with a smaller parameter set. Specifically, we propose a Camouflaged Region Awareness Module (CRAM) consisting of a Hierarchical Attention Mechanism (HAM) that groups features to reveal intrinsic consistency between sub-features. This CRAM can be embedded into the backbone network, giving it powerful modeling capabilities. And, we present a Contextual Knowledge Collector (CKC) that exploits a cross-aggregation approach for neighboring feature layers, promoting the flow of semantic information from high-level to low-level features, and ensuring the integrity of camouflaged objects at each level of features. Furthermore, we introduce a progressive decoder that utilizes a cascade of attention units to filter noise and explores knowledge aggregation to emphasize features from different levels, ensuring that camouflaged objects have complete spatial details at the local level. Extensive experimental results show that UTNet achieves competitive results compared to 20 state-of-the-art methods. Codes and results are released on https://github.com/hjy0518/UTNet.},
  archive      = {J_TMM},
  author       = {Fuming Sun and Jinyu Han and Weiyi Wu and Jing Sun and Mengyin Wang},
  doi          = {10.1109/TMM.2025.3613076},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A UNet-like transformer network for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed two-tier cache optimization in metaverse scenarios combining MADDPG and GCN. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid emergence of the Metaverse requires higher network throughput and lower latency to deliver immersive and responsive virtual experiences. Traditional centralized data processing approaches are constrained by limited computational and bandwidth resources when handling large-scale user data. A Cloud-Edge-End transmission architecture is proposed in this study, tailored for Metaverse scenarios to optimize resource allocation, minimize latency, and enhance rendering efficiency. A real-time trajectory segment prediction scheme (FDK) was developed, which combines FastDTW with K-means by leveraging user behavior trajectories to determine subscene popularity and store them on GPU servers, thereby reducing user wait time. A two-tier cache optimization scheme (MAE2C) is also proposed, incorporating GCN for subscene feature identification. GPU servers employ the MADDPG strategy to cache popular subscenes, while edge servers utilize DDPG to cache missed scenes. This approach effectively reduces cloud access and cache replacement frequency. Simulation results demonstrate that the subscene cache hit rate of the MAE2C scheme significantly outperforms existing methods across various cache capacities, with a 6.9% reduction in cache replacement frequency. This research provides effective technical support for Metaverse scene rendering and offers insights into the development of generative Metaverse systems.},
  archive      = {J_TMM},
  author       = {Zheng Wan and Shenglu Zhao and Xiaogang Dong and Xuelin Liu and Yifeng Tan and Yuming Fang},
  doi          = {10.1109/TMM.2025.3613168},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distributed two-tier cache optimization in metaverse scenarios combining MADDPG and GCN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep no-reference quality assessment for underwater enhanced images. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of underwater image enhancement (UIE) is to boost the acquired underwater image quality, which increases the value of the underwater image significantly. However, without effective underwater enhanced image quality assessment (UEIQA) measures that benchmark the UIE, the process of UIE becomes driftless and the enhanced results of different UIE algorithms cannot be fairly compared. Toward this end, we in this work construct a dedicated UEIQA scheme on the basis of deep investigation of the underwater enhanced image characteristics. Specifically, in our proposed method, we respectively design deep neural networks to represent the unique attributes of the underwater enhanced image, such as color cast, local distortions, naturalness degree, sharpness, contrast, fog density, etc., that are highly correlated with the image quality. Then we introduce the Vision Transformer (ViT) to capture the dependencies among different image attributes and infer the image quality level. Extensive experiments conducted on three typical UEIQA databases, i.e., SOTA, UID2021 and SAUD, show that the proposed UEIQA model yields noteworthy higher prediction accuracy than the representative IQA and UEIQA metrics, e.g., achieving SRCC values of 0.891 ( vs. 0.749 in SAUD) and 0.933 ( vs. 0.798 in UID2021). The proposed UEIQA model will be released at https://github.com/YT2015?tab=repositories.},
  archive      = {J_TMM},
  author       = {Yutao Liu and Baochao Zhang and Runze Hu and Ke Gu and Guangtao Zhai and Junyu Dong},
  doi          = {10.1109/TMM.2025.3613105},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep no-reference quality assessment for underwater enhanced images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unrevealed threats: Adversarial robustness analysis of underwater image enhancement models. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based methods for underwater image enhancement (UWIE) have undergone extensive exploration. However, learning-based models are usually vulnerable to adversarial examples so as the UWIE models. To the best of our knowledge, there is no comprehensive study on the adversarial robustness of UWIE models, which indicates that UWIE models are potentially under the threat of adversarial attacks. In this paper, we propose a general adversarial attack protocol. We make a first attempt to conduct adversarial attacks on five well-designed UWIE models on three common underwater image benchmark datasets. Considering the scattering and absorption of light in the underwater environment, there exists a strong correlation between color correction and underwater image enhancement. On the basis of that, we also design two effective UWIE-oriented adversarial attack methods, Pixel Attack and Color Shift Attack targeting different color spaces. The results show that five models exhibit varying degrees of vulnerability to adversarial attacks and well-designed small perturbations on degraded images are capable of preventing UWIE models from generating enhanced results. In addition, we conduct adversarial training on these models and successfully mitigated the effectiveness of adversarial attacks. In summary, we reveal the adversarial vulnerability of UWIE models and propose a new evaluation dimension of UWIE models.},
  archive      = {J_TMM},
  author       = {Siyu Zhai and Zhibo He and Xiaofeng Cong and Junming Hou and Jie Gui and Jian Wei You and Xin Gong and James Tin-Yau Kwok and Yuan Yan Tang},
  doi          = {10.1109/TMM.2025.3613152},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unrevealed threats: Adversarial robustness analysis of underwater image enhancement models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-supervised asymmetric co-training for semi-supervised medical domain generalization. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudo-labels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.},
  archive      = {J_TMM},
  author       = {Jincai Song and Haipeng Chen and Jun Qin and Na Zhao},
  doi          = {10.1109/TMM.2025.3613080},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-supervised asymmetric co-training for semi-supervised medical domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BTDGNet: A dual-guided camouflaged object detection network leveraging boundary and texture information. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection aims to identify objects that blend seamlessly with their background, posing a greater challenge compared to general object detection tasks. Due to its ability to recognize camouflaged objects, such detection models hold significant practical value across various fields. To accurately identify camouflaged targets in various complex environments, we designed a dual-guided camouflaged object detection network based on boundary and texture information(BTDGNet). The process consists of two main stages. The first stage is the localization stage, which leverages a convolutional neural network (CNN) to capture boundary and texture information of objects. These features are then fused to achieve coarse localization of the camouflaged objects. In the second stage, the recognition stage, we employ a Transformer to extract global information from the image, enhancing the differentiation between foreground and background. An interactive fusion module is designed to fully exploit and integrate both global and local features, producing precise prediction images. By leveraging boundary and texture information, the model's adaptability to different camouflaged objects is improved. The integration of local and global features enhances the model's detection accuracy from various perspectives, ultimately building a camouflaged object detection model suitable for a wide range of complex scenarios. The proposed method was extensively compared with other state-of-the-art methods across four public datasets, and the results demonstrated superior performance. Furthermore, benefiting from our dual-guidance strategy that leverages both texture and boundary information, our model demonstrates robust performance. We conducted tests on detection tasks across four different domains, and the results confirm that our model can accurately segment camouflaged objects in complex scenes.},
  archive      = {J_TMM},
  author       = {Xiaogang Song and Pengfei Zhang and Xiaochang Li and Xinhong Hei and Rongrong Liu},
  doi          = {10.1109/TMM.2025.3613150},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BTDGNet: A dual-guided camouflaged object detection network leveraging boundary and texture information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled denoising and counterfactual balance for multimodal recommendation. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional network-based dual-view multimodal recommendation methods have achieved great success. They extract multimodal and behavior features based on item-item and user-item graphs, respectively. However, they still have two- fold limitations. First, the relevance between multimodal semantics and user preferences is ignored, resulting in the propagation and coupling of preference-irrelevant noise. Second, the direct use of uneven factual user-item graphs is suboptimal, as both redundant noisy edges and missing positive interaction edges impair recommendations. To solve the above issues, we propose a DisentAngled deNoising and Counterfactual balancE method for multimodal recommendation, dubbed as DANCE. Specifically, for multimodal features, we explicitly disentangle them into preference-relevant and preference-irrelevant representations, to absorb and discard irrelevant noise via the latter. An orthogonal regularization and a contrastive learning task on preference relevance score prediction are proposed as the dual safeguard to prevent preference-relevant representations from encoding irrelevant noise. For behavior feature extraction, we construct a balanced user-item graph by integrating factual and counterfactual graphs. In this process, we pre-train a behavior simulator to build the counterfactual graph with full interactions. Top-$K$ sampling is adopted to omit noisy edges and add missing edges in the graph. The final recommendation is performed upon the fused representation of preference-relevant multimodal and behavior representations. Extensive experiments on three public datasets verify the power of our DANCE.},
  archive      = {J_TMM},
  author       = {Xin Wen and Weizhi Nie and Jing Liu and Ning Zhang and Yuting Su and An-An Liu},
  doi          = {10.1109/TMM.2025.3613112},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangled denoising and counterfactual balance for multimodal recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeRegNet: Edge feature-based multimodal registration network between images and LiDAR point clouds. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal data registration has long been a critical task in computer vision, with extensive applications in autonomous driving and robotics. Accurate and robust registration methods are essential for aligning data from different modalities, forming the foundation for multimodal sensor data fusion and enhancing perception systems' accuracy and reliability. The registration task between 2D images captured by cameras and 3D point clouds captured by Light Detection and Ranging (LiDAR) sensors is usually treated as a visual pose estimation problem. High-dimensional feature similarities from different modalities are leveraged to identify pixel-point correspondences, followed by pose estimation techniques using least squares methods. However, existing approaches often resort to downsampling the original point cloud and image data due to computational constraints, inevitably leading to a loss in precision. Additionally, high-dimensional features extracted using different feature extractors from various modalities require specific techniques to mitigate cross-modal differences for effective matching. To address these challenges, we propose a method that uses edge information from the original point clouds and images for cross-modal registration. We retain crucial information from the original data by extracting edge points and pixels, enhancing registration accuracy while maintaining computational efficiency. The use of edge points and edge pixels allows us to introduce an attention-based feature exchange block to eliminate cross-modal disparities. Furthermore, we incorporate an optimal matching layer to improve correspondence identification. We validate the accuracy of our method on the KITTI and nuScenes datasets, demonstrating its state-of-the-art performance. Our code is publicly available on GitHub at https://github.com/ESRSchao/EdgeRegNet.},
  archive      = {J_TMM},
  author       = {Yuanchao Yue and Hui Yuan and Qinglong Miao and Xiaolong Mao and Raouf Hamzaoui and Peter Eisert},
  doi          = {10.1109/TMM.2025.3613107},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EdgeRegNet: Edge feature-based multimodal registration network between images and LiDAR point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiftFormer: Lifting and frame theory based monocular depth estimation using depth and edge oriented subspace representation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation (MDE) has attracted increasing interest in the past few years, owing to its important role in 3D vision. MDE is the estimation of a depth map from a monocular image/video to represent the 3D structure of a scene, which is a highly ill-posed problem. To solve this problem, in this paper, we propose a LiftFormer based on lifting theory topology, for constructing an intermediate subspace that bridges the image color features and depth values, and a subspace that enhances the depth prediction around edges. MDE is formulated by transforming the depth value prediction problem into depth-oriented geometric representation (DGR) subspace feature representation, thus bridging the learning from color values to geometric depth values. A DGR subspace is constructed based on frame theory by using linearly dependent vectors in accordance with depth bins to provide a redundant and robust representation. The image spatial features are transformed into the DGR subspace, where these features correspond directly to the depth values. Moreover, considering that edges usually present sharp changes in a depth map and tend to be erroneously predicted, an edge-aware representation (ER) subspace is constructed, where depth features are transformed and further used to enhance the local features around edges. The experimental results demonstrate that our LiftFormer achieves state-of-the-art performance on widely used datasets, and an ablation study validates the effectiveness of both proposed lifting modules in our LiftFormer.},
  archive      = {J_TMM},
  author       = {Shuai Li and Huibin Bai and Yanbo Gao and Chong Lv and Hui Yuan and Chuankun Li and Wei Hua and Tian Xie},
  doi          = {10.1109/TMM.2025.3613146},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LiftFormer: Lifting and frame theory based monocular depth estimation using depth and edge oriented subspace representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLOSS: Global-local matching network towards outfit recommendation for diverse body shapes and scenes. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of individuals' living standards has transformed clothing preferences, elevating fashion beyond mere utility to a potent means of self-expression. However, the intricate task of outfit selection persists as a challenge, marked by traditional methods facing challenges such as the oversight of combined factors of scene and body shape, insufficient emphasis on detail-oriented matching, and overreliance on rigid hierarchical structures. To tackle these challenges, this article introduces a novel model, termed Global-Local matching network towards Outfit recommendation for diverse body Shapes and Scenes (GLOSS). Specifically, we first introduce a newly compiled fashion dataset, StreetFashion, to capture the combined factors of body shapes and scene characteristics. Additionally, we develop innovative multi-level globality- and locality-aware matching methods to enhance the accuracy of outfit recommendations by comprehensively considering both global and local relationships among clothing items, outfits, users, and scenes. Furthermore, we develop a personalized outfit heterogeneous graph that incorporates historical interactions among fashion entities, enabling effective modeling of nonstrict hierarchical relationships. Evaluation conducted on both our collected dataset and an adapted existing dataset demonstrates the effectiveness of our proposed approach in outfit recommendation. Our codes are released at: https://github.com/ChenrMa/GLOSS.},
  archive      = {J_TMM},
  author       = {Chenrui Ma and Huiyue Sun and Jianghong Ma and Haijun Zhang and Yuxin Ding and Zhao Zhang},
  doi          = {10.1109/TMM.2025.3613163},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GLOSS: Global-local matching network towards outfit recommendation for diverse body shapes and scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Propagation based recycling contrastive learning for coupled noisy visible-infrared person re-identification. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a crucial role in round-the-clock security surveillance systems, aiming to detect consistent identity recognition across transitions from day to night. A significant challenge in this field is the variation in the appearance of the same identity across visible and infrared modalities, which often leads to coupled noisy labels, referring to both Noisy Annotation (NA) and Noisy Correspondence (NC). Therefore, learning noisy-tolerant and discriminative representations is the primary objective in VI-ReID. However, existing research typically faces two principal limitations: (1) Learning strategies for noisy labeled scenarios usually rely on analyzing the distribution of loss response while ignoring the rich semantic information from neighboring samples. (2) When dealing with identified noisy samples, most previous approaches usually employ filtering strategies to mitigate the impact of noisy samples but fail to consider the valuable information in the noisy samples. To address these challenges, we propose a Propagation based Recycling Contrastive Learning (PRCL) approach. This method utilizes a label propagation strategy to distinguish clean annotations to learn identity-wise semantic information and recycles filtered noisy samples to capture the geometric-wise representation. Thus, even in the presence of noisy labels, the method can help learn robust representations across visible and infrared modalities. Specifically, we design a Noisy-aware Heterogeneous Graph Propagation module, which identifies noisy samples by aggregating the effects of neighboring labels using a graph propagation strategy. In addition, we develop a Cross Modality Recycling Debiased Contrastive Learning algorithm, which leverages the identity-wise information from clean samples and geometry-wise information from noisy samples. This approach utilizes identity-wise and geometric-wise information to mitigate the effect of noisy labels and retain as much valuable information as possible. Extensive experiments on two VI-ReID benchmark datasets demonstrate that our proposed method achieves highly competitive performance.},
  archive      = {J_TMM},
  author       = {Yongxi Li and Wenzhong Tang and Shuai Wang and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3613106},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Propagation based recycling contrastive learning for coupled noisy visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level contrastive learning for multimodal sentiment analysis. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3613116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis has garnered increasing attention. The bulk of existing work in multimodal sentiment analysis primarily focuses on designing various networks to align and subsequently fuse representations from individual modalities. Contrastive learning, recognized for its intrinsic alignment capabilities, has also been extensively applied in multimodal sentiment analysis. However, current contrastive learning methods are often limited to pairwise modalities and typically perform contrastive learning prior to modality fusion, neglecting the consistency of interactions across multiple modalities. Moreover, they overlook the overall consistency within samples. To address these issues, we introduce a novel Multi-Level Contrastive Learning (MLCL) framework for multimodal sentiment analysis, composed of Uni-Modal Contrastive Learning (UMCL), Bi-Modal Contrastive Learning (BMCL) and Tri-Modal Contrastive Learning (TMCL). UMCL enhances intra-modal representations by creating positive pairs using modality-specific random dropout, while BMCL leverages the asymmetry of attention mechanisms, using two directional attentions as positive samples. TMCL aligns non-overlapping uni-modal and bi-modal representations, underscoring the complementarity of tri-modal information. The effectiveness of MLCL is demonstrated through its performance on multiple datasets. Our comprehensive experiments across multiple datasets demonstrate the superiority of the MLCL framework, which achieves new state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Yan Zhuang and Wei Bai and Yanru Zhang and Jiawen Deng and Zheng Hu and Xiaoyue Zhang and Fuji Ren},
  doi          = {10.1109/TMM.2025.3613116},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-level contrastive learning for multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Critical contour prior-guided graph learning with pose calibration for identity-aware deepfake detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3613159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake has recently raised severe public concerns about security issues, such as creating fake news of celebrities. As countermeasures, identity-aware detection methods leverage identity information to expose forged videos by measuring identity consistency between the suspicious input and its reference samples. However, the performance of existing methods suffers from notable degradation due to undesired variations of head poses and capturing environments. In this work, we first conduct a statistical analysis to illustrate the influence of different facial regions for forensic purposes, which infers more reliable identity information is located in critical face regions. Motivated by this analysis, we propose a graph learning-based identity-aware deepfake detection framework considering critical contour prior as guidance. First, feature sampling based on contour landmarks is applied to construct the graph data as the input of our critical contour prior-guided graph attention network (CP-GAT), where a node position prediction task is constructed as auxiliary supervision to explore rich relationships between nodes. To enhance pose-invariant ability, a rotation compensation block is integrated into CP-GAT and trained using a pose-calibrated contrastive learning to extract identity features, which takes high-quality front faces as the calibration goal with a progressively updating selection. Besides, an adversarial node masking-based training strategy is proposed as feature augmentation to further enhance the reliability. During the inference stage, the similarity between identity features of the input sample and its reference samples extracted by the trained CP-GAT is used to obtain the detection result. Extensive experiments are conducted on various face forgery datasets and state-of-the-art methods are compared to verify the superiority of the proposed method in terms of detection capability and robustness.},
  archive      = {J_TMM},
  author       = {Liyue Ming and Peisong He and Haoliang Li and Shiqi Wang and Xinghao Jiang},
  doi          = {10.1109/TMM.2025.3613159},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Critical contour prior-guided graph learning with pose calibration for identity-aware deepfake detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSDP: Real-time asymmetric dual-stream instance segmentation embedding depth-predictive architecture for enhanced scene understanding. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3613161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation can help vehicles or robots enhance their understanding of a scene through the pixel-level segmentation of different objects. However, occlusion and boundary blur, especially in cases with similar colors or textures, are still challenges encountered in real-time robust segmentation tasks. To segment a complete instance boundary, the existing 2D approaches fuse local and abstract semantic features derived from the color domain, which leads to homogeneous semantic information, and efficiently separating different objects is difficult in some cases. To address these complicated scenes, inspired by a human prediction processing strategy, where “the brain fills in missing information in advance to help make better decisions”, this study proposes a real-time asymmetric dual-stream instance segmentation algorithm embedding a depth-predictive architecture that provides the covisible depth information of objects. Furthermore, a cross-domain data fusion method and an enhancement-decoupling loss are designed to complement RGB data by utilizing the rich foreground and boundary details of the predicted depth map. In addition, our model can be fine-tuned to integrate it with real depth domain data provided by different input devices. Extensive experiments conducted on the COCO, OCHuman and CityScapes datasets demonstrate the effectiveness of our method. We further deployed our DSDP method on a UAV platform for validation purposes and qualitatively confirmed its validity.},
  archive      = {J_TMM},
  author       = {Mingyu Chen and Qiang Li and Weizhi Nie and Jing Liu and Jingjing Geng and Yongtao Ma and Xin Guan},
  doi          = {10.1109/TMM.2025.3613161},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSDP: Real-time asymmetric dual-stream instance segmentation embedding depth-predictive architecture for enhanced scene understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCTFormer: A dual-branch transformer with cloze tests for video anomaly detection. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3613082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is of critical importance in safety-critical scenarios. The key challenge is to effectively capture the spatio-temporal features of videos and learn normal patterns from the training data. However, existing methods often fall short in modelling intra-channel and inter-channel correlations as well as dynamic dependencies between video frames, leading to challenges in model robustness and generalization. To address these issues, we propose DCTFormer, a dual-branch framework that integrates both RGB and optical flow branches to handle Video Anomaly Detection. Firstly, we design a novel module TRAECT (Transformer-based Residual Autoencoder with Cloze Tests), which incorporates high-level semantics and temporal context information to improve the spatio-temporal relationships learning ability by capturing intra-channel and inter-channel correlations. More importantly, conditioned on the RGB branch, we propose a new optical flow completion approach incorporating richer motion dynamics to learn dynamic dependencies between video frames and optical flows through a conditional variational autoencoder. At last, we introduce an ensemble strategy to compute anomaly scores for both branches, and thus fully exploit the branches modality information. The experimentation on three challenging benchmark datasets evinces the efficacy of our framework, which outperforms current state-of-the-art approaches with regard to anomaly detection performance.},
  archive      = {J_TMM},
  author       = {Pengzhan Chen and Shengdong Du and Xiaole Zhao and Jie Hu and Jingjing Li and Tianrui Li},
  doi          = {10.1109/TMM.2025.3613082},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCTFormer: A dual-branch transformer with cloze tests for video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive diversity augmentation for single domain generalization. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single domain generalization aims to train a model on a single source domain that generalizes to unseen target domains, which is critical in multimedia applications. Current methods typically use adversarial data augmentation to enrich the source domain distribution with novel samples. However, these methods typically rely on labeled data and require adversarial training between generators and classifiers, which may limit sample diversity and introduce spurious correlations. To tackle these problems, we propose a method that integrates Contrastive clustering regularization with an Unsupervised Diversity Augmentation (UDA), termed C-UDA. Specifically, UDA is a flexible and general framework in which two customized models iteratively optimize a novel adversarial loss to enable fully unsupervised data augmentation. Within UDA, we design a lightweight generator that diversifies each input image along three distinct visual attributes. Based on both original and augmented images, we further introduce contrastive clustering regularization to encourage the model to learn domain-invariant representations, resulting in robust decision boundaries. Extensive experiments on four challenging benchmarks demonstrate that C-UDA significantly outperforms 22 state-of-the-art methods. The source code is available at https://github.com/Ruiding1/C-UDA.},
  archive      = {J_TMM},
  author       = {Rui Ding and Kehua Guo and Huiling Chen and Xiangyuan Zhu},
  doi          = {10.1109/TMM.2025.3613123},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive diversity augmentation for single domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust temporal action localization with meta boundary refinement. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3613078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Action Localization (TAL) aims to localize the start and end timestamps of actions with specific categories in untrimmed videos. Despite great success, noisy action boundary labels may be included due to the inherent subjectivity of manual annotations. This can lead TAL models to learn inaccurate action boundaries during training, potentially impairing their localization performance. To systematically analyze and enhance the TAL models' robustness against noisy action boundary labels, we introduce a new task termed TAL with Noisy Label. We demonstrate that introducing even minimal random noise to action boundary labels in training data can substantially degrade the performance of leading TAL methods, thereby underscoring their vulnerability to noisy action boundary labels. To be specific, we propose a novel plug-and-play method called Energy-based Meta Boundary Refinement (EMBR), where a meta-learning pipeline is employed to rectify noisy action boundary labels, ameliorating the misguidance of noisy labels on model training. Under this meta-learning pipeline, EMBR utilizes an energy function to calculate the magnitude of label noise and re-weights samples, assigning lower weights to samples with higher noise, alleviating the impact of noisy samples on model training. In addition, considering the energy difference between action and background segments, an energy-based loss function is proposed to achieve larger energy differences across the boundary, assisting in the boundary refinement. Experimental results on the THUMOS14, ActivityNet1.3, and HACS datasets demonstrate the effectiveness of EMBR in enhancing the robustness of TAL models.},
  archive      = {J_TMM},
  author       = {Jiahua Li and Kun Wei and Zhe Xu and Liejun Wang and Cheng Deng},
  doi          = {10.1109/TMM.2025.3613078},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust temporal action localization with meta boundary refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revealing directions for text-guided 3D face editing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face editing is a significant task in multimedia, aimed at the manipulation of 3D face models across various control signals. The success of 3D-aware GAN provides expressive 3D models learned from 2D single-view images only, encouraging researchers to discover semantic editing directions in its latent space. However, previous methods face challenges in balancing quality, efficiency, and generalization. To solve the problem, we explore the possibility of introducing the strength of diffusion model into 3D-aware GANs. In this paper, we present Face Clan, a fast and text-general approach for generating and manipulating 3D faces based on arbitrary attribute descriptions. To achieve disentangled editing, we propose to diffuse on the latent space under a pair of opposite prompts to estimate the mask indicating the region of interest on latent codes. Based on the mask, we then apply denoising to the masked latent codes to reveal the editing direction. Our method offers a precisely controllable manipulation method, allowing users to intuitively customize regions of interest with the text description. Experiments demonstrate the effectiveness and generalization of our Face Clan for various pre-trained GANs. It offers an intuitive and wide application for text-guided face editing that contributes to the landscape of multimedia content creation. Our project page: https://windlikestone.github.io/Face_clan_website/.},
  archive      = {J_TMM},
  author       = {Zhuo Chen and Yichao Yan and Sehngqi Liu and Yuhao Cheng and Weiming Zhao and Lincheng Li and Mengxiao Bi and Xiaokang Yang},
  doi          = {10.1109/TMM.2025.3604978},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Revealing directions for text-guided 3D face editing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferential and commonsense visual question generation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Visual Question Generation (VQG) task generally aims to produce questions based on images in natural language. Existing studies often handle VQG as a reverse Visual Question Answering (VQA), training data-driven generators on VQA datasets. However, this solution pipeline struggles to generate high-quality questions that effectively challenge robots and humans, even by leveraging the most advanced large-scale foundational models. There are also some other VQG methods depending on elaborate and costly manual preprocessing heavily. To address these limitations, we propose a novel method with a two-module framework for automatically generating inferential visual questions that also follow commonsense. The “Scene Graph Generation” module constructs specialized scene graphs by progressively expanding connections from high-confidence nodes. This module ensures semantic consistency by aligning visual, textual, and salient features. Additionally, we incorporate external knowledge to extend abstract semantic concepts and associated facts, enriching the content of generated questions and facilitating the generated question to better follow the commonsense of human. Another module “Question Generation” utilizes the above scene graph as a foundation to search and instantiate for the question. The generated questions will match with the program templates and have diverse inferential paths. Experimental results demonstrate that our method is both effective and highly scalable. The generated questions are controllable in terms of semantic richness and difficulty, exhibiting clear inferential and commonsense properties. Furthermore, we automatically utilize our method to create a large-scale dataset, ICVQA, which includes approximately 160,000 images and 800,000 questionanswer pairs, thereby facilitating further research in VQA and visual dialogue.},
  archive      = {J_TMM},
  author       = {Chao Bi and Shuhui Wang and Na Li and Qingming Huang},
  doi          = {10.1109/TMM.2025.3604975},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Inferential and commonsense visual question generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection difference-guided geometry quality enhancement for video-based point cloud compression. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based point cloud compression (V-PCC) developed by MPEG has achieved remarkable compression efficiency for dynamic point clouds. However, the point clouds compressed by V-PCC still suffer from serious artifacts due to the lossy compression and lose a large number of points. In this paper, we propose a new geometry quality enhancement method for the V-PCC compressed point clouds and it can effectively recover the lost points. Our method is applied to the 2D projected near and far frames rather than 3D point clouds. It is designed to enhance the quality of 2D frames, guided by the predicted difference information between them. More specifically, we firstly construct a gradient-based difference prediction network (G-DPnet) to predict the difference between near and far frames. This difference is introduced in the enhancement of 2D frames, for the recovery of the lost 3D points. Meanwhile, we propose the single-frame quality enhancement network (SFQEnet) to separately enhance near and far frames. The enhanced frames are then used to produce the near-far frame difference with G-DPnet. After obtaining the difference, we feed it into a dualframe quality enhancement network (DFQEnet) to guide the further enhancement of near and far frames. Experimental results demonstrate that our method can effectively recover a large number of lost points and improve the quality of point clouds compressed by V-PCC.},
  archive      = {J_TMM},
  author       = {Yu Liu and Jingwei Bao and Zeliang Li and Qiang Zhu and Shuyuan Zhu and Siu-Kei Au Yeung and Bing Zeng},
  doi          = {10.1109/TMM.2025.3613122},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Projection difference-guided geometry quality enhancement for video-based point cloud compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTRec: Cross-modal learning for multimodal recommendation via optimal transport. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in multimodal recommendation systems due to the rapid growth of multimedia and the explosion of information. Despite notable advancements, current models often fuse multimodal embeddings with ID (name or concept) embeddings in a weighted or concatenated manner for items. Under this circumstance, they may overlook the heterogeneity problem between different modalities, and lack theoretical guarantees, potentially leading to suboptimal item representations. To overcome this challenge, we introduce a novel model named OTRec, which employs optimal transport (OT) to align heterogeneous multimodal embeddings with ID embeddings. Specifically, OTRec captures co-occurrence features across modalities and distinctive features within modalities, enabling the formation of the unified representation from both modalinvariant and modal-specific perspectives. This dual strategy ensures a comprehensive alignment of heterogeneous multimodal data, significantly improving the accuracy of capturing user preferences. Additionally, traditional recommendation models typically match an item's ID with its multimodal data as positive samples for contrastive learning, neglecting the potential complementary information from other items' multimodal data. To address this issue, we introduce a semanticenhanced contrastive learning module, which can learn latent semantic correlations across items by a semantic-similarity weighting matrix. It can be integrated as a plug-in for other models to effectively explore latent semantics. On top of this, we provide theoretical guarantees that demonstrate the effectiveness of OTRec in aligning multimodal and ID information and in enhancing the mutual information between them. Extensive evaluations on three public datasets illustrate OTRec's effectiveness and achieve state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Zongsheng Cao and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TMM.2025.3607735},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OTRec: Cross-modal learning for multimodal recommendation via optimal transport},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint deep-unfolding optimization learning for depth map arbitrary-scale super-resolution. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3613083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color-guided Depth map Super-Resolution (DSR) based on Convolutional Neural Networks (CNN) is a crucial technology to remedy the defects of mainstream commercial depth cameras and has made significant progress in recent years. Nevertheless, this technology is inevitably facing some huge challenges. Firstly, existing CNN-based DSR methods are designed as black-box network architectures. Secondly, few approaches study single model to achieve arbitrary-scale DSR. Thirdly, due to structural inconsistency between dual-modality, color-guided DSR methods always face texture-copying issue. To this end, we propose a novel joint DSR and high-low frequency decomposition optimization model and this model is unfolded into Deep Arbitrary-Scale Unfolding Network (DASU-Net). DASUNet can achieve robust continuous representation ability by alternately-iterative updating of high-low frequencies and depth features. More importantly, Arbitrary-scale Up-sampling Fusion (AUF) module is proposed to achieve arbitrary-scale up-sampling and dual-modality feature fusion. Specifically, two essential components make up the cores of AUF module including arbitraryscale up-sampling block as well as Feature Enhancement and Multiple Strategies Fusion (FEMSF) blocks. In FEMSF block, color features are first enhanced to highlight its inherentlycorrelated structure with the guidance of depth features, and then the enhanced features are modulated according to different fusion strategies. Furthermore, a fast version of DASU-Net is proposed to fit real-time scenes, named FDASU-Net, which can diminish the runtime by several times for a depth map with a size of 640 × 480 during inference. A large number of experiments can demonstrate that our DASU-Net and FDASUNet can transcend many state-of-the-art DSR methods in terms of several quantitative and qualitative indexes.},
  archive      = {J_TMM},
  author       = {Jialong Zhang and Lijun Zhao and Jinjing Zhang and Anhong Wang and Huihui Bai},
  doi          = {10.1109/TMM.2025.3613083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint deep-unfolding optimization learning for depth map arbitrary-scale super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
