<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm">TMM - 85</h2>
<ul>
<li><details>
<summary>
(2025). Instructive probabilistic transformer for complex action recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3599089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex action recognition aims to identify multiple actions over a long time. Multiple actions may occur at the same time (defined as simultaneous actions), and may occur after each other (defined as each action) Complex action recognition may suffer from two challenges. (1) Temporal repeated bias. The same action may repeat in a temporal duration. In this duration, the prediction may be biased to the majority of actions, which occur repeatedly in the past temporal frames. (2) Epistemic uncertainty of multiple actions. When there are multiple simultaneous actions in one frame, this frame's feature may result in the distribution of multiple actions overlapping each other. Without modeling proper relations between actions, the model may hinder accurately explaining certain categories in multiple actions (defined as the model's epistemic uncertainty). In this work, we propose an Instructive Probabilistic Transformer, which contains a probabilistic temporal memorizer, and a probabilistic prototype Transformer. First, to alleviate temporal repeated bias, we design a probabilistic temporal memory module, which learns probabilistic temporal gates to localize each action. The probabilistic gates instruct the selective memory of each action in long-term frames. Second, we cluster features to capture common action semantics among features (defined as action prototypes). To alleviate the epistemic uncertainty of multiple actions, we design a probabilistic prototype Transformer module. This module learns probabilistic relations depending on each prototype, which can ensure the separation between different prototypes. Third, to ensure the proper probabilistic relations depending on each prototype, we extend action loss with distribution loss to learn uncertainty-aware action loss. In uncertainty-aware action loss, the distribution loss measures the consistency between probabilistic relations and prototype relation distribution. The prediction uncertainty is learned by analyzing the entropy of multiple predictions, and helps to ensure the effect between action loss and distribution loss. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Charades, Breakfast Actions, and MultiTHUMOS.},
  archive      = {J_TMM},
  author       = {Zhao Xie and Longsheng Lu and Kewei Wu and Zhehan Kan and Xingming Yang and Dan Guo},
  doi          = {10.1109/TMM.2025.3599089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instructive probabilistic transformer for complex action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AS-GCL: Asymmetric spectral augmentation on graph contrastive learning. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.},
  archive      = {J_TMM},
  author       = {Ruyue Liu and Rong Yin and Yong Liu and Xiaoshuai Hao and Haichao Shi and Can Ma and Weiping Wang},
  doi          = {10.1109/TMM.2025.3604953},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AS-GCL: Asymmetric spectral augmentation on graph contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards semi-supervised dual-modal semantic segmentation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D and 2D data acquisition techniques, it has become easy to obtain point clouds and images of scenes simultaneously, which further facilitates dual-modal semantic segmentation. Most existing methods for simultaneously segmenting point clouds and images rely heavily on the quantity and quality of the labeled training data. However, massive point-wise and pixel-wise labeling procedures are time-consuming and labor-intensive. To address this issue, we propose a parallel dual-stream network to handle the semi-supervised dual-modal semantic segmentation task, called PD-Net, by jointly utilizing a small number of labeled point clouds, a large number of unlabeled point clouds, and unlabeled images. The proposed PD-Net consists of two parallel streams (called original stream and pseudo-label prediction stream). The pseudo-label prediction stream predicts the pseudo labels of unlabeled point clouds and their corresponding images. Then, the unlabeled data is sent to the original stream for self-training. Each stream contains two encoder-decoder branches for 3D and 2D data respectively. In each stream, multiple dual-modal fusion modules are explored for fusing the dual-modal features. In addition, a pseudo-label optimization module is explored to optimize the pseudo labels output by the pseudo-label prediction stream. Experimental results on two public datasets demonstrate that the proposed PD-Net not only outperforms the comparative semi-supervised methods but also achieves competitive performances with some fully-supervised methods in most cases.},
  archive      = {J_TMM},
  author       = {Qiulei Dong and Jianan Li and Shuang Deng},
  doi          = {10.1109/TMM.2025.3604939},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards semi-supervised dual-modal semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high resolution (UHR) image segmentation is a challenging task that requires efficient processing of large images while maintaining high accuracy. Existing approaches usually employ both shallow and deep networks to extract high-resolution details and global context from different-resolution inputs, achieving a balance between performance, memory, and speed. However, these methods still rely on preserving relatively high-resolution features within the deep network, leading to increased time and memory costs. This also indicates that the full potential of the high-resolution information from the shallow network remains underexplored. To address this, we propose a novel framework called the Hierarchical Grafting Network (HGN), wherein the shallow network is hierarchically grafted to the deep network from multiple perspectives, enabling comprehensive utilization of the features from the shallow network. Our framework involves carefully designed global structure aggregated grafting and local structure aligned grafting mechanism, which progressively integrate semantic details and spatial structure from the shallow network to the deep network. In addition, to enhance the discriminative power of the high-resolution local features extracted by the shallow network, we introduce a shallow-deep contrastive loss to encourage the shallow network to learn semantically similar features to those of the deep network. Extensive experiments on several UHR image segmentation datasets demonstrate that our approach outperforms state-of-the-art UHR methods. The results demonstrate an overall improvement in terms of memory efficiency, accuracy, and speed.},
  archive      = {J_TMM},
  author       = {Ting Liu and Jing Yang and Shikui Wei and Yanning Zhang},
  doi          = {10.1109/TMM.2025.3604913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical grafting network with structural alignment for ultra-high resolution image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crafting more transferable adversarial examples via quality-aware transformation combination. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input diversity is an effective technique for crafting transferable adversarial examples that can deceive unknown AI models. Existing input-diversity-based methods typically use single input transformation, limiting targeted transferability and defense robustness. Combining different transformation types is challenging, as keeping increasing types would degrade semantic information and targeted transferability. This paper proposes a quality-aware transformation combination attack (TCA) that selects high-quality transformation combinations. The quality-aware selection enables expansion of transformation types, enhances input diversity, and hence improves targeted transferability and defense robustness. We first design a quality-evaluation framework to quantify the effectiveness of transformation combinations, which jointly considers convergence, transferability, and robustness. Only a small group (up to 10) of images are required for computation-efficient quality evaluation. Experiments validate TCA's superiority over state-of-the-art baselines in adversarial transferability and robustness. When defenses are secured, the average targeted success rate of TCA with four transformation types (i.e., TCA-t4) outperforms the best baseline by 26%$\sim$42% on ImageNet.},
  archive      = {J_TMM},
  author       = {Junlin Liu and Xinchen Lyu and Chenshan Ren and Qimei Cui},
  doi          = {10.1109/TMM.2025.3604967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Crafting more transferable adversarial examples via quality-aware transformation combination},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code is made publicly available at https://github.com/hangyu94/KE2NT.},
  archive      = {J_TMM},
  author       = {Hangyu Li and Yihan Xu and Jiangchao Yao and Nannan Wang and Xinbo Gao and Bo Han},
  doi          = {10.1109/TMM.2025.3604916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-enhanced facial expression recognition with emotional-to-neutral transformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement-based equivariant learning for compositional VQA. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional visual question answering (VQA) represents a challenging yet fundamental task that requires models to comprehend novel combinations of previously learned concepts. The current methods often overlook the disentanglement of underlying concepts and are restricted in terms of their ability to effectively capture the compositional variation mechanism. Moreover, the state-of-the-art techniques depend on additional clues for training, which is not feasible in real-world VQA scenarios. To address these issues, in this paper, we introduce a novel Disentanglement-based EquivAriant Learning (DEAL) framework for compositional VQA, which is guided exclusively by ground-truth answers. In DEAL, we employ causality-inspired interventions to disentangle concepts derived from visual and textual inputs within a re-encoding framework. Based on the principle of equivariance, we subsequently perform a compositional transformation on the inference input and impose the equivariant constraint on the output to augment the compositional reasoning capacity of the model. Comprehensive experiments conducted on the benchmark CLEVR-CoGenT and GQA-SGL datasets validate the superiority of our proposed DEAL approach over the existing state-of-the-art methods for compositional VQA tasks in both visual and linguistic generalization settings.},
  archive      = {J_TMM},
  author       = {Zhou Du and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604897},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentanglement-based equivariant learning for compositional VQA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient SDRTV-to-HDRTV by learning from image formation. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary display enables video content rendering with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of existing content remains in standard dynamic range (SDR) format. Therefore, the conversion of SDR content to HDRTV standards holds significant value. This paper delineates and analyzes the SDRTV-to-HDRTV conversion by modeling the formation of SDRTV/HDRTV content. The findings reveal that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this, we propose a new three-step solution called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step utilizes global statistics for image-adaptive color adjustments, followed by a local enhancement network for detail improvement. These two components are integrated as a generator, with GAN-based joint training ensuring highlight consistency. Our method, tailored for ultra-high-definition TV content, offers both effectiveness and computational efficiency in processing 4K resolution images. We also construct HDRTV1K, a dataset comprising HDR videos adhering to the HDR10 standard, featuring 1235 training and 117 testing images at 4K resolution. Furthermore, we employ five metrics to assess SDRTV-to-HDRTV performance. Our results demonstrate state-of-the-art performance both quantitatively and visually. The codes and models are available at https://github.com/xiaom233/HDRTVNet-plus.},
  archive      = {J_TMM},
  author       = {Xiangyu Chen and Zheyuan Li and Zhengwen Zhang and Jimmy S. Ren and Yihao Liu and Jingwen He and Yu Qiao and Jiantao Zhou and Chao Dong},
  doi          = {10.1109/TMM.2025.3604961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient SDRTV-to-HDRTV by learning from image formation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised hyperspectral image (HSI) clustering remains a fundamental yet challenging task due to the absence of labeled data and the inherent complexity of spatial-spectral interactions. While recent advancements have explored innovative approaches, existing methods face critical limitations in clustering accuracy, feature discriminability, computational efficiency, and robustness to noise, hindering their practical deployment. In this paper, a self-supervised efficient low-pass contrastive graph clustering (SLCGC) is introduced for HSIs. Our approach begins with homogeneous region generation, which aggregates pixels into spectrally consistent regions to preserve local spatial-spectral coherence while drastically reducing graph complexity. We then construct a structural graph using an adjacency matrix A and introduce a low-pass graph denoising mechanism to suppress high-frequency noise in the graph topology, ensuring stable feature propagation. A dual-branch graph contrastive learning module is developed, where Gaussian noise perturbations generate augmented views through two multilayer perceptrons (MLPs), and a cross-view contrastive loss enforces structural consistency between views to learn noise-invariant representations. Finally, latent embeddings optimized by this process are clustered via K-means. Extensive experiments and repeated comparative analysis have verified that our SLCGC contains high clustering accuracy, low computational complexity, and strong robustness. The code source will be available at https://github.com/DY-HYX.},
  archive      = {J_TMM},
  author       = {Yao Ding and Zhili Zhang and Aitao Yang and Yaoming Cai and Xiongwu Xiao and Danfeng Hong and Junsong Yuan},
  doi          = {10.1109/TMM.2025.3604954},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLCGC: A lightweight self-supervised low-pass contrastive graph clustering network for hyperspectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional quality assessment for text-to-3D assets: Dataset and model. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively. Specifically, the proposed method utilizes the projection videos of text-to-3D assets to extract 3D shape, texture and text-asset correspondence features, then fuses them to calculate the final three preference scores respectively. Extensive experimental results demonstrate the effectiveness of the proposed T23DAQA method in evaluating the quality of AI generated 3D asset, which is more consistent with human perception. To the best of our knowledge, this is the first work that studies the problem of text-guided 3D generation quality assessment, and our database and codes will be released to facilitate future research.},
  archive      = {J_TMM},
  author       = {Kang Fu and Huiyu Duan and Zicheng Zhang and Xiaohong Liu and Xiongkuo Min and Jia Wang and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3604905},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-dimensional quality assessment for text-to-3D assets: Dataset and model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of ‘Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in $2.55\% / 5.7\%$ under different settings, compared with the existing methods.},
  archive      = {J_TMM},
  author       = {Ming-Hui Liu and Xiao-Qian Liu and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TMM.2025.3604932},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DATA: Multi-disentanglement based contrastive learning for open-world semi-supervised deepfake attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a weakly supervised referring image segmentation method, named CollabLearn, that segments objects described by free-form referring expression utilizing solely image-text pairs. Existing methods suffer from incorrect localization of referring expressions due to the lack of high-level semantics in cross-modal alignment or rough segmentation of referenced objects stemming from the absence of low-level details. To address these issues, we propose an innovative framework for generating cross-modal features encompassing both high-level semantics and low-level details via two fusion modules: a semantic awareness module and a detail cognition module. Each of these modules generates an activation map, and they mutually correct each other through a collaborative learning strategy. Specifically, the semantic awareness module performs in-depth cross-modal interaction and achieves accurate localization in a top-down manner. The detail cognition module facilitates the segmentation of entire objects in a bottom-up manner. A collaborative learning strategy is designed to enable interaction between these two modules, enforcing sufficient vision-language alignment. Experiments on three benchmarks demonstrate that CollabLearn consistently outperforms state-of-the-art weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Chao Jiang and Yuqiu Kong and Mengnan Zhao and Lihe Zhang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3604944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CollabLearn: Propelling weakly-supervised referring image segmentation through collaboration between semantics and details},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced head: Exploring strong detection heads with vision transformer. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial component of object detectors, current detection heads often lack the capability to effectively utilize contextual information, adapt to deformable objects, and align features and tasks. However, most existing methods prioritize a single capability, lacking comprehensive approaches to introduce them simultaneously. In this paper, we propose the Enhanced Head to integrate the above three capabilities into the detectors concurrently. Specifically, we propose three attention blocks with linear complexity: Global Concentrated Attention (GCA), Local Deformable Cross-Task Attention (LDCA), and Boundary-Aware Cross-Task Attention (BACA). The GCA captures long-range dependencies efficiently by employing Spatial Information Concentration (SIC). The LDCA improves feature alignment and deformation adaptability by enabling local deformable cross-task feature interactions. The BACA aligns classification features with localization results, enhancing task alignment and further improving deformation adaptability through a region-deformable interaction scheme. We implement Enhanced Head as a plug-and-play detection head and evaluate its effectiveness through extensive experiments on the MS COCO and VisDrone datasets. For instance, on the COCO detection benchmark, our Enhanced Head achieves +3.6 AP gain for FSAF, +3.3 AP for RetinaNet, and +2.9 AP for ATSS while reducing the FLOPs.},
  archive      = {J_TMM},
  author       = {Zewen Du and Zhenjiang Hu and Guiyu Zhao and Ying Jin and Hongbin Ma},
  doi          = {10.1109/TMM.2025.3604917},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced head: Exploring strong detection heads with vision transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual high-order total variation model for underwater image restoration. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are typically characterized by color cast, haze, blurring, and uneven illumination due to the selective absorption and scattering when light propagates through the water, which limits their practical applications. Underwater image enhancement and restoration (UIER) is one crucial mode to improve the visual quality of underwater images. However, most existing UIER methods concentrate on enhancing contrast and dehazing, rarely pay attention to the local illumination differences within the image caused by illumination variations, thus introducing some undesirable artifacts and unnatural color. To address this issue, an effective variational framework is proposed based on an extended underwater image formation model (UIFM). Technically, dual high-order regularizations are successfully integrated into the variational model to acquire smoothed local ambient illuminance and structure-revealed reflectance in a unified manner. In our proposed framework, the weight factors-based color compensation is combined with the color balance to compensate for the attenuated color channels and remove the color cast. In particular, the local ambient illuminance with strong robustness is acquired by performing the local patch brightest pixel estimation and an improved gamma correction. Additionally, we design an iterative optimization algorithm relying on the alternating direction method of multipliers (ADMM) to accelerate the solution of the proposed variational model. Considerable experiments on three real-world underwater image datasets demonstrate that the proposed method outperforms several state-of-the-art methods with regard to visual quality and quantitative assessments. In the quantitative assessments, the proposed method achieves average scores of 0.205 FADE, 7.688 Entropy, 0.628 UCIQE, and 0.775 FDUM across the UIEB and UIQS datasets. Moreover, the proposed method can also be extended to outdoor image dehazing and low-light image enhancement tasks. The code is available at https://github.com/HouGuojia/UDHTV.},
  archive      = {J_TMM},
  author       = {Yuemei Li and Guojia Hou and Peixian Zhuang and Zhenkuan Pan},
  doi          = {10.1109/TMM.2025.3604900},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual high-order total variation model for underwater image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FontGuard: A robust font watermarking approach leveraging deep font knowledge. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard.},
  archive      = {J_TMM},
  author       = {Kahim Wong and Jicheng Zhou and Kemou Li and Yain-Whar Si and Xiaowei Wu and Jiantao Zhou},
  doi          = {10.1109/TMM.2025.3604908},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FontGuard: A robust font watermarking approach leveraging deep font knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the behavior of contrastive regularization in improving chinese text recognizer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense representation space in Chinese scene text recognition (STR) makes discriminating between categories highly challenging, because of the large candidate category set. Mainstream STR methods have achieved remarkable advancements by leveraging linguistic knowledge to implicitly address this challenge. In this paper, inspired by the correlation between recognizer performance and the distributional properties of character representations, as well as the inherent consistency between this correlation and supervised contrastive learning (SupCon), we thoroughly investigate how to integrate SupCon with an STR model to alleviate this challenge, and elucidate some dynamic behaviors underlying the performance improvements. Specifically, we analyze the SupCon-STR models instantiated with different projectors and evaluate their distributional properties through metrics, including intra-class compactness, inter-class separability, and feature redundancy, while assessing performances that involve in-domain accuracy and cross-domain recognition generalization. The main results reveal how the temperature $\tau$ and projectors affect the representation distribution, and highlight that suitable intra-class compactness and sufficient inter-class separability are key factors for delivering competitive performances in both in-domain and cross-domain STR scenarios. Moreover, these results also provide valuable insights into the design of SupCon-STR architectures for diverse resource constraints. Taking existing Chinese STR models as baselines, and combining SupCon-STR with them, the average improvements in cross-domain recognition performance are over 5% across 7 testing datasets. A new state-of-the-art accuracy of 77.19% on the Chinese Scene benchmark is also established.},
  archive      = {J_TMM},
  author       = {Dekang Liu and Tianlei Wang and Huanqiang Zeng and Jiuwen Cao},
  doi          = {10.1109/TMM.2025.3604892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {On the behavior of contrastive regularization in improving chinese text recognizer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep frequency-separable temporal network for efficient video denoising. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively explore inter-frame information is critical for video denoising. Existing methods often rely on complex architectures, such as optical flow estimation and cross-frame self-attention, which introduce high computational costs and limit their practicality in real-world scenarios. To address this limitation, we propose a simple yet efficient deep Frequency-Separable Temporal Network (FSTN) for video denoising. FSTN utilizes the multi-scale analysis capability of wavelet transform to extract high-frequency and low-frequency information at the feature level, enabling faster processing while maintaining high-quality reconstruction. To further reduce computational complexity and enhance detail preservation, we develop a learnable high-frequency processing module that adaptively filters noise and recovers edge details. Additionally, to effectively utilize information from long-range frames, we propose a low-frequency propagation method equipped with a temporal feature alignment module. This method enables the efficient transfer of structural information from distant frames, ensuring temporal consistency and enhancing denoising performance. Extensive experiments demonstrate that our method has 1.28× fewer network parameters than state-of-the-art efficient video denoising methods, such as BasicVSR++, and requires less computational cost while achieving comparable performance.},
  archive      = {J_TMM},
  author       = {Zhulin Tao and Jinjuan Wang and Lifang Yang and Jinshan Pan and Jinhui Tang},
  doi          = {10.1109/TMM.2025.3604914},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep frequency-separable temporal network for efficient video denoising},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BASNet: Boundary assisted network for image splicing forgery detection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing is a common technique used in image forgery. With the rapid development of digital image processing technology, detecting image splicing forgery has become increasingly challenging. Existing splicing forgery localization methods lack exploration in effectively utilizing tampered region boundary information. To address this issue, we propose a novel model for detecting image splicing forgery called boundary-assisted network (BASNet). We introduce a boundary-motivated module (BMM) to explore valuable and additional boundary features related to tampered regions, enhancing representation learning for detecting tampered regions. Additionally, we present a boundary-enhanced module (BEM) to enhance boundary information using the cross-channel attention mechanism. To efficiently merge features from various levels and boundary features, we further present the feature fusion module (FFM). To optimize performance, the BASNet incorporates weighted binary cross-entropy loss, dice loss, and boundary loss, which can effectively leverage edge supervision while mitigating imbalance between positive and negative samples. Evaluation of five widely-used forgery detection datasets demonstrates the state-of-the-art performance of the BASNet. Robustness experiments verify that the BASNet is robust enough to detect image splicing forgery across various common attacks.},
  archive      = {J_TMM},
  author       = {Enji Liang and Kuiyuan Zhang and Zhongyun Hua and Xiaohua Jia},
  doi          = {10.1109/TMM.2025.3604911},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BASNet: Boundary assisted network for image splicing forgery detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy protection based on hopfield cross neural network in WBANs for medical images. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable medical data collection and surveillance devices provides real-time guarantees for the whole process of a patient's medical treatment, especially medical image data plays a key role. However, existing medical images face data leakage, pollution and vulnerability to attacks during transmission over wireless body area networks(WBANs). To address these issues, a privacy protection algorithm based on Hopfield cross neural network (HCNN) for medical data is proposed. Specifically, the HCNN model is first constructed and its dynamic behavior is analyzed, which is suitable for application to image encryption. Then, a confusion method of NZ fractal curve sorting matrix (NZ-FCSM) is designed to achieve good encryption effect. Subsequently, the secret image sharing (SIS) technique based on sharing matrix is introduced to enhance the algorithm robustness. Finally, an alignment embedding of double diamond prediction (AEDDP) method is proposed to implement lossless hiding of private information. The present issues in medical image protection include ensuring the security and effectiveness of encryption algorithms while maintaining the robustness and concealment of ciphertext data, and balancing the need for preservation with the limited resources of complex work environment. Experimental results show that the proposed algorithm achieves PSNR of 53 dB for the cipher image, more than 36 dB for the reconstructed image, and the information entropy of the secret image is over 7.99, and displays good robustness. These findings highlight the validity of the algorithm in medical image data privacy preserving applications that ensure confidentiality and extend to practical applications of concealed transmission of confidential information and secure multi-party transactions.},
  archive      = {J_TMM},
  author       = {Xiuli Chai and Guoqiang Long and Yakun Ma and Changbo Li and Zhihua Gan and Yushu Zhang},
  doi          = {10.1109/TMM.2025.3604898},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy protection based on hopfield cross neural network in WBANs for medical images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification via hölder divergence for multi-view representation learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the “distance” between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpasses the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.},
  archive      = {J_TMM},
  author       = {Yan Zhang and Ming Li and Chun Li and Zhaoxia Liu and Ye Zhang and F. Yu},
  doi          = {10.1109/TMM.2025.3604966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty quantification via hölder divergence for multi-view representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral pedestrian detection has been shown to be effective in improving performance in complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To efficiently compress multispectral object detection networks, we propose a novel distillation method, the Adaptive Modal Fusion Distillation (AMFD) framework. Unlike traditional distillation methods, the AMFD framework fully leverages the original modal features from the teacher network, thereby significantly enhancing the performance of the student network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP, SUNRGB-D and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.},
  archive      = {J_TMM},
  author       = {Zizhao Chen and Yeqiang Qian and Xiaoxiao Yang and Chunxiang Wang and Ming Yang},
  doi          = {10.1109/TMM.2025.3604937},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AMFD: Distillation via adaptive multimodal fusion for multispectral pedestrian detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic frame aggregation-based transformer for live video comment generation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments that align with viewer interests. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with cross-attention mechanism to attend to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large-scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.},
  archive      = {J_TMM},
  author       = {Anam Fatima and Yi Yu and Janak Kapuriya and Julien Lalanne and Jainendra Shukla},
  doi          = {10.1109/TMM.2025.3604921},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic frame aggregation-based transformer for live video comment generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of repetitive actions in videos, a task commonly referred to as Video Action Counting (VAC), is a critical challenge in understanding and analyzing content in sports, fitness, and daily activities. Traditional approaches to VAC have largely overlooked the nuanced irregularities inherent in action repetitions, such as interruptions and variable lengths between cycles. Addressing this gap, our study introduces a novel perspective on VAC, focusing on Irregular Video Action Counting (IVAC), which emphasizes the importance of modeling the irregular repetition priors present in video content. We conceptualize these priors through two key aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures that the spatiotemporal representations across all cycle segments in a video remain homogeneous, thereby reflecting the uniformity of actions between different cycle segments. In contrast, Cycle-interval Inconsistency mandates a clear semantic distinction between the representations of cycle segments and intervals, acknowledging the inherent dissimilarities in content. To effectively encapsulate these priors, we introduce a novel methodology consisting of consistency and inconsistency modules, underpinned by a tailored pull-push loss ($\mathrm {P^{2}~L}$) mechanism. This approach employs a pull loss to enhance the cohesion among cycle segment features and a push loss to distinctly differentiate between cycle and interval segment features. Empirical evaluations on the RepCount dataset illustrate that our IVAC-$\mathrm {P^{2}~L}$ model sets a new benchmark in state-of-the-art performance for the VAC task. Moreover, our model demonstrates adaptability and generalization across diverse video content, achieving superior performance on two additional datasets, UCFRep and Countix, without necessitating dataset-specific fine-tuning. These findings not only validate the effectiveness of our approach in addressing the complexities of irregular repetitions in videos but also open new avenues for future research in video understanding and analysis.},
  archive      = {J_TMM},
  author       = {Hang Wang and Zhi-Qi Cheng and Youtian Du and Lei Zhang},
  doi          = {10.1109/TMM.2025.3604935},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IVAC-$\mathrm {P^{2}~L}$: Leveraging irregular repetition priors for improving video action counting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception-distortion- tradeoff reveals the limitation of current low-level deep learning paradigms, i.e., minimizing reconstruction distortion does not guarantee improved perceptual quality. Acknowledging the lack of a reliable perception-oriented optimization function, we are motivated to explore a flexible approach for enhancing perceptual quality by steering the tradeoff to prioritize perception. To this end, we reconsider the perception-distortion function by incorporating the Just-Noticeable-Distortion (JND) mechanism. We mathematically demonstrate that in the common image restoration process, altering the optimization target from natural images to distorted images—where the distortion intensity is constrained by the JND threshold and the distortion type aligns with that arising from the restorer itself—effectively obtained improved perception indices without any changes to the restorer or optimization function. Accordingly, to facilitate various low-level learning models, we are motivated to construct the first large-scale CNN-oriented JND image dataset. Our dataset comprises 500 natural images and 4,500 degraded versions generated by a series of autoencoders, as well as the actual JND judgment results collected through rigorous subjective testing from twenty volunteers. Finally, a learning-based JND inference model is established on the proposed dataset and employed in the proposed JND-based adaptation scheme, where the inferred JND images serve as pseudo-ground truth for the training or fine-tuning processes of low-level vision models. Extensive experiments on image super-resolution and end-to-end image compression across multiple models have shown encouraging improvements in perceptual quality, demonstrating the effectiveness of the proposed scheme. Our dataset is available at: https://github.com/ohq17/CNN-Oriented-JND-Dataset.},
  archive      = {J_TMM},
  author       = {Xuelin Shen and Haoqiao Ou and Zhangkai Ni and Wenhan Yang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TMM.2025.3604973},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transferring from distortion to perception-oriented optimization: Just-noticeable-distortion-based domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RandomViG: Random vision graph neural network for image classification. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Graph Neural Network (ViG) is the first graph neural network model capable of directly processing image data. The community primarily focuses on the model structures to improve ViG's performance but lacks attention to its graph construction method. To avoid quadratic computational complexity, ViG uses clustering algorithms (K-nearest neighbor) to construct graph structures. Nevertheless, clustering algorithms introduce biases, which limit ViG's ability to obtain global information. To address this problem, we propose RandomViG, which abandons clustering algorithms and uses a random manner to obtain relationships between nodes. Our RandomViG is sparse in computation and can approximate a complete graph, enabling ViG to gain global interaction capability. In order to obtain the local dependence, we design a local feature extraction module for RandomViG. In addition, to alleviate the over-smoothing problem, we propose a novel method called MRN (maintaining relationships among nodes). Considering that the increased feature diversity does not necessarily lead to better performance, MRN does not aim to maximize the feature diversity of the model but instead strives to maintain consistency between the feature similarity and the inherent similarity of the original image. We validate our proposal in three major computer visual tasks, including image classification, object detection, and instance segmentation. Without extra data, RandomViG-Ti achieves 79.4% ImageNet-1 K top-1 accuracy, outperforming the baseline (ViG) by 1.2%. Under the same model scale, our RandomViG performs better with fewer FLOPs compared with existing state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Xun Gong and Daisong Yan and Zhemin Zhang},
  doi          = {10.1109/TMM.2025.3604948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RandomViG: Random vision graph neural network for image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLE: Out-of-distribution detection with shallow layer-driven enhancement. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3604940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution detection aims to protect models against overconfidently categorizing samples from unknown categories, i.e., out-of-distribution data (OOD), into known categories, i.e., in-distribution data (ID). From the perspective of feature distribution, the difference between OOD samples and ID samples can be decomposed into semantic shifts and covariate shifts. Most DL-based methods only extract deeper features, which represent semantic shifts, to discern feature variances in the data, ignoring the exploration of covariance shifts. In this paper, we propose a Shallow Layer-driven Enhanced OOD detection method (SLE), which enhances the difference of OOD samples by exploiting covariate shifts in shallow features. Specifically, it contains three main components: Hierarchical Feature Extractor (HFE), Adaptive Dimensionality Reduction Strategy (ADR), Cross-layer Score Aggregator (CSA). HFE is responsible for extracting both deeper and shallow features from the deep network. ADR adaptively reduces all hierarchical feature dimensionality according to sample characteristics, avoiding feature redundancy. CSA defines a novel confidence score for OOD samples, that effectively prevents confusion in the feature representation space at each layer. In SLE, these three closely related components cooperate with each other to effectively enhance the representation ability of OOD samples and divide OOD data better. We conduct extensive experiments to examine the performance of SLE in four benchmarks and discuss its individual components. This method performs well on the OOD datasets.},
  archive      = {J_TMM},
  author       = {Zhenni Yang and Chengxu Liu and Xueming Qian},
  doi          = {10.1109/TMM.2025.3604940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SLE: Out-of-distribution detection with shallow layer-driven enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active cross-modal domain adaptation. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most cross-modal methods assume that training and testing data come from the same domain, which is often not the case in real-world scenarios due to cross-modal domain shifts and potential unknown concepts. Moreover, cross-modal shifts hinder the capture of unknown concepts, and the presence of unknown concepts can in turn exacerbate the cross-modal shifts. To address these challenges, this paper proposes a new paradigm called Active Cross-Modal Domain Adaptation (ACM-DA), wherein only cross-modal data from the source domain and uni-modal data from the target domain are utilized. To concurrently mitigate the adverse effects of both cross-modal domain shifts and unknown concepts, we propose a Curiosity-Driven Active Adaptation Network (CD-A2N), selectively annotating samples to maximize performance gain. First, we present Curiosity Arousal within Cross-modal Domain Adaptation (CA-CDA) to explore the complexity and novelty characteristics of target samples, while reducing cross-modal discrepancy and aligning source and target domains. Second, Curiosity-driven Active Learning (CAL) is devised to strategically select a subset of target samples for annotation, aiming to achieve more valuable data selection at a small labeling cost. Finally, we jointly train CA-CDA and CAL with the newly labeled target domain sub-dataset to alleviate the above issues. Extensive experiments demonstrate that CD-A2N provides an effective solution for achieving ACM-DA. Code will be available at https://github.com/Feliciaxyao/ACM-DA.},
  archive      = {J_TMM},
  author       = {Xuan Yao and Xiao Peng and Junyu Gao and Zhaoquan Yuan and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3604968},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Active cross-modal domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards multimodal emotional support conversation systems. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support. This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities. This dataset captures the intricate interplay of user emotions, system strategies, system emotions, and system responses, setting a new precedent in the field. Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. This work not only pushes the boundaries of AI's role in mental health care but also establishes a foundation for developing conversational agents that can provide more empathetic and effective emotional support.},
  archive      = {J_TMM},
  author       = {Yuqi Chu and Lizi Liao and Zhiyuan Zhou and Chong-Wah Ngo and Richang Hong},
  doi          = {10.1109/TMM.2025.3604951},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards multimodal emotional support conversation systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HARG: Hierarchical adaptive reasoning graph for activity parsing. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a video understanding task, activity parsing aims at encompassing actions into multiple levels of activity components, including activity, sub-activity and atomic action, enabling understanding of complex video scenes within multimedia systems. Existing methods form activity parsing as a multi-task learning problem to predict multi-granular activity labels simultaneously, which ignores modeling the hierarchical structure and the fine-grained transitions of activity components at different levels. In this paper, we propose a Hierarchical Adaptive Reasoning Graph (HARG) to model the hierarchical structure (i.e., object level $\rightarrow$ atomic action level $\rightarrow$ activity level) dynamically and precisely. To achieve that, an object reasoning graph (ORG) and an atomic action reasoning graph (ARG) are designed to reason fine-grained information transitions between multiple actors at different levels. In addition, an adaptive segmentation module (ASM) is investigated for bridging the gap among different levels, permitting step-by-step reasoning from the object level to the atomic action level. Experimental results show our method outperforms state-of-the-art methods on two activity parsing datasets, achieving hierarchical modeling and fine-grained reasoning for activity understanding. The code is available on GitHub: https://github.com/whuoyj/HARG.},
  archive      = {J_TMM},
  author       = {Yangjun Ou and Li Mi and Zhenzhong Chen},
  doi          = {10.1109/TMM.2025.3604927},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HARG: Hierarchical adaptive reasoning graph for activity parsing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware wavelet transformer for pyramid learning object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3604963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer displays the impressive capabilities on vision tasks. The built-in self-attention retains the quadratic computation burden in respect of the spatial resolution of image features. The traditional downsampling (e.g., average pooling) can reduce the resolution. Nonetheless, it may suffer from the dropping of detailed information. In this work, we propose an Efficient Wavelet Attention (EWA), which injects the wavelet transform and a Mean GELU (MGELU) function. Firstly, the wavelet transform enables the detailed information to participate in the efficient interaction modeling. Secondly, MGELU regards the statistical mean as reference and loosely passes the high relative responses. Building upon EWA, we present an effective Semantic-aware Wavelet Transformer (SWFormer), which is then employed for pyramid learning, including CNN feature hierarchy or Region of Interest (RoI) features. For the feature hierarchy, a Pyramid SWFormer (PSWFormer) incorporates SWFormer at each level to fit the bidirectional features. For RoIs, a Recognition-Localization SWFormer (RLSWFormer) is inserted into the head to fit their features from all levels. The effectiveness of our SWFormer is displayed experimentally on the MS COCO detection dataset and the Pascal VOC dataset. When exploiting Swin-small backbone, our SWFormer-based method acquires AP of 52.1 in the single-scale evaluation on the COCO test-dev set. This work will have the codes at https://github.com/TimeIsFuture/Dt2_SWFormer.},
  archive      = {J_TMM},
  author       = {Yang Li and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Puhua Chen},
  doi          = {10.1109/TMM.2025.3604963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-aware wavelet transformer for pyramid learning object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-contact respiratory monitoring, reducing motion artifact and selecting the appropriate Region of Interest (ROI) pose significant challenges. Most motion artifact removal methods rely on signal periodicity assumptions, while respiratory signals usually are non-periodic in real-world scenarios. Existing automated ROI selection approaches are mostly primarily impacted by the texture of clothing, absence of chest landmarks, and obstruction of face. To improve the quality of respiratory signals, in this study, we propose a framework for automatic respiratory ROI selection based on video, namely, Optimizing Video-based Respiration Monitoring (OVRM), which consists of peak-trough adaptive motion artifact removal and characteristic-driven adaptive ROI selection. This motion artifact removal strategy removes motion artifacts by using a dynamic ratio-based judgment mechanism, and reconstructs signals using sinusoidal interpolation. The adaptive ROI method scores signals based on periodicity, similarity, smoothness, and energy, selecting the highest-scoring blocks as the ROIs to match respiratory signals efficiently. Experimental results, validated across four datasets, demonstrate that OVRM effectively reduces signal noise caused by subject movement and outperforms state-of-the-art non-contact respiratory monitoring algorithms. The dataset and code are publicly available at: https://github.com/zxx5058/OVRM.},
  archive      = {J_TMM},
  author       = {Xinxin Zhang and Xudong Tan and Yan Zhu and Mei Zhou and Menghan Hu and Zhanzhan Cheng and Nengfeng Qian and Changyin Wu and Guangtao Zhai and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3604970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing video-based respiration monitoring: Motion artifact reduction and adaptive ROI selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action-responsive contrastive network for fine-grained skeleton-based action recognition. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, fine-grained skeleton action recognition based on graph convolutional networks (GCNs) has become an important research focus. Fine-grained action recognition refers to the accurate recognition of subtle, complex or detailed actions. This task is particularly challenging due to the limited appearance information in skeleton data and the limitations of predefined single-topology skeleton structures. To address these challenges, we propose an action-responsive contrastive network (ARCN). The network consists of two main components: an action-responsive graph convolutional network (ARGCN) with enhanced skeleton topology and a fine-grained action comparator (FAC) that uses feature contrastive learning to explore the latent space of motion features. The ARGCN contains two specialized modules: the action-responsive topology (ART) module, which captures important motion features through the learned action-specific topology structure matrix and multiscale temporal features; and the action-responsive attention (ARA) module, which learns complex spatiotemporal skeleton attention information. These modules jointly generate a multichannel cross-temporal dynamic skeleton joint attention topology map tailored for the specific action being analysed. To further clarify the fine-grained action feature differences, the FAC is integrated in some stages of the ARGCN. The FAC performs spatiotemporal decoupling of feature maps, classifies and contrasts similar and different fine-grained motion features, and builds a learnable latent space for fine-grained motion, thereby improving classification performance. Our model is evaluated on six public datasets: NTU RGB+D, NTU RGB+D 120, NW-UCLA, UAV-Human, Finegym, and Diving48. It achieves 91.2% accuracy on the NTU RGB+D 120 dataset X-Set, 97.2% accuracy on the NW-UCLA dataset, 44.6% accuracy on the UAV-Human dataset CSv1, 72.0% accuracy on the UAV-Human dataset CSv2, 95.3% accuracy on the Finegym dataset, and 54.3% accuracy on the Diving48 dataset, which are competitive results compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hongjun Li and Tian Bai},
  doi          = {10.1109/TMM.2025.3604906},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Action-responsive contrastive network for fine-grained skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3604982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed AutoRegressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed AutoRegressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can achieve over a magnitude acceleration in decoding time without industrial level optimization or achieve state-of-the-art reconstruction quality compared with other INR codecs. To the best of our knowledge, our method is the first INR-based codec comparable with Ballé et al. [1] in both decoding speed and quality while maintaining low complexity.},
  archive      = {J_TMM},
  author       = {Xiang Liu and Jiahong Chen and Bin Chen and Zimo Liu and Baoyi An and Shu-Tao Xia and Zhi Wang},
  doi          = {10.1109/TMM.2025.3604982},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An efficient implicit neural representation image codec based on mixed autoregressive model for low-complexity decoding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3604965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To counter the security threats posed by the realism of image inpainting generated through diffusion models and GANs, in this paper, we propose a texture enhancement and progressive refinement network (TEPR-Net) for image inpainting localization (IIL). The IIL task is divided into two phases: coarse and fine locating. In the coarse locating phase, we utilize an anomaly texture encoder to capture tampering traces in textures, employ a texture–context feature interaction strategy to effectively integrate texture features with contextual features, and utilize a pixel-level contrastive learning strategy to enhance feature clustering and model generalization. In the fine locating phase, we first enhance the receptive field features in the frequency domain by transforming the features and separately enhancing the low- and high-frequency components. Then, we utilize the coarse localization result to augment the model's sensitivity to tampered regions. Additionally, we introduce a progressive edge distribution guidance and reconstruction strategy that progressively refines the edges of the tampered regions at each level, ultimately generating refined localization results. To support the research and evaluation of the IIL task, we create the Inpaint32K dataset, which is characterized by its large scale, diversity, comprehensiveness, high quality, and authenticity. Finally, extensive experiments demonstrate that TEPR-Net has significant advantages in terms of localization performance, generalizability, extensibility, and robustness.},
  archive      = {J_TMM},
  author       = {Qixian Hao and Kai Wang and Haoliang Cui and Jiwei Zhang and Shaozhang Niu},
  doi          = {10.1109/TMM.2025.3604965},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TEPR-net: Image inpainting localization network via texture enhancement and progressive refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and robust 3D panoptic segmentation is crucial for scene perception in autonomous driving. Modern methods widely adopt multi-modal fusion based simple feature concatenation to enhance 3D scene understanding, resulting in generated multi-modal representations typically lack comprehensive semantic and geometry information. These methods focused on panoptic prediction in a single step also limit the capability to progressively refine panoptic predictions under varying noise levels, which is essential for enhancing model robustness. To address these limitations, we first utilize BEV space to unify semantic-geometry perceptual representation, allowing for a more effective integration of LiDAR and camera data. Then, we propose PrimePSegter, a progressively combined diffusion 3D panoptic segmentation model that is conditioned on BEV maps to iteratively refine predictions by denoising samples generated from Gaussian distribution. PrimePSegter adopts a conditional encoder-decoder architecture for fine-grained panoptic predictions. Specifically, a multi-modal conditional encoder is equipped with BEV fusion network to integrate semantic and geometric information from LiDAR and camera streams into unified BEV space. Additionally, a diffusion transformer decoder operates on multi-modal BEV features with varying noise levels to guide the training of diffusion model, refining the BEV panoptic representations enriched with semantics and geometry in a progressive way. PrimePSegter achieves state-of-the-art performance on the nuScenes and competitive results on the SemanticKITTI, respectively. Moreover, PrimePSegter demonstrates superior robustness towards various scenarios, outperforming leading methods.},
  archive      = {J_TMM},
  author       = {Hongqi Yu and Sixian Chan and Xiaolong Zhou and Xiaoqin Zhang},
  doi          = {10.1109/TMM.2025.3604903},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PrimePSegter: Progressively combined diffusion for 3D panoptic segmentation with multi-modal BEV refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided adversarial attack in the low-frequency space. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3604964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples can assess the robustness of machine learning models, which has attracted the attention of many researchers to adversarial example generation methods. Transferability and imperceptibility stand out as two crucial metrics for evaluating the quality of adversarial examples. However, achieving a balance between these two indicators poses a formidable challenge. In this paper, we propose a low-frequency guided adversarial attack method (LGA) to generate adversarial examples with strong transferability and good imperceptibility. Specifically, we enhance the transferability of adversarial examples by increasing the diversity of attack algorithms, and introduce the guiding principle and the triplet loss constraint to ensure that the generated adversarial examples are optimized away from the class regions of the clean examples. We find that the low-frequency component in the frequency domain of the image contains the vast majority of the semantic information of the image. Therefore, we constrain the attack perturbations to low-frequency component space to enhance the covert nature while maintaining visual coherence, rendering the adversarial examples more difficult to perceive. We conduct extensive experiments on various models with different network structures and multiple defense strategies, and the experimental results demonstrate that our method outperforms existing methods in the tradeoff between transferability and imperceptibility, achieving the SOTA performance.},
  archive      = {J_TMM},
  author       = {Jiang Zhu and Lingping Tan and Yanchun Li and Shujuan Tian and Jianqi Li and Yaonan Wang},
  doi          = {10.1109/TMM.2025.3604964},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guided adversarial attack in the low-frequency space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view clustering via mutual information. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering focus on mining useful information from low-quality multiple sources, such as missing and distorted data that are prevalent in real life. However, after representation learning and the processing of incomplete information, existing methods often leave representations containing information task-irrelevant information. In addition, the separation between missing data imputation and clustering tasks leads to sub-optimal multi-view clustering performance. To address these issues, we propose an incomplete multi-view clustering method based on mutual information. For the problem of task-irrelevant information, we use incomplete view prediction to extract sufficient and minimal task-relevant information and provide theoretical proof from the perspective of mutual information. For the problem of separation between missing data imputation and clustering tasks, we integrate incomplete-view prediction with contrastive clustering, collaboratively enhancing the clustering performance. Comparative experiments on five public datasets, under both complete and incomplete scenarios, reveal that our method outperforms nine other competing approaches, demonstrating its effectiveness and robustness in handling multi-view data.},
  archive      = {J_TMM},
  author       = {Xuejiao Yu and Guoqing Chao and Yi Jiang and Guanzhou Ke and Dianhui Chu},
  doi          = {10.1109/TMM.2025.3604942},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incomplete multi-view clustering via mutual information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Like humans to few-shot learning through knowledge permeation of visual and language. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3604977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario. With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes. However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge. In this paper, we set out to fill this gap via a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in human intuition: a class name description offers a more general representation, whereas an image captures the specificity of individuals. BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation. On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information. Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop, particularly outperforming previous methods by a substantial margin in the 1-shot setting (improving the accuracy by 7.58% on miniImageNet).},
  archive      = {J_TMM},
  author       = {Yuyu Jia and Qing Zhou and Junyu Gao and Qiang Li and Qi Wang},
  doi          = {10.1109/TMM.2025.3604977},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Like humans to few-shot learning through knowledge permeation of visual and language},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCPoint: Contrasting corrupted point clouds for self-supervised representation learning. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised Learning (SSL), including mainstream contrastive learning, has achieved significant success in learning visual representations without the need for data annotations in 3D vision. While most contrastive learning methods focus on instance-level information through random affine transformations, they pay limited attention to the intrinsic structures within point clouds. In this work, we propose a novel SSL paradigm for point cloud representation learning, called CCPoint, which incorporates a novel form of data corruption as a negative augmentation strategy. Specifically, we degrade the input point cloud with various corruptions and conduct contrastive learning among the augmented, raw, and corrupted points to learn robust and discriminative representations. To preserve the semantic structure of the point cloud even under heavy degradation, an auxiliary reconstruction decoder is introduced into the corruption branch to provide an additional supervision signal. We explore four families of corruptions—affine, noise, masking, and combined transformations. Different from previous methods that rely on multi-modal data or complex network architectures, CCPoint achieves state-of-the-art performance on three widely used datasets (ModelNet40, ScanObjectNN, and ShapeNetPart) with a lightweight and efficient structure, reaching top linear accuracies of 92.4% and 86.2% on ModelNet40 and ScanObjectNN, respectively.},
  archive      = {J_TMM},
  author       = {Xiaoyang Xiao and Shaoyi Du and Zhiqiang Tian and Meiqin Liu and Xinhu Zheng},
  doi          = {10.1109/TMM.2025.3604890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCPoint: Contrasting corrupted point clouds for self-supervised representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive gradient-guided self-distillation keypoint detection. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of autonomous driving and 3D reconstruction, keypoint detection, as a key link in visual localization, has become a hot topic in current research. However, existing keypoint detection methods rarely pay attention to the difficulty differences of samples and lack a progressive learning mechanism, which often leads to overfitting for simple samples and underfitting for complex samples, limiting the overall performance of the model. To address these issues, we propose a novel progressive gradient-guided self-distillation method (PG2 SD) for keypoint detection, which possesses self-evolutionary learning capabilities. Specifically, we propose a progressive gradient constraint strategy (PGCS) that dynamically adjusts the gradient contributions of different samples, enabling the model to adapt to the evolving learning capability during training. On this basis, we propose a gradient-guided self-distillation strategy (G2 SDS), which integrates seamlessly with PGCS to alleviate the insufficient feature representation of hard samples in the early training stage. We further design a novel loss function to achieve dynamic collaboration between PGCS and G2 SDS, allowing G2 SDS to adaptively adjust the self-distillation parameters through the PGCS. Experimental results on multiple benchmark datasets show that our method achieves state-of-theart performance on image matching, visual localization, and 3D reconstruction tasks without designing a proprietary network, indicating broad application prospects.},
  archive      = {J_TMM},
  author       = {Zhaoyang Li and Jie Cao and Qun Hao and Haifeng Yao and Yingbo Wang},
  doi          = {10.1109/TMM.2025.3607731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive gradient-guided self-distillation keypoint detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-Based Signature (ABS) provides a critical solution for ensuring data integrity, fine-grained access control, and anonymous authentication in security-sensitive systems such as the Multimedia Internet of Things (MIoT) and multimedia streaming platforms. However, practical adoption of ABS faces three fundamental challenges: vulnerability to key exposure and escrow risks, linear growth of computational cost, and insufficient robustness in multi-authority environments. To address these issues, we propose a forward secure and threshold authorized multi-authority ABS scheme called FORT in this paper. By employing a binary tree structure to divide multiple time periods, historical signatures remain valid even in the event of key exposure. Furthermore, to balance robustness and resistance to corruption while mitigating the key escrow problem, we construct a threshold authorized multi-authority structure based on Lagrange interpolation. This structure effectively reduces the impact of a single authority on the MIoT. Additionally, through the adoption of outsourced computation technology, which offloads complex computations in the signature and verification phases to the edge server, the computational burden for both the signer and verifier is significantly reduced to a small constant. Rigorous security analysis demonstrates that the FORT scheme achieves forward security, collusion attack resistance, corrupt authority resistance and anonymity. Theoretical comparisons and simulation experiments demonstrate the lightweight nature of the FORT scheme in terms of computation and communication.},
  archive      = {J_TMM},
  author       = {Chong Guo and Bei Gong and Zhe Li and Mowei Gong and Haotian Zhu},
  doi          = {10.1109/TMM.2025.3607696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FORT: A forward secure and threshold authorized multi-authority attribute-based signature scheme for multimedia IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foodfusion: A novel approach for food image composition via diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image composition requires the use of existing dish images and background images to synthesize a natural new image, while diffusion models have made significant advancements in image generation, enabling the construction of end-to-end architectures that yield promising results. However, existing diffusion models face challenges in processing and fusing information from multiple images and lack access to high-quality publicly available datasets, which prevents the application of diffusion models in food image composition. In this paper, we introduce a large-scale, high-quality food image composite dataset, FC22k, which comprises 22,000 foreground, background, and ground truth ternary image pairs. Additionally, we propose a novel food image composition method, Foodfusion, which leverages the capabilities of the pre-trained diffusion models and incorporates a Fusion Module for processing and integrating foreground and background information. This fused information aligns the foreground features with the background structure by merging the global structural information at the cross-attention layer of the denoising UNet. To further enhance the content and structure of the background, we also integrate a Content-Structure Control Module. Extensive experiments demonstrate the effectiveness and scalability of our proposed method.},
  archive      = {J_TMM},
  author       = {Chaohua Shi and Xuan Wang and Si Shi and Xule Wang and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3607683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Foodfusion: A novel approach for food image composition via diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Large Language Models (MLLMs) have demonstrated remarkable abilities in image-language reasoning. However, they deal with Video Question Answering (VideoQA) insufficiently, especially for questions demanding causal-temporal reasoning. Typically, they directly concatenate features of uniformly sampled frames as visual inputs for VideoQA. This gives rise to two challenges. For one thing, uniformly sampled frames are discrete and separately distributed across different timestamps, disrupting the coherence of question-critical events or actions. For another, it considers every scene within videos equally and introduces redundant frames that may distract the model from discovering the truth. Towards this, we highlight the importance of identifying continuous frames that are crucial for answering the questions, and propose a lightweight and differentiable Coherence Recognizer (CoRe) to achieve this. Guided by the semantics of questions, CoRe computes scores recording the relevance between each frame and the question, and selects a set of continuous frames with the highest scores for answer prediction. Additionally, CoRe encodes the unselected frames into a short and coarse-grained representation as a completion of the general context. Equipped with CoRe, we can efficiently fine-tune the current MLLMs for VideoQA in an end-to-end manner, without suffering from the problems of incoherence or distraction. Extensive experiments demonstrate that our method achieves substantial improvements on several VideoQA benchmarks.},
  archive      = {J_TMM},
  author       = {Haibo Wang and Chenghang Lai and Weifeng Ge},
  doi          = {10.1109/TMM.2025.3607780},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adapting multimodal large language models for video question answering by capturing question-critical and coherent moments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage causal intervention framework for long-tailed SAR target recognition. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of SAR targets generally conforms to a long-tailed distribution. Due to the existence of sample distribution bias and sample selection bias, training classifiers on this distribution of data often introduces spurious correlations between samples and classes. To address this issue, we propose a two-stage causal intervention framework. The core is that structural causality allows for independent interventions on multiple biases, thereby ensuring high-quality tail class predictions while maintaining unbiased performance for head classes. Firstly, we construct a structural causal graph for the long-tailed recognition task from causal perspective. Based on this graph, the causal paths underlying the two types of biases are identified. Secondly, we design a data augmentation method named DiagPatch-M, which identifies causal features within samples. In this process, these generated patches randomly integrate causal and non-causal features from two different samples, disrupting the original recognition process and effectively eliminating biases induced by sample selection. Thirdly, we design an unbiased structural risk minimization (USRM) optimization strategy, which eliminates the “head preference” of conventional models and the “tail preference” of modified models. This strategy reduces the bias introduced by the model's dependence on the original sample distribution, and achieves stable recognition under different sample distributions. Experimental results on two long-tailed and two balanced datasets demonstrate that the effectiveness of our model surpasses the state-of-the-art (SOTA) methods, indicating the efficacy of our proposed framework in tackling the challenges posed by the long-tailed distribution in SAR target recognition.},
  archive      = {J_TMM},
  author       = {Jiaxiang Liu and Zhunga Liu and Longfei Wang and Zuowei Zhang},
  doi          = {10.1109/TMM.2025.3607804},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A two-stage causal intervention framework for long-tailed SAR target recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTSNet: Dynamic transformer slimming for efficient vision recognition. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have recently adopted increasingly complex structure (e.g., deeper or wider stacked network) to promote the representation learning capabilities of vision recognition. However, progressively deeper or wider stacked network cause the expensive computation cost, which hinders their effective deployment in resource-constrained edge clouds or end devices. In this paper, we propose DTSNet, a dynamic transformer slimming model, which scales vision transformers (ViTs) down across layers from both of the model depth and input width. This is the first time to explore the joint reduction of input tokens and model parameters for ViTs under maintaining performance. Specifically, DTSNet adopts a diversity-enhanced weight sharing module to reduce network parameters, where the weight knowledge of multiple adjacent blocks is effectively integrated into one block. Furthermore, DTSNet designs a unified and massively scalable token pruning mechanism that dynamically discarding less important tokens with a model-driven manner, by introducing a series of discriminant parameters, which is a simple change to the common architecture of vision transformers. Extensive experiments are conducted to verify that DTSNet is able to yield high efficacy in compressing parameter space and accelerating model inference. DTSNet-T/-S/-B on ImageNet achieves 3.0M/11.1M/42.9M parameters and 0.8/2.9/13.7 GFLOPs, where number of parameters are reduced by 48%$\sim$51% and inference speed are improved by 1.3$\times \sim 1.5\times$. Experiments results on semantic segmentation and object detection dataset further demonstrate the potential of DTSNet on complex dense prediction tasks. Code will be available upon publication.},
  archive      = {J_TMM},
  author       = {Wenjing Xiao and Xianzhi Li and Long Hu and Yixue Hao and Min Chen},
  doi          = {10.1109/TMM.2025.3607796},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTSNet: Dynamic transformer slimming for efficient vision recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.},
  archive      = {J_TMM},
  author       = {Jianwu Fang and Lei-Lei Li and Zhedong Zheng and Hongkai Yu and Jianru Xue and Zhengguo Li and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3607808},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EQ-TAA: Equivariant traffic accident anticipation via diffusion-based accident video synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comp-diff: A unified pruning and distillation framework for compressing diffusion models. <em>TMM</em>, 1-11. (<a href='https://doi.org/10.1109/TMM.2025.3607799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, generative models such as diffusion models (DMs) have gained prominence in various applications, and there is a growing demand for their deployment on resourceconstrained devices. Model pruning provides an effective solution by reducing the model redundancy without significantly impacting performance. However, most existing model pruning methods are designed for classification models and often lead to substantial performance degradation when applied to generative models. To address this issue, we propose Comp-Diff, a novel two stage framework of pruning and knowledge distillation tailored for diffusion models. In the pruning stage, we propose a new structured content-aware pruning (CaP) method within CompDiff to identify and preserve informative units (filters/channels) that actually contribute to the generative capability of the model. Specifically, we introduce input perturbations to the pre-trained model and measure each unit's importance score using gradients induced by these perturbations. Units with higher importance scores are considered more informative and are retained to maintain the model's generative power. In the fine-tuning stage of Comp-Diff, we propose the distribution-aware knowledge distillation (DaKD) method, which effectively transfers finegrained knowledge from the original model to the pruned one on both attention and noise distribution levels. In addition, DaKD includes an adversarial loss to improve the quality and diversity of generated outputs. To verify and evaluate our method, we apply the proposed Comp-Diff on three representative tasks: unconditional image generation, conditional image generation, and text-to-image generation. Extensive experiments on both multi-step and one-step diffusion models demonstrate that the proposed framework consistently yields compact models and outperforms existing pruning techniques by a large margin.},
  archive      = {J_TMM},
  author       = {Lu Yu and Wei Xiang and Kang Han and Gaowen Liu and Ramana Kompella},
  doi          = {10.1109/TMM.2025.3607799},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comp-diff: A unified pruning and distillation framework for compressing diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online video super-resolution methods utilize implicit memories of previous frames to provide reference information, which have a single memory stream path and are highly dependent on the continuous memory stream. However, video capture in real-world scenes is typically affected by abnormal exposures resulting in sudden changes of lightness thus interrupting the memory stream, while long-term memories suffer from memory vanishing problems during transmission. To address this problem, we propose a novel multi-memory streams based online video super-resolution paradigm that adaptively corrects for abnormal exposures and creates multi-memory streams to accurately converge long-term memories. Specifically, we first propose an exposure detection-correction module, which utilizes optical flow overfitting property and temporal lightness information to detect and correct abnormal exposures to avoid interruption of memory streams. In addition, we propose a dynamic-static decoupled alignment strategy, which can adaptively select the alignment method based on pixel displacement, thus accurately aggregating past long-term memories to create multiple memory streams. Further, we propose an adaptive memory fusion module to mine complementary information between multiple memory streams to solve the memory vanishing problem. Extensive experimental results show that our method outperforms existing video super-resolution methods on complex exposure datasets. We also conduct detailed ablation experiments to analyze and validate our contributions. The implementation code is available at https://github.com/GZ-T/MMVSR.},
  archive      = {J_TMM},
  author       = {Guozhi Tang and Hongwei Ge and Yong Luo and Bo Li and Chunguo Wu},
  doi          = {10.1109/TMM.2025.3607758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-memory streams: A paradigm for online video super-resolution in complex exposure scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clean image may be dangerous: Data poisoning attacks against deep hashing. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image retrieval using deep hashing has become increasingly popular due to the exponential growth of image data and the remarkable feature extraction capabilities of deep neural networks (DNNs). However, deep hashing methods are vulnerable to malicious attacks, including adversarial and backdoor attacks. It is worth noting that these attacks typically involve altering the query images, which is not a practical concern in real-world scenarios. In this paper, we point out that even clean query images can be dangerous, inducing malicious target retrieval results, like undesired or illegal images. To the best of our knowledge, we are the first to study data poisoning attacks against deep hashing (PADHASH). Specifically, we first train a surrogate model to simulate the behavior of the target deep hashing model. Then, a strict gradient matching strategy is proposed to generate the poisoned images. Extensive experiments on different models, datasets, hash methods, and hash code lengths demonstrate the effectiveness and generality of our attack method.},
  archive      = {J_TMM},
  author       = {Shuai Li and Jie Zhang and Yuang Qi and Kejiang Chen and Tianwei Zhang and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TMM.2025.3607774},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clean image may be dangerous: Data poisoning attacks against deep hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale invertible neural network for wide-range variable-rate learned image compression. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder-based structures have dominated recent learned image compression methods. However, the inherent information loss associated with autoencoders limits their rate-distortion performance at high bit rates and restricts their flexibility of rate adaptation. In this paper, we present a variable-rate image compression model based on invertible transform to overcome these limitations. Specifically, we design a lightweight multi-scale invertible neural network, which bijectively maps the input image into multi-scale latent representations. To improve the compression efficiency, a multi-scale spatial-channel context model with extended gain units is devised to estimate the entropy of the latent representation from high to low levels. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared to existing variable-rate methods, and remains competitive with recent multi-model approaches. Notably, our method is the first learned image compression solution that outperforms VVC across a very wide range of bit rates using a single model, especially at high bit rates.},
  archive      = {J_TMM},
  author       = {Hanyue Tu and Siqi Wu and Li Li and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2025.3607748},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale invertible neural network for wide-range variable-rate learned image compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIFormer+: Rethinking rotation-invariant feature learning in transformer. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved remarkable success in the field of computer vision due to their advantage in capturing the global information of images. However, they fail to model the variance of rotation, resulting in significant performance loss in target detection in remote sensing imagery. In this paper, a rotation-invariant transformer plus model, namely RIFormer+ is proposed to enhance the capabilities of transformers in rotation-invariant feature learning at both long-overlooked local-level and the acknowledged global-level. At the local-level, a rotation-invariant cross-patch embedding (RICPE) module is designed to generate dense patches, which handles encoding inconsistency of tokens with similar semantic information before and after rotation. Moreover, response-enhanced attention (REA) is proposed to extract more rotation-robust global features, which highlights overly dispersed responses ensure sustained attention on discriminative regions. Extensive experiments on three datasets demonstrate the effectiveness of RIFormer+. Without bells and whistles, RIFormer+ increases the classification accuracy by an average of 10% and improves the accuracy on rotated datasets by 20% compared with some state-of-the-art transformers. The code of this paper is available at: https://github.com/psychAo/RIFormerPlus.},
  archive      = {J_TMM},
  author       = {Chao Song and Yifan Zhang and Mingyang Ma and Shaohui Mei},
  doi          = {10.1109/TMM.2025.3607728},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RIFormer+: Rethinking rotation-invariant feature learning in transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Unmanned Aerial Vehicles (UAVs) and their unique vantage points present both new opportunities and challenges for person Re-Identification (ReID). Uncertain rotations and scale variations of targets in UAV images, coupled with complex environmental factors, hinder existing methods from extracting robust feature representations. Some methods either make minor modifications to the traditional model architecture or apply simple image rotations but still fail to effectively address the challenges of UAV person ReID. To overcome these limitations, we propose a novel Data Augmentation and Rotation Invariance (DARI) algorithm. First, rotation-invariant convolution is introduced to adaptively extract features, mitigating the uncertainty caused by target rotation. Second, a refined data augmentation correction strategy is employed to reduce noise interference by increasing the richness of global features at different stages. Additionally, considering that multiple features of the same identity should yield consistent recognition result, invariant constraints are designed to enhance the clustering effect. We conducted extensive experiments on both UAV and fixed-camera datasets. The results on PRAI-1581 demonstrate a 5.6% and 6.1% improvement in mAP and Rank-1, respectively, compared to baseline. These findings highlight the model's effectiveness in addressing the challenges of UAV ReID, demonstrating its robustness and superiority. The source code will be released at https://github.com/ZFZ314/DARI.},
  archive      = {J_TMM},
  author       = {Fuzeng Zhang and Eksan Firkat and Hongbing Ma and Jihong Zhu and Bin Zhu and Askar Hamdulla},
  doi          = {10.1109/TMM.2025.3607835},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DARI: Transformer-based data augmentation and rotation invariance for UAV person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential of diffusion-generated data on salient object detection. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, salient object detection (SOD) has made significant progress. However, this advancement is often constrained by the requirement for extensive training data and expensive manual annotation. To eliminate the laborious cost of dataset collection and pixel-level annotation, in this work, we employ Stable Diffusion to synthesize data and subsequently automate annotation for the SOD task. Firstly, we design a unified prompt and ChatGPT4 driven diverse prompts, which guide generating images with simple and complex scenes using Stable Diffusion. Secondly, the reliable pseudo-labels of these synthetic images are generated. For simple images, we propose the simple pseudo-label generation (SPLG) strategy which combines SAM segmentation and CLIP classifier, then train the initial SOD model. For complex images, we utilize the inference capability of the initial SOD model to generate pseudo-labels using the complex pseudo-label generation (CPLG) strategy, and employ iterative training to dynamically update the pseudo-labels. Finally, we design a simple yet effective SOD model which combines a feature fusion module (FFM) and an edge enhancement module (EEM), the former is employed to extract saliency via fusing high-level features, and the latter extracts spatial positional information from low-level features to enhance the edges of saliency results. Experiments on five benchmarks show that our method outperforms the unannotated methods, and also demonstrates better or comparable performance than weak annotation based methods. Our code will be published at https://github.com/FangWenRE/PotentialOfSDSOD.},
  archive      = {J_TMM},
  author       = {Xiangquan Liu and Xianlong Luo and Ying Ye and Xiaoming Huang},
  doi          = {10.1109/TMM.2025.3607734},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Potential of diffusion-generated data on salient object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced subspace clustering network with information bottleneck. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data mining, subspace clustering is a crucial technique which determines the union of the underlying subspace to cluster data points in an unsupervised manner. Although deep-learning-based subspace clustering, typically referred to as deep subspace clustering (DSC), has significantly improved clustering accuracy, existing DSC models still struggle to capture a comprehensive and compact latent representation as they generally explore the spatial domain to extract useful information and face difficulty in balancing the high mutual and low redundant information between the original input space and latent subspace. This leads to the performance of the model being dependent on initialization, resulting in a lack of stability. In this study, a novel network is proposed to extract features in both the frequency domain and spatial domain. We introduce three types of ResBlocks in the discrete Fourier transform (DFT), discrete cosine transform (DCT), or discrete wavelet transform (DWT) frequency domains separately to learn both the low-frequency and high-frequency information in the proposed networks. Additionally, to extract concise and rich latent representations, IB loss is employed by deriving a variational lower bound on the IB objective. Extensive experiments on several benchmark datasets verify the effectiveness of our networks compared to state-of-the-art models. In addition, detailed ablation studies are performed to demonstrate the advantages of the two introduced components.},
  archive      = {J_TMM},
  author       = {Mengran Hou and Mengyao Li and Chengli Tan and Junmin Liu and Jinhai Li and Huirong Li},
  doi          = {10.1109/TMM.2025.3607797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-enhanced subspace clustering network with information bottleneck},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local prompts distillation for domain generalization. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has been proven effective for Domain Generalization (DG) by enhancing the generalization capability of visual-language models with fewer learnable tokens. Existing methods adopt mostly inferring global-level individual prompts for the whole dataset to capture domain-invariant knowledge across different domains. However, since domain shifts exist, a single global-level individual prompt is easily overfitted to source domain datasets, thus lacking generalizability to the whole dataset's feature distribution. Moreover, fluctuations in the generalization performance during the training process in DG problems often pose significant challenges to model selection strategies. To address the aforementioned problems, inspired by the Mixture-of-Expert (MOE) and knowledge distillation, we propose a novel Multiple Local Prompts Distillation (MLPD) method to inject the knowledge of multiple local prompts into a unique global prompt, improving both the generalization and discriminative ability. To ensure the diversity of local prompts, we split the whole dataset into several subsets to infer the discriminative local prompts for each subset, which is further applied to generate the generability global prompt. Formally, for each subset, Meta Prompt Tuning (MPT) is proposed to constrain each local prompt to capture both the domain-specific and domain-shared generalization knowledge on the basis of the domain label and meta-learning mechanism. After that, Prompt Knowledge Distillation (PKD) is proposed to distill the knowledge captured in the local-level prompts into the global-level prompt with prompt-level and feature-level knowledge distillations. The final evaluation on multiple benchmarks underscores the effectiveness of the proposed MLPD, e.g., achieving mAPs of 97.3%, 84.8%, 85.2%, 57.3%, and 60.7% on PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, respectively.},
  archive      = {J_TMM},
  author       = {Huaihai Lyu and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607719},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple local prompts distillation for domain generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuInNet: Dual-modality feature interaction for point cloud completion. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further promote the development of multimodal point cloud completion, we contribute a large-scale multimodal point cloud completion benchmark ModelNet-MPC with richer shape categories and more diverse test data, which contains nearly 400,000 pairs of high-quality point clouds and rendered images of 40 categories. Besides the fully supervised point cloud completion task, two additional tasks including denoising completion and zero-shot learning completion are proposed in ModelNet-MPC, to simulate real-world scenarios and verify the robustness to noise and the transfer ability across categories of current methods. Meanwhile, considering that existing multimodal completion pipelines usually adopt a unidirectional fusion mechanism and ignore the shape prior contained in the image modality, we propose a Dual-Modality Feature Interaction Network (DuInNet) in this paper. DuInNet iteratively interacts features between point clouds and images to learn both geometric and texture characteristics of shapes with the dual feature interactor. To adapt to specific tasks such as fully supervised, denoising, and zero-shot learning point cloud completions, an adaptive point generator is proposed to generate complete point clouds in blocks with different weights for these two modalities. Extensive experiments on the ShapeNet-ViPC and ModelNet-MPC benchmarks demonstrate that DuInNet exhibits superiority, robustness and transfer ability in all completion tasks over state-of-the-art methods. The code and dataset will be available at https://github.com/xinpuliu/DuInNet.},
  archive      = {J_TMM},
  author       = {Xinpu Liu and Baolin Hou and Hanyun Wang and Ke Xu and Jianwei Wan and Yulan Guo},
  doi          = {10.1109/TMM.2025.3607739},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DuInNet: Dual-modality feature interaction for point cloud completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module-a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: (i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and (ii) Attribute binding, ensuring that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models like Stable Diffusion and Gligen, markedly enhancing models' performance in addressing these key challenges. We assess our technique on the well-established CompBench and TIFA score benchmarks, and HRS dataset where B2B not only surpasses methods specialized in either attribute binding or layout guidance but also uniquely excels by integrating these capabilities to deliver enhanced overall performance.},
  archive      = {J_TMM},
  author       = {Ashkan Taghipour and Morteza Ghahremani and Mohammed Bennamoun and Aref Miri Rekavandi and Hamid Laga and Farid Boussaid},
  doi          = {10.1109/TMM.2025.3607759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Box it to bind it: Unified layout control and attribute binding in text-to-image diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in point cloud compression (PCC) is how to evaluate the perceived distortion so that the codec can be optimized for perceptual quality. Current standard practices in PCC highlight a primary issue: while single-feature metrics are widely used to assess compression distortion, the classic method of searching point-to-point nearest neighbors frequently fails to adequately build precise correspondences between point clouds, resulting in an ineffective capture of human perceptual features. To overcome the related limitations, we propose a novel assessment method called RBFIM, utilizing radial basis function (RBF) interpolation to convert discrete point features into a continuous feature function for the distorted point cloud. By substituting the geometry coordinates of the original point cloud into the feature function, we obtain the bijective sets of point features. This enables an establishment of precise corresponding features between distorted and original point clouds and significantly improves the accuracy of quality assessments. Moreover, this method avoids the complexity caused by bidirectional searches. Extensive experiments on multiple subjective quality datasets of compressed point clouds demonstrate that our RBFIM excels in addressing human perception tasks, thereby providing robust support for PCC optimization efforts.},
  archive      = {J_TMM},
  author       = {Zhang Chen and Shuai Wan and Siyu Ren and Fuzheng Yang and Mengting Yu and Junhui Hou},
  doi          = {10.1109/TMM.2025.3607782},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RBFIM: Perceptual quality assessment for compressed point clouds using radial basis function interpolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRFCNet: Feature refinement and flexible concatenation for object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art YOLO detection algorithms still suffer from the issue of redundant extraction of similar features during feature propagation, and the simplistic stacking approach of connecting different features limits the flexibility of feature fusion. We propose a new feature recombination mechanism involving refining feature extraction and flexible concatenation. It includes the HFConv (Hybrid Flexibility Convolution) module, the MFD (Multivariate Flexibility Downsampling) module, and the DFSPP (Deformable and Flexible Spatial Pyramid Pooling) module. Specifically, the HFConv module employs feature refinement and flexible connection strategies to optimize feature representation and reduce redundancy in a dynamic way, acquiring diverse feature information from local and surrounding regions. The MFD module leverages multiple downsampling methods to address the issue of feature redundancy that may arise from a single downsampling method, thereby enhancing feature diversity. The DFSPP module learns an offset corresponding to the pooling kernel size, allowing for the extraction of the most critical information in a dynamic manner. By incorporating these modules into the YOLO architecture, we develop a more robust network called FRFCNet, and the experimental results show a notable 4.1% and 2.8% improvement in AP values on the VOC2012 and COCO2017 datasets, respectively, compared to the baseline (YOLOV7-Tiny-SiLu), outperforming current one-stage detectors.},
  archive      = {J_TMM},
  author       = {Tao Zhang and Zhiheng Wu and Xiangjian He and Qiang Wu},
  doi          = {10.1109/TMM.2025.3607701},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FRFCNet: Feature refinement and flexible concatenation for object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrackletGait: A robust framework for gait recognition in the wild. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8% and 80.4% rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.},
  archive      = {J_TMM},
  author       = {Shaoxiong Zhang and Jinkai Zheng and Shangdong Zhu and Chenggang Yan},
  doi          = {10.1109/TMM.2025.3607705},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TrackletGait: A robust framework for gait recognition in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriPHiT: Privacy-preserving hierarchical training of deep neural networks. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training phase of deep neural networks requires substantial resources and as such is often performed on cloud servers. However, this raises privacy concerns when the training dataset contains sensitive content, e.g., facial or medical images. In this work, we propose a method to perform the training phase of a deep learning model on both an edge device and a cloud server that prevents sensitive content being transmitted to the cloud while retaining the desired information. The proposed privacy-preserving method uses adversarial early exits to suppress the sensitive content at the edge and transmits the task-relevant information to the cloud. This approach incorporates noise addition during the training phase to provide a differential privacy guarantee. We extensively test our method on different facial and medical datasets with diverse attributes using various deep learning architectures, showcasing its outstanding performance. We also demonstrate the effectiveness of privacy preservation through successful defenses against different white-box, deep and GAN-based reconstruction attacks. This approach is designed for resource-constrained edge devices, ensuring minimal memory usage and computational overhead.},
  archive      = {J_TMM},
  author       = {Yamin Sepehri and Pedram Pad and Pascal Frossard and L. Andrea Dunbar},
  doi          = {10.1109/TMM.2025.3607801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PriPHiT: Privacy-preserving hierarchical training of deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screenshot, which is a common tool in office work, has become a significant threat to organizations like companies and research institutions. Malicious users can easily leak sensitive information like business secrets and research data by taking a screenshot and spreading onto the Internet. While existing watermarking schemes serve as useful tools for leakage tracing, they fall short in the scenario of arbitrary screenshot. Most current methods are file-targeted, focusing on embedding watermark for a single file of one type at a time, making it hard to handle arbitrary content on screen. To address the issues above and better satisfy the need of the scenario, we propose ScreenGuard, a novel watermarking scheme targeted for the screen itself to protect arbitrary screen content shown on it. Unlike previous watermarking schemes, ScreenGuard does not modify the content itself. Instead, we generate a transparent mask template based on the watermark, tile it to the size of the screen to form a complete transparent mask, and overlay this mask onto the screen. This ensures that any screenshots taken will contain our watermark. We then train a locator and a decoder to extract watermarks from suspected leaked screenshots to trace leaks to their source. We summarized five properties that needs to be satisfied in the scenario of arbitrary screenshot (Generalizable, Unseeable, Adaptable, Robust, Dynamic) and evaluate our method on these criteria. Extensive experiments demonstrate that ScreenGuard meets these five properties effectively, showcasing its superiority and broad practical applications.},
  archive      = {J_TMM},
  author       = {Gaozhi Liu and Xiujian Liang and Xiaoxiao Hu and Yichao Si and Xinpeng Zhang and Zhenxing Qian},
  doi          = {10.1109/TMM.2025.3607779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ScreenGuard: A screen-targeted watermarking scheme against arbitrary screenshot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical semantic distillation framework for open-vocabulary object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, e.g., CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin.},
  archive      = {J_TMM},
  author       = {Shenghao Fu and Junkai Yan and Qize Yang and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2025.3607729},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hierarchical semantic distillation framework for open-vocabulary object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-attention transformers for class-incremental learning: A tale of two memories. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (Class-IL) aims to continuously learn a model from a sequence of tasks, which suffers from the issue of catastrophic forgetting. Recently, a few transformer based methods are proposed to address this issue by transferring self-attention into task-specific attention. However, these methods utilize shared task-specific attention modules across the whole incremental learning process, and are unable to achieve the balance between consolidation and plasticity, i.e., to remember the knowledge learned from previous tasks and absorb the knowledge from the current task simultaneously. Motivated by the mechanism of LSTM and hippocampus memory, we point out that dual attention on long and short-term memories can handle the consolidation-plasticity dilemma of Class-IL. Typically, we propose Dual-Attention Transformers (DAFormer) to learn external attention and internal attention. The former utilizes sample-dependent keys which exclusively focused on the new tasks, while the latter consolidates the knowledge from previous tasks by using sample-agnostic keys. We present two editions of DAFormer: DAFormer-S and DAFormer-M: the former utilizes shared external keys and maintains a small parameter size, while the latter utilizes multiple external keys and enhances the long-term memory. Furthermore, we propose the $K$-nearest neighbor invariant based distillation scheme, which distills knowledge from previous tasks to current task by maintaining the same neighborhood relationship of each sample over old and new models. Experimental results on CIFAR-100, ImageNet-subset and ImageNet-full demonstrate that DAFormer significantly outperforms all the state-of-the-art parameter-static and parameter-growing methods.},
  archive      = {J_TMM},
  author       = {Shaofan Wang and Weixing Wang and Yanfeng Sun and Zhiyong Wang and Boyue Wang and Baocai Yin},
  doi          = {10.1109/TMM.2025.3607800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-attention transformers for class-incremental learning: A tale of two memories},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-aware dynamic label assignment with high-quality proposals for oriented object detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented objects in images are typically embedded in complex backgrounds and exhibit arbitrary orientations. When using oriented bounding boxes (OBBs) to represent these objects, the periodicity of the angles and associated variations in side lengths lead to discontinuities in the angle loss. This paper fundamentally addresses this problem by proposing a trigonometric loss function in the complex plane. Moreover, a conformer RPN head is designed with convolution and multi-head self-attention, which can dynamically capture angular and classification information. The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals. A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for oriented proposal label assignment. This method makes negative sample selection more representative, ensuring consistency between classification and regression features. Experiments were conducted on five realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs. Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, 46.45%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, STAR, and HRSC2016 datasets, respectively.},
  archive      = {J_TMM},
  author       = {Mingkui Feng and Hancheng Yu and Xiaoyu Dang and Ming Zhou},
  doi          = {10.1109/TMM.2025.3607785},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-aware dynamic label assignment with high-quality proposals for oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively representing and transferring user preferences across various domains presents a significant challenge in cross-domain recommendation (CDR). Some approaches utilize graph neural networks that use interaction behavior to establish relationships between entities, providing a comprehensive understanding of user interests. However, the impact of consistent semantics across various types, fields, and perspectives of social media information on user preferences is overlooked, i.e. the multidimensional consistency of user preferences. This oversight results in graph node representations that inadequately reflect user preferences. To address these limitations, we propose a multi-layer transfer learning network (MTLG) for CDR based on graph node representation enhancement via multi-dimensional consistent user preferences. Firstly, the model introduces a set of globally shared semantic units to perform different-grained semantic alignment of multiple media information without clear alignment boundaries, thereby modeling multi-dimensional consistent user preference features. These features are then seamlessly integrated with the initial high-order graph structure embedding features, thus significantly improving the quality of graph node representation. Secondly, the model innovatively designs a multi-layer transfer learning network that hierarchically aligns the domain distribution differences. It calculates the similarity between domains to derive layer weights for more precise transfer learning, thereby mitigating the possibility of information error accumulation resulting from inaccurate feature aggregation processes. We conducted numerous experiments on 3 scenarios, including 7,954,943 rating information from the Amazon dataset. The results indicate that MTLG's recommendation accuracy surpasses those of state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xin Ni and Jie Nie and Niantai Jing and Jianliang Xu and Xiaodong Wang and Xuesong Gao and MingXing Jiang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3607706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-layer transfer learning for cross-domain recommendation based on graph node representation enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth map super-resolution via deep cross-modality and cross-scale guidance. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided depth super-resolution is essential in many applications, which enhances low-resolution (LR) depth maps using high-resolution (HR) RGB images from the same scene. However, the challenge lies in avoiding the texture-copy artifacts issue caused by structural inconsistencies between two modalities. To mitigate, we propose a cross-modality and cross-scale guided depth super-resolution network (D2CNet). We first design a novel two-stage feature integration module to effectively fuse multi-modal RGB and depth while minimizing texture-copy artifacts. That is, a cross-modality fusion stage transfers consistent structures from RGB to depth in a multi-scale manner, and a cross-scale refinement stage mitigates inconsistent structures across modalities. In addition, we design a convolution group as the basic module to well extract high-frequency features and an LR and HR domain projection strategy to enrich features between the fusion and refinement stages. We then develop a new network architecture by progressively repeating the feature integration module and the convolution group, which is flexibly controllable to strike a balance between accuracy and cost for easy implementation in real world. Extensive experiments on multiple benchmarks demonstrate that our D2CNet consistently achieves superior accuracy and generalization ability across sampling scales in both qualitative and quantitative evaluations, when compared to state-of-the-art baselines. The code is at https://github.com/suzdl/d2cnet},
  archive      = {J_TMM},
  author       = {Shuzhe Liu and Delong Suzhang and Meng Yang and Xinhu Zheng and Ce Zhu},
  doi          = {10.1109/TMM.2025.3607763},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Depth map super-resolution via deep cross-modality and cross-scale guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to identify novel compositions formed by known primitives (attributes and objects). Motivated by recent advancements in pre-trained vision-language models such as CLIP, many methods attempt to fine-tune CLIP for CZSL and achieve remarkable performance. However, the existing CLIP-based CZSL methods focus mainly on text prompt tuning, which lacks the flexibility to dynamically adapt both modalities. To solve this issue, an intuitive solution is to additionally introduce visual prompt tuning. This insight is not trivial to achieve because effectively learning prompts for CZSL involves the challenge of entanglement between visual primitives as well as appearance shifts in different compositions. In this paper, we propose a novel Synergetic Prompts as Disentanglement Queries (SPDQ) framework for CZSL. It can disentangle primitive features based on synergetic prompts to jointly alleviate these challenges. Specifically, we first design a low-rank primitive modulator to produce synergetic adaptive attribute and object prompts based on prior knowledge of each instance for model adaptation. Then, we additionally utilize text prefix prompts to construct synergetic prompt queries, which are used to resample corresponding visual features from local visual patches. Comprehensive experiments conducted on three benchmarks demonstrate that our SPDQ approach achieves state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Han Jiang and Xiaoshan Yang and Chaofan Chen and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3607726},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPDQ: Synergetic prompts as disentanglement queries for compositional zero-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning trimaps via clicks for image matting. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant advancements achieved in image matting, the existing models heavily depend on manually drawn trimaps to produce accurate results in natural image scenarios. However, the process of obtaining trimaps is time-consuming and lacks user-friendliness and device compatibility. This greatly limits the practical applicability of all trimap-based matting methods. To address this issue, we introduce Click2Trimap, an interactive model that is capable of predicting high-quality trimaps and alpha mattes with minimal user click inputs. By analyzing real users' behavioral logic and the characteristics of trimaps, we successfully propose a powerful iterative three-class training strategy and a dedicated simulation function, making Click2Trimap exhibit versatility across various scenarios. Compared with all existing trimap-free matting methods, Click2Trimap achieves superior performance in quantitative and qualitative assessments conducted on synthetic and real-world matting datasets. In particular, in a user study, Click2Trimap yields high-quality trimap and matting predictions in just 5 seconds per image on average, demonstrating its substantial practical value for use in real-world applications.},
  archive      = {J_TMM},
  author       = {Chenyi Zhang and Yihan Hu and Henghui Ding and Humphrey Shi and Yao Zhao and Yunchao Wei},
  doi          = {10.1109/TMM.2025.3607710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning trimaps via clicks for image matting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although screen-camera resilient watermarking addresses issues such as privacy leakage and copyright infringement in digital images to some extent during screen-camera communication. However, in screen-camera scenarios, uncontrolled shooting environments, various display devices, and different lens types introduce more complex noise into the watermarked images. Because some noise generated during the screen-camera process cannot be quantitatively analyzed, the integrity of the embedded watermark is compromised, making copyright verification and information acquisition still difficult. To solve this problem, we establish a large-scale screen-camera image dataset (SCISet) and propose a noise simulation network (NoS-Net). Specifically, we obtain 36,000 screen-camera images under various shooting environments with multiple types of screens and cameras. Then, we use SCISet to train the proposed NoS-Net based on the U-Net architecture, which can learn multi-level and complementary feature information of screen-camera images, enhancing its ability to simulate complex noise. Experimental results show that integrating the proposed NoS-Net into mainstream screen-camera resilient watermarking methods significantly improves their ability to resist screen-camera noise attacks. Furthermore, the diversity of SCISet plays an important role in advancing robust watermarking research.},
  archive      = {J_TMM},
  author       = {Daidou Guo and Chuan Qin and Fengyong Li and Heng Yao and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3607783},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving robustness of screen-camera resilient watermarking: A large-scale dataset and a noise simulation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3607708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD). However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture. To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and total Floating Point Operations (FLOPs). Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework. Simulations show that the proposed model is robust against noise and negative samples. Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest parameters and FLOPs. Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities.},
  archive      = {J_TMM},
  author       = {Ruituo Wu and Yang Chen and Jian Xiao and Bing Li and Jicong Fan and Frédéric Dufaux and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TMM.2025.3607708},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DA-flow: Dual attention normalizing flow for skeleton-based video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oversampling with GAN via meta-learning for imbalanced data. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3607712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing generative adversarial networks (GANs) for oversampling imbalanced data has demonstrated its effectiveness. However, many GAN-based oversampling methods are confronted with a significant challenge, namely, mode collapse, especially when dealing with tabular imbalanced data. In this paper, two unique penalty terms are respectively incorporated into the loss functions of the discriminator and the generator of GAN to promote the generated samples to exhibit not just statistical but also spatial information consistency with the minority samples, thereby alleviating the issue of mode collapse. In contrast to other studies that fix the coefficient of the penalty terms, the optimal coefficients of the penalty terms are adaptively searched using a meta-learning approach, where Bayesian optimization is firstly employed to effectively handle situations involving small size of minority samples in the imbalanced data. We call the proposed model as META_GAN. Experimental results demonstrate that META_GAN outperforms alternative oversampling methods on general tabular and image imbalanced datasets and long-tailed datasets in terms of different metrics.},
  archive      = {J_TMM},
  author       = {Yueqi Chen and Witold Pedrycz and Chao Zhang and Jian Wang and Jie Yang},
  doi          = {10.1109/TMM.2025.3607712},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Oversampling with GAN via meta-learning for imbalanced data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting long-tailed recognition with label descriptor and beyond. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3607812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Tailed Recognition (LTR) poses significant challenges due to the heavily imbalanced nature of real-world data, which severely skews data-driven deep neural networks. Despite the rapid progress of Vision-Language Models (VLMs), they still face challenges in effectively learning from long-tailed visual data. In this paper, we present a comprehensive analysis of the reasons behind the underperformance of VLMs and propose a hierarchical inference framework to address this issue. Specifically, we prompt the large language models to generate sentencelevel descriptors for class labels and conduct the open vocabulary classification by computing the average similarity between the image and each descriptor. A reweighting mechanism is further proposed to filter out uninformative descriptors. To mitigate model bias incurred by the long-tail distribution, we propose a feature adapter with the logit adjustment technique and finetune the CLIP model via visual prompt tokens. We introduce the Shared Feature space Mixup (SFM) to enhance the interaction between modalities to address tail visual feature insufficiency. Finally, we propose a hierarchical inference manner to combine the aforementioned proposals. Extensive evaluations demonstrate that our approach achieves state-of-the-art performance by finetuning only a few parameters on the Places-LT, ImageNet-LT, and iNaturalist 2018 benchmarks.},
  archive      = {J_TMM},
  author       = {Zhengzhuo Xu and Ruikang Liu and Zenghao Chai and Yiyan Qi and Lei Li and Haiqin Yang and Chun Yuan},
  doi          = {10.1109/TMM.2025.3607812},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting long-tailed recognition with label descriptor and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive action quality assessment through multi-branch modeling. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA) aims to evaluate and score human actions in videos accurately. Existing approaches involve extracting features from the input video and implementing regression based on those features. However, representations derived from a single branch often lack the necessary diversity and flexibility to capture the complexity of human actions effectively. This work addresses these limitations by introducing a multi-branch architecture designed to capture a broad spectrum of video dynamics at varying levels of granularity. Specifically, we enhance video representation in the flow-guided branch by integrating optical flow with video features. This combination of multimodal features offers a more comprehensive context of global motion. Meanwhile, the momentfocused branch is tailored to extract frame-specific features, constructing two distinct quality-based representations with different focuses on moments, which achieves adaptive clues aggregation. Furthermore, the detail-aware branch leverages multiscale deep embeddings from a hierarchy convolutional neural network to capture fine-grained spatial information, which is useful when objects have complex spatial changes. Finally, a post-fusion strategy is employed to merge outputs from all branches, contributing to the comprehensive action quality assessment. Experimental evaluations on three benchmark datasets, FineDiving, MTLAQA, and AQA-7, demonstrate the superiority of our model in providing reliable assessments of action quality.},
  archive      = {J_TMM},
  author       = {Siyuan Xu and Peilin Chen and Yue Liu and Meng Wang and Shiqi Wang and Hong Yan and Sam Kwong},
  doi          = {10.1109/TMM.2025.3607713},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comprehensive action quality assessment through multi-branch modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3604930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the optimization of quality of experience (QoE) through learning adaptive bitrate (ABR) algorithms from internet video streams. However, the complex nature of the real-world Internet, characterized by heavy-tailed behavior, diversity, and unpredictability, hinder the effective learning of off-the-shelf reinforcement learning (RL)-based ABR algorithms. As a result, existing methods inevitably fail to achieve optimal performance under various network conditions and user QoE objectives. We propose Fortuna, a novel offline meta RL ABR algorithm that can effectively learn from these heavy-tailed internet data features and become more practical. Fortuna is primarily divided into two phases. In the offline phase, Fortuna utilizes diverse offline data for learning to reduce the costly online RL interaction expense, while in the online phase, we gradually increase video streaming sessions complexity through curriculum learning to quickly adapt to specific network conditions. Fortuna then utilizes meta-learning to optimize ABR policies and enhance generalization. Additionally, to better learn network features, Fortuna further optimizes QoE by learning low-level TCP congestion control information. Experimental results from trace-driven and real-world scenarios demonstrate that Fortuna enhances learning efficiency by more than 7.5%-4 ×, reduces stall time by 4.6%-14.2%, and generalizes to different network conditions and video streams.},
  archive      = {J_TMM},
  author       = {Ling Yi and Yongbin Qin and Ruizhang Huang},
  doi          = {10.1109/TMM.2025.3604930},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing adaptive video streaming: Offline reinforcement learning and meta-learning in diverse networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal transformer for cross-modal long document classification. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3608295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.},
  archive      = {J_TMM},
  author       = {Tengfei Liu and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1109/TMM.2025.3608295},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-modal transformer for cross-modal long document classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USTC-TD: A test dataset and benchmark for image and video coding in 2020s. <em>TMM</em>, 1-16. (<a href='https://doi.org/10.1109/TMM.2025.3608643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image/video coding has been a remarkable research area for both academia and industry for many years. Testing datasets, especially high-quality image/video datasets, are desirable for the justified evaluation of coding-related research, practical applications, and standardization activities. We put forward a test dataset, namely USTC-TD, which has been successfully adopted in the practical end-to-end image/video coding challenge of IEEE International Conference on Visual Communications and Image Processing (VCIP) in 2022 and 2023. USTC-TD contains 40 images at 4K spatial resolution and 10 video sequences at 1080p spatial resolution, featuring various content due to the diverse environmental factors (e.g., scene type, texture, motion, view) and the designed imaging factors (e.g., illumination, lens, shadow). We quantitatively evaluate USTC-TD on different image/video features (spatial, temporal, color, lightness), and compare it with the previous image/video test datasets, which verifies its excellent compensation for the shortcomings of existing datasets. We also evaluate both classic standardized and recently learned image/video coding schemes on USTC-TD using objective quality metrics (PSNR, MS-SSIM, VMAF) and subjective quality metric (MOS), providing an extensive benchmark for these evaluated schemes. Based on the characteristics and specific design of the proposed test dataset, we analyze the benchmark performance and shed light on the future research and development of image/video coding. All the data are released online: https://esakak.github.io/USTC-TD.},
  archive      = {J_TMM},
  author       = {Zhuoyuan Li and Junqi Liao and Chuanbo Tang and Haotian Zhang and Yuqi Li and Yifan Bian and Xihua Sheng and Xinmin Feng and Yao Li and Changsheng Gao and Li Li and Dong Liu and Feng Wu},
  doi          = {10.1109/TMM.2025.3608643},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {USTC-TD: A test dataset and benchmark for image and video coding in 2020s},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JPEG AI compressed domain face detection: A multi-scale bridging perspective. <em>TMM</em>, 1-10. (<a href='https://doi.org/10.1109/TMM.2025.3609179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image coding is showing improved compression efficiency, while also offering a novel advantage in enabling computer vision tasks directly within the compressed domain. The latent representation created by deep learning methods inherently contains all visual features, without a computationally expensive synthesis process at the decoder. This paper is an invited extension of a previous solution for JPEG AI compressed domain face detection that adapts a RetinaFace-based detector to operate directly on the latent tensor. In addition to a former single-scale bridging solution, this work provides a novel multi-scale bridging architecture to enable a more effective multi-scale compressed domain face detection. The results show a significant performance gain, improving accuracy up to 20% for detection of tiny faces on the WIDER FACE dataset compared to single-scale bridging, and further narrowing the gap when compared to detection on uncompressed or JPEG AI decoded images. Furthermore, since the computationally expensive decoding step is bypassed and since the bridges consist of lower-complexity networks, the overall processing cost is significantly reduced. Single and multi-scale bridging, respectively, have about 10% and 32% the complexity of applying pixel domain face detection on decoded images. The proposed architecture is expected to be extended to other multiscale sensitive vision tasks, as JPEG AI is not specifically designed for any single downstream application.},
  archive      = {J_TMM},
  author       = {Ayman Alkhateeb and Alessandro Gnutti and Fabrizio Guerrini and Riccardo Leonardi and João Ascenso and Fernando Pereira},
  doi          = {10.1109/TMM.2025.3609179},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {JPEG AI compressed domain face detection: A multi-scale bridging perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-led prompting learning caption generating model and benchmark. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) is an important intelligent system application, but most current research views it as a coarse binary classification task that lacks a fine-grained understanding of abnormal video sequences. We explore a new task for video anomaly analysis called Comprehensive Video Anomaly Caption (CVAC), which aims to generate comprehensive textual captions (containing scene information such as time, location, anomalous subject, anomalous behavior, etc.) for surveillance videos. CVAC is more consistent with human understanding than VAD, but it has not been well explored. We constructed a large-scale benchmark CVACBench to lead this research. For each video clip, we provide 6 fine-grained annotations, including scene information and abnormal keywords. A new evaluation metric Abnormal-F1 (A-F1) is also proposed to more accurately evaluate the caption generation performance of the model. We also designed a method called Anomaly-Led Generating Prompting Transformer (AGPFormer) as a baseline. In AGPFormer, we introduce an anomaly-led language modeling mechanism (Anomaly-Led MLM, AMLM) to focus on anomalous events in videos. To achieve more efficient cross-modal semantic understanding, we design the Interactive Generating Prompting (IGP) module and Scene Alignment Prompting (SAP) module to explore the divide between video and text modalities from multiple perspectives, and to improve the model's performance in understanding and reasoning about the complex semantics of videos. We conducted experiments on CVACBench by using traditional caption metrics and the proposed metrics, and the experimental results demonstrate the effectiveness of AGPFormer in the field of anomaly caption.},
  archive      = {J_TMM},
  author       = {Qianyue Bao and Fang Liu and Licheng Jiao and Yang Liu and Shuo Li and Lingling Li and Xu Liu and Xinyi Wang and Baoliang Chen},
  doi          = {10.1109/TMM.2025.3607837},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anomaly-led prompting learning caption generating model and benchmark},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3608947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised text-based person re-identification (Text-ReID) confronts the challenge of matching target person images with textual descriptions, hindered by the absence of identity annotations during training. Traditional approaches, which rely solely on global features, overlook the rich, fine-grained information within both text and image modalities. Besides, merely aligning features at the semantic level is insufficient due to the significant differences in feature representation spaces between the two modalities. Existing methods also neglect the information inequality caused by person-irrelevant factors in images. In this paper, we introduce a novel framework called Attribute-Centric Cross-modal Alignment (ACCA), specifically designed to overcome these issues. Our approach concentrates on two main aspects: visual-text attribute alignment and prediction distribution alignment. To effectively capture fine-grained information without identity labels, we implement a visual-text attribute alignment method based on momentum contrastive learning to synchronize visual and textual attribute features within a unified embedding space. We also propose a unique strategy for negative sample filtering and enrichment, creating robust and comprehensive negative attribute sample spaces to support the attribute alignment. Additionally, we establish two methods of label-free prediction distribution alignment to encourage the learning of invariant feature representations across modalities. The first method, bias-reduction distribution alignment, aligns features and predictions within each text-image pair by utilizing semantic information from the text and reduces the impact of person-irrelevant factors in images. The second method, global-attribute distribution alignment, enhances the interaction between global and local prediction distributions across visual and textual modalities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets validate our superior performances across all standard benchmarks.},
  archive      = {J_TMM},
  author       = {Jiajia Xu and Weiwei Cai and Xuemiao Xu and Yi Xie and Huaidong Zhang and Shengfeng He},
  doi          = {10.1109/TMM.2025.3608947},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-centric cross-modal alignment for weakly supervised text-based person re-ID},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSG: Multi-scale speaker graph network for active speaker detection. <em>TMM</em>, 1-12. (<a href='https://doi.org/10.1109/TMM.2025.3608949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The active speaker detection task is to determine whether a person is speaking or not across a series of video frames. Existing methods heavily rely on facial information within the annotated face bounding boxes for cross-modal learning with audio. This leads to a substantial decline in detection performance when facial cues are unclear, such as in cases of face occlusion or low-resolution facial appearances. In this paper, we extend the perception scale using only face bounding box annotations to model both facial and gestural cues, addressing the over-reliance on facial cues in active speaker detection. We propose a novel graph neural network that models inter-speaker interactions and integrates various cues from individual speakers. The final detection results are obtained through a binary graph node classification task. Our method achieves state-of-the-art performance on the AVA-ActiveSpeaker dataset (mAP: 95.6%) and the ASW dataset (mAP: 99.4%), with a model size only 21% that of the second-best method. Additionally, when facial cues are of poor quality, our method demonstrates a significant performance advantage over existing approaches. The code and model weights will be available at https://github.com/sdqdlgj/MSSG.},
  archive      = {J_TMM},
  author       = {Guanjun Li and Jiangyan Yi and Zhengqi Wen and Ruibo Fu and Yuwang Wang and Jianhua Tao},
  doi          = {10.1109/TMM.2025.3608949},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSSG: Multi-scale speaker graph network for active speaker detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning orthogonal latent representations for multi-view clustering. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view clustering, latent representations are often employed to address the challenge posed by low-quality data. Traditional approaches typically assume that multiple views are fully dependent, directly learning a common latent representation from the observed data. However, this assumption is overly restrictive in real-world scenarios and may overlook valuable information, as the independence of different views can reveal critical view-specific characteristics. To overcome this limitation, we propose learning Orthogonal Latent Representations for Multi-View Clustering (OLR-MVC), which jointly captures both cross-view dependence and independence. Specifically, our model maps multi-view data into shared and private latent spaces using distinct projection bases. To accurately capture both dependence and independence, we enforce orthogonality between the shared and private latent representations while also encouraging pairwise orthogonality among private representations. Furthermore, we leverage the self-expressive property of these latent representations to capture global data structures. Extensive experimental evaluations demonstrate that OLR-MVC outperforms state-of-the-art multi-view clustering methods.},
  archive      = {J_TMM},
  author       = {Xiaolin Xiao and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TMM.2025.3607704},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning orthogonal latent representations for multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dehazing approach: Recovery of color and polarization information using polarized characteristics. <em>TMM</em>, 1-15. (<a href='https://doi.org/10.1109/TMM.2025.3607694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarization provides valuable physical information, making it beneficial for various computer vision tasks. However, haze reduces both the color and polarization information of a scene. While existing single-image dehazing methods can restore color information, they are poor at recovering polarization information. Furthermore, current polarization-based dehazing approaches neglect the physical mechanisms of polarization degradation, resulting in inaccurate reconstruction of polarization information. In this paper, we propose a novel polarization dehazing algorithm, along with a polarization degradation model, to accurately recover both polarization and color information. First, we combine two key characteristics (the polarization achromatism prior and polarization attenuation prior) with the polarization degradation model to precisely reconstruct the scene's polarization. Then, we utilize the reconstructed polarization information to recover the color information of the scene. Finally, a multi-scale fusion optimization framework is introduced to further enhance the image quality. Our method shows excellent performance on both real-world indoor and outdoor polarized images, outperforming existing dehazing algorithms in both objective evaluation metrics and subjective visual assessment.},
  archive      = {J_TMM},
  author       = {Zhenshuo Yang and Chunhui Hao and Yang Lu and Yiming Su and Yukuan Zhang and Junchao Zhang and Jiandong Tian},
  doi          = {10.1109/TMM.2025.3607694},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel dehazing approach: Recovery of color and polarization information using polarized characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained domain generalization with feature structuralization. <em>TMM</em>, 1-14. (<a href='https://doi.org/10.1109/TMM.2025.3607716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained domain generalization (FGDG) is a more challenging task than traditional DG tasks due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the vulnerability of subtle features leads to a severe deterioration in model performance. Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning the commonality and specificity within categories. Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is accomplished through joint optimization of five constraints: a decorrelation function applied to disentangled segments, three constraints ensuring common feature consistency and specific feature distinctiveness, and a prediction calibration term. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.2% in FGDG performance. Beyond that, the explainability analysis on explicit concept matching intensity between the shared concepts among categories and the model channels, along with experiments on various mainstream model architectures, substantiates the validity of FS.},
  archive      = {J_TMM},
  author       = {Wenlong Yu and Dongyue Chen and Qilong Wang and Qinghua Hu},
  doi          = {10.1109/TMM.2025.3607716},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained domain generalization with feature structuralization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detailed object description with controllable dimensions. <em>TMM</em>, 1-13. (<a href='https://doi.org/10.1109/TMM.2025.3607747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object description plays an important role for visually impaired individuals to understand and compare the differences between objects. Recent multimodal large language models (MLLMs) exhibit powerful perceptual abilities and demonstrate impressive potential for generating object-centric descriptions. However, the descriptions generated by such models may still usually contain a lot of content that is not relevant to the user intent or miss some important object dimension details. Under special scenarios, users may only need the details of certain dimensions of an object. In this paper, we propose a training-free object description refinement pipeline, Dimension Tailor, designed to enhance user-specified details in object descriptions. This pipeline includes three steps: dimension extracting, erasing, and supplementing, which decompose the description into user-specified dimensions. Dimension Tailor can not only improve the quality of object details but also offer flexibility in including or excluding specific dimensions based on user preferences. We conducted extensive experiments to demonstrate the effectiveness of Dimension Tailor on controllable object descriptions. Notably, the proposed pipeline can consistently improve the performance of the recent MLLMs. The code is currently accessible at https://github.com/PRIS-CV/ControllableObjectDescription.},
  archive      = {J_TMM},
  author       = {Xinran Wang and Haiwen Zhang and Baoteng Li and Kongming Liang and Hao Sun and Zhongjiang He and Zhanyu Ma and Jun Guo},
  doi          = {10.1109/TMM.2025.3607747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detailed object description with controllable dimensions},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
