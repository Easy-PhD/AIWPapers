<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc">TC - 30</h2>
<ul>
<li><details>
<summary>
(2025). An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wearable electronic technology has facilitated the integration of smart wearable devices into artificial intelligence (AI)-driven medical assisted diagnosis. Embedded multi-core processors (MPs) have gradually emerged as pivotal hardware components for smart wearable medical diagnostic devices due to their high performance and flexibility. However, embedded MPs face the challenge of balancing performance, power consumption, and load-balancing. In response, we introduce a Pareto-based iterated local search (PILS) algorithm for task scheduling, which systematically optimizes multiple objectives, alongside a task list model to reduce the dimension of the decision space and enhance scheduling performance. In addition, we present a two-stage discretization scheme to ensure that the proposed algorithm offers meaningful guidance throughout the scheduling process. Simulation and on-board testing results show that the proposed algorithm effectively optimizes energy consumption, task execution time, and load balancing in embedded MPs task scheduling, indicating the potential of the proposed algorithm in enhancing the performance of smart wearable medical diagnostic devices powered by embedded MPs.},
  archive      = {J_TC},
  author       = {Qinglin Zhao and Lixin Zhang and Qi Pan and Kunbo Cui and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TC.2025.3603699},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetKG: Synthesizing interpretable network router configurations with knowledge graph. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced router configuration synthesizers aim to prevent network outages by automatically synthesizing configurations that implement routing protocols. However, the lack of interpretability makes operators uncertain about how low-level configurations are synthesized and whether the automatically generated configurations correctly align with routing intents. This limitation restricts the practical deployment of synthesizers. In this paper, we present NetKG, an interpretable configuration synthesis tool.(i) NetKG leverages a knowledge graph as the intermediate representation for configurations, reformulating the configuration synthesis problem as a configuration knowledge completion task; (ii) NetKG regards network intents as query tasks that need to be satisfied in the current configuration space, achieving this through knowledge reasoning and completion; (iii) NetKG explains the synthesis process and the consistency between configuration and intent through the configuration knowledge involved in reasoning and completion. We show that NetKG can scale to realistic networks and automatically synthesize intent-compliant configurations for static routes, OSPF, and BGP. It can explain the consistency between configuration and intent at different granularities through a visual interface. Experimental results indicate that NetKG synthesizes configurations in 2 minutes for a network with up to 197 routers, which is 7.37x faster than the SMT-based synthesizer.},
  archive      = {J_TC},
  author       = {Zhenbei Guo and Fuliang Li and Peng Zhang and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603712},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetKG: Synthesizing interpretable network router configurations with knowledge graph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multi-attribute data. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reverse k-nearest neighbor (RkNN) query schemes typically assume that users are available online in real-time for interactive key reception, overlooking scenarios where users might be offline. Moreover, existing privacy-preserving RkNN query schemes primarily focus on user features or spatial data, neglecting the significance of user reputation values. To address these limitations, we propose a privacy-preserving resilient RkNN query scheme over encrypted outsourced multi-attribute data (PRRQ). Specifically, to mitigate the challenges posed by resilient online presence (i.e., non-real-time online) of users for interactive key reception, we incorporate a non-interactive key exchange (NIKE) protocol and the Diffie-Hellman two-party key exchange algorithm to propose a multi-party NIKE algorithm (2K-NIKE), facilitating non-interactive key reception for multiple users. Considering the privacy leakage issues, PRRQ encodes original multi-attribute data (i.e., spatial, feature, and reputation values) alongside query requests based on formalized criteria. Additionally, we integrate the proposed 2K-NIKE and the improved symmetric homomorphic encryption (iSHE) algorithms to encrypt them. Furthermore, catering to the requirements of ciphertext-based RkNN queries, we propose a private RkNN query eligibility-checking (PREC) algorithm and a private reputation-verifying (PRRV) algorithm, which validate the compliance of encrypted outsourced multi-attribute data with query requests. Security analysis demonstrates that PRRQ achieves simulation-based security under an honest-but-curious model. Experimental results show that PRRQ offers superior computational efficiency compared to comparative schemes.},
  archive      = {J_TC},
  author       = {Jing Wang and Haiyong Bao and Na Ruan and Qinglei Kong and Cheng Huang and Hong-Ning Dai},
  doi          = {10.1109/TC.2025.3603688},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multi-attribute data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of Large Language Models (LLMs) requires dependable operation in the presence of errors in the hardware (caused by for example radiation) as this has become a pressing concern. At the same time, the scale and complexity of LLMs limit the overhead that can be added to detect errors. Therefore, there is a need for low-cost error detection schemes. Concurrent Error Detection (CED) uses the properties of a system to detect errors, so it is an appealing approach. In this paper, we present a new methodology and scheme for error detection in LLMs: Concurrent Linguistic Error Detection (CLED). Its main principle is that the output of LLMs should be valid and generate coherent text; therefore, when the text is not valid or differs significantly from the normal text, it is likely that there is an error. Hence, errors can potentially be detected by checking the linguistic features of the text generated by LLMs. This has the following main advantages: 1) low overhead as the checks are simple and 2) general applicability, so regardless of the LLM implementation details because the text correctness is not related to the LLM algorithms or implementations. The proposed CLED has been evaluated on two LLMs: T5 and OPUS-MT. The results show that with a 1% overhead, CLED can detect more than 87% of the errors, making it suitable to improve LLM dependability at low cost.},
  archive      = {J_TC},
  author       = {Jinhua Zhu and Javier Conde and Zhen Gao and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2025.3603682},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FSA-hash: Flow-size-aware sketch hashing for software switches. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data centers and enterprise networks, software switches have become critical components for achieving flexible and efficient network management. Due to resource constraints in software switches, sketches have emerged as a promising approach for network traffic measurement. However, their accuracy is often impacted by hash collisions. Existing hash functions treat all collisions equally, failing to account for the differing impacts of collisions involving elephant flows versus mouse flows. We propose FSA-Hash, a novel flow-size-aware hashing scheme that separates elephant flows from each other and from mouse flows, minimizing the most detrimental collisions. FSA-Hash is designed based on two insights: separating elephant flows from mouse flows avoids overestimating mouse flows, while separating elephant flows from each other enables accurate heavy-hitter detection. We implement FSA-Hash using machine learning models trained on network traffic data (LFSA-Hash), and also design a lightweight online variant (OLFSA-Hash) that learns the hash model solely from sketch queries on the software switch, obviating traffic collection overheads. Evaluations across four sketches and two tasks demonstrate FSA-Hash’s superior accuracy over standard hash functions. Moreover, OLFSA-Hash closely matches LFSA-Hash’s performance, making it an attractive option for adaptively refining the hash model without monitoring traffic.},
  archive      = {J_TC},
  author       = {Fuliang Li and Kejun Guo and Yiming Lv and Jiaxing Shen and Yuting Liu and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603716},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FSA-hash: Flow-size-aware sketch hashing for software switches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared L1-memory clusters of streamlined instruction processors (processing elements - PEs) are commonly used as building blocks in modern, massively parallel computing architectures (e.g. GP-GPUs). Scaling out these architectures by increasing the number of clusters incurs computational and power overhead, caused by the requirement to split and merge large data structures in chunks and move chunks across memory hierarchies via the high-latency global interconnect. Scaling up the cluster reduces buffering, copy, and synchronization overheads. However, the complexity of a fully connected cores-to-L1-memory crossbar grows quadratically with PE-count, posing a major physical implementation challenge. We present TeraPool, a physically implementable, >1000 floating-point-capable RISC-V PEs scaled-up cluster design, sharing a Multi-MegaByte >4000-banked L1 memory via a low latency hierarchical interconnect (1-7/9/11 cycles, depending on target frequency). Implemented in 12nm FinFET technology, TeraPool achieves near-gigahertz frequencies (910MHz) typical, 0.80V/25 °C. The energy-efficient hierarchical PE-to-L1-memory interconnect consumes only 9-13.5 pJ for memory bank accesses, just 0.74-1.1× the cost of a FP32 FMA. A high bandwidth main memory link is designed to manage data transfers in/out of the shared L1, sustaining transfers at the full bandwidth of an HBM2E main memory. At 910MHz, the cluster delivers up to 1.89 single precision TFLOP/s peak performance and up to 200GFLOP/s/W energy efficiency (at a high IPC/PE of 0.8 on average) in benchmark kernels, demonstrating the feasibility of scaling a shared-L1 cluster to a thousand PEs, four times the PE count of the largest clusters reported in literature.},
  archive      = {J_TC},
  author       = {Yichao Zhang and Marco Bertuletti and Chi Zhang and Samuel Riedel and Diyou Shen and Bowen Wang and Alessandro Vanelli Coralli and Luca Benini},
  doi          = {10.1109/TC.2025.3603692},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load balancing scheduling for batch-ordered job-store: Online vs. offline. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3603725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization is crucial in real-world applications, especially for balancing loads across machines handling specific job types. This paper introduces a novel batch-ordered job-store scheduling model, where jobs in a batch are scheduled sequentially, with their operations allocated in a round-robin fashion across two scenarios. We establish that this problem is NP-hard and analyze it in both online and offline settings. In the online case, we first examine the exclusive scenario, where operations within the same job must be scheduled on different machines, and show that a load greedy (LG) algorithm achieves a tight competitive ratio of $2 - \frac{1}{m}$, with m representing the number of machines. Next, we consider the circular scenario, which requires maintaining the circular order of operations across ordered machines. In this context, we analyze potential anomalies in load distribution during local optimality achieved by the ordered load greedy (OLG) algorithm and provide bounds on the occurrence of these anomalies and the maximum load in each local scheduling round. In the offline case, we abstract each OLG scheduling process as a generalized circular sequence alignment (CSA) problem and develop a dynamic programming-based matching (DPM) algorithm to solve it. To further enhance load balancing, we develop a dynamic programming-based optimization (DPO) algorithm to schedule multiple jobs simultaneously in both scenarios. Experimental results confirm the efficiency of DPM for the CSA problem, and we validate the load balancing effectiveness of both online and offline algorithms using real traffic datasets. These theoretical findings and algorithmic implementations lay a solid groundwork for future practical advancements.},
  archive      = {J_TC},
  author       = {Mengbing Zhou and Yang Wang and Bocong Zhao and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3603725},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Load balancing scheduling for batch-ordered job-store: Online vs. offline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BaDFL: Mitigating model poisoning in decentralized federated learning. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3603683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized federated learning (DFL) has gained significant attention due to its ability to facilitate collaborative model training without relying on a central server. However, it is highly vulnerable to backdoor attacks, where malicious participants can manipulate model updates to embed hidden functionalities. In this paper, we propose BaDFL, a novel Backdoor Attack defense mechanism for Decentralized Federated Learning. BaDFL enhances robustness by applying strategic model clipping at the local update level. To the best of our knowledge, BaDFL is the first decentralized federated learning algorithm with theoretical guarantees against model poisoning attacks. Specifically, BaDFL achieves an asymptotically optimal convergence rate of $O(\frac{1}{\sqrt{nT}})$, where n is the number of nodes and T is the global maximum iteration number. Furthermore, we provide a comprehensive analysis under two different attack scenarios, showing that BaDFL maintains robustness within a specific defense radius. Extensive experimental results show that, on average, BaDFL can effectively defend against model poisoning within 6 mitigation rounds, with less than a 1% drop in accuracy.},
  archive      = {J_TC},
  author       = {Yuan Yuan and Anhao Zhou and Xiao Zhang and Yifei Zou and Yangguang Shi and Dongxiao Yu},
  doi          = {10.1109/TC.2025.3603683},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BaDFL: Mitigating model poisoning in decentralized federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3604487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing systems struggle to efficiently manage multiple concurrent deep neural network (DNN) workloads while meeting strict latency requirements, minimizing power consumption, and maintaining environmental sustainability. This paper introduces Ecomap, a sustainability-driven framework that dynamically adjusts the maximum power threshold of edge devices based on real-time carbon intensity. Ecomap incorporates the innovative use of mixed-quality models, allowing it to dynamically replace computationally heavy DNNs with lighter alternatives when latency constraints are violated, ensuring service responsiveness with minimal accuracy loss. Additionally, it employs a transformer-based estimator to guide efficient workload mappings. Experimental results using NVIDIA Jetson AGX Xavier demonstrate that Ecomap reduces carbon emissions by an average of 30% and achieves a 25% lower carbon delay product (CDP) compared to state-of-the-art methods, while maintaining comparable or better latency and power efficiency.},
  archive      = {J_TC},
  author       = {Varatheepan Paramanayakam and Andreas Karatzas and Dimitrios Stamoulis and Iraklis Anagnostopoulos},
  doi          = {10.1109/TC.2025.3604487},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning over geo-distributed clouds enables joint training of data located in different regions, alleviating the burden of transferring large volumes of training datasets, which greatly saves bandwidth. However, the limited capacity of WAN links slows down the inter-cloud communications, which significantly decelerates the synchronization of distributed machine learning over geo-distributed clouds. Besides, the multi-tenancy in clouds results in multiple training tasks running simultaneously, whose synchronizations consistently compete for the limited WAN bandwidth with each other, which further aggravates the training performance of each task. While existing works optimize synchronizations through techniques like gradient compression, multi-resource interleaving and so on, none of them targets at the synchronization congestion especially due to multi-tenant learning, which results in inferior training performance. To solve these problems, we propose a simple but effective scheme, SCC, for fast and efficient multi-tenant learning via synchronization congestion control. SCC monitors the cross-cloud network conditions and evaluates the synchronization congestion level based on the round-trip transmission time for each synchronization. Then SCC alleviates synchronization congestion via controlling the synchronization frequency according to the synchronization congestion level in a probabilistic way. Extensive experiments are conducted within our testbeds consisted of 16 NVIDIA V100 GPUs to evaluate the performance of SCC, and comparison results show that SCC can reduce the average training completion time and makespan by up to 28.6% and 43.2% over SAP-SGD [1]. Targeted experiments are conducted to demonstrate the effectiveness and robustness of SCC.},
  archive      = {J_TC},
  author       = {Chengxi Gao and Fuliang Li and Kejiang Ye and Yang Wang and Pengfei Wang and Xingwei Wang and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3604486},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-aware optimization of task offloading for UAV-assisted edge computing. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3604463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAV) are widely used for edge computing in poor infrastructure scenarios due to their deployment flexibility and mobility. In UAV-assisted edge computing systems, multiple UAVs can cooperate with the cloud to provide superior computing capability for diverse innovative services. However, many service-related computational tasks may fail due to the unreliability of UAVs and wireless transmission channels. Diverse solutions were proposed, but most of them employ timedriven strategies which introduce unwanted decision waiting delays. To address this problem, this paper focuses on a taskdriven reliability-aware cooperative offloading problem in UAV-assisted edge-enhanced networks. The issue is formulated as an optimization problem which jointly optimizes UAV trajectories, offloading decisions, and transmission power, aiming to maximize the long-term average task success rate. Considering the discrete-continuous hybrid action space of the problem, a dependenceaware latent-space representation algorithm is proposed to represent discrete-continuous hybrid actions. Furthermore, we design a novel deep reinforcement learning scheme by combining the representation algorithm and a twin delayed deep deterministic policy gradient algorithm. We compared our proposed algorithm with four alternative solutions via simulations and a realistic Kubernetes testbed-based setup. The test results show how our scheme outperforms the other methods, ensuring significant improvements in terms of task success rate.},
  archive      = {J_TC},
  author       = {Hao Hao and Changqiao Xu and Wei Zhang and Xingyan Chen and Shujie Yang and Gabriel-Miro Muntean},
  doi          = {10.1109/TC.2025.3604463},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliability-aware optimization of task offloading for UAV-assisted edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-storage verifiable data streaming with efficient revocation approach. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3604531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) is proposed to authenticate a sequence of ordered data, such that the misbehavior on the data returned by cloud server can be effectively detected. VDS also allows to efficiently replace the outsourced data by another value. However, the old authentication information can make the expired data pass the verification. To prevent this attack, VDS schemes must provide a revocation approach to revoke the old authentication information. The current approach employs the tree-like authentication structure or cryptographic accumulator, which will influence the efficiency of the VDS scheme. In this work, we find an approach to construct the low-storage VDS scheme supporting efficient revocation. Towards this end, we fully exploit the property of chameleon hash function with ephemeral trapdoor to propose a signature, which is the crucial step to construct the VDS scheme. In our VDS scheme, the size of the authentication information can be reduced to be less than the scale of the data streaming (i.e., low storage). Furthermore, the client is able to revoke the old authentication information in an efficient manner, where she only needs to release a message (i.e., efficient revocation). The performance evaluation shows that the proposed VDS scheme is efficient and practical.},
  archive      = {J_TC},
  author       = {Haining Yang and Dengguo Feng and Jing Qin},
  doi          = {10.1109/TC.2025.3604531},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-storage verifiable data streaming with efficient revocation approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoFormer: Collaborating with heterogeneous edge devices for scalable transformer inference. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1× inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3%. CoFormer can also reduce energy consumption by approximately 40% while maintaining satisfactory inference performance.},
  archive      = {J_TC},
  author       = {Guanyu Xu and Zhiwei Hao and Li Shen and Yong Luo and Fuhui Sun and Xiaoyan Wang and Han Hu and Yonggang Wen},
  doi          = {10.1109/TC.2025.3604473},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CoFormer: Collaborating with heterogeneous edge devices for scalable transformer inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain enhances trust and collaboration among entities through its inherent features of transparency, immutability, and traceability, leading to its extensive integration into Multi-access Edge Computing (MEC). However, existing transaction propagation mechanisms require MEC devices to consume significant computing resources for complex transaction verification, increasing their vulnerability to malicious attacks. Adversaries can exploit this by flooding the blockchain network with spam transactions, aiming to deplete device energy and disrupt system performance. To cope with these issues, this paper proposes a reputation-based energy-efficient transaction propagation mechanism that alleviates spam transaction attacks while reducing computing resources and energy consumption. Firstly, we design a subjective logic-based reputation scheme that assesses node trust by integrating local and recommended opinions and incorporates opinion acceptance to counteract false evidence. Then, we optimize the transaction verification method by adjusting transaction discard and verification probabilities based on the proposed reputation scheme to curb the propagation of spam transactions and reduce verification consumption. Finally, we enhance the transaction transmission strategy by prioritizing nodes with higher reputations, enhancing both resilience to spam transactions and transmission reliability. A series of simulations demonstrate the effectiveness of the proposed mechanism.},
  archive      = {J_TC},
  author       = {Xijia Lu and Qiang He and Xingwei Wang and Jaime Lloret and Peichen Li and Ying Qian and Min Huang},
  doi          = {10.1109/TC.2025.3604480},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient conjunctive geometric range query over encrypted spatial data with learned index. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of geo-positioning technologies and mobile Internet, spatial data query services have attracted extensive attention. To protect the confidentiality of sensitive information outsourced to cloud servers, much efforts have been devoted to designing geometric range query schemes over encrypted spatial data without affecting availability. However, existing works focus on the privacy-preserving schemes with traditional tree indexes, causing more computing and storage issues. In this paper, we propose an efficient conjunctive geometric range query scheme over encrypted spatial data with a learned index. In particular, we design a new privacy-preserving learned index for spatial data to reduce the search space and storage overhead. The main idea is to add noise disturbance to the objective function instead of directly adding it to output results, reducing the leakage of private information and ensuring the correctness of output results. Moreover, we propose a spatial segmentation algorithm to avoid accessing a large number of unnecessary Z codes in the query process. The formal security analysis shows that our scheme ensures index data security and query privacy. Simulation results show that the query efficiency is improved while the storage overhead is significantly reduced compared with the state-of-the-art schemes.},
  archive      = {J_TC},
  author       = {Mingyue Li and Chunfu Jia and Ruizhong Du and Guanxiong Ha},
  doi          = {10.1109/TC.2025.3604470},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient conjunctive geometric range query over encrypted spatial data with learned index},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sketching for heavy item-oriented data stream mining with memory constraints. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and fast data stream mining is critical to many tasks, including real-time series analysis for mobile sensor data, big data management and machine learning. Various heavy-oriented item detection tasks, such as identifying heavy hitters, heavy changers, persistent items, and significant items, have garnered considerable attention from both industry and academia. Unfortunately, as data stream speeds continue to increase and the available memory, particularly in L1 cache, remains limited for real-time processing, existing schemes face challenges in simultaneously achieving high detection accuracy, memory efficiency, and fast update throughput, as we reveal. To tackle this conundrum, we propose a versatile and elegant sketch framework named Tight-Sketch, which supports a spectrum of heavy-based detection tasks. Recognizing that, in practice, most items are cold (non-heavy/persistent/significant), we implement distinct eviction strategies for different item types. This approach allows us to swiftly discard potentially cold items while offering enhanced protection to hot ones (heavy/persistent/significant). Additionally, we introduce an eviction method based on stochastic decay, ensuring that Tight-Sketch incurs only small one-sided errors without overestimation. To further enhance detection accuracy under extremely constrained memory allocations, we introduce Tight-Opt, a variant incorporating two optimization strategies. We conduct extensive experiments across various detection tasks to demonstrate that Tight-Sketch significantly outperforms existing methods in terms of both accuracy and update speed. Furthermore, by utilizing Single Instruction Multiple Data (SIMD) instructions, we enhance Tight-Sketch’s update throughput by up to 36%. We also implement Tight-Sketch on FPGA to validate its practicality and low resource overhead in hardware deployments.},
  archive      = {J_TC},
  author       = {Weihe Li and Paul Patras},
  doi          = {10.1109/TC.2025.3604467},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient sketching for heavy item-oriented data stream mining with memory constraints},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems. <em>TC</em>, 1-12. (<a href='https://doi.org/10.1109/TC.2025.3604468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multiprocessor systems scale up, h-extra connectivity and h-extra diagnosability serve as two pivotal metrics for assessing the reliability of the underlying interconnection networks. To ensure that each component of the survival graph holds no fewer than h + 1 vertices, the h-extra connectivity and h-extra diagnosability have been proposed to characterize the fault tolerability and self-diagnosing capability of networks, respectively. Many efforts have been made to establish the quantifiable relationship between these metrics but it is less than optimal. This work addresses the flaws of the existing results and proposes a novel proof to determine the metric relationship between h-extra connectivity and h-extra diagnosability under the PMC and MM* models. Our approach overcomes the defect of previous results by abandoning the network’s regularity and independence number. Furthermore, we apply the suggested metric to establish the h-extra diagnosability of a new network class, named generalized exchanged X-cube-like network GEXC(s, t), which takes dual-cube-like network, generalized exchanged hypercube, generalized exchanged crossed cube, and locally generalized exchanged twisted cube as special cases. Finally, we propose the h-extra diagnosis strategy (h-EDS) and design two self-diagnosis algorithms AhED-PMC and AhED-MM*, and then conduct experiments on GEXC(s, t) and the real-world network DD-g648 to show the high accuracy and superior performance of the proposed algorithms.},
  archive      = {J_TC},
  author       = {Yifan Li and Shuming Zhou and Sun-Yuan Hsieh and Qifan Zhang},
  doi          = {10.1109/TC.2025.3604468},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-path bound for parallel tasks with conditional branches. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3604469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel execution and conditional execution are increasingly prevalent in modern embedded systems. In real-time scheduling, a fundamental problem is how to upper-bound the response times of a task. Recent work applied the multi-path technique to reduce the response time bound for tasks with parallel execution, but left tasks with conditional execution as an open problem. This paper focuses on upper-bounding response times for tasks with both parallel execution and conditional execution using the multi-path technique. By designing a delicate abstraction regarding the multiple paths of various conditional branches, we derive a new response time bound. We further apply this response time bound into the scheduling of multiple parallel tasks with conditional branches. Experiments demonstrate that the proposed bound significantly advances the state-of-the-art, reducing the response time bound by 9.4% and improving the schedulability by 31.2% on average.},
  archive      = {J_TC},
  author       = {Qingqiang He and Nan Guan and Zhe Jiang and Mingsong Lv},
  doi          = {10.1109/TC.2025.3604469},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-path bound for parallel tasks with conditional branches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-power multiplier designs by leveraging correlations of 2×2 encoded partial products. <em>TC</em>, 1-8. (<a href='https://doi.org/10.1109/TC.2025.3604478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipliers, particularly those with small bit widths, are essential for modern neural network (NN) applications. In addition, multiple-precision multipliers are in high demand for efficient NN accelerators; therefore, recursive multipliers used in low-precision fusion schemes are gaining increasing attention. In this work, we design exact recursive multipliers based on customized approximate full adders (AFAs) for low-power purposes. Initially, the partial products (PPs) encoded by 2×2 multiplications are analyzed, which reveals the correlations among adjacent PPs. Based on these correlations, we propose 4×4 recursive multiplier architectures where certain full adders (FAs) can be simplified without affecting the correctness of the multiplication. Manually and synthesis tool-based FA simplifications are performed separately. The obtained 4×4 multipliers are then used to construct 8×8 multipliers based on a low-power recursive architecture. Finally, the proposed signed and unsigned 4×4 and 8×8 multipliers are evaluated using a 28nm CMOS technology. Compared with DesignWare (DW) multipliers, the proposed signed and unsigned 4×4 multipliers achieve power reductions of 16.5% and 11.6%, respectively, without compromising area or delay; alternatively, the delay can be reduced by 20.9% and 39.4%, respectively, without compromising power or area. For signed and unsigned 8×8 multipliers, the maximum power reductions are 9.7% and 13.7%, respectively, albeit with a trade-off in area.},
  archive      = {J_TC},
  author       = {Ao Liu and Siting Liu and Hui Wang and Qin Wang and Fabrizio Lombardi and Zhigang Mao and Honglan Jiang},
  doi          = {10.1109/TC.2025.3604478},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-power multiplier designs by leveraging correlations of 2×2 encoded partial products},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StageWise: Accelerating persistent key-value stores by thread model redesigning. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of fast NVMe SSDs, key-value stores are becoming more CPU-efficient in order to reap their bandwidth. However, current CPU-optimized key-value stores adopt suboptimal intra- and inter-thread models, hence incurring memory-level stalling and load imbalance that hinder cores from realizing their full potential. We present STAGEWISE, an CPU-efficient key-value store on fast NVMe SSDs with high throughput. To achieve this, we introduce a new thread model for StageWise to process KV requests. Specifically, STAGEWISE converts the processing of each KV request into multiple asynchronous stages, and thus enables pipelining across all stages. STAGEWISE further introduces a client-driven share-index architecture to ease inter-thread load imbalance and maximize the pipelining opportunity. Guided by Little’s Law, STAGEWISE improves concurrency, and therefore efficiently uses CPU to reach higher throughput. Extensive experimental results show that STAGEWISE outperforms CPUoptimized key-value stores (e.g., KVell) by up to 3.5× with writeintensive workloads, and storage-optimized ones (e.g., RocksDB) by over an order of magnitude. STAGEWISE also shows higher read performance and excellent scalability under various workloads.},
  archive      = {J_TC},
  author       = {Zeqi Li and Youmin Chen and Qing Wang and Youyou Lu and Jiwu Shu},
  doi          = {10.1109/TC.2025.3605763},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {StageWise: Accelerating persistent key-value stores by thread model redesigning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competition-style sorting networks (CSN): A framework for hardware-based sorting operations. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting operations are considered to be a significant part of any computer system and are widely used in many applications. In applications where sorting has to be efficiently accomplished (i.e., in O(1) time) on small-sized entries, hardware accelerators, such as ASICs, FPGAs, or GPUs, are used to speed up the sorting operations. In the literature, the bitonic sort algorithm (or variants thereof) is still considered to be the most commonly used approach in many hardware sort implementations for decades. However, the time complexity of the bitonic sort is O((log(n))2) for sorting n elements, which does not satisfy the constant-time constraint we demand for our setting. In this paper, we propose competition-style sorting networks (CSNs), a framework for designing hardware-based competition-style class of sorting networks that captures all forms of two-stage sorting networks where the first stage (competition) consists of pairwise comparisons and the second stage (evaluation) ranks the entries and sorts them. To illustrate the utility of this framework, we develop and test one instance of this design, called the Competition Sort Algorithm (CSA), which has a time complexity of O(1), and specifically, one clock cycle. We implemented and tested CSA on both an Intel Cyclone V FPGA and the NVIDIA Quadro T1000 GPU then measured its gain, which combines the trade-offs between the relative speedup and the relative area increase, against the bitonic sort. Our results show that the CSA achieves a significant gain of up to 11.01× on the FPGA and a relative speedup of up to 3.32× on the GPU. We also compare the area, power, and latency of CSA with the bitonic sort algorithm on the FPGA.},
  archive      = {J_TC},
  author       = {Abbas A. Fairouz and Jassim M. Aljuraidan and Ameer Mohammed},
  doi          = {10.1109/TC.2025.3605766},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Competition-style sorting networks (CSN): A framework for hardware-based sorting operations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Multi-access Edge Computing (MEC) has empowered Internet of Things (IoT) devices and edge servers to deploy sophisticated Deep Neural Network (DNN) applications, enabling real-time inference. Many concurrent inference requests and intricate DNN models demand efficient multi-DNN inference in MEC networks. However, the resource-limited IoT device/edge server and expanding model size force models to be dynamically deployed, resulting in significant undesired energy consumption. In addition, parallel multi-DNN inference on the same device complicates the inference process due to the resource competition among models, increasing the inference latency. In this paper, we propose a Resource-aware and Dynamic DNN Deployment (R3D) scheme with the collaboration of end-edge-cloud. To mitigate resource competition and waste during multi-DNN parallel inference, we develop a Resource Adaptive Management (RAM) algorithm based on the Roofline model, which dynamically allocates resources by accounting for the impact of device-specific performance bottlenecks on inference latency. Additionally, we design a Deep Reinforcement Learning (DRL)-based online optimization algorithm that dynamically adjusts DNN deployment strategies to achieve fast and energy-efficient inference across heterogeneous devices. Experiment results demonstrate that R3D is applicable in MEC environments and performs well in terms of inference latency, resource utilization, and energy consumption.},
  archive      = {J_TC},
  author       = {Tong Zheng and Yuanguo Bi and Guangjie Han and Xingwei Wang and Yuheng Liu and Yufei Liu and Xiangyi Chen},
  doi          = {10.1109/TC.2025.3605749},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal embodied carbon models with dual carbon attribution for embodied carbon accounting of computer systems. <em>TC</em>, 1-13. (<a href='https://doi.org/10.1109/TC.2025.3605743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied carbon is the carbon emissions in the manufacturing process of products, which dominates the overall carbon footprint in many industries. Existing studies derive the embodied carbon through life cycle analysis (LCA) reports. Current LCA reports only provide the carbon emission of a product class, e.g. 28nm CPU, whereas a product instance can be made in various regions and time periods. Carbon emissions depend on the electricity generation process, which has spatial-temporal dynamics. Therefore, the embodied carbon of a product instance can differ from its product class. Additionally, different carbon attribution methods (e.g., location-based and market-based) can affect the carbon emissions of electricity, thus further affecting the embodied carbon of products. In this paper, we present new Spatial-Temporal Embodied Carbon (STEC) accounting models with dual attribution methods. We observe significant differences between STEC and current models, e.g., for 7nm CPU the difference is 13.69%. We further examine the impact of STEC models on existing embodied carbon accounting schemes on computer applications, such as Large Language Model (LLM) training and LLM inference. We observe that using STEC results in much greater differences in the embodied carbon of certain applications as compared to others (e.g., 32.26% vs. 6.35%).},
  archive      = {J_TC},
  author       = {Xiaoyang Zhang and Yijie Yang and Dan Wang},
  doi          = {10.1109/TC.2025.3605743},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spatial-temporal embodied carbon models with dual carbon attribution for embodied carbon accounting of computer systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OOLU: An operation-based optimized sparse LU decomposition accelerator for circuit simulation. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As scientific and engineering challenges grow in complexity and scale, the demand for effective solutions for sparse matrix computations becomes increasingly critical. LU decomposition, known for its ability to reduce computational load and enhance numerical stability, serves as a promising approach. This study focuses on accelerating sparse LU decomposition for circuit simulations, addressing the prolonged simulation times caused by large circuit matrices. We present a novel Operation-based Optimized LU (OOLU) decomposition architecture that significantly improves circuit analysis efficiency. OOLU employs a VLIW-like processing element array and incorporates a scheduler that decomposes computations into a fine-grained operational task flow graph, maximizing inter-operation parallelism. Specialized scheduling and data mapping strategies are applied to align with the adaptable pipelined framework and the characteristics of circuit matrices. The OOLU architecture is prototyped on an FPGA and validated through extensive tests on the University of Florida sparse matrix collection, benchmarked against multiple platforms. The accelerator achieves speedups ranging from 3.48× to 32.25× (average 12.51×) over the KLU software package. It also delivers average speedups of 2.64× over a prior FPGA accelerator and 25.18× and 32.27× over the GPU accelerators STRUMPACK and SFLU, respectively, highlighting the substantial efficiency gains our approach delivers.},
  archive      = {J_TC},
  author       = {Ke Hu and Fan Yang},
  doi          = {10.1109/TC.2025.3605751},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OOLU: An operation-based optimized sparse LU decomposition accelerator for circuit simulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JCSRC: Joint client selection and resource configuration for energy-efficient multi-task federated learning. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables privacy-preserving distributed machine learning by training models on edge client devices using their local data without revealing their raw data. In edge environments, various applications require different neural network models, making it crucial to perform joint training of multiple models on edge devices, known as multi-task FL. While existing multi-task FL approaches enhance resource utilization on edge devices through adaptive resource configuration or client selection, optimizing either of these aspects alone may lead to suboptimality. Therefore, in this paper, we explore a joint client selection and resource configuration method called JCSRC for multi-task FL, aiming to maximize energy efficiency in environments with limited computation and communication resources and heterogeneous client devices. Firstly, we formalize this problem as a mixed-integer nonlinear programming problem considering all these characteristics and prove its NP-hardness. To address this problem, we first design a multi-agent reinforcement learning (MARL)-based client selection method that selects appropriate clients for each task to train their models. The MARL method makes client selection decisions based on the clients’ data quality, energy efficiency, communication, and computation capacity to ensure fast convergence and energy efficiency. Then, we design a particle swarm optimization (PSO)-based resource configuration scheme that configures appropriate computation and bandwidth resources for each task on each client. The PSO scheme makes resource configuration decisions based on theoretically derived optimal CPU frequency and bandwidth to achieve high energy efficiency. Finally, we carry out extensive simulations and testbed-based experiments to validate our proposed JCSRC. The results demonstrate that, in comparison to state-of-the-art solutions, JCSRC can save energy consumption by up to 59% to achieve the target accuracy.},
  archive      = {J_TC},
  author       = {Junpeng Ke and Junlong Zhou and Dan Meng and Yue Zeng and Yizhou Shi and Xiangmou Qu and Song Guo},
  doi          = {10.1109/TC.2025.3605765},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {JCSRC: Joint client selection and resource configuration for energy-efficient multi-task federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the performance demands and stringent timing requirements of safety-critical systems like avionics and autonomous vehicles, research has focused on providing timing guarantees for the scheduling of Directed Acyclic Graph (DAG) tasks in multicore systems. The structural complexity and timing anomalies make this problem challenging. Existing methods bound the Worst-Case Response Time (WCRT) of tasks through static analysis, but these bounds are complicated, difficult to validate, and often remain pessimistic for many scheduling scenarios. Runtime intervention can be effective in eliminating timing anomalies and providing timing guarantees; however, it is ineffective for anomaly-free scheduling scenarios, leads to non-work-conserving schedules, and incurs additional overhead. This paper proposes a hybrid approach to identify timing anomalies in DAG scheduling scenarios within a system, providing tighter WCRT solutions. The static analysis first offers a sufficient anomaly test to directly identify some anomaly-free DAG scheduling scenarios. Leveraging a wide range of scheduling data collected from the running system or its simulator, we then apply a machine learning approach to train a binary classification model, achieving an accuracy of 99.5%. Identifying the anomaly status enables the application of more precise WCRT bounds for different scheduling scenarios, leading to improved system performance. Specifically, we shorten the WCRT bounds for anomaly-free DAG scheduling by an average of up to 21.58%, with a maximum reduction of up to 55.47% compared to the state-of-the-art method.},
  archive      = {J_TC},
  author       = {Nan Chen and Xiaotian Dai and Alan Burns and Iain Bate},
  doi          = {10.1109/TC.2025.3603674},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic modeling of intrusion tolerant systems based on redundancy and diversity. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3606189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with unforeseen attacks to software systems in critical application domains, redundancy-based ITSs schemes are among popular countermeasures to deploy. Designing the adequate ITS for the stated security requirements calls for stochastic analysis supports, able to assess the impact of variety of attack patterns on different ITS configurations. As contribution to this purpose, a stochastic model for ITS is proposed, whose novel aspects are the ability to account for both camouflaging components and for correlation aspects between the security failures affecting the diverse implementations of the software cyber protections adopted in the ITS. Extensive analyses are conducted to show the applicability of the model; the obtained results allow to understand the limits and strengths of selected ITS configurations when subject to attacks occurring in unfavorable conditions for the defender.},
  archive      = {J_TC},
  author       = {Silvano Chiaradonna and Felicita Di Giandomenico and Giulio Masetti},
  doi          = {10.1109/TC.2025.3606189},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stochastic modeling of intrusion tolerant systems based on redundancy and diversity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIVIDE: Efficient RowHammer defense via in-DRAM cache-based hot data isolation. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3603729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RowHammer poses a serious reliability challenge to modern DRAM systems. As technology scales down, DRAM resistance to RowHammer has decreased by 30× over the past decade, causing an increasing number of benign applications to suffer from this issue. However, existing defense mechanisms have three limitations: 1) they rely on inefficient mitigation techniques, such as time-consuming victim row refresh; 2) they do not reduce the number of effective RowHammer attacks, leading to frequent mitigations; and 3) they fail to recognize that frequently accessed data is not only a root cause of RowHammer but also presents an opportunity for performance optimization. In this paper, we observe that frequently accessed hot data plays a distinct role in security and efficiency: it can induce RowHammer by interfering with adjacent cold data, while also being performance-critical due to its frequent accesses. To this end, we propose Data Isolation via In-DRAM Cache (DIVIDE), a novel defense mechanism that leverages in-DRAM cache to isolate and exploit hot data. DIVIDE offers three key benefits: 1) It reduces the number of effective RowHammer attacks, as hot data in the cache cannot interfere with each other. 2) It provides a simple yet effective mitigation measure by isolating hot data from cold data. 3) It caches frequently accessed hot data, improving average access latency. DIVIDE employs a two-level protection structure: the first level mitigates RowHammer in cache arrays with high efficiency, while the second level addresses the remaining threats in normal arrays to ensure complete protection. Owing to the high in-DRAM cache hit rate, DIVIDE efficiently mitigates RowHammer while preserving both the performance and energy efficiency of the in-DRAM cache. At a RowHammer threshold of 128, DIVIDE with probabilistic mitigation achieves an average performance improvement of 19.6% and energy savings of 20.4% over DDR4 DRAM for fourcore workloads. Compared to an unprotected in-DRAM cache DRAM, DIVIDE incurs only a 2.1% performance overhead while requiring just a modest 1KB per-channel CAM in the memory controller, with no modification to the DRAM chip.},
  archive      = {J_TC},
  author       = {Haitao Du and Yuxuan Yang and Song Chen and Yi Kang},
  doi          = {10.1109/TC.2025.3603729},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DIVIDE: Efficient RowHammer defense via in-DRAM cache-based hot data isolation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical signature-free multivalued validated byzantine agreement and asynchronous common subset in constant time. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3607476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract—Asynchronous common subset (ACS) is a powerful paradigm enabling applications such as Byzantine fault-tolerance (BFT) and multi-party computation (MPC). The most efficient ACS framework in the information-theoretic setting is due to Ben-Or, Kelmer, and Rabin (BKR, 1994). The BKR ACS protocol has been both theoretically and practically impactful. BKR ACS has an O(log $n$) running time (where $n$ is the number of replicas) due to the usage of $n$ parallel asynchronous binary agreement (ABA) instances, impacting both performance and scalability. Indeed, for a network of 16∼64 replicas, the parallel ABA phase occupies about 95%∼97% of the total runtime. A long-standing open problem is whether we can build an ACS framework with O(1) time while not increasing the message or communication complexity of the BKR protocol. We resolve the open problem, presenting the first constant-time ACS protocol with O($n$3) messages in the information-theoretic and signature-free settings. Our key ingredient is the first information-theoretic and constant-time multivalued validated Byzantine agreement (MVBA) protocol. Our results can improveߞasymptotically and concretelyߞvarious applications using ACS and MVBA. As an example, we implement FIN, a BFT protocol instantiated using our framework. Via a 121-server deployment on Amazon EC2, we show FIN reduces the overhead of the ABA phase to as low as 1.23% of the total runtime.},
  archive      = {J_TC},
  author       = {Xin Wang and Xiao Sui and Sisi Duan and Haibin Zhang},
  doi          = {10.1109/TC.2025.3607476},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Practical signature-free multivalued validated byzantine agreement and asynchronous common subset in constant time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid kernels: Seamlessly conquering the embedded computing continuum. <em>TC</em>, 1-14. (<a href='https://doi.org/10.1109/TC.2025.3605745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve seamless portability across the embedded computing continuum, we introduce a new kernel architecture: fluid kernels. Fluid kernels can be thought of as the intersection between embedded unikernels and general purpose monolithic kernels, allowing to seamlessly develop applications both in kernel space and user space in a unified way. This scalable kernel architecture can manage the trade-off between performance, code size, isolation and security. We compare our fluid kernel implementation, Miosix, to Linux and FreeRTOS on the same hardware with standard benchmarks. Compared to Linux, we achieve an average speedup of 3.5× and a maximum of up to 15.4×. We also achieve an average code size reduction of 84% and a maximum of up to 90%. By moving application code from user space to kernel space, an additional code size reduction up to 56% and a speedup up to 1.3× can be achieved. Compared to FreeRTOS, the use of Miosix only costs a moderate amount of code size (at most 47KB) for significant advantages in application performance with speedups averaging at 1.5× and up to 5×.},
  archive      = {J_TC},
  author       = {Federico Terraneo and Daniele Cattaneo},
  doi          = {10.1109/TC.2025.3605745},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fluid kernels: Seamlessly conquering the embedded computing continuum},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
