<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 14</h2>
<ul>
<li><details>
<summary>
(2025). Action quality assessment via hierarchical pose-guided multi-stage contrastive regression. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA), which aims at the automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains different durations. However, existing methods focus on segmenting the video into fixed frames, which disrupts the temporal continuity of sub-actions resulting in unavoidable prediction errors. To address these challenges, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. Firstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture fine-grained spatio-temporal visual and skeletal features. Compared to mask or auxiliary visual features, skeletal features provide a more accurate representation during athletic movements. Then, a procedure segmentation network is introduced to separate different sub-actions and obtain segmented features. Afterwards, the segmented visual and skeletal features are both fed into a multi-modal fusion module as physics structural priors, to guide the model in learning refined activity similarities and variances. Finally, a multi-stage contrastive learning regression approach is employed to learn discriminative representations and output prediction results. In addition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the current low-quality human pose labels. In experiments, the results on FineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority of our proposed approach. Our source code and dataset are available at https://github.com/Lumos0507/HP-MCoRe.},
  archive      = {J_TIP},
  author       = {Mengshi Qi and Hao Ye and Jiaxuan Peng and Huadong Ma},
  doi          = {10.1109/TIP.2025.3613952},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action quality assessment via hierarchical pose-guided multi-stage contrastive regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super-resolving dynamic scenes with spike camera via multi-frame sequential alignment with motion propagation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike camera is a neuromorphic sensor that can capture high-speed dynamic scenes by firing a continuous stream of binary spikes with extremely high temporal resolution, essentially forming a dense sampling in the temporal dimension. Due to the relative motion between camera and scene, each pixel is actually sampling at a large number of different spatial positions on the object in a short period. Converting this dense sampling from temporal dimension to spatial domain, high resolution images can be reconstructed from the spike stream. However, spike fluctuations and large motion in high-speed scenes pose great challenges for this task, especially for intensity information extraction and temporal alignment. In this paper, we propose a spike camera super resolution network to address these issues. Considering the local temporal correlation of spike stream and correlation consistency within a local region, we introduce a representation module that performs region-adaptive temporal filtering on spikes to mitigate fluctuations and extract stable intensity information from binary data. Additionally, we develop a module for multi-frame feature alignment, leveraging the long-term temporal information of spike stream. To handle large motions, we propagate the motion information from neighboring moment to current feature alignment module, which provides a prior that helps to narrow the search range for current motion offset, improving the accuracy of temporal alignment. Experimental results demonstrate that the proposed network achieves state-of-the-art performance on synthetic and real-captured spike data.},
  archive      = {J_TIP},
  author       = {Yuanlin Wang and Yiyang Zhang and Ruiqin Xiong and Jian Zhang and Xinfeng Zhang and Tiejun Huang},
  doi          = {10.1109/TIP.2025.3613943},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Super-resolving dynamic scenes with spike camera via multi-frame sequential alignment with motion propagation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGHL: Anchor-guided point cloud registration network with hybrid local feature perception. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration, which estimates a rigid transformation matrix between two point clouds, is a fundamental process in numerous applications. While existing detector-free techniques present exceptional performance, they overlook the extraction of hybrid local features that capture correlations between points and their neighbours, thereby limiting the quality of point cloud recognition. Moreover, these approaches typically treat point clouds as sequential data and employ the transformer to integrate global context from all points, which inevitably introduces interference from irrelevant regions, hence affecting the registration accuracy. In this work, we propose a novel detector-free approach AGHL to address these challenges. For the first issue, AGHL introduces a hybrid local feature perception module that designs two parallel branches to concurrently extract low-level and high-level local features, which effectively encode the correlations between each point and its neighborhood points in both Euclidean space and high-dimensional feature space. For the second issue, AGHL develops an anchor-guided cross attention that adheres to the local geometric consistency to constrain the networkâ€™s attention on reliable anchors, thereby effectively suppressing interference from irrelevant regions. Benefiting from these techniques, AGHL achieves impressive point cloud registration accuracy across all synthetic, indoor, and outdoor datasets. Furthermore, we build an experimental platform and conduct a real-world robot localization experiment, with results showing the strong generalization ability of AGHL.},
  archive      = {J_TIP},
  author       = {Kun Dai and Tao Xie and Zhiqiang Jiang and Ke Wang and Ruifeng Li and Lijun Zhao and Chuqing Cao},
  doi          = {10.1109/TIP.2025.3613987},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AGHL: Anchor-guided point cloud registration network with hybrid local feature perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BITS: Bit-extendable incremental hashing in open environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an effective technique for large-scale image retrieval. However, traditional hashing models typically follow a closed-set assumption, which fails to satisfy the practicality of real-world tasks. In this paper, we explore a meaningful yet overlooked question: is there a hashing paradigm that not only supports rehearsal-free online incremental coding for single-pass data streams but also adapts to potentially expanding concept spaces in open environments? Instead of presetting fixed bit lengths, we suggest adjusting the bit length dynamically based on the number of encountered categories, meanwhile enabling bit extension of existing hash codes to match the adaptive code lengths without knowledge forgetting. Therefore, we propose a Bit-extendable IncremenTal haShing (BITS) method for image retrieval in open environments. Specifically, we identify a blurry incremental setup to better simulate realistic scenarios, revisiting the widely-used data-incremental and class-incremental settings. With this challenging setup, a three-phase framework is designed to efficiently perform incremental hashing, which jointly solves online continual coding and bit extension with adaptive code lengths. Through the well-designed hashing paradigm, BITS achieves comparable performance to offline hashing methods while significantly saving computational resources. Comprehensive experiments on six benchmarks demonstrate the superiority of our BITS in dynamic scenarios. The source code is available at https://github.com/yxinwang/BITS.},
  archive      = {J_TIP},
  author       = {Yongxin Wang and Zhen-Duo Chen and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2025.3613924},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BITS: Bit-extendable incremental hashing in open environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust reversible watermarking with invisible distortion against VAE watermark removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Moment-based Robust Reversible Watermarking (OM-RRW) is crucial for intellectual property protection, providing the dual benefits of robustness and reversibility. However, OM-RRW embeds watermarks into visually sensitive global low-frequency features, which easily leads to ring-like distortions that expose watermark locations, making them vulnerable to removal through image inpainting. To address this issue, this paper makes the first attempt to introduce an innovative strategy to eliminate these visible distortions, thereby overcoming OM-RRWâ€™s inherent limitations. The strategy innovates on two fronts: first, it customizes varying embedding step sizes based on the stability differences of moment values to minimize distortion; second, it designs a texture-aware adaptive basis function fine-tuning strategy. This strategy adjusts the representation capability of the basis functions in different regions based on the human eyeâ€™s sensitivity to various texture areas, helping to avoid visible ring-like distortions. The performance of the proposed method is evaluated using Polar Harmonic Transform (PHT) moments, comprising three moments that exhibit remarkable performance in existing OM-RRW methods. Extensive experiments show that the proposed method can embed 128-bit watermarks with no visible distortions while minimizing the loss of robustness. In addition, this paper finds that OM-RRW demonstrates satisfactory robustness against VAE watermark removal attacks.},
  archive      = {J_TIP},
  author       = {Bobiao Guo and Ping Ping and Fan Liu and Feng Xu},
  doi          = {10.1109/TIP.2025.3613958},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust reversible watermarking with invisible distortion against VAE watermark removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of blood vessels is essential for various clinical assessments and postoperative analyses. However, the inherent challenges of vascular imagingâ€”such as sparsity, fine granularity, low contrast, data distribution variability, and the critical need for preserving topological integrityâ€”make generalized vessel segmentation particularly complex. While specialized segmentation methods have been developed for specific anatomical regions, their over-reliance on tailored models hinders broader applicability and generalization. General-purpose segmentation models introduced in medical imaging often fail to address critical vascular characteristics, including the connectivity of segmentation results. In this study, we propose OVS-Net, an optimized vessel segmentation framework designed to generalize across diverse vessel structures and imaging modalities. It introduces a dual-branch architecture design for improving small vessel segmentation and a morphology-aware correction module to preserve vascular topology and connectivity. We compiled a comprehensive multi-modality dataset from 17 sources to train and benchmark the proposed OVS-Net against 6 SAM-based methods and 17 expert models under various conditions. The results demonstrate that our approach achieves superior segmentation accuracy, generalization, and a 34.6% improvement in connectivity, underscoring its potential for clinical applications. The code and dataset will be released at github.com.},
  archive      = {J_TIP},
  author       = {Dongning Song and Weijian Huang and Jiarun Liu and Md Jahidul Islam and Hao Yang and Shuqiang Wang and Hairong Zheng and Shanshan Wang},
  doi          = {10.1109/TIP.2025.3607583},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring vision-based active 3D object detection by informativeness characterization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based 3D object detection (3DOD) gains lots of attention due to its low cost for deployment compared to Lidar-based tasks, while it suffers from labor-expensive data annotations. At the same time, active learning (AL) has shown great potential in reducing annotation costs in related tasks, which can maximize model performance within very limited labeled data. In this paper, we explore active learning for vision-based 3DOD for the first time. Inspired by the entropy analysis, we involve three concerns to characterize the sample informativeness: sample diversity in input space, feature informativeness in BEV space, and result distribution in prediction space. Based on these concerns, we propose a novel AL framework named HMAD, which utilizes Height Modeling and Adaptive Diversity-based sampling for comprehensive informativeness characterization. In HMAD, we first propose a novel height-guided adversarial module in BEV space, which measures the informativeness of height modeling for 2D-to-3D mapping in an adversarial manner. Furthermore, Budget-aware SpatioTemporal diversity Sampling (BSTS) and Class Balance Sampling (CBS) are proposed to adaptively measure the sample informativeness in input and prediction space, respectively. Finally, the three components are integrated into a two-stage sampling strategy, with which the most informative samples can be selected and annotated for the next iteration. Experiments evidence that HMAD achieves comparable performances by only using 50% annotated training data, and can generalize well on different conditions.},
  archive      = {J_TIP},
  author       = {Ruixiang Li and Yiming Wu and Yehao Lu and Xuewei Li and Xian Wang and Xiubo Liang and Xi Li},
  doi          = {10.1109/TIP.2025.3613927},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring vision-based active 3D object detection by informativeness characterization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the low-light video enhancement: Benchmark datasets and methods. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light video enhancement is a critical task in computer vision with a wide range of applications. However, there is a lack of high-quality benchmark datasets in this field. To address this issue, we collect a high-quality low-light video dataset using a well-designed camera system. The videos in our dataset feature apparent camera motion and strict spatial alignment. In order to achieve general low-light video enhancement, we propose a Retinex-based method called Light Adjustable Network (LAN). LAN iteratively adjusts the brightness and adapts to different lighting conditions in various real-world scenarios, producing visually appealing results. We further develop a new dataset capture method and low-light video enhancement method to address the limitation of our previous dataset in capturing dynamic scenes and previous method. The new camera setup and capture method enable the recording of real continuous videos and generate the new dataset. Our new low-light video enhancement method, LAN++, leverages a new inter-frame relationship, difference images. It utilizes the texture information contained in the difference images of dynamic scenes to supplement the high-frequency details of the original features, which produce sharper and more realistic output images. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset and code will be publicly available.},
  archive      = {J_TIP},
  author       = {Jiaxuan Wang and Huiyuan Fu and Wenkai Zheng and Xicong Wang and Xin Wang and Heng Zhang and Huadong Ma},
  doi          = {10.1109/TIP.2025.3616639},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking the low-light video enhancement: Benchmark datasets and methods},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S3OIL: Semi-supervised SAR-to-optical image translation via multi-scale and cross-set matching. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation has achieved great success, but still faces the significant challenge of limited paired data, particularly in translating Synthetic Aperture Radar (SAR) images to optical images. Furthermore, most existing semi-supervised methods place limited emphasis on leveraging the data distribution. To address those challenges, we propose a Semi-Supervised SAR-to-Optical Image Translation (S3OIL) method that achieves high-quality image generation using minimal paired data and extensive unpaired data while strategically exploiting the data distribution. To this end, we first introduce a Cross-Set Alignment Matching (CAM) mechanism to create local correspondences between the generated results of paired and unpaired data, ensuring cross-set consistency. In addition, for unpaired data, we apply weak and strong perturbations and establish intra-set Multi-Scale Matching (MSM) constraints. For paired data, intra-modal semantic consistency (ISC) is presented to ensure alignment with the ground truth. Finally, we propose local and global cross-modal semantic consistency (CSC) to boost structural identity during translation. We conduct extensive experiments on SAR-to-optical datasets and another sketch-to-anime task, demonstrating that S3OIL delivers competitive performance compared to state-of-the-art unsupervised, supervised, and semi-supervised methods, both quantitatively and qualitatively. Ablation studies further reveal that S3OIL can ensure the preservation of both semantic content and structural integrity of the generated images. Our code is available at: https://github.com/XduShi/SOIL.},
  archive      = {J_TIP},
  author       = {Xi Yang and Haoyuan Shi and Ziyun Li and Maoying Qiao and Fei Gao and Nannan Wang},
  doi          = {10.1109/TIP.2025.3616576},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {S3OIL: Semi-supervised SAR-to-optical image translation via multi-scale and cross-set matching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A shared-memory parallel alpha-tree algorithm for extreme dynamic ranges. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Î±-tree is an effective hierarchical image representation used for connected filtering or segmentation in remote sensing and other image applications. The Î±-tree constructs a tree based on the dissimilarities of the pixels in an image. Compared to other hierarchical image representations such as the component tree, the Î±-tree provides a better representation of the granularity of images and is easier to apply to multichannel images. The major drawback of the Î±-tree is its processing speed, due to the large amount of data to be processed and the lack of studies on an efficient algorithms, especially on multichannel and high dynamic range images. In this study, we introduce a novel adaptation of the hybrid component tree algorithm on the Î±-tree for fast parallel Î±-tree construction in any dynamic range of pixel dissimilarity. We tested the hybrid Î±-tree algorithm on Sentinel-2 remote sensing images from the European Space Agency (ESA) as well as randomly generated images, on the HÃ¡brÃ³k high performance computing cluster. Experimental results show that the hybrid Î±-tree algorithm achieves the processing speed of 10â€“30Mpix/s and the speedup of 10â€“30 on a 128-core computer, proving the efficiency of the first parallel Î±-tree algorithm in high dynamic range, to the best of our knowledge.},
  archive      = {J_TIP},
  author       = {Jiwoo Ryu and Scott C. Trager and Michael H.F. Wilkinson},
  doi          = {10.1109/TIP.2025.3616578},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A shared-memory parallel alpha-tree algorithm for extreme dynamic ranges},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFIG-palm: Controllable palmprint generation via pixel and feature identity guidance. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint recognition offers a promising solution for convenient and private authentication. However, the scarcity of large-scale palmprint datasets constrains its development and application. Recent approaches have sought to mitigate this issue by synthesizing palmprints based on BÃ©zier curves. Due to the lack of paired data between curves and palmprints, it is difficult to generate curve-driven palmprints with precise identity. To address this challenge, we propose a novel Pixel and Feature Identity Guidance (PFIG) framework to synthesize realistic palmprints, whose IDs are strictly governed by the BÃ©zier curves. In order to establish ID mapping, an ID Injection (IDI) module is constructed to synthesize pseudo-paired data. Two cross-domain ID consistency losses at pixel and feature levels are further proposed to strictly preserve the semantic information of the input ID curves. Experimental results demonstrate that our ID-guided approach can synthesize more realistic palmprints with controllable identities. Based on only 80,000 synthesized palmprints for pre-training, the recognition accuracy can be improved by more than 18% in terms of TAR@1e-6. When trained exclusively on synthetic data, our method achieves superior performance to existing synthetic approaches. The source code is available at https://github.com/YuchenZou/PFIG-Palm.},
  archive      = {J_TIP},
  author       = {Yuchen Zou and Huikai Shao and Chengcheng Liu and Siyu Zhu and Zongqing Hou and Dexing Zhong},
  doi          = {10.1109/TIP.2025.3616611},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PFIG-palm: Controllable palmprint generation via pixel and feature identity guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust single-shot 3D reconstruction by sparse-to-dense stereo matching and spline function based parallax modeling. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-shot 3D surface imaging techniques with high accuracy and high resolution are very important in both academia and industry. In this paper, we propose a sparse-to-dense structured light (SL) line-pattern based active stereo vision (ASV) approach to reconstruct the 3D shapes robustly with high-resolution. We propose a sparse-to-dense stereo matching (SDSM) method to solve the challenging problem of line clustering and line matching. We design the structured light line pattern with four colors and the distances between lines of different color range from sparse to dense. Accordingly, the sparse color lines could be clustered and matched at first while the dense color lines could be matched subsequently with the constraint of the clustered and matched sparse color lines. After all the color lines are matched, a spline-function based parallax model (SFPM) is computed based on the points on the matched color lines. Then, the depths of the points in the regions between the color lines are computed by the parallax model. Experimental results show that the proposed SDSM-SFPM ASV approach is more robust than existing methods especially in reconstructing the complex 3D shapes.},
  archive      = {J_TIP},
  author       = {ZhenZhou Wang},
  doi          = {10.1109/TIP.2025.3616615},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust single-shot 3D reconstruction by sparse-to-dense stereo matching and spline function based parallax modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing feature learning with hard samples in mutual learning for online class incremental learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Class-Incremental Learning (OCIL) aims to solve the problem of incrementally learning new classes from a non-i.i.d. and single-pass data stream. Compared to the offline setting, OCIL is much closer to a live learning experience requiring higher model update frequency at less computational budget. Due to its one-epoch training constraint, the model is likely to learn non-essential features and encounter the under-fitting issue, which severely affects the modelâ€™s stability. In this paper, we investigate how to use hard samples to improve data variability, eventually enhancing feature learning and addressing the under-fitting problem. Specifically, by introducing a scoring function assessing the sample value, we build an OCIL formulation that simultaneously generates high-value samples and optimizes the OCIL model, improving generalization ability within the constraint of single-epoch training. Empirically, we found that strong data augmentation is a simple but effective way to generate a higher proportion of high-score samples. To make the most of these augmented samples, we design an OCIL model based on mutual learning with two networks of identical structures. Moreover, a collaborative learning mechanism is developed by aligning the features and class probabilities from the two networks to promote their interaction. Extensive experiments on three widely used datasets for OCIL have demonstrated the effectiveness of our method, obtaining superior performance to state-of-the-art methods. The code is available at https://github.com/susususushi/SDA-MCL.},
  archive      = {J_TIP},
  author       = {Guoqiang Liang and Shibin Su and De Cheng and Shizhou Zhang and Peng Wang and Yanning Zhang},
  doi          = {10.1109/TIP.2025.3616626},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing feature learning with hard samples in mutual learning for online class incremental learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-time video super-resolution with neural operator. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3616609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the task of space-time video super-resolution (STVSR). Existing methods generally suffer from inaccurate motion estimation and motion compensation (MEMC) problems for large motions. Inspired by recent progress in physics-informed neural networks, we model the challenges of MEMC in STVSR as a mapping between two continuous function spaces. Specifically, our approach transforms independent low-resolution representations in the coarse-grained continuous function space into refined representations with enriched spatiotemporal details in the fine-grained continuous function space. To achieve efficient and accurate MEMC, we design a Galerkin-type attention function to perform frame alignment and temporal interpolation. Due to the linear complexity of the Galerkin-type attention mechanism, our model avoids patch partitioning and offers global receptive fields, enabling precise estimation of large motions. The experimental results show that the proposed method surpasses state-of-the-art techniques in both fixed-size and continuous space-time video super-resolution tasks. Code is publicly available at the URL https://github.com/hahazh/STVSR-NO.},
  archive      = {J_TIP},
  author       = {Yuantong Zhang and Hanyou Zheng and Daiqin Yang and Zhenzhong Chen and Haichuan Ma and Wenpeng Ding},
  doi          = {10.1109/TIP.2025.3616609},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Space-time video super-resolution with neural operator},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
